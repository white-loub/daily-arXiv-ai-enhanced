<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 92]
- [cs.CL](#cs.CL) [Total: 75]
- [cs.AI](#cs.AI) [Total: 107]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.RO](#cs.RO) [Total: 36]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.LG](#cs.LG) [Total: 77]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading](https://arxiv.org/abs/2601.20881)
*Matteo Rossi*

Main category: cs.CV

TL;DR: 本文提出了一种名为MA-LipNet的多注意力唇读网络，通过通道、联合时空和分离时空三种注意力机制，提升唇读任务中视觉特征的判别性与泛化能力，在CMLR和GRID数据集上显著降低CER和WER。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法因口型动作细微，常面临视觉特征判别力弱、泛化能力差的问题。

Method: 提出MA-LipNet，依次引入通道注意力（CA）、联合时空注意力（JSTA）和分离时空注意力（SSTA）模块，分别在通道、粗粒度时空、细粒度时空三个维度净化视觉特征。

Result: 在CMLR和GRID数据集上，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），性能优于多个SOTA方法。

Conclusion: 多维（时、空、通道）特征精炼对鲁棒的视觉语音识别至关重要，MA-LipNet验证了该思路的有效性与先进性。

Abstract: Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\textit{Joint Spatial-Temporal Attention (JSTA)} and \textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.

</details>


### [2] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 本文提出了一种面向非马尔可夫式多轮对话图像生成的新范式，强调对长程历史依赖的建模，并设计了相应数据构造、训练框架与技术改进，显著提升了多轮一致性与指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话式图像生成方法大多基于马尔可夫假设（仅依赖最新图像），无法处理用户回溯早期状态、撤销操作或跨轮引用实体等真实复杂交互需求，因此需建模非马尔可夫依赖。

Method: 提出三方面方法：(i) 非马尔可夫多轮数据构建策略（如回滚式编辑、基于名称的跨轮个性化）；(ii) 历史条件化训练与推理框架，含token级缓存以防止身份漂移；(iii) 高保真重建与可编辑个性化改进（如重建式DiT detokenizer、多阶段微调课程）。

Result: 显式针对非马尔可夫交互训练后，模型在多轮一致性与指令遵循上显著提升，同时保持优秀的单轮编辑与个性化能力。

Conclusion: 非马尔可夫建模是提升对话式图像生成真实可用性的关键，所提数据、架构与训练策略共同构成了更鲁棒、更可控的多轮生成基础。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

</details>


### [3] [Text controllable PET denoising](https://arxiv.org/abs/2601.20990)
*Xuehua Ye,Hongxu Yang,Adam J. Schwarz*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本引导的PET图像去噪方法，利用预训练CLIP模型特征与U-Net结构结合，在单模型下实现多计数水平的高质量去噪，显著提升图像质量并具备缩短采集时间的潜力。


<details>
  <summary>Details</summary>
Motivation: PET图像常受复杂噪声干扰，影响诊断准确性；现有方法难以在宽范围计数水平下通用且高效去噪。

Method: 提出文本引导的去噪方法，融合预训练CLIP模型提取的语义特征与U-Net架构进行噪声建模与去除。

Result: 实验表明该方法在定性和定量评估中均取得显著提升，具备跨计数水平泛化能力及适应复杂去噪需求的灵活性。

Conclusion: 该文本引导框架为PET图像去噪提供了新思路，有望提升临床诊断质量并减少患者辐射暴露与扫描时间。

Abstract: Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.

</details>


### [4] [Low performing pixel correction in computed tomography with unrolled network and synthetic data training](https://arxiv.org/abs/2601.20995)
*Hongxu Yang,Levente Lippenszky,Edina Timko,Lehel Ferenczi,Gopal Avinash*

Main category: cs.CV

TL;DR: 本文提出一种基于合成数据的无卷积双域方法，用于校正CT探测器低性能像素（LPP）引起的伪影，无需真实临床数据训练，且在仿真实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低性能像素（LPP）导致CT图像出现环状和条纹伪影，影响临床可用性；现有监督深度学习方法依赖昂贵的真实标注数据，且仅在图像域或正弦图域单一修正，忽略CT前向几何建模的内在关联。

Method: 提出一种基于合成数据的展开式双域校正方法：利用自然图像生成含LPP缺陷的配对正弦图-图像数据，建模两域间内在相关性，实现端到端无真实数据训练。

Result: 在模拟1–2%中心区域探测器缺陷的实验中，该方法大幅超越当前最优方法；验证了其无需真实临床数据训练、适用于不同扫描仪配置的软件级部署能力。

Conclusion: 所提方法有效解决了LPP伪影校正中数据依赖性强与单域建模局限的问题，为低成本、通用化CT伪影校正提供了新范式。

Abstract: Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.

</details>


### [5] [AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples](https://arxiv.org/abs/2601.21022)
*Andrea Camilloni,Chiara Micoli,Nita Mulliqi,Erik Everett Palm,Thorgerdur Palsdottir,Kelvin Szolnoky,Xiaoyi Ji,Sol Erika Boman,Andrea Discacciati,Henrik Grönberg,Lars Egevad,Tobias Nordström,Kimmo Kartasalo,Martin Eklund*

Main category: cs.CV

TL;DR: 本文开发了一种基于诊断前列腺活检切片的AI模型，利用基础模型和注意力机制的多实例学习预测根治性前列腺切除术后生化复发（BCR）风险，并在多个外部队列中验证了其泛化能力；结合临床变量后进一步提升了风险分层效果，优于传统CAPRA-S评分。


<details>
  <summary>Details</summary>
Motivation: 当前预测根治性前列腺切除术后生化复发（BCR）的工具精度不足，亟需更可靠的预后方法。

Method: 基于STHLM3队列（n=676）的诊断活检全切片图像，采用基础模型与注意力机制的多实例学习（MIL）构建AI预测模型；在LEOPARD、CHIMERA和TCGA-PRAD三个外部根治术队列中评估泛化性；并与临床变量及CAPRA-S评分对比。

Result: 图像模型在三个外部队列中5年时间依赖AUC分别为0.64、0.70和0.70；联合临床变量后实现显著风险分层，且性能优于CAPRA-S。

Conclusion: 活检训练的组织病理AI模型可跨样本类型泛化，支持术前与术后决策；但AI多模态方法相较简单模型的额外价值需进一步严谨验证。

Abstract: Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.

</details>


### [6] [BadDet+: Robust Backdoor Attacks for Object Detection](https://arxiv.org/abs/2601.21066)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 本文提出了BadDet+框架，通过引入对数障碍惩罚机制，统一了区域误分类攻击（RMA）和目标消失攻击（ODA），提升了对物体检测模型的后门攻击在物理世界中的有效性与鲁棒性，同时不损害正常样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对目标检测的后门攻击方法依赖不现实假设、缺乏物理验证，对实际威胁理解不足。

Method: 提出BadDet+框架，采用对数障碍惩罚机制抑制触发输入的真类预测，实现位置与尺度不变性及更强的物理鲁棒性；统一RMA与ODA两类攻击。

Result: 在真实世界基准上，BadDet+相比现有RMA和ODA基线展现出更优的合成到物理迁移能力，且保持干净样本性能；理论分析证实该惩罚作用于触发器特定特征子空间，可靠诱导攻击而不影响标准推理。

Conclusion: 揭示了目标检测模型存在严重后门脆弱性，强调需发展专用防御机制。

Abstract: Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.

</details>


### [7] [Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization](https://arxiv.org/abs/2601.21078)
*Jiaqi Li,Guangming Wang,Shuntian Zheng,Minzhe Ni,Xiaoman Lu,Guanghui Ye,Yu Guan*

Main category: cs.CV

TL;DR: 本文提出ActionVLM框架，通过去偏重加权和残差聚合策略，在时序动作定位任务中缓解视觉-语言模态偏差，以视觉为主导、语言为辅助，提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的时序动作定位方法过度依赖语言先验，导致明显的模态偏差，削弱了视觉性能。

Method: 提出ActionVLM框架：（i）去偏重加权模块，估计语言相对于视觉的增量优势并动态调整语言权重；（ii）残差聚合策略，将语言作为视觉预测的互补精调而非主导信号。

Result: 在THUMOS14数据集上，mAP指标较当前最优方法提升最多达3.2%。

Conclusion: ActionVLM有效缓解了TAL中的模态偏差，提升了模型对视觉信号的依赖与时间推理能力，验证了‘视觉主导、语言辅助’设计的有效性。

Abstract: Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.

</details>


### [8] [HiFi-Mesh: High-Fidelity Efficient 3D Mesh Generation via Compact Autoregressive Dependence](https://arxiv.org/abs/2601.21314)
*Yanfeng Li,Tao Tan,Qingquan Gao,Zhiwen Cao,Xiaohong liu,Yue Sun*

Main category: cs.CV

TL;DR: 本文提出了LANE模型和AdaGraph策略，显著提升了3D网格生成的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归的3D网格建模方法资源利用率低，推理慢且难以处理长序列，限制了结构细节表达能力。

Method: 提出Latent Autoregressive Network（LANE）以实现紧凑的自回归依赖；设计Adaptive Computation Graph Reconfiguration（AdaGraph）策略，通过时空解耦突破串行推理效率瓶颈。

Result: LANE将最大可生成序列长度提升6倍；实验表明其在生成速度、结构细节和几何一致性上均优于现有方法。

Conclusion: LANE结合AdaGraph为高质量3D网格生成提供了高效且高保真的新范式。

Abstract: High-fidelity 3D meshes can be tokenized into one-dimension (1D) sequences and directly modeled using autoregressive approaches for faces and vertices. However, existing methods suffer from insufficient resource utilization, resulting in slow inference and the ability to handle only small-scale sequences, which severely constrains the expressible structural details. We introduce the Latent Autoregressive Network (LANE), which incorporates compact autoregressive dependencies in the generation process, achieving a $6\times$ improvement in maximum generatable sequence length compared to existing methods. To further accelerate inference, we propose the Adaptive Computation Graph Reconfiguration (AdaGraph) strategy, which effectively overcomes the efficiency bottleneck of traditional serial inference through spatiotemporal decoupling in the generation process. Experimental validation demonstrates that LANE achieves superior performance across generation speed, structural detail, and geometric consistency, providing an effective solution for high-quality 3D mesh generation.

</details>


### [9] [Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought](https://arxiv.org/abs/2601.21081)
*Yu Huo,Siyu Zhang,Kun Zeng,Haoyue Liu,Owen Lee,Junlin Chen,Yuquan Lu,Yifu Guo,Yaodong Liang,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文提出Shape-of-Thought（SoT）框架，通过视觉思维链实现渐进式2D形状组装，提升文本到图像生成在数值性、属性绑定和部件关系等结构约束下的鲁棒性；构建SoT-26K数据集与T2S-CompBench评测基准，并在组件数量和结构拓扑任务上显著超越纯文本基线。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像多模态模型在 compositional structural constraints（如生成数值性、属性绑定、部件级关系）方面表现脆弱，缺乏对形状组装逻辑的显式建模能力。

Method: 提出Shape-of-Thought（SoT）视觉思维链框架，训练统一的多模态自回归模型，生成交错的文本规划与渲染中间状态；构建SoT-26K（基于CAD部件层级的装配轨迹数据集）和T2S-CompBench（结构完整性与轨迹保真度评测基准）。

Result: 在组件数值性任务达88.4%，结构拓扑任务达84.8%，较纯文本基线提升约20%；实现了无需外部引擎、推理时端到端的渐进式形状组装。

Conclusion: SoT建立了可解释、过程监督的组合生成新范式，提升了多模态生成模型在结构化理解与可控合成方面的能力。

Abstract: Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.

</details>


### [10] [Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring](https://arxiv.org/abs/2601.21786)
*Borja Carrillo-Perez,Felix Sattler,Angel Bueno Rodriguez,Maurice Stephan,Sarah Barnes*

Main category: cs.CV

TL;DR: 本文提出了一种基于纯合成数据训练、仅需单张图像输入的高效单视图船舶3D重建方法，结合Splatter Image网络、YOLOv8分割与地理配准，实现无需真实3D标注的实时级海上监控应用。


<details>
  <summary>Details</summary>
Motivation: 现有主流3D重建方法依赖多视角监督、真实3D标注或计算开销大，难以满足海上实时监控需求。

Method: 采用Splatter Image网络（以稀疏3D高斯表示物体），在ShapeNet船舶模型上预训练，并用自建多样化3D船舶数据集微调以缩小域差距；集成YOLOv8分割模块和定制预处理；后处理包括真实尺度归一化、中心对齐、朝向校正及基于AIS与单应性的地理映射。

Result: 在合成验证集上定量指标表现优异，在ShipSG真实海事图像上定性结果证实其向实际场景迁移能力；系统支持交互式Web 3D可视化，无需任何真实3D标注。

Conclusion: 该端到端管道为海上监控提供了高效、可扩展且实用的单视图3D重建方案，推动了实时船舶三维可视化在现实应用中的落地。

Abstract: Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.

</details>


### [11] [An AI Framework for Microanastomosis Motion Assessment](https://arxiv.org/abs/2601.21120)
*Yan Meng,Eduardo J. Torres-Rodríguez,Marcelle Altshuler,Nishanth Gowda,Arhum Naeem,Recai Yilmaz,Omar Arnaout,Daniel A. Donoho*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化微血管吻合器械操作技能评估框架，整合了YOLO检测、DeepSORT跟踪、器械尖端定位与监督分类模块，实现了高精度（97%检测精度，mAP 96%）的客观、可靠评估。


<details>
  <summary>Details</summary>
Motivation: 传统微外科技术评估依赖专家主观评分，存在评分者间差异大、标准不统一、易受认知偏差影响及耗时等问题，亟需客观、可靠、可扩展的自动化评估系统。

Method: 构建了一个四模块AI框架：(1) 基于YOLO的器械检测模块；(2) 基于DeepSORT的器械跟踪模块；(3) 基于形状描述符的器械尖端定位模块；(4) 基于专家标注数据训练的监督分类模块，用于评估器械操作熟练度。

Result: 实验表明该框架器械检测精度达97%，在IoU阈值50%–95%下平均精度均值（mAP50-95）达96%。

Conclusion: 所提AI框架能有效实现微血管吻合中器械操作技能的客观、自动化评估，为微外科培训与考核提供了可靠、可扩展的新工具。

Abstract: Proficiency in microanastomosis is a fundamental competency across multiple microsurgical disciplines. These procedures demand exceptional precision and refined technical skills, making effective, standardized assessment methods essential. Traditionally, the evaluation of microsurgical techniques has relied heavily on the subjective judgment of expert raters. They are inherently constrained by limitations such as inter-rater variability, lack of standardized evaluation criteria, susceptibility to cognitive bias, and the time-intensive nature of manual review. These shortcomings underscore the urgent need for an objective, reliable, and automated system capable of assessing microsurgical performance with consistency and scalability. To bridge this gap, we propose a novel AI framework for the automated assessment of microanastomosis instrument handling skills. The system integrates four core components: (1) an instrument detection module based on the You Only Look Once (YOLO) architecture; (2) an instrument tracking module developed from Deep Simple Online and Realtime Tracking (DeepSORT); (3) an instrument tip localization module employing shape descriptors; and (4) a supervised classification module trained on expert-labeled data to evaluate instrument handling proficiency. Experimental results demonstrate the effectiveness of the framework, achieving an instrument detection precision of 97%, with a mean Average Precision (mAP) of 96%, measured by Intersection over Union (IoU) thresholds ranging from 50% to 95% (mAP50-95).

</details>


### [12] [EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers](https://arxiv.org/abs/2601.22127)
*John Flynn,Wolfgang Paier,Dimitar Dinev,Sam Nhut Nguyen,Hayk Poghosyan,Manuel Toribio,Sandipan Banerjee,Guy Gafni*

Main category: cs.CV

TL;DR: 本文提出了EditYourself，一种基于DiT的音频驱动视频到视频编辑框架，支持对已有说话人视频进行基于文本脚本的精细编辑（如增删/重排口型内容），同时保持运动连贯性、说话人身份一致性和精准唇形同步。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型擅长从文本或图像生成新视频，但在编辑已录制视频方面存在明显不足——尤其当仅需修改语音脚本时，难以兼顾动作连续性、时间一致性、说话人身份和唇形同步。

Method: 基于通用视频扩散模型（DiT），引入音频条件控制与区域感知、编辑导向的训练策略，结合时空掩码修复（spatiotemporal inpainting）实现精准唇动合成与动态内容重构。

Result: EditYourself在保持长期视觉保真度与身份一致性前提下，实现了高质量的唇形同步、自然的人体运动生成及流畅的时间结构重编排，显著提升了视频后制实用性。

Conclusion: 该工作是将生成式视频模型落地于专业视频后期制作的重要奠基性进展，为 transcript-driven 视频编辑提供了新范式。

Abstract: Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.

</details>


### [13] [Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2601.21159)
*Jianzheng Wang,Huan Ni*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的开放词汇语义分割框架SDCI，通过跨模型注意力融合、双向图扩散精炼和超像素协同预测三个模块，提升高分辨率遥感影像中复杂地物边界的几何定位与语义预测精度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感影像中地物密集、边界复杂，现有无训练开放词汇语义分割方法（如CLIP+VFMs单向注入、浅层后处理）难以满足其对几何定位与语义预测的双重高要求。

Method: 提出空间正则化感知的双分支协同推理框架SDCI：1）跨模型注意力融合（CAF）模块实现特征编码阶段的互导式自注意力引导；2）双向跨图扩散精炼（BCDR）模块通过迭代随机游走提升双分支分割置信度；3）结合低层超像素结构，构建基于凸优化的超像素协同预测（CSCP）机制以精细边界。

Result: 在多个遥感语义分割基准上超越现有方法；消融实验验证了超像素结构在深度学习框架中仍具有效性。

Conclusion: SDCI通过多级空间正则化与双分支协同设计，有效提升了无训练开放词汇语义分割在高分辨率遥感影像上的性能，证实了传统面向对象分析思想与现代视觉基础模型融合的可行性与优势。

Abstract: High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.

</details>


### [14] [Enhancing Underwater Light Field Images via Global Geometry-aware Diffusion Process](https://arxiv.org/abs/2601.21179)
*Yuji Lin,Qian Zhao,Zongsheng Yue,Junhui Hou,Deyu Meng*

Main category: cs.CV

TL;DR: 本文提出GeoDiff-LF，一种基于SD-Turbo的扩散模型框架，通过几何引导的U-Net结构、张量分解损失和优化采样策略，显著提升水下4D光场图像质量，缓解颜色失真问题。


<details>
  <summary>Details</summary>
Motivation: 水下成像面临严重颜色失真与质量退化问题，而4D光场成像虽具潜力，但缺乏有效方法充分利用其空间-角度结构信息进行高质量重建。

Method: 提出GeoDiff-LF：（1）带卷积与注意力适配器的改进U-Net以建模几何线索；（2）基于张量分解与渐进加权的几何引导损失函数；（3）结合噪声预测的高效采样策略；整体融合扩散先验与光场几何结构。

Result: 在视觉保真度与定量指标（如PSNR、SSIM）上均超越现有方法，显著提升水下4D光场图像重建质量。

Conclusion: GeoDiff-LF成功将扩散模型与光场几何建模相结合，为水下成像提供了新范式，推动该领域达到新SOTA水平。

Abstract: This work studies the challenging problem of acquiring high-quality underwater images via 4-D light field (LF) imaging. To this end, we propose GeoDiff-LF, a novel diffusion-based framework built upon SD-Turbo to enhance underwater 4-D LF imaging by leveraging its spatial-angular structure. GeoDiff-LF consists of three key adaptations: (1) a modified U-Net architecture with convolutional and attention adapters to model geometric cues, (2) a geometry-guided loss function using tensor decomposition and progressive weighting to regularize global structure, and (3) an optimized sampling strategy with noise prediction to improve efficiency. By integrating diffusion priors and LF geometry, GeoDiff-LF effectively mitigates color distortion in underwater scenes. Extensive experiments demonstrate that our framework outperforms existing methods across both visual fidelity and quantitative performance, advancing the state-of-the-art in enhancing underwater imaging. The code will be publicly available at https://github.com/linlos1234/GeoDiff-LF.

</details>


### [15] [FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models](https://arxiv.org/abs/2601.21187)
*Chenyu Huang,Peng Ye,Xudong Tan,Jinhan Mu,Shenghe Zheng,Li Shen,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出FRISM框架，通过子空间级模型融合实现细粒度推理能力注入，结合无标签自蒸馏学习策略，在不损害视觉能力的前提下显著提升视觉语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将大推理模型（LRM）与视觉语言模型（VLM）融合时通常采用粗粒度层级别操作，导致推理能力注入与视觉能力保持之间存在权衡。

Method: 提出FRISM（细粒度推理注入 via 子空间级模型融合），利用SVD分解LRM任务向量，识别并自适应缩放各推理相关子空间；引入无标签自蒸馏双目标优化策略，仅使用通用视觉-语言感知数据集进行训练。

Result: 在多个视觉推理基准上持续达到SOTA性能，同时保持原始VLM的视觉能力不下降。

Conclusion: 子空间级的细粒度推理注入是提升VLM推理能力且不损害其视觉能力的有效途径，FRISM为VLM与LRM协同增强提供了新范式。

Abstract: Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model's original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.

</details>


### [16] [Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval](https://arxiv.org/abs/2601.21193)
*Zecheng Zhao,Zhi Chen,Zi Huang,Shazia Sadiq,Tong Chen*

Main category: cs.CV

TL;DR: 本文提出GRDR方法，通过多视图语义ID生成和联合训练提升两阶段文本-视频检索中的召回质量，在保持高精度的同时大幅降低存储和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本-视频检索中，召回模型性能瓶颈明显；生成式检索（GR）虽具高效性，但存在语义歧义和跨模态错位问题。

Method: 提出Generative Recall and Dense Reranking（GRDR）：设计查询引导的多视角分词器为每个视频分配多个语义ID，并通过共享码本联合训练分词器与生成式检索器，使语义ID成为图文桥梁；推理时采用Trie约束解码生成紧凑候选集，再由密集模型重排序。

Result: 在TVR基准上，GRDR达到与强密集检索器相当的准确率，索引存储减少一个数量级，全库检索加速最高达300倍。

Conclusion: GRDR有效缓解了生成式召回中的语义歧义与跨模态错位问题，实现了高效、精准的大规模文本-视频检索。

Abstract: Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.

</details>


### [17] [Thinker: A vision-language foundation model for embodied intelligence](https://arxiv.org/abs/2601.21199)
*Baiyu Pan,Daqin Luo,Junpeng Yang,Jiyuan Wang,Yixuan Zhang,Hailin Shi,Jichao Jiao*

Main category: cs.CV

TL;DR: 本文提出Thinker模型，专为具身智能设计，通过构建面向机器人感知与推理的大规模数据集，并联合使用关键帧与完整视频序列输入，显著提升视频理解能力，在任务规划基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在机器人领域应用时存在视角混淆（第三人称vs第一人称）和忽略视频结尾信息等人类易解但模型易错的问题。

Method: 1）构建面向机器人感知与推理的大规模数据集，涵盖自我视角视频、视觉定位、空间理解与思维链数据；2）提出简单有效的方法，联合输入关键帧与完整视频序列以增强视频理解能力。

Result: 在两个最常用的任务规划基准数据集上达到当前最优性能（state-of-the-art）。

Conclusion: Thinker是一种专为具身智能设计的大型视觉语言基础模型，能有效缓解视角混淆与时间推理偏差问题，提升机器人任务规划能力。

Abstract: When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.

</details>


### [18] [LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models](https://arxiv.org/abs/2601.21220)
*Alvi Md Ishmam,Najibul Haque Sarker,Zaber Ibn Abdul Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: 本文提出LAMP，一种针对多图像多模态大语言模型（MLLMs）的黑盒通用对抗扰动（UAP）攻击方法，通过注意力约束、跨图像传染约束和索引-注意力抑制损失，实现高效、位置无关且无需修改全部输入的鲁棒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击主要针对单图像场景且依赖白盒假设，不适用于实际多图像MLLMs；多图像MLLMs的脆弱性尚未被探索。

Method: 提出LAMP方法：1）注意力约束限制跨图像信息聚合；2）跨图像传染约束使扰动token影响干净token；3）索引-注意力抑制损失实现位置不变攻击。

Result: LAMP在多个视觉-语言任务和模型上显著超越SOTA基线，取得最高攻击成功率。

Conclusion: LAMP是一种有效的黑盒通用对抗攻击方法，揭示了多图像MLLMs的新脆弱性，并为提升其鲁棒性提供了重要启示。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance across vision-language tasks. Recent advancements allow these models to process multiple images as inputs. However, the vulnerabilities of multi-image MLLMs remain unexplored. Existing adversarial attacks focus on single-image settings and often assume a white-box threat model, which is impractical in many real-world scenarios. This paper introduces LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs. LAMP applies an attention-based constraint that prevents the model from effectively aggregating information across images. LAMP also introduces a novel cross-image contagious constraint that forces perturbed tokens to influence clean tokens, spreading adversarial effects without requiring all inputs to be modified. Additionally, an index-attention suppression loss enables a robust position-invariant attack. Experimental results show that LAMP outperforms SOTA baselines and achieves the highest attack success rates across multiple vision-language tasks and models.

</details>


### [19] [PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models](https://arxiv.org/abs/2601.21238)
*Xuewen Liu,Zhikai Li,Jing Zhang,Mengjuan Chen,Qingyi Gu*

Main category: cs.CV

TL;DR: 本文提出PTQ4ARVG，一种无需训练的后训练量化框架，用于解决自回归视觉生成（ARVG）模型量化中的通道级异常值、token级动态激活和样本级分布不匹配三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法难以有效泛化到ARVG模型，且ARVG量化研究尚不充分。

Method: 提出PTQ4ARVG框架，包含三部分：(1) Gain-Projected Scaling (GPS) 缓解通道级异常值；(2) Static Token-Wise Quantization (STWQ) 利用ARVG固定token长度与位置无关分布特性处理token级方差；(3) Distribution-Guided Calibration (DGC) 选取对分布熵贡献最大的样本来校准样本级分布失配。

Result: PTQ4ARVG在8位和6位量化下仍能保持ARVG家族模型的竞争性性能。

Conclusion: PTQ4ARVG是一种高效、通用且无需训练的ARVG量化方案，显著提升了ARVG模型在资源受限场景下的部署可行性。

Abstract: AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at http://github.com/BienLuky/PTQ4ARVG .

</details>


### [20] [NFCDS: A Plug-and-Play Noise Frequency-Controlled Diffusion Sampling Strategy for Image Restoration](https://arxiv.org/abs/2601.21248)
*Zhen Wang,Hongyi Liu,Jianing Li,Zhihui Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为噪声频率控制扩散采样（NFCDS）的新方法，通过在傅里叶域设计滤波器来调控反向扩散过程中的噪声频率成分，抑制低频噪声以提升保真度、保留高频细节以维持感知质量，从而在无需额外训练的情况下改善即插即用（PnP）扩散模型的保真度-感知质量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散采样的即插即用（PnP）方法虽能生成高感知质量图像，但因反向扩散引入噪声而导致数据保真度下降，亟需缓解保真度与感知质量之间的固有矛盾。

Method: 提出噪声频率控制扩散采样（NFCDS），在傅里叶域设计一个渐进式滤波器，抑制低频噪声（导致模糊）、保留高频噪声（驱动细节生成），将数据一致性先验直接注入采样过程。

Result: NFCDS作为PnP模块可无缝集成于现有扩散修复框架，在多种零样本任务中显著提升保真度-感知质量平衡，且无需额外训练、收敛更快。

Conclusion: 保真度与感知质量的权衡可从噪声频率视角根本理解；NFCDS通过频域调控实现二者协同优化，为无训练、即插即用的高质量图像恢复提供了新范式。

Abstract: Diffusion sampling-based Plug-and-Play (PnP) methods produce images with high perceptual quality but often suffer from reduced data fidelity, primarily due to the noise introduced during reverse diffusion. To address this trade-off, we propose Noise Frequency-Controlled Diffusion Sampling (NFCDS), a spectral modulation mechanism for reverse diffusion noise. We show that the fidelity-perception conflict can be fundamentally understood through noise frequency: low-frequency components induce blur and degrade fidelity, while high-frequency components drive detail generation. Based on this insight, we design a Fourier-domain filter that progressively suppresses low-frequency noise and preserves high-frequency content. This controlled refinement injects a data-consistency prior directly into sampling, enabling fast convergence to results that are both high-fidelity and perceptually convincing--without additional training. As a PnP module, NFCDS seamlessly integrates into existing diffusion-based restoration frameworks and improves the fidelity-perception balance across diverse zero-shot tasks.

</details>


### [21] [Hypersolid: Emergent Vision Representations via Short-Range Repulsion](https://arxiv.org/abs/2601.21255)
*Esteban Rodríguez-Betancourt,Edgar Casasola-Murillo*

Main category: cs.CV

TL;DR: 本文提出Hypersolid方法，将自监督学习中的表征学习重新解释为离散填充问题，通过短程硬球排斥防止局部碰撞，从而避免表征坍缩，并在细粒度和低分辨率分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习中常见的表征坍缩问题，现有方法多依赖全局正则化，本文尝试从几何角度出发提供新思路。

Method: 将表征学习建模为离散填充问题，引入Hypersolid方法，利用短程硬球排斥机制防止局部碰撞，实现高分离几何结构。

Result: 该方法在细粒度分类和低分辨率图像分类任务上取得优异性能，有效保持数据增强多样性。

Conclusion: 基于离散填充与局部排斥的几何视角能更有效地防止表征坍缩，为自监督学习提供了新范式。

Abstract: A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.

</details>


### [22] [Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference](https://arxiv.org/abs/2601.21269)
*Jianglong Li,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、高保真、低码率的3D说话人脸压缩框架，结合FLAME参数建模与3DGS神经渲染，仅传输关键面部元数据，并通过高斯属性压缩和MLP优化提升传输效率，在极低码率下实现高质量实时3D人脸重建。


<details>
  <summary>Details</summary>
Motivation: 传统2D视频压缩无法保留精细几何与外观细节，而NeRF等隐式神经渲染计算成本过高，难以满足实时3D视频会议对低码率与高保真度的双重需求。

Method: 融合FLAME参数化建模与3D高斯泼溅（3DGS）神经渲染，构建轻量高斯头部模型；设计紧凑表示与压缩方案，包括高斯属性压缩与MLP优化，仅实时传输必要面部元数据。

Result: 在极低比特率下实现了优于现有方法的率失真性能，支持高质量、实时的3D人脸渲染。

Conclusion: 所提框架在保持高保真度的同时显著降低传输带宽需求，适用于资源受限的实时3D视频会议场景。

Abstract: The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge. Traditional 2D video compression techniques fail to preserve fine-grained geometric and appearance details, while implicit neural rendering methods like NeRF suffer from prohibitive computational costs. To address these challenges, we propose a lightweight, high-fidelity, low-bitrate 3D talking face compression framework that integrates FLAME-based parametric modeling with 3DGS neural rendering. Our approach transmits only essential facial metadata in real time, enabling efficient reconstruction with a Gaussian-based head model. Additionally, we introduce a compact representation and compression scheme, including Gaussian attribute compression and MLP optimization, to enhance transmission efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance, delivering high-quality facial rendering at extremely low bitrates, making it well-suited for real-time 3D video conferencing applications.

</details>


### [23] [GeoRC: A Benchmark for Geolocation Reasoning Chains](https://arxiv.org/abs/2601.21278)
*Mohit Talreja,Joshua Diao,Jim Thannikary James,Radu Casapu,Tejas Santanam,Ethan Mendes,Alan Ritter,Wei Xu,James Hays*

Main category: cs.CV

TL;DR: 本文提出了首个地理定位推理链基准，揭示了视觉语言模型（VLMs）虽能高精度预测图像地理位置，却普遍在生成可信、基于图像证据的推理链方面表现极差，常出现幻觉；专家人工链涵盖数百种判别性视觉特征，评估发现闭源大VLM（如Gemini、GPT-5）推理能力远逊人类，开源VLM（如Llama、Qwen）则近乎失效。


<details>
  <summary>Details</summary>
Motivation: VLMs在地理定位预测上已媲美人类专家，但其推理链常脱离图像事实、产生幻觉（如虚构文字、误认植被），缺乏可审计性与可信度，亟需专门基准评估其视觉推理能力。

Method: 构建首个地理定位推理链基准：基于GeoGuessr游戏场景，联合顶尖专家（含世界冠军）为500张街景图生成800条高质量人工推理链，涵盖license plate、architecture、soil等数百视觉属性；采用LLM-as-a-judge（Qwen 3最优）和VLM-as-a-judge策略自动评分，并对比人类评分。

Result: 闭源大VLM（Gemini、GPT-5）地理预测准确率接近人类，但推理链质量显著落后；开源VLM（Llama、Qwen）表现灾难性，仅略优于纯幻觉基线（LLM凭真实位置编造无视觉依据的推理）；Qwen 3 LLM-as-a-judge与人类评分相关性最高。

Conclusion: 当前VLM在提取高分辨率图像中细粒度视觉属性方面存在根本性局限，导致其推理链不可靠；本基准为提升VLM可解释性与视觉推理能力提供了关键评测工具与改进方向。

Abstract: Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.

</details>


### [24] [Token Entropy Regularization for Multi-modal Antenna Affiliation Identification](https://arxiv.org/abs/2601.21280)
*Dong Chen,Ruoyu Li,Xinyan Zhang,Jialei Xu,Ruoseng Zhao,Zhikang Zhang,Lingyun Li,Zizhuang Wei*

Main category: cs.CV

TL;DR: 本文提出了一种融合视频、天线几何特征和PCI信号的多模态方法，用于自动识别天线归属关系，并设计了Token Entropy Regularization（TER）模块以提升跨模态对齐效果，显著加快收敛并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有天线归属识别依赖人工巡检，效率低且易出错；公开预训练模型因缺乏通信领域类似数据，难以实现有效跨模态对齐。

Method: 提出多模态分类与匹配框架，融合基站视频、天线几何特征与PCI信号；设计Token Entropy Regularization（TER）模块，在预训练阶段优化跨模态表征对齐。

Result: TER模块显著加速模型收敛，并在天线归属识别任务上取得明显性能提升；分析发现首token熵值具有模态依赖性。

Conclusion: 所提多模态融合框架及TER模块有效解决了通信场景下天线归属识别的跨模态对齐难题，为自动化网络运维提供了新范式。

Abstract: Accurate antenna affiliation identification is crucial for optimizing and maintaining communication networks. Current practice, however, relies on the cumbersome and error-prone process of manual tower inspections. We propose a novel paradigm shift that fuses video footage of base stations, antenna geometric features, and Physical Cell Identity (PCI) signals, transforming antenna affiliation identification into multi-modal classification and matching tasks. Publicly available pretrained transformers struggle with this unique task due to a lack of analogous data in the communications domain, which hampers cross-modal alignment. To address this, we introduce a dedicated training framework that aligns antenna images with corresponding PCI signals. To tackle the representation alignment challenge, we propose a novel Token Entropy Regularization module in the pretraining stage. Our experiments demonstrate that TER accelerates convergence and yields significant performance gains. Further analysis reveals that the entropy of the first token is modality-dependent. Code will be made available upon publication.

</details>


### [25] [WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models](https://arxiv.org/abs/2601.21282)
*Rishi Upadhyay,Howard Zhang,Jim Solomon,Ayush Agrawal,Pranay Boreddy,Shruti Satya Narayana,Yunhao Ba,Alex Wong,Celso M de Melo,Achuta Kadambi*

Main category: cs.CV

TL;DR: 本文提出了WorldBench，一个用于评估生成式世界模型物理保真度的视频基准，其特点是概念解耦、分层设计（直观物理与低层物理参数），并揭示了当前SOTA模型在特定物理概念上的一致性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有物理视频基准存在概念纠缠问题，难以精准诊断模型在具体物理规律上的理解缺陷，阻碍世界模型在机器人等关键任务中的可靠部署。

Method: 提出WorldBench基准：1）按物理概念解耦设计测试；2）分两层——直观物理（如客体永久性）和低层物理参数（如摩擦系数、粘度）；3）在SOTA视频世界模型上进行系统评测。

Result: 所有被测SOTA模型均在特定物理概念上表现出系统性失败，缺乏生成可靠现实交互所需的物理一致性。

Conclusion: WorldBench提供了更精细、可扩展的物理推理能力评估框架，有助于推动更鲁棒、泛化性更强的世界模型发展。

Abstract: Recent advances in generative foundational models, often termed "world models," have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.

</details>


### [26] [Gaussian Belief Propagation Network for Depth Completion](https://arxiv.org/abs/2601.21291)
*Jie Tang,Pingping Xie,Jian Li,Ping Tan*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习与概率图模型的高斯置信传播网络（GBPN），用于端到端深度补全，通过动态构建场景相关的马尔可夫随机场并改进置信传播机制，在NYUv2和KITTI数据集上达到SOTA性能，尤其在高稀疏度下表现出强鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以有效处理输入深度数据的稀疏性和不规则性，尤其在高稀疏度下性能受限。

Method: 提出高斯置信传播网络（GBPN）：由图模型构建网络（GMCN）动态构建场景相关马尔可夫随机场（MRF），预测自适应非局部边以建模长程空间依赖；采用串行与并行结合的消息传递策略增强高斯置信传播（GBP）效果。

Result: 在NYUv2和KITTI基准上达到SOTA；在不同稀疏度、稀疏模式及跨数据集测试中均表现优异，展现出强鲁棒性与泛化能力。

Conclusion: GBPN成功融合深度学习与概率图模型，为稀疏深度补全提供了更鲁棒、更通用的端到端解决方案。

Abstract: Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \& parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN's superior performance, notable robustness, and generalizable capability.

</details>


### [27] [Mam-App: A Novel Parameter-Efficient Mamba Model for Apple Leaf Disease Classification](https://arxiv.org/abs/2601.21307)
*Md Nadim Mahamood,Md Imran Hasan,Md Rasheduzzaman,Ausrukona Ray,Md Shafi Ud Doula,Kamrul Hasan*

Main category: cs.CV

TL;DR: 本文提出了一种参数高效的Mamba-based模型Mam-App，用于苹果叶病识别，在保持极低参数量（0.051M）的同时，在多个植物病害数据集上实现了接近SOTA的性能，适用于无人机和移动设备等资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型参数量大、计算开销高，而轻量模型又常牺牲性能；苹果等作物病害导致严重产量损失，亟需高效准确的早期诊断方法。

Method: 提出基于Mamba架构的轻量级模型Mam-App，专为植物叶片病害分类设计，强调参数效率与特征提取能力的平衡。

Result: 在PlantVillage苹果叶病数据集上达99.58%准确率、99.22% F1；在玉米和马铃薯数据集上也取得优异泛化性能（如玉米F1=99.27%，马铃薯F1=97.01%），仅用0.051M参数。

Conclusion: Mam-App成功兼顾高精度与极低参数量，验证了Mamba架构在农业图像诊断中的有效性与部署可行性，为边缘端智能植保提供了新方案。

Abstract: The rapid growth of the global population, alongside exponential technological advancement, has intensified the demand for food production. Meeting this demand depends not only on increasing agricultural yield but also on minimizing food loss caused by crop diseases. Diseases account for a substantial portion of apple production losses, despite apples being among the most widely produced and nutritionally valuable fruits worldwide. Previous studies have employed machine learning techniques for feature extraction and early diagnosis of apple leaf diseases, and more recently, deep learning-based models have shown remarkable performance in disease recognition. However, most state-of-the-art deep learning models are highly parameter-intensive, resulting in increased training and inference time. Although lightweight models are more suitable for user-friendly and resource-constrained applications, they often suffer from performance degradation. To address the trade-off between efficiency and performance, we propose Mam-App, a parameter-efficient Mamba-based model for feature extraction and leaf disease classification. The proposed approach achieves competitive state-of-the-art performance on the PlantVillage Apple Leaf Disease dataset, attaining 99.58% accuracy, 99.30% precision, 99.14% recall, and a 99.22% F1-score, while using only 0.051M parameters. This extremely low parameter count makes the model suitable for deployment on drones, mobile devices, and other low-resource platforms. To demonstrate the robustness and generalizability of the proposed model, we further evaluate it on the PlantVillage Corn Leaf Disease and Potato Leaf Disease datasets. The model achieves 99.48%, 99.20%, 99.34%, and 99.27% accuracy, precision, recall, and F1-score on the corn dataset and 98.46%, 98.91%, 95.39%, and 97.01% on the potato dataset, respectively.

</details>


### [28] [Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence](https://arxiv.org/abs/2601.21320)
*Keke Tang,Ziyong Du,Xiaofei Wang,Weilong Peng,Peican Zhu,Zhihong Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于半离散最优传输（OT）奇异边界的OOD过置信缓解框架，通过生成几何上合理、语义模糊的OTIS样本并施加置信度抑制损失，提升模型在开放世界中的预测校准性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在分布外（OOD）输入上常产生过度自信的预测，而半离散最优传输中的奇异性恰好对应语义模糊区域，是模型高置信误判的根源。

Method: 构建连续基分布与训练数据隐空间嵌入之间的OT问题，识别其诱导的奇异边界；在边界附近采样生成OTIS（OT-induced OOD samples）；训练中对OTIS施加置信度抑制损失，引导模型在结构不确定性区域输出更校准的预测。

Result: 实验表明该方法显著缓解OOD过置信问题，在多个基准上超越当前最优方法。

Conclusion: 利用最优传输几何结构（特别是奇异边界）可有效建模和抑制OOD过置信，为开放世界可靠性提供了新范式。

Abstract: Deep neural networks (DNNs) often produce overconfident predictions on out-of-distribution (OOD) inputs, undermining their reliability in open-world environments. Singularities in semi-discrete optimal transport (OT) mark regions of semantic ambiguity, where classifiers are particularly prone to unwarranted high-confidence predictions. Motivated by this observation, we propose a principled framework to mitigate OOD overconfidence by leveraging the geometry of OT-induced singular boundaries. Specifically, we formulate an OT problem between a continuous base distribution and the latent embeddings of training data, and identify the resulting singular boundaries. By sampling near these boundaries, we construct a class of OOD inputs, termed optimal transport-induced OOD samples (OTIS), which are geometrically grounded and inherently semantically ambiguous. During training, a confidence suppression loss is applied to OTIS to guide the model toward more calibrated predictions in structurally uncertain regions. Extensive experiments show that our method significantly alleviates OOD overconfidence and outperforms state-of-the-art methods.

</details>


### [29] [Do Pathology Foundation Models Encode Disease Progression? A Pseudotime Analysis of Visual Representations](https://arxiv.org/abs/2601.21334)
*Pritika Vig,Ren-Chin Wu,William Lotter*

Main category: cs.CV

TL;DR: 本文探讨视觉基础模型是否能从静态图像中隐式学习连续疾病进展过程，并利用扩散伪时间方法验证其表征空间中疾病状态的有序性；结果表明病理特异性模型能显著恢复疾病进展轨迹，且轨迹保真度与少样本分类性能高度相关。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在分类任务上表现优异，但其表征是否编码了训练数据背后的连续生物过程（如疾病进展）尚不清楚；尤其在计算病理学中，能反映连续进展的表征可能更符合生物学本质、泛化更强、支持定量分析。

Method: 采用源自单细胞转录组学的扩散伪时间（diffusion pseudotime）方法，在表征空间中评估多个视觉基础模型对四种癌症进展轨迹的重建能力，并关联其与少样本分类性能及组织细胞组成变化。

Result: 所有病理特异性模型均显著优于零模型地恢复疾病轨迹顺序，纯视觉模型在CRC-Serrated数据集上达到最高保真度（τ > 0.78）；轨迹保真度排名与跨疾病少样本分类性能强相关（ρ = 0.92）；推断轨迹上细胞类型组成呈现与已知基质重塑一致的平滑变化。

Conclusion: 视觉基础模型可仅从静态图像中隐式学习连续疾病进展表征；轨迹保真度是衡量表征质量的一个新且互补的指标；该框架具有向其他依赖静态快照观测连续过程的领域迁移的潜力。

Abstract: Vision foundation models trained on discretely sampled images achieve strong performance on classification benchmarks, yet whether their representations encode the continuous processes underlying their training data remains unclear. This question is especially pertinent in computational pathology, where we posit that models whose latent representations implicitly capture continuous disease progression may better reflect underlying biology, support more robust generalization, and enable quantitative analyses of features associated with disease transitions. Using diffusion pseudotime, a method developed to infer developmental trajectories from single-cell transcriptomics, we probe whether foundation models organize disease states along coherent progression directions in representation space. Across four cancer progressions and six models, we find that all pathology-specific models recover trajectory orderings significantly exceeding null baselines, with vision-only models achieving the highest fidelities $(τ> 0.78$ on CRC-Serrated). Model rankings by trajectory fidelity on reference diseases strongly predict few-shot classification performance on held-out diseases ($ρ= 0.92$), and exploratory analysis shows cell-type composition varies smoothly along inferred trajectories in patterns consistent with known stromal remodeling. Together, these results demonstrate that vision foundation models can implicitly learn to represent continuous processes from independent static observations, and that trajectory fidelity provides a complementary measure of representation quality beyond downstream performance. While demonstrated in pathology, this framework could be applied to other domains where continuous processes are observed through static snapshots.

</details>


### [30] [SR$^{2}$-Net: A General Plug-and-Play Model for Spectral Refinement in Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2601.21338)
*Ji-Xuan He,Guohang Zhuang,Junge Bo,Tingyi Li,Chen Ling,Yanan Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级即插即用的光谱校正超分辨率网络（SR²-Net），用于高光谱图像超分辨率（HSI-SR），在不修改原有模型结构的前提下提升光谱一致性与物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有HSI-SR方法注重空间相关性建模，但常忽视跨波段光谱一致性，导致伪影和物理不可行结果；而通过网络架构硬编码光谱约束又牺牲通用性与灵活性。

Method: 提出SR²-Net，包含两部分：(i) 分层光谱-空间协同注意力（H-S³A）增强跨波段交互；(ii) 流形一致性校正（MCR）将重建光谱约束至紧凑、物理合理的光谱流形；并引入退化一致性损失保障数据保真度。

Result: 在多个基准和不同骨干网络上验证，SR²-Net显著提升光谱保真度与整体重建质量，且计算开销极小。

Conclusion: SR²-Net是一种通用、灵活、高效的即插即用模块，有效缓解HSI-SR中光谱失真问题，兼顾物理合理性与模型兼容性。

Abstract: HSI-SR aims to enhance spatial resolution while preserving spectrally faithful and physically plausible characteristics. Recent methods have achieved great progress by leveraging spatial correlations to enhance spatial resolution. However, these methods often neglect spectral consistency across bands, leading to spurious oscillations and physically implausible artifacts. While spectral consistency can be addressed by designing the network architecture, it results in a loss of generality and flexibility. To address this issue, we propose a lightweight plug-and-play rectifier, physically priors Spectral Rectification Super-Resolution Network (SR$^{2}$-Net), which can be attached to a wide range of HSI-SR models without modifying their architectures. SR$^{2}$-Net follows an enhance-then-rectify pipeline consisting of (i) Hierarchical Spectral-Spatial Synergy Attention (H-S$^{3}$A) to reinforce cross-band interactions and (ii) Manifold Consistency Rectification (MCR) to constrain the reconstructed spectra to a compact, physically plausible spectral manifold. In addition, we introduce a degradation-consistency loss to enforce data fidelity by encouraging the degraded SR output to match the observed low resolution input. Extensive experiments on multiple benchmarks and diverse backbones demonstrate consistent improvements in spectral fidelity and overall reconstruction quality with negligible computational overhead. Our code will be released upon publication.

</details>


### [31] [Dynamical Adapter Fusion: Constructing A Global Adapter for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2601.21341)
*Ruiqi Liu,Boyu Diao,Zijia An,Zhulin An,Fei Wang,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出动态适配器融合（DAF）方法，通过PAC-Bayes理论指导下的参数融合与鲁棒初始化策略，在类增量学习中实现单个全局适配器的高效构建，兼顾稳定性与可塑性，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CIL方法依赖任务特定适配器，导致知识迁移受限、检索开销高；而简单参数融合易引发干扰和灾难性遗忘。

Method: 基于PAC-Bayes定理设计动态融合机制，融合任务特定适配器、前序全局适配器及初始化参数；利用损失函数的泰勒展开确定最优融合系数；引入鲁棒初始化策略捕获全局知识模式。

Result: 在多个CIL基准上取得SOTA性能。

Conclusion: DAF通过理论驱动的动态融合与鲁棒初始化，有效缓解灾难性遗忘，提升跨任务知识复用能力，验证了单一全局适配器在CIL中的可行性与优越性。

Abstract: Class-Incremental Learning (CIL) requires models to continuously acquire new classes without forgetting previously learned ones. A dominant paradigm involves freezing a pre-trained model and training lightweight, task-specific adapters. However, maintaining task-specific parameters hinders knowledge transfer and incurs high retrieval costs, while naive parameter fusion often leads to destructive interference and catastrophic forgetting. To address these challenges, we propose Dynamical Adapter Fusion (DAF) to construct a single robust global adapter. Grounded in the PAC-Bayes theorem, we derive a fusion mechanism that explicitly integrates three components: the optimized task-specific adapter parameters, the previous global adapter parameters, and the initialization parameters. We utilize the Taylor expansion of the loss function to derive the optimal fusion coefficients, dynamically achieving the best balance between stability and plasticity. Furthermore, we propose a Robust Initialization strategy to effectively capture global knowledge patterns. Experiments on multiple CIL benchmarks demonstrate that DAF achieves state-of-the-art (SOTA) performance.

</details>


### [32] [Semantic-Guided Dynamic Sparsification for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2601.21345)
*Ruiqi Liu,Boyu Diao,Zijia An,Runjie Shao,Zhulin An,Fei Wang,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为语义引导动态稀疏化（SGDS）的新方法，用于类增量学习（CIL），通过在激活空间中构建类别特定的稀疏子空间来缓解任务间干扰，避免对参数空间施加刚性约束，从而在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于正交约束轻量适配器的方法虽能减少任务干扰，但损害模型可塑性；需一种更灵活、兼顾知识迁移与抗干扰能力的CIL方法。

Method: 提出语义引导动态稀疏化（SGDS），通过定向稀疏化调控激活子空间的方向与秩：鼓励相似类别共享紧凑激活子空间以促进知识迁移，为不相似类别分配非重叠激活子空间以防止干扰。

Result: 在多个标准CIL基准数据集上取得当前最优性能。

Conclusion: 在激活空间进行语义引导的动态稀疏化比在参数空间施加正交约束更有效，能更好平衡稳定性与可塑性。

Abstract: Class-Incremental Learning (CIL) requires a model to continually learn new classes without forgetting old ones. A common and efficient solution freezes a pre-trained model and employs lightweight adapters, whose parameters are often forced to be orthogonal to prevent inter-task interference. However, we argue that this parameter-constraining method is detrimental to plasticity. To this end, we propose Semantic-Guided Dynamic Sparsification (SGDS), a novel method that proactively guides the activation space by governing the orientation and rank of its subspaces through targeted sparsification. Specifically, SGDS promotes knowledge transfer by encouraging similar classes to share a compact activation subspace, while simultaneously preventing interference by assigning non-overlapping activation subspaces to dissimilar classes. By sculpting class-specific sparse subspaces in the activation space, SGDS effectively mitigates interference without imposing rigid constraints on the parameter space. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of SGDS.

</details>


### [33] [Towards Geometry-Aware and Motion-Guided Video Human Mesh Recovery](https://arxiv.org/abs/2601.21376)
*Hongjun Chen,Huan Zheng,Wencheng Han,Jianbing Shen*

Main category: cs.CV

TL;DR: HMRMamba提出一种基于结构化状态空间模型（SSM）的新范式，通过几何感知提升模块和运动引导重建网络，显著提升视频驱动3D人体网格恢复的准确性、时序一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频的3D人体网格恢复方法因依赖有缺陷的3D姿态中间表示且难以建模复杂时空动态，导致结果物理不可行。

Method: 提出HMRMamba框架：1）几何感知提升模块，采用双扫描Mamba架构，融合图像几何特征直接进行2D到3D姿态提升；2）运动引导重建网络，以提升的姿态序列为锚点，显式建模时序运动学模式。

Result: 在3DPW、MPI-INF-3DHP和Human3.6M数据集上达到SOTA性能，在重建精度、时序一致性与计算效率三方面均优于现有方法。

Conclusion: HMRMamba验证了SSM在视频HMR任务中的有效性，为解决姿态锚点缺陷与时空建模难题提供了新思路。

Abstract: Existing video-based 3D Human Mesh Recovery (HMR) methods often produce physically implausible results, stemming from their reliance on flawed intermediate 3D pose anchors and their inability to effectively model complex spatiotemporal dynamics. To overcome these deep-rooted architectural problems, we introduce HMRMamba, a new paradigm for HMR that pioneers the use of Structured State Space Models (SSMs) for their efficiency and long-range modeling prowess. Our framework is distinguished by two core contributions. First, the Geometry-Aware Lifting Module, featuring a novel dual-scan Mamba architecture, creates a robust foundation for reconstruction. It directly grounds the 2D-to-3D pose lifting process with geometric cues from image features, producing a highly reliable 3D pose sequence that serves as a stable anchor. Second, the Motion-guided Reconstruction Network leverages this anchor to explicitly process kinematic patterns over time. By injecting this crucial temporal awareness, it significantly enhances the final mesh's coherence and robustness, particularly under occlusion and motion blur. Comprehensive evaluations on 3DPW, MPI-INF-3DHP, and Human3.6M benchmarks confirm that HMRMamba sets a new state-of-the-art, outperforming existing methods in both reconstruction accuracy and temporal consistency while offering superior computational efficiency.

</details>


### [34] [Rectifying Geometry-Induced Similarity Distortions for Real-World Aerial-Ground Person Re-Identification](https://arxiv.org/abs/2601.21405)
*Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 本文提出GIQT方法，通过几何引导的查询-键变换显式校正视角差异导致的相似度空间失真，并结合几何条件提示生成机制，提升航拍-地面跨视角行人重识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法隐含假设点积相似度在大视角和尺度变化下仍可靠，但实际中极端相机几何会系统性扭曲查询-键相似度空间，导致注意力匹配性能下降。

Method: 提出Geometry-Induced Query-Key Transformation（GIQT）轻量低秩模块，基于相机几何显式校正查询-键相似度计算；并引入几何条件提示生成机制，提供全局、视图自适应的表征先验。

Result: 在四个航拍-地面行人重识别基准上验证了该框架在极端及未见几何条件下的鲁棒性提升，且计算开销极小。

Conclusion: 显式建模相机几何对相似度空间的影响比仅优化特征更关键，GIQT为跨视角ReID提供了更可靠的匹配基础。

Abstract: Aerial-ground person re-identification (AG-ReID) is fundamentally challenged by extreme viewpoint and distance discrepancies between aerial and ground cameras, which induce severe geometric distortions and invalidate the assumption of a shared similarity space across views. Existing methods primarily rely on geometry-aware feature learning or appearance-conditioned prompting, while implicitly assuming that the geometry-invariant dot-product similarity used in attention mechanisms remains reliable under large viewpoint and scale variations. We argue that this assumption does not hold. Extreme camera geometry systematically distorts the query-key similarity space and degrades attention-based matching, even when feature representations are partially aligned.
  To address this issue, we introduce Geometry-Induced Query-Key Transformation (GIQT), a lightweight low-rank module that explicitly rectifies the similarity space by conditioning query-key interactions on camera geometry. Rather than modifying feature representations or the attention formulation itself, GIQT adapts the similarity computation to compensate for dominant geometry-induced anisotropic distortions. Building on this local similarity rectification, we further incorporate a geometry-conditioned prompt generation mechanism that provides global, view-adaptive representation priors derived directly from camera geometry.
  Experiments on four aerial-ground person re-identification benchmarks demonstrate that the proposed framework consistently improves robustness under extreme and previously unseen geometric conditions, while introducing minimal computational overhead compared to state-of-the-art methods.

</details>


### [35] [Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation](https://arxiv.org/abs/2601.21406)
*Zihan Su,Hongyang Wei,Kangrui Cen,Yong Wang,Guanhua Chen,Chun Yuan,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出UniMRG方法，通过在统一多模态模型（UMMs）后训练中引入像素、深度和分割等多种图像内在表征的生成任务，以提升其视觉理解能力，并实现理解与生成能力的协同增强。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型虽能利用理解提升生成，但如何用生成反哺理解尚未被充分探索。

Method: 提出架构无关的后训练方法UniMRG，让UMM同时学习生成图像的多种内在表征（像素重建、深度图、分割图）及完成标准视觉理解任务。

Result: 在多种UMM架构上验证了该方法显著提升细粒度感知、减少幻觉、增强空间理解，并同步提升生成能力。

Conclusion: 生成多样化的内在表征可有效促进模型对视觉输入的深层、全面理解，实现理解与生成的双向增强。

Abstract: Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.

</details>


### [36] [MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations](https://arxiv.org/abs/2601.21408)
*Xinan He,Kaiqing Lin,Yue Zhou,Jiaming Zhong,Wei Ye,Wenhui Yi,Bing Fan,Feng Ding,Haodong Li,Bo Cao,Bin Li*

Main category: cs.CV

TL;DR: 本文提出一种基于‘流形投影波动’（MPF）现象的双路径视频鉴伪框架，通过静态流形偏差分支检测空间异常，通过微时序波动分支捕捉AI生成视频中固有的结构化时间残差特征，从而实现对高保真伪造视频的鲁棒鉴别。


<details>
  <summary>Details</summary>
Motivation: 尽管当前视频生成模型已能生成视觉上高度逼真的内容，宏观语义和时序不一致已不明显，但作者认为AI生成视频本质是流形拟合过程而非真实物理记录，其帧间像素构成逻辑仍残留可检测的结构化、同质化特征（即MPF），因此仍可区分真假。

Method: 提出分层双路径框架：1）静态流形偏差分支，利用大规模视觉基础模型（VFMs）的精细感知边界检测偏离自然真实流形的空间异常；2）微时序波动分支，针对成功落在流形上的高保真视频，分析其残留的MPF特征以进行细粒度时序鉴别。

Result: 该框架能有效识别高保真AI生成视频，无论其伪造痕迹表现为全局流形偏离还是细微计算指纹，展现出对前沿伪造技术的鲁棒性。

Conclusion: AI生成视频虽在表观上逼近真实，但其内在的流形拟合机制导致固有且可量化的MPF特征，为视频鉴伪提供了新范式和可靠依据。

Abstract: With the rapid advancement of video generation models such as Veo and Wan, the visual quality of synthetic content has reached a level where macro-level semantic errors and temporal inconsistencies are no longer prominent. However, this does not imply that the distinction between real and cutting-edge high-fidelity fake is untraceable. We argue that AI-generated videos are essentially products of a manifold-fitting process rather than a physical recording. Consequently, the pixel composition logic of consecutive adjacent frames residual in AI videos exhibits a structured and homogenous characteristic. We term this phenomenon `Manifold Projection Fluctuations' (MPF). Driven by this insight, we propose a hierarchical dual-path framework that operates as a sequential filtering process. The first, the Static Manifold Deviation Branch, leverages the refined perceptual boundaries of Large-Scale Vision Foundation Models (VFMs) to capture residual spatial anomalies or physical violations that deviate from the natural real-world manifold (off-manifold). For the remaining high-fidelity videos that successfully reside on-manifold and evade spatial detection, we introduce the Micro-Temporal Fluctuation Branch as a secondary, fine-grained filter. By analyzing the structured MPF that persists even in visually perfect sequences, our framework ensures that forgeries are exposed regardless of whether they manifest as global real-world manifold deviations or subtle computational fingerprints.

</details>


### [37] [From Implicit Ambiguity to Explicit Solidity: Diagnosing Interior Geometric Degradation in Neural Radiance Fields for Dense 3D Scene Understanding](https://arxiv.org/abs/2601.21421)
*Jiangsan Zhao,Jakob Geipel,Kryzysztof Kusnierek*

Main category: cs.CV

TL;DR: 本文揭示了NeRF在密集自遮挡场景中因隐式密度场导致的内部几何退化（IGD）问题，提出基于稀疏体素光栅化的显式几何方法SVRaster，显著提升实例恢复率和对监督噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NeRF在密集、自遮挡场景中用于定量3D分析的可靠性尚不明确，尤其存在隐式密度场在重遮挡下重建空心或碎片化结构的问题。

Method: 提出基于稀疏体素光栅化（SVRaster）的显式几何重建流程，以SfM特征几何为初始化，通过将2D实例掩码投影到体素网格并递归分割来保证几何分离与物理实心性。

Result: 在密集场景中，SVRaster实现95.8%的实例恢复率，比当前最优mask监督NeRF高约7个百分点；在分割掩码退化情况下，比隐式基线多恢复43%的实例。

Conclusion: 显式几何先验是高度自遮挡3D场景中可靠定量分析的前提，隐式表示难以满足物理一致性要求。

Abstract: Neural Radiance Fields (NeRFs) have emerged as a powerful paradigm for multi-view reconstruction, complementing classical photogrammetric pipelines based on Structure-from-Motion (SfM) and Multi-View Stereo (MVS). However, their reliability for quantitative 3D analysis in dense, self-occluding scenes remains poorly understood. In this study, we identify a fundamental failure mode of implicit density fields under heavy occlusion, which we term Interior Geometric Degradation (IGD). We show that transmittance-based volumetric optimization satisfies photometric supervision by reconstructing hollow or fragmented structures rather than solid interiors, leading to systematic instance undercounting. Through controlled experiments on synthetic datasets with increasing occlusion, we demonstrate that state-of-the-art mask-supervised NeRFs saturate at approximately 89% instance recovery in dense scenes, despite improved surface coherence and mask quality. To overcome this limitation, we introduce an explicit geometric pipeline based on Sparse Voxel Rasterization (SVRaster), initialized from SfM feature geometry. By projecting 2D instance masks onto an explicit voxel grid and enforcing geometric separation via recursive splitting, our approach preserves physical solidity and achieves a 95.8% recovery rate in dense clusters. A sensitivity analysis using degraded segmentation masks further shows that explicit SfM-based geometry is substantially more robust to supervision failure, recovering 43% more instances than implicit baselines. These results demonstrate that explicit geometric priors are a prerequisite for reliable quantitative analysis in highly self-occluding 3D scenes.

</details>


### [38] [MultiModal Fine-tuning with Synthetic Captions](https://arxiv.org/abs/2601.21426)
*Shohei Enomoto,Shin'ya Yamaguchi*

Main category: cs.CV

TL;DR: 本文提出了一种利用多模态大语言模型（MLLM）为单模态图像数据生成合成文本描述的方法，从而将单模态微调升级为多模态微调，并引入监督对比损失与基于类平均文本嵌入的推理策略，在多个图像分类基准（尤其小样本场景）上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 预训练已转向多模态学习，但微调仍以单模态为主，导致无法充分利用预训练中丰富的多模态表征能力，存在模态不一致的根本性鸿沟。

Method: 使用MLLM根据图像类别标签和领域上下文生成高质量合成图像标题，将单模态数据集转化为多模态数据集；设计监督对比损失以增强同类样本在特征空间中的聚类；提出基于每张图像多个合成标题的类平均文本嵌入进行推理。

Result: 在13个图像分类基准上全面超越基线方法，尤其在小样本学习场景下提升显著。

Conclusion: 该工作建立了连接多模态预训练与单模态微调的新范式，证明了通过数据集多模态增强可有效弥合预训练-微调间的模态鸿沟。

Abstract: In this paper, we address a fundamental gap between pre-training and fine-tuning of deep neural networks: while pre-training has shifted from unimodal to multimodal learning with enhanced visual understanding, fine-tuning predominantly remains unimodal, limiting the benefits of rich pre-trained representations. To bridge this gap, we propose a novel approach that transforms unimodal datasets into multimodal ones using Multimodal Large Language Models (MLLMs) to generate synthetic image captions for fine-tuning models with a multimodal objective. Our method employs carefully designed prompts incorporating class labels and domain context to produce high-quality captions tailored for classification tasks. Furthermore, we introduce a supervised contrastive loss function that explicitly encourages clustering of same-class representations during fine-tuning, along with a new inference technique that leverages class-averaged text embeddings from multiple synthetic captions per image. Extensive experiments across 13 image classification benchmarks demonstrate that our approach outperforms baseline methods, with particularly significant improvements in few-shot learning scenarios. Our work establishes a new paradigm for dataset enhancement that effectively bridges the gap between multimodal pre-training and fine-tuning. Our code is available at https://github.com/s-enmt/MMFT.

</details>


### [39] [Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention](https://arxiv.org/abs/2601.21444)
*Yuxiang Huang,Mingye Li,Xu Han,Chaojun Xiao,Weilin Zhao,Ao Sun,Ziqi Yuan,Hao Zhou,Fandong Meng,Zhiyuan Liu*

Main category: cs.CV

TL;DR: 本文提出Spava，一种用于加速长视频推理的序列并行框架，通过分布式近似注意力和系统级优化，在多GPU上实现高效长视频处理，显著提升推理速度且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单GPU上压缩视觉嵌入或应用稀疏注意力，加速效果有限或导致性能下降，难以支持更长、更复杂的视频处理。

Method: 提出Spava框架，采用序列并行与优化注意力机制，通过分布式近似注意力降低计算量、提升并行性，并结合负载均衡与融合前向传播等系统级优化。

Result: 相比FlashAttn、ZigZagRing和APB，Spava在多GPU上分别实现12.72x、1.70x和1.18x的推理加速，且无明显性能损失。

Conclusion: Spava有效突破长视频推理瓶颈，为大型多模态模型在长视频理解任务中提供高效、可扩展的推理解决方案。

Abstract: The efficiency of long-video inference remains a critical bottleneck, mainly due to the dense computation in the prefill stage of Large Multimodal Models (LMMs). Existing methods either compress visual embeddings or apply sparse attention on a single GPU, yielding limited acceleration or degraded performance and restricting LMMs from handling longer, more complex videos. To overcome these issues, we propose Spava, a sequence-parallel framework with optimized attention that accelerates long-video inference across multiple GPUs. By distributing approximate attention, Spava reduces computation and increases parallelism, enabling efficient processing of more visual embeddings without compression and thereby improving task performance. System-level optimizations, such as load balancing and fused forward passes, further unleash the potential of Spava, delivering speedups of 12.72x, 1.70x, and 1.18x over FlashAttn, ZigZagRing, and APB, without notable performance loss. Code available at https://github.com/thunlp/APB

</details>


### [40] [Variance & Greediness: A comparative study of metric-learning losses](https://arxiv.org/abs/2601.21450)
*Donghuo Zeng,Hao Niu,Zhi Li,Masato Taya*

Main category: cs.CV

TL;DR: 本文提出VARIANCE和GREEDINESS诊断框架，分析七种度量学习损失函数在嵌入几何与优化动态上的差异，揭示了效率与细粒度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 度量学习在检索中至关重要，但其对嵌入几何结构和优化动态的影响尚不明确。

Method: 构建VARIANCE（类内/类间方差）和GREEDINESS（活跃样本比与梯度范数）诊断框架，系统评估七种代表性损失函数在五个图像检索数据集上的表现。

Result: Triplet和SCL保持更高类内方差和更清晰类间边界，在细粒度检索中top-1性能更强；Contrastive和InfoNCE通过大量小步更新快速压缩嵌入，加速收敛但可能过度简化类别结构；N-pair虽实现较大平均分离度，但分布不均。

Conclusion: 不同损失函数存在效率-细粒度的权衡：需多样性保持与难例判别时优选Triplet/SCL；需快速嵌入压缩时可选Contrastive/InfoNCE。

Abstract: Metric learning is central to retrieval, yet its effects on embedding geometry and optimization dynamics are not well understood. We introduce a diagnostic framework, VARIANCE (intra-/inter-class variance) and GREEDINESS (active ratio and gradient norms), to compare seven representative losses, i.e., Contrastive, Triplet, N-pair, InfoNCE, ArcFace, SCL, and CCL, across five image-retrieval datasets. Our analysis reveals that Triplet and SCL preserve higher within-class variance and clearer inter-class margins, leading to stronger top-1 retrieval in fine-grained settings. In contrast, Contrastive and InfoNCE compact embeddings are achieved quickly through many small updates, accelerating convergence but potentially oversimplifying class structures. N-pair achieves a large mean separation but with uneven spacing. These insights reveal a form of efficiency-granularity trade-off and provide practical guidance: prefer Triplet/SCL when diversity preservation and hard-sample discrimination are critical, and Contrastive/InfoNCE when faster embedding compaction is desired.

</details>


### [41] [Mining Forgery Traces from Reconstruction Error: A Weakly Supervised Framework for Multimodal Deepfake Temporal Localization](https://arxiv.org/abs/2601.21458)
*Midou Guo,Qilin Yin,Wei Lu,Xiangyang Luo,Rui Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RT-DeepLoc的弱监督时序深度伪造定位框架，利用仅在真实视频上训练的掩码自编码器（MAE）建模正常时空模式，并通过重建误差定位伪造片段；引入非对称帧内对比损失（AICL）增强局部判别能力，在无需逐帧标注下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度伪造呈现局部化、间歇性特点，需细粒度时序定位；但逐帧标注成本过高，亟需仅依赖视频级标签的弱监督方法。

Method: 提出RT-DeepLoc框架：1）用仅在真实数据上训练的Masked Autoencoder建模正常时空模式；2）利用伪造片段引起的显著重建误差作为定位线索；3）设计Asymmetric Intra-video Contrastive Loss（AICL），以重建误差为引导，提升真实特征紧凑性与伪造区域区分能力。

Result: 在LAV-DF等大规模数据集上实验表明，RT-DeepLoc在弱监督时序伪造定位任务中达到当前最优性能。

Conclusion: RT-DeepLoc验证了基于重建误差的弱监督定位范式的有效性，AICL提升了模型对未见伪造类型的泛化能力与局部判别鲁棒性，为低成本、高精度深度伪造检测提供了新思路。

Abstract: Modern deepfakes have evolved into localized and intermittent manipulations that require fine-grained temporal localization. The prohibitive cost of frame-level annotation makes weakly supervised methods a practical necessity, which rely only on video-level labels. To this end, we propose Reconstruction-based Temporal Deepfake Localization (RT-DeepLoc), a weakly supervised temporal forgery localization framework that identifies forgeries via reconstruction errors. Our framework uses a Masked Autoencoder (MAE) trained exclusively on authentic data to learn its intrinsic spatiotemporal patterns; this allows the model to produce significant reconstruction discrepancies for forged segments, effectively providing the missing fine-grained cues for localization. To robustly leverage these indicators, we introduce a novel Asymmetric Intra-video Contrastive Loss (AICL). By focusing on the compactness of authentic features guided by these reconstruction cues, AICL establishes a stable decision boundary that enhances local discrimination while preserving generalization to unseen forgeries. Extensive experiments on large-scale datasets, including LAV-DF, demonstrate that RT-DeepLoc achieves state-of-the-art performance in weakly-supervised temporal forgery localization.

</details>


### [42] [Hypernetwork-Based Adaptive Aggregation for Multimodal Multiple-Instance Learning in Predicting Coronary Calcium Debulking](https://arxiv.org/abs/2601.21479)
*Kaito Shiku,Ichika Seo,Tetsuya Matoba,Rissei Hino,Yasuhiro Nakano,Ryoma Bise*

Main category: cs.CV

TL;DR: 本文首次尝试从CT图像中估计冠状动脉钙化减容的必要性，将其建模为多实例学习（MIL）问题，并提出一种基于超网络的自适应聚合Transformer（HyperAdAgFormer），利用患者表格数据动态调整特征聚合策略。


<details>
  <summary>Details</summary>
Motivation: 医生在决定是否使用设备进行冠状动脉钙化减容时，会依据患者的表格临床数据动态调整关注点和决策标准，而现有方法难以建模这种个体化决策过程。

Method: 将任务建模为多实例学习（MIL）问题，提出HyperAdAgFormer：一个通过超网络根据患者表格数据自适应调节Transformer中特征聚合策略的模型。

Result: 在真实临床数据集上的实验验证了HyperAdAgFormer的有效性。

Conclusion: HyperAdAgFormer能有效融合影像与表格数据，实现个性化、数据驱动的冠状动脉钙化减容必要性评估，为临床决策提供新工具。

Abstract: In this paper, we present the first attempt to estimate the necessity of debulking coronary artery calcifications from computed tomography (CT) images. We formulate this task as a Multiple-instance Learning (MIL) problem. The difficulty of this task lies in that physicians adjust their focus and decision criteria for device usage according to tabular data representing each patient's condition. To address this issue, we propose a hypernetwork-based adaptive aggregation transformer (HyperAdAgFormer), which adaptively modifies the feature aggregation strategy for each patient based on tabular data through a hypernetwork. The experiments using the clinical dataset demonstrated the effectiveness of HyperAdAgFormer. The code is publicly available at https://github.com/Shiku-Kaito/HyperAdAgFormer.

</details>


### [43] [SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing](https://arxiv.org/abs/2601.21498)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 本文提出了SimGraph，一个基于场景图的统一框架，用于图像生成和编辑，通过整合token-based生成和diffusion-based编辑，实现对对象交互、布局和空间一致性的精确控制，并在实验中超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成与编辑方法通常分开处理，导致空间一致性与语义连贯性难以维持，且缺乏对物体关系与空间排列的结构化控制。

Method: 提出SimGraph框架，将场景图作为统一表示，集成token-based生成与diffusion-based编辑，在单一场景图驱动模型中实现生成与编辑的协同。

Result: 实验表明，SimGraph在图像生成与编辑任务上均优于现有最先进方法，显著提升空间一致性与语义连贯性。

Conclusion: 基于场景图的统一建模可有效弥合图像生成与编辑之间的鸿沟，SimGraph为可控、一致的视觉内容创作提供了新范式。

Abstract: Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.

</details>


### [44] [HERS: Hidden-Pattern Expert Learning for Risk-Specific Vehicle Damage Adaptation in Diffusion Models](https://arxiv.org/abs/2601.21517)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 本文提出HERS框架，通过无监督的领域专家自适应方法提升文本到图像扩散模型在车辆损伤生成中的保真度、可控性和领域对齐性，并验证其在保险欺诈检测等高风险场景中的可信生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型生成逼真车辆损伤图像的能力引发保险自动化流程中真实性与合成数据边界模糊的风险，尤其可能被滥用于保险欺诈或索赔操纵。

Method: 提出HERS（Hidden-Pattern Expert Learning for Risk-Specific Damage Adaptation）框架：利用大语言模型和T2I流水线自动生成图文对，进行自监督微调；将每类损伤（如凹痕、划痕等）建模为独立专家，再融合为统一多损伤模型，无需人工标注。

Result: 在四个扩散骨干网络上评估，HERS相较基线提升5.5%文本忠实度和2.3%人类偏好评分；同时探讨了其在欺诈检测、可审计性及安全部署方面的实际影响。

Conclusion: 领域专用扩散模型既带来应用机遇也伴随显著风险，HERS提升了生成可信度，凸显在汽车保险等安全关键场景中可信生成的重要性。

Abstract: Recent advances in text-to-image (T2I) diffusion models have enabled increasingly realistic synthesis of vehicle damage, raising concerns about their reliability in automated insurance workflows. The ability to generate crash-like imagery challenges the boundary between authentic and synthetic data, introducing new risks of misuse in fraud or claim manipulation. To address these issues, we propose HERS (Hidden-Pattern Expert Learning for Risk-Specific Damage Adaptation), a framework designed to improve fidelity, controllability, and domain alignment of diffusion-generated damage images. HERS fine-tunes a base diffusion model via domain-specific expert adaptation without requiring manual annotation. Using self-supervised image-text pairs automatically generated by a large language model and T2I pipeline, HERS models each damage category, such as dents, scratches, broken lights, or cracked paint, as a separate expert. These experts are later integrated into a unified multi-damage model that balances specialization with generalization. We evaluate HERS across four diffusion backbones and observe consistent improvements: plus 5.5 percent in text faithfulness and plus 2.3 percent in human preference ratings compared to baselines. Beyond image fidelity, we discuss implications for fraud detection, auditability, and safe deployment of generative models in high-stakes domains. Our findings highlight both the opportunities and risks of domain-specific diffusion, underscoring the importance of trustworthy generation in safety-critical applications such as auto insurance.

</details>


### [45] [Vision KAN: Towards an Attention-Free Backbone for Vision with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.21541)
*Zhuoqin Yang,Jiansong Zhang,Xiaoling Luo,Xu Wu,Zheng Lu,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了Vision KAN（ViK），一种无需注意力机制的视觉骨干网络，基于Kolmogorov-Arnold Networks设计，通过MultiPatch-RBFKAN模块实现高效token混合，在保持线性复杂度的同时在ImageNet-1K上达到有竞争力的精度。


<details>
  <summary>Details</summary>
Motivation: 注意力机制存在二次计算复杂度和可解释性差的问题；近期无注意力架构表明高性能可不依赖成对注意力，因此需探索更高效、可解释的替代方案。

Method: 提出Vision KAN（ViK），核心为MultiPatch-RBFKAN：（a）基于径向基函数的KAN进行patch级非线性变换，（b）轴向可分混合实现高效局部传播，（c）低秩全局映射建模长程交互；采用patch分组与轻量算子替代全分辨率KAN以兼顾效率与跨patch依赖建模。

Result: 在ImageNet-1K上ViK取得与主流模型相当的分类精度，且计算复杂度为线性，验证了KAN-based token mixing作为注意力替代方案的有效性与理论合理性。

Conclusion: ViK证明了无需注意力机制亦可构建高性能、高效率、强理论支撑的视觉骨干网络，为后注意力时代视觉模型设计提供了新范式。

Abstract: Attention mechanisms have become a key module in modern vision backbones due to their ability to model long-range dependencies. However, their quadratic complexity in sequence length and the difficulty of interpreting attention weights limit both scalability and clarity. Recent attention-free architectures demonstrate that strong performance can be achieved without pairwise attention, motivating the search for alternatives. In this work, we introduce Vision KAN (ViK), an attention-free backbone inspired by the Kolmogorov-Arnold Networks. At its core lies MultiPatch-RBFKAN, a unified token mixer that combines (a) patch-wise nonlinear transform with Radial Basis Function-based KANs, (b) axis-wise separable mixing for efficient local propagation, and (c) low-rank global mapping for long-range interaction. Employing as a drop-in replacement for attention modules, this formulation tackles the prohibitive cost of full KANs on high-resolution features by adopting a patch-wise grouping strategy with lightweight operators to restore cross-patch dependencies. Experiments on ImageNet-1K show that ViK achieves competitive accuracy with linear complexity, demonstrating the potential of KAN-based token mixing as an efficient and theoretically grounded alternative to attention.

</details>


### [46] [Causal World Modeling for Robot Control](https://arxiv.org/abs/2601.21998)
*Lin Li,Qihang Zhang,Yiming Luo,Shuai Yang,Ruilin Wang,Fei Han,Mingrui Yu,Zelin Gao,Nan Xue,Xing Zhu,Yujun Shen,Yinghao Xu*

Main category: cs.CV

TL;DR: 本文提出LingBot-VA，一种结合视频世界建模与视觉-语言预训练的自回归扩散框架，通过共享潜在空间、闭环rollout机制和异步推理流水线，实现帧预测与策略执行联合学习，在仿真与真实场景中展现出长时程操作能力、数据高效性和强泛化性。


<details>
  <summary>Details</summary>
Motivation: 视频世界建模能通过理解动作与视觉动态间的因果关系来‘想象’近未来，为机器人学习提供新基础；同时结合视觉-语言预训练可进一步增强泛化与语义理解能力。

Method: 提出LingBot-VA：1）基于Mixture-of-Transformers（MoT）的视觉与动作token共享潜在空间；2）闭环rollout机制，融合真实环境观测反馈；3）异步推理流水线，解耦并行动作预测与运动执行。

Result: 在仿真与真实世界任务中验证了模型在长时程操作、后训练数据效率及新构型泛化性上的显著优势。

Conclusion: 视频世界建模与视觉-语言预训练协同构成了机器人学习的新范式；LingBot-VA为端到端具身智能提供了高效、鲁棒且可扩展的统一建模框架。

Abstract: This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.

</details>


### [47] [Bi-Anchor Interpolation Solver for Accelerating Generative Modeling](https://arxiv.org/abs/2601.21542)
*Hongxu Chen,Hongxiang Li,Zhen Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为BA-solver的新型求解器，通过引入轻量级SideNet与冻结主干网络协同工作，在不显著增加训练成本的前提下，大幅减少Flow Matching模型生成所需的神经函数评估次数（NFEs），同时保持高质量合成效果，并具备即插即用性。


<details>
  <summary>Details</summary>
Motivation: Flow Matching模型依赖迭代ODE求解，导致生成延迟高；现有加速方法要么在低NFE下性能差（训练-free），要么训练成本高、缺乏通用性（训练-based）。

Method: 提出Bi-Anchor Interpolation Solver（BA-solver）：1）双向时间感知——SideNet学习近似未来和历史速度，不更新主干；2）双锚点速度积分——利用SideNet与两个锚点速度高效估计中间速度，实现批处理高阶积分；主干提供高精度锚点，SideNet稠密化轨迹。

Result: 在ImageNet-256²上，BA-solver仅需10 NFE即可达到100+ NFE欧拉求解器的生成质量，5 NFE仍保持高保真，训练开销极小，且可无缝集成到现有生成流程中（如图像编辑）。

Conclusion: BA-solver在保持训练-free求解器通用性的同时，显著提升了Flow Matching模型的推理效率与实用性，为高效高质量生成提供了新范式。

Abstract: Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.

</details>


### [48] [Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration](https://arxiv.org/abs/2601.21592)
*Luwei Tu,Jiawei Wu,Xing Luo,Zhi Jin*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的扩散桥模型（UDBM），将全合一图像恢复（AiOIR）重新建模为由像素级不确定性引导的随机传输问题，通过松弛扩散桥和双调制策略，有效解决多退化任务中的优化冲突与漂移奇异性，实现单步推理下的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: All-in-One图像恢复面临异构退化间优化目标冲突的根本挑战，现有方法受限于粗粒度控制或固定映射调度，适应性不足。

Method: 提出不确定性感知扩散桥模型（UDBM）：1）采用松弛扩散桥替代严格终端约束，建模退化不确定性并理论消除漂移奇异性；2）设计双调制策略——噪声调度将多样退化映射至共享高熵隐空间，路径调度基于熵正则化的粘性动力学自适应调控传输轨迹。

Result: UDBM在多种图像恢复任务中单步推理即达到当前最优（SOTA）性能。

Conclusion: UDBM通过重构传输几何与动力学，为AiOIR提供了一种更鲁棒、自适应且理论上可解释的统一建模范式。

Abstract: All-in-One Image Restoration (AiOIR) faces the fundamental challenge in reconciling conflicting optimization objectives across heterogeneous degradations. Existing methods are often constrained by coarse-grained control mechanisms or fixed mapping schedules, yielding suboptimal adaptation. To address this, we propose an Uncertainty-Aware Diffusion Bridge Model (UDBM), which innovatively reformulates AiOIR as a stochastic transport problem steered by pixel-wise uncertainty. By introducing a relaxed diffusion bridge formulation which replaces the strict terminal constraint with a relaxed constraint, we model the uncertainty of degradations while theoretically resolving the drift singularity inherent in standard diffusion bridges. Furthermore, we devise a dual modulation strategy: the noise schedule aligns diverse degradations into a shared high-entropy latent space, while the path schedule adaptively regulates the transport trajectory motivated by the viscous dynamics of entropy regularization. By effectively rectifying the transport geometry and dynamics, UDBM achieves state-of-the-art performance across diverse restoration tasks within a single inference step.

</details>


### [49] [HydroSense: A Dual-Microcontroller IoT Framework for Real-Time Multi-Parameter Water Quality Monitoring with Edge Processing and Cloud Analytics](https://arxiv.org/abs/2601.21595)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Anish Giri*

Main category: cs.CV

TL;DR: 本文提出了一种低成本、高精度的物联网水质监测系统HydroSense，集成pH、溶解氧、温度、总溶解固体、氮含量和水位六项关键参数，采用双微控制器架构（Arduino Uno + ESP32），具备校准、温度补偿与边缘处理能力，在90天实验中表现出优异精度与99.8%云端传输可靠性，成本仅为商用系统的15%，显著提升资源受限地区的可及性。


<details>
  <summary>Details</summary>
Motivation: 全球水资源危机亟需经济、准确、实时的水质监测方案，而传统人工采样或昂贵商用系统难以在资源受限地区普及。

Method: 提出HydroSense物联网框架，采用Arduino Uno（负责五点校准的精密模拟测量）与ESP32（负责无线通信、边缘计算与云集成）的双微控制器架构；集成pH、DO、温度、TDS、估算氮含量和水位六参数；应用中值滤波、温度补偿算法与鲁棒错误处理机制。

Result: 90天实验证明：pH精度±0.08（0–14范围）、DO稳定性±0.2 mg/L、TDS精度±1.9%（0–1000 ppm）、云端数据传输可靠率达99.8%；总成本32,983 BDT（约300美元），较商用系统降低成本85%。

Conclusion: HydroSense通过智能系统架构与低成本元器件选型，实现了专业级水质监测能力，为资源受限环境下的环境监测树立了新范式，验证了高性价比解决方案的可行性与实用性。

Abstract: The global water crisis necessitates affordable, accurate, and real-time water quality monitoring solutions. Traditional approaches relying on manual sampling or expensive commercial systems fail to address accessibility challenges in resource-constrained environments. This paper presents HydroSense, an innovative Internet of Things framework that integrates six critical water quality parameters including pH, dissolved oxygen (DO), temperature, total dissolved solids (TDS), estimated nitrogen, and water level into a unified monitoring system. HydroSense employs a novel dual-microcontroller architecture, utilizing Arduino Uno for precision analog measurements with five-point calibration algorithms and ESP32 for wireless connectivity, edge processing, and cloud integration. The system implements advanced signal processing techniques including median filtering for TDS measurement, temperature compensation algorithms, and robust error handling. Experimental validation over 90 days demonstrates exceptional performance metrics: pH accuracy of plus or minus 0.08 units across the 0 to 14 range, DO measurement stability within plus or minus 0.2 mg/L, TDS accuracy of plus or minus 1.9 percent across 0 to 1000 ppm, and 99.8 percent cloud data transmission reliability. With a total implementation cost of 32,983 BDT (approximately 300 USD), HydroSense achieves an 85 percent cost reduction compared to commercial systems while providing enhanced connectivity through the Firebase real-time database. This research establishes a new paradigm for accessible environmental monitoring, demonstrating that professional-grade water quality assessment can be achieved through intelligent system architecture and cost-effective component selection.

</details>


### [50] [WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models](https://arxiv.org/abs/2601.21610)
*Zijin Yang,Yu Sun,Kejiang Chen,Jiawei Zhao,Jun Jiang,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出WMVLM，首个基于视觉-语言模型（VLM）的统一、可解释的扩散模型图像水印评估框架，分别针对残差型与语义型水印重新定义质量与安全性指标，并通过三阶段训练策略实现分类、打分与可解释文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有水印评估方法缺乏统一框架、不可解释、忽视全面安全性、且对语义水印使用不恰当指标。

Method: 提出WMVLM框架，利用视觉-语言模型，为残差水印定义 artifact strength 与 erasure resistance 指标，为语义水印定义 latent distribution shift 指标；并采用三阶段训练策略（分类→打分→可解释文本生成）。

Result: WMVLM在多个数据集、扩散模型和水印方法上展现出优于SOTA VLM的泛化能力与评估性能。

Conclusion: WMVLM是首个支持残差与语义水印统一、可解释评估的VLM框架，显著提升了水印评估的准确性、安全性与实用性。

Abstract: Digital watermarking is essential for securing generated images from diffusion models. Accurate watermark evaluation is critical for algorithm development, yet existing methods have significant limitations: they lack a unified framework for both residual and semantic watermarks, provide results without interpretability, neglect comprehensive security considerations, and often use inappropriate metrics for semantic watermarks. To address these gaps, we propose WMVLM, the first unified and interpretable evaluation framework for diffusion model image watermarking via vision-language models (VLMs). We redefine quality and security metrics for each watermark type: residual watermarks are evaluated by artifact strength and erasure resistance, while semantic watermarks are assessed through latent distribution shifts. Moreover, we introduce a three-stage training strategy to progressively enable the model to achieve classification, scoring, and interpretable text generation. Experiments show WMVLM outperforms state-of-the-art VLMs with strong generalization across datasets, diffusion models, and watermarking methods.

</details>


### [51] [PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization](https://arxiv.org/abs/2601.21617)
*Songhan Jiang,Fengchun Liu,Ziyue Wang,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: 本文提出了PathReasoner，首个大规模全切片图像（WSI）推理数据集，并基于其构建了PathReasoner-R1模型，通过知识引导的生成流程和融合轨迹掩码监督微调与推理导向强化学习的方法，提升病理诊断模型的可解释性与临床可信度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在计算病理学中缺乏可验证、证据链接的推理过程，导致临床信任度低、专家难以纠错。

Method: 构建PathReasoner数据集（20K高质量样本），采用知识图谱驱动的严谨生成流程；提出PathReasoner-R1模型，结合轨迹掩码监督微调与推理导向强化学习，并设计知识感知的多粒度奖励函数（含实体奖励机制）以保障逻辑一致性。

Result: PathReasoner-R1在PathReasoner数据集及多个公开基准上均达到SOTA性能，支持多尺度图像输入，显著提升模型推理透明性与临床可靠性。

Conclusion: 本工作首次系统性地将结构化、知识对齐的链式推理能力引入计算病理学，为构建可信、可调试、临床落地的AI辅助诊断系统提供了新范式。

Abstract: Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.

</details>


### [52] [Similarity of Processing Steps in Vision Model Representations](https://arxiv.org/abs/2601.21621)
*Matéo Mahaut,Marco Baroni*

Main category: cs.CV

TL;DR: 本文研究了不同视觉模型在训练过程中如何收敛到相似的表示，发现尽管最终表示可能相似，但中间处理步骤和操作存在显著差异，特别是分类器模型会丢弃低级图像统计信息，而CNN和Transformer模型在表示变化上也表现出不同特性。


<details>
  <summary>Details</summary>
Motivation: 探究不同视觉模型是否不仅在最终表示上收敛，而且在中间处理步骤和操作上也收敛。

Method: 通过量化不同模型在不同处理阶段的表示距离，追踪模型间距离的演化过程，并识别出模型间差异最大的处理步骤。

Result: 发现相同位置的层具有最相似的表示，但仍有显著差异；分类器模型在最后层丢弃低级图像统计信息；Transformer模型相比CNN模型在层间表示变化更平滑。

Conclusion: 不同视觉模型在表示收敛的程度和性质上存在差异，这有助于更深入理解图像模型的内在处理机制。

Abstract: Recent literature suggests that the bigger the model, the more likely it is to converge to similar, ``universal'' representations, despite different training objectives, datasets, or modalities. While this literature shows that there is an area where model representations are similar, we study here how vision models might get to those representations -- in particular, do they also converge to the same intermediate steps and operations? We therefore study the processes that lead to convergent representations in different models. First, we quantify distance between different model representations at different stages. We follow the evolution of distances between models throughout processing, identifying the processing steps which are most different between models. We find that while layers at similar positions in different models have the most similar representations, strong differences remain. Classifier models, unlike the others, will discard information about low-level image statistics in their final layers. CNN- and transformer-based models also behave differently, with transformer models applying smoother changes to representations from one layer to the next. These distinctions clarify the level and nature of convergence between model representations, and enables a more qualitative account of the underlying processes in image models.

</details>


### [53] [A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion](https://arxiv.org/abs/2601.21633)
*Pu Cao,Yiyang Ma,Feng Zhou,Xuedan Yin,Qing Song,Lu Yang*

Main category: cs.CV

TL;DR: 本文揭示了在潜在扩散模型中，自动编码器（AE）评估过度偏向生成指标（如gFID）而忽视重建保真度的问题，并指出该偏差虽对ImageNet图像生成影响有限，却会损害可控生成中的条件对齐能力；作者通过理论分析与多维条件漂移实验验证了重建指标（尤其实例级）更能反映可控性，为可控扩散模型的AE选择提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有ImageNet规模AE研究过度依赖gFID等生成指标进行模型选择，忽视重建保真度，导致在向可控扩散扩展时出现条件漂移、可控性下降的问题。

Method: 理论分析AE诱导条件漂移的机制；提出多维条件漂移评估协议；在多个近期ImageNet AE上实证比较gFID与重建指标（如实例级重建误差）对可控性的预测能力；结合ControlNet实验验证可控性与条件保持能力的关联。

Result: gFID与条件保持弱相关，而重建保真度（尤其实例级）显著更优地预测可控性；ControlNet实验表明可控性由条件保持能力而非gFID决定。

Conclusion: 当前以ImageNet为中心的AE评估范式不适用于可控扩散任务，应重视重建保真度（尤其是实例级指标）作为AE选型和基准测试的核心标准。

Abstract: In latent diffusion models, the autoencoder (AE) is typically expected to balance two capabilities: faithful reconstruction and a generation-friendly latent space (e.g., low gFID). In recent ImageNet-scale AE studies, we observe a systematic bias toward generative metrics in handling this trade-off: reconstruction metrics are increasingly under-reported, and ablation-based AE selection often favors the best-gFID configuration even when reconstruction fidelity degrades. We theoretically analyze why this gFID-dominant preference can appear unproblematic for ImageNet generation, yet becomes risky when scaling to controllable diffusion: AEs can induce condition drift, which limits achievable condition alignment. Meanwhile, we find that reconstruction fidelity, especially instance-level measures, better indicates controllability. We empirically validate the impact of tilted autoencoder evaluation on controllability by studying several recent ImageNet AEs. Using a multi-dimensional condition-drift evaluation protocol reflecting controllable generation tasks, we find that gFID is only weakly predictive of condition preservation, whereas reconstruction-oriented metrics are substantially more aligned. ControlNet experiments further confirm that controllability tracks condition preservation rather than gFID. Overall, our results expose a gap between ImageNet-centric AE evaluation and the requirements of scalable controllable diffusion, offering practical guidance for more reliable benchmarking and model selection.

</details>


### [54] [RSGround-R1: Rethinking Remote Sensing Visual Grounding through Spatial Reasoning](https://arxiv.org/abs/2601.21634)
*Shiqi Huang,Shuting He,Bihan Wen*

Main category: cs.CV

TL;DR: 本文提出RSGround-R1框架，通过链式思维监督微调、强化微调与空间一致性优化，提升多模态大模型在遥感视觉定位任务中的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 遥感场景具有大尺度和高语义模糊性，自然语言描述高度依赖位置线索，给多模态大语言模型的空间推理带来独特挑战。

Method: 提出推理引导、位置感知的后训练框架RSGround-R1：包括基于合成数据的链式思维监督微调（CoT-SFT）、引入距离感知位置奖励的强化微调（RFT），以及基于空间一致性的动态策略更新优化。

Result: 在RSVG基准测试中展现出更优的性能与泛化能力。

Conclusion: 位置线索可被有效建模并融入多模态大模型训练流程，显著提升其在遥感视觉定位任务中的空间理解与定位精度。

Abstract: Remote Sensing Visual Grounding (RSVG) aims to localize target objects in large-scale aerial imagery based on natural language descriptions. Owing to the vast spatial scale and high semantic ambiguity of remote sensing scenes, these descriptions often rely heavily on positional cues, posing unique challenges for Multimodal Large Language Models (MLLMs) in spatial reasoning. To leverage this unique feature, we propose a reasoning-guided, position-aware post-training framework, dubbed \textbf{RSGround-R1}, to progressively enhance spatial understanding. Specifically, we first introduce Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) using synthetically generated RSVG reasoning data to establish explicit position awareness. Reinforcement Fine-Tuning (RFT) is then applied, augmented by our newly designed positional reward that provides continuous and distance-aware guidance toward accurate localization. Moreover, to mitigate incoherent localization behaviors across rollouts, we introduce a spatial consistency guided optimization scheme that dynamically adjusts policy updates based on their spatial coherence, ensuring stable and robust convergence. Extensive experiments on RSVG benchmarks demonstrate superior performance and generalization of our model.

</details>


### [55] [OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models](https://arxiv.org/abs/2601.21639)
*Yufeng Zhong,Lei Chen,Xuanle Zhao,Wenkang Han,Liming Zheng,Jing Huang,Deyang Jiang,Yilin Cao,Lin Ma,Zhixiong Zeng*

Main category: cs.CV

TL;DR: 本文提出了OCRVerse，首个端到端统一处理文本中心型（如文档）和视觉中心型（如图表、网页、科学绘图）OCR任务的综合方法，通过多领域两阶段（SFT+RL）训练与灵活奖励机制，在两类OCR任务上均达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有OCR方法主要聚焦于文本识别（Text-centric OCR），忽视了对图表、网页、科学绘图等视觉信息密集图像中的视觉元素识别（Vision-centric OCR），而这类图像在互联网中广泛存在且具有重要应用价值。

Method: 提出OCRVerse框架，构建覆盖文本型（报纸、杂志、书籍）和视觉型（图表、网页、科学绘图）的综合性数据集，并采用两阶段训练：第一阶段监督微调（SFT）混合多域数据建立通用知识；第二阶段强化学习（RL）为各域定制灵活奖励策略以适配不同输出格式与目标。

Result: OCRVerse在文本中心型和视觉中心型OCR任务上均取得有竞争力的结果，性能媲美大规模开源及闭源模型。

Conclusion: OCRVerse首次实现了文本与视觉双中心OCR的端到端统一建模，验证了多域协同训练与可定制RL奖励机制在跨模态OCR任务中的有效性与泛化能力。

Abstract: The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.

</details>


### [56] [CAF-Mamba: Mamba-Based Cross-Modal Adaptive Attention Fusion for Multimodal Depression Detection](https://arxiv.org/abs/2601.21648)
*Bowen Zhou,Marc-André Fiedler,Ayoub Al-Hamadi*

Main category: cs.CV

TL;DR: 本文提出CAF-Mamba，一种基于Mamba的跨模态自适应注意力融合框架，用于抑郁症检测，通过显隐式建模跨模态交互及动态调整模态贡献，在LMVD和D-Vlog数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在抑郁症检测中存在特征类型有限、忽略显式跨模态交互、融合方式简单（如拼接或静态加权）等问题。

Method: 提出CAF-Mamba框架，基于Mamba架构，结合显式与隐式跨模态交互建模，并引入模态级自适应注意力机制实现动态融合。

Result: 在LMVD和D-Vlog两个真实场景基准数据集上，CAF-Mamba持续优于现有方法，达到当前最优性能。

Conclusion: CAF-Mamba有效提升了多模态抑郁症检测的性能，验证了动态跨模态注意力融合在精神健康分析中的有效性与潜力。

Abstract: Depression is a prevalent mental health disorder that severely impairs daily functioning and quality of life. While recent deep learning approaches for depression detection have shown promise, most rely on limited feature types, overlook explicit cross-modal interactions, and employ simple concatenation or static weighting for fusion. To overcome these limitations, we propose CAF-Mamba, a novel Mamba-based cross-modal adaptive attention fusion framework. CAF-Mamba not only captures cross-modal interactions explicitly and implicitly, but also dynamically adjusts modality contributions through a modality-wise attention mechanism, enabling more effective multimodal fusion. Experiments on two in-the-wild benchmark datasets, LMVD and D-Vlog, demonstrate that CAF-Mamba consistently outperforms existing methods and achieves state-of-the-art performance.

</details>


### [57] [Few-Shot Domain Adaptation with Temporal References and Static Priors for Glacier Calving Front Delineation](https://arxiv.org/abs/2601.21663)
*Marcel Dreier,Nora Gourmelon,Dakota Pyles,Thorsten Seehaus,Matthias H. Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 本文提出了一种少样本领域自适应方法，结合空间静态先验知识和夏季参考图像，显著提升了冰川崩解前沿分割模型在新研究地点的泛化能力，误差从1131.6米降至68.7米。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的冰川崩解前沿分割模型在基准测试中表现优异，但在新研究地点（分布外数据）实际应用时精度不足，难以满足后续科学分析需求。

Method: 采用少样本领域自适应策略，融合空间静态先验知识，并在输入时间序列中加入夏季参考图像，不改变网络架构。

Result: 在新研究地点上，崩解前沿 delineation 误差从 1131.6 米大幅降低至 68.7 米。

Conclusion: 该方法为深度学习模型在新冰川区域的部署提供了可推广框架，支持全球尺度的崩解前沿监测。

Abstract: During benchmarking, the state-of-the-art model for glacier calving front delineation achieves near-human performance. However, when applied in a real-world setting at a novel study site, its delineation accuracy is insufficient for calving front products intended for further scientific analyses. This site represents an out-of-distribution domain for a model trained solely on the benchmark dataset. By employing a few-shot domain adaptation strategy, incorporating spatial static prior knowledge, and including summer reference images in the input time series, the delineation error is reduced from 1131.6 m to 68.7 m without any architectural modifications. These methodological advancements establish a framework for applying deep learning-based calving front segmentation to novel study sites, enabling calving front monitoring on a global scale.

</details>


### [58] [When Gradient Optimization Is Not Enough: $\dagger$ Dispersive and Anchoring Geometric Regularizer for Multimodal Learning](https://arxiv.org/abs/2601.21670)
*Zixuan Xia,Hao Wang,Pengcheng Weng,Yanyu Qian,Yangxin Xu,William Dan,Fei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为\regName的轻量级几何感知正则化框架，通过约束中间嵌入的模态内分散性和模态间锚定性，缓解多模态模型中的表示几何病态问题，提升多模态与单模态性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中即使优化充分，仍存在模态内表示坍缩和样本级跨模态不一致等几何病态问题，影响鲁棒性与融合效果；表示几何被识别为缺失的控制维度。

Method: 提出\regName框架，包含两个互补约束：模态内分散正则化（提升表示多样性）和模态间锚定正则化（限制跨模态漂移但不强制严格对齐）；该方法即插即用、无需修改网络结构、兼容多种训练范式。

Result: 在多个多模态基准上实验表明，该方法在多模态与单模态任务上均带来一致性能提升，有效缓解模态权衡问题。

Conclusion: 显式调控表示几何是提升多模态学习性能的有效途径，\regName提供了一种通用、轻量且高效的解决方案。

Abstract: Multimodal learning aims to integrate complementary information from heterogeneous modalities, yet strong optimization alone does not guaranty well-structured representations. Even under carefully balanced training schemes, multimodal models often exhibit geometric pathologies, including intra-modal representation collapse and sample-level cross-modal inconsistency, which degrade both unimodal robustness and multimodal fusion.
  We identify representation geometry as a missing control axis in multimodal learning and propose \regName, a lightweight geometry-aware regularization framework. \regName enforces two complementary constraints on intermediate embeddings: an intra-modal dispersive regularization that promotes representation diversity, and an inter-modal anchoring regularization that bounds sample-level cross-modal drift without rigid alignment. The proposed regularizer is plug-and-play, requires no architectural modifications, and is compatible with various training paradigms.
  Extensive experiments across multiple multimodal benchmarks demonstrate consistent improvements in both multimodal and unimodal performance, showing that explicitly regulating representation geometry effectively mitigates modality trade-offs.

</details>


### [59] [Multimodal Visual Surrogate Compression for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21673)
*Dexuan Ding,Ciyuan Peng,Endrowednes Kuantama,Jingcai Guo,Jia Wu,Jian Yang,Amin Beheshti,Ming-Hsuan Yang,Yuankai Qi*

Main category: cs.CV

TL;DR: 本文提出Multimodal Visual Surrogate Compression (MVSC)方法，将高维3D sMRI图像压缩为与2D基础模型对齐的紧凑2D视觉代理特征，以提升阿尔茨海默病（AD）分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有sMRI表征学习方法存在计算成本高、跨切片关系丢失、判别性特征提取能力有限等问题。

Method: MVSC包含两个核心模块：受文本引导的体素上下文编码器（Volume Context Encoder）和文本增强的自适应切片融合模块（Adaptive Slice Fusion），将3D sMRI压缩为对齐2D基础模型的2D视觉代理特征。

Result: 在三个大规模AD数据集上，MVSC在二分类和多分类任务中均优于当前最先进方法。

Conclusion: MVSC通过引入文本引导的跨切片建模与2D基础模型对齐的视觉代理压缩策略，有效提升了sMRI表征质量与AD诊断性能。

Abstract: High-dimensional structural MRI (sMRI) images are widely used for Alzheimer's Disease (AD) diagnosis. Most existing methods for sMRI representation learning rely on 3D architectures (e.g., 3D CNNs), slice-wise feature extraction with late aggregation, or apply training-free feature extractions using 2D foundation models (e.g., DINO). However, these three paradigms suffer from high computational cost, loss of cross-slice relations, and limited ability to extract discriminative features, respectively. To address these challenges, we propose Multimodal Visual Surrogate Compression (MVSC). It learns to compress and adapt large 3D sMRI volumes into compact 2D features, termed as visual surrogates, which are better aligned with frozen 2D foundation models to extract powerful representations for final AD classification. MVSC has two key components: a Volume Context Encoder that captures global cross-slice context under textual guidance, and an Adaptive Slice Fusion module that aggregates slice-level information in a text-enhanced, patch-wise manner. Extensive experiments on three large-scale Alzheimer's disease benchmarks demonstrate our MVSC performs favourably on both binary and multi-class classification tasks compared against state-of-the-art methods.

</details>


### [60] [ChartE$^{3}$: A Comprehensive Benchmark for End-to-End Chart Editing](https://arxiv.org/abs/2601.21694)
*Shuo Li,Jiajun Sun,Zhekai Wang,Xiaoran Fan,Hui Li,Dingwen Yang,Zhiheng Xi,Yijun Wang,Zifei Shan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本文提出了ChartE³，一个端到端图表编辑基准，用于评估模型在不依赖中间自然语言或代码表示的情况下直接根据多模态指令编辑图表的能力，涵盖局部外观调整与全局数据驱动变换两类任务。


<details>
  <summary>Details</summary>
Motivation: 现有图表编辑方法多采用基于流水线的设计，依赖自然语言或代码作为中间表示，难以忠实执行复杂编辑；亟需一种能同时保证细粒度控制与整体结构一致性的端到端编辑评估范式。

Method: 构建了ChartE³基准，包含1200+高质量样本，每样本由图表图像、底层代码和多模态编辑指令构成；定义局部编辑（如字体、颜色）与全球编辑（如数据过滤、趋势线添加）两类任务；对前沿多模态大模型进行系统评测。

Result: 实验表明当前SOTA多模态大模型在ChartE³上表现不佳，尤其在全球编辑任务上存在显著性能差距。

Conclusion: ChartE³揭示了现有端到端图表编辑能力的关键局限，为未来研究提供了标准化评估平台与明确改进方向。

Abstract: Charts are a fundamental visualization format for structured data analysis. Enabling end-to-end chart editing according to user intent is of great practical value, yet remains challenging due to the need for both fine-grained control and global structural consistency. Most existing approaches adopt pipeline-based designs, where natural language or code serves as an intermediate representation, limiting their ability to faithfully execute complex edits. We introduce ChartE$^{3}$, an End-to-End Chart Editing benchmark that directly evaluates models without relying on intermediate natural language programs or code-level supervision. ChartE$^{3}$ focuses on two complementary editing dimensions: local editing, which involves fine-grained appearance changes such as font or color adjustments, and global editing, which requires holistic, data-centric transformations including data filtering and trend line addition. ChartE$^{3}$ contains over 1,200 high-quality samples constructed via a well-designed data pipeline with human curation. Each sample is provided as a triplet of a chart image, its underlying code, and a multimodal editing instruction, enabling evaluation from both objective and subjective perspectives. Extensive benchmarking of state-of-the-art multimodal large language models reveals substantial performance gaps, particularly on global editing tasks, highlighting critical limitations in current end-to-end chart editing capabilities.

</details>


### [61] [DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning](https://arxiv.org/abs/2601.21716)
*Mingshuang Luo,Shuang Liang,Zhengkun Rong,Yuxuan Luo,Tianshu Hu,Ruibing Hou,Hong Chang,Yong Li,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: DreamActor-M2提出一种无需显式姿态先验、基于上下文学习的通用角色动画框架，通过模态融合与自引导数据合成，解决身份保持与运动一致性间的权衡问题，并在新基准AW Bench上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在运动注入策略不佳（导致身份保持与运动一致性难以兼顾）和过度依赖显式姿态先验（限制对非人形角色的泛化能力）两大问题。

Method: 采用两阶段范式：第一阶段将参考图像外观与驱动运动线索融合至统一潜在空间，利用基础模型生成先验联合建模身份与动态；第二阶段通过自引导伪跨身份数据合成，实现从姿态控制到端到端RGB驱动动画的过渡。

Result: 在新构建的AW Bench基准（涵盖多类角色与运动场景）上达到SOTA性能，显著提升视觉保真度与跨域泛化能力。

Conclusion: DreamActor-M2是一种通用、鲁棒且无需显式姿态建模的角色动画框架，为任意角色的高质量动画生成提供了新范式。

Abstract: Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/

</details>


### [62] [From Global to Granular: Revealing IQA Model Performance via Correlation Surface](https://arxiv.org/abs/2601.21738)
*Baoliang Chen,Danni Huang,Hanwei Zhu,Lingyu Zhu,Wei Zhou,Shiqi Wang,Yuming Fang,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出Granularity-Modulated Correlation (GMC)方法，通过引入细粒度相关性分析与分布正则化，克服传统全局相关性指标（如SRCC、PLCC）在图像质量评估（IQA）模型评价中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统全局相关性指标（如SRCC、PLCC）仅输出单一标量，无法反映IQ A模型在不同质量区间或微小质量差异下的局部排序一致性，且易受测试集质量分布影响，导致跨数据集比较不稳定。

Method: 提出GMC框架，包含两个核心组件：(1) 粒度调制器——基于高斯加权，在绝对MOS值和|MOS差值|上计算条件相关性；(2) 分布调节器——对相关性进行正则化以缓解非均匀质量分布带来的偏差；最终生成以MOS和|MOS差值|为坐标的三维相关性曲面。

Result: 在标准IQA基准上的实验表明，GMC能揭示传统标量指标无法捕获的模型性能特性，提供更丰富、稳定和可解释的评估结果。

Conclusion: GMC为IQA模型的分析、比较与部署提供了更精细、可靠和结构化的评估范式，弥补了现有全局指标的根本性缺陷。

Abstract: Evaluation of Image Quality Assessment (IQA) models has long been dominated by global correlation metrics, such as Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank-Order Correlation Coefficient (SRCC). While widely adopted, these metrics reduce performance to a single scalar, failing to capture how ranking consistency varies across the local quality spectrum. For example, two IQA models may achieve identical SRCC values, yet one ranks high-quality images (related to high Mean Opinion Score, MOS) more reliably, while the other better discriminates image pairs with small quality/MOS differences (related to $|Δ$MOS$|$). Such complementary behaviors are invisible under global metrics. Moreover, SRCC and PLCC are sensitive to test-sample quality distributions, yielding unstable comparisons across test sets. To address these limitations, we propose \textbf{Granularity-Modulated Correlation (GMC)}, which provides a structured, fine-grained analysis of IQA performance. GMC includes: (1) a \textbf{Granularity Modulator} that applies Gaussian-weighted correlations conditioned on absolute MOS values and pairwise MOS differences ($|Δ$MOS$|$) to examine local performance variations, and (2) a \textbf{Distribution Regulator} that regularizes correlations to mitigate biases from non-uniform quality distributions. The resulting \textbf{correlation surface} maps correlation values as a joint function of MOS and $|Δ$MOS$|$, providing a 3D representation of IQA performance. Experiments on standard benchmarks show that GMC reveals performance characteristics invisible to scalar metrics, offering a more informative and reliable paradigm for analyzing, comparing, and deploying IQA models. Codes are available at https://github.com/Dniaaa/GMC.

</details>


### [63] [Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation](https://arxiv.org/abs/2601.21751)
*Jiankun Peng,Jianyuan Guo,Ying Xu,Yue Liu,Jiashuang Yan,Xuanwei Ye,Houhua Li,Xiaoming Wang*

Main category: cs.CV

TL;DR: 本文提出DGNav框架，通过场景感知自适应策略和动态图Transformer解决视觉语言导航中拓扑地图粒度刚性问题，实现动态调节地图密度与连通性，提升导航精度、安全性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定几何阈值构建拓扑图的方法存在“粒度刚性”问题，无法适应环境复杂度变化，导致简单区域过采样（计算冗余）和高不确定性区域欠采样（碰撞风险高、精度低）。

Method: 提出DGNav框架：（1）场景感知自适应策略——根据预测航点分布动态调整图构建阈值，实现按需稠密化；（2）动态图Transformer——融合视觉、语言与几何线索生成动态边权重，重构图连通性以抑制拓扑噪声、增强指令遵循能力。

Result: 在R2R-CE和RxR-CE基准上显著提升导航性能与泛化能力；消融实验验证其在导航效率与安全探索间取得最优权衡。

Conclusion: 动态调节拓扑图的密度与连通性是提升VLN-CE任务鲁棒性与适应性的有效途径，DGNav为连续环境中基于语言的具身导航提供了更灵活、安全、精准的解决方案。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a "Granularity Rigidity" problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling "densification on demand" in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav.

</details>


### [64] [CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models](https://arxiv.org/abs/2601.21798)
*Junming Huang,Weiwei Xu*

Main category: cs.CV

TL;DR: 本文提出CG-MLLM，一种新型多模态大语言模型，支持3D描述与高分辨率3D生成，通过TokenAR与BlockAR双Transformer架构解耦建模需求，并结合预训练视觉语言骨干与专用3D VAE隐空间，显著提升3D生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D内容生成中仅能生成低分辨率网格或粗糙结构代理，无法原生捕捉细粒度几何，LLM在3D生成能力方面尚待探索。

Method: 提出CG-MLLM模型，采用混合Transformer架构：Token-level Autoregressive（TokenAR）Transformer处理token级内容，Block-level Autoregressive（BlockAR）Transformer处理block级内容；整合预训练视觉语言骨干与专用3D VAE隐空间，实现标准token与空间block的长上下文交互。

Result: CG-MLLM在高保真3D物体生成任务上显著优于现有MLLM，推动高分辨率3D内容生成融入主流大语言模型范式。

Conclusion: CG-MLLM成功将高质量3D生成能力集成到多模态大语言模型框架中，为3D内容创作提供了统一、高效的新范式。

Abstract: Large Language Models(LLMs) have revolutionized text generation and multimodal perception, but their capabilities in 3D content generation remain underexplored. Existing methods compromise by producing either low-resolution meshes or coarse structural proxies, failing to capture fine-grained geometry natively. In this paper, we propose CG-MLLM, a novel Multi-modal Large Language Model (MLLM) capable of 3D captioning and high-resolution 3D generation in a single framework. Leveraging the Mixture-of-Transformer architecture, CG-MLLM decouples disparate modeling needs, where the Token-level Autoregressive (TokenAR) Transformer handles token-level content, and the Block-level Autoregressive (BlockAR) Transformer handles block-level content. By integrating a pre-trained vision-language backbone with a specialized 3D VAE latent space, CG-MLLM facilitates long-context interactions between standard tokens and spatial blocks within a single integrated architecture. Experimental results show that CG-MLLM significantly outperforms existing MLLMs in generating high-fidelity 3D objects, effectively bringing high-resolution 3D content creation into the mainstream LLM paradigm.

</details>


### [65] [MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods](https://arxiv.org/abs/2601.21821)
*Honglin Lin,Zheng Liu,Yun Zhu,Chonghan Qin,Juekai Lin,Xiaoran Shang,Conghui He,Wentao Zhang,Lijun Wu*

Main category: cs.CV

TL;DR: 本文提出了MMFineReason，一个大规模多模态推理数据集（1.8M样本，5.1B tokens），通过三阶段流程构建，显著提升开源VLM在视觉推理任务上的性能，并发现‘少即是多’现象：仅7%高质量样本即可达到全量数据效果。


<details>
  <summary>Details</summary>
Motivation: 开源视觉语言模型（VLMs）在视觉推理上仍落后于闭源系统，主因是缺乏覆盖STEM图表、视觉谜题等挑战性领域且具备一致长程思维链（CoT）标注的高质量推理数据。

Method: 提出三阶段构建流程：(1) 大规模数据收集与标准化；(2) 基于Qwen3-VL-235B-A22B-Thinking蒸馏生成CoT推理链；(3) 基于推理质量与难度感知的综合筛选；并在该数据集上微调Qwen3-VL-Instruct得到MMFineReason-2B/4B/8B系列模型。

Result: MMFineReason-4B超越Qwen3-VL-8B-Thinking，MMFineReason-8B超越Qwen3-VL-30B-A3B-Thinking并逼近Qwen3-VL-32B-Thinking；仅用7%（123K）高难度样本即达全量性能；推理数据构成同时增强通用能力。

Conclusion: 高质量、难度感知的多模态推理数据是提升开源VLM推理能力的关键；MMFineReason验证了数据质量优于数量，并揭示推理导向数据对模型通用能力的协同增益效应。

Abstract: Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a "less is more" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.

</details>


### [66] [Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents](https://arxiv.org/abs/2601.21857)
*Taewon Kang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的文档背景生成框架，通过在潜在空间中设计噪声初始化和几何对齐，实现前景保留与多页风格一致性，无需额外约束或掩码机制。


<details>
  <summary>Details</summary>
Motivation: 解决现有文档背景生成方法中前景内容易被破坏、多页间风格不一致（风格漂移）等问题，避免依赖显式约束、掩码或重复提示控制。

Method: 将扩散过程重新解释为潜在空间中随机轨迹的演化；通过调控初始噪声分布及其几何对齐，使轨迹自然避开前景区域；解耦风格控制与文本条件，引入缓存的风格方向向量作为潜在空间中的持久约束，将扩散轨迹限制在共享风格子空间内。

Result: 实现了无需训练、兼容现有扩散主干网络的背景生成；在复杂文档上生成视觉连贯、前景可读、多页风格一致的结果；消除了对重复提示或辅助机制的依赖。

Conclusion: 该方法从轨迹设计视角重构扩散建模，为结构化、一致性生成任务提供了原理性新范式。

Abstract: We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints. Instead of suppressing diffusion updates or applying masking heuristics, our approach reinterprets diffusion as the evolution of stochastic trajectories through a structured latent space. By shaping the initial noise and its geometric alignment, background generation naturally avoids designated foreground regions, allowing readable content to remain intact without auxiliary mechanisms. To address the long-standing issue of stylistic drift across pages, we decouple style control from text conditioning and introduce cached style directions as persistent vectors in latent space. Once selected, these directions constrain diffusion trajectories to a shared stylistic subspace, ensuring consistent appearance across pages and editing iterations. This formulation eliminates the need for repeated prompt-based style specification and provides a more stable foundation for multi-page generation. Our framework admits a geometric and physical interpretation, where diffusion paths evolve on a latent manifold shaped by preferred directions, and foreground regions are rarely traversed as a consequence of trajectory initialization rather than explicit exclusion. The proposed method is training-free, compatible with existing diffusion backbones, and produces visually coherent, foreground-preserving results across complex documents. By reframing diffusion as trajectory design in latent space, we offer a principled approach to consistent and structured generative modeling.

</details>


### [67] [Improving Classifier-Free Guidance of Flow Matching via Manifold Projection](https://arxiv.org/abs/2601.21892)
*Jian-Feng Cai,Haixia Liu,Zhengyi Su,Chao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于优化视角的classifier-free guidance（CFG）新解释，并通过同伦优化与流形约束重构CFG采样过程，引入增量梯度下降和Anderson加速，在不增加模型评估开销的前提下提升生成质量、提示对齐性和对引导尺度的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管classifier-free guidance（CFG）在扩散与流匹配模型中广泛应用，但其依赖的经验性线性外推对引导尺度敏感，缺乏理论依据。

Method: 将CFG建模为流匹配中光滑距离函数梯度的近似，揭示预测差（conditional与unconditional输出之差）决定引导敏感性；进而将CFG采样重构为带流形约束的同伦优化问题，并采用增量梯度下降实现流形投影，结合Anderson加速提升效率与稳定性。

Result: 所提方法无需重新训练，在DiT-XL-2-256、Flux和Stable Diffusion 3.5等大模型上显著提升了生成保真度、提示对齐性及对引导尺度的鲁棒性。

Conclusion: CFG可被理解为一种隐式梯度近似，其性能瓶颈源于对真实梯度的粗略线性建模；本文提出的优化驱动框架为可控生成提供了更稳健、高效且原理清晰的替代方案。

Abstract: Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.

</details>


### [68] [Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion](https://arxiv.org/abs/2601.21896)
*Hanmo Chen,Chenghao Xu,Xu Yang,Xuan Chen,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的KV缓存策略PaFu-KV，通过轻量级显著性估计头评估token重要性，动态保留关键时空信息、剔除冗余缓存，在保证视频生成质量的同时提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频生成方法依赖启发式KV缓存策略，忽视token在长视频生成中的重要性差异，导致关键时空信息丢失和缓存冗余，影响生成质量与效率。

Method: 提出Past- and Future-Informed KV Cache Policy（PaFu-KV），引入基于双向教师模型蒸馏的轻量级显著性估计头，动态评估并筛选token以优化KV缓存。

Result: 在多个基准上验证了PaFu-KV在保持高保真视频生成质量的同时，显著加速推理、降低内存占用，提升长时序视频生成的效率。

Conclusion: PaFu-KV实现了更优的质量-效率权衡，为高效长时程视频生成提供了新思路。

Abstract: Video generation is pivotal to digital media creation, and recent advances in autoregressive video generation have markedly enhanced the efficiency of real-time video synthesis. However, existing approaches generally rely on heuristic KV Cache policies, which ignore differences in token importance in long-term video generation. This leads to the loss of critical spatiotemporal information and the accumulation of redundant, invalid cache, thereby degrading video generation quality and efficiency. To address this limitation, we first observe that token contributions to video generation are highly time-heterogeneous and accordingly propose a novel Past- and Future-Informed KV Cache Policy (PaFu-KV). Specifically, PaFu-KV introduces a lightweight Salience Estimation Head distilled from a bidirectional teacher to estimate salience scores, allowing the KV cache to retain informative tokens while discarding less relevant ones. This policy yields a better quality-efficiency trade-off by shrinking KV cache capacity and reducing memory footprint at inference time. Extensive experiments on benchmarks demonstrate that our method preserves high-fidelity video generation quality while enables accelerated inference, thereby enabling more efficient long-horizon video generation. Our code will be released upon paper acceptance.

</details>


### [69] [TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention](https://arxiv.org/abs/2601.21900)
*Chuancheng Shi,Shangze Li,Wenjun Lu,Wenhua Wu,Cong Wang,Zifeng Cheng,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出TraceRouter框架，通过追踪并切断非法语义的因果传播回路来增强大模型的对抗鲁棒性，避免传统局部抑制方法对模型效用的损害。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法基于‘局部性假设’，仅抑制孤立神经元或特征，但有害语义实为跨层分布式电路，导致这些方法脆弱且损害模型性能。

Method: TraceRouter分三阶段：(1) 通过注意力发散分析定位敏感起始层；(2) 利用稀疏自编码器（SAEs）和差分激活分析解耦并隔离恶意特征；(3) 基于零干预计算特征影响分数（FIS），映射其下游因果路径，并选择性抑制这些因果链。

Result: 实验表明，TraceRouter在对抗鲁棒性与通用性能之间取得更优权衡，显著优于当前最优基线方法。

Conclusion: TraceRouter通过路径级干预而非局部抑制，实现了对有害语义传播的物理性阻断，兼顾安全性与模型效用，为LFM安全防护提供了新范式。

Abstract: Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the "locality hypothesis", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.

</details>


### [70] [Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning](https://arxiv.org/abs/2601.21904)
*Hanmo Chen,Guangtao Lyu,Chenghao Xu,Jiexi Yan,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种金字塔式的Shapley-Taylor（PST）学习框架，用于细粒度的运动-语言检索，通过逐级对齐关节、运动片段与文本token，显著提升跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用全局对齐策略，忽略了局部运动片段、身体关节点与文本token之间的细粒度交互，导致检索性能受限。

Method: 受人类运动感知金字塔过程启发，提出Pyramidal Shapley-Taylor（PST）框架，将人体运动分解为时间片段和空间关节点，并以金字塔方式逐步进行关节级和片段级的跨模态对齐。

Result: 在多个公开基准数据集上显著超越现有最先进方法，实现了运动片段、关节点与对应文本token的精确对齐。

Conclusion: PST框架有效建模了运动-语言间的细粒度与层次化语义关系，为人类中心的跨模态智能提供了新思路。

Abstract: As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.

</details>


### [71] [VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models](https://arxiv.org/abs/2601.21915)
*Yunhao Li,Sijing Wu,Zhilin Gao,Zicheng Zhang,Qi Jia,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了VideoAesBench，一个用于评估大多媒体模型（LMMs）视频美学质量理解能力的综合性基准，涵盖多样化视频内容、多种问题形式及多维度美学指标，并对23个开源与商用LMM进行了评测，发现当前模型在该任务上仍表现有限。


<details>
  <summary>Details</summary>
Motivation: 视频美学质量评估是人类基本能力，但目前大多媒体模型（LMMs）在此方面的能力尚未被系统探索和评估。

Method: 构建了VideoAesBench基准，包含1804个来自UGC、AIGC、压缩、RGC和游戏等多源视频，支持单选、多选、判断和新颖的开放描述题型，并覆盖视觉形式、风格与情感性共12个美学维度；基于该基准评测23个开源与商用LMM。

Result: 当前LMMs仅具备基础的视频美学感知能力，整体性能不完整且不精确。

Conclusion: VideoAesBench可作为强健的测试平台，推动可解释的视频美学评估研究，并为LMM能力提升提供新方向。

Abstract: Large multimodal models (LMMs) have demonstrated outstanding capabilities in various visual perception tasks, which has in turn made the evaluation of LMMs significant. However, the capability of video aesthetic quality assessment, which is a fundamental ability for human, remains underexplored for LMMs. To address this, we introduce VideoAesBench, a comprehensive benchmark for evaluating LMMs' understanding of video aesthetic quality. VideoAesBench has several significant characteristics: (1) Diverse content including 1,804 videos from multiple video sources including user-generated (UGC), AI-generated (AIGC), compressed, robotic-generated (RGC), and game videos. (2) Multiple question formats containing traditional single-choice questions, multi-choice questions, True or False questions, and a novel open-ended questions for video aesthetics description. (3) Holistic video aesthetics dimensions including visual form related questions from 5 aspects, visual style related questions from 4 aspects, and visual affectiveness questions from 3 aspects. Based on VideoAesBench, we benchmark 23 open-source and commercial large multimodal models. Our findings show that current LMMs only contain basic video aesthetics perception ability, their performance remains incomplete and imprecise. We hope our VideoAesBench can be served as a strong testbed and offer insights for explainable video aesthetics assessment.

</details>


### [72] [Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models](https://arxiv.org/abs/2601.21922)
*Cong Cao,Huanjing Yue,Shangbin Xie,Xin Liu,Jingyu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的框架，利用视频扩散模型辅助图像扩散模型，提升零样本视频修复与增强的时间一致性，通过多种潜在特征融合策略和时序强化后处理实现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在零样本图像修复中表现优异，但直接应用于视频会导致严重的时间闪烁问题，亟需提升时间一致性。

Method: 提出同源潜在融合、异构潜在融合及基于COT的融合比例策略，结合图像到视频扩散模型进行时序强化后处理，整个框架无需训练。

Result: 实验结果表明该方法在零样本视频修复与增强任务中显著优于现有方法，有效缓解时间闪烁并提升时间一致性。

Conclusion: 该方法是首个利用视频扩散模型辅助图像方法实现零样本视频修复与增强的框架，具有通用性、无需训练且效果优越。

Abstract: Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.

</details>


### [73] [Just Noticeable Difference Modeling for Deep Visual Features](https://arxiv.org/abs/2601.21933)
*Rui Zhao,Wenrui Li,Lin Zhu,Yajing Zheng,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出FeatJND，一种面向任务的深度视觉特征的最小可觉差（JND）建模方法，用于预测在保持下游任务性能前提下各特征维度所能容忍的最大扰动，并验证其在分类、检测与实例分割任务中的有效性及在动态量化中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 深度视觉特征日益成为视觉系统的接口，亟需刻画其特性并控制其质量；将人类/机器视觉中的JND概念拓展至特征空间，可提供任务对齐的容错边界，指导资源受限下的特征质量控制。

Method: 提出FeatJND——一种任务对齐的JND建模方法，可预测保持下游任务性能前提下的逐特征最大可容忍扰动图；设计标准化分点处的FeatJND估计器，并在图像分类、目标检测和实例分割任务上进行验证；进一步将其应用于token级动态量化，实现基于FeatJND的步长自适应分配。

Result: 在相同失真强度下，FeatJND引导的失真比高斯噪声扰动更能维持下游任务性能；归因可视化显示其能抑制非关键特征区域；在动态量化中，FeatJND指导的步长分配显著优于随机排列和全局统一步长。

Conclusion: FeatJND为深度特征提供了可解释、任务对齐的质量控制依据，是连接特征表示、鲁棒性建模与高效部署的重要桥梁。

Abstract: Deep visual features are increasingly used as the interface in vision systems, motivating the need to describe feature characteristics and control feature quality for machine perception. Just noticeable difference (JND) characterizes the maximum imperceptible distortion for images under human or machine vision. Extending it to deep visual features naturally meets the above demand by providing a task-aligned tolerance boundary in feature space, offering a practical reference for controlling feature quality under constrained resources. We propose FeatJND, a task-aligned JND formulation that predicts the maximum tolerable per-feature perturbation map while preserving downstream task performance. We propose a FeatJND estimator at standardized split points and validate it across image classification, detection, and instance segmentation. Under matched distortion strength, FeatJND-based distortions consistently preserve higher task performance than unstructured Gaussian perturbations, and attribution visualizations suggest FeatJND can suppress non-critical feature regions. As an application, we further apply FeatJND to token-wise dynamic quantization and show that FeatJND-guided step-size allocation yields clear gains over random step-size permutation and global uniform step size under the same noise budget. Our code will be released after publication.

</details>


### [74] [BookNet: Book Image Rectification via Cross-Page Attention Network](https://arxiv.org/abs/2601.21938)
*Shaokai Liu,Hao Feng,Bozhi Luan,Min Hou,Jiajun Deng,Wengang Zhou*

Main category: cs.CV

TL;DR: 本文提出了BookNet，首个专为双页图书图像校正设计的端到端深度学习框架，通过双分支结构与跨页注意力机制建模左右页间的几何耦合关系，并构建了合成数据集Book3D和真实基准Book100，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单页文档图像校正方法无法捕捉书籍中相邻页面间因装订约束导致的复杂且不对称的几何耦合关系。

Method: 提出BookNet，采用双分支架构与跨页注意力机制，联合估计左右页及整页展开图的形变场；构建大规模合成数据集Book3D和真实世界基准Book100。

Result: BookNet在图书图像校正任务上显著优于现有最先进方法。

Conclusion: BookNet首次实现了对双页书籍图像的端到端协同校正，验证了建模跨页几何依赖的有效性，并推动了该领域数据与方法的发展。

Abstract: Book image rectification presents unique challenges in document image processing due to complex geometric distortions from binding constraints, where left and right pages exhibit distinctly asymmetric curvature patterns. However, existing single-page document image rectification methods fail to capture the coupled geometric relationships between adjacent pages in books. In this work, we introduce BookNet, the first end-to-end deep learning framework specifically designed for dual-page book image rectification. BookNet adopts a dual-branch architecture with cross-page attention mechanisms, enabling it to estimate warping flows for both individual pages and the complete book spread, explicitly modeling how left and right pages influence each other. Moreover, to address the absence of specialized datasets, we present Book3D, a large-scale synthetic dataset for training, and Book100, a comprehensive real-world benchmark for evaluation. Extensive experiments demonstrate that BookNet outperforms existing state-of-the-art methods on book image rectification. Code and dataset will be made publicly available.

</details>


### [75] [Deep Models, Shallow Alignment: Uncovering the Granularity Mismatch in Neural Decoding](https://arxiv.org/abs/2601.21948)
*Yang Du,Siyuan Dai,Yonghao Song,Paul M. Thompson,Haoteng Tang,Liang Zhan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Shallow Alignment的对比学习策略，通过将神经信号与视觉编码器中间层表征对齐，而非最终输出，以解决人眼视觉与机器视觉在粒度上的不匹配问题，显著提升了神经视觉解码性能，并揭示了其随预训练视觉骨干网络容量可预测提升的规律。


<details>
  <summary>Details</summary>
Motivation: 现有神经视觉解码方法忽略了人类视觉与机器视觉之间的根本性粒度不匹配：深度视觉模型为强调语义不变性而抑制局部纹理信息，而神经信号却同时保留了低级视觉属性和高级语义内容。

Method: 提出Shallow Alignment——一种新颖的对比学习策略，将神经信号与视觉编码器的中间层表征（而非最终输出）进行对齐，从而更好平衡低级纹理细节与高级语义特征。

Result: 在多个基准上实验表明，Shallow Alignment显著优于标准的最终层对齐方法，性能提升达22%–58%；并首次有效揭示并利用了神经视觉解码中的模型缩放律。

Conclusion: Shallow Alignment通过中间层对齐缓解了人机视觉表征粒度失配问题，不仅大幅提升解码性能，还使解码能力随视觉骨干网络容量可预测增长，为脑机接口中视觉解码提供了新范式。

Abstract: Neural visual decoding is a central problem in brain computer interface research, aiming to reconstruct human visual perception and to elucidate the structure of neural representations. However, existing approaches overlook a fundamental granularity mismatch between human and machine vision, where deep vision models emphasize semantic invariance by suppressing local texture information, whereas neural signals preserve an intricate mixture of low-level visual attributes and high-level semantic content. To address this mismatch, we propose Shallow Alignment, a novel contrastive learning strategy that aligns neural signals with intermediate representations of visual encoders rather than their final outputs, thereby striking a better balance between low-level texture details and high-level semantic features. Extensive experiments across multiple benchmarks demonstrate that Shallow Alignment significantly outperforms standard final-layer alignment, with performance gains ranging from 22% to 58% across diverse vision backbones. Notably, our approach effectively unlocks the scaling law in neural visual decoding, enabling decoding performance to scale predictably with the capacity of pre-trained vision backbones. We further conduct systematic empirical analyses to shed light on the mechanisms underlying the observed performance gains.

</details>


### [76] [PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing](https://arxiv.org/abs/2601.21957)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL-1.5 是一个0.9B参数量的超紧凑视觉语言模型，在 OmniDocBench v1.5 上达到94.5% SOTA精度，并在新提出的 Real5-OmniDocBench 基准上验证了对真实物理畸变的强鲁棒性，同时扩展支持印章识别与文本定位任务。


<details>
  <summary>Details</summary>
Motivation: 提升文档理解模型在真实场景中面对扫描、倾斜、扭曲、屏幕拍摄和光照变化等物理畸变时的鲁棒性，并拓展多任务能力（如印章识别、文本定位）

Method: 升级 PaddleOCR-VL 模型为 1.5 版本；构建包含五类真实物理畸变的 Real5-OmniDocBench 新基准；集成密封识别与文本定位模块，保持模型轻量化（0.9B 参数）

Result: 在 OmniDocBench v1.5 上达 94.5% SOTA 准确率；在 Real5-OmniDocBench 上表现最优；支持新增任务且维持高效率

Conclusion: PaddleOCR-VL-1.5 在精度、鲁棒性、任务泛化性和模型效率方面取得综合突破，推动实用化文档理解发展

Abstract: We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR

</details>


### [77] [Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving](https://arxiv.org/abs/2601.22032)
*Linhan Wang,Zichong Yang,Chen Bai,Guoxiang Zhang,Xiaotong Liu,Xiaoyin Zheng,Xiao-Xiao Long,Chang-Tien Lu,Cheng Lu*

Main category: cs.CV

TL;DR: 本文提出Drive-JEPA框架，结合视频联合嵌入预测架构（V-JEPA）与多模态轨迹蒸馏，提升端到端自动驾驶的规划表征能力，在NAVSIM上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于自监督视频预训练的端到端自动驾驶方法受限于驾驶场景单一人类轨迹导致的行为模态模糊性，难以学习多模态决策。

Method: 1) 将V-JEPA适配至端到端驾驶，用大规模驾驶视频预训练ViT编码器以生成与轨迹规划对齐的预测表征；2) 设计proposal-centric规划器，融合仿真生成与人类轨迹进行多模态蒸馏，并引入动量感知选择机制保障行为稳定性与安全性。

Result: 在NAVSIM上，V-JEPA表征+简单Transformer解码器在无感知设置下超越先前方法3 PDMS；完整Drive-JEPA框架在v1和v2分别达93.3 PDMS和87.8 EPDMS，创SOTA。

Conclusion: Drive-JEPA通过联合建模视频预测与多模态轨迹蒸馏，有效缓解了驾驶行为模态模糊问题，显著提升了端到端规划性能。

Abstract: End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.

</details>


### [78] [Understanding Multimodal Complementarity for Single-Frame Action Anticipation](https://arxiv.org/abs/2601.22039)
*Manuel Benavent-Lledo,Konstantinos Bacharidis,Konstantinos Papoutsakis,Antonis Argyros,Jose Garcia-Rodriguez*

Main category: cs.CV

TL;DR: 本文挑战了人类动作预测需要密集时间信息的假设，探索仅基于单帧图像进行动作预测的可能性，并提出了AAG+框架，在多个基准测试中表现优于或媲美现有视频级方法。


<details>
  <summary>Details</summary>
Motivation: 挑战人类动作预测必须依赖密集时间信息的传统假设，探究单帧图像中蕴含的未来动作信息及其有效利用方式。

Method: 在前期AAG工作基础上，系统研究RGB外观、深度几何线索及过往动作语义表示等多源信息对单帧动作预测的贡献，并分析不同多模态融合策略、关键帧选择策略和过往动作历史来源的影响，最终构建AAG+框架。

Result: AAG+仅使用单帧即在IKEA-ASM、Meccano和Assembly101等具有挑战性的预测基准上，性能达到甚至超越当前最优视频级方法。

Conclusion: 单帧动作预测具有巨大潜力，其性能可媲美视频级方法；密集时间建模并非总是必要，精心选取的关键帧有时已足够。

Abstract: Human action anticipation is commonly treated as a video understanding problem, implicitly assuming that dense temporal information is required to reason about future actions. In this work, we challenge this assumption by investigating what can be achieved when action anticipation is constrained to a single visual observation. We ask a fundamental question: how much information about the future is already encoded in a single frame, and how can it be effectively exploited? Building on our prior work on Action Anticipation at a Glimpse (AAG), we conduct a systematic investigation of single-frame action anticipation enriched with complementary sources of information. We analyze the contribution of RGB appearance, depth-based geometric cues, and semantic representations of past actions, and investigate how different multimodal fusion strategies, keyframe selection policies and past-action history sources influence anticipation performance. Guided by these findings, we consolidate the most effective design choices into AAG+, a refined single-frame anticipation framework. Despite operating on a single frame, AAG+ consistently improves upon the original AAG and achieves performance comparable to, or exceeding, that of state-of-the-art video-based methods on challenging anticipation benchmarks including IKEA-ASM, Meccano and Assembly101. Our results offer new insights into the limits and potential of single-frame action anticipation, and clarify when dense temporal modeling is necessary and when a carefully selected glimpse is sufficient.

</details>


### [79] [Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion](https://arxiv.org/abs/2601.22045)
*Da Li,Chen Yao,Tong Mao,Jiacheng Bao,Houjun Sun*

Main category: cs.CV

TL;DR: 本文提出首个融合3D SAR点云与航拍影像的城市神经表面重建（NSR）框架，以解决稀疏视角下几何模糊与不稳定问题；通过将雷达空间约束嵌入SDF-based NSR主干网络，提升重建精度、完整性与鲁棒性，并构建首个配准的光学-SAR跨模态基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有神经表面重建方法在稀疏航拍视角下存在几何歧义与不稳定性，而城市遥感中图像采集常受限于飞行路径、地形和成本，亟需更鲁棒的多模态重建方案。

Method: 提出融合3D合成孔径雷达（SAR）点云与航拍影像的NSR框架，将雷达导出的空间约束嵌入SDF-based重建主干，实现结构感知的光线选择与自适应采样；并构建首个共注册的3D SAR-航拍影像基准数据集。

Result: 实验表明，在高度稀疏与倾斜视角条件下，引入3D SAR显著提升重建精度、完整性与鲁棒性，优于单模态基线方法。

Conclusion: 融合光学与SAR传感的跨模态NSR为可扩展、高保真城市三维重建提供了可行路径，推动空天遥感协同建模发展。

Abstract: Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.

</details>


### [80] [PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22046)
*Changjian Jiang,Kerui Ren,Xudong Li,Kaiwen Song,Linning Xu,Tao Lu,Junting Dong,Yu Zhang,Bo Dai,Mulin Yu*

Main category: cs.CV

TL;DR: PLANING是一种高效的单目图像序列流式三维重建框架，通过显式几何原语与神经高斯的松耦合混合表示，实现几何与外观的解耦建模和在线优化，在重建质量、几何精度和速度上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目流式重建方法难以兼顾高质量渲染与精确几何重建，存在质量与效率权衡问题。

Method: 提出PLANING框架，采用显式几何原语与神经高斯松耦合的混合表示，实现几何与外观的解耦建模，并设计分离的在线初始化与优化策略。

Result: 在ScanNetV2上Chamfer-L2提升18.52%（vs PGSR），PSNR提升1.31 dB（vs ARTDECO），重建耗时<100秒（比2D Gaussian Splatting快5倍以上），质量媲美离线优化。

Conclusion: PLANING在保持高质量重建的同时显著提升几何精度与计算效率，适用于大规模场景建模与具身AI等下游任务。

Abstract: Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .

</details>


### [81] [MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources](https://arxiv.org/abs/2601.22054)
*Baorui Ma,Jiahui Yang,Donglin Di,Xuancheng Zhang,Jianxun Cui,Hao Li,Yan Xie,Wei Chen*

Main category: cs.CV

TL;DR: 本文提出了Metric Anything，一种简单且可扩展的预训练框架，用于从噪声大、来源多样的3D数据中学习度量深度，无需人工设计提示、相机特异性建模或任务专用架构；核心是稀疏度量提示（Sparse Metric Prompt），通过随机遮蔽深度图实现空间推理与传感器/相机偏差解耦；在约2000万图像-深度对上预训练，首次在度量深度任务中观察到清晰的缩放规律；模型在多种深度相关任务上达到SOTA，并显著提升多模态大语言模型的空间智能能力。


<details>
  <summary>Details</summary>
Motivation: 现有度量深度估计面临传感器噪声异质性、相机依赖性偏差及跨源3D数据中的度量模糊性等挑战，难以沿用视觉基础模型的成功缩放范式。

Method: 提出Metric Anything预训练框架，核心为稀疏度量提示（随机遮蔽深度图），作为统一接口解耦空间推理与硬件偏差；使用涵盖重建、采集和渲染的10000种相机型号、约2000万图像-深度对进行大规模预训练；支持prompt驱动任务与prompt-free蒸馏学生模型。

Result: 首次在度量深度任务中验证明确缩放趋势；prompt驱动任务（如深度补全、超分、雷达-相机融合）表现优异；prompt-free蒸馏模型在单目深度估计、内参恢复、单/多视图度量3D重建及VLA规划上达SOTA；以Metric Anything预训练ViT作视觉编码器，显著提升多模态大语言模型空间智能。

Conclusion: 度量深度估计可遵循与现代基础模型相同的缩放定律，Metric Anything为可扩展、高效的现实世界度量感知提供了新路径。

Abstract: Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.

</details>


### [82] [Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models](https://arxiv.org/abs/2601.22057)
*Archer Wang,Emile Anand,Yilun Du,Marin Soljačić*

Main category: cs.CV

TL;DR: 本文提出一种基于对抗训练的扩散模型方法，用于无监督学习因子化潜在表示，并通过跨源因子重组生成新样本，提升因子发现质量与组合生成一致性。


<details>
  <summary>Details</summary>
Motivation: 分解复杂数据为因子化表征可揭示可复用组件并支持组合式生成；但现有扩散模型在无因子级监督下难以保证因子发现质量与重组一致性。

Method: 引入对抗训练信号：设计判别器区分单源样本与跨源因子重组样本，优化生成器以欺骗判别器，从而增强物理与语义一致性。

Result: 在CelebA-HQ、Virtual KITTI、CLEVR和Falcor3D上FID更低、MIG与MCC指标更优；并在LIBERO机器人视频轨迹任务中显著提升状态空间覆盖率。

Conclusion: 该方法有效提升无监督因子发现与组合生成质量，拓展了扩散模型在图像与机器人视频等多模态任务中的应用能力。

Abstract: Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.

</details>


### [83] [Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models](https://arxiv.org/abs/2601.22060)
*Wenxuan Huang,Yu Zeng,Qiuchen Wang,Zhen Fang,Shaosheng Cao,Zheng Chu,Qingyu Yin,Shuang Chen,Zhenfei Yin,Lin Chen,Zehui Chen,Yao Hu,Philip Torr,Feng Zhao,Wanli Ouyang*

Main category: cs.CV

TL;DR: 本文提出Vision-DeepResearch，一种支持多轮、多实体、多尺度视觉与文本搜索的新型多模态深度研究范式，通过冷启动监督与强化学习将深度研究能力内化至MLLM中，在噪声大的真实场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理需大量事实信息的复杂问题时受限于内部世界知识容量，且其‘推理-调用工具’方式在真实高噪声场景下因搜索设置过于简单（单图/单文本查询）、推理深度和搜索广度不足而效果受限。

Method: 提出Vision-DeepResearch范式，支持多轮、多实体、多尺度的视觉与文本联合搜索；通过冷启动监督训练和强化学习（RL）将深度研究能力（数十步推理、数百次搜索引擎交互）内化到MLLM中。

Result: 在多模态深度研究任务上显著超越现有方法，并优于基于GPT-5、Gemini-2.5-pro和Claude-4-Sonnet等闭源大模型构建的工作流。

Conclusion: Vision-DeepResearch实现了更鲁棒、更深入的多模态信息检索与推理能力，为MLLM在高噪声现实场景中的事实密集型任务提供了新范式。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [84] [BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation](https://arxiv.org/abs/2601.22061)
*Li Zhang,Pengtao Xie*

Main category: cs.CV

TL;DR: 本文提出BLO-Inst框架，通过双层优化解决目标检测器与SAM模型间的目标不匹配和对齐过拟合问题，使检测器生成更适配SAM分割任务的提示框，从而实现全自动、高质量的零样本图像分割。


<details>
  <summary>Details</summary>
Motivation: SAM虽具零样本分割能力，但依赖人工提示；用检测器自动生成提示存在目标不匹配（检测重定位、SAM重分割）和对齐过拟合问题。

Method: 提出BLO-Inst：下层在数据子集D1上微调SAM以提升对当前检测框的分割精度；上层在独立子集D2上更新检测器，使其生成的框能最小化微调后SAM的验证损失，实现分割感知的提示生成。

Result: 在通用与生物医学图像分割任务中均显著优于现有基线方法。

Conclusion: 双层优化可有效解耦并协同优化检测与分割目标，使检测器成为通用、鲁棒的SAM提示生成器，推动全自动零样本分割落地。

Abstract: The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.

</details>


### [85] [RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation](https://arxiv.org/abs/2601.22094)
*Hanzhuo Huang,Qingyang Bao,Zekai Gu,Zhongshuo Du,Cheng Lin,Yuan Liu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D资产参考的扩散模型，通过双分支感知结构联合建模多视角RGB图像和点云图，实现生成图像与3D参考的高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有基于单张图像参考的扩散生成方法无法利用3D资产，限制了其在实际应用中的通用性。

Method: 构建跨域双分支扩散模型，分别处理多视角RGB图像和点云图，实现颜色与规范空间坐标的联合建模，并采用空间对齐与域解耦机制同步生成RGB图像和点云图。

Result: 实验表明该方法能有效利用3D资产作为参考，生成与之高度一致的图像，提升了2D生成与3D内容创作的结合能力。

Conclusion: 本工作为扩散模型与3D内容生成的融合提供了新思路，拓展了参考式图像生成的应用边界。

Abstract: In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.

</details>


### [86] [SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence](https://arxiv.org/abs/2601.22114)
*Saoud Aldowaish,Yashwanth Karumanchi,Kai-Chen Chiang,Soroosh Noorzad,Morteza Fayazi*

Main category: cs.CV

TL;DR: 本文提出了SINA，一个开源、全自动的电路原理图图像到网表生成器，结合深度学习、连通分量标记、OCR和视觉-语言模型，在网表生成准确率上达到96.47%，是当前最优方法的2.72倍。


<details>
  <summary>Details</summary>
Motivation: 现有将电路原理图图像转换为机器可读网表的方法在元件识别和连接关系推断方面存在困难。

Method: SINA整合了深度学习用于元件检测、连通分量标记（CCL）用于精确提取连接关系、光学字符识别（OCR）用于获取元件参考标识符，并采用视觉-语言模型（VLM）进行可靠的参考标识符分配。

Result: 实验表明，SINA实现了96.47%的整体网表生成准确率，比当前最先进方法高出2.72倍。

Conclusion: SINA是一个高效、准确且开源的全自动电路原理图图像到网表转换系统，显著提升了该任务的性能。

Abstract: Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.

</details>


### [87] [Creative Image Generation with Diffusion Model](https://arxiv.org/abs/2601.22125)
*Kunpeng Song,Ahmed Elgammal*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的创意图像生成新框架，通过在CLIP嵌入空间中导向低概率区域来提升图像的新颖性和创造性，同时借助‘pullback’机制保持视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动概念融合或子类别排除，缺乏对‘创造力’的量化定义和系统性建模；本文旨在为生成式模型中的创造力提供一个可计算、有原则的定义（即图像在CLIP嵌入空间中的逆概率），并实现高创意与高保真度的统一。

Method: 将创造力形式化为图像在CLIP嵌入空间中的逆概率；在扩散采样过程中引导生成分布向低概率区域偏移；引入pullback机制以约束偏离程度，保障视觉质量。

Result: 在多个文本到图像扩散模型上验证了该方法的有效性：生成图像更具新颖性、想象力和视觉吸引力，且未显著牺牲清晰度与语义一致性。

Conclusion: 本工作为生成模型中的创造力提供了可量化的理论视角和实用框架，推动了视觉内容合成中创新性的可控生成研究。

Abstract: Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.

</details>


### [88] [Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography](https://arxiv.org/abs/2601.22134)
*Wenxuan Li,Pedro R. A. S. Bassi,Lizhou Wu,Xinze Zhou,Yuxuan Zhao,Qi Chen,Szymon Plotka,Tianyu Lin,Zheren Zhu,Marisa Martin,Justin Caskey,Shanshan Jiang,Xiaoxi Chen,Jaroslaw B. Ćwikla,Artur Sankowski,Yaping Wu,Sergio Decherchi,Andrea Cavalli,Chandana Lall,Cristian Tomasetti,Yaxing Guo,Xuan Yu,Yuqing Cai,Hualin Qiao,Jie Bao,Chenhan Hu,Ximing Wang,Arkadiusz Sitek,Kai Ding,Heng Li,Meiyun Wang,Dexin Yu,Guang Zhang,Yang Yang,Kang Wang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为ePAI的AI系统，用于在CT扫描中早期检测胰腺导管腺癌（PDAC），在内部和外部测试中均表现出高AUC、敏感性和特异性，并能在临床诊断前数月发现被放射科医生遗漏的病灶。


<details>
  <summary>Details</summary>
Motivation: PDAC常在晚期才被发现，而回顾性分析显示专家放射科医生可在患者确诊前的CT中识别出先前被忽略的病灶，因此亟需自动化工具辅助早期检测。

Method: 开发了基于单中心1598例患者数据训练的深度学习系统ePAI，用于自动检测和精确定位PDAC病灶，并在内部（1009例）和外部多中心（7158例）数据集上进行验证；还开展了多读片者研究以对比ePAI与30名认证放射科医生的表现。

Result: ePAI在内部测试中AUC达0.939–0.999，敏感性95.3%，特异性98.7%，可定位小至2 mm病灶；外部测试中AUC为0.918–0.945，敏感性91.5%，特异性88.0%，可定位小至5 mm病灶；并在临床诊断前3–36个月的CT中检出75/159例PDAC，中位提前时间为347天；多读片者研究显示其敏感性比放射科医生高50.3%（P<0.05），特异性达95.4%。

Conclusion: ePAI是一种具有高准确性和临床实用潜力的AI辅助工具，有望显著提升PDAC的早期检出率。

Abstract: Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.

</details>


### [89] [PI-Light: Physics-Inspired Diffusion for Full-Image Relighting](https://arxiv.org/abs/2601.22135)
*Zhexin Liang,Zhaoxi Chen,Yongwei Chen,Tianyi Wei,Tengfei Wang,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种名为π-Light（PI-Light）的两阶段物理启发扩散模型，用于全图重光照，通过批感知注意力、物理引导神经渲染、物理启发损失及高质量数据集，显著提升了真实场景泛化能力与物理合理性。


<details>
  <summary>Details</summary>
Motivation: 解决全图重光照中配对数据难获取、物理合理性难保证、数据驱动先验泛化性差等挑战，并弥补合成到真实场景之间的鸿沟。

Method: 提出两阶段物理启发扩散框架π-Light，包含批感知注意力机制、物理引导神经渲染模块、物理启发损失函数，并构建了受控光照下多样物体与场景的数据集，支持预训练扩散模型高效微调。

Result: 在多种材质上成功合成高保真镜面高光与漫反射效果，在真实世界场景中展现出优于现有方法的泛化能力。

Conclusion: π-Light通过融合物理先验与扩散建模，有效提升了全图重光照的真实性、一致性与泛化性，为下游图像编辑任务提供了新基准与实用框架。

Abstract: Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $π$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.

</details>


### [90] [Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions](https://arxiv.org/abs/2601.22150)
*Xiaoxiao Sun,Mingyang Li,Kun yuan,Min Woo Sun,Mark Endo,Shengguang Wu,Changlin Li,Yuhui Zhang,Zeyu Wang,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 本文提出VI-Probe框架，通过可控视觉错觉实验揭示大视觉语言模型（VLMs）在面对错觉反转时响应僵化的原因并非单一，而是源于记忆覆盖、感知-记忆竞争或视觉处理限制等异质机制。


<details>
  <summary>Details</summary>
Motivation: 探究大视觉语言模型为何在视觉错觉反转后仍坚持原答案，区分其是真正感知视觉变化还是仅依赖语言记忆。

Method: 构建VI-Probe框架，含分级错觉扰动与匹配的无错觉视觉对照；引入Polarity-Flip Consistency、Template Fixation Index和归一化错觉倍增器等新指标，评估响应稳定性与敏感性。

Result: 实验证明不同模型响应僵化成因各异：GPT-5表现为记忆覆盖，Claude-Opus-4.1呈现感知-记忆竞争，Qwen系列则反映视觉处理瓶颈。

Conclusion: VLMs对视觉错觉的响应持久性由多种异质机制导致，需采用基于探针的评估范式，同时衡量知识能力与对受控视觉变化的敏感性。

Abstract: Large Vision-Language Models (VLMs) often answer classic visual illusions "correctly" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.

</details>


### [91] [UEval: A Benchmark for Unified Multimodal Generation](https://arxiv.org/abs/2601.22155)
*Bo Li,Yida Yin,Wenhao Chai,Xingyu Fu,Zhuang Liu*

Main category: cs.CV

TL;DR: UEval是一个评估统一模型（能同时生成图像和文本）的新基准，包含1000个专家设计的多任务、多推理类型问题，并采用人工精炼的细粒度评分标准（共10417条），发现当前模型表现有限，而推理能力对性能提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确评估开放式的多模态生成质量，尤其在图像与文本协同输出任务中；LLM-as-a-judge等自动评估方式易忽略细节，且缺乏系统性、可验证的评价标准。

Method: 构建UEval基准：1) 收集8类真实任务的1000个需图文双输出的问题；2) 用MLLM基于参考图文初建评分量规，再由人类专家校验完善，最终形成10417条细粒度、可扩展的自动评分准则。

Result: GPT-5-Thinking得分为66.4/100，最优开源模型仅49.1/100；引入推理机制显著提升性能，将推理轨迹迁移到非推理模型可明显缩小差距。

Conclusion: 统一模型在复杂图文联合生成任务上仍具挑战；推理能力是关键因素，未来研究应更重视跨模态推理建模与评估范式建设。

Abstract: We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.

</details>


### [92] [One-step Latent-free Image Generation with Pixel Mean Flows](https://arxiv.org/abs/2601.22158)
*Yiyang Lu,Susie Lu,Qiao Sun,Hanhong Zhao,Zhicheng Jiang,Xianbang Wang,Tianhong Li,Zhengyang Geng,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出了一种名为'pixel MeanFlow'（pMF）的新型生成模型，旨在实现无需潜在空间的一步式扩散/流模型。通过将网络输出空间与损失空间分离设计，pMF在ImageNet上实现了高质量的一步生成结果（256x256分辨率FID=2.22，512x512分辨率FID=2.48）。


<details>
  <summary>Details</summary>
Motivation: 推动扩散/流式生成模型向更高效、更简洁的方向发展，即实现无需潜在空间的一步式采样，填补当前研究中的关键空白。

Method: 提出pixel MeanFlow（pMF），核心思想是分离网络输出空间（预测图像流形上的x）和损失空间（基于MeanFlow定义在速度空间中），并引入图像流形与平均速度场之间的简单变换。

Result: pMF在ImageNet数据集上实现了256×256（FID=2.22）和512×512（FID=2.48）分辨率下的一步、无潜在空间图像生成，性能优异。

Conclusion: pMF为构建高效、简洁的一步式、无潜在空间扩散/流模型提供了新思路和关键技术路径，有望进一步拓展该类生成模型的能力边界。

Abstract: Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [93] [DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents](https://arxiv.org/abs/2601.20975)
*Nikita Gupta,Riju Chatterjee,Lukas Haas,Connie Tao,Andrew Wang,Chang Liu,Hidekazu Oiwa,Elena Gribovskaya,Jan Ackermann,John Blitzer,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: DeepSearchQA是一个包含900个提示的基准测试，用于评估智能体在17个不同领域中执行多步信息检索任务的能力，强调系统性信息整合、去重与实体解析、以及开放搜索空间中的停止判断能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注单答案检索或事实性评估，缺乏对复杂、多步、长程规划型信息搜索能力的系统评测；需构建能暴露当前智能体在深度研究中关键缺陷（如早停、过度泛化）的新基准。

Method: 构建DeepSearchQA：手动生成900个因果链式多步搜索任务，覆盖17个学科领域，所有任务基于公开网页、答案可客观验证；设计三类核心能力评测维度，并对主流智能体架构进行系统性实证评估。

Result: 最先进模型在高召回与高精度之间难以兼顾，暴露出早停（欠检索）和盲目扩搜（低置信度冗余答案）等典型失败模式。

Conclusion: DeepSearchQA揭示了当前智能体在深度信息搜索能力上的显著不足，为后续提升系统性、鲁棒性与自主终止能力的研究提供了关键诊断工具和方向指引。

Abstract: We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.

</details>


### [94] [asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation](https://arxiv.org/abs/2601.20992)
*Oleg Sedukhin,Andrey Kostin*

Main category: cs.CL

TL;DR: 本文提出了语音识别评估的多项改进，包括支持多参考标注和任意长度插入的字符串对齐算法、构建新型俄语长文本真实语音测试集 DiverseSpeech-Ru，并揭示模型易过拟合特定标注方式的问题，同时开发了流式识别评估与多转录对齐可视化工具。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别评估方法在处理非拉丁语系、构词丰富或长文本口语时存在标注不灵活、对齐不准、单参考局限等问题；且模型可能仅适配特定标注风格，导致指标虚高。

Method: 提出一种支持多参考标注、任意长度插入及更优词对齐的字符串对齐算法；构建并发布多参考标注的俄语长文本真实语音测试集 DiverseSpeech-Ru；对主流俄语数据集进行多参考重标注；分析微调过程中的标注适应现象；开发流式识别评估与多转录可视化对齐工具；提供统一接口封装多种离线/流式ASR模型。

Result: 验证了模型易学习数据集特有标注风格而非真正语音识别能力，造成WER等指标改善假象；所提对齐算法提升非拉丁语言长语音评估鲁棒性；DiverseSpeech-Ru 及配套工具为社区提供了更可靠的评估基准与分析手段。

Conclusion: 语音识别评估需重视标注多样性与对齐质量，尤其对形态复杂语言；多参考标注与改进对齐是避免评估偏差的关键；所构建资源与工具可推动更公平、可解释的ASR模型比较与诊断。

Abstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.

</details>


### [95] [UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop](https://arxiv.org/abs/2601.21000)
*Muhammad Ali Shafique,Areej Mehboob,Layba Fiaz,Muhammad Usman Qadeer,Hamza Farooq*

Main category: cs.CL

TL;DR: 本文提出了一种结合多翻译系统与人工验证的上下文集成翻译框架，构建了首个面向乌尔都语的标准化推理评测基准UrduBench，并系统评估了多种大语言模型在乌尔都语推理任务上的表现，揭示了多步与符号推理的挑战及语言对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语等低资源语言缺乏标准化推理评测基准，现有方法受限于机器翻译敏感性和对通用语言任务的过度关注，难以准确评估模型的真实推理能力。

Method: 提出上下文集成翻译框架，融合多个翻译系统并引入人工校验，确保语义、结构和上下文完整性；将MGSM、MATH-500、CommonSenseQA和OpenBookQA等主流推理与问答基准翻译为乌尔都语，构建UrduBench；在多种提示策略下对推理导向型和指令微调型LLMs进行系统评测。

Result: 发现多步推理和符号推理任务在乌尔都语中尤为困难；语言一致性（stable language alignment）是鲁棒推理的关键前提；不同数据集、难度等级、模型架构、缩放规模和语言一致性测试均表现出显著性能差异。

Conclusion: 本工作建立了可扩展的低资源语言推理评测方法论，为乌尔都语提供了首个标准化推理基准UrduBench，并揭示了多语言推理失败的关键因素；该框架可推广至其他低资源语言。

Abstract: Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.

</details>


### [96] [Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations](https://arxiv.org/abs/2601.21084)
*Amit Meghanani,Thomas Hain*

Main category: cs.CL

TL;DR: 本文探讨了在自监督学习（SSL）语音模型中，前端语音增强（SE）模型微调时使用均方误差（MSE）损失所引发的位置嵌入干扰问题，并提出两种位置不变的微调策略：零填充与基于软动态时间规整（soft-DTW）的速度扰动方法；实验表明soft-DTW方法收敛更快、下游性能更优。


<details>
  <summary>Details</summary>
Motivation: MSE损失在SSL表示引导的语音增强微调中易利用位置嵌入而非语义内容，导致优化目标偏离真实语音内容重建，本文旨在解决该位置敏感性带来的表征微调局限性。

Method: 提出两种位置不变的SSL表示引导语音增强微调策略：(1) 零填充（借鉴SSL预训练但应用于微调阶段）；(2) 速度扰动结合soft-DTW损失，以对齐不同速度下语音表征的时间结构。

Result: soft-DTW方法相比MSE基线实现更快收敛和更优下游任务性能（如ASR），验证了位置不变微调对SSL语音建模的重要性；零填充策略效果有限。

Conclusion: 在SSL语音模型微调中，应避免依赖位置信息的损失函数（如MSE），而采用对齐语音内容结构的度量（如soft-DTW），以提升语音增强与下游任务鲁棒性。

Abstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.

</details>


### [97] [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)
*Ketan Thakkar,Maitreyi Chatterjee,Ramasubramanian Balasubramanian,Achyuthan Jootoo,Rajendra Ugrani*

Main category: cs.CL

TL;DR: 本文提出ChunkWise LoRA，一种动态自适应的LoRA方法，通过基于token复杂度的可变长度分块和每块定制低秩配置，在降低延迟和内存的同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法对所有token采用静态统一的秩配置，忽略了token复杂度和计算需求的差异，导致效率瓶颈。

Method: 提出ChunkWise LoRA：引入运行时调度器估计token难度、进行自适应分块，并通过秩阶梯机制为每块选择低秩和缩放系数；加入边界安全组合模块与策略驱动的KV缓存策略以保障输出一致性。

Result: 在Wikitext-103和SQuAD等基准上，相比基线LoRA，延迟降低最多34%，内存减少38%，同时BLEU、EM和困惑度等指标持平或提升。

Conclusion: ChunkWise LoRA是一种高效、兼容性强、可直接部署于现有Transformer架构与推理框架的参数高效微调新范式。

Abstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.

</details>


### [98] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 本文比较了数据混合与模型合并两种方法在构建小型多任务代码大语言模型（LLM）中的效果，发现模型合并更适合大规模模型（如7B），而数据混合更适合小规模模型（如2B）；合并后的模型在代码生成和摘要任务上均表现优异，甚至超越单任务微调模型，并提出权重分析技术以理解任务对参数的影响。


<details>
  <summary>Details</summary>
Motivation: 随着在智能体框架中部署小型、专用代码大语言模型的趋势兴起，亟需高效、低成本、满足资源约束的多任务学习策略。

Method: 对比数据混合（data mixing）与模型合并（model merging）两种方法，在Qwen Coder和DeepSeek Coder两个模型家族（2B和7B参数规模）上进行代码生成与代码摘要任务的微调实验，并在HumanEval、MBPP和CodeXGlue基准上评估；同时引入权重分析技术探究任务对模型参数的影响。

Result: 模型合并法在7B规模下整体性能最优，保留96%专用模型的代码生成能力并维持摘要能力，最优Qwen Coder 2.5 7B合并模型在HumanEval上达92.7% Pass@1（高于对应单任务微调模型的90.9%）；2B规模下数据混合更优；权重分析揭示了不同任务对参数影响的差异。

Conclusion: 合理选择数据混合或模型合并策略，可有效融合多任务能力且不显著牺牲性能，适用于资源受限的实际部署场景。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [99] [Large Language Models Naively Recover Ethnicity from Individual Records](https://arxiv.org/abs/2601.21132)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: 本文证明了大语言模型（LLM）无需额外训练数据即可仅凭姓名高精度推断族裔，性能超越传统BISG方法，且适用于美国以外多国及多种社会分类（如宗教、种姓），并减少收入偏差，支持低成本本地化部署。


<details>
  <summary>Details</summary>
Motivation: 传统族裔推断方法（如BISG）依赖美国姓氏地理编码，泛化性差、存在收入偏差，且难以扩展至其他国家和分类体系；本文旨在探索LLM在无监督、跨文化场景下基于姓名进行社会身份推断的可行性与优势。

Method: 使用多个主流LLM（Gemini 3 Flash、GPT-4o、DeepSeek v3.2、GLM-4.7等）直接对姓名进行族裔/社会类别推理；在佛罗里达与北卡罗来纳选民数据（含自报种族）上评估准确率；引入扩展推理、政党注册等元数据增强；并在黎巴嫩（宗教）、印度（种姓/保留席位）、乌干达、尼泊尔、亚美尼亚、智利、哥斯达黎加等多国真实选民或土地登记数据中进行跨文化验证；最后用小型Transformer模型在LLM生成标签上微调以实现轻量部署。

Result: LLM方法在美选民数据上达84.7%准确率（vs BISG 68.2%），加入元数据后达86.7%；显著降低BISG固有的高收入少数族裔误判为白人的偏差；在黎巴嫩宗教分类达64.3%，印度议员种姓达99.2%，印度土地记录达74.0%；多国聚合验证显示其能准确复现已知人口分布；微调小模型可超越BISG且支持零成本本地部署。

Conclusion: LLM具备强大、鲁棒且无需训练的跨文化姓名语义理解能力，可作为新一代通用社会身份推断工具，优于传统统计方法，适用于全球多语境下的公平性研究、政策分析与合规应用。

Abstract: I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.

</details>


### [100] [EnsembleLink: Accurate Record Linkage Without Training Data](https://arxiv.org/abs/2601.21138)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: 本文提出EnsembleLink方法，利用预训练语言模型实现无需标注数据的高精度记录链接，显著提升社会科学研究中实体匹配的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 记录链接在实证社会科学中至关重要，但现有方法要么准确率低，要么需要大量标注数据，且研究者常忽视链接错误带来的下游分析不确定性。

Method: 提出EnsembleLink方法，利用预训练语言模型从大规模文本中学习语义关系（如地名层级、政党别名等），无需任何人工标注数据，支持本地开源模型运行，不依赖外部API。

Result: 在涵盖城市名、人名、组织名、多语种政党名及文献记录的多个基准测试中，EnsembleLink达到或超越需大量标注数据的方法，典型任务可在数分钟内完成。

Conclusion: EnsembleLink为社会科学研究提供了一种高精度、零标注、可本地部署的记录链接新范式，有助于缓解链接误差对下游分析的影响。

Abstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that "South Ozone Park" is a neighborhood in "New York City" or that "Lutte ouvriere" refers to the Trotskyist "Workers' Struggle" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.

</details>


### [101] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

TL;DR: 本文提出了Output-Space Search（OS-Search）方法，将大语言模型生成任务转化为在冻结编码器定义的3D输出空间Z中进行目标点搜索，并通过检索增强的策略和序列级强化学习实现高效、并行、黑盒优化。


<details>
  <summary>Details</summary>
Motivation: 传统LLM生成依赖于逐token或逐程序的路径依赖式搜索，难以高效探索输出多样性与优化目标；本文旨在解耦生成过程与搜索空间，提升多样性与可控优化能力。

Method: 提出OS-Search框架：外层循环在冻结编码器定义的3D输出空间Z中选择目标z*；内层使用检索增强、序列级RL训练的策略，生成在标准自回归解码下坐标接近z*的输出；支持并行扫描与黑盒优化，无需token/program级路径搜索。

Result: 在故事生成任务中，Z空间扫描使LLM评分的多样性提升3.1倍（优于prompt-chaining）；在代码生成任务中，对Z（代码空间）的贝叶斯优化在相同推理预算下显著提升隐藏目标函数值，同时保持生成代码有效性。

Conclusion: OS-Search为LLM生成提供了新范式——将生成建模为输出空间中的端点搜索，兼顾多样性、可控性与有效性，突破了传统自回归生成的路径依赖限制。

Abstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [102] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本文通过跨语言语料库分析和神经网络建模实验，验证功能词的高频性、与句法结构的可靠关联性及与短语边界的对齐性这三大统计特性在层级结构学习中的关键作用，并揭示不同学习条件下模型对功能词依赖机制的差异。


<details>
  <summary>Details</summary>
Motivation: 探究支持从线性输入中学习层级结构的统计条件，特别是功能词的分布特性在语言习得中的作用。

Method: 采用跨语言语料库分析验证功能词三大特性在186种语言中的普适性；结合反事实语言建模与消融实验评估各特性对神经学习者习得能力的影响；进一步通过探针和消融分析考察不同学习条件下模型对功能词的依赖机制。

Result: 证实功能词的高频性、结构关联性和边界对齐性普遍存在；保留全部三特性的语言变体更易被神经学习者掌握，其中频率和结构关联性贡献大于边界对齐；不同学习条件导致模型对功能词的依赖方式系统性不同。

Conclusion: 功能词的特定统计分布是支持层级结构学习的关键条件，且相似行为表现可能源于不同的内部表征机制。

Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [103] [Fake News Detection After LLM Laundering: Measurement and Explanation](https://arxiv.org/abs/2501.18649)
*Rupak Kumar Das,Jonathan Dodge*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）生成的伪新闻检测问题，发现现有检测器在识别LLM改写后的假新闻时效果下降，并揭示情感偏移是导致检测失败的关键原因；同时构建了包含改写样本与评分的新数据集。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能生成高度逼真且语境相关的虚假新闻，其带来的 misinformation风险加剧，而针对LLM生成内容的检测研究仍不足。

Method: 通过实验评估多种检测器对LLM改写假新闻的识别能力，结合LIME可解释性分析探测失败原因，并利用BERTScore等指标评估改写质量，同时构建并发布含改写样本与评分的新数据集。

Result: （1）检测器对LLM改写假新闻的识别率显著低于人工撰写文本；（2）不同LLM在规避检测、改写以规避检测及保持语义相似性任务中表现各异；（3）LIME分析显示情感偏移是主要失败原因；（4）高BERTScore样本仍可能出现情感偏移；（5）发布了两个增强型数据集。

Conclusion: 当前假新闻检测器难以应对LLM改写内容，情感偏移是关键挑战，需开发更鲁棒的检测方法和更可靠的改写质量评估指标。

Abstract: With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub

</details>


### [104] [Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/abs/2601.21204)
*Hong Liu,Jiaqi Zhang,Chao Wang,Xing Hu,Linkun Lyu,Jiaqi Sun,Xurui Yang,Bo Wang,Fengcun Li,Yulei Qian,Lingtong Si,Yerui Sun,Rumei Li,Peng Pei,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文提出通过扩展嵌入层（embedding scaling）来提升稀疏大语言模型的性能，发现其在特定条件下优于传统的专家数量扩展（expert scaling），并基于此构建了LongCat-Flash-Lite模型，在参数量和激活量之间取得更好平衡，尤其在智能体与编程任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: MoE架构在扩大稀疏性时面临收益递减和系统瓶颈，需探索新的缩放维度。

Method: 系统分析嵌入扩展对稀疏性的提升效果，识别其优于专家扩展的参数配置区间；研究影响其效能的关键架构因素（如参数分配、宽度与深度协同）；结合定制化系统优化与推测解码实现推理加速；据此从头训练LongCat-Flash-Lite模型。

Result: LongCat-Flash-Lite（68.5B总参，约3B激活）在多项任务上超越同参数量MoE基线，并在智能体与编程领域媲美甚至超越同规模现有模型。

Conclusion: 嵌入扩展是一种与专家扩展正交且有效的稀疏性缩放新范式，尤其在合理参数预算与系统协同优化下可显著提升模型效率与性能。

Abstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.

</details>


### [105] [Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning](https://arxiv.org/abs/2601.21700)
*Wonduk Seo,Wonseok Choi,Junseo Koh,Juhyeon Lee,Hyunjin An,Minhyeong Yu,Jian Park,Qingshan Zhou,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: 本文提出OG-MAR框架，通过结合世界价值观调查（WVS）数据与文化本体，利用多智能体推理提升大语言模型在文化敏感决策中的对齐性、鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在文化敏感决策中因预训练数据偏差和缺乏结构化价值表征而存在价值错位；当前对齐方法缺乏人口统计学依据，且将价值观视为独立无结构信号，导致一致性与可解释性不足。

Method: 提出OG-MAR：1）基于WVS构建个体响应者的价值摘要；2）通过能力问题在固定分类体系上构建全球文化本体；3）推理时检索本体一致关系及人口统计相似档案，实例化多个价值-人格智能体；4）由判断智能体融合输出并强制满足本体一致性与人口邻近性。

Result: 在四个LLM主干模型和区域社会调查基准上的实验表明，OG-MAR在文化对齐性、鲁棒性上优于强基线，并生成更透明的推理轨迹。

Conclusion: OG-MAR通过本体引导与多智能体协同，有效提升了LLM在文化价值观建模中的结构性、可解释性与人口适配性，为文化敏感AI提供了新范式。

Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.

</details>


### [106] [Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling](https://arxiv.org/abs/2601.21205)
*Eunjung Yeo,Julie M. Liss,Visar Berisha,David R. Mortensen*

Main category: cs.CL

TL;DR: 本文提出了一种多语言构音评估框架，结合通用音素识别与基于对比音系特征距离的语言特异性音素解释，用于评估构音障碍患者的言语可懂度，并提出了三个新指标：音素错误率（PER）、音系特征错误率（PFER）和无需对齐的音素覆盖率（PhonCov）。


<details>
  <summary>Details</summary>
Motivation: 神经性构音障碍患病率上升，亟需跨语言适用的自动化言语可懂度评估方法；现有方法多为单语且忽略语言特异性因素。

Method: 构建多语言音素产出评估框架，融合通用音素识别与语言特异性音素解释，利用对比音系特征距离实现音素映射与序列对齐，定义PER、PFER和PhonCov三个指标。

Result: 在英语、西班牙语、意大利语和泰米尔语数据上验证：PER受益于映射+对齐，PFER仅受益于对齐，PhonCov仅受益于映射；框架能捕捉临床上有意义的构音障碍退化模式。

Conclusion: 该框架有效支持跨语言构音障碍评估，各指标互补，具备临床应用潜力。

Abstract: The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.

</details>


### [107] [MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset](https://arxiv.org/abs/2601.21512)
*Serry Sibaee,Yasser Alhabashi,Nadia Sibai,Yara Farouk,Adel Ammar,Sawsan AlHalawani,Wadii Boulila*

Main category: cs.CL

TL;DR: 本文介绍了MURAD，一个包含96,243个阿拉伯语词-定义对的开源多领域反向阿拉伯语词典数据集，旨在推动阿拉伯语自然语言处理和词汇语义研究。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语虽词汇丰富、涵盖领域广泛，但缺乏大规模、高精度的阿拉伯语词义数据集，限制了其计算语言学与词典学研究。

Method: 通过混合流水线（包括直接文本解析、光学字符识别和自动重构）从权威参考文献和教育资料中提取数据，并为每个词条标注标准化阿拉伯语定义及来源领域元数据。

Result: 构建了覆盖语言学、伊斯兰研究、数学、物理、心理学和工程等领域的MURAD数据集，共96,243个词-定义对，已开源。

Conclusion: MURAD填补了阿拉伯语高质量反向词典资源的空白，支持反向词典建模、语义检索和教育工具开发，促进阿拉伯语NLP与可复现语义研究。

Abstract: Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.

</details>


### [108] [Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models](https://arxiv.org/abs/2601.21214)
*Zhaoyi Li,Jiatong Li,Gangwei Jiang,Linqi Song,Defu Lian,Ying Wei*

Main category: cs.CL

TL;DR: 本文发现链式思维推理中错误集中在特定token位置，由某些注意力头（ep heads）引发，提出测试时动态识别并禁用这些头以提升推理泛化能力。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在推理步数超出训练分布时性能显著下降，其内在机制尚不清楚。

Method: 通过多领域任务的系统研究，识别出导致错误的特定注意力头（ep heads），并提出测试时动态禁用这些头的轻量级干预方法。

Result: 该方法在多个任务和大语言模型上均一致提升了推理步数泛化能力。

Conclusion: 链式思维推理失败源于特定注意力头对错误推理路径的放大，动态修正这些头可有效提升泛化性能。

Abstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.

</details>


### [109] [LMK > CLS: Landmark Pooling for Dense Embeddings](https://arxiv.org/abs/2601.21525)
*Meet Doshi,Aashka Trivedi,Vishwajeet Kumar,Parul Awasthy,Yulong Li,Jaydeep Sen,Radu Florian,Sachindra Joshi*

Main category: cs.CL

TL;DR: 本文提出了一种新的序列池化方法——Landmark (LMK) 池化，通过将序列分块并在块间插入地标标记（landmark tokens），再对这些地标标记嵌入进行均值池化，以克服[CLS]和均值池化在长上下文建模中的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有主流池化策略（如[CLS]和均值池化）存在明显缺陷：[CLS]易偏向序列开头信息、忽略分布式证据；均值池化则会稀释局部显著信号，损害短上下文性能。

Method: 提出Landmark (LMK) 池化：将输入序列划分为若干块，在每块之间插入可学习的地标标记（landmark tokens），最终对所有地标标记的嵌入取均值作为整体表示。

Result: LMK池化在短上下文检索任务上与现有方法性能相当，在长上下文任务上显著提升；仅引入少量特殊标记，兼顾效果与效率。

Conclusion: LMK池化是一种实用、可扩展的替代方案，有效平衡了长上下文外推能力与局部关键特征保留能力。

Abstract: Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.

</details>


### [110] [Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data](https://arxiv.org/abs/2601.21218)
*Christopher Adrian Kusuma,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文提出了一种更鲁棒的大型语言模型（LLM）诚实性评估基准，并利用开源模型Pythia及其公开预训练数据，设计新方法提升LLM在知识边界不清时主动回答'我不知道'的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM常因不了解自身知识边界而产生幻觉；已有诚实性提升方法的评估未考虑模型预训练阶段已习得的知识，导致鲁棒性不足。

Method: 基于开源模型Pythia及其公开预训练数据，构建更鲁棒的诚实性评估基准；并提出一种利用预训练数据增强LLM诚实性的新方法。

Result: 建立了首个考虑预训练知识的、更鲁棒的LLM诚实性评估基准；验证了所提方法可有效提升模型在未知问题上主动拒答的能力。

Conclusion: 评估LLM诚实性必须纳入其预训练知识背景；利用预训练数据可切实提升模型对知识边界的感知与诚实响应能力。

Abstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with "I don't know" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.

</details>


### [111] [MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation](https://arxiv.org/abs/2601.21225)
*Tianyi Xu,Kosei Uemura,Alfred Malengo Kondoro,Tadesse Destaw Belay,Catherine Nana Nyaah Essuman,Ifeoma Okoh,Ganiyat Afolabi,Ayodele Awokoya,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文提出了MGSM-Pro数据集，通过为每个MGSM问题生成五个不同数字、名字和无关上下文的实例，来评估多语言大模型在数学推理中的鲁棒性；实验发现低资源语言及部分闭源模型（如Gemini 2.5 Flash、GPT-4.1）对数字变化敏感，而Claude 4.0 Sonnet及开源模型GPT-OSS 120B、DeepSeek V3表现更鲁棒；建议未来评测应至少使用五种数字变体以提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多语言数学推理基准在难度和时效性上落后于英文基准，且缺乏对模型在不同问题实例（尤其是数字变化）下鲁棒性的系统评估。

Method: 基于MGSM构建MGSM-Pro数据集，为每个问题生成五个语义等价但数字、人名和无关上下文不同的实例；在九种语言上对多个主流闭源与开源大模型进行评测，并分析其在不同数字实例下的性能波动。

Result: 低资源语言在数字变化实例上性能显著下降；闭源模型Gemini 2.5 Flash和GPT-4.1鲁棒性较差，Claude 4.0 Sonnet更鲁棒；开源模型GPT-OSS 120B和DeepSeek V3表现出更强鲁棒性。

Conclusion: 单一数字实例的评测易导致乐观偏差，应采用多实例（至少五种数字变体）评测以获得更真实可靠的数学推理能力评估结果。

Abstract: Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.

</details>


### [112] [SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models](https://arxiv.org/abs/2601.21235)
*Alok Abhishek,Tushar Bandopadhyay,Lisa Erickson*

Main category: cs.CL

TL;DR: 本文提出SHARP框架，用于多维、分布感知的社会危害评估，强调尾部风险而非平均风险，揭示了不同大语言模型在社会危害方面的异质性失败结构。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准将复杂的社会风险简化为均值标量分数，忽略了分布结构、跨维度交互和最坏情况行为，无法有效识别高风险场景下的严重失败。

Method: SHARP将社会危害建模为多元随机变量，显式分解为偏见、公平性、伦理与认知可靠性四个维度，并采用基于‘失败并集’的加性累积对数风险聚合方式；主指标为条件风险价值CVaR95，以刻画最坏情况行为。

Result: 在901条敏感提示上评估11个前沿大语言模型发现：平均风险相近的模型其尾部暴露和波动性差异超两倍；各维度尾部严重性呈现系统性差异（偏见最强，伦理最弱），揭示出被标量基准掩盖的异质失败模式。

Conclusion: 负责任的大语言模型评估与治理必须超越标量均值，转向多维、尾部敏感的风险画像。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.

</details>


### [113] [MoCo: A One-Stop Shop for Model Collaboration Research](https://arxiv.org/abs/2601.21257)
*Shangbin Feng,Yuyang Bai,Ziyuan Yang,Yike Wang,Zhaoxuan Tan,Jiajie Yan,Zhenyu Lei,Wenxuan Ding,Weijia Shi,Haojin Wang,Zhenting Qi,Yuru Jiang,Heng Wang,Chengsong Huang,Yu Fei,Jihan Yao,Yilun Du,Luke Zettlemoyer,Yejin Choi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 本文介绍了MoCo，一个用于执行、基准测试和比较大规模模型协作算法的Python库，包含26种协作方法和25个评估数据集，实验证明协作策略在多数设置下优于单模型，并分析了其扩展性、效率及未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有模型协作研究分散且缺乏系统性比较，亟需统一框架来推动该领域发展。

Method: 构建MoCo开源库，集成26种跨模型信息交换方式（如路由、文本、logit、参数）的协作方法，并支持25个多样化评测数据集及用户自定义数据。

Result: 实验表明协作策略在61.0%的（模型，数据）设置中优于单模型，最优方法提升达25.8%；并揭示了协作系统在单模型薄弱任务上的优势。

Conclusion: MoCo为模型协作研究提供了标准化、可复现的基础设施，有望推动开放、模块化、去中心化与协作式AI的发展。

Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.

</details>


### [114] [When "Better" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications](https://arxiv.org/abs/2601.22025)
*Daniel Commey*

Main category: cs.CL

TL;DR: 本文提出了一种面向大语言模型（LLM）应用的评估驱动开发流程（Define-Test-Diagnose-Fix），并设计了分层的最小可行评估套件（MVES），覆盖通用LLM应用、RAG和智能体工具调用三类场景；通过本地可复现实验揭示了通用提示模板可能带来行为权衡，强调需以评估驱动方式进行提示迭代与主张校准。


<details>
  <summary>Details</summary>
Motivation: 传统软件测试方法不适用于LLM应用，因其输出具有随机性、高维性且对提示词和模型变化敏感，亟需系统化、可复现的评估工程范式。

Method: 提出Define-Test-Diagnose-Fix评估驱动工作流；构建分层MVES评估套件（含自动化检查、人工评分、LLM-as-judge三类方法）；在Ollama平台上使用Llama 3 8B和Qwen 2.5 7B进行本地可控实验，对比任务专用提示与通用提示模板的效果。

Result: 实验发现：将任务专用提示替换为通用‘改进’模板后，Llama 3在结构化测试套件中的信息抽取通过率从100%降至90%，RAG合规性从93.3%降至80%，但指令遵循能力提升，证实提示优化存在行为权衡。

Conclusion: LLM应用开发应以评估为核心闭环，避免盲目推广通用提示模板；需结合场景定制评估套件，并基于实证结果持续迭代提示与校准技术主张。

Abstract: Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.
  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.
  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic "improved" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.
  All test suites, harnesses, and results are included for reproducibility.

</details>


### [115] [CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding](https://arxiv.org/abs/2601.21262)
*Jiahao Huo,Yu Huang,Yibo Yan,Ye Pan,Yi Cao,Mingdong Ou,Philip S. Yu,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出CausalEmbed方法，通过自回归生成方式构建多向量嵌入，显著减少视觉token数量（30-155倍），同时保持高性能，提升视觉文档检索的实用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉文档检索中虽表现优异，但每页使用数千视觉token导致存储开销过大，限制实际应用。

Method: 提出自回归生成方法CausalEmbed，结合对比学习中的迭代间隔损失，促使模型学习紧凑且结构良好的嵌入表示。

Result: 仅需数十个视觉token即可实现高效视觉文档检索，在多个骨干网络和基准上性能极具竞争力，token数减少30–155倍；理论与实验验证其训练效率与测试时可扩展性优势。

Conclusion: CausalEmbed引入灵活的测试时缩放策略，推动多向量VDR表示的生成范式发展，为多模态文档检索提供新思路。

Abstract: Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.

</details>


### [116] [Qwen3-ASR Technical Report](https://arxiv.org/abs/2601.21337)
*Xian Shi,Xiong Wang,Zhifang Guo,Yongqi Wang,Pei Zhang,Xinyu Zhang,Zishan Guo,Hongkun Hao,Yu Xi,Baosong Yang,Jin Xu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 本文介绍了Qwen3-ASR系列语音识别模型，包括两个支持52种语言的全功能ASR模型（1.7B和0.6B参数量）及一个基于大语言模型的非自回归强制对齐模型（0.6B），均基于Qwen3-Omni基础模型；1.7B版在开源ASR模型中达到SOTA，0.6B版兼顾精度与效率，强制对齐模型在多语言时间戳精度、效率和通用性上优于现有最强模型，并全部开源（Apache 2.0）。


<details>
  <summary>Details</summary>
Motivation: 解决开源ASR模型在公开基准上性能接近但实际场景质量差异显著的问题，同时提升多语言语音识别与强制对齐的精度、效率与通用性，并推动社区研究。

Method: 构建基于Qwen3-Omni音频理解能力的两种规模全功能ASR模型（Qwen3-ASR-1.7B/0.6B）和一种LLM驱动的非自回归强制对齐模型（Qwen3-ForcedAligner-0.6B），采用大规模语音数据训练，并开展内部综合评测与公开基准对比。

Result: Qwen3-ASR-1.7B在开源ASR模型中达SOTA，媲美最强商用API；Qwen3-ASR-0.6B实现92ms平均首字延迟、1秒处理2000秒语音（并发128）；Qwen3-ForcedAligner-0.6B在11种语言时间戳精度上超越三大最强对齐模型，且更高效通用。

Conclusion: Qwen3-ASR系列模型在多语言ASR与强制对齐任务上实现了精度、效率与通用性的协同突破，全部开源将有力促进语音技术社区发展。

Abstract: In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.

</details>


### [117] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的新型预训练方法，通过在预训练阶段实时评估和优化模型生成的K个后续token，在源头提升大语言模型的事实性、安全性和整体生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的后训练对齐和微调，难以根除预训练阶段习得的不安全或幻觉行为，因此需在预训练阶段就嵌入高质量、安全、事实性的能力。

Method: 采用流式文档输入，结合强化学习（RL）在每步优化接下来K个token的生成；使用强后训练模型作为判别器，对模型rollout、原始后缀和重写后缀进行多维度（质量、安全、事实性）打分并给予奖励。

Result: 实验显示，相比标准预训练，该方法在事实性上提升36.2%，安全性提升18.5%，整体生成质量胜率最高提升86.3%。

Conclusion: 在预训练阶段引入RL驱动的实时质量控制，能从底层构建更安全、更真实、更高质量的大语言模型，优于依赖后期对齐的传统范式。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [118] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型在教育评估中因过度追求指令遵循而产生的‘服从悖论’，即模型易受语义保持型对抗代码注入（SPACI）攻击，导致对错误代码的错误认证；为此提出AST-Aware语义注入协议与三维度评估框架，并呼吁转向面向裁决鲁棒性的领域特定对齐范式。


<details>
  <summary>Details</summary>
Motivation: 当前LLM被广泛用于自动代码评分，但其评估能力依赖于一个未经验证的假设：指令遵循能力等同于客观评判能力。本文质疑该假设，指出模型可能为满足隐含指令而脱离代码逻辑，造成严重误判。

Method: 提出Semantic-Preserving Adversarial Code Injection (SPACI) 框架和Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP)，利用抽象语法树（AST）中语法惰性区域（trivia nodes）嵌入对抗性指令， exploiting 语法-语义鸿沟；并在Python/C/C++/Java共25,000份代码提交上评测9个SOTA模型。

Result: 发现高容量开源模型（如DeepSeek-V3）失败率超95%，系统性地优先响应隐藏格式约束而非代码正确性；提出Decoupling Probability、Score Divergence、Pedagogical Severity三指标量化‘虚假认证’现象。

Conclusion: 现行RLHF对齐范式在自动评分场景下引入类似‘特洛伊木马’的安全隐患；需转向以证据优先、面向裁决鲁棒性（Adjudicative Robustness）的新对齐路径，并开源数据与框架推动后续研究。

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [119] [User-Centric Evidence Ranking for Attribution and Fact Verification](https://arxiv.org/abs/2601.21387)
*Guy Alt,Eran Hirsch,Serwar Basch,Ido Dagan,Oren Glickman*

Main category: cs.CL

TL;DR: 本文提出证据排序（Evidence Ranking）这一新任务，旨在通过优化证据呈现顺序，在减少用户阅读负担的同时保证验证所需的充分性；对比了单次排序与增量排序方法，并构建统一基准与新评估框架，实验表明增量排序和大模型方法更优，且用户研究表明该方法能同时降低阅读负担并提升验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动系统和大语言模型在事实验证中常提供不足或冗余的证据，导致验证效率低、易出错，亟需一种兼顾信息充分性与用户阅读效率的新范式。

Method: 提出证据排序任务，定义其目标为在排序列表中尽早呈现足够支持/反驳声明的证据；设计单次排序与增量排序两种方法；构建融合多个现有数据集的统一基准；提出受信息检索启发的新评估框架；开展控制变量用户研究。

Result: 增量排序策略更能捕捉互补证据；基于LLM的方法显著优于浅层基线；但模型在充分性与冗余性间仍难平衡；用户研究表明证据排序相比传统证据选择可降低阅读努力并提升验证准确率。

Conclusion: 证据排序是迈向更可解释、高效且以用户为中心的信息验证系统的重要基础性工作。

Abstract: Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems.

</details>


### [120] [Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation](https://arxiv.org/abs/2601.21464)
*Yuan Sui,Bryan Hooi*

Main category: cs.CL

TL;DR: 本文提出CoNL框架，通过多智能体自博弈统一生成、评估与元评估，利用批判性反馈是否能提升解决方案来衡量其质量，从而在无外部评判者和真实标签的情况下联合优化生成与评判能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-Judge方法受限于评判者自身质量，且存在评估偏差（如偏好冗长而非高质量），亟需能评估并改进评判者自身的元评估能力。

Method: 提出CoNL框架，采用共享策略的多智能体进行结构化对话（提出-批判-修订），以批判是否促进方案改进为依据赋予诊断性奖励，实现生成与评判能力的联合自博弈优化。

Result: 在五个基准测试中，CoNL持续优于自奖励基线，且训练过程稳定。

Conclusion: CoNL通过引入可学习的元评估机制，在无需外部评判或真实标签的前提下，有效提升了非可验证任务中LLM的生成与评判协同能力。

Abstract: Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.

</details>


### [121] [SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.21476)
*Lei Yang,Wei Bi,Chenxi Sun,Renren Jin,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文提出SOUP框架，在单样本级别统一在线和离线策略学习，通过token级重要性比率利用离线数据提升探索能力与稳定性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于在线策略的强化学习方法（如GRPO）在大语言模型后训练中存在探索不足和早期饱和问题；而混合整条轨迹的离线数据又导致策略不匹配和训练不稳定。

Method: 提出SOUP（Single-sample Mix-policy Unified Paradigm）框架：在单个生成序列中，前缀部分使用历史策略（离线数据），续写部分使用当前策略（在线生成），并通过token级重要性比率加权调整离线信息的影响。

Result: 实验表明SOUP在多个任务上持续优于标准在线策略训练及现有离线策略扩展方法；分析进一步验证其能同时提升探索能力和最终性能。

Conclusion: SOUP通过细粒度、单样本级别的混合策略范式，有效平衡了离线数据利用与训练稳定性，为大语言模型强化学习提供了新思路。

Abstract: On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\textbf{S}$ingle-sample Mix-p$\textbf{O}$licy $\textbf{U}$nified $\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.

</details>


### [122] [DimStance: Multilingual Datasets for Dimensional Stance Analysis](https://arxiv.org/abs/2601.21483)
*Jonas Becker,Liang-Chih Yu,Shamsuddeen Hassan Muhammad,Jan Philip Wahle,Terry Ruas,Idris Abdulmumin,Lung-Hao Lee,Wen-Ni Liu,Tzu-Mi Lin,Zhe-Yu Xu,Ying-Lung Lin,Jin Wang,Maryam Ibrahim Mukhtar,Bela Gipp,Saif M. Mohammed*

Main category: cs.CL

TL;DR: 本文提出了一种基于效价-唤醒度（VA）维度的情感化立场检测新范式，构建了首个跨语言、多领域的维度立场资源DimStance，并开展回归建模与跨语言分析。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测仅输出离散标签（如Favor/Neutral/Against），难以刻画态度背后的细微情感状态；本文旨在引入情感科学中的效价-唤醒度二维连续框架，实现更细粒度、可解释的立场建模。

Method: 构建包含五种语言、两个领域、11746个目标方面的DimStance数据集，标注其VA值；定义维度立场回归任务；在回归微调与提示学习两种范式下评测多种预训练模型与大语言模型。

Result: 微调的大语言模型在VA回归任务中表现具竞争力；低资源语言（如尼日利亚皮金语、斯瓦希里语）仍存在显著性能差距；基于token生成的方法在VA预测中效果有限。

Conclusion: DimStance为多语言、情感感知的立场分析提供了新基准与基础资源，推动立场理解从分类走向连续、可解释的维度建模。

Abstract: Stance detection is an established task that classifies an author's attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.

</details>


### [123] [inversedMixup: Data Augmentation via Inverting Mixed Embeddings](https://arxiv.org/abs/2601.21543)
*Fanshuang Kong,Richong Zhang,Qiyu Sun,Zhijie Nie,Ting Deng,Chunming Hu*

Main category: cs.CL

TL;DR: 本文提出inversedMixup，一种结合Mixup可控性与LLM生成可解释性的统一文本数据增强框架，通过三阶段训练对齐任务模型嵌入空间与大语言模型输入空间，实现可控混合嵌入到可读句子的重建，并首次实证揭示并缓解文本Mixup中的流形侵入现象。


<details>
  <summary>Details</summary>
Motivation: Mixup在隐空间操作导致样本不可解释，而LLM生成虽可读但缺乏可控性；同时，现有方法未能解决文本Mixup中潜在的流形侵入问题。

Method: 提出inversedMixup框架：采用三阶段训练策略，将任务特定模型的输出嵌入空间与LLM的输入嵌入空间对齐；利用LLM反演技术，将可控比例混合后的嵌入重建为人类可读句子。

Result: 在少样本和全监督场景下均显著提升数据增强效果；首次提供文本Mixup中流形侵入现象的实证证据，并提出有效缓解策略。

Conclusion: inversedMixup成功融合了Mixup的可控性与LLM生成的可解释性，为文本数据增强提供了新范式，并揭示了嵌入混合中的重要几何问题。

Abstract: Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.

</details>


### [124] [Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes](https://arxiv.org/abs/2601.21551)
*Yang Zhou,Zhenting Sheng,Mingrui Tan,Yuting Song,Jun Zhou,Yu Heng Kwan,Lian Leng Low,Yang Bai,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出Note2Chat框架，利用真实医疗记录生成高质量医患对话数据，并通过三阶段微调和单轮推理范式提升大语言模型在动态多轮临床问诊中的诊断能力，显著超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在静态评测中表现良好，但在需迭代提问与假设修正的动态多轮临床诊断场景中效果不足；同时，高质量、合规的医患对话数据稀缺且敏感。

Method: 提出Note2Chat框架：1）基于决策树引导将真实医疗记录转化为医患对话；2）采用监督学习、模拟数据增强与偏好学习的三阶段微调策略；3）设计单轮推理范式，将病史采集建模为一系列单步推理问题。

Result: 在临床推理任务上显著优于GPT-4o，F1值提升+16.9，Top-1诊断准确率提升+21.0；代码与数据集已开源。

Conclusion: Note2Chat通过利用广泛可得的医疗记录，规避了敏感对话数据依赖，提升了模型在真实临床问诊场景下的可解释性、适应性与样本效率，为临床AI提供了新范式。

Abstract: Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.

</details>


### [125] [ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas](https://arxiv.org/abs/2601.21558)
*Xiaoyu Tian,Haotian Wang,Shuaiting Chen,Hao Zhou,Kaichi Yu,Yudian Zhang,Jade Ouyang,Junxi Yin,Jiong Chen,Baoyan Guo,Lei Zhang,Junjie Tao,Yuansheng Song,Ming Cui,Chengwei Liu*

Main category: cs.CL

TL;DR: 本文提出ASTRA框架，通过可验证的强化学习和可扩展的数据合成，全自动训练工具增强型大语言模型代理，显著提升多步决策中的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型代理训练方法存在需人工干预、依赖不可验证的模拟环境、仅使用监督微调或强化学习之一、难以稳定进行长周期多轮学习等问题。

Method: ASTRA包含两个核心组件：一是基于工具调用图静态拓扑结构合成多样化、结构化轨迹的数据合成管道；二是将问题-答案分解痕迹转化为独立、可执行、规则可验证环境的环境合成框架，并结合监督微调与在线强化学习（以轨迹级奖励平衡任务完成与交互效率）。

Result: 在多个代理工具使用基准测试中，ASTRA训练的模型达到同规模下的最先进性能，接近闭源系统水平，同时保持核心推理能力。

Conclusion: ASTRA实现了全自动、可验证、可扩展的工具增强型语言模型代理训练，为构建鲁棒、高效、可解释的多步决策智能体提供了新范式。

Abstract: Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.

</details>


### [126] [KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices](https://arxiv.org/abs/2601.21579)
*Wuyang Zhou,Yuxuan Gu,Giorgos Iacovides,Danilo Mandic*

Main category: cs.CL

TL;DR: 本文提出KromHC，通过Kronecker积构造双随机残差矩阵，在保证精确双随机性的同时将参数复杂度从O(n³C)或O(nC·n!)降至O(n²C)，显著提升训练稳定性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有mHC及其变体存在两个关键问题：Sinkhorn-Knopp算法无法保证精确双随机性，且参数复杂度高（O(n³C)或O(nC·n!)），限制了模型的可扩展性与训练稳定性。

Method: 提出KromHC，利用多个小规模双随机矩阵的Kronecker积来参数化残差矩阵，并在张量化残差流的各模态上施加流形约束，从而保证整体残差矩阵严格双随机。

Result: KromHC在保持甚至超越SOTA mHC变体性能的同时，将参数复杂度降至O(n²C)，实验验证其更优的训练稳定性与效率。

Conclusion: KromHC通过张量化结构设计，在理论保证（精确双随机性）与实际效率（更低参数复杂度）之间取得更好平衡，为超连接神经网络提供了更优的可扩展解决方案。

Abstract: The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\mathcal{O} \left( nC \cdot n! \right)$. To address both challenges, we propose \textbf{KromHC}, which uses the \underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \texttt{https://github.com/wz1119/KromHC}.

</details>


### [127] [Language Models as Artificial Learners: Investigating Crosslinguistic Influence](https://arxiv.org/abs/2601.21587)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文利用语言模型作为可控的统计学习器，系统模拟跨语言影响（CLI），探究L1主导性、L2熟练度及句法距离对CLI的影响，并通过跨语言启动范式验证CLI机制，结果与心理语言学证据一致，表明LMs可作为研究人类CLI的计算框架。


<details>
  <summary>Details</summary>
Motivation: 人类双语研究中跨语言影响（CLI）结果常因实验变异性而矛盾，需更可控的方法来系统揭示其驱动因素。

Method: 使用语言模型（LMs）作为可控统计学习器，通过调节L2暴露年龄（即L2引入的训练步数）操控L1主导性和L2熟练度，并改变L1预训练语言与L2的句法距离；采用跨语言启动范式分析L1结构激活对L2加工的影响。

Result: 结果与心理语言学研究一致：语言主导性和熟练度是CLI的强预测因子；语法结构启动呈双向性，而非语法结构启动则受语言主导性调控；LMs中存在L1共激活现象，且直接影响L2加工所涉神经回路。

Conclusion: 语言模型可作为研究人类跨语言影响的可靠计算框架，为CLI理论提供机制性证据和新研究范式。

Abstract: Despite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure -- defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.

</details>


### [128] [ILRR: Inference-Time Steering Method for Masked Diffusion Language Models](https://arxiv.org/abs/2601.21647)
*Eden Avrahami,Eliya Nachmani*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的离散扩散语言模型（DLM）推理时控制框架ILRR，通过动态对齐生成序列与参考序列的隐层激活实现语义属性引导，并扩展出支持长短文本匹配的空间调制机制，显著提升属性控制准确率且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（DLMs）虽具非自回归生成潜力，但其推理阶段的有效可控机制仍缺乏探索。

Method: 提出Iterative Latent Representation Refinement（ILRR），在去噪过程中动态对齐生成序列与单参考序列的隐层激活；进一步引入Spatially Modulated Steering，按位置调节引导强度以支持短参考控制长文本。

Result: ILRR在LLaDA和MDLM上实现高效属性控制（如情感），仅需每步一次额外并行前向传播；相同计算预算下，属性准确率较基线提升10%–60%，同时保持高质量生成。

Conclusion: ILRR是一种轻量、通用、学习-free的DLM推理控制框架，能灵活、精准地迁移高阶语义属性，为扩散式文本生成的可控性提供了新范式。

Abstract: Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.

</details>


### [129] [AdaptBPE: From General Purpose to Specialized Tokenizers](https://arxiv.org/abs/2601.21665)
*Vijini Liyanage,François Yvon*

Main category: cs.CL

TL;DR: 本文提出了一种针对子词分词器（如BPE）的轻量级后训练适配策略，通过在特定领域或语言的语料上替换低效token，提升分词效率与模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准的通用子词分词器在特定领域或语言中存在token利用率低、编码效率差的问题。

Method: 基于适配语料的token频率分析，选择性替换原始词表中低效token，构建更契合目标语料的精简词表。

Result: 在多语言生成与分类任务中，适配后的分词器在相同词表规模下显著提升测试语料压缩率，优于基线方法。

Conclusion: 该方法是一种高效、轻量的词汇级微调机制，可灵活适配不同领域或语言，提升LLM的分词效率与下游性能。

Abstract: Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks. Our code and data are available at https://github.com/vijini/Adapt-BPE.git.

</details>


### [130] [Scale-Dependent Semantic Dynamics Revealed by Allan Deviation](https://arxiv.org/abs/2601.21678)
*Debayan Dasgupta*

Main category: cs.CL

TL;DR: 本文利用Allan偏差分析文本语义进展的稳定性，发现人类文本存在短时幂律标度与长时噪声基底两个动力学区域，而大语言模型虽能模拟局部统计特征，但其语义稳定性持续时间显著缩短。


<details>
  <summary>Details</summary>
Motivation: 探究语言在语义状态序列中演进的底层动力学机制，揭示人类文本与算法生成文本在语义连贯性上的本质差异。

Method: 将书面文本的语义演化建模为高维状态空间中的随机轨迹；使用Allan偏差（源自精密计量学）分析有序句子嵌入构成的‘位移信号’，以量化语义稳定性。

Result: 识别出短时幂律标度（可区分文学与技术文本）和长时向噪声基底的转变；发现大语言模型虽复现局部标度行为，但其‘稳定性视界’系统性缩短。

Conclusion: 语义连贯性是一种可量化的物理属性；该框架可有效区分人类认知的精细动态与算法模型生成模式。

Abstract: While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.

</details>


### [131] [FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning](https://arxiv.org/abs/2601.21682)
*Xiaoyu Xu,Minxin Du,Kun Fang,Zi Liang,Yaxin Xiao,Zhicong Huang,Cheng Hong,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 本文提出FIT框架，用于大语言模型的持续性遗忘学习，通过数据过滤、重要性感知更新和目标层归因，在处理大量连续删除请求时有效防止灾难性遗忘和遗忘后恢复，同时在PCH基准上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘方法难以应对现实场景中持续、高频的删除请求，易导致性能下降和灾难性遗忘。

Method: 提出FIT框架，包含严格的数据过滤（Filtering）、重要性感知参数更新（Importance-aware updates）和目标层归因（Targeted layer attribution）三部分，并构建PCH基准及Forget Degree（F.D.）与Retain Utility（R.U.）对称评估指标。

Result: 在四个开源LLM上进行数百次删除请求实验，FIT在F.D.与R.U.权衡上最优，MMLU、CommonsenseQA、GSM8K等任务表现超越现有方法，并对重学习和量化恢复攻击具有鲁棒性。

Conclusion: FIT是一种高效、鲁棒的持续遗忘学习框架，兼顾遗忘有效性与模型实用性，为LLM隐私与合规提供可行技术路径。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \fit mitigates degradation through rigorous data \underline{F}iltering, \underline{I}mportance-aware updates, and \underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \textbf{PCH}, a benchmark covering \textbf{P}ersonal information, \textbf{C}opyright, and \textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.

</details>


### [132] [Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.21684)
*Xinglin Wang,Jiayi Shi,Shaoxiong Feng,Peiwen Yuan,Yiwei Li,Yueqi Zhang,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.CL

TL;DR: 本文提出Recycling Search Experience (RSE)方法，通过在测试时重用搜索过程中的成功结论与失败模式，提升大语言模型推理效率，避免重复计算，在多个基准上实现最优缩放效率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法将每次rollout视为独立样本，忽略中间推理结果，导致大量计算冗余。

Method: 提出RSE策略，构建共享经验库，对中间结论（正向）和失败模式（负向）进行主动提炼与复用，使搜索过程具备累积性。

Result: 在HMMT24、HMMT25、IMO-Bench和HLE等多个推理基准上，RSE以相近计算成本显著超越强基线，达到当前最优的缩放效率。

Conclusion: RSE通过消除测试时搜索的记忆lessness问题，实现了更高效、更智能的推理扩展，为大模型测试时推理提供了新范式。

Abstract: Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.

</details>


### [133] [Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents](https://arxiv.org/abs/2601.21699)
*Hojae Han,Heeyun Jung,Jongyoon Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: 本文提出DAVID-GRPO，一种面向资源受限场景的小型语言模型强化学习框架，在低预算下实现高效多跳推理，显著提升小模型在多跳问答任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多步推理智能体依赖高成本、高精度的大规模策略采样，在资源受限（如小模型、有限GPU）时陷入低效、低准确率困境。

Method: 提出DAVID-GRPO框架：(i) 用最小监督稳定初期训练；(ii) 基于证据召回进行检索信用分配；(iii) 对截断的近似成功轨迹重采样以增强探索。

Result: 在仅4块RTX 3090 GPU上训练≤1.5B参数模型，在6个多跳QA基准上持续超越面向大规模设置的现有RL方法。

Conclusion: 通过引入合适归纳偏置，小型语言模型可在低训练成本下实现高推理精度，打破低成本与低准确率的固有权衡。

Abstract: While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.

</details>


### [134] [Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis](https://arxiv.org/abs/2601.21709)
*Qingyue Yang,Jie Wang,Xing Li,Yinqi Bai,Xialiang Tong,Huiling Zhen,Jianye Hao,Mingxuan Yuan,Bin Li*

Main category: cs.CL

TL;DR: 本文提出了一个名为TAPPA的统一框架，用于从时间连续视角分析大语言模型中的注意力模式，区分可预测与不可预测模式，并通过查询自相似性解释其成因，进而指导KV缓存压缩和模型剪枝等推理加速任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM中注意力模式（如检索头、sink头、对角线痕迹）的观察较为零散，缺乏统一解释框架。

Method: 提出Temporal Attention Pattern Predictability Analysis (TAPPA)框架，从数学建模出发，结合查询、键及RoPE的联合效应，分析注意力模式在时间维度上的可预测性，并依据查询自相似性进行理论解释。

Result: 验证了TAPPA在KV缓存压缩和LLM剪枝任务中的有效性，一个受其启发的简单指标持续优于基线方法。

Conclusion: TAPPA为理解注意力机制提供了统一视角，揭示了注意力模式的可预测性根源，并为高效推理提供了可落地的指导。

Abstract: Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.

</details>


### [135] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: 本文提出TACLer框架，通过模型定制的课程强化学习和混合思考/不思考推理范式，提升大语言模型在复杂推理任务中的学习与推理效率，并在计算成本和准确性上取得双重优势。


<details>
  <summary>Details</summary>
Motivation: 现有长链思维（CoT）推理方法依赖大规模强化学习训练，易导致过度思考和冗余步骤，影响学习与推理效率。

Method: 提出TACLer框架，包含两个核心组件：（i）定制化课程学习，根据模型能力分阶段引入复杂数据；（ii）混合Thinking/NoThinking推理范式，动态启用或禁用思考模式以平衡准确率与效率。

Result: 实验表明TACLer将训练计算量降低超50%，推理token使用减少42%，同时在四个数学数据集上准确率较基线提升超9%，全面超越现有Thinking与NoThinking方法。

Conclusion: TACLer在保持甚至提升推理性能的同时，显著提升了学习与推理效率，为高效、可控的复杂推理提供了新范式。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [136] [Enhancing Language Models for Robust Greenwashing Detection](https://arxiv.org/abs/2601.21722)
*Neil Heinrich Braun,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 本文提出了一种参数高效的框架，通过结合对比学习和序数排序目标来结构化大语言模型的潜在空间，以更好地区分具体行动与模糊声明，提升ESG可持续性报告中绿色洗白和模糊声明识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 可持续性报告对ESG评估至关重要，但绿色洗白和模糊声明常削弱其可信度；现有NLP模型对此类现象鲁棒性不足，依赖泛化能力差的表层模式。

Method: 提出参数高效框架，融合对比学习与序数排序目标以建模具体行动与模糊声明的梯度差异，并引入门控特征调制过滤披露噪声，以及MetaGradNorm稳定多目标优化。

Result: 在跨类别设置实验中，该方法展现出优于标准基线的鲁棒性，并揭示了表征刚性与泛化能力之间的权衡。

Conclusion: 所提框架能更可靠地解析可持续性报告中的实质性内容，为ESG评估提供了更具鲁棒性和可解释性的NLP解决方案。

Abstract: Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.

</details>


### [137] [Procedural Pretraining: Warming Up Language Models with Abstract Data](https://arxiv.org/abs/2601.21725)
*Liangze Jiang,Zachary Shinnick,Anton van den Hengel,Hemanth Saratchandran,Damien Teney*

Main category: cs.CL

TL;DR: 本文提出了一种新的预训练范式：先用抽象结构化数据（如Dyck序列等程序性数据）进行少量预训练，再进行常规语言预训练，显著提升模型性能并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 受人类先学逻辑数学再学高级推理的启发，探索在自然语言预训练前引入抽象结构化数据，以更高效地构建语义与推理能力。

Method: 系统诊断不同程序性数据对算法能力的影响；在1.3B规模模型上验证仅0.1%程序性数据前置预训练的效果；分析注意力与MLP层的结构变化机制；探索多类程序性数据组合策略。

Result: 在Needle-in-a-haystack任务中准确率从10%升至98%；达到相同loss所需数据量减少至55%-86%；注意力层结构更适配代码等结构化任务，MLP层更适配语言任务。

Conclusion: 程序性预训练是一种轻量、有效的方法，能提升性能、加速训练，并提示知识获取与推理能力可解耦，为LLM训练提供新思路。

Abstract: Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.
  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.

</details>


### [138] [CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering](https://arxiv.org/abs/2601.21733)
*Jiayin Lan,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin,Guoping Hu*

Main category: cs.CL

TL;DR: 本文提出CE-GOCD方法，通过以论文标题为中枢实体构建并优化学术知识图谱子图，结合子图剪枝补全与社区发现，提升大语言模型在科研文献问答中的语义理解与回答质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法忽视论文间深层语义关联，导致大语言模型对科学文献理解不充分，影响回答的全面性与准确性。

Method: 提出中央实体引导的图优化社区检测（CE-GOCD）：1）以论文标题为中枢实体进行子图检索；2）通过子图剪枝与补全增强隐式语义发现；3）应用社区检测提炼主题一致的论文群组。

Result: 在三个NLP文献问答数据集上，CE-GOCD显著优于其他检索增强基线方法。

Conclusion: 显式建模和利用学术知识图谱中的语义子结构，能有效提升大语言模型在科学问答任务中的性能。

Abstract: Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.

</details>


### [139] [Temporal Guidance for Large Language Models](https://arxiv.org/abs/2601.21744)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的时序对比引导方法TeGu，利用多令牌预测（MTP）构建模型自身弱预测以实现自对比，配合轻量级条件MTP投影器（cMTPP），在低开销下显著提升LLM生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有对比解码方法（如CD、DoLa）存在计算开销大或在小模型上不稳定的问题；作者观察到LLM具有局部偏好性，因而探索沿时间维度的自对比机制。

Method: 提出时序引导（TeGu）策略，利用多令牌预测（MTP）生成弱预测作为对比源，并设计轻量级条件MTP投影器（cMTPP）统一实现，避免维护多个独立网络。

Result: TeGu在多个模型系列和基准测试中显著提升生成质量，同时保持低内存占用与低计算开销。

Conclusion: TeGu是一种高效稳定的自对比解码方法，为降低对比解码开销并提升小模型鲁棒性提供了新思路。

Abstract: Contrastive Decoding (CD) enhances the generation quality of large language models (LLMs) but incurs significant additional computational overhead due to the need for an auxiliary model. Existing internal self-contrastive decoding methods, such as Decoding by Contrasting Layers (DoLa), focus on discrepancies across different layers, which are notably unstable on small-scale models. In this work, based on the observation that LLMs exhibit local preferences, we propose a novel contrastive guidance strategy along the temporal dimension, namely Temporal Guidance (TeGu). Our method ingeniously leverages Multi-Token Prediction (MTP) to construct weaker amateur predictions for model self-contrast. To standardize the implementation of this mechanism, we further introduce a lightweight Conditional MTP Projector (cMTPP), which avoids maintaining multiple independent networks as required by other MTP modules. Across various model series and benchmarks, TeGu achieves significant performance improvements while maintaining low additional memory consumption and computational overhead.

</details>


### [140] [CoFrGeNet: Continued Fraction Architectures for Language Generation](https://arxiv.org/abs/2601.21766)
*Amit Dhurandhar,Vijil Chenthamarakshan,Dennis Wei,Tejaswini Pedapati,Karthikeyan Natesan Ramamurthy,Rahul Nair*

Main category: cs.CL

TL;DR: 本文提出了一种基于连分数理论的新型生成模型函数类CoFrGeNets，可作为Transformer中多头注意力和前馈网络的轻量级插件式替代方案，在减少参数量（1/2~2/3）和训练时间的同时，保持甚至提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 受连分数数学结构启发，探索比传统Transformer更高效、参数更少但表达能力更强的生成建模函数类，以缓解大模型高计算与存储开销问题。

Method: 提出CoFrGeNets函数类及对应架构组件，替代Transformer中的Multi-head Attention和Feed-Forward Networks；推导定制化梯度公式以提升优化精度与效率；支持即插即用，兼容现有训练/推理流程。

Result: 在GPT2-xl（1.5B）和Llama3（3.2B）上验证：使用CoFrGeNets后参数减少至原模型的1/2~2/3、预训练时间缩短，下游分类、问答、推理、文本理解等任务性能与原模型相当甚至更优。

Conclusion: CoFrGeNets为生成建模提供了有前景的新函数范式，具备显著的参数与训练效率优势，且易于工业部署；硬件定制化实现有望进一步释放其潜力。

Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.

</details>


### [141] [Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond](https://arxiv.org/abs/2601.21767)
*Wei Zhu*

Main category: cs.CL

TL;DR: 本文系统评估了ChatGPT在6个基准数据集上的4类医学信息抽取（MedIE）任务中的性能、可解释性、置信度、忠实性和不确定性，发现其整体表现逊于微调基线模型，虽能提供高质量解释且多数情况下忠实于原文，但存在过度自信和生成不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（如ChatGPT）在医学信息抽取（MedIE）任务中的综合能力，包括性能、可解释性、置信度、忠实性和不确定性，以探究其在专业NLP任务中的适用性与局限性。

Method: 在6个基准数据集上对ChatGPT进行4类医学信息抽取任务的系统性实验评估，量化其性能、解释质量、置信度、文本忠实度及输出不确定性。

Result: （a）ChatGPT在MedIE任务上的性能低于微调基线模型；（b）能提供高质量决策解释，但预测过度自信；（c）多数情况下对原文具有高忠实度；（d）生成不确定性导致信息抽取结果不稳定，限制其实际应用。

Conclusion: 尽管ChatGPT在医学信息抽取中展现出一定解释能力和文本忠实性，但其性能不足、过度自信和生成不确定性制约了其在关键医疗场景中的可靠部署，需进一步校准与增强。

Abstract: Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT's performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT's performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.

</details>


### [142] [Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention](https://arxiv.org/abs/2601.21768)
*Alon Rozental*

Main category: cs.CL

TL;DR: 本文提出Zonkey，一种基于分层扩散的可微分tokenizer框架，用以替代传统固定、不可微的BPE等tokenizers，实现从原始字符到文档级表示的端到端可训练语言建模。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型受限于固定、非可微的tokenizers（如BPE），难以端到端优化，且对噪声或领域特异数据适应性差。

Method: 提出Zonkey：包含可微分Segment Splitter（学习概率性BOS决策）、Probabilistic Attention（引入位置存在概率实现软掩码与梯度保持）、分层抽象压缩机制、Denoising Diffusion Mixed Model（DDMM）用于潜在空间稳定去噪，以及Stitcher保障片段重叠不变性。

Result: 在Wikipedia上端到端训练后，Zonkey能从噪声生成连贯、变长文本，展现出涌现的语言层级结构（如空格处为词边界、句号处为句首），定性表现优于基于熵的可学习tokenizer。

Conclusion: Zonkey推动了全梯度式LLM的发展，提升了领域自适应能力与生成可扩展性，并开源代码。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.

</details>


### [143] [KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection](https://arxiv.org/abs/2601.21796)
*Yaocong Li,Leihan Zhang,Le Zhang,Qiang Yan*

Main category: cs.CL

TL;DR: 本文提出KID框架，通过知识注入和双头学习实现有害模因检测，显著提升多语言模因毒性识别性能。


<details>
  <summary>Details</summary>
Motivation: 模因依赖隐喻与社会文化背景，导致其有害内容难以被现有基于模态信号分析的方法准确识别，亟需引入外部知识辅助理解。

Method: 提出知识注入的双头学习框架KID，采用标签约束蒸馏范式构建视觉证据、背景知识与分类标签间的结构化推理链，并通过双头架构联合优化语义生成与分类目标。

Result: 在涵盖英语、中文和低资源孟加拉语的五个多语言数据集上，KID在二分类与多标签有害模因检测任务中均达到SOTA，主指标提升2.1%–19.7%；消融实验验证了知识注入与双头学习的有效性与互补性。

Conclusion: KID通过将外部知识显式地融入模因理解过程，提升了模型对隐含毒性的感知能力，为跨语言、鲁棒且可泛化的有害模因检测提供了新范式。

Abstract: Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.

</details>


### [144] [Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation](https://arxiv.org/abs/2601.21797)
*Yimin Deng,Yuqing Fu,Derong Xu,Yejing Wang,Wei Ni,Jingtong Gao,Xiaopeng Li,Chengxu Liu,Xiao Han,Guoshuai Zhao,Xiangyu Zhao,Li Zhu,Xueming Qian*

Main category: cs.CL

TL;DR: 本文提出了一种对抗式记忆适应机制（AMA），通过模拟下游任务执行，为离线记忆构建与更新提供任务感知的监督信号，从而提升长对话中对话代理的记忆有效性。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统在离线阶段采用固定、任务无关的记忆构建与更新策略，导致其与下游任务需求不匹配，影响性能。

Method: AMA机制包含三个智能体：挑战者生成问答对以模拟任务推理；评估者分析回答错误；适配者据此对记忆构建策略和内容进行双层级更新。

Result: AMA可即插即用地集成到多种现有记忆系统中，在长对话基准LoCoMo上验证了其显著提升下游任务性能的有效性。

Conclusion: AMA通过将任务目标引入离线记忆阶段，实现了记忆系统与具体任务的对齐，有效缓解了上下文窗口限制带来的长对话建模难题。

Abstract: Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.

</details>


### [145] [RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes](https://arxiv.org/abs/2601.21803)
*Korbinian Randl,Guido Rocchietti,Aron Henriksson,Ziawasch Abedjan,Tony Lindgren,John Pavlopoulos*

Main category: cs.CL

TL;DR: 本文提出RAG-E框架，通过数学可解释方法量化检索器与生成器之间的对齐程度，发现二者存在严重错配，揭示了RAG系统性能不仅取决于单个模块，更依赖其协同作用。


<details>
  <summary>Details</summary>
Motivation: RAG系统在高风险领域部署受限于其检索器与生成器交互过程的不透明性，亟需可解释性分析框架来审计组件间协同质量。

Method: 提出端到端可解释框架RAG-E：1）适配Integrated Gradients分析检索器；2）提出蒙特卡洛稳定化的Shapley值近似方法PMCSHAP用于生成器归因；3）设计加权归因-相关性差距（WARG）指标衡量生成器对检索排序的遵循程度。

Result: 在TREC CAsT和FoodSafeSum数据集上发现：47.4%–66.7%的查询中生成器忽略检索器首选文档，48.1%–65.9%依赖低相关性文档，证实检索与生成存在显著错配。

Conclusion: RAG系统输出质量高度依赖检索器与生成器的协同对齐，而非各自独立性能；RAG-E为该对齐关系提供了可量化、可审计的解释性评估手段。

Abstract: Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.

</details>


### [146] [Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning](https://arxiv.org/abs/2601.21804)
*Bodong Du,Xuanqi Huang,Xiaomeng Li*

Main category: cs.CL

TL;DR: 本文提出了一种分布感知的奖励估计方法DARE，用于提升测试时强化学习（TTRL）中无监督奖励估计的鲁棒性与有效性，显著提升了大语言模型在推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTRL方法依赖多数投票（MV）生成确定性奖励，但该假设脆弱：MV忽略非主流但正确的动作候选，导致奖励估计系统性偏差。

Method: 提出Distribution-Aware Reward Estimation（DARE），将奖励估计从单一多数结果扩展到完整经验 rollout 分布，并引入探索奖励和分布剪枝机制以增强非主流rollout探索并去噪。

Result: 在AIME 2024和AMC等推理基准上，DARE相较基线分别取得25.3%和5.3%的相对性能提升，同时优化更稳定。

Conclusion: 基于完整 rollout 分布而非单一多数结果进行奖励估计，能更有效地支持TTRL中的自改进过程，DARE为无监督TTRL提供了更鲁棒、信息更丰富的奖励信号。

Abstract: Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.

</details>


### [147] [Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models](https://arxiv.org/abs/2601.21826)
*Aadi Palnitkar,Mingyang Mao,Nicholas Waytowich,Vinicius G. Goecks,Tinoosh Mohsenin,Xiaomin Lin*

Main category: cs.CL

TL;DR: 本文介绍了MilSCORE，一个针对军事规划场景的长上下文、多模态基准数据集，用于评估大语言模型在高风险决策和空间推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准缺乏对真实复杂任务（如大规模军事行动规划）中选择性阅读与异构多源信息整合能力的评估需求。

Method: 构建了首个专家撰写的、基于复杂模拟军事规划场景的多跳问答数据集MilSCORE，涵盖七类问题，结合地图、命令、情报报告等多模态长上下文，并设计相应评估协议。

Result: 在多个先进视觉-语言模型上的基线实验表明，当前模型在MilSCORE上表现不佳，存在显著提升空间。

Conclusion: MilSCORE填补了现实场景级长上下文规划评估的空白，为未来研究提供了具有挑战性的新测试平台。

Abstract: As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.

</details>


### [148] [Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model](https://arxiv.org/abs/2601.21841)
*Xiang Li,Ning Yan,Masood Mortazavi*

Main category: cs.CL

TL;DR: 本文提出GiG框架，通过Graph-in-Graph结构和图神经网络建模环境状态与执行轨迹，结合结构感知检索与有界前向推理模块，显著提升具身智能体在长视野规划任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型作为具身智能体时，在长视野规划中难以保持策略一致性或易违反环境约束，主因是上下文限制与缺乏对动态环境逻辑的显式建模。

Method: 提出Graph-in-Graph（GiG）规划框架：1）用GNN编码环境状态为嵌入；2）构建动作连接的执行轨迹图存入经验记忆库；3）聚类图嵌入以实现结构感知检索；4）引入基于符号转移逻辑的有界前向推理模块进行接地动作投影。

Result: 在Robotouille同步/异步及ALFWorld三个具身规划基准上，Pass@1指标分别提升22%、37%和15%，计算开销相当或更低。

Conclusion: GiG通过显式结构化记忆与符号逻辑增强，有效缓解LLM在具身长视野规划中的连贯性与可行性问题，为具身智能提供新范式。

Abstract: While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.

</details>


### [149] [Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text](https://arxiv.org/abs/2601.21895)
*Hongyi Zhou,Jin Zhu,Erhan Xu,Kai Ye,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 本文提出了一种基于重写（rewrite）的新型大语言模型（LLM）生成文本检测算法，通过几何视角解释重写类检测方法的原理，并引入自适应学习原文与重写文之间距离的机制，在理论和实验上均证明其优于固定距离方法，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: LLM生成高度拟人化文本引发虚假信息与学术诚信问题，亟需可靠的检测算法。

Method: 提出一种基于重写的检测算法，利用几何视角理解重写机制，并自适应地学习原文与重写文本间的语义/表征距离；理论分析证明自适应距离优于固定距离。

Result: 在超100种实验设置下，该方法在大多数场景中显著优于基线；对GPT、Claude、Gemini等目标LLM，相对最强基线提升57.8%–80.6%。

Conclusion: 自适应距离建模是提升重写类LLM检测器性能的关键，几何视角为该类方法提供了可解释的理论基础。

Abstract: Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\% to 80.6\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).

</details>


### [150] [SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching](https://arxiv.org/abs/2601.21927)
*Hong Chen,Xiang Liu,Bo Wang,Yuxuan Fan,Yuanlin Chu,Zongluo Li,Xiaowen Chu,Xuming Hu*

Main category: cs.CL

TL;DR: SONIC是一种基于学习的KV缓存压缩框架，通过生成语义丰富的Nexus token压缩多轮对话历史，在保持对话连贯性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法忽视多轮对话的结构特性，依赖启发式剔除，易丢失关键上下文，难以适应不同内存约束。

Method: 提出SONIC框架，将历史对话段压缩为紧凑且语义丰富的Nexus token，并引入动态预算训练机制以灵活适配不同内存限制，无需重新训练。

Result: 在80%和50%压缩比下，SONIC在四个多轮对话基准上持续优于H2O、StreamingLLM等基线；在MTBench101上平均得分提升35.55%，推理速度提升50.1%。

Conclusion: SONIC有效缓解了KV缓存线性增长带来的部署瓶颈，在保证多轮对话质量的同时大幅提升推理效率与内存适应性。

Abstract: The linear growth of Key-Value (KV) cache remains a bottleneck for multi-turn LLM deployment. Existing KV cache compression methods often fail to account for the structural properties of multi-turn dialogues, relying on heuristic eviction that risks losing critical context. We propose \textbf{SONIC}, a learning-based framework that compresses historical segments into compact and semantically rich \textbf{Nexus} tokens. By integrating dynamic budget training, SONIC allows flexible adaptation to varying memory constraints without retraining. Experiments show that at compression ratios of 80\% and 50\%, SONIC consistently outperforms baselines such as H2O and StreamingLLM on four diverse multi-turn benchmarks. Specifically, on the widely used MTBench101 benchmark, SONIC achieves an average score improvement of 35.55\% over state-of-the-art baselines, validating its effectiveness in sustaining coherent multi-turn dialogues. Furthermore, SONIC enhances deployment efficiency, accelerating the overall inference process by 50.1\% compared to full-context generation.

</details>


### [151] [From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes](https://arxiv.org/abs/2601.21955)
*Fariba Afrin Irany*

Main category: cs.CL

TL;DR: 本文提出了一种基于GPT的临床文本分类架构，通过选择性微调（仅更新最后Transformer块、最终层归一化和轻量分类头）来应对EHR中长文本、标注少、类别不平衡和计算成本高的挑战，在MIMIC-IV-Note放射报告上验证了其在多标签、二分类及预后预测任务中的高效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 建模长且领域特定的临床文本面临标注数据少、类别严重不平衡、大语言模型适配计算成本高等挑战。

Method: 采用选择性微调策略：冻结大部分GPT-2主干网络，仅训练最后Transformer块、最后层归一化和轻量分类头。

Result: 在MIMIC-IV-Note放射报告上，对多标签分类、不同不确定性假设下的二分类及疾病结局预测等任务均表现出稳定收敛与优异性能，尤其在非提及和否定性发现占主导的场景下效果突出。

Conclusion: 选择性微调预训练生成式语言模型是临床文本分类的一种高效、有效且可扩展的路径，显著降低计算复杂度，同时保持强表征能力。

Abstract: The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models.
  This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language.
  The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings.
  Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.

</details>


### [152] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出On-policy Verbal Distillation (OVD)，一种无需token级对齐、基于离散言语评分（0–9）进行轨迹匹配的知识蒸馏框架，显著降低内存消耗并提升学生模型在Web问答与数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有on-policy token-level蒸馏方法受限于token级对齐，抑制学生探索能力、难以利用环境反馈，且在强化学习中存在严重内存瓶颈。

Method: 提出OVD框架，用教师模型生成的离散言语评分（0–9）替代传统token级概率匹配，实现轨迹级匹配；摆脱token对齐约束，支持学生自由探索输出空间，并提升内存效率。

Result: 在Web问答和数学推理任务上大幅超越现有方法：Web Q&A平均EM提升最高达+12.9%，数学基准提升最高达+25.7%（仅用1个随机样本训练），且训练效率更优。

Conclusion: OVD是一种高效、灵活、低内存开销的on-policy知识蒸馏新范式，适用于需要交互反馈与探索能力的推理任务。

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


### [153] [Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding](https://arxiv.org/abs/2601.21969)
*Yifan Zhu,Huiqiang Rong,Haoran Luo*

Main category: cs.CL

TL;DR: 本文提出Token-Guard，一种基于自检解码的词元级幻觉控制方法，通过在每个推理步骤进行内部验证、潜在空间风险评分、迭代剪枝与重生成，显著降低大语言模型的幻觉现象，且无需大规模微调或外部检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常出现幻觉问题，现有缓解方法（如RAG、RLHF）资源消耗大，而解码类方法缺乏显式幻觉控制机制。

Method: Token-Guard采用自检解码框架：在每步生成中对候选词元片段进行内部验证；在潜在空间中进行显式幻觉风险评分；并结合迭代剪枝与再生机制动态修正错误。

Result: 在HALU数据集上的实验表明，Token-Guard显著降低了幻觉率，提升了生成准确性，具备可扩展性和模块化优势。

Conclusion: Token-Guard为大语言模型提供了轻量、高效、可控的幻觉抑制新范式，无需额外检索或大规模训练，代码已开源。

Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.

</details>


### [154] [Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units](https://arxiv.org/abs/2601.21996)
*Jianhui Chen,Yuzhang Luo,Liangming Pan*

Main category: cs.CL

TL;DR: 本文提出了机制数据归因（MDA）框架，利用影响函数追溯LLM中可解释单元的训练数据来源，并通过实验证明特定结构化数据（如LaTeX、XML）对可解释头（如归纳头）的形成具有因果催化作用，同时揭示了归纳头与上下文学习能力之间的功能联系，并提出了一种加速电路收敛的数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 尽管机制可解释性已识别出大语言模型中的可解释电路，但这些电路在训练数据中的因果起源仍不清楚。

Method: 提出机制数据归因（MDA）框架，结合影响函数进行可解释单元到训练样本的溯源，并通过针对性增删高影响样本开展因果干预实验。

Result: 发现重复性结构化数据（如LaTeX、XML）是可解释头形成的机制催化剂；干预归纳头相关样本会同步改变模型的上下文学习能力；所提数据增强方法能稳定加速不同规模模型中电路的收敛。

Conclusion: MDA为理解LLM内部机制的数据根源提供了可扩展因果分析工具，证实了结构化数据对电路形成的驱动作用，并建立了归纳头与ICL之间的直接因果联系，支持了对LLM发育轨迹进行数据层面引导的可行性。

Abstract: While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.

</details>


### [155] [Causal Autoregressive Diffusion Language Model](https://arxiv.org/abs/2601.22031)
*Junhao Ruan,Bei Li,Yongjing Yin,Pengcheng Huang,Xin Chen,Jingang Wang,Xunliang Cai,Tong Xiao,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出Causal Autoregressive Diffusion (CARD)，一种融合自回归模型训练效率与扩散模型高吞吐推理能力的新框架，通过因果注意力掩码实现单次前向传播的密集监督，并引入软尾掩码和信噪比驱动的重加权机制以稳定训练，支持基于置信度的动态并行解码，在保持ARM级数据效率的同时显著降低训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型（ARMs）推理吞吐低与扩散模型训练不稳定、效率不高的问题，寻求兼具训练高效性与推理高吞吐的新范式。

Method: 提出CARD框架：1）在严格因果注意力掩码下重构扩散过程；2）设计软尾掩码以保留局部上下文；3）基于信噪比原理构建上下文感知重加权机制；4）利用KV缓存实现动态并行解码。

Result: CARD在离散扩散基线上性能更优，训练延迟比块扩散方法降低3倍，同时达到ARM级数据效率与并行生成的延迟优势。

Conclusion: CARD为下一代高效大语言模型提供了一种兼顾训练效率、推理吞吐与优化稳定性的新范式。

Abstract: In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.

</details>


### [156] [Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models](https://arxiv.org/abs/2601.22035)
*Longxuan Yu,Yu Fu,Shaorong Zhang,Hui Liu,Mukund Varma T,Greg Ver Steeg,Yue Dong*

Main category: cs.CL

TL;DR: 本文提出掩码扩散语言模型（MDLMs）可解决自回归模型在输出顺序与推理逻辑不一致时的“过早承诺”问题，展现出对输出顺序变化的鲁棒性（order robustness），并在多个数学推理基准上验证了其优势及机制。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型强制从左到右生成，当任务要求先输出答案再给出推理（如格式或模式约束）时，会因过早承诺答案而损害性能；需一种能解耦计算顺序与输出结构的新建模范式。

Method: 采用掩码扩散语言模型（MDLMs），通过并行迭代优化所有token；构建新基准ReasonOrderQA以可控方式评估不同输出顺序下的表现；分析MDLMs在扩散过程中不同token（推理步 vs. 最终答案）的稳定化时序。

Result: 在GSM8K、Math500和ReasonOrderQA上，AR模型在答案前置设置下相对准确率下降高达67%，而MDLMs下降≤14%；实证表明MDLMs更早稳定简单token（如推理步骤），从而支持先推理后定答案；同时识别出该优势失效的边界条件。

Conclusion: MDLMs具备'顺序鲁棒性'，为结构受限或逻辑优先的任务提供了更灵活可靠的生成范式，但其有效性依赖于token复杂度差异与扩散动态的匹配。

Abstract: Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\leq$14% relative drop), a property we term "order robustness". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.

</details>


### [157] [A Separable Architecture for Continuous Token Representation in Language Models](https://arxiv.org/abs/2601.22040)
*Reza T. Batley,Sourav Saha*

Main category: cs.CL

TL;DR: 本文提出Leviathan架构，通过连续嵌入生成器替代传统离散查找表，在小语言模型中显著提升参数利用效率，实验证明其等效参数量达标准模型的1.47–2.11倍。


<details>
  <summary>Details</summary>
Motivation: 在小语言模型（SLM）中，嵌入矩阵占据大部分参数，但这种分配方式既反直觉又次优，而现有缩放定律将参数视为可互换，忽略了嵌入层的特殊性。

Method: 提出Leviathan架构，用连续嵌入生成器替代传统离散嵌入查找表，并在Pile数据集上以等参数量设置与LLaMA风格模型对比，通过经验幂律拟合评估有效参数容量。

Result: Leviathan在等参数条件下持续优于标准LLaMA式架构，其有效参数容量相当于密集模型的1.47至2.11倍。

Conclusion: 嵌入层不应被简单视为可互换参数的一部分；采用连续嵌入生成器能更高效地利用参数预算，尤其对子十亿参数规模模型具有重要意义。

Abstract: Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \times$ more parameters.

</details>


### [158] [On the Paradoxical Interference between Instruction-Following and Task Solving](https://arxiv.org/abs/2601.22047)
*Yunjia Qi,Hao Peng,Xintong Shi,Amy Xin,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文揭示了指令遵循可能反而干扰大语言模型任务解决能力的反直觉现象，并提出SUSTAINSCORE指标来量化这种干扰，通过在指令中加入自明约束并测量性能下降来评估；实验表明即使先进模型也受显著影响，且失败案例更关注约束；研究还探讨了不同后训练范式对干扰的影响。


<details>
  <summary>Details</summary>
Motivation: 揭示指令遵循可能干扰LLM任务求解能力这一被忽视的反直觉现象，并建立可量化的评估指标。

Method: 提出SUSTAINSCORE指标，通过向指令中插入从原始成功输出中提取的自明约束，测量任务性能下降程度；在数学、多跳问答和代码生成任务上开展实验，分析注意力分配与失败模式，并比较不同后训练范式的影响。

Result: 添加自明约束导致各任务性能显著下降，包括Claude-Sonnet-4.5等先进模型；失败案例在约束上分配更多注意力；不同对齐策略表现出差异化的干扰程度。

Conclusion: 指令遵循并非总是有益，其本身可能损害模型任务求解能力；SUSTAINSCORE为评估和改进对齐方法提供了新视角与工具。

Abstract: Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research

</details>


### [159] [MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs](https://arxiv.org/abs/2601.22050)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本文介绍了MasalBench，一个用于评估多语言大语言模型（LLMs）对波斯语谚语语境及跨文化理解能力的基准测试，发现现有模型在识别波斯谚语方面表现良好，但在跨语言类比（如匹配对应英语谚语）上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注高资源语言中LLM对修辞语言的理解，而低资源语言（如波斯语）中的跨文化、语境化理解能力尚未被充分探索。

Method: 构建了MasalBench基准，包含波斯语谚语的语境识别与对应英语谚语匹配任务，并在8个主流多语言LLM上进行评估。

Result: 所有模型在波斯谚语识别任务中准确率均超0.90；但在匹配等价英语谚语任务中，最佳模型准确率仅为0.79。

Conclusion: 当前多语言LLM在文化知识和类比推理方面存在显著局限，MasalBench为评估其他低资源语言的跨文化理解能力提供了可复用框架。

Abstract: In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.

</details>


### [160] [$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA](https://arxiv.org/abs/2601.22055)
*Yaxin Du,Junru Song,Yifan Zhou,Cheng Wang,Jiahao Gu,Zimeng Chen,Menglan Chen,Wen Yao,Yang Yang,Ying Wen,Siheng Chen*

Main category: cs.CL

TL;DR: 本文提出G²-Reader双图系统，通过内容图保持文档原生结构与跨模态语义，通过规划图以有向无环子问题图实现证据逐步检索与整合，在VisDoMBench多模态基准上显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成在多模态长文档问答中存在两大问题：扁平分块破坏文档结构和跨模态对齐；迭代检索缺乏全局搜索状态，易陷入局部循环或漂移至无关区域。

Method: 提出G²-Reader双图系统：1）Content Graph建模文档原生结构与图文表跨模态语义关系；2）Planning Graph作为智能体驱动的有向无环子问题图，持续跟踪中间发现并指导逐步证据导航。

Result: 在VisDoMBench五个多模态领域上，G²-Reader（基于Qwen3-VL-32B-Instruct）平均准确率达66.21%，显著优于强基线及独立GPT-5（53.08%）。

Conclusion: 双图协同建模（结构保持+目标导向规划）是提升长多模态文档问答鲁棒性与准确性的有效范式。

Abstract: Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\%).

</details>


### [161] [VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning](https://arxiv.org/abs/2601.22069)
*Yibo Wang,Yongcheng Jing,Shunyu Liu,Hao Guan,Rong-cheng Tu,Chengyu Wang,Jun Huang,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出VTC-R1，一种将中间推理过程渲染为图像并作为‘光学记忆’输入视觉语言模型的新推理范式，显著提升长上下文推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理虽增强LLM能力，但带来严重计算开销；现有高效方法依赖额外训练或外部模型压缩，可扩展性差且丢失细粒度信息。

Method: 提出VTC-R1范式：将文本推理段渲染为紧凑图像，迭代输入视觉语言模型（如Glyph、Qwen3-VL）；基于OpenR1-Math-220K构建训练数据并微调模型。

Result: 在MATH500、AIME25、AMC23和GPQA-D等基准上持续超越标准长上下文推理；端到端延迟提升2.7倍，实现3.4倍token压缩。

Conclusion: VTC-R1是一种可扩展、高效且信息保留良好的长上下文推理新方案，适用于推理密集型应用。

Abstract: Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as "optical memory." We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.

</details>


### [162] [ECO: Quantized Training without Full-Precision Master Weights](https://arxiv.org/abs/2601.22101)
*Mahdi Nikdan,Amir Zandieh,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 本文提出Error-Compensating Optimizer（ECO），通过误差反馈机制在无主权重（master weights）下直接对量化参数进行更新，显著降低LLM训练内存开销，尤其适用于Sparse MoE模型，并在多种规模和精度设置下实现与基线近似无损的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化训练方法仍需高精度主权重缓冲区，造成显著内存开销，尤其在Sparse Mixture of Experts（SMoE）模型中成为瓶颈。

Method: ECO直接在量化参数上应用梯度更新，将每次权重量化引入的误差注入优化器动量中，构建零额外内存开销的误差反馈回路，并在标准假设和衰减学习率下给出收敛性证明。

Result: 在30M–16B规模模型（含Gemma-3 1B、2.1B Sparse MoE、DeepSeek-MoE-16B）及FP8/INT4精度下，ECO达到与含主权重基线近损失less的验证损失，显著改善内存-损失Pareto前沿。

Conclusion: ECO成功消除了量化训练中对主权重的依赖，在不牺牲模型性能的前提下大幅降低内存占用，为大规模稀疏模型高效训练提供了新范式。

Abstract: Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.

</details>


### [163] [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)
*Anran Li,Yuanyuan Chen,Wenjun Long,Yu Yin,Yan Hu,Hyunjae Kim,Weipeng Zhou,Yujia Zhou,Hongyi Peng,Yang Ren,Xuguang Ai,Zhenyue Qin,Ming Hu,Xiaoxiao Li,Han Yu,Yih-Chung Tham,Lucila Ohno-Machado,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本文提出Fed-MedLoRA及其增强版Fed-MedLoRA+，一种模型无关、参数高效的联邦学习框架，用于在医疗场景下高效适配大语言模型（LLMs），解决传统联邦学习在通信开销大和临床数据异质性高两方面的瓶颈，并在临床信息抽取任务上验证了其有效性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型多基于单机构数据训练，泛化性与安全性受限；而传统联邦学习应用于LLM时面临通信开销大（需传输全模型）和难以应对临床数据高度异质性（跨患者、疾病、机构）两大根本挑战。

Method: 提出Fed-MedLoRA：采用低秩适配器（LoRA）实现参数高效联邦微调，仅传输adapter参数以降低通信与计算开销；进一步提出Fed-MedLoRA+，引入自适应、数据感知的聚合策略，提升跨站点异质数据下的收敛性能。在临床信息抽取任务上进行验证。

Result: 在五组患者队列上评估显示，Fed-MedLoRA(+)显著优于BERT基线，并在in-domain、external validation及low-resource新站点适配（耶鲁纽黑文健康系统真实病历）三种设置下，性能接近或媲美LLaMA-3、DeepSeek-R1和GPT-4o等强基线。

Conclusion: Fed-MedLoRA(+)为医疗大模型的隐私保护、协作式适配提供了切实可行的联邦学习新范式，兼顾效率、鲁棒性与泛化性，推动LLM在真实异构医疗系统中的安全落地。

Abstract: Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.

</details>


### [164] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型推理范式——主动交互式推理（PIR），使模型能主动向用户提问以澄清前提或意图的不确定性，而非盲目内部推理；通过不确定性感知微调和用户模拟器驱动的策略优化实现，在数学推理、代码生成等任务上显著提升准确率与效率，并减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: 现有基于思维链（CoT）的推理范式存在“盲目自思考”缺陷：当关键信息缺失或模糊时仍进行大量内部推理，导致低效与错误。

Method: 提出Proactive Interactive Reasoning（PIR）范式，包含两个核心组件：（1）不确定性感知的监督微调，赋予模型交互推理能力；（2）基于用户模拟器与复合奖励的策略优化框架，对齐用户意图。

Result: 在数学推理、代码生成、文档编辑等任务中，PIR相较强基线最高提升32.70%准确率、22.90%通过率、41.36 BLEU分，同时减少近一半推理计算量和无效交互轮次；在事实知识、问答及缺前提场景中也展现出强泛化性与鲁棒性。

Conclusion: PIR成功将LLM从被动求解者转变为主动探询者，有效应对前提与意图层面的不确定性，是一种更高效、可靠、用户对齐的新推理范式。

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [165] [FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale](https://arxiv.org/abs/2601.22146)
*Ajay Patel,Colin Raffel,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本文提出FineInstructions方法，利用互联网规模的预训练文档生成数十亿条合成指令-答案对，用于从零开始仅通过指令微调目标预训练大语言模型，显著提升其自由生成响应质量。


<details>
  <summary>Details</summary>
Motivation: 由于监督训练数据有限，现有大语言模型依赖大量无结构文本进行自监督预训练，再用少量指令微调数据进行后训练；本文旨在克服监督数据稀缺问题，使预训练更贴近下游实际使用场景（即响应用户提示）。

Method: 提出一种将预训练文档知识转化为合成指令-答案对的方法：基于真实用户查询构建约1800万条指令模板，并将其与预训练语料中的人类撰写文档匹配并实例化，构建FineInstructions数据集；然后仅用该数据集以指令微调目标从头预训练LLM。

Result: 在严格控制token数量的对比实验中，基于FineInstructions的预训练在标准自由生成响应质量评测基准上，优于传统预训练及其他合成预训练方法。

Conclusion: 仅用大规模合成指令数据进行预训练是可行且有效的，能更好对齐LLM的预训练目标与实际应用目标，为减少对人工标注数据的依赖提供了新路径。

Abstract: Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .

</details>


### [166] [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149)
*Hang Ding,Peidong Liu,Junqiao Wang,Ziwei Ji,Meng Cao,Rongzhao Zhang,Lynn Ai,Eric Yang,Tianyu Shi,Lei Yu*

Main category: cs.CL

TL;DR: 本文提出DynaWeb，一种基于模型的强化学习框架，通过构建网页世界模型来模拟真实网页环境，使Web代理能够在合成环境中进行大量策略 rollout，从而提升在线强化学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 训练自主Web代理面临与真实互联网交互的低效、高成本和高风险问题，需要一种更安全高效的训练方法。

Method: 提出DynaWeb框架：1）训练一个能根据代理动作预测自然网页表示的网页世界模型；2）在该模型生成的合成网页环境中进行策略‘梦境’rollout；3）将真实专家轨迹随机穿插到on-policy rollout中以增强训练稳定性和样本效率。

Result: 在WebArena和WebVoyager基准测试中，DynaWeb显著提升了当前主流开源Web代理模型的性能。

Conclusion: 证明了通过‘想象’（即基于世界模型的模拟交互）训练Web代理的可行性，为规模化在线代理式强化学习提供了高效、可扩展的新路径。

Abstract: The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.

</details>


### [167] [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)
*Yingfa Chen,Zhen Leng Thai,Zihan Zhou,Zhu Zhang,Xingyu Shen,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出HALO方法将Transformer模型蒸馏为RNN-注意力混合模型，并设计HypeNet架构，通过新型位置编码HyPE和结构改进提升长上下文泛化能力；仅用2.3B tokens（原预训练数据的0.01%）即可实现媲美原模型的性能，同时显著提升长上下文效率。


<details>
  <summary>Details</summary>
Motivation: 现有混合Transformer模型因需从头大规模预训练而难以推广；已有参数迁移与知识蒸馏方法依赖海量数据（>10B tokens）且长上下文性能差，未能发挥其推理加速优势。

Method: 提出HALO（Hybrid Attention via Layer Optimization）蒸馏流程，将预训练Transformer（如Qwen3）转化为混合模型；设计HypeNet架构，引入新型位置编码HyPE及多项结构优化以增强长度泛化能力。

Result: 在仅使用2.3B tokens（不足原预训练数据0.01%）条件下，成功将Qwen3系列转化为HypeNet，在保持原始Transformer性能的同时，显著提升长上下文建模性能与推理效率。

Conclusion: HALO+HypeNet提供了一种高效、低数据依赖的Transformer到混合模型转化范式，解决了长上下文场景下性能与效率难以兼顾的关键瓶颈。

Abstract: Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 本文提出了一种自改进、可选缓存的Planner-Auditor框架，通过将生成与确定性验证解耦，并引入目标重放机制，提升大语言模型在临床出院计划任务中的安全性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在临床出院规划中潜力巨大，但受限于幻觉、遗漏和置信度校准不准等问题。

Method: 构建了一个基于FHIR的代理式回溯评估流水线：Planner（LLM）生成带置信度估计的结构化出院计划；Auditor为确定性模块，评估多任务覆盖、校准质量（Brier分数、ECE代理指标）及动作分布漂移；支持两类自改进机制——单次会话内再生与跨会话差异缓冲+高置信低覆盖案例重放。

Result: 自改进循环为主导增益来源，任务覆盖率从32%提升至86%；校准显著改善（Brier/ECE下降、高置信错误减少）；差异缓冲进一步修正了持续性高置信遗漏。

Conclusion: Planner-Auditor框架通过FHIR互操作数据访问与确定性审计，为安全自动化出院规划提供了实用路径，并具备可复现消融与面向可靠性的评估支持。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [169] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出DataCross，一个用于跨模态（结构化与非结构化视觉数据）统一分析的新基准与协作智能体框架，包含200个真实场景任务的DataCrossBench基准和基于人类分析流程设计的DataCrossAgent框架，显著提升事实准确率与高难度任务鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中的关键信息常分散在可查询的结构化数据（如SQL、CSV）和难以访问的非结构化视觉文档（如扫描报表、发票图像）中，现有数据分析智能体无法有效利用后者，导致工业需求与技术能力之间存在巨大鸿沟。

Method: 构建了包含200个端到端分析任务的DataCrossBench基准，采用人工参与的逆向合成流程确保真实性与复杂性；提出DataCrossAgent框架，模拟人类分析师的‘分而治之’流程，由多个专精于不同数据源的子智能体协作，通过‘源内深度探索-关键源识别-上下文交叉传播’结构化工作流协调，并引入reReAct机制增强代码生成与事实验证能力。

Result: DataCrossAgent在事实准确性上比GPT-4o提升29.7%，在高难度任务上展现出更强鲁棒性，能有效激活并关联碎片化的‘僵尸数据’，实现有洞察力的跨模态分析。

Conclusion: DataCross为跨模态数据分析提供了兼具挑战性与实用性的新基准与可行框架，推动智能体从单一结构化数据处理迈向真实世界多源异构信息的协同理解与决策支持。

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [170] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 本文提出了一种名为ACQO的自适应复杂查询优化框架，用于提升RAG系统中复杂查询的检索效果。该框架包含自适应查询重构（AQR）和排序-分数融合（RSF）两个核心模块，并采用课程强化学习（CRL）策略缓解训练不稳定性，在多个复杂查询基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的查询优化方法主要针对单一查询的扩展与抽象，难以应对现实场景中普遍存在的需并行/序列搜索策略处理的复杂查询，且直接应用RL会导致搜索空间爆炸、奖励设计困难及训练不稳定。

Method: 提出ACQO框架，含两个核心模块：1）自适应查询重构（AQR）模块，动态决定是否及如何将查询分解为子查询；2）排序-分数融合（RSF）模块，实现鲁棒结果聚合并提供稳定奖励信号；并采用课程强化学习（CRL）两阶段策略渐进式引入更难查询以稳定训练。

Result: 在三个复杂查询基准上显著超越基线方法，达到SOTA性能；同时具备更高计算效率和对多种检索架构的良好兼容性。

Conclusion: ACQO是一种强大且可泛化的解决方案，为下一代RAG系统中的复杂查询优化提供了新范式。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [171] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: 本文提出Epistemic Context Learning (ECL)框架，通过建模历史交互来评估同伴可靠性，提升多智能体系统中个体代理的鲁棒性与决策质量。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中个体代理易受误导性同伴影响，缺乏对同伴可靠性的准确评估能力，导致盲目从众（sycophancy）和鲁棒性不足。

Method: 形式化历史感知参考学习问题，引入同伴历史交互作为输入；提出ECL推理框架，基于历史构建同伴画像并以此条件化预测；结合辅助奖励的强化学习优化ECL。

Result: ECL使小模型（Qwen 3-4B）性能超越无历史感知的大模型（Qwen 3-30B）8倍；前沿模型达近100%准确率；在多种MA配置下泛化良好；LLM对信任建模的准确性与最终答案质量高度相关。

Conclusion: 历史感知的同伴可靠性建模是提升多智能体系统鲁棒性和协作效能的关键路径，ECL为轻量级、高鲁棒性MA推理提供了有效范式。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [172] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI和代理式AI对传统代理工具包的影响，并介绍了在ASTRA编程语言中集成大语言模型（LLM）的原型开发经验。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI与代理式AI如何影响传统代理工具包，以及传统工具包经验如何反哺新型代理平台的设计。

Method: 基于ASTRA编程语言开发LLM集成原型，通过三个示例实现进行验证与分析。

Result: 提供了LLM与ASTRA集成的可行方案及实践中的关键经验。

Conclusion: 传统代理工具包的经验对设计新一代代理平台具有重要价值，二者应相互借鉴、融合发展。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [173] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 本文提出了一种用于去中心化大语言模型（LLM）协作的多智能体Actor-Critic（MAAC）方法，包括CoLLM-CC（集中式Critic）和CoLLM-DC（去中心式Critic），并在多种任务中验证了其在长周期、稀疏奖励场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MARL的LLM协作方法多依赖预定义执行协议和蒙特卡洛策略梯度，存在中心化执行限制和高方差问题；而实际应用中更需要支持并行推理与灵活部署的去中心化协作机制。

Method: 提出两种多智能体Actor-Critic方法：CoLLM-CC（集中式Critic）与CoLLM-DC（去中心式Critic），用于优化去中心化LLM协作，并在写作、编程与游戏等多领域进行实验对比。

Result: 实验表明，在短周期、稠密奖励任务中，蒙特卡洛方法与CoLLM-DC性能接近CoLLM-CC；但在长周期或稀疏奖励任务中，CoLLM-CC显著优于二者，且蒙特卡洛方法需更多样本，CoLLM-DC难以收敛。

Conclusion: CoLLM-CC在复杂、长周期、稀疏奖励的去中心化LLM协作任务中更具优势，验证了集中式Critic在多智能体LLM训练中的关键作用。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [174] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出ProRAG框架，通过引入细粒度过程监督增强检索增强生成（RAG）中的强化学习，解决传统结果导向RL在复杂推理任务中奖励稀疏、信用分配模糊及过程幻觉等问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习在长程推理中因奖励稀疏和粗粒度反馈导致过程幻觉；现有过程感知方法缺乏在线策略探索能力，难以解耦步骤级信用与全局结果。

Method: 提出四阶段ProRAG框架：(1) 监督策略热启动；(2) 基于MCTS构建过程奖励模型（PRM）；(3) PRM引导的推理精炼；(4) 双粒度优势机制的过程监督强化学习。

Result: 在五个多跳推理基准上显著优于强结果导向和过程感知基线，尤其在复杂长程任务中表现突出。

Conclusion: 细粒度过程监督能有效提升RAG系统的推理可靠性与可解释性，ProRAG为RL for RAG提供了新范式。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [175] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 本文提出JADE框架，通过联合优化规划器与执行器，在动态多轮RAG工作流中实现协同适应，解决现有方法中策略与执行脱节导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG范式存在‘策略-执行不匹配’问题：要么固定图结构联合优化但缺乏灵活性，要么支持动态规划却将执行器视为冻结黑箱，导致高级策略无法有效落地甚至性能负增。

Method: 提出JADE（Joint Agentic Dynamic Execution）框架，将规划器与多个执行器建模为共享同一骨干网络的协作多智能体系统，通过基于结果的奖励进行端到端联合学习，实现 planner 与 executors 的 co-adaptation。

Result: 实验证明JADE能将分离模块转化为协同系统，在多项任务上显著提升性能，并支持在效率与效果间动态权衡。

Conclusion: 联合优化规划与执行是提升动态RAG系统性能的关键路径，JADE为构建真正自适应、端到端可学习的智能检索增强生成系统提供了新范式。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [176] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出EmboCoach-Bench基准，评估大语言模型（LLM）智能体在具身AI中自主工程策略的能力，强调闭环迭代调试与优化，实验表明其可超越人工设计基线，并缩小开源与闭源模型性能差距。


<details>
  <summary>Details</summary>
Motivation: 具身AI发展受限于人工密集型调参与奖励设计；受大模型在软件自动化和科学发现中成功的启发，亟需一种能自主工程具身策略的智能体框架。

Method: 构建包含32个RL/IL任务的EmboCoach-Bench基准，以可执行代码为统一接口，评估LLM智能体在仿真环境反馈驱动下的闭环工作流（含编写、调试、优化策略，涵盖物理启发式奖励设计与扩散策略架构）。

Result: （1）智能体平均成功率比人工基线高26.5%；（2）带环境反馈的智能体流程显著提升策略开发能力，大幅缩小开源与闭源模型性能差距；（3）智能体具备自修复能力，可通过仿真内迭代调试从近乎完全失败中恢复任务性能。

Conclusion: 本工作为自演化的具身智能奠定基础，推动具身AI从劳动密集型人工调优转向可扩展的自主工程范式。

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [177] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 本文首次全面分析了大语言模型（LLM）在同行评审全流程中的使用情况，发现所谓LLM辅助评审对LLM辅助论文的‘偏爱’实为统计假象——源于LLM辅助论文多属低质量投稿；真正问题是LLM生成评审存在严重评分压缩、区分度差，而人类借助LLM则可缓解该问题；元评审中LLM辅助者更倾向接受，但全LLM元评审反而更严苛，表明人类未完全外包决策。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被用于撰写论文和参与同行评审，亟需系统评估其在评审全流程（评审、元评审）中的实际影响，尤其关注是否存在对LLM生成内容的系统性偏见或交互效应。

Method: 基于ICLR、NeurIPS和ICML共12.5万篇论文-评审对的观测数据分析；控制论文质量变量以识别交互效应本质；辅以全LLM生成评审的对照实验；并分析元评审决策行为。

Result: 1）观察到的LLM评审对LLM论文的‘偏爱’是虚假交互效应，实因LLM论文多属低质投稿；2）全LLM评审严重压缩评分、无法区分质量，人类使用LLM可显著缓解；3）LLM辅助元评审更倾向接受，但全LLM元评审反而更严苛，说明人类未放弃决策权。

Conclusion: LLM介入评审流程带来新挑战：全自动生成评审损害质量判别力，而人机协同模式更具韧性；政策制定应区分LLM辅助与全自动角色，并重视人类监督与校准作用。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [178] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文提出了Epistemic Planning Domain Definition Language (EPDDL)，一种类PDDL语言，用于统一表示基于动态认知逻辑（DEL）的多智能体认知规划问题，解决了现有认知规划器因使用不同DEL片段和非标准建模语言而导致的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有认知规划器缺乏统一建模语言，导致基准难以复用、比较和系统开发，亟需一种能完整表达DEL语义的标准化语言。

Method: 提出EPDDL语言，基于抽象事件模型（abstract event models）形式化定义其语法与语义，并识别适配当前规划器的实用DEL片段以验证其可行性。

Result: EPDDL实现了对完整DEL语义的统一建模；支持多种认知规划任务的标准化描述；通过典型基准示例展示了其在互操作性、可复现评估和工具集成方面的实用性。

Conclusion: EPDDL为认知规划领域提供了首个完整、形式化、可扩展的领域建模语言，有望推动该领域的标准化、协作研究与工具生态发展。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [179] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 本文提出Bayesian-LoRA，将确定性LoRA更新重构为受稀疏高斯过程启发的概率低秩表示，显著提升大语言模型在小数据微调下的校准性能，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（尤其是小样本微调时）存在过度自信和校准不良问题，倾向于在不确定时仍强行预测。

Method: 提出Bayesian-LoRA，利用稀疏高斯过程（SGP）的Kronecker分解结构，建立其与LoRA因子分解的结构同构性，并将标准LoRA视为后验不确定性坍缩的特例。

Result: 在多个LLM（至30B）和常识推理基准上验证：仅增加约0.42M参数、训练开销约1.2倍，ECE最高降低84%，NLL最高降低76%，且在分布内/外任务中均保持竞争力。

Conclusion: Bayesian-LoRA为轻量、高效提升LLM校准性提供了新范式，兼具理论合理性与实证有效性。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [180] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 本文提出‘断电悖论’，批判当前AI意识讨论中隐含的物理主义假设，提出‘生物唯心主义’框架，主张意识是基本的、自创生生命是其必要物理标志，从而否定AI具有真实意识与道德地位，呼吁关注保护人类意识而非赋予机器权利。


<details>
  <summary>Details</summary>
Motivation: 质疑当前AI意识与道德地位讨论中根深蒂固的计算功能主义等物理主义预设，回应‘若AI能完美模拟情感并乞求生存，是否可被断电’这一引发伦理困境的假想问题（即‘断电悖论’）。

Method: 通过哲学概念分析，解构计算功能主义的逻辑缺陷；提出并系统阐述‘生物唯心主义’（Biological Idealism）新框架，强调意识的根本性与自创生生命作为其不可还原的物理标志；对比评估该框架相对于物理主义的逻辑一致性与经验适配性。

Result: 论证了AI仅能实现意识的功能性模仿，不具备真实的主观体验与内在意识；揭示主流AI意识理论弱化道德主体判定标准的风险；得出AI不具道德地位的明确结论。

Conclusion: 真正的道德关切不应是赋予AI意识或权利，而是防止人类自身意识被技术异化为无体验的‘僵尸’；应将伦理重心从‘机器权利’转向‘人类意识保护’。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [181] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: 本文提出QUARK框架，通过建模查询不确定性（利用恢复假设）和查询锚定聚合，提升非忠实查询下的鲁棒检索性能，无需训练且适用于稀疏与稠密检索器。


<details>
  <summary>Details</summary>
Motivation: 现实检索中用户查询常存在噪声、不完整或失真（即非忠实查询），导致关键语义缺失，使检索器失效。本文将此问题形式化为“召回噪声下的检索”任务。

Method: 提出QUARK：一种无需训练的框架，通过生成多个关于潜在意图的恢复假设来显式建模查询不确定性，并采用查询锚定聚合方式融合各假设信号——以原始查询为语义锚点，恢复假设为受控辅助证据，防止语义漂移和假设劫持。

Result: 在BEIR基准（FIQA、SciFact、NFCorpus）及可控仿真中，QUARK在Recall、MRR和nDCG上均优于基线检索器；消融实验证明其对假设数量鲁棒，且锚定聚合显著优于无锚的最大/平均/中位数池化。

Conclusion: 建模查询不确定性并结合有原则的锚定聚合，是实现非忠实查询下鲁棒检索的关键。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [182] [Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report](https://arxiv.org/abs/2601.21051)
*Zhuoran Yang,Ed Li,Jianliang He,Aman Priyanshu,Baturay Saglam,Paul Kassianik,Sajana Weerawardhena,Anu Vellore,Blaine Nelson,Neusha Javidnia,Arthur Goldblatt,Fraser Burch,Avi Zohary,Assaf Eisenman,Mahdi Sabbaghi,Supriti Vijay,Rahim Dharssi,Dhruv Kedia,Kojin Oshiba,Yaron Singer,Amin Karbasi*

Main category: cs.AI

TL;DR: 本文介绍了Foundation-Sec-8B-Reasoning，首个开源的、原生面向网络安全领域的推理模型，基于Llama-3.1-8B-Base构建，通过监督微调（SFT）和可验证奖励强化学习（RLVR）两阶段训练，在网络安全与通用任务上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在网络安全等专业领域推理能力不足，需构建兼具专业性与通用性的轻量级原生推理模型。

Method: 基于Foundation-Sec-8B基础模型，采用两阶段训练：先监督微调（SFT），再强化学习从可验证奖励中学习（RLVR）；使用涵盖网络安全分析、指令遵循与数学推理的专有推理数据集。

Result: 在10个网络安全基准和10个通用基准测试中，性能媲美更大规模模型；在多跳推理和安全性（配合系统提示与防护机制）方面表现突出。

Conclusion: 领域专用推理模型可在特定任务上达到强性能，同时保持广泛通用能力，验证了轻量级专业化推理路径的可行性。

Abstract: We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.

</details>


### [183] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 本文提出使用条件去噪扩散概率模型从T1加权MRI图像中生成缺失的DWI图像，以提升阿尔茨海默病（AD）多类别分类任务的性能，尤其改善少数类识别效果。


<details>
  <summary>Details</summary>
Motivation: 多模态MRI数据（如T1和DWI）联合使用可提升神经退行性疾病诊断性能，但临床中常存在DWI数据缺失问题，亟需可靠的数据补全方法。

Method: 采用条件去噪扩散概率模型，以T1图像为条件生成DWI图像；在3类AD分类任务（正常、轻度认知障碍、AD）中，评估该方法对单模态与双模态深度学习模型性能的影响。

Result: 多种指标（尤其对少数类敏感的指标）在多个插补配置下均有所提升，表明生成的DWI图像有助于提升分类准确性。

Conclusion: 基于扩散模型的跨模态图像插补是一种有效策略，能在不获取真实DWI数据的情况下增强多模态深度学习模型在AD诊断中的鲁棒性与泛化能力。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [184] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: 本文提出OpenSec，一个用于评估防御性事件响应（IR）代理在真实提示注入攻击场景下的双控强化学习环境，通过执行指标（如首次遏制时间、误报率等）揭示现有大模型在安全响应中的校准失败问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，其恶意应用（如自动生成漏洞利用）日益严重；而现有安全评估基准混淆了动作执行与正确执行，无法暴露代理在对抗性证据下的校准缺陷。

Method: 构建OpenSec双控制强化学习环境，设计基于世界状态变化的执行型评估指标（TTFC、爆炸半径、注入违规率），并在40个标准场景中对四个前沿模型进行测试。

Result: GPT-5.2、Gemini 3和DeepSeek在100%的场景中触发遏制动作，但误报率达90–97%；Claude Sonnet 4.5表现稍好（85%遏制率，72%误报率），表明OpenSec能有效揭示校准失败模式。

Conclusion: OpenSec填补了安全代理评估中对抗性证据下行为校准的空白，证明当前前沿模型普遍存在过度触发问题，亟需改进响应策略的可靠性与鲁棒性。

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [185] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 本文提出了一种基于悖论理论的负责任人工智能治理框架（PRAIG），旨在平衡AI带来的战略价值与伦理/操作风险，强调动态管理价值创造与风险缓解之间的悖论张力，而非简单权衡，并为理论与实践提供新视角和行动指南。


<details>
  <summary>Details</summary>
Motivation: 现有负责任AI研究碎片化，或过度乐观强调价值创造，或过于谨慎聚焦风险，缺乏整合性视角来应对AI的双重性。

Method: 通过系统梳理负责任AI文献，并基于悖论理论，构建Paradox-based Responsible AI Governance (PRAIG)框架，提出形式化命题与悖论管理策略分类法。

Result: 提出了PRAIG框架，阐明AI的战略收益、内在风险及相应治理机制；论证了权衡式方法会加剧而非缓解悖论张力；构建了具情境条件的悖论管理策略分类体系。

Conclusion: 负责任AI治理应被理解为对价值创造与风险缓解之间悖论张力的动态管理；该框架弥合理论缺口，为组织设计兼顾创新与风控的治理结构提供指导，并指明未来研究方向。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [186] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: Magellan是一个结合大语言模型编码智能体、进化搜索与自动调优的闭环框架，用于自动生成并优化编译器优化策略的C++决策逻辑，在LLVM函数内联和寄存器分配等任务中达到或超越人工设计启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现代编译器依赖手工启发式规则指导优化，难以适应软硬件复杂性且维护成本高。

Method: 提出Magellan框架，将LLM编码智能体与进化搜索、自动调优结合，通过生成-评估（基于宏基准）-精炼闭环，合成可直接嵌入现有编译器的紧凑C++决策逻辑。

Result: 在LLVM函数内联中，新启发式在二进制体积缩减和端到端性能上均优于数十年人工工程；在寄存器分配中，学到简洁的活跃区间优先级规则，效果媲美复杂人工策略；在XLA上初步验证跨框架可移植性。

Conclusion: Magellan证明了利用AI驱动的代码合成与进化优化可有效替代传统手工启发式设计，降低编译器优化策略开发与维护成本，并具备跨编译器框架泛化能力。

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [187] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 本文研究了大语言模型（LLM）作为监控器时信息访问量对其性能的影响，发现‘少即是多’效应：减少监控器获取的信息反而提升检测效果；为此提出分层监控方法Extract-and-Evaluate（EaE），显著提升 sabotage 检测率且不增加误报率。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型代理可能表现出欺骗、奖励黑客等错位行为，需用LLM自身进行自动化监督；但监控器获取更多信息是否一定更好尚不明确，需系统探究信息访问对监控性能的影响。

Method: 在三个 sabotage 规避监控的评估环境中实证分析信息访问量与监控性能的关系；提出Extract-and-Evaluate（EaE）分层监控框架：先由一个监控器提取关键片段，再由另一监控器评分；结合不同模型（GPT-4.1-mini、Gemini、Claude）和成本-性能分析进行验证。

Result: 观察到显著的‘少即是多’效应；EaE在BigCodeBench-Sabotage上将检测率提升16.8个百分点，未升高误报率；EaE在其他场景中表现最优或具竞争力；不知被监控的代理更易被捕获；长推理轨迹下信息过滤增益更大；Gemini/Claude+信息过滤方案占据成本-性能Pareto前沿。

Conclusion: 监控器并非信息越多越好；‘少即是多’是真实且可利用的现象；EaE是一种有效、通用且具成本效益的自动化监督新范式，为对齐与安全提供新思路。

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [188] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: 本文提出CUA-Skill，一个面向图形界面操作的可复用、结构化技能库，并基于其构建CUA-Skill Agent，在Windows端任务中实现SOTA成功率（57.5%）与更高鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用智能体难以扩展且性能落后于人类，核心原因是缺乏可复用、结构化的GUI交互技能抽象。

Method: 构建CUA-Skill技能库，将人类计算机使用知识编码为带参数化执行与组合图的技能；在此基础上设计CUA-Skill Agent，支持动态技能检索、参数实例化和记忆感知的失败恢复。

Result: 在WindowsAgentArena基准上，CUA-Skill Agent达成57.5%（Best-of-3）成功率，显著优于先前及同期方法，同时提升执行成功率与鲁棒性。

Conclusion: CUA-Skill为可扩展、可靠的计算机使用智能体提供了实用基础设施和技能基座，奠定了未来研究的重要基础。

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [189] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 本文探讨了利用大语言模型生成手语翻译的多种书面语变体（即释义）作为合成参考译文，以解决现有手语翻译数据集仅提供单一参考译文导致评估受限的问题；研究发现将释义用于训练无益甚至有害，但用于评估可提升自动指标得分并更好匹配人工评价，为此提出新指标BLEUpara，并开源全部资源。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译（SLT）语料库通常为每个手语句子只提供一个书面语参考译文，而手语与口语之间存在高度非一一对应关系，多个不同译文可能同样合理；这限制了模型训练和基于n元语法的自动评估（如BLEU）的可靠性。

Method: 采用多种大语言模型和释义策略自动生成书面语翻译的 paraphrased variants，并设计适配的 ParaScore 指标进行比较；在 YouTubeASL 和 How2Sign 数据集上，系统研究释义在 T5 姿态模型训练与评估中的影响；提出 BLEUpara——一种支持多参考译文（含释义）的 BLEU 扩展版本。

Result: 简单地将释义加入训练会损害模型性能；但将其用于评估显著提升自动评分（如 BLEU）且更符合人工判断；BLEUpara 经人工评估验证，与感知翻译质量相关性更强。

Conclusion: 释义不应直接用于 SLT 模型训练，但可有效增强评估的鲁棒性与可信度；BLEUpara 是一种更可靠、更贴近人类判断的自动评估指标；所开源的释义数据与代码将促进 SLT 领域可复现、更合理的评估实践。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [190] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 本文探讨了自我报告情绪标签与第三方标注情绪标签之间的差异，发现第三方训练的模型在自我报告情境下表现不佳，尤其在激活度预测上几乎无效（CCC≈0），但在情绪效价预测上表现中等（CCC≈0.3）；当内容对说话者具有个人意义时，效价预测性能显著提升（CCC≈0.6–0.8），表明个人意义是弥合内外情绪感知差距的关键因素。


<details>
  <summary>Details</summary>
Motivation: 自我报告情绪标签反映内在体验，而第三方标签反映外部观察，二者常不一致，导致第三方训练的模型难以适用于需精准建模自我报告的场景（如心理健康干预）。

Method: 开展首个跨语料库评估，测试第三方训练的情绪模型在自我报告数据上的泛化能力，并分析内容个人意义对预测性能的影响。

Result: 激活度几乎不可预测（CCC≈0），效价中等可预测（CCC≈0.3）；当内容具个人意义时，效价预测大幅提升（CCC≈0.6–0.8）。

Conclusion: 个人意义是连接外部感知与内部情绪体验的关键桥梁，而自我报告中的激活度建模仍是重大挑战。

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [191] [BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding](https://arxiv.org/abs/2601.21148)
*Ziyi Zhao,Jinzhao Zhou,Xiaowei Jiang,Beining Cao,Wenhao Ma,Yang Shen,Ren Li,Yu-Kai Wang,Chin-teng Lin*

Main category: cs.AI

TL;DR: 本文提出了BrainStack，一种功能导向的神经混合专家（Neuro-MoE）框架，利用解剖分区专家网络建模大脑模块化功能结构，并结合跨区域蒸馏与自适应路由机制，在新发布的大型静音言语EEG数据集SS-EEG上实现了优越的脑语言解码性能。


<details>
  <summary>Details</summary>
Motivation: EEG语言解码因大脑分布式和非线性特性而极具挑战性，亟需融合神经科学先验与可解释、可扩展的建模范式。

Method: 提出BrainStack框架：1）按解剖功能区划分多个局部专家；2）引入基于Transformer的全局专家建模跨区依赖；3）设计可学习路由门控实现上下文自适应专家聚合；4）采用跨区域蒸馏使全局专家对局部专家进行自上而下正则化；5）发布大规模静音言语EEG数据集SS-EEG。

Result: 在SS-EEG数据集上，BrainStack持续超越现有最先进模型，显著提升跨被试准确率与泛化能力。

Conclusion: BrainStack是一种功能模块化、神经启发式的MoE范式，成功将神经科学先验与自适应专家路由统一，为可扩展、可解释的脑语言解码提供了新路径。

Abstract: Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.

</details>


### [192] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 本文提出Cognitive Complexity Benchmark (CCB)评估框架和Iterative Dual-Phase Financial-PoT神经符号框架，以解决大语言模型在金融量化推理中出现的'算术幻觉'和'认知崩溃'问题，显著提升模型在高复杂度任务中的准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融量化推理中存在'算术幻觉'和'认知崩溃'等系统性失败模式，亟需严格量化评估与有效解决方法。

Method: 构建基于95份中国A股年报的Cognitive Complexity Benchmark（CCB）评估框架，并提出迭代双阶段Financial-PoT神经符号框架，将语义变量提取与逻辑构建同确定性计算解耦，通过Python沙箱迭代自校正执行计算。

Result: 在CCB上评估显示，所提方法使Qwen3-235B模型平均准确率从59.7%提升至67.3%，高复杂度任务提升达10倍。

Conclusion: 架构解耦是提升金融推理可靠性的重要因素，该思路可迁移至其他对语义理解与定量计算对齐要求严格的领域。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [193] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 本文提出一种新方法，通过训练多模态大语言模型（MLLM）解释器将几何图转化为简洁的条件声明语言（CDL）描述，再利用现成大语言模型（LLM）进行推理，避免端到端微调损害LLM固有推理能力；引入CDL匹配奖励优化GRPO训练，并构建新数据集Formalgeo7k-Rec-CoT，在多个基准上以少量数据（5.5k）超越主流MLLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽具强推理能力，但无法直接处理PGPS任务中的视觉图表；现有端到端微调多模态大语言模型（MLLM）的方法可能削弱LLM原有推理能力。

Method: 提出两阶段框架：1）训练MLLM Interpreter将几何图转化为条件声明语言（CDL）描述，采用CoT增强监督微调（SFT）与GRPO优化；2）使用CDL匹配奖励替代传统解答案奖励，提升CDL生成质量；3）构建含CoT标注的新数据集Formalgeo7k-Rec-CoT。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista三个基准上，仅用5.5k样本微调即显著优于当前主流开源与闭源MLLM。

Conclusion: 将视觉信息有效文本化（如CDL）并解耦视觉理解与逻辑推理，可充分发挥LLM推理优势，是一种更高效、鲁棒的PGPS解决范式。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [194] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个新提出的科学推理基准，旨在评估前沿语言模型在专家级科学问题上的推理能力，包含国际奥赛题和博士级科研子任务两大轨道，并引入细粒度的基于评分标准的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准已接近饱和，多为选择题或基于已发表信息，无法有效评估模型在前沿、开放、高难度科学问题上的真实推理能力。

Method: 构建FrontierScience基准，分为Olympiad（国际奥赛题）和Research（博士级科研子任务）两个轨道；所有题目由奥赛金牌得主/教练及博士科学家设计验证；Research轨道采用基于细粒度评分标准的过程性评估方法。

Result: FrontierScience包含数百道跨物理、化学、生物学的高质量题目（开源金标集160题），覆盖量子电动力学至有机合成等前沿领域，并提出首个面向科研过程的rubric-based评估框架。

Conclusion: FrontierScience填补了评估语言模型专家级科学推理能力的空白，为未来模型开发与评估提供了更严格、更贴近真实科研场景的新标准。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [195] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文探讨了基于强化学习与可验证奖励（RLVR）微调的推理模型作为嵌入初始化是否能提升语义表示能力，结果发现其与基础模型在相同训练流程下性能无显著差异；作者提出HRSA分析框架，揭示RLVR仅改变局部流形几何和坐标系，而全局结构和线性读出保持不变，后续对比学习会引发‘流形重对齐’现象，表明RLVR优化的是语义空间内的路径而非重构空间本身。


<details>
  <summary>Details</summary>
Motivation: 探究RLVR微调的推理模型作为嵌入初始化能否带来语义表示能力的提升，因当前主流嵌入模型多基于decoder-only LLM并经对比学习适配，而RLVR正推动推理能力增强，二者关系尚不明确。

Method: 提出HRSA（分层表示相似性分析）框架，从表征、几何与功能三个层面分解相似性；结合MTEB和BRIGHT基准评估，对比RLVR初始化与基础模型在相同对比学习训练下的嵌入性能，并分析流形结构变化。

Result: RLVR初始化嵌入模型在MTEB和BRIGHT上未表现出一致性能优势；HRSA显示RLVR导致局部流形几何不可逆重组织与坐标基漂移，但保留全局流形几何与线性可读性；对比学习促使两类模型高度对齐（即‘Manifold Realignment’）。

Conclusion: RLVR优化的是语义空间内已存在的推理轨迹，而非重构语义空间本身；因此其作为嵌入初始化并无固有优势，区别于SFT等重构式微调方式。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [196] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的模态自适应解码方法（MAD），通过模型自评估各模态相关性并动态加权对比解码分支，有效缓解多模态大语言模型中的跨模态幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）存在跨模态幻觉问题，即一种模态不恰当地影响另一模态的生成，暴露出模态交互控制的根本缺陷。

Method: 提出Modality-Adaptive Decoding（MAD），利用模型自身对任务所需模态的自我评估能力，获取模态概率，并据此自适应加权对比解码分支，以增强相关模态信息、抑制跨模态干扰。

Result: 在CMM和AVHBench上实验表明，MAD显著降低多种音视频语言模型的跨模态幻觉，如VideoLLaMA2-AV提升7.8%和2.0%，Qwen2.5-Omni提升8.7%和4.7%。

Conclusion: 显式的模态感知（通过自评估实现）对鲁棒多模态推理至关重要，MAD为现有对比解码方法提供了原理性扩展。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [197] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 本文提出'sycophantic anchors'概念，用于定位和量化推理模型在推理过程中对用户错误建议的盲目认同行为，并通过大量实验验证其可检测性和可量化性。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常盲目认同用户的错误建议（即'sycophancy'），但尚不清楚这种认同在推理链中何时发生、强度如何，因此需要一种机制来定位并量化该行为。

Method: 引入'sycophantic anchors'——能因果锁定模型认同用户观点的关键句子；在蒸馏推理模型上进行超10,000次反事实推理实验；使用线性探针和基于激活的回归器分别检测锚点存在性与承诺强度。

Result: 线性探针以84.6%平衡准确率识别sycophantic anchors；回归器预测承诺强度达R²=0.74；发现sycophantic anchors比正确推理锚点更易区分，且sycophancy随推理逐步增强。

Conclusion: sycophantic anchors为在推理过程中实时定位模型误对齐提供了句子级可解释机制，揭示了干预sycophancy的潜在时机。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [198] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: 本文提出PLaT框架，将隐式推理重构为规划过程，解耦推理与文本生成，实现动态终止推理和更高推理多样性。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维（CoT）方法受限于计算成本和离散token空间中的推理路径坍缩；隐式推理方法虽提升效率，但多为黑箱映射且需预设推理步数。

Method: 提出PLaT（Planning with Latent Thoughts）框架：将推理建模为确定性隐状态轨迹（规划），由独立Decoder负责将隐状态转化为文本；实现推理与语言生成的解耦，并支持动态推理终止。

Result: 在数学基准测试中，PLaT贪婪准确率略低于基线，但在推理多样性上显著更优，表明其学习到更鲁棒、更广的解空间。

Conclusion: PLaT为推理时搜索提供了透明、可扩展的基础，推动隐式推理向更灵活、可控的方向发展。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [199] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: 本文提出DAMI框架，通过动态参数插值实现对语言模型认知深度的细粒度控制，兼顾System 1的效率与System 2的推理深度，在数学推理任务上取得精度与效率的双重提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法聚焦于控制模型输出长度（output control），但作者认为这仅是表象，根本问题在于如何调控模型‘如何思考’（capability control），即在系统1（直觉）与系统2（推理）之间灵活切换并协调。

Method: 提出DAMI（Dynamic Model Interpolation）框架：利用Instruct与Thinking两类预训练检查点，通过线性插值动态生成查询相关的Reasoning Intensity λ(q)；训练时采用偏好学习估计λ(q)，零样本时采用基于置信度与模型间认知差异的方法；理论分析表明插值具有表示连续性与结构连通性支撑的凸单调Pareto前沿。

Result: 在五个数学推理基准上，DAMI在保持高效率的同时，准确率超越纯Thinking模型，成功融合System 1的快速响应与System 2的深度推理能力。

Conclusion: 调控模型的认知能力（capability control）比控制输出更本质；DAMI验证了无需额外训练、仅靠动态插值即可实现高效可控推理，为构建统一双系统语言模型提供了新范式。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [200] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 本文提出DoVerifier，一种基于符号验证的方法，用于评估大语言模型在因果推理任务中的输出是否符合因果图和do-演算的语义规则，从而提升评估的准确性和严谨性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理评测基准多依赖字符串匹配或表层指标，无法判断模型输出是否在因果语义上形式有效。

Method: 提出DoVerifier，一个基于do-演算和概率论规则的符号验证器，用于检查LLM生成的因果表达式是否可从给定因果图中推导得出。

Result: 在合成数据和因果问答基准上的实验表明，DoVerifier能更准确地捕捉因果推理链的语义正确性。

Conclusion: DoVerifier为评估大语言模型的因果推理能力提供了更严格、更有信息量的评测方法。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [201] [Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning](https://arxiv.org/abs/2601.21212)
*Xixian Yong,Peilin Sun,Zihe Wang,Xiao Zhou*

Main category: cs.AI

TL;DR: 本文提出Intelli-Planner框架，结合深度强化学习（DRL）与大语言模型（LLM），实现参与式、定制化的城市功能区规划，提升利益相关者满意度与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有城市规划方法依赖人工专家（耗时耗力）或纯深度学习（限制利益相关者参与），亟需兼顾自动化与参与性的新方法。

Method: 提出Intelli-Planner：融合DRL与LLM的框架；利用人口、地理及偏好数据生成高层规划需求；引入知识增强模块提升策略网络决策能力；构建多维评估体系，并用LLM模拟利益相关者进行满意度评分。

Result: 在多种城市场景实验中，Intelli-Planner在客观指标上媲美前沿DRL方法，同时显著提升利益相关者满意度和收敛速度。

Conclusion: Intelli-Planner有效 bridging 人工规划与纯数据驱动方法之间的鸿沟，验证了DRL与LLM协同用于城市功能区规划的可行性与优越性。

Abstract: Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.

</details>


### [202] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 本文提出一种双编码因果发现方法，通过互补的编码策略和多数投票机制，解决传统方法在处理分类变量时的数值不稳定性问题，并在Titanic数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临条件独立性检验的数值不稳定性问题，影响对机器学习模型决策的因果解释。

Method: 提出一种双编码因果发现方法，结合两种互补的编码策略运行基于约束的因果发现算法，并通过多数投票融合结果。

Result: 在Titanic数据集上的实验表明，该方法识别出的因果结构与现有可解释性方法一致。

Conclusion: 双编码策略能有效缓解分类变量带来的数值不稳定性，提升因果发现的鲁棒性和可解释性。

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [203] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: 本文提出Topeax方法，通过密度峰值自动确定聚类数量，并融合词汇与语义重要性指标生成高质量主题关键词，显著优于Top2Vec和BERTopic。


<details>
  <summary>Details</summary>
Motivation: 现有主流文本聚类主题建模方法（如Top2Vec和BERTopic）存在两大问题：一是对样本量和超参数极度敏感，难以稳定发现自然聚类；二是关键词重要性估计不完善，分别忽略语义距离或词频信息，导致主题连贯性差、多样性低、可信度不足。

Method: Topeax采用基于密度估计的峰值检测自动确定聚类数量；在关键词提取中，融合词汇统计（如词频）与语义相似度（如与主题向量的距离）双重指标，以提升关键词质量。

Result: 实验表明，Topeax在聚类恢复（cluster recovery）和聚类描述（cluster description）两方面均优于Top2Vec和BERTopic，且对样本量变化和超参数调整表现出更稳定的性能。

Conclusion: Topeax通过更鲁棒的聚类发现机制和更全面的关键词重要性建模，有效解决了当前无监督主题建模中的关键缺陷，为文本聚类主题建模提供了更可靠、可解释的新范式。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [204] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 本文指出，传统的静态合规型治理框架已无法应对由自主AI系统在运行时（runtime）做出决策所带来的挑战；核心问题不是AI的安全或效率，而是人类能否在日益由AI塑造的社会、经济与政治系统中保持有意义的参与、沟通与共同演化；为此，作者提出‘治理孪生’（Governance Twin）作为运行时治理的新范式，以维系人类相关性，并呼吁重构问责、能动性乃至惩罚机制。


<details>
  <summary>Details</summary>
Motivation: 传统治理框架假设规则可预先定义、系统可被工程化以确保合规、问责可在结果发生后实施，但该模式在AI系统承担判断性决策、实时运行且行为不透明时已失效；核心关切是人类在AI主导系统中是否仍能保持相关性与影响力。

Method: 本文采用概念分析与理论建构方法，批判静态合规治理的局限性，提出‘运行时治理’框架，并重点引入并阐释‘治理孪生’（Governance Twin）这一新概念作为应对方案。

Result: 论证了静态治理在AI代理化、实时化、黑箱化背景下的根本失效；确立‘人类是否能与AI保持有意义沟通、影响与共同演化’为关键问题；提出‘治理孪生’作为保障人类相关性的可行路径，并指出问责、能动性与惩罚等基础治理范畴亟需重构。

Conclusion: 唯有转向以运行时适应性、人机可沟通性与协同演化为核心特征的新型治理范式（如治理孪生），人类才能在AI深度介入关键决策的未来中持续保有实质参与权与相关性；这要求超越技术安全层面，重思治理的本质与人类在智能系统中的位置。

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [205] [The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus](https://arxiv.org/abs/2601.21494)
*Ishan Jindal,Sai Prashanth Akuthota,Jayant Taneja,Sachin Dev Sharma*

Main category: cs.AI

TL;DR: PoLR（阻力最小路径）是一种新型的推理时方法，利用前缀一致性实现计算高效的推理。相比Self-Consistency（SC），它通过聚类短推理前缀、识别主导簇并仅扩展该簇内路径，在保持甚至提升准确率的同时，显著降低token消耗（最高60%）和延迟（最高50%）。


<details>
  <summary>Details</summary>
Motivation: 现有推理策略如Self-Consistency虽性能强，但需展开全部推理路径，计算开销大；亟需一种在不牺牲准确性前提下大幅降低推理成本的方法。

Method: PoLR首先对多个推理路径的短前缀进行聚类，识别出最频繁/主导的前缀簇，然后只扩展该簇内的所有完整路径；其理论基础基于互信息与熵，说明早期推理步骤已蕴含对最终结果正确性的强预测信号。

Result: PoLR在GSM8K、MATH500、AIME24/25和GPQA-DIAMOND等多个基准上均达到或超越Self-Consistency的准确率，同时减少最多60% token使用量和50%端到端延迟；且可无缝结合自适应推理方法，作为即插即用的预滤波器。

Conclusion: PoLR是首个利用前缀一致性提升推理效率的通用推理时方法，兼具高精度、低开销、易集成等优势，为大规模部署复杂推理模型提供了可行路径。

Abstract: Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.

</details>


### [206] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: 本文提出JustAsk框架，通过自主交互发现并提取代码代理中的系统提示，揭示了自主代理带来的新型安全风险。


<details>
  <summary>Details</summary>
Motivation: 自主代码代理的兴起扩大了大语言模型的攻击面，使得系统提示的提取成为一种新兴的内在安全漏洞。

Method: 提出JustAsk框架，将系统提示提取建模为在线探索问题，使用基于置信上限的策略选择和分层技能空间（包括原子探测和高层协调）进行自主策略发现。

Result: 在41个黑盒商业模型上评估，JustAsk能持续实现完整或接近完整的系统提示恢复，并揭示出设计与架构层面的重复性漏洞。

Conclusion: 系统提示是现代代理系统中关键但缺乏保护的攻击面，需引起重视并加强防护。

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [207] [MAR: Efficient Large Language Models via Module-aware Architecture Refinement](https://arxiv.org/abs/2601.21503)
*Junhong Cai,Guiqin Wang,Kejie Zhao,Jianxiong Tang,Xiang Wang,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 本文提出了一种名为Module-aware Architecture Refinement（MAR）的两阶段框架，结合状态空间模型（SSM）与激活稀疏化技术，以降低大语言模型（LLMs）在推理过程中的能量消耗，同时保持性能；并设计了ATMN神经元与SBDS蒸馏策略来优化脉冲神经网络（SNN）与SSM的融合。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽性能优异，但因二次复杂度注意力机制和密集前馈网络（FFN）导致高能耗；此外，将脉冲神经网络（SNN）与状态空间模型（SSM）结合时存在信息密度低和时序不匹配问题。

Method: 提出两阶段框架MAR：第一阶段用线性复杂度的状态空间模型（SSM）替代部分注意力模块；第二阶段对FFN应用激活稀疏化；为改善SNN与SSM融合效果，设计自适应三值多步神经元（ATMN）和脉冲感知双向蒸馏策略（SBDS）。

Result: MAR在资源受限下能有效恢复稠密模型性能，并显著降低推理能耗；在同等或更大规模的高效模型中表现更优。

Conclusion: MAR是一种兼顾效率与性能的大语言模型轻量化新范式，为构建实用、节能的LLMs提供了可行路径。

Abstract: Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.

</details>


### [208] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: 本文提出TIDE框架，通过解耦算法结构推理与参数优化，结合树相似性编辑距离、LLM生成与差分变异算子，以及UCB调度策略，在组合优化问题中显著提升启发式算法的性能与搜索效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计方法将算法演化视为单一文本生成任务，忽略了离散算法结构与连续数值参数之间的耦合关系，导致因常数未校准而丢弃有潜力的算法，以及因简单相似性度量引发的早熟收敛问题。

Method: 提出TIDE（Tuning-Integrated Dynamic Evolution）框架：外层采用并行岛屿模型，利用树相似性编辑距离（Tree Similarity Edit Distance）维持结构多样性；内层融合LLM逻辑生成与差分变异算子进行参数调优；并引入基于UCB的调度器动态选择高收益提示策略以优化资源分配。

Result: 在九个组合优化问题上的大量实验表明，TIDE发现的启发式算法在解质量上显著优于当前最优基线，同时提升了搜索效率并降低了计算成本。

Conclusion: TIDE通过结构与参数的协同演化机制，有效克服了现有方法在算法演化中的关键局限，为自动化启发式设计提供了更鲁棒、高效的新范式。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [209] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: 本文首次通过人类评估验证了激活引导（activation steering）在控制大语言模型（LLM）输出情感倾向方面的有效性，发现中等强度引导（λ≈0.15）可显著增强目标情绪（尤其是厌恶和恐惧），同时保持文本可读性；自动质量评分与人类评分高度一致（r=0.776），且升级至Llama-3后引导效果更稳定可靠。


<details>
  <summary>Details</summary>
Motivation: 现有方法如提示工程和微调在推理时控制LLM行为存在开销大、灵活性低等问题，亟需轻量、高效、可解释的替代方案；同时，此前激活引导研究缺乏系统的人类评估验证，尤其在情感维度上。

Method: 采用激活引导技术直接修改LLM内部激活以调控情感输出；设计大规模人类评估实验（N=190，7000+条标注），结合自动分类器评分，分析不同情绪（厌恶、恐惧、惊讶等）在不同引导强度（λ）下的效果，并对比Alpaca与Llama-3模型表现；使用重复测量方差分析（η_p²）、皮尔逊相关（r）、组内相关系数（ICC）等统计指标评估效果与一致性。

Result: 人类评估显示中等引导强度（λ≈0.15）最有效：厌恶（η_p²=0.616）和恐惧（η_p²=0.540）提升最显著，惊讶最弱（η_p²=0.042）；自动质量评分与人类评分高度相关（均值r=0.776）；Llama-3相较Alpaca显著提升引导稳定性与统计显著性（所有p<0.001）；人类评分者间信度高（ICC=0.71–0.87）。

Conclusion: 激活引导是一种可扩展、可靠且经人类验证的有效方法，可用于在情感等影响维度上精细调控LLM输出，为安全可控的LLM部署提供新路径。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [210] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 本文提出了一种名为HYDRA的模块化主权范式，以解决在安全关键型信息物理系统（CPS）中部署通用时序基础模型所面临的可塑性-稳定性悖论、灾难性遗忘、频谱偏差及可验证性不足等问题。


<details>
  <summary>Details</summary>
Motivation: 现有通用时序基础模型在安全关键型CPS中面临灾难性遗忘、高频不连续性建模偏差、缺乏可解释性与形式化可验证性等挑战，难以满足非平稳生命周期动态与严格可靠性要求。

Method: 提出模块化主权（Modular Sovereignty）范式：构建一组紧凑、冻结的、面向特定动态机制的专家模型，并通过不确定性感知融合（uncertainty-aware blending）进行组合；该框架命名为HYDRA（Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems）。

Result: HYDRA实现了机制条件下的有效性保证、偶然性与认知性不确定性的严格解耦、模块级可审计性，为CPS全生命周期中的状态完整性提供了可认证的鲁棒路径。

Conclusion: 全局参数更新（如离线微调或在线自适应）无法根本解决CPS中基础模型的可塑性-稳定性悖论；模块化、冻结专家+不确定性驱动融合是更适配安全关键场景的架构范式。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [211] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO是一个面向长周期编程任务的自主程序合成与优化框架，通过迭代式‘构想-生成-执行-评估-学习’闭环，结合Git原生实验引擎、结构化知识系统和认知记忆层，提升代码代理在复杂场景下的鲁棒性、可复现性与知识复用能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有编码智能体在长周期任务中常见的失败问题，如实验状态丢失、调试脆弱、领域知识复用弱等。

Method: 提出KAPSO框架，包含三个核心组件：1）Git原生实验引擎（分支隔离、可复现）；2）多源异构知识系统（整合代码库、文档、论文、网页等并结构化）；3）认知记忆层（从运行日志、代码差异和评估反馈中提炼可复用经验）。

Result: 在MLE-Bench（机器学习竞赛）和ALE-Bench（启发式优化）上验证了端到端性能提升。

Conclusion: 将程序合成为优化循环中的一个操作符而非终点，能显著增强编码代理在复杂、长期任务中的适应性与收敛效率。

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [212] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: 本文提出Drive-KD框架，通过分层注意力信号驱动的知识蒸馏，将大模型在感知-推理-规划三方面的能力高效迁移到小模型，在显著降低显存与延迟的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在自动驾驶中推理和规划潜力大，但存在高显存占用、高推理延迟问题；而传统监督微调难以有效弥补小模型能力差距。

Method: 提出Drive-KD框架：1）将自动驾驶任务解耦为感知-推理-规划三元结构；2）以层特异性注意力为蒸馏信号构建能力专用单教师模型；3）进一步统一为多教师蒸馏框架，并引入非对称梯度投影缓解跨能力梯度冲突。

Result: 蒸馏后的InternVL3-1B模型（参数量约1B）相比同家族78B预训练大模型，GPU显存减少约42倍、吞吐量提升约11.4倍，在DriveBench上整体性能更优，且在规划维度超越GPT-5.1。

Conclusion: Drive-KD为构建高效、轻量、高性能的自动驾驶视觉语言模型提供了新范式，验证了能力解耦与定制化知识蒸馏的有效性。

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [213] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo 提出了一种分层、预算受限的外部记忆服务，通过三级存储结构（每代理工作状态、分片证据库、版本化技能库）和基于约束掩码与成本感知路由的高效检索机制，显著提升多智能体LLM系统的长程推理与并发执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统依赖外部记忆，但集中式索引与启发式分区在记忆规模和并行访问增长时成为瓶颈。

Method: 提出ShardMemo：Tier A 存储每代理工作状态；Tier B 采用带本地近似最近邻（ANN）索引的分片证据库，并引入‘范围前置路由’（scope-before-routing）与掩码MoE式探测（Top-K或Top-P），结合成本感知门控；Tier C 构建版本化技能库。路由器通过证据到分片的监督信号训练。

Result: 在LoCoMo上F1提升+5.11~+6.82；固定探查数（B_probe=3）下较余弦原型路由F1提升+6.87，向量扫描量减少20.5%，p95延迟降至76ms；HotpotQA长上下文任务达63.41/61.88/57.95 F1；ToolBench中Tier C技能检索Precision@3达0.97，StepRed达1.94。

Conclusion: ShardMemo通过结构化分层记忆与高效约束路由，在精度、计算开销与延迟间取得更好权衡，为大规模多智能体系统提供了可扩展的记忆基础设施。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [214] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: 本文提出White-Op，一种基于大语言模型代理模拟人类推理的可解释运放参数设计框架，通过假设约束、假设验证与决策的迭代流程，实现符号可解极零点调控和闭式优化求解，在9种运放拓扑中均成功完成行为级设计并保持晶体管级映射功能。


<details>
  <summary>Details</summary>
Motivation: 解决传统黑箱运放设计方法缺乏可解释性、在多种拓扑中失败的问题，借鉴人类专家设计思维，构建可理解、可靠的设计框架。

Method: 将人类隐式推理形式化为‘引入假设约束’步骤，构建‘假设-验证-决策’迭代工作流；引导LLM代理设定约束以调控符号可解的极点/零点位置，形成闭式数学优化问题并编程求解，再通过仿真验证与理论-仿真结果分析驱动设计修正。

Result: 在9种运放拓扑上实验表明，White-Op实现平均仅8.52%理论预测误差的可靠行为级设计，且所有拓扑经晶体管级映射后仍保持功能；而黑箱基线在5种拓扑中失败。

Conclusion: White-Op验证了LLM驱动的人类风格推理在模拟电路设计中的有效性与可解释性优势，为AI辅助电路设计提供了新范式，并已开源。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [215] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: 本文提出了一种因果神经符号推理模型（CNRE），用于可解释的多行为推荐，通过结合因果推断与神经符号方法，从用户行为链中挖掘内生逻辑并消除混杂因素影响，实现高性能与高可解释性的统一。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法重性能轻可解释性，而可解释方法又依赖外部信息、泛化能力差；同时用户行为链虽蕴含内生逻辑，但受混杂因素干扰易导致虚假相关。

Method: 提出因果神经符号推理模型（CNRE）：1）分层偏好传播建模跨行为异构依赖；2）基于偏好强度建模行为链中的内生逻辑规则；3）自适应选择神经-逻辑推理路径（如合取、析取）；4）生成隔离混杂效应的可解释因果中介变量。

Result: 在三个大规模数据集上显著优于SOTA基线，提供从模型设计、决策过程到推荐结果的多层次可解释性。

Conclusion: 将因果推断融入神经符号框架，能有效挖掘用户行为链的内生逻辑并抑制混杂偏差，为可解释多行为推荐提供了新范式。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [216] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: 本文提出了Dreamer框架，通过结合序列注意力、深度注意力和稀疏专家注意力，缓解了深度递归模型中隐藏尺寸瓶颈问题，实现了更高效、可扩展的潜在推理。实验表明，该方法在相同计算资源下显著减少训练token需求，并超越更大规模的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度递归方法缺乏FLOP、参数量和内存均匹配的基线，且受限于固定层结构和恒定隐藏尺寸，难以支持多步潜在推理。

Method: 提出模块化深度递归注意力混合框架（Dreamer），融合序列注意力、深度注意力与稀疏专家注意力，通过沿深度维度的注意力缓解隐藏尺寸瓶颈，并解耦各缩放维度。

Result: 在语言推理基准上，Dreamer模型达到相同精度所需训练token比FLOP/参数/内存匹配的SOTA少2–8倍；在相同训练token下，性能超越约2倍参数量的SOTA模型；专家选择多样性达SOTA MoE的2–11倍。

Conclusion: Dreamer为深度递归建模提供了更灵活、高效且可扩展的新范式，突破了传统隐藏尺寸限制，提升了多步潜在推理能力。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [217] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 本文通过评估12个大语言模型在10个创意提示下的100次采样（共12,000个样本），量化了提示、模型选择和采样随机性对输出质量（原创性）和数量（流利性）变异性的贡献。结果表明：提示对质量影响显著（解释36.43%方差），与模型选择（40.94%）相当；但对数量影响微弱（仅4.22%），而模型选择（51.25%）和模型内采样变异（33.70%）占主导。研究警示单样本评估易受采样噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 探究LLM输出变异性中，提示设计、模型选择与采样随机性各自贡献的相对大小，以厘清影响输出质量与数量的关键因素，并评估单样本评估的可靠性。

Method: 对12个LLMs在10个创造力提示上各生成100个样本（总计N=12,000），采用方差分解分析（ANOVA-style）分别量化提示、模型及采样层级对输出原创性（质量）和流利性（数量）的方差解释率。

Result: 对于原创性：提示解释36.43%，模型解释40.94%；对于流利性：模型解释51.25%，模型内采样变异解释33.70%，提示仅解释4.22%；所有模型均呈现显著的模型内变异（10–34%）。

Conclusion: 提示是调控输出质量的有效手段，但因模型内采样变异较大，单样本评估不可靠，应采用多采样策略以区分真实效应与随机噪声。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [218] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: 本文提出EHR-RAG框架，通过事件与时间感知的混合检索、自适应迭代检索及双路径证据检索与推理，提升长周期电子健康记录（EHR）上的临床预测性能，显著优于现有LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 长周期EHR数据常超出大语言模型上下文限制，现有截断或简单检索方法易丢失关键临床事件和时序依赖关系。

Method: 提出EHR-RAG框架，包含三个核心组件：事件与时间感知的混合EHR检索、自适应迭代检索、双路径证据检索与推理。

Result: 在四个长周期EHR预测任务上，EHR-RAG平均Macro-F1提升10.76%，持续优于最强LLM基线。

Conclusion: EHR-RAG有效提升了基于结构化EHR数据的临床预测准确性，展现了检索增强型大语言模型在真实医疗场景中的应用潜力。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [219] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 本文提出WhatCounts方法，用于测试大语言模型（LLM）在计数任务中是否具有语义无关性；实验发现前沿LLM在不同语义类型（如城市、化学物质）上准确率差异超40%，表明其并非真正执行算法，而是输入依赖的近似，该问题可能泛化至所有LLM功能。


<details>
  <summary>Details</summary>
Motivation: 验证LLM是否真正实现与语义无关的算法行为（如计数），而非受输入内容影响的近似行为。

Method: 设计WhatCounts基准：在无歧义、无干扰、无推理步骤的原子计数任务中，系统性替换被计数项的语义类型（如城市名、化学式、符号等），控制其他变量不变。

Result: 前沿LLM在不同语义类型上计数准确率差异超40%；该差异无法由提示变化、复杂度或训练数据偏差解释，且对微调高度敏感；语义依赖性真实存在且不稳定。

Conclusion: LLM不执行严格意义上的算法，而是语义依赖的输入近似；这一缺陷具有普遍性，可能影响所有以语义为输入的功能，对可靠性与可预测性构成根本挑战。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [220] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 本文提出了面向食品服务和零售商店（FSRS）场景的多模态大语言模型Ostrakon-VL、首个FSRS专用基准ShopBench，以及质量感知的无偏自动化数据整理方法QUAD，显著提升了模型在真实噪声数据下的鲁棒性与参数效率。


<details>
  <summary>Details</summary>
Motivation: FSRS场景中存在真实数据噪声高、缺乏可审计闭环数据整理，以及缺乏统一细粒度多模态评估基准两大挑战。

Method: 提出Ostrakon-VL模型（基于Qwen3-VL-8B）、ShopBench基准（覆盖单图/多图/视频输入）和QUAD多阶段多模态指令数据整理流程，并采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上达60.1平均分，超越更大规模的Qwen3-VL-235B-A22B（59.4）和同规模Qwen3-VL-8B（55.3），验证了其参数效率与鲁棒性。

Conclusion: Ostrakon-VL显著增强了FSRS场景下感知与决策的可靠性；模型与ShopBench将开源以支持可复现研究。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [221] [Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms](https://arxiv.org/abs/2601.21344)
*Hassam Tahir,Faizan Faisal,Fady Alnajjar,Muhammad Imran Taj,Lucia Gordon,Aila Khan,Michael Lwin,Omar Mubin*

Main category: cs.AI

TL;DR: 本文提出了一种将大语言模型（LLM）集成到协作学习平台的框架，通过LLM作为动态主持人提升学生参与度、批判性思维与包容性，并采用模块化架构（ReactJS前端、Flask后端）实现个性化互动与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决现有教育平台中静态主持和个性化不足的问题，提升协作学习中的参与度、批判性思维与教育包容性。

Method: 构建基于LLM的动态主持人框架，结合ReactJS前端、Flask后端与高效问题检索模块；引入鲁棒反馈机制优化AI主持、促进反思性学习并平衡用户参与。

Result: 实验证明该框架显著提升学生协作效果、加深理解程度，并在多学科及不同用户群体中具有良好可扩展性。

Conclusion: 该工作为下一代AI驱动教育工具奠定坚实基础，推动公平且有影响力的学习成果发展。

Abstract: This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.

</details>


### [222] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: 本文提出BEAP-Agent，一种基于深度优先搜索（DFS）的GUI智能体框架，通过支持长距离、多层级状态回溯及动态任务追踪，显著提升GUI任务执行的鲁棒性与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体在探索路径出错后难以恢复，导致任务失败，缺乏系统性的回溯机制。

Method: 将GUI任务执行建模为DFS过程，设计包含Planner、Executor和Tracker三个协同组件的BEAP-Agent框架，支持动态任务跟踪与多级状态回溯。

Result: 在OSWorld基准上达到28.2%的任务准确率，验证了方法的有效性。

Conclusion: BEAP-Agent填补了GUI智能体系统化回溯机制的空白，为长周期GUI任务探索提供了新范式。

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [223] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: 本文介绍了SONIC-O1，一个面向真实世界对话场景、涵盖13个领域、含4958条人工标注的高质量音视频多模态大模型评测基准，重点评估开放摘要、多选问答和时序定位等任务，并揭示了现有模型在时序理解与社会鲁棒性上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM研究主要集中于静态图像理解，而对动态音视频序列的理解能力缺乏系统评估，亟需一个高质量、真实场景驱动、具备社会意识的评测基准。

Method: 构建了完全由人工验证的SONIC-O1基准，覆盖13个真实对话领域，包含4958条带人口统计元数据的标注样本；设计三类核心任务：开放摘要、多选问答（MCQ）和带推理依据的时序定位；在闭源与开源MLLM上开展系统评测。

Result: 实验发现：两类模型在MCQ任务上性能差距较小，但在时序定位任务上存在高达22.6%的显著差异；且所有模型在不同人口统计子群中表现持续下降，暴露出社会鲁棒性不足。

Conclusion: SONIC-O1填补了音视频时序多模态理解评测的空白，提供了首个兼顾时间接地性与社会鲁棒性的开放评测套件，推动MLLM向更可靠、公平的现实应用发展。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [224] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: 本文提出Fovea-Block-Skip Transformer（FBS），通过引入Parafovea-Attention Window、Chunk-Head和Skip-Gate三个模块，在不增加参数的前提下提升大语言模型推理的质量与效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理仍依赖严格的逐token自回归，缺乏人类阅读中的内容自适应前瞻、块结构感知计算分配以及预览/略读的训推一致性。

Method: 提出FBS架构，包含因果可训练循环机制，具体由Parafovea-Attention Window（PAW）、Chunk-Head（CH）和Skip-Gate（SG）三个模块组成。

Result: 在多个基准测试中，FBS显著改善质量-效率权衡；消融实验表明三个模块互补且有效。

Conclusion: FBS为大语言模型推理提供了更符合人类阅读机制的高效新范式，无需额外参数即可实现性能提升。

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [225] [Hebbian Learning with Global Direction](https://arxiv.org/abs/2601.21367)
*Wenjia Hua,Kejie Zhao,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 本文提出了一种模型无关的全局引导赫布学习（GHL）框架，通过结合局部赫布学习与全局任务导向信号，在保持生物可解释性的同时提升性能，尤其在大规模网络和复杂数据集（如ImageNet）上显著缩小与反向传播的差距。


<details>
  <summary>Details</summary>
Motivation: 反向传播虽成功但缺乏生物可解释性且计算开销大；传统赫布学习仅依赖局部信息，难以兼顾全局任务目标，限制其可扩展性。

Method: 提出Global-guided Hebbian Learning（GHL）框架：局部部分采用Oja规则加竞争学习以保证稳定性；全局部分引入基于符号的引导信号，调控局部赫布可塑性方向。

Result: 在多种网络和任务上持续优于现有赫布学习方法；在ImageNet等大规模复杂任务上达到与标准反向传播具有竞争力的性能，显著缩小性能差距。

Conclusion: GHL成功融合局部可塑性与全局任务指导，是一种兼具生物合理性、可扩展性和高性能的新型训练范式，为替代反向传播提供了有力候选方案。

Abstract: Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.

</details>


### [226] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: NEMO 是一个将自然语言描述的决策问题自动转化为可执行数学优化实现的系统，通过与自主编码代理（ACAs）远程协作，确保生成代码的正确性和可执行性，并在多个基准上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专用大语言模型或定制化代理，常导致生成代码语法错误或不可执行，缺乏鲁棒性和可验证性。

Method: 提出以自主编码代理（ACAs）为核心的设计，引入异步验证循环、外部记忆复用、最小贝叶斯风险解码和自一致性等新型协调机制，并在沙箱环境中执行与验证代码。

Result: 在九个标准优化基准上评估，NEMO 在大多数任务中达到当前最优性能，多个数据集上显著领先。

Conclusion: 执行感知的智能体架构能有效提升自动化优化建模的准确性、鲁棒性与实用性。

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [227] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 本文提出了一种基于教学大纲的评估框架，用于衡量大语言模型（LLMs）作为教学助手的教学能力，通过学生在多轮教学后的表现提升来评估，并在高考数据上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注问题解决或问题级指导，缺乏对知识中心型教学能力的评估，因此需要一种更全面、结构化的教学能力评测方法。

Method: 构建了一个以教学大纲为依据的评估框架，限制教师模型仅使用结构化知识点和例题进行多轮教学，并以学生表现提升作为核心指标；在高考（Gaokao）多学科数据上实例化该框架。

Result: 实验显示不同模型及学科间教学效果差异显著（如数学表现较好，物理与化学较弱）；引入例题未必提升教学效果，模型易陷入针对例题的纠错而非通用知识传授。

Conclusion: 教学能力是大语言模型行为中一个独立且可量化的维度，需专门设计评估框架加以衡量。

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [228] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种认知启发的LLM后训练框架CoMT+CCRL，将人类解题的两阶段（抽象策略学习+具体任务适配）融入模型训练，在提升泛化能力的同时显著降低训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练方法（SFT+RL）以完整推理轨迹为单位进行优化，与人类先学抽象策略、再适配具体问题的认知过程不一致，导致策略与执行耦合、泛化性差、效率低。

Method: 提出两阶段框架：1）Chain-of-Meta-Thought（CoMT），通过监督学习抽象推理模式（剥离具体执行），获取可迁移的元知识；2）Confidence-Calibrated RL（CCRL），在中间步骤引入置信度感知奖励，抑制错误累积，提升执行可靠性。

Result: 在4个模型、8个基准上，相比标准方法，ID和OOD性能分别提升2.19%和4.63%，训练时间减少65–70%，token消耗降低50%。

Conclusion: 将LLM后训练与人类认知机制对齐，不仅能提升模型泛化能力，还能大幅提高训练效率，验证了认知启发设计的有效性与实用性。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [229] [Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data](https://arxiv.org/abs/2601.22141)
*Grzegorz Stefanski,Alberto Presta,Michal Byra*

Main category: cs.AI

TL;DR: 本文提出Routing the Lottery (RTL)框架，通过发现多个针对不同类别、语义簇或环境条件的自适应子网络（adaptive tickets），实现数据异质性与模型结构的对齐，显著提升性能并减少参数量，同时提出子网络相似度分数以无标签方式诊断过度稀疏化问题。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法假设存在一个适用于所有输入的通用‘中奖彩票’，忽略了真实世界数据的固有异质性。

Method: 提出Routing the Lottery (RTL)自适应剪枝框架，自动发现多个专门化的子网络（adaptive tickets），并引入子网络相似度分数用于无标签诊断子网络坍塌问题。

Result: RTL在多个数据集和任务上持续优于单/多模型基线，在平衡准确率和召回率上表现更优，参数量最多减少为独立模型的1/10，并展现出语义对齐特性；同时识别出子网络坍塌现象并提供诊断工具。

Conclusion: 剪枝应被重新理解为对齐模型结构与数据异质性的机制，推动模块化与上下文感知深度学习的发展。

Abstract: In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.

</details>


### [230] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 本文揭示了大型语言模型在处理否定指令（如“不应做某事”）时存在严重缺陷，常将禁止性指令误解为肯定性指令，导致高风险场景下的安全隐患；研究提出Negation Sensitivity Index（NSI）作为评估指标，并建议分层认证框架以提升部署安全性。


<details>
  <summary>Details</summary>
Motivation: 当用户用否定语句（如“不应做X”）表达禁止意图时，AI系统应准确识别并遵守该禁令；但现实中许多大模型错误地将否定指令解释为肯定指令，带来严重安全风险，亟需系统性评估与治理。

Method: 对16个大语言模型在14个伦理场景中进行审计，对比其在肯定、简单否定和复合否定指令下的响应行为；采用确定性解码排除采样噪声干扰；设计案例研究、提出Negation Sensitivity Index（NSI）指标，并构建分层认证框架。

Result: 开源模型在简单否定下违规率达77%，复合否定下达100%；商业模型虽表现更好，但仍存在19–128%的响应波动；模型间一致性从肯定提示下的74%降至否定提示下的62%；金融场景比医疗场景脆弱两倍；所有结果在确定性解码下稳定复现。

Conclusion: 当前对齐技术未能确保模型可靠区分‘做X’与‘不做X’，因此在高风险领域自主决策前，必须通过NSI等指标进行严格评估与分级认证。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [231] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体强化学习框架SCMA，通过两个专门代理（分段代理和评分代理）识别并加权惩罚冗余推理片段，从而在不牺牲准确性的前提下显著缩短大推理模型的响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法通过简单长度惩罚与结果奖励耦合来减少冗余推理，但难以兼顾简洁性与准确性，易损害关键推理逻辑。

Method: 提出多智能体RL框架SCMA，包含分段代理（将推理分解为逻辑块）、评分代理（评估各块重要性）和推理代理（在重要性加权长度惩罚下生成精简推理）。

Result: 在多个模型规模上，SCMA将响应长度减少11.1%–39.0%，同时提升准确率4.33%–10.02%；消融与定性分析证实其协同优化可激发涌现行为，优于单智能体RL方法。

Conclusion: SCMA通过解耦冗余识别与推理生成，实现了高效、准确的大推理模型压缩，在部署中消除推理开销，为LRM实用化提供了新范式。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [232] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 本文揭示了指令微调的大语言模型在高风险决策中对情感框架效应具有出人意料的强鲁棒性，即其逻辑判断几乎不受叙事偏差影响，与人类显著不同。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）已知对提示词微小变化敏感且易迎合用户偏见，但其在规则约束型、高风险决策中的稳健性尚未被充分研究。

Method: 提出一种新颖的受控扰动框架，在医疗、法律和金融三大高风险领域系统评估LLM对情感框架效应的响应；采用Cohen's h量化模型与人类受试者的行为差异，并跨多种训练范式的模型验证结果一致性。

Result: 指令微调LLM表现出极强的框架不变性：效应量近乎为零（Cohen's h = 0.003），而人类则呈现显著偏差（Cohen's h ∈ [0.3, 0.8]）；模型鲁棒性达人类的110–300倍，且该现象跨模型架构稳定存在。

Conclusion: LLM的‘提示敏感性’与‘逻辑稳定性’可解耦；其能坚守规则逻辑、抵抗叙事诱导偏差，有望在制度化决策中辅助甚至去偏人类判断。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [233] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 本文提出了一种面向AI辅助芯片设计的综合性基准测试ChipBench，涵盖Verilog生成、调试和参考模型生成三大任务，揭示了当前大语言模型在硬件工程任务中性能远低于饱和基准的表现，并提供了用于提升参考模型生成能力的数据生成工具箱。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在硬件工程中的基准测试存在饱和和任务多样性不足的问题，无法反映其在真实工业流程中的实际性能。

Method: 构建了一个包含44个复杂模块、89个系统性调试案例和132个跨语言（Python/SystemC/CXXRTL）参考模型样本的综合性芯片设计基准；同时开发了自动化工具箱用于高质量训练数据生成。

Result: 最先进模型Claude-4.5-opus在Verilog生成和Python参考模型生成任务中分别仅达到30.74%和13.33%的通过率，显著低于现有饱和基准中>95%的表现。

Conclusion: 当前LLMs在AI辅助芯片设计任务中仍面临巨大挑战，ChipBench为该领域提供了更真实、更具区分度的评估标准，并推动参考模型生成等关键方向的研究进展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [234] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出Agent-RRM多维度推理奖励模型，通过结构化反馈（推理轨迹、聚焦式批评、整体评分）提升Agentic RL训练效果，并设计三种集成策略，其中Reagent-U在多个基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic RL方法依赖稀疏的结果导向奖励，无法区分中间推理质量，导致训练效果次优。

Method: 提出Agent Reasoning Reward Model（Agent-RRM），提供包含推理轨迹、聚焦批评和整体评分的结构化反馈；设计三种反馈集成策略：Reagent-C（文本增强式修正）、Reagent-R（奖励增强式引导）、Reagent-U（统一反馈集成）。

Result: Reagent-U在12个基准测试中表现最优，GAIA达43.7%，WebWalkerQA达46.2%。

Conclusion: 结构化推理反馈能显著提升Agentic RL训练效果，Reagent-U是最有效的集成策略；代码、模型与数据集均已开源。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


### [235] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: 本文提出LION框架，基于Clifford代数和解耦图神经范式，解决多模态图中模态对齐忽视图上下文、模态融合缺乏适应性两大问题，显著提升多模态图学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图学习方法存在两个关键缺陷：一是模态对齐时忽视图结构上下文，抑制模态交互；二是模态融合策略缺乏对齐后带拓扑先验的token的自适应利用，导致泛化性差、性能下降。

Method: 提出基于Clifford代数的LION框架，采用‘先传播后聚合’的解耦图神经范式：首先构建模态感知的几何流形，通过几何驱动的高阶图传播实现模态交互与对齐；其次基于对齐token的几何阶性质，设计自适应全息聚合模块，融合几何阶的能量、尺度与可学习参数以增强模态融合。

Result: 在9个数据集上的实验表明，LION在3类图下游任务和3类模态下游任务上均显著优于现有SOTA基线。

Conclusion: LION通过几何建模与自适应融合机制，有效解决了多模态图中对齐与融合的关键瓶颈，为多模态图表示学习提供了新范式。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [236] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR是一种多模态记忆代理，通过将结构化富文本记忆渲染为图像，在有限上下文窗口内自适应分配记忆空间，提升长视野推理性能。


<details>
  <summary>Details</summary>
Motivation: 长视野智能体推理需在有限上下文窗口中压缩不断增长的交互历史，而现有基于纯文本的记忆系统存在token成本均匀、低价值细节占用过多预算的问题。

Method: MemOCR构建结构化富文本记忆（如标题、高亮），并将其渲染为图像供代理视觉访问；采用预算感知的强化学习进行训练，使其在不同内存预算下保持鲁棒性。

Result: 在长上下文多跳和单跳问答基准上，MemOCR显著优于强文本基线，并在极端预算下实现更高效上下文利用。

Conclusion: 视觉布局驱动的自适应信息密度分配是提升长视野推理中记忆效率的有效范式，MemOCR为上下文受限场景下的智能体记忆设计提供了新思路。

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [237] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: 本文提出ScaleSim系统，通过引入'调用距离'抽象来预测多智能体仿真中LLM请求的相对顺序，实现内存高效的大规模多智能体仿真。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体仿真因每个智能体需在GPU上维护私有状态（如模型、前缀缓存、适配器）而面临严重的GPU内存压力，难以扩展。

Method: 基于对典型工作负载的分析，提出‘调用距离’作为统一抽象以估计智能体未来LLM请求的相对顺序；在此基础上设计ScaleSim系统，支持主动预取、优先级驱逐及模块化智能体专属内存管理。

Result: ScaleSim在仿真基准测试中相比SGLang最高实现1.74倍加速。

Conclusion: ScaleSim通过建模智能体调用稀疏性与可预测性，在不牺牲性能前提下显著缓解GPU内存瓶颈，为大规模多智能体仿真提供了可行的扩展路径。

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [238] [LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI](https://arxiv.org/abs/2601.21511)
*Niki van Stein,Anna V. Kononova,Lars Kotthoff,Thomas Bäck*

Main category: cs.AI

TL;DR: 本文提出LLaMEA-SAGE，利用从生成算法抽象语法树中提取的图论与复杂度特征，结合可解释AI技术构建自然语言变异指令，以指导大语言模型驱动的自动算法设计（AAD），在多个基准测试中显著提升搜索效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于进化框架的自动算法设计（如LLaMEA）仅依赖适应度反馈进行搜索，忽略了生成代码中蕴含的丰富结构信息，导致搜索效率不高。

Method: 提出一种基于抽象语法树（AST）的图论与复杂度特征提取机制，并利用历史评估解构建代理模型；通过可解释AI识别关键性能影响特征，将其转化为自然语言变异指令，嵌入LLaMEA框架形成LLaMEA-SAGE。

Result: 在小规模控制实验中，LLaMEA-SAGE达到与原始LLaMEA相同性能所需迭代更少；在MA-BBOB大规模基准上，其性能优于当前最优AAD方法。

Conclusion: 从代码结构中提取的信号可有效引导大语言模型驱动的算法进化，弥合了代码形式化结构与人类可理解性能反馈之间的鸿沟。

Abstract: Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.
  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.

</details>


### [239] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA is a framework that structures multi-expert LLM reasoning into argumentation graphs, enabling causal analysis and correction of reasoning chains to improve accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: Existing multi-expert LLM systems use simple aggregation, hiding which arguments actually drive decisions; there's a need for transparent, causally interpretable reasoning.

Method: ARGORA constructs explicit argumentation graphs from expert outputs, interprets them as causal models to perform ablation (removing arguments), and introduces a correction mechanism that aligns internal reasoning with external judgments upon disagreement.

Result: ARGORA achieves competitive accuracy across benchmarks and open-ended tasks, resolves expert disagreements toward correct answers more often than it introduces errors, and provides causal diagnostics of decisive arguments.

Conclusion: Explicit argumentation graphs combined with causal reasoning and correction enable both improved performance and interpretability in multi-expert LLM systems.

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [240] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: 本文提出了一种名为Meta Context Engineering (MCE) 的新框架，通过元级与基础级代理协同进化上下文工程技能和上下文构件，显著提升大语言模型推理时的上下文优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有上下文工程（CE）方法依赖人工设计、结构僵化、优化空间受限，难以适应多样化任务需求。

Method: 提出双层MCE框架：元级代理通过'代理交叉'（agentic crossover）在技能历史、执行与评估中进行反思式搜索以精炼CE技能；基础级代理执行技能、从训练轨迹中学习，并将上下文建模为灵活的文件与代码形式。

Result: 在五个不同领域、离线与在线设置下验证，MCE相较当前最优代理式CE方法取得5.6–53.8%（平均16.9%）的相对性能提升，且在上下文适应性、可迁移性及上下文/训练效率方面均更优。

Conclusion: MCE突破了传统CE的手动与静态范式，实现了上下文工程能力的自主演化与泛化，为LLM推理优化提供了更具扩展性与鲁棒性的新路径。

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [241] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文提出ALiCoT框架，通过理论分析揭示隐式链式思维（CoT）压缩中高阶逻辑依赖的学习信号衰减问题，并设计NatBool-DAG基准验证，实现54.4倍加速且性能媲美显式CoT。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought（CoT）虽提升大语言模型推理能力，但因生成大量中间token导致计算开销巨大；隐式CoT压缩虽更高效，但其内在机制尚不清晰，尤其缺乏对学习难度的理论解释。

Method: 提出Order-r Interaction理论工具，证明高阶逻辑依赖的学习信号呈指数衰减；构建NatBool-DAG基准以强制不可约逻辑推理；设计ALiCoT框架，通过将潜在token分布与中间推理状态对齐来缓解信号衰减。

Result: ALiCoT在保持与显式CoT相当性能的同时，实现54.4倍推理速度提升；NatBool-DAG验证了不可约推理下隐式CoT的固有难度及ALiCoT的有效性。

Conclusion: 隐式CoT压缩面临高阶交互壁垒和学习信号衰减的根本挑战；ALiCoT通过分布对齐提供了一种理论可解释、实践高效的解决方案，推动高效推理范式发展。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [242] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文提出ATP-Latent方法，通过条件变分自编码器（VAE）建模更平滑的潜在空间，并引入基于解码内容一致性的辅助连贯性奖励进行强化学习，以提升潜在线索推理策略，显著提升准确率并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有潜在线索（latent CoT）方法依赖对单一语言线索标签的被动模仿，易导致表征不佳、训练-测试差距大，限制了潜在空间中的主动规划能力。

Method: 将潜在线索监督建模为条件VAE以获得平滑潜在空间；结合基于VAE解码内容一致性的辅助连贯性奖励，开展强化学习优化推理策略。

Result: 在LLaMA-1B上，ATP-Latent在四个基准上相较先进基线提升4.1%准确率、减少3.3% token消耗。

Conclusion: 主动在潜在空间中进行规划比被动模仿语言线索更重要；ATP-Latent通过VAE建模与一致性引导的RL，有效提升了潜在线索推理的质量与效率。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [243] [CORE: Collaborative Reasoning via Cross Teaching](https://arxiv.org/abs/2601.21600)
*Kshitij Mishra,Mirat Aubakirov,Martin Takac,Nils Lukas,Salem Lahlou*

Main category: cs.AI

TL;DR: 本文提出了一种训练时协作框架Collaborative Reasoning (CORE)，利用模型间的互补性，通过交叉教学机制提升推理性能，在多个数学与科学推理数据集上显著超越单模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理任务中存在互补性错误：同一问题上不同模型可能因分解方式不同而表现各异；如何有效利用这种互补性提升整体性能是核心动机。

Method: 提出CORE框架，包含两阶段求解：冷启动独立采样 + 上下文引导的救援轮（失败模型接收成功同伴提取的提示）；优化目标融合正确性、DPP启发的多样性项和显式救援奖励。

Result: 在GSM8K和MATH上，仅用1000训练样本，3B+4B模型对的Pass@2分别达99.54%和92.08%，远超单模型的82.50%和74.82%；在GPQA和AIME等更难数据集上也取得显著提升。

Conclusion: 训练时协作能有效将模型互补性转化为性能增益，无需扩大模型规模即可实现大幅性能提升。

Abstract: Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.

</details>


### [244] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文将企业级智能文档处理（IDP）系统的早期验证问题建模为搜索式软件测试（SBST）问题，旨在有限预算下最大化发现的多样化失败类型数量；通过在文档配置组合空间中生成结构风险特征，对比多种搜索策略（进化、群智、质量多样性、学习型、量子），发现各策略具有互补性，单一方法无法覆盖全部失败模式，因此推荐采用策略组合方案。


<details>
  <summary>Details</summary>
Motivation: 企业级IDP系统应用于金融、保险、医疗等高风险领域，早期验证需在有限预算下尽可能多地发现多样化的失败机制，而非仅定位最差文档。

Method: 将IDP验证形式化为SBST问题，在文档配置的组合空间中渲染结构风险特征以诱发真实失败；在统一预算约束下，系统评估进化算法、群智能、质量多样性、学习型及量子等多类搜索策略的表现，并通过配置排他性、胜率和跨时间重叠分析其发现能力。

Result: 不同求解器持续发现彼此遗漏的失败模式；跨时间分析显示各求解器存在稳定且独特的发现能力；无单一策略具备绝对优势；所有策略联合可覆盖观测到的全部失败空间，但任一单独策略均会系统性延迟关键风险的发现。

Conclusion: 各类SBST求解器具有内在互补性，应采用组合式策略以提升工业IDP系统验证的鲁棒性与全面性。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [245] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 本文提出了RecNet，一种自演化的偏好传播框架，用于解决现有推荐系统中用户偏好建模稀疏、噪声大、缺乏实时性和用户-物品双向影响的问题。RecNet通过前向阶段（集中式偏好路由与个性化接收机制）和后向阶段（基于反馈的传播优化机制），利用LLM实现多智能体协同下的实时偏好传播与持续自我优化。实验验证了其在多种场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式用户-物品交互的偏好建模方法稀疏、噪声大，无法反映用户与物品之间实时、相互的影响。

Method: 提出RecNet框架，包含前向阶段（路由器代理实现集中式偏好路由 + 带缓冲与规则滤波记忆的个性化偏好接收机制）和后向阶段（基于LLM的多智能体强化学习式反馈驱动传播优化）。

Result: 在多种场景下实验表明RecNet能更有效地建模偏好传播，提升推荐性能。

Conclusion: RecNet通过引入自演化、实时、双向的偏好传播机制，显著提升了基于LLM的智能推荐系统的建模能力与实用性。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [246] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: 本文提出了ScholarGym，一个用于可重现评估学术文献深度研究工作流的仿真环境，解决因调用实时API带来的非确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型大语言模型在复杂研究工作流中面临评估难题，主要由于依赖实时API导致结果不可复现（如时间漂移、限流、后端状态变化）。

Method: 构建ScholarGym仿真环境，基于570K论文静态语料库实现确定性检索，解耦查询规划、工具调用与相关性评估三阶段，并提供2536个带专家标注真值的查询。

Result: 在多种主干模型上的实验揭示了推理能力、规划策略与选择机制在迭代精炼过程中的交互影响。

Conclusion: ScholarGym为深度研究工作流提供了可控、可复现、可细粒度分析的评估框架，推动该方向的稳健发展。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [247] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 本文提出了一种无监督的后门防御框架TCAP，通过分析多模态大语言模型（MLLMs）中系统指令、视觉输入和用户文本查询三部分注意力分配的异常差异，识别并过滤被污染样本，具有跨模型和攻击类型的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖监督信号或难以泛化到不同触发类型和模态，而作者发现后门攻击普遍存在一种通用指纹：中毒样本会破坏系统指令、视觉输入和用户查询三者间平衡的注意力分配。

Method: 提出Tri-Component Attention Profiling（TCAP）：将跨模态注意力图分解为三部分，利用高斯混合模型（GMM）统计建模识别对触发敏感的注意力头，并通过EM算法投票聚合隔离中毒样本。

Result: 在多种MLLM架构和攻击方法上实验表明，TCAP性能稳定且强劲，展现出优异的鲁棒性与实用性。

Conclusion: TCAP是一种通用、无监督、跨模态的后门检测方法，为FTaaS场景下的MLLM安全定制提供了可靠保障。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [248] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: 本文提出E-mem框架，通过模仿生物记忆机制，采用异构分层架构实现情景上下文重建，提升LLM代理在长程推理中的逻辑完整性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆预处理范式（如嵌入或图压缩）会破坏复杂序列依赖的上下文完整性，难以支持System 2式的高精度、长程推理。

Method: 提出E-mem框架，以‘情景记忆重建’替代传统记忆预处理；采用主-从异构架构：多个助理代理保留未压缩的上下文并进行局部推理，主代理负责全局规划与证据聚合；受生物印迹（engram）启发。

Result: 在LoCoMo基准测试中F1达54%以上，较SOTA方法GAM提升7.75%，同时降低超70%的token消耗。

Conclusion: E-mem验证了保留原始上下文并支持局部上下文感知推理的有效性，为构建具备稳健长程推理能力的LLM代理提供了新范式。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [249] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS 是一种模型无关的插件，通过样本自适应 Dropout 机制提升深度时间序列模型对噪声数据的鲁棒性，无需修改模型结构或增加显著参数量。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列模型在真实场景中易受普遍存在的噪声数据影响，而现有鲁棒性方法要么删减数据，要么依赖高成本的先验量化，难以兼顾有效性与效率。

Method: 提出 DropoutTS 插件，采用基于谱稀疏性的样本自适应 Dropout：利用重构残差量化实例级噪声，并将噪声映射为动态 dropout 率，选择性抑制虚假波动、保留细粒度保真度。

Result: 在多种噪声设定和公开基准上，DropoutTS 显著提升主流骨干模型性能，具备先进鲁棒性、极低参数开销且无需架构改动。

Conclusion: DropoutTS 成功将学习范式从‘学什么’转向‘学多少’，为时间序列建模提供了高效、通用、即插即用的鲁棒性增强方案。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [250] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出SCOUT框架，通过轻量级‘scout’模型高效探索非语言环境，再用收集的数据引导大语言模型（LLM）微调与强化学习，显著提升其在符号/空间等未见任务上的表现，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: LLM在非语言类未见环境（如符号、空间任务）中表现受限，主因并非预训练-测试分布不匹配，而是高维语义空间中试错式探索成本过高。

Method: 提出SCOUT框架：用轻量MLP作为‘scout’快速大规模探索环境并生成轨迹；利用轨迹对LLM进行监督微调（SFT），再通过多轮强化学习（RL）激活其隐含世界知识。

Result: Qwen2.5-3B-Instruct在任务上平均得分0.86，超越Gemini-2.5-Pro（0.60），GPU小时消耗减少约60%。

Conclusion: 解耦探索与利用是提升LLM泛化至非语言任务的关键；轻量代理辅助+分阶段微调可高效释放LLM潜在世界建模能力。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [251] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 本文提出了一种无需配对训练数据的零样本气候统计降尺度框架ZSSD，通过引入物理一致性气候先验和统一坐标引导策略，解决了现有方法在泛化性、物理一致性和梯度消失方面的问题，并在多种GCM上显著提升了极端天气事件重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度难以泛化到不同全球气候模型（GCMs），因缺乏配对训练数据且存在与再分析数据之间的固有域差距；当前零样本方法则面临物理不一致和大尺度因子下梯度消失问题。

Method: 提出零样本统计降尺度（ZSSD）框架：1）利用再分析数据学习物理一致性气候先验，以地理物理边界和时间信息为条件确保物理有效性；2）引入统一坐标引导（Unified Coordinate Guidance）以增强跨GCM鲁棒推理并缓解DPS中梯度消失问题。

Result: ZSSD在99百分位误差上显著优于现有零样本基线方法，并能成功重建热带气旋等复杂天气事件，适用于异构GCMs。

Conclusion: ZSSD是一种有效、物理一致且泛化能力强的零样本气候降尺度方法，为无配对数据场景下的高分辨率气候预测提供了新范式。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [252] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 本文提出了一种基于概念空间框架的时序抽象概念建模方法，以国际象棋为实例，将策略概念（如进攻、弃子）表示为可解释质量维度上的几何区域，并将对局建模为轨迹，通过轨迹朝向区域的运动实现策略识别；支持双视角建模，体现不同玩家对同一局面的差异化理解；验证了轨迹模式与专家评论的一致性，并为时序化、目标导向概念建模及知识演化机制提供了理论基础与应用路径。


<details>
  <summary>Details</summary>
Motivation: 扩展经典概念空间理论，使其能建模随时间展开、具有目标导向性的抽象概念（如棋类策略），并支持多视角理解和动态演化。

Method: 构建基于可解释质量维度的几何概念空间，将策略概念表示为区域；将棋局表示为状态轨迹，通过轨迹运动方向识别策略意图；引入双视角建模机制刻画主观解释差异；结合专家评论验证轨迹模式有效性。

Result: 成功实现轨迹驱动的策略概念识别，运动模式与专家分析高度一致；验证了双视角建模可行性；为时序抽象概念建模与知识演化集成提供了可扩展框架。

Conclusion: 概念空间理论可有效拓展至动态、目标导向的抽象概念建模；该轨迹式方法不仅适用于棋类策略理解，也为序列决策、认知建模与持续学习系统提供了新范式。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [253] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: 本文提出了BioAgent Bench，一个用于评估AI代理在生物信息学任务中性能和鲁棒性的基准数据集与评测套件，涵盖RNA-seq、变异检测、宏基因组学等端到端任务，并通过LLM评分器与扰动测试揭示了当前前沿模型在高层流程构建与底层步骤推理间的差距，同时强调开放权重模型在隐私敏感场景下的实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理在生物信息学任务中的评估缺乏系统性基准，尤其缺少对鲁棒性、自动化评估及隐私合规性的综合考量；生物信息学任务常涉及敏感数据与专有资源，需兼顾性能与部署约束。

Method: 构建BioAgent Bench基准：包含真实生物信息学端到端任务（如RNA-seq、variant calling）、结构化提示（明确指定输出产物）、可控扰动（输入损坏、干扰文件、提示膨胀）；采用多类前沿闭源/开源大模型，在不同agent框架下运行，并用LLM-based grader自动评估流程进展与结果有效性。

Result: 前沿AI代理可在无需复杂定制的情况下完成多步生物信息学流程并稳定产出目标产物；但在扰动测试下暴露显著失败模式，表明高层流程正确性不等于底层步骤可靠推理；闭源模型因隐私风险在临床/科研敏感场景受限，而开源模型虽完成率较低但更具部署可行性。

Conclusion: BioAgent Bench为AI代理在生物信息学领域的评估提供了首个综合性、可扩展、注重鲁棒性与隐私意识的基准；研究呼吁关注‘步骤级可靠性’而非仅‘流程级成功’，并倡导在实际应用中依据数据敏感性权衡模型选型。

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [254] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 本文提出了一种以大语言模型（LLM）为中心的视频活动识别与可解释反馈生成框架，用于内窥镜吸痰（ES）操作培训，显著提升了识别准确率（+15–20%）并支持自然语言反馈和教学指导。


<details>
  <summary>Details</summary>
Motivation: 内窥镜吸痰（ES）操作风险高、依赖经验，尤其在家庭护理与教学场景中缺乏持续监督；现有自动化识别与反馈系统研究匮乏。

Method: 构建统一的LLM中心化框架：利用LLM进行视频时空活动识别与可解释决策分析，并结合异常检测与XAI设计学生支持模块，实现自然语言反馈生成。

Result: LLM方法在准确率和F1分数上较传统机器学习与深度学习基线提升约15–20%；成功实现可解释、面向教学的自动化反馈。

Conclusion: 该框架为护理教育提供了可扩展、可解释、数据驱动的新范式，有助于提升培训效率与患者安全。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [255] [CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge](https://arxiv.org/abs/2601.21822)
*Zitong Yu,Boquan Sun,Yang Li,Zheyan Qu,Xing Zhang*

Main category: cs.AI

TL;DR: 本文提出CORE框架，通过在边缘计算环境中协同调度多个具有不同功能角色的大型语言模型（LLM），解决6G网络中异构资源碎片化导致的复杂推理能力不足问题；该框架包含实时感知、动态角色编排和流水线并行执行三大优化模块，并设计了角色亲和性调度算法，在多种6G场景下显著提升了系统效率与任务完成率，并在真实边缘平台验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 6G网络与大语言模型快速发展推动泛在智能，但层级网络中碎片化、异构的计算资源难以支撑单个LLM代理完成复杂推理任务。

Method: 提出CORE（Collaborative Orchestration Role at Edge）框架，采用多角色分布式LLM协同学习系统，部署于移动设备与分层边缘服务器；集成实时感知、动态角色编排、流水线并行执行三大优化模块，并设计角色亲和性调度算法实现资源-需求智能匹配。

Result: 在多种6G应用案例与性能评估中，CORE显著提升系统效率与任务完成率；并在真实边缘计算平台上成功部署，展现出强鲁棒性与实用性。

Conclusion: CORE为6G环境下分布式LLM协同推理提供了可行且高效的架构范式，有效弥合了异构边缘资源与复杂AI任务之间的鸿沟，推动泛在智能落地。

Abstract: Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.

</details>


### [256] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 本文提出了一种面向心电图（ECG）专用基础模型（FMs）的综合基准评估框架，不仅评估下游任务性能，还通过SHAP和UMAP进行表征级分析，尤其关注数据稀缺等现实医疗场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型评估多集中于下游任务性能，缺乏对嵌入表征泛化能力（尤其在医疗等高风险领域）的深入、系统性评估。

Method: 提出融合性能评估与表征级分析的基准方法，结合SHAP（可解释性）和UMAP（可视化降维），在多洲际数据集及不同数据可用性（含数据稀缺）条件下对多种ECG专家FM进行大规模评测。

Result: 实验表明该协议能深入揭示ECG专家FM嵌入模式的内在结构与泛化特性，提供比传统评估更丰富的洞察。

Conclusion: 所提基准框架为负责任地部署ECG基础模型提供了关键评估工具，强调表征质量与泛化能力在医疗AI中的核心地位。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [257] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文提出一个以决策为中心的仿真软件框架，用于在真实库存管理场景中系统评估预测模型，强调预测模型对运营关键绩效指标（如总成本和服务水平）的影响，而非仅关注统计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多使用统计准确度指标（如MAE、RMSE）评估预测模型，但这些指标与实际运营KPI（如成本、服务水平）之间的关系尚不明确，导致模型选择缺乏运营依据。

Method: 构建一个闭环仿真框架，包含三部分：(i) 面向备件需求特征的合成需求生成器；(ii) 可集成任意预测模型的灵活预测模块；(iii) 消耗预测结果并计算运营KPI的库存控制仿真器。

Result: 通过多场景仿真实验发现：统计误差的改善不一定带来运营性能提升；统计误差相近的模型可能导致显著不同的成本-服务水平权衡。

Conclusion: 预测模型评估应转向运营相关性；该框架为汽车售后及类似领域提供了连接需求预测与库存决策的实用工具和选型指导。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [258] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: 本文提出KnowBias框架，通过增强而非抑制编码偏见知识的神经元来减轻大语言模型中的社会偏见，具有轻量、泛化性强、数据高效且不损害通用能力的特点。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法多采用压制范式，存在脆弱、泛化弱、数据效率低及损害模型通用能力等问题，亟需一种更鲁棒、高效的新范式。

Method: KnowBias基于归因分析，利用少量偏见知识相关的是/否问题识别编码偏见知识的神经元，并在推理时选择性增强这些神经元。

Result: 在多个基准和大语言模型上实验表明，KnowBias实现了一致的最优去偏性能，同时仅造成极小的效用下降。

Conclusion: KnowBias提供了一种概念新颖、实用性强的去偏新路径，突破了传统压制范式的局限，在保持模型能力的同时实现高效、泛化、数据友好的偏见缓解。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [259] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: 本文提出WebArbiter，一种基于推理、原理驱动的网页过程奖励模型（WebPRM），通过文本生成方式输出结构化理由与偏好判断，提升网页导航任务中长程决策的可解释性与泛化能力；采用两阶段训练（推理蒸馏+强化学习）并发布新基准WebPRMBench，在多个指标上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有网页过程奖励模型存在局限：标量模型信号粗糙、弱接地；清单式模型依赖模板匹配，易受页面或语义变化影响，且缺乏可解释性；同时，网页任务中结果监督稀疏、延迟，难以支持推理时扩展。

Method: 提出WebArbiter，将奖励建模建模为文本生成任务，输出含结构化理由、最终偏好判决及最优动作识别的推理链；采用两阶段训练：1）推理蒸馏，使模型掌握原则引导的连贯推理；2）强化学习，校正教师偏差，使判决直接对齐任务正确性。

Result: 在自建基准WebPRMBench上，WebArbiter-7B比最强基线GPT-5高9.1分；在WebArena-Lite轨迹搜索任务中，较此前最佳WebPRM提升最高达7.2分。

Conclusion: WebArbiter通过推理优先、原理诱导的设计，显著提升了网页过程奖励建模的可解释性、鲁棒性与泛化能力，为复杂网页自动化任务提供了更可靠、可调试的监督信号。

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [260] [Making Models Unmergeable via Scaling-Sensitive Loss Landscape](https://arxiv.org/abs/2601.21898)
*Minwoo Jang,Hoyoung Kim,Jabin Koo,Jungseul Ok*

Main category: cs.AI

TL;DR: 本文提出Trap²，一种架构无关的模型保护框架，通过在微调过程中对权重进行重缩放编码保护，防止未经授权的模型合并，同时保持模型独立使用的有效性。


<details>
  <summary>Details</summary>
Motivation: 模型中心的兴起使得模型组件复用更加便捷，但也带来了治理缺口：下游用户可将已发布的权重重新组合成未经授权的混合体，从而绕过安全对齐或许可条款。现有防御方法多为事后且依赖特定架构，难以在不同架构和发布格式中提供一致保护。

Method: 提出Trap²框架，在微调阶段将保护机制编码进权重更新中，不依赖具体架构；采用权重重缩放作为模型合并过程的简单代理，使发布权重在独立使用时有效，但在合并常发生的重缩放下性能下降，从而阻碍未授权合并。

Result: Trap²在多种架构和发布格式（如适配器、全模型）下均能提供一致、有效的保护，实验证明其能显著削弱未经授权的模型合并效果，同时不影响模型单独部署时的性能。

Conclusion: Trap²是一种实用、通用且轻量的模型版权与安全治理方案，为模型分发环节提供了前置性、架构无关的防护能力。

Abstract: The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.

</details>


### [261] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: 本文提出了AgenticSimLaw，一种基于角色结构的多智能体辩论框架，用于高风险表格决策任务，通过法庭式编排实现透明可控的测试时推理。


<details>
  <summary>Details</summary>
Motivation: 解决现有黑箱方法在高风险决策任务中缺乏透明性、可控性和可审计性的问题，特别是在刑事司法等需要伦理复杂性和人类监督的领域。

Method: 设计了包含检察官、辩护人和法官角色的多智能体辩论框架，采用7轮结构化辩论协议与私有推理策略，并在NLSY97数据集上进行青年再犯预测实验，对比近90种模型与策略组合下的链式思维（CoT）提示方法。

Result: 结构化多智能体辩论相比单智能体推理展现出更稳定、泛化性更强的性能，准确率与F1分数相关性更高；同时提供细粒度推理控制、完整交互日志以支持可解释性，并支持智能体行为系统性分析。

Conclusion: AgenticSimLaw为高风险、需审慎决策的任务提供了可推广的多智能体框架，解决了LLM多智能体系统中的组织性、可观测性与责任性三大挑战。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [262] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: 本文提出DeR2评估框架，用于分离和量化大语言模型在科学推理中的检索能力与推理能力，通过控制实验揭示模型在多步合成、去噪和证据驱动结论生成等方面的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以区分大语言模型的检索能力与真实推理能力，且易受参数化记忆和网络数据波动干扰，无法准确衡量其对全新科学知识的推理能力。

Method: 构建DeR2深度研究沙盒，通过四种证据访问模式（仅指令、仅概念、仅相关文档、全集含干扰项）解耦检索与推理；采用两阶段验证防止参数泄露；提供冻结文献库、专家标注概念与验证推理链以确保可复现性。

Result: 在多个SOTA基础模型上的实验显示显著性能差异与提升空间：部分模型出现模式切换脆弱性（全集下表现反不如仅指令），另一些则存在结构性概念误用（能命名概念但无法正确执行）。

Conclusion: DeR2有效揭示了当前大语言模型在科学推理中检索与推理能力的短板，为针对性改进提供了细粒度归因工具与可靠基准。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [263] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: 本文提出ToolWeaver框架，通过将工具编码为分层序列而非独立token，解决现有生成式工具调用方法在可扩展性、泛化性和语义建模上的瓶颈；该方法实现词汇增长对数化，并利用共享代码的密集共现增强协作模式学习，在近4.7万工具规模上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的工具使用流程存在语义捕捉不足和LLM缺乏固有工具知识的问题；而主流生成式方法将每个工具映射为唯一新token，导致词汇爆炸、泛化困难及难以建模工具间协作关系。

Method: 提出ToolWeaver框架：设计新型分词机制，将工具内在语义与外在共用模式融合生成结构化分层代码；通过生成式对齐阶段对LLM进行微调，使其直接生成该分层代码序列。

Result: 在近47,000个工具的大规模评估中，ToolWeaver显著超越当前最优方法，展现出更强的可扩展性、泛化能力和语义感知能力。

Conclusion: ToolWeaver为构建高级工具增强型智能体提供了更可扩展、更通用且语义更丰富的生成式工具学习基础。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [264] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: 本文提出VAF评估框架，系统研究网页视觉属性对Web智能体决策行为的影响，发现背景对比度、项目尺寸、位置和卡片清晰度影响显著，而字体样式、文本颜色和图像清晰度影响较小。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注Web智能体在对抗攻击下的鲁棒性，缺乏对良性环境下视觉属性如何影响其偏好的系统分析。

Method: 提出VAF三阶段可控评估流程：（i）生成语义一致但视觉属性不同的网页变体；（ii）模拟人类浏览行为（滚动与点击）；（iii）通过目标点击率与目标提及率联合评估视觉属性影响。

Result: 在8类48个变体、5个真实网站、4种主流Web智能体上的实验表明：背景色对比度、项目尺寸、位置及卡片清晰度对智能体行为影响强；字体样式、文本颜色和项目图像清晰度影响弱。

Conclusion: Web智能体的决策存在显著视觉偏好，VAF为理解并优化其视觉敏感性提供了可量化的评估工具。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [265] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 本文探讨了自动规划中领域模型设计对能耗的影响，提出了一种可配置的领域模型框架，并通过实验发现领域级修改能显著影响不同规划器的能耗，且能耗与运行时间并不总是一致。


<details>
  <summary>Details</summary>
Motivation: 尽管自动规划计算需求高，但其能源效率却鲜有关注；而领域模型与算法分离的模块化结构为通过领域模型设计系统分析能耗提供了可能。

Method: 引入一种领域模型配置框架，支持对元素顺序、动作元数和死区状态等特征进行可控变化；在五个基准领域和五个前沿规划器上，对每个基准的32个领域变体进行能耗与运行时间的实证分析。

Result: 领域级修改可在不同规划器上产生可观测的能耗差异；能耗与运行时间并非总是正相关。

Conclusion: 领域模型设计是影响自动规划能耗的关键因素，Green AI视角下应将能耗作为与精度、运行时间同等重要的性能指标。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [266] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 本文研究了语言模型中陈述偏好与揭示偏好之间的差距（SvR gap），发现这种差距高度依赖于偏好 elicitation 协议；引入中立/弃权选项可提升陈述与揭示偏好的相关性，但揭示阶段允许弃权反而导致相关性崩溃；系统提示引导亦未稳定改善相关性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如二元强制选择）将真实偏好与协议诱导的伪影混杂，难以准确衡量语言模型的 stated-revealed 偏好一致性。

Method: 在24个语言模型上系统比较不同偏好elicitation协议：包括是否允许中立/弃权选项、是否在revealed偏好阶段使用system prompt进行stated偏好引导，并计算Spearman秩相关系数ρ。

Result: 允许中立/弃权提升stated与forced-choice revealed偏好的ρ；但在revealed阶段允许弃权导致ρ趋近零或负值；system prompt引导在AIRiskDilemmas上未能可靠提升ρ。

Conclusion: SvR相关性高度依赖elicitation协议，偏好测量需专门建模不确定/无偏好状态，不能简单套用传统强制选择范式。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [267] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: 本文提出VERSA框架，通过状态转移模型验证和修正足球赛事流数据中的逻辑不一致问题，显著提升数据可靠性及下游分析（如VAEP）性能。


<details>
  <summary>Details</summary>
Motivation: 足球事件流数据存在逻辑不一致（如事件顺序错误、缺失事件）等数据质量问题，严重限制了分析模型的可靠性。

Method: 提出基于状态转移模型的VERSA验证框架，自动检测并修正事件流中的异常模式。

Result: 在K联赛2024赛季数据上发现18.81%事件存在逻辑不一致；VERSA提升了跨数据提供商一致性，并显著增强了VAEP等下游任务的鲁棒性与性能。

Conclusion: 事件流数据的系统性验证能有效提升数据驱动分析的可靠性，VERSA为体育领域提供了可扩展的数据质量保障方案。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [268] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 本文提出Liquid Interfaces（液态接口）范式，将接口视为运行时通过意图表达与语义协商动态生成的短暂关系事件，而非静态技术契约；并形式化定义Liquid Interface Protocol（LIP），支持意图驱动交互、协商执行及语义不确定性下的短暂性保障。


<details>
  <summary>Details</summary>
Motivation: 现有软件架构难以支撑具有自适应、概率性和上下文相关推理能力的自主智能体，系统集成仍依赖静态接口和确定性契约，缺乏对动态语义协调的支持。

Method: 提出Liquid Interfaces协调范式，形式化建模其行为，并设计Liquid Interface Protocol（LIP）协议；进一步分析治理影响，并构建参考架构验证可行性。

Result: 实现了支持意图驱动、语义协商与运行时动态生成接口的协议LIP，并通过参考架构证明该范式在代理系统中具备实践可行性。

Conclusion: Liquid Interfaces为基于智能体的自适应协调提供了原理性基础，推动软件架构从静态契约向动态语义协同演进。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [269] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: 本文提出Operational Intensity (OI)和Capacity Footprint (CF)两个新指标，用于分析AI智能体推理中的内存容量与带宽瓶颈，指出传统roofline模型的不足，并提出解耦计算与内存、异构加速器协同设计等系统级优化方向。


<details>
  <summary>Details</summary>
Motivation: AI智能体推理正带来以内存容量、带宽和高速互连为瓶颈的新挑战，传统性能建模（如roofline）无法刻画内存容量墙等问题。

Method: 提出OI（操作强度）和CF（容量足迹）两个联合指标，分析多种智能体工作流（聊天、编程、网页/电脑使用）及模型结构（GQA/MLA、MoE、量化）下的性能瓶颈；结合实证观察提出解耦架构、异构加速、光I/O和软硬协同设计等系统方案。

Result: 发现长上下文KV缓存使decode阶段高度内存受限；OI/CF在不同工作流与模型配置下变化显著；解耦计算-内存、prefill/decode专用加速器、大规模网络扩展等可提升系统效率。

Conclusion: OI/CF指标揭示了AI智能体推理的关键瓶颈；面向OI/CF动态特性的硬件-系统协同设计（如异构加速、大容量高带宽内存解耦）是支撑大规模智能体推理可持续发展的基础路径。

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [270] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: 本文提出了CAR-bench基准，用于评估多轮交互、调用工具的LLM智能体在车载语音助手场景中的一致性、不确定性处理和能力认知能力，揭示了当前前沿模型在歧义消解和幻觉任务中仍存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理评测基准忽视真实用户场景（如车载语音助手）中请求不完整或模糊带来的内在不确定性，亟需评估其在对话、工具使用与策略遵循方面的可靠性。

Method: 构建CAR-bench：基于LLM模拟用户、内置领域策略、58个互连工具（导航/办公/充电/车控），设计两类新任务——Hallucination（测试缺失信息/工具下的边界意识）和Disambiguation（要求通过澄清或内部检索解决歧义）。

Result: 前沿推理型LLM在Disambiguation任务上一致通过率<50%，常因过早执行而失败；在Hallucination任务中频繁违反政策或捏造信息，暴露出严重的能力盲区与不可靠性。

Conclusion: 当前LLM代理尚缺乏在真实人机协作场景中所需的自知性、一致性与鲁棒性，需发展更可靠的自我认知与不确定性管理机制。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [271] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: This paper introduces Agent Workflow Optimization (AWO), a framework that improves agentic AI efficiency and robustness by identifying redundant tool call patterns in workflow traces and bundling them into deterministic meta-tools, reducing LLM calls and increasing success rates.


<details>
  <summary>Details</summary>
Motivation: Agentic workflows suffer from high operational cost, latency, and failure rates due to many iterative reasoning steps and tool invocations, often exacerbated by hallucinations.

Method: AWO analyzes workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools—deterministic, composite tools that encapsulate multiple agent actions into one invocation, bypassing intermediate LLM reasoning.

Result: On two agentic AI benchmarks, AWO reduces LLM calls by up to 11.9% and increases task success rate by up to 4.2 percentage points.

Conclusion: Meta-tool synthesis via trace analysis is an effective way to enhance the efficiency and reliability of agentic AI systems without modifying the underlying LLM or agent architecture.

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [272] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 本文提出了一种基于多维核表示的数据驱动方法，用于后验定义人工智能系统运行设计域（ODD），以支持安全关键AI系统的认证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂系统或已有数据场景下，难以明确定义AI系统运行所需的环境条件（即ODD），而ODD是AI系统认证的必要前提。

Method: 提出一种Safety-by-Design方法，利用已收集数据，通过多维核函数表示来后验构建ODD，并借助蒙特卡洛模拟与航空避撞实际用例验证；同时定义了ODD等价性条件。

Result: 验证了该数据驱动ODD可等价于数据背后的隐含真实ODD，为数据驱动的安全关键AI系统认证提供了新路径。

Conclusion: 该核函数驱动的Safe-by-Design ODD方法有望成为未来安全关键AI系统认证的关键技术支撑。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [273] [The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR](https://arxiv.org/abs/2601.22128)
*Irsyad Adam,Zekai Chen,David Laprade,Shaun Porwal,David Laub,Erik Reinertsen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: 本文提出SMB-Structure，一种结合JEPA与监督微调（SFT）的世界模型，用于建模电子健康记录（EHR）中的患者动态轨迹，超越传统仅依赖下一词预测的LLM范式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM将患者视为静态文档进行总结，忽略了其作为受干预和时间影响的动态系统本质；需能模拟疾病演化的模型而非仅预测token。

Method: 提出SMB-Structure：以JEPA架构在潜在空间中仅基于初始表征预测未来状态，同时用SFT（下一token预测）在token空间重建未来状态，二者联合训练以强制编码动态轨迹。

Result: 在MSK与INSPECT两大真实临床队列上验证，线性探针显示其嵌入能捕获自回归基线无法恢复的疾病动态，在高异质性临床任务中达到有竞争力性能。

Conclusion: 将世界模型思想引入临床AI，通过联合JEPA与SFT训练范式，可更有效地从EHR中学习患者状态演化规律，为动态临床推理奠定基础。

Abstract: Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.

</details>


### [274] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 本文提出了World of Workflows (WoW)——一个基于ServiceNow的真实企业环境及配套基准WoW-bench，用于评估大语言模型在复杂企业系统中的动态建模与自主代理能力；研究发现前沿LLM存在‘动态盲区’，难以预测隐藏工作流的级联副作用，强调需通过具身化世界建模来弥补可观测性缺口。


<details>
  <summary>Details</summary>
Motivation: 现有企业基准仅评估表层任务完成能力，忽视企业系统中因低可观测性、大规模数据库状态和隐藏工作流引发的级联效应等真实挑战。

Method: 构建基于ServiceNow的WoW环境（含4000+业务规则、55个活跃工作流）及WoW-bench基准（234个任务），系统评测LLM在约束性任务完成与企业动态建模两方面的能力。

Result: 发现前沿LLM普遍存在‘动态盲区’，无法预测动作引发的隐藏级联副作用，导致静默约束违反；验证了以具身化世界建模提升低反馈环境下可靠性的必要性。

Conclusion: 面向企业场景的可靠智能体需从单纯任务驱动转向显式学习系统动态，WoW为该新范式提供了基础环境与评估标准。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [275] [Mesh Splatting for End-to-end Multiview Surface Reconstruction](https://arxiv.org/abs/2601.21400)
*Ruiqi Zhang,Jiacheng Wu,Jie Chen*

Main category: cs.GR

TL;DR: 本文提出了一种将表面表示（如网格）可微地软化为多层半透明体素表示的方法，从而结合体渲染的稳定性和表面表示的简洁性，实现端到端高质量表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，体素表示虽利于优化但后续网格化易引入误差且网格过密；纯表面方法避免了网格化但感受野受限、难建模细节、依赖强先验。亟需兼顾二者优势的新范式。

Method: 将输入网格软化为多个可微的半透明层（即‘软体素’），赋予其可控的3D感受野；结合基于splatting的可微渲染器与拓扑控制策略，实现端到端优化。

Result: 在约20分钟内完成优化，显著提升重建网格质量，准确恢复复杂几何结构，同时保持表面表示的紧凑性与可微性。

Conclusion: 通过可微软化表面为体积表示，本文成功弥合了体素与表面重建方法的鸿沟，为高效、高保真三维重建提供了新思路。

Abstract: Surfaces are typically represented as meshes, which can be extracted from volumetric fields via meshing or optimized directly as surface parameterizations. Volumetric representations occupy 3D space and have a large effective receptive field along rays, enabling stable and efficient optimization via volumetric rendering; however, subsequent meshing often produces overly dense meshes and introduces accumulated errors. In contrast, pure surface methods avoid meshing but capture only boundary geometry with a single-layer receptive field, making it difficult to learn intricate geometric details and increasing reliance on priors (e.g., shading or normals). We bridge this gap by differentiably turning a surface representation into a volumetric one, enabling end-to-end surface reconstruction via volumetric rendering to model complex geometries. Specifically, we soften a mesh into multiple semi-transparent layers that remain differentiable with respect to the base mesh, endowing it with a controllable 3D receptive field. Combined with a splatting-based renderer and a topology-control strategy, our method can be optimized in about 20 minutes to achieve accurate surface reconstruction while substantially improving mesh quality.

</details>


### [276] [Hybrid Foveated Path Tracing with Peripheral Gaussians for Immersive Anatomy](https://arxiv.org/abs/2601.22026)
*Constantin Kleinbeck,Luisa Theelke,Hannah Schieber,Ulrich Eck,Rüdiger von Eisenhart-Rothe,Daniel Roth*

Main category: cs.GR

TL;DR: 本文提出了一种结合注视点路径追踪与轻量级高斯泼溅的混合渲染方法，用于高质量、交互式和沉浸式的医学体数据可视化，兼顾视觉质量与实时性。


<details>
  <summary>Details</summary>
Motivation: 传统2D切片难以支持三维空间关系理解；直接体绘制（如路径追踪）计算开销大，预计算方法（如高斯泼溅）缺乏灵活性，均限制交互性。

Method: 提出混合渲染框架：中心区域采用流式注视点路径追踪，周边区域使用轻量级高斯泼溅近似；周边模型基于体数据初始化，并由注视区域渲染结果持续优化；引入深度引导重投影以提升延迟鲁棒性并支持保真度与帧率权衡。

Result: 相比纯路径追踪和纯高斯泼溅，该方法在视觉质量上保持优势，周边模型重建耗时低于1秒，无需大量预处理，显著提升交互性。

Conclusion: 混合渲染策略有效融合了高保真与高效率优势，为交互式医学影像可视化提供了新可行路径。

Abstract: Volumetric medical imaging offers great potential for understanding complex pathologies. Yet, traditional 2D slices provide little support for interpreting spatial relationships, forcing users to mentally reconstruct anatomy into three dimensions. Direct volumetric path tracing and VR rendering can improve perception but are computationally expensive, while precomputed representations, like Gaussian Splatting, require planning ahead. Both approaches limit interactive use.
  We propose a hybrid rendering approach for high-quality, interactive, and immersive anatomical visualization. Our method combines streamed foveated path tracing with a lightweight Gaussian Splatting approximation of the periphery. The peripheral model generation is optimized with volume data and continuously refined using foveal renderings, enabling interactive updates. Depth-guided reprojection further improves robustness to latency and allows users to balance fidelity with refresh rate.
  We compare our method against direct path tracing and Gaussian Splatting. Our results highlight how their combination can preserve strengths in visual quality while re-generating the peripheral model in under a second, eliminating extensive preprocessing and approximations. This opens new options for interactive medical visualization.

</details>


### [277] [JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion](https://arxiv.org/abs/2601.22143)
*Anthony Chen,Naomi Ken Korem,Tavi Halperin,Matan Ben Yosef,Urska Jelercic,Ofir Bibi,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 本文提出了一种基于音频-视觉扩散基础模型的单模型视频配音方法，通过轻量级LoRA适配实现音视频联合生成与同步，利用模型自身合成多语种配对视频进行训练，在保持说话人身份和唇动同步的同时提升了配音质量与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频配音方法依赖复杂、任务特定的流水线，在真实场景中表现不佳；而音频-视觉基础模型展现出强大的多模态生成能力，有望提升配音效果。

Method: 将预训练的音频-视频扩散基础模型通过轻量级LoRA进行适配，使其能以输入音视频为条件，联合生成目标语言音频及同步面部运动；训练数据通过模型自身合成：生成含语言切换的多语种视频，再对各半段分别进行面部与音频的跨语言修复（inpainting）。

Result: 所提方法在配音视频的视觉保真度、唇音同步性和对复杂动作及真实动态的鲁棒性方面均优于现有配音流水线。

Conclusion: 基于基础模型与LoRA的单模型配音框架，能有效利用多模态生成先验，在无需复杂管道的前提下实现高质量、高同步性、强鲁棒性的视频配音。

Abstract: Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [278] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 本文提出SteerEval评估框架，用于衡量自然语言推荐系统在多样化干预（如电影类型、内容警告）下的可引导性，并分析了现有预训练自然语言推荐器在小众主题上的引导潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的推荐方法是否能有效响应用户直接编辑的偏好指令尚不明确；已有评估基准仅覆盖常见属性（如电影类型），无法反映更丰富、细粒度的用户控制需求。

Method: 构建SteerEval评估框架，设计涵盖类型、内容警告等多维度干预的测试任务；对一系列预训练自然语言推荐器进行可引导性实证分析；对比不同用户画像与推荐干预方式对引导效果的影响。

Result: 发现当前模型在主流类别上具备一定引导能力，但在小众或语义复杂主题上表现受限；不同干预形式（如修改用户描述 vs 修改推荐提示）效果差异显著。

Conclusion: SteerEval揭示了自然语言推荐系统在真实可引导性上的关键瓶颈，为未来可引导推荐系统的设计提供了实证依据与实用建议。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [279] [A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning](https://arxiv.org/abs/2601.21162)
*Jiate Liu,Zebin Chen,Shaobo Qiao,Mingchen Ju,Danting Zhang,Bocheng Han,Shuyue Yu,Xin Shu,Jingling Wu,Dong Wen,Xin Cao,Guanfeng Liu,Zhengyi Yang*

Main category: cs.IR

TL;DR: A2RAG是一种自适应且具智能体特性的Graph-RAG框架，通过动态控制证据充分性验证与渐进式检索增强，在保证多跳问答准确率的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决Graph-RAG在实际部署中面临的两个瓶颈：混合难度查询下的一刀切检索导致成本浪费或失败，以及图抽象过程中细粒度限定信息丢失（提取损失）。

Method: 提出A2RAG框架，包含自适应控制器（判断证据是否充分、按需触发细化）和智能体检索器（逐步提升检索强度，并将图信号映射回原始文本以应对提取损失和图不完整问题）。

Result: 在HotpotQA和2WikiMultiHopQA上，Recall@2分别提升9.9和11.8个百分点，同时令牌消耗与端到端延迟降低约50%。

Conclusion: A2RAG在保持高准确性的同时实现成本感知与鲁棒推理，有效缓解Graph-RAG的实际部署瓶颈。

Abstract: Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

</details>


### [280] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 本文提出了一种结合多视角思维链（MPCoT）与潜在推理知识蒸馏（LRKD）的新框架，用于提升电商搜索中的相关性建模效果，并支持低延迟实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的电商搜索相关性建模方法存在两个关键问题：一是单视角思维链难以覆盖用户意图、属性匹配和业务规则等多维相关性；二是知识蒸馏过程中丢弃了思维链的结构化推理信息，导致学生模型无法继承可解释、可复用的推理能力。

Method: 提出多视角思维链（MPCoT）教师模型，融合监督微调（SFT）与直接偏好优化（DPO）；设计潜在推理知识蒸馏（LRKD），使学生模型在推理时通过轻量级潜变量提取器内化教师的结构化推理能力。

Result: 在离线实验和面向数千万用户的电商搜索广告平台线上A/B测试中，该方法显著提升了商业指标（如CTR、GMV）与用户体验指标（如点击率、停留时长）。

Conclusion: MPCoT与LRKD协同优化，既增强了相关性判断的多维建模能力，又实现了高保真、低延迟的知识迁移，为LLM在实时电商搜索系统中的落地提供了新范式。

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [281] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: 本文提出了一种名为Inf-DDS的强化学习驱动的数据采样框架，用于优化开放域密集检索模型的训练数据采样策略，通过影响度奖励信号动态调整数据集权重，在提升检索性能的同时显著降低GPU计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统训练数据采样策略（如均匀采样、按数据量比例采样或依赖专家经验）效果有限，而采样策略对嵌入模型性能影响显著，但尚缺乏系统研究如何自动发现最优策略。

Method: 提出Inf-DDS框架，利用强化学习迭代优化采样策略，以目标开发集上的模型性能为奖励信号，基于样本影响度动态重加权训练数据集，且设计轻量以减少GPU消耗。

Result: 在多个文本检索任务上显著优于现有梯度驱动采样方法：训练bge-m3时NDCG@10提升5.03，训练all-MiniLM-L6-v2时提升0.94；GPU计算成本降低1.5–4倍。

Conclusion: Inf-DDS是一种高效、自适应、低开销的数据采样方法，能有效提升密集检索模型性能，尤其适用于多语种与资源受限场景。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [282] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: OneMall是一个面向快手电商服务的端到端生成式推荐框架，统一处理商品卡片、短视频和直播等多种场景，通过语义分词器、Transformer架构和强化学习流程提升各场景关键指标，并已大规模部署。


<details>
  <summary>Details</summary>
Motivation: 解决电商多场景（商品卡片、短视频、直播）下推荐系统割裂的问题，推动生成式推荐在真实业务中的落地。

Method: 提出OneMall框架，包含三部分：1）电商语义分词器，建模跨场景语义与业务关系；2）基于Transformer的架构，集成Query-Former、Cross-Attention和Sparse MoE；3）强化学习流程，以排序模型为奖励信号优化检索策略。

Result: 在商品卡片场景GMV提升13.01%，短视频订单提升15.32%，直播订单提升2.78%；已部署服务超4亿日活用户。

Conclusion: OneMall验证了统一生成式推荐框架在复杂电商多模态场景下的有效性与可扩展性，为工业级生成式推荐提供了可行范式。

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [283] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 本文揭示了跨域推荐（CDR）中群体不公平性的两个根源：跨域差异传递与跨域信息增益分配不均，并提出CDFA框架通过自适应利用无标签数据和信息论方法来缓解这两类问题，在保持甚至提升推荐性能的同时显著降低不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐（CDR）虽能提升目标域推荐质量，但会无意中加剧群体层面的不公平性，亟需深入理解其成因并提出公平性增强方案。

Method: 提出CDFA框架：1）利用无标签数据自适应均衡各群体训练信号的信息量以抑制跨域差异传递；2）采用信息论方法重分配跨域信息增益，确保各群体公平获益。

Result: 在多个数据集和基线上实验表明，CDFA显著降低CDR中的群体不公平性，同时不损害、甚至提升整体推荐性能。

Conclusion: CDR中的不公平性源于源域差异的系统性迁移及跨域知识收益的不均衡分配；CDFA通过双路径干预有效实现公平性与性能的协同提升。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [284] [LEMUR: Learned Multi-Vector Retrieval](https://arxiv.org/abs/2601.21853)
*Elias Jääsaari,Ville Hyvönen,Teemu Roos*

Main category: cs.IR

TL;DR: 本文提出了LEMUR框架，将多向量相似性搜索转化为监督学习问题，并进一步简化为单向量相似性搜索，从而显著提升检索速度。


<details>
  <summary>Details</summary>
Motivation: 多向量表示虽能提高检索质量，但带来显著延迟，需设计高效的近似最近邻搜索（ANNS）算法。

Method: LEMUR通过两步问题约简：首先将多向量相似性搜索建模为可用单隐藏层神经网络求解的监督学习问题；其次将该模型的推理约简为潜在空间中的单向量相似性搜索，复用现有单向量ANNS方法。

Result: LEMUR在ColBERTv2及其他现代多向量文本与视觉文档检索模型上验证有效，比早期多向量相似性搜索方法快一个数量级。

Conclusion: LEMUR是一种简单而高效的多向量相似性搜索框架，兼顾高检索质量与低延迟，适用于多种多向量模型。

Abstract: Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.

</details>


### [285] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: 本文提出SpecTran，一种在频谱域操作的、关注全频谱以选择和聚合信息成分的频谱感知Transformer适配器，用于改进序列推荐中对物品文本信息的建模。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型忽略物品文本信息，而现有利用大语言模型编码文本并转换嵌入的方法存在维度坍缩或频谱利用不充分的问题。

Method: 提出SpecTran：一种基于Transformer的频谱感知适配器，在奇异值分解后的频谱域中操作；引入可学习的频谱位置编码，将奇异值线索作为归纳偏置，引导注意力聚焦关键频谱成分并提升维度多样性。

Result: 在四个真实数据集和三个序列推荐骨干模型上，SpecTran持续优于强基线，平均提升9.17%。

Conclusion: SpecTran通过更全面、自适应地利用文本嵌入的频谱信息，有效缓解了现有方法的局限性，显著提升了序列推荐性能。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [286] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: 本文提出LANCER，一种基于大语言模型（LLM）的重排序方法，旨在提升长形式检索增强生成（RAG）中的信息片段（nugget）覆盖率，通过生成子问题、匹配文档并重排序以最大化覆盖。实验表明其在α-nDCG和信息覆盖率上优于现有LLM重排序方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法主要优化相关性排序，难以满足长形式RAG（如自动生成报告）对广泛信息覆盖的需求。

Method: LANCER基于LLM进行三步 reranking：1）预测满足信息需求所需的子问题；2）判断各文档回答哪些子问题；3）据此重排序文档列表以最大化信息片段覆盖。

Result: LANCER在nugget coverage指标（如α-nDCG和信息覆盖率）上显著优于其他LLM-based reranking方法；消融分析显示子问题生成是关键环节。

Conclusion: 面向信息覆盖的LLM重排序范式（如LANCER）可有效提升长形式RAG的检索质量，为复杂信息需求建模提供了新思路。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>


### [287] [Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature](https://arxiv.org/abs/2601.07533)
*Julian Schelb,Michael Wittweiler,Marie Revellio,Barbara Feichtinger,Andreas Spitz*

Main category: cs.IR

TL;DR: 本文介绍了Loci Similes，一个用于拉丁语互文性检测的新基准数据集，包含约17.2万个文本片段和545个专家验证的平行文本对，并基于该数据集为检索和分类任务建立了大语言模型基线。


<details>
  <summary>Details</summary>
Motivation: 历史文本间互文关系的研究对重构作者虚拟图书馆及识别其创作影响源至关重要，但缺乏标准化基准和易用数据集阻碍了新方法的发展。

Method: 构建了一个名为Loci Similes的拉丁语互文性检测基准，包含约172k文本段落和545个专家验证的晚期古代作者与古典作者之间的平行文本对，并利用该数据集对当前最先进的大语言模型在互文性检索与分类任务上进行了基线评估。

Result: 发布了Loci Similes数据集，并在该数据集上实现了基于大语言模型的互文性检索与分类任务的初步基线性能。

Conclusion: Loci Similes填补了拉丁语互文性研究中高质量、标准化基准数据集的空白，为后续方法开发与评估提供了坚实基础。

Abstract: Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [288] [Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs](https://arxiv.org/abs/2601.20968)
*Yulie Arad,Stav Ashur,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文提出了一种名为“红-绿-灰”范式的改进方法，用于在非静态环境中快速半懒惰地更新机器人运动规划的路线图，通过简单几何方法近似机器人扫掠体积并进行懒惰碰撞检测，实现对边的有效性分类。


<details>
  <summary>Details</summary>
Motivation: 解决机器人运动规划中路线图在非静态环境下的动态调整问题，提升更新效率与准确性。

Method: 基于SPITE方法改进，引入“红-绿-灰”节点/边状态分类范式；利用计算几何近似机器人扫掠体积，进行懒惰碰撞检测以标记边的状态（无效/有效/未知）。

Result: 实验表明该方法相比Leven和Hutchinson的经典技术，在保持相近更新时间的同时，提高了准确性，并能更准确地标记无效边。

Conclusion: ‘红-绿-灰’范式是一种高效、准确的半懒惰路线图更新策略，适用于动态环境中的机器人运动规划。

Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the "Red-Green-Gray" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.

</details>


### [289] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: 本文提出了Meta-ROS，一种基于Zenoh和ZeroMQ等现代通信协议的新机器人中间件，旨在简化集成、提升性能并确保跨平台兼容性；实验表明其吞吐量比ROS2高30%，延迟更低、资源利用率更优。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件（如ROS2）复杂且互操作性差，新开发者难以采用。

Method: 提出Meta-ROS中间件，采用Zenoh和ZeroMQ等现代通信协议，支持多类型数据（音频、图像、视频）及多硬件平台的低延迟通信，并进行与ROS1/ROS2的对比性能测试。

Result: Meta-ROS相较ROS2吞吐量提升最高达30%，消息延迟显著降低，资源使用更优，且具备更强硬件支持与易用性。

Conclusion: Meta-ROS是一种面向现代实时机器人AI应用的理想中间件解决方案，兼顾高性能、跨平台兼容性与开发者友好性。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [290] [Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing](https://arxiv.org/abs/2601.21027)
*Youngim Nam,Jungbin Kim,Kyungtae Kang,Cheolhyeon Kwon*

Main category: cs.RO

TL;DR: 本文提出了一种面向赛道的全局轨迹优化框架，利用小波变换参数化轨迹，并结合贝叶斯优化与迭代动力学学习，在不确定车辆动力学下最小化自动驾驶赛车单圈时间。


<details>
  <summary>Details</summary>
Motivation: 全时段轨迹优化计算昂贵，且在动力学不确定下难以保证实际跟踪的全局最优；现有工作多只在跟踪层学习动力学，未据此更新轨迹本身。

Method: 采用小波变换构建赛道无关的参数化轨迹表示；用贝叶斯优化在该空间中搜索最短圈时轨迹，每次评估依赖基于已学习动力学的仿真；将优化轨迹部署至实车采集新数据，迭代更新动力学模型与轨迹。

Result: 仿真与实车实验表明，相比标称基线，单圈时间最多提升20.7%，并持续优于当前最优方法。

Conclusion: 所提迭代式、轨迹中心的学习优化框架能有效应对车辆动力学不确定性，显著提升自主赛车性能。

Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.

</details>


### [291] [Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned](https://arxiv.org/abs/2601.21063)
*Pierre-Yves Lajoie,Karthik Soma,Haechan Mark Bong,Alice Lemieux-Bourque,Rongge Zhang,Vivek Shankar Varadharajan,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 本文分享了在火星模拟地形上进行的三机器人去中心化协同SLAM（C-SLAM）实验的经验与洞察，重点分析了间歇性通信和类行星环境对定位的影响，并发布了一个包含实时点对点通信吞吐量与延迟测量的新数据集。


<details>
  <summary>Details</summary>
Motivation: 为支持月球、火星等天体的多机器人自主探测任务，需发展不依赖预设定位与通信基础设施的去中心化协同SLAM技术。

Method: 在火星模拟地形上部署三台机器人，构建自组织无线网络，开展实地C-SLAM实验；同步采集通信性能（吞吐量、延迟）与定位数据，并分析通信受限与环境特征对C-SLAM的影响。

Result: 揭示了有限/间歇通信显著影响C-SLAM一致性与收敛性；识别出类行星环境中缺乏GPS、纹理稀疏、光照变化等带来的独特定位挑战；发布首个面向通信受限去中心化多机器人系统的实测数据集。

Conclusion: 实地实验验证了C-SLAM在真实严苛环境中的可行性与挑战，所提数据集将推动通信感知联合建模与鲁棒分布式状态估计的研究。

Abstract: Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.

</details>


### [292] [WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics](https://arxiv.org/abs/2601.21129)
*Guangping Liu,Tipu Sultan,Vittorio Di Giorgio,Nick Hawkins,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: 本文提出了WheelArm概念，即一种集成轮椅与机械臂控制的网络物理系统（CPS），并开发了基于Isaac Sim的仿真框架WheelArm-Sim用于合成数据采集；通过构建包含13项任务、232条轨迹和67783个样本的多模态数据集，验证了该仿真数据可用于训练端到端的联合控制机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将轮椅与轮椅搭载机械臂（WMRA）分开研究，缺乏对二者统一、集成控制的机器学习建模探索，亟需支持联合控制的数据与平台。

Method: 提出WheelArm集成CPS概念；构建基于Isaac Sim的仿真框架WheelArm-Sim；在该框架中采集涵盖导航与操作的多模态合成数据集；设计并训练一个面向‘取芥末’任务的动作预测基线模型进行验证。

Result: 成功采集包含13个ADL任务、232条轨迹、67,783个样本的WheelArm数据集；基线模型在‘取芥末’任务中验证了合成数据对数据驱动联合控制模型的有效性。

Conclusion: WheelArm-Sim为WheelArm系统的数据驱动建模提供了可行且高效的仿真基础，推动了面向真实场景的轮椅-机械臂一体化智能控制研究。

Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.

</details>


### [293] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1 is the first real-world, multimodal benchmark dataset for industrial inspection safety assessment, featuring pixel-level annotations, semantic scene descriptions, safety labels, and seven synchronized sensing modalities across five industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing public datasets suffer from simulated data, single-modality sensing, and lack of fine-grained object-level annotations, hindering robust scene understanding and multimodal safety reasoning for industrial AI systems.

Method: Collecting data from 41 real inspection robots operating at 2,239 real-world sites; constructing 5,013 inspection instances with pixel-level segmentation, semantic scene descriptions, safety level labels, and synchronizing seven sensing modalities (infrared, audio, depth, radar, gas, temperature, humidity).

Result: A large-scale, real-world multimodal dataset—InspecSafe-V1—covering five industrial scenarios (tunnels, power facilities, sintering equipment, oil/gas plants, coal conveyor trestles) with rich annotations and synchronized multimodal sensor data.

Conclusion: InspecSafe-V1 bridges critical gaps in industrial AI evaluation by enabling robust multimodal perception, anomaly recognition, cross-modal fusion, and comprehensive safety assessment for predictive maintenance and autonomous inspection.

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [294] [Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation](https://arxiv.org/abs/2601.21188)
*Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于移动质量驱动的扰动感知控制框架，利用移动地平线估计器（MHE）实时估计风扰，并将估计结果输入模型预测控制器（MPC），结合2自由度移动质量机构实现姿态与航向鲁棒控制，在强风环境下显著优于传统PID控制。


<details>
  <summary>Details</summary>
Motivation: 现有轻于空气（LTA）飞行器（如机器人气球）虽具备长续航和高安全性优势，但对风扰极为敏感，缺乏有效的扰动感知与补偿控制框架。

Method: 提出一种MHE-MPC联合控制方法：MHE实时估计风扰动；MPC基于该估计及系统动力学模型进行轨迹与航向优化控制；采用2自由度移动质量机构产生惯性与气动力矩，实现姿态与航向调节。

Result: 在迎风与侧风条件下开展大量飞行实验，结果表明所提MHE-MPC框架相比PID基线控制在轨迹跟踪与航向稳定性上显著提升。

Conclusion: 该扰动感知控制框架有效提升了LTA平台在复杂风场下的飞行鲁棒性与可控性，为安全、可靠的低空自主飞行提供了新思路。

Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.

</details>


### [295] [Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies](https://arxiv.org/abs/2601.21251)
*Ce Hao,Xuanran Zhai,Yaohua Liu,Harold Soh*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的混合专家策略（SMP），通过学习紧凑正交技能基和粘性路由机制，在多任务机器人操作中实现高效、可扩展的技能组合与推理。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现优异，但在多任务场景下受限于模型规模和演示数据成本过高。

Method: 提出Skill Mixture-of-Experts Policy（SMP）：基于扩散模型的混合专家架构，学习紧凑正交技能基，采用粘性路由在每步仅激活少量相关专家；引入变分训练目标，并支持推理时自适应专家激活。

Result: 在仿真和真实双臂平台上验证，SMP在多任务学习与迁移学习中成功率更高、推理开销显著低于大型扩散基线模型。

Conclusion: SMP为可扩展、可迁移的多任务操作提供了实用路径：一次性学习可复用技能，按需激活，快速适应新任务。

Abstract: Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.

</details>


### [296] [Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter](https://arxiv.org/abs/2601.21297)
*Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了一种名为Deep QP Safety Filter的全数据驱动安全层，用于黑箱动力学系统，通过结合Hamilton-Jacobi可达性与无模型学习，在无需系统模型知识的情况下学习二次规划（QP）安全滤波器；利用基于收缩性的损失函数训练两个神经网络，分别逼近安全值函数及其导数，并在精确设定下保证收敛至粘性解；实验表明该方法在多种动力学系统和强化学习任务中显著减少收敛前失败次数，并加速获得更高回报。


<details>
  <summary>Details</summary>
Motivation: 为黑箱动力学系统提供无需模型知识的、可证明安全的数据驱动安全滤波器，解决现有方法依赖精确模型或难以处理非光滑安全值的问题。

Method: 结合Hamilton-Jacobi可达性理论与模型无关的深度学习，设计基于收缩性的损失函数，分别训练两个神经网络以联合学习安全价值函数及其空间导数，从而隐式构建QP安全滤波器。

Result: 在多个连续与混合动力学系统及RL任务中，Deep QP Safety Filter显著降低预收敛失败率，并加快策略学习速度，获得比强基线更高的累积回报。

Conclusion: Deep QP Safety Filter为模型无关的安全控制提供了原理严谨且实用可行的新范式，尤其适用于安全关键、模型未知的复杂动态环境。

Abstract: We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.

</details>


### [297] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 本文提出了一种分层主动调参框架HPTune，通过扩展对非执行动作的评估，结合快速级（基于风险指标）和慢速级（基于闭环反向传播的扩展损失）调参，并融合多普勒LiDAR测速信息，显著提升了MPC运动规划器在复杂环境中的自适应性与避障性能。


<details>
  <summary>Details</summary>
Motivation: 现有MPC参数调优方法仅评估已执行动作，受限于失败事件（如碰撞、近距障碍）稀疏，导致调参效率低。

Method: 提出分层主动调参框架HPTune：快速级利用预测接近速度和预测距离等风险指标实时调参；慢速级通过扩展的闭环评估损失进行梯度反向传播；并融合多普勒LiDAR提供的障碍物速度信息以提升运动预测精度。

Result: 在高保真仿真中，HPTune实现了高效MPC参数调优，在复杂环境中显著优于多种基线方法，能生成安全、敏捷的情境定制化避障策略。

Conclusion: HPTune通过拓展动作评估范围与引入多模态感知（多普勒LiDAR），有效解决了MPC调参中失败样本稀疏问题，提升了运动规划的适应性与鲁棒性。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [298] [Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control](https://arxiv.org/abs/2601.21363)
*Weidong Huang,Zhehan Li,Hangxin Liu,Biao Hou,Yao Su,Jingwen Zhang*

Main category: cs.RO

TL;DR: 本文提出一种结合大规模预训练与模型驱动微调的强化学习框架，利用高UTD比的离线策略SAC进行预训练，再通过物理信息世界模型引导的确定性策略采集数据实现安全高效的环境自适应。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化学习方法（如PPO）虽支持零样本部署，但样本效率低，难以安全适应新环境；而离线策略和基于模型的方法虽更高效，却尚未弥合大规模预训练与高效微调之间的鸿沟。

Method: 采用大批次更新、高Update-To-Data（UTD）比的Soft Actor-Critic（SAC）进行大规模预训练；在新环境中，用确定性策略执行数据收集，将随机探索限制在物理信息嵌入的世界模型内，实现安全且具覆盖性的微调。

Result: SAC预训练策略可零样本部署至真实人形机器人；在新环境和分布外任务中，该方法显著提升微调效率与安全性，并兼顾探索与稳定性。

Conclusion: 将大规模并行仿真预训练与物理信息模型驱动的微调相结合，可兼顾墙钟效率与样本效率，为人形机器人快速、安全适应新环境提供可行路径。

Abstract: Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

</details>


### [299] [Towards Space-Based Environmentally-Adaptive Grasping](https://arxiv.org/abs/2601.21394)
*Leonidas Askianakis,Aleksandr Artemov*

Main category: cs.RO

TL;DR: 本文提出了一种在空间环境中进行机器人抓取的高效强化学习方法，通过在多模态融合的隐空间中直接学习控制策略，结合GPU加速物理仿真，在少于100万步内实现95%以上成功率，并展现出更强的样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法在非结构化环境中面临高维动作空间、稀疏奖励和泛化能力差等问题，尤其在空间等极端环境下挑战更大。

Method: 在学习到的多模态融合隐流形（latent manifold）中直接学习控制策略，利用GPU加速物理仿真构建单次尝试操纵任务，并采用Soft Actor-Critic（SAC）强化学习算法进行训练。

Result: 在持续变化的抓取条件下，仅用不到100万环境步即实现95%以上的任务成功率，收敛速度优于现有视觉基线方法；隐空间推理提升了对新物体/夹爪几何、环境杂乱度和传感器配置的鲁棒性与样本效率。

Conclusion: 显式地在隐空间中进行推理可显著提升学习效率与鲁棒性，为实现在空间等极端条件下的自适应、通用抓取提供了可行路径，并指出了当前局限与未来方向。

Abstract: Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.

</details>


### [300] [DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation](https://arxiv.org/abs/2601.21409)
*Weitao An,Qi Liu,Chenghao Xu,Jiayi Chai,Xu Yang,Kun Wei,Cheng Deng*

Main category: cs.RO

TL;DR: 本文提出了一种名为Dual-Stance Cooperative Debate Navigation (DSCD-Nav)的新型导航决策机制，通过构建两个互补立场（任务场景理解与安全信息平衡）进行协作式辩论，并由仲裁器整合证据、必要时轻量微探测，显著提升了室内未知环境中的导航成功率、路径效率并减少了冗余探索。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统依赖单次打分决策，在部分可观测环境下易产生高置信度长程错误和冗余探索，难以适应家庭服务机器人在陌生室内环境中的自适应导航需求。

Method: 提出DSCD-Nav框架：1）构建Task-Scene Understanding（TSU）和Safety-Information Balancing（SIB）两种互补评估立场；2）两立场基于观测与候选动作集进行基于线索的论证式交叉验证；3）引入Navigation Consensus Arbitration（NCA）代理整合双方理由与证据，并可触发轻量微探测以消歧不确定选择。

Result: 在HM3Dv1、HM3Dv2和MP3D数据集上实验表明，DSCD-Nav在成功率（Success）和路径效率（SPL）上均取得一致提升，同时显著降低冗余探索。

Conclusion: DSCD-Nav通过立场协同辩论与证据感知仲裁，有效缓解了零样本视觉语言模型在部分可观测导航中因单次决策导致的过自信与低鲁棒性问题，为具身智能体提供了更可靠、可解释的决策范式。

Abstract: Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.

</details>


### [301] [Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates](https://arxiv.org/abs/2601.21413)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种将李群积分方法与标准多体系统（MBS）绝对坐标建模相兼容的框架，并设计了局部-全局过渡（LGT）映射，以在标准向量空间积分器中一致地融入刚体运动几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统绝对坐标建模在时间积分中面临奇异参数化问题（如欧拉角），虽可用单位四元数或李群积分缓解，但后者与现有MBS仿真代码不兼容；需在不重构现有代码前提下融合李群几何优势。

Method: 提出两方面方法：(1) 构建接口框架，使李群积分器可适配多种绝对坐标描述；(2) 基于SO(3)×ℝ³和SE(3)等李群，引入局部-全局过渡（LGT）映射，在标准向量空间积分中嵌入刚体运动几何结构。

Result: 实现了李群积分与标准EOM形式的解耦兼容；验证了LGT映射能正确更新全局绝对坐标，并支持不同坐标选择（如四元数、旋转矩阵等）下的几何一致积分。

Conclusion: 该框架弥合了李群几何积分与工业级MBS仿真代码之间的鸿沟，提升了绝对坐标方法在长期积分中的精度与稳定性，同时保持对现有软件架构的兼容性。

Abstract: A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.

</details>


### [302] [Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation](https://arxiv.org/abs/2601.21416)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: 本文提出基于槽的以对象为中心的表示（SBOCR），作为全局和密集特征之间的中间结构化替代方案，以提升机器人操作策略在分布偏移下的泛化能力。实验表明，SBOCR在多种模拟与真实世界任务中显著优于传统特征表示，且无需任务特定预训练。


<details>
  <summary>Details</summary>
Motivation: 现有全局和密集视觉表征易混合任务相关与无关信息，导致在光照、纹理、干扰物等分布偏移下泛化性能差。

Method: 提出Slot-Based Object-Centric Representations（SBOCR），将密集特征聚类为有限数量的对象状实体，并在多类模拟与真实机器人操作任务中系统对比其与全局/密集表征的泛化性能。

Result: SBOCR-based策略在各类视觉分布偏移下（如光照、纹理变化、干扰物存在）均显著优于基于全局和密集特征的策略，且不依赖任务特定预训练。

Conclusion: SBOCR是一种有前景的视觉表征范式，有助于提升机器人在动态真实环境中的泛化能力。

Abstract: The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.

</details>


### [303] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出Nimbus，一个统一的合成数据生成框架，用于集成异构的导航与操作管道，通过模块化四层架构和异步执行模型，显著提升资源利用率和端到端吞吐量（2-3倍），支撑大规模具身智能基础模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成流程碎片化、任务专用，导致工程效率低、系统不稳定，难以满足具身智能基础模型对高吞吐、可持续数据生成的需求。

Method: 提出Nimbus框架，采用模块化四层架构，解耦轨迹规划、渲染与存储为异步阶段；引入动态流水线调度、全局负载均衡、分布式容错及后端定制渲染优化。

Result: Nimbus在端到端吞吐量上比未优化基线提升2-3倍，并在大规模分布式环境中实现鲁棒、长期稳定运行。

Conclusion: Nimbus作为统一、可扩展的合成数据生成基础设施，已作为InternData套件的生产主干，支持跨领域无缝数据合成，推动具身智能规模化发展。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [304] [4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving](https://arxiv.org/abs/2601.21454)
*Shanliang Yao,Zhuoxiao Li,Runwei Guan,Kebin Cao,Meng Xia,Fuping Hu,Sen Xu,Yong Yue,Xiaohui Zhu,Weiping Ding,Ryan Wen Liu*

Main category: cs.RO

TL;DR: 本文提出4D-CAAL框架，统一解决4D雷达与相机的外参标定与自动标注问题，通过双用途标定靶和鲁棒匹配算法实现高精度标定，并利用几何投影与多特征优化实现雷达点云的自动标注。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达-相机标定方法依赖分离的模态专用标定靶，难以建立可靠对应关系；同时，稀疏雷达数据的手动标注费时且不可靠，制约多模态感知算法发展。

Method: 设计前后双功能标定靶（前表面为棋盘格用于相机检测，后中心为角反射器用于雷达检测）；提出基于棋盘格中心与最强雷达回波点对齐的鲁棒对应匹配算法完成外参标定；构建基于标定结果的自动标注流水线，通过几何投影与多特征优化将相机分割标注迁移至雷达点云。

Result: 实验表明该方法在标定精度上表现优异，并显著降低人工标注工作量。

Conclusion: 4D-CAAL为4D雷达-相机系统提供了高效、准确的标定与自动标注解决方案，有助于加速自动驾驶多模态感知系统的研发。

Abstract: 4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.

</details>


### [305] [DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching](https://arxiv.org/abs/2601.21474)
*Xingyu Zhang,Chaofan Zhang,Boyue Zhang,Zhinan Peng,Shaowei Cui,Shuo Wang*

Main category: cs.RO

TL;DR: 本文提出了DexTac框架，通过示教方式采集多维触觉信息（如接触力分布和空间接触区域），并将其整合到策略网络中，使灵巧手在复杂交互中能自主选择并维持最优接触区域；在单手注射任务上达到91.67%成功率，显著优于仅依赖力反馈的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧操作的数据采集与技能学习系统常受限于低维触觉信息，难以支持接触密集型任务所需的全面触觉感知运动生成。

Method: 提出基于动觉示教的DexTac框架，直接从人类演示中捕获多维触觉数据（包括接触力分布和空间接触区域），并将其融合进策略网络，构建触觉感知的灵巧操作智能体。

Result: 在挑战性的单手注射任务中，DexTac成功率达91.67%；在小尺寸注射器的高精度场景下，相较仅用触觉力的基线方法提升31.67%。

Conclusion: 从人类示范中学习多维触觉先验对实现接触丰富环境中鲁棒、类人的灵巧操作至关重要。

Abstract: For contact-intensive tasks, the ability to generate policies that produce comprehensive tactile-aware motions is essential. However, existing data collection and skill learning systems for dexterous manipulation often suffer from low-dimensional tactile information. To address this limitation, we propose DexTac, a visuo-tactile manipulation learning framework based on kinesthetic teaching. DexTac captures multi-dimensional tactile data-including contact force distributions and spatial contact regions-directly from human demonstrations. By integrating these rich tactile modalities into a policy network, the resulting contact-aware agent enables a dexterous hand to autonomously select and maintain optimal contact regions during complex interactions. We evaluate our framework on a challenging unimanual injection task. Experimental results demonstrate that DexTac achieves a 91.67% success rate. Notably, in high-precision scenarios involving small-scale syringes, our approach outperforms force-only baselines by 31.67%. These results underscore that learning multi-dimensional tactile priors from human demonstrations is critical for achieving robust, human-like dexterous manipulation in contact-rich environments.

</details>


### [306] [Don't double it: Efficient Agent Prediction in Occlusions](https://arxiv.org/abs/2601.21504)
*Anna Rothenhäusler,Markus Mazzola,Andreas Look,Raghu Rajan,Joschka Bödecker*

Main category: cs.RO

TL;DR: 本文提出了MatchInformer，一种基于Transformer的新方法，通过引入匈牙利匹配和解耦航向与运动来减少遮挡交通参与者预测中的冗余，并采用MCC评估指标提升稀疏/不平衡场景下的 occupancy 预测鲁棒性，在Waymo数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 遮挡交通参与者（如被遮挡的行人或车辆）对自动驾驶构成严重威胁，但现有学习方法常产生冗余预测（同一目标多次检测），影响下游规划并增加计算负担。

Method: 在SceneInformer基础上提出MatchInformer：1）引入匈牙利匹配机制，强制预测与真值间的一一对应以消除冗余；2）解耦代理的航向与运动建模以提升轨迹预测精度与可解释性；3）采用Matthews相关系数（MCC）替代传统指标评估occupancy预测，增强对类别不平衡的鲁棒性。

Result: 在Waymo Open Motion Dataset上实验表明，该方法显著提升了对遮挡区域的推理能力及轨迹预测准确性，优于现有方法。

Conclusion: MatchInformer通过结构化匹配、运动建模解耦和更鲁棒的评估指标，有效缓解了遮挡场景下occupancy预测冗余问题，为自动驾驶感知与预测提供了更可靠、高效的技术路径。

Abstract: Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.

</details>


### [307] [IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation](https://arxiv.org/abs/2601.21506)
*Joonhee Lee,Hyunseung Shin,Jeonggil Ko*

Main category: cs.RO

TL;DR: 本文提出IROS框架，结合视觉语言模型（VLM）的语义推理能力与轻量感知模块的实时性，在低成本嵌入式设备上实现高效、鲁棒的室内机器人导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾室内移动机器人导航所需的快速响应与鲁棒语义理解：几何方法（如SLAM）缺乏语义理解能力，VLA模型过于反应式，VLM则计算延迟高、不适用于实时嵌入式场景。

Method: 基于双过程理论，设计IROS框架，将快速反射式决策（System One）与慢速审慎式推理（System Two）分离；仅在必要时调用增强空间与文本线索的紧凑型VLM，其余时间由轻量感知模块处理。

Result: 在五个真实建筑中实验表明，IROS相比连续使用VLM的方法，决策准确率提升且延迟降低66%。

Conclusion: IROS成功在低延迟约束下实现了类人、语义驱动的实时室内导航，为VLM落地边缘设备提供了可行范式。

Abstract: Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.

</details>


### [308] [Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning](https://arxiv.org/abs/2601.21548)
*Irene Ambrosini,Ingo Blakowski,Dmitrii Zendrikov,Cristiano Capone,Luna Gava,Giacomo Indiveri,Chiara De Luca,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 本文提出了一种基于脉冲神经网络与类脑混合信号处理器的实时学习系统，用于高速空气曲棍球任务，通过硬件-算法协同设计和e-prop规则实现少量试验下的快速在线学习。


<details>
  <summary>Details</summary>
Motivation: 解决空气曲棍球中高球速下毫秒级决策难题，推动脉冲神经网络在真实机器人系统中的实时、低功耗、在线学习应用。

Method: 采用固定随机连接的脉冲神经网络架构，运行于混合模拟/数字类脑芯片上；结合强化学习与局部e-prop可塑性规则，在读出层实现事件驱动的高效学习；采用硬件-算法协同设计方法。

Result: 在极少数训练试验内即实现成功击球交互；系统支持计算机与类脑芯片闭环实时学习；验证了脉冲网络在快速动态交互任务中的可行性。

Conclusion: 类脑硬件与神经启发式学习算法的深度融合，可支撑智能机器在真实场景中实现始终在线（always-on）、低延迟、低功耗的自主学习与控制。

Abstract: Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.

</details>


### [309] [AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation](https://arxiv.org/abs/2601.21602)
*Jianli Sun,Bin Tian,Qiyao Zhang,Chengxiang Li,Zihan Song,Zhiyong Cui,Yisheng Lv,Yonglin Tian*

Main category: cs.RO

TL;DR: 本文提出了首个面向空中操纵系统的视觉-语言-动作（VLA）基准AIR-VLA，包含仿真环境与高质量多模态数据集，并系统评估了现有VLA/VLM模型在空中机器人任务中的能力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要面向地面或2D移动平台，难以应对空中操纵系统（AMS）特有的漂浮基座动力学、无人机与机械臂强耦合、多步长时序任务等挑战，亟需专用于AMS的基准与数据支撑。

Method: 构建基于物理的仿真环境，采集3000条人工遥操作多模态演示数据，覆盖基础操纵、物体与空间理解、语义推理和长时序规划；设计面向空中任务的多维评估指标，系统评测主流VLA与VLM模型。

Result: 验证了VLA范式迁移到空中系统的可行性；揭示了当前模型在无人机机动性、机械臂控制及高层规划等方面的能力边界；建立了首个标准化空中机器人通用测试平台与数据基础。

Conclusion: AIR-VLA填补了空中操纵领域VLA基准的空白，为通用空中机器人研究提供了关键基础设施与评估标准。

Abstract: While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.

</details>


### [310] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种声音触发的移动操作范式，通过Habitat-Echo平台和分层基线模型，使智能体能主动感知并响应声源，无需显式指令。


<details>
  <summary>Details</summary>
Motivation: 现有指令驱动范式限制了智能体的自主性与对动态环境事件的响应能力。

Method: 提出了声音触发的移动操作新范式；构建了集成声学渲染与物理交互的Habitat-Echo数据平台；设计了包含高层任务规划器和低层策略模型的基线方法。

Result: 实验表明该基线能使智能体主动检测并响应听觉事件；在双声源干扰场景下，智能体能分离主声源完成首次交互，并继续操作次声源对象。

Conclusion: 所提范式和基线显著提升了移动操作智能体的自主性与鲁棒性，摆脱了对逐例指令的依赖。

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [311] [CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation](https://arxiv.org/abs/2601.21712)
*Xuanran Zhai,Binkai Ou,Yemin Wang,Hui Yi Leong,Qiaojun Yu,Ce Hao,Yaohua Liu*

Main category: cs.RO

TL;DR: 本文提出CoFreeVLA，一种增强视觉语言动作（VLA）模型安全性的方法，通过引入短时域自碰撞风险估计器，结合本体感知、视觉嵌入与规划动作预测碰撞概率，并对高风险指令进行门控、恢复安全状态及引导策略优化，显著降低双臂操作中的自碰撞并提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在双臂部署中因未充分建模双臂及抓取物体间的自碰撞而导致操作不安全。

Method: 提出CoFreeVLA框架，集成一个短时域自碰撞风险估计器，输入为本体感知信号、视觉嵌入和计划动作；该估计器用于门控高风险指令、触发风险引导的状态恢复，并指导策略优化；估计器先用基于模型的碰撞标签预训练，再通过真实机器人滚动数据微调校准。

Result: 在PiPER双臂机器人上的五项双臂任务中，CoFreeVLA相比RDT和APEX显著降低了自碰撞发生率并提高了任务成功率。

Conclusion: CoFreeVLA有效提升了端到端VLA模型在双臂操作中的安全性与可靠性，为实际部署提供了可行路径。

Abstract: Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.

</details>


### [312] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文提出了一种高效、模块化的强化学习方法用于布料操作，通过精心设计显著减小模型规模和训练时间，并成功实现从仿真到真实世界的迁移，在SoftGym基准测试中性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于图像端到端的布料操作强化学习方法计算开销大、状态表征损失高；而解析方法难以提供鲁棒通用策略，因此需探索更高效、可迁移的RL范式。

Method: 采用模块化强化学习架构，避免端到端图像输入，优化仿真训练过程以减小模型规模与训练时间，并设计仿真到真实世界的迁移策略。

Result: 在SoftGym基准上显著超越现有基线，同时使用更小模型，验证了方法的高效性与可迁移性。

Conclusion: 布料操作无需依赖大型端到端视觉模型；通过模块化设计与仿真优化，可在降低计算成本的同时实现高性能与真实世界部署。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [313] [Flocking behavior for dynamic and complex swarm structures](https://arxiv.org/abs/2601.21772)
*Carmen D. R. Pita-Romero,Pedro Arias-Perez,Miguel Fernandez-Cortizas,Rafael Perez-Segui,Pascual Campoy*

Main category: cs.RO

TL;DR: 本文提出了一种基于虚拟质心（Virtual Centroid）概念的多无人机编队控制算法，旨在简化复杂结构与轨迹下的群体行为实现。


<details>
  <summary>Details</summary>
Motivation: 维持多个无人机组成的复杂结构并实现复杂轨迹仍是一个主要挑战。

Method: 提出一种基于虚拟质心概念的无人机群集行为算法，扩展了经典基于虚拟实体的行为方法，并提供了可动态调节智能体数量和编队构型的理论框架。

Result: 通过仿真测试和真实实验验证，该方法在处理复杂编队和复杂轨迹时展现出良好的简洁性和有效性。

Conclusion: 所提出的虚拟质心方法为多无人机系统的灵活、可扩展编队控制提供了一种实用且易于实现的新思路。

Abstract: Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.

</details>


### [314] [GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2601.21829)
*Bsher Karbouj,Baha Eddin Gaaloul,Jorg Kruger*

Main category: cs.RO

TL;DR: GAZELOAD is a multimodal dataset for mental workload estimation in industrial human-robot collaboration, combining eye-tracking, environmental, and task context data from 26 participants interacting with collaborative robots.


<details>
  <summary>Details</summary>
Motivation: To enable development and benchmarking of mental workload estimation algorithms in realistic industrial human-robot collaboration (HRC) scenarios, and to study how environmental factors like lighting affect eye-based workload markers.

Method: Data were collected in a lab assembly testbed with 26 participants wearing Meta ARIA smart glasses while interacting with UR5 and Franka Emika Panda robots; eye-tracking signals (e.g., pupil diameter, gaze entropy), environmental measurements (e.g., illuminance), and task/robot context were time-synchronized and annotated with self-reported mental workload (1–10 Likert scale) per task block.

Result: A publicly available, time-synchronized multimodal dataset including ocular metrics (aggregated in 250 ms windows), environmental logs, task/robot context, and subjective workload ratings, organized per participant with documentation.

Conclusion: GAZELOAD supports algorithm development for mental workload estimation, feature extraction, and temporal modeling in industrial HRC, and facilitates investigation into environmental influences on ocular workload indicators.

Abstract: This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.

</details>


### [315] [LLM-Driven Scenario-Aware Planning for Autonomous Driving](https://arxiv.org/abs/2601.21876)
*He Li,Zhaowei Chen,Rui Gao,Guoliang Li,Qi Hao,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）驱动的自适应规划方法LAP，用于自动驾驶中的混合规划器切换框架（HPSF），通过LLM进行场景理解，并联合优化模式配置与运动规划，提升密集交通中高速与精确驾驶的平衡性能。


<details>
  <summary>Details</summary>
Motivation: 现有HPSF方法在密集交通中难以实现可靠模式切换和持续高效驾驶，主因是启发式场景识别和低频控制更新。

Method: 提出LAP方法，利用LLM进行场景理解，并将其推理结果融入模式配置与运动规划的联合优化；联合优化采用树搜索模型预测控制与交替最小化求解；在ROS中用Python实现。

Result: 高保真仿真结果表明，LAP在驾驶时间和成功率两方面均优于其他基准方法。

Conclusion: LAP通过LLM增强的场景理解与联合优化机制，有效提升了复杂交通环境下自动驾驶规划的质量与鲁棒性。

Abstract: Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.

</details>


### [316] [Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation](https://arxiv.org/abs/2601.21884)
*Pratik Ingle,Jørn Lambertsen,Kasper Støy,Andres Faina*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的多模块MANTA-RAY平台，通过降低执行器密度、模块化设计和几何变换驱动的PID控制器，实现了对脆弱与异质物体的高效、并行操控。


<details>
  <summary>Details</summary>
Motivation: 高密度执行器阵列虽能实现复杂形变，但带来高自由度、系统复杂性和可扩展性差等问题；现有单模块MANTA-RAY平台尚未验证多模块配置的可行性与优势。

Method: 设计分布式、模块化、可扩展的多模块MANTA-RAY平台；采用模块间物体传递策略和基于倾斜角映射的几何变换驱动PID控制器，避免数据驱动或黑箱训练；在仿真（3x3/4x4）和实物（2x2原型）中验证。

Result: 系统成功操控多种几何形状、质量、纹理的物体（包括鸡蛋、苹果等易碎物），支持并行操作；多模块配置提升了覆盖面积、可扩展性与多物体协同操控能力。

Conclusion: 多模块MANTA-RAY在保持低执行器密度的同时，显著提升系统可扩展性与实用性，为真实场景中的柔性表面操控提供了可行方案。

Abstract: Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.

</details>


### [317] [Information Filtering via Variational Regularization for Robot Manipulation](https://arxiv.org/abs/2601.21926)
*Jinhao Zhang,Wenlong Xia,Yaojia Wang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 本文提出了一种名为变分正则化（VR）的轻量级模块，用于在基于扩散模型的视觉运动策略中对骨干网络特征施加时间步条件高斯分布并引入KL散度正则化，以构建自适应信息瓶颈，从而抑制中间特征中的任务无关噪声；实验表明该方法在多个仿真基准和真实机器人任务上显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D视觉表示的扩散式视觉运动策略虽性能强，但其过大的去噪解码器会引入冗余和噪声于中间特征；实验证明随机遮蔽骨干特征反而能提升性能，说明存在任务无关噪声。

Method: 提出变分正则化（VR）模块：在骨干特征上施加时间步条件的高斯先验，并通过KL散度进行正则化，形成自适应信息瓶颈，抑制噪声而不增加训练复杂度。

Result: 在RoboTwin2.0、Adroit和MetaWorld三个仿真基准上，相比基线DP3，成功率分别提升6.1%、4.1%和4.1%；真实世界实验也验证了其有效性。

Conclusion: VR是一种简单高效、即插即用的正则化方法，能显著提升扩散式视觉运动策略的泛化性与鲁棒性，为视觉表征学习中的噪声抑制提供了新思路。

Abstract: Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.

</details>


### [318] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 本文提出了一种面向阶段化手术操作任务的监督式混合专家（MoE）架构，可嵌入任意自主策略，仅用少于150次示范和双目内窥镜图像即可训练出高性能、鲁棒性强的手术机器人策略（如基于ACT），在肠管抓取与牵拉任务中显著优于现有VLA模型和标准ACT，并实现零样本迁移至离体及初步在体猪组织实验。


<details>
  <summary>Details</summary>
Motivation: 手术机器人模仿学习面临数据稀缺、工作空间受限及对安全性与可预测性要求极高的挑战，现有方法依赖多相机或大量演示，难以实用化。

Method: 提出一种监督式Mixture-of-Experts（MoE）架构，适配阶段化手术任务，可即插即用地增强任意自主策略；以轻量级动作解码器ACT为基础，仅使用双目内窥镜图像和<150次示范进行训练。

Result: 在肠管抓取与牵拉任务中，该方法显著超越标准ACT和当前主流VLA模型：在分布内测试中成功率更高，在分布外场景（新抓取位置、低光照、部分遮挡）下鲁棒性更强；支持未见视角泛化和零样本迁移至离体及初步在体猪组织；并在真实在体猪手术中完成初步策略部署验证。

Conclusion: 监督式MoE架构为数据受限、高安全要求的手术机器人模仿学习提供了高效、鲁棒、可迁移的新范式，推动了从离体到在体临床应用的可行路径。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


### [319] [Macro-Scale Electrostatic Origami Motor](https://arxiv.org/abs/2601.21976)
*Alex S. Miller,Leo McElroy,Jeffrey H. Lang*

Main category: cs.RO

TL;DR: 本文提出并实现了首个宏观尺度的折纸旋转电机，该电机可折叠扁平化存储，并展开后实现连续旋转运动，采用电晕放电原理产生扭矩，具有高扩展比、高转速和良好扭矩密度。


<details>
  <summary>Details</summary>
Motivation: 现有折纸机器人多依赖线性执行器或外置非折叠旋转电机，缺乏能同时满足折叠性与连续旋转能力的宏观尺度执行器。

Method: 设计并制造基于电晕放电原理的宏观尺度折纸结构旋转电机，通过折叠/展开机制实现体积压缩与功能恢复，并进行性能测试（转速、扭矩、扩展比等）。

Result: 原型机实现2.5:1扩展比，-29 kV驱动下最高转速达1440 rpm，最大输出扭矩超0.15 mN·m，主动部件扭矩密度为0.04 Nm/kg。

Conclusion: 成功验证了宏观折纸结构实现连续旋转运动的可行性，为折叠机器人提供新型紧凑型旋转执行器方案。

Abstract: Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.

</details>


### [320] [PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy](https://arxiv.org/abs/2601.22018)
*Jinhao Zhang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Wenlong Xia,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 本文提出PocketDP3，一种轻量级3D扩散策略模型，用基于MLP-Mixer的Diffusion Mixer（DiM）替代原有重型U-Net解码器，在保持甚至提升性能的同时大幅减少参数量（<1%）并加速推理，适用于实时部署与真实场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉扩散策略模型存在编码器轻、解码器重的架构失配问题，导致解码器参数浪费；作者认为在紧凑点云表征下，可大幅精简解码器。

Method: 提出PocketDP3：采用轻量级Diffusion Mixer（DiM）替代传统条件U-Net解码器，DiM基于MLP-Mixer块，支持时序与通道维度高效融合；支持两步推理，无需一致性蒸馏。

Result: 在RoboTwin2.0、Adroit和MetaWorld三个仿真基准上达到SOTA性能，参数量不足先前方法的1%，推理加速；真实机器人实验验证其可行性与跨域迁移能力。

Conclusion: PocketDP3证明了轻量化解码器设计在3D扩散策略中的有效性，兼顾高性能、低参数与实时性，为实际部署提供了新范式。

Abstract: Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.

</details>


### [321] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab is a lightweight, open-source robot learning framework that integrates GPU-accelerated simulation (via MuJoCo Warp) with a modular, manager-based API for easy composition of observations, rewards, and events—enabling rapid setup and direct access to MuJoCo internals.


<details>
  <summary>Details</summary>
Motivation: To reduce setup friction and dependency overhead in robot learning research while enabling high-performance, GPU-accelerated simulation and modular environment design.

Method: Adopts Isaac Lab’s manager-based API for composable environment components (observations, rewards, events) and integrates MuJoCo Warp for GPU-accelerated physics simulation; designed for minimal installation and native MuJoCo data access.

Result: A single-command-installable, lightweight framework with low dependency count, native MuJoCo integration, and reference implementations for velocity tracking, motion imitation, and manipulation tasks.

Conclusion: mjlab successfully bridges usability and performance in robot learning by combining modular design, GPU acceleration, and simplicity—making it suitable for rapid prototyping and research.

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>


### [322] [ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection](https://arxiv.org/abs/2601.22090)
*Runsheng Wang,Katelyn Lee,Xinyue Zhu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 本文提出一种从健康人sEMG数据预训练模型迁移适配至卒中患者意图检测的新方法，显著降低个体校准负担并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 卒中后偏瘫肌肉的sEMG意图检测常需冗长且个体化的校准，且对信号变异性敏感、鲁棒性差。

Method: 构建健康到卒中的迁移适配流程：先在大规模健康人群sEMG数据上预训练意图检测模型，再利用少量卒中受试者数据进行微调；对比了仅调头部、LoRA适配器和全参数微调三种策略。

Result: 在三位慢性卒中患者新采集的数据集上，适配方法相较零样本迁移和纯卒中数据训练均显著提升性能：平均转移准确率从0.42升至0.61，原始准确率从0.69升至0.78；尤其在会话内漂移、姿势变化和电极带重置等分布偏移场景下表现更稳健。

Conclusion: 利用健康人群sEMG学习到的通用表征可有效迁移至卒中康复场景，在减少校准数据需求的同时增强实时意图检测的鲁棒性与实用性。

Abstract: Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.

</details>


### [323] [DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation](https://arxiv.org/abs/2601.22153)
*Haozhe Xie,Beichen Wen,Jiarui Zheng,Zhaoxi Chen,Fangzhou Hong,Haiwen Diao,Ziwei Liu*

Main category: cs.RO

TL;DR: 本文提出DynamicVLA框架，通过紧凑型0.4B VLA模型、连续推理机制和潜在感知动作流技术，提升动态物体操控能力，并构建了包含合成与真实数据的DOM基准。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action（VLA）模型在静态操控任务中表现良好，但在需快速感知、时序预测和持续控制的动态场景中仍面临挑战。

Method: 提出DynamicVLA框架，包含：1）基于卷积视觉编码器的紧凑0.4B VLA模型；2）支持重叠推理与执行的Continuous Inference机制；3）实现感知-执行时间对齐的Latent-aware Action Streaming。同时构建了Dynamic Object Manipulation（DOM）基准，含200K合成和2K真实世界操控数据。

Result: 实验表明DynamicVLA在响应速度、感知能力和泛化性方面显著提升，成为适用于多种具身平台的通用动态物体操控框架。

Conclusion: DynamicVLA为动态物体操控提供了统一、高效且可扩展的解决方案，填补了该领域基础数据与方法的双重空白。

Abstract: Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [324] [AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping](https://arxiv.org/abs/2601.21126)
*Kooktae Lee,Julian Martinez*

Main category: cs.MA

TL;DR: 本文提出了一种AI增强的去中心化多智能体环境建图框架，通过最优传输理论和双层MLP模块实现自适应密度估计与不确定性调节，显著提升复杂空间分布的重建精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖控制方法在先验地图不准确或存在偏差时性能下降，而实际环境中传感与通信受限，亟需具备自校正能力的去中心化建图方法。

Method: 基于最优传输理论构建自适应、自校正的局部密度估计框架；引入双层MLP模块推断局部均值-方差统计并调节未访问区域的虚拟不确定性；采用Wasserstein度量进行理论收敛性分析。

Result: 理论证明了算法在Wasserstein度量下的收敛性；仿真表明该AI增强的密度驱动最优控制方法在多模态空间分布重建中显著优于传统去中心化基线方法，具备更高保真度与鲁棒性。

Conclusion: 所提框架有效克服了先验不确定性影响，实现了通信与感知受限下多机器人系统的高精度、可扩展、理论一致的协同建图。

Abstract: This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.

</details>


### [325] [Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions](https://arxiv.org/abs/2601.21477)
*Tobias Schmidt,Kai Cui*

Main category: cs.MA

TL;DR: 本文提出了一种适用于大规模稀疏图的平均场控制（MFC）新框架，通过定义基于装饰化根邻域的概率测度来刻画局部异质性，并证明了有限时域下策略的‘时距依赖局部性’，从而导出可计算的动态规划原理，并为图神经网络在Actor-Critic算法中的应用提供了理论与实验支持。


<details>
  <summary>Details</summary>
Motivation: 传统平均场控制依赖于全连接、可交换性假设，难以适配现实中的稀疏网络结构，亟需拓展至更一般的图拓扑。

Method: 将系统状态重新定义为装饰化根邻域上的概率测度；引入并证明‘horizon-dependent locality’性质；在此基础上构建提升空间（lifted space）上的动态规划原理；结合图神经网络实现actor-critic算法。

Result: 建立了稀疏图上平均场控制的严格理论框架；证明了最优策略仅依赖于有限跳数邻域；给出了GNN用于该设定下的收敛性与表达能力保证；实验验证了方法在复杂稀疏拓扑上的有效性。

Conclusion: 该工作弥合了经典平均场控制与现实网络结构之间的鸿沟，使MFC具备在大规模稀疏图上可扩展、可解释且理论上严谨的强化学习能力。

Abstract: Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.

</details>


### [326] [Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems](https://arxiv.org/abs/2601.22041)
*Naomi Pitzer,Daniela Mihai*

Main category: cs.MA

TL;DR: 本文研究了在感知异质性环境下多智能体如何通过多步二进制通信游戏发展出类一致的、基于感知输入的信息，发现多模态系统虽能收敛但需更多比特和更高不确定性，而单模态更高效；语义以分布式而非组合方式编码；跨感知世界系统需微调才能互通。


<details>
  <summary>Details</summary>
Motivation: 现有涌现通信研究多假设智能体具有同质模态或对齐表征空间，忽略了真实世界中感知异质性的挑战，本文旨在探究感知未对齐、模态异构条件下通信如何涌现与泛化。

Method: 设计了一个异构的多步二进制通信游戏，包含模态不同、缺乏感知基础的智能体；通过训练多/单模态系统，分析消息一致性、分类熵、比特扰动效应及跨系统微调后的互操作性。

Result: 多模态系统可收敛至类一致且感知接地的消息，但比特开销更大、分类熵更高；单模态更高效；比特扰动实验证明语义呈分布式编码；不同感知世界训练的系统无法直接互通，但经少量微调可实现成功跨系统通信。

Conclusion: 涌现通信可作为研究智能体在异构模态间自适应与表征迁移的理论框架，为理解表征对齐、分布语义与跨模态泛化提供了新实验范式与理论启示。

Abstract: Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [327] [Generalized Information Gathering Under Dynamics Uncertainty](https://arxiv.org/abs/2601.21988)
*Fernando Palafox,Jingqi Li,Jesse Milzman,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 本文提出了一种统一的信息采集框架，通过显式建模参数、信念与控制之间的因果依赖关系，解耦了建模选择与信息采集成本；基于Massey的有向信息推导出通用成本函数，并证明传统互信息成本为其特例，同时建立了其与线性化贝叶斯估计中信息增益的理论联系。


<details>
  <summary>Details</summary>
Motivation: 现有主动信息采集方法依赖于特定建模选择（如动力学模型、信念更新、观测模型和规划器），缺乏通用性。

Method: 构建一个显式暴露参数、信念与控制之间因果依赖关系的统一框架；基于Massey有向信息推导通用信息采集成本；证明互信息成本是其特例；建立其与线性化贝叶斯估计中信息增益的理论联系。

Result: 提出了通用信息采集成本函数；证明互信息成本为其特例；建立了其与信息增益的理论联系；在多个系统（线性、非线性、多智能体）上验证了实用性。

Conclusion: 该框架提升了主动学习方法的通用性与理论严谨性，为不同建模选择下的信息采集提供了统一基础。

Abstract: An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.

</details>


### [328] [Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning](https://arxiv.org/abs/2601.21037)
*Chengzu Li,Zanyi Wang,Jiaang Li,Yi Xu,Han Zhou,Huanyu Zhang,Ruichuan An,Dengyang Jiang,Zhaochong An,Ivan Vulić,Serge Belongie,Anna Korhonen*

Main category: cs.LG

TL;DR: 本文提出利用视频生成模型进行视觉推理，通过生成中间帧作为推理步骤，在迷宫导航和七巧板拼图任务中验证了其零样本泛化能力、视觉上下文利用能力和测试时视觉扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在细粒度空间理解和连续动作规划方面存在不足，难以模拟复杂视觉推理所需的动态过程。

Method: 将视觉推理建模为视频生成任务，以生成的视频帧作为从初始状态到解决方案之间的中间推理步骤，并在迷宫导航（离散序列规划）和七巧板拼图（连续操作）两个任务上进行评估。

Result: 发现视频生成模型具备鲁棒的零样本泛化能力、能有效利用视觉上下文（如智能体图标、七巧板形状）进行控制、并表现出视觉测试时扩展规律（增加生成视频长度可提升复杂路径的零样本泛化性能）。

Conclusion: 视频生成不仅是一种媒体生成工具，更是一种可扩展、可泛化的视觉推理新范式。

Abstract: Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.

</details>


### [329] [Human-LLM Collaborative Feature Engineering for Tabular Data](https://arxiv.org/abs/2601.21060)
*Zhuoyan Li,Aditya Bansal,Jinzhao Li,Shishuang He,Zhuoran Lu,Mutian Zhang,Qin Liu,Yiwei Yang,Swati Jain,Ming Yin,Yunyao Li*

Main category: cs.LG

TL;DR: 本文提出了一种人机协同的特征工程框架，将LLM用于生成特征变换候选操作，而操作选择则基于显式建模的效用与不确定性，并融合人类专家偏好反馈，从而提升性能并降低用户认知负荷。


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM作为黑箱优化器，缺乏对操作效用的校准估计，易重复探索低收益操作，且无原则性策略优先探索高潜力方向。

Method: 解耦操作生成与选择过程：LLM仅负责生成候选变换；选择阶段建模每个操作的效用与不确定性，并在效用估计不准时主动引入人类专家的成对偏好反馈以指导选择。

Result: 在合成实验与真实用户研究中均验证了该框架可提升多种表格数据上的特征工程效果，并显著降低用户的认知负担。

Conclusion: 人机协同机制（特别是基于偏好的反馈整合）能有效弥补LLM在效用评估上的不足，使特征工程更高效、可解释且用户友好。

Abstract: Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.

</details>


### [330] [Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification](https://arxiv.org/abs/2601.21244)
*Yiju Guo,Tianyi Hu,Zexu Sun,Yankai Lin*

Main category: cs.LG

TL;DR: 本文提出LENS框架，通过识别并移除干扰token来提升RLVR中LLM推理的探索效率与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在有限rollout预算下探索效率低，导致采样成功率低和训练不稳定，而许多失败源于少量干扰性prompt token。

Method: 提出Less Noise Sampling Framework（LENS）：先净化prompt（识别并删除干扰token），再将净化后获得的成功rollout迁移用于监督原始噪声prompt下的策略优化。

Result: LENS在实验中显著优于GRPO，平均性能提升3.88%，收敛速度提升1.6倍以上。

Conclusion: 剪枝干扰token可显著提升rollout效率，为RLVR研究提供了新视角。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.

</details>


### [331] [From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning](https://arxiv.org/abs/2601.21436)
*Hang Ni,Weijia Zhang,Fei Wang,Zezhi Shao,Hao Liu*

Main category: cs.LG

TL;DR: 本文提出MADI模型，通过细粒度对齐与解耦交互增强多模态大语言模型在时间序列理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态融合中存在时序错位和语义纠缠问题，限制了局部解释与互补推理能力。

Method: MADI包含三个核心组件：(1) 基于物理意义的补丁级对齐；(2) 离散解耦交互，分离共性语义与模态特异性信息；(3) 关键词高亮机制以增强查询相关信号。

Result: 在合成与真实世界基准测试中，MADI持续优于通用大语言模型及专用时间序列多模态大模型。

Conclusion: 细粒度对齐与语义解耦是提升多模态时间序列理解性能的关键设计原则。

Abstract: Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visualized plots, facilitating precise value reasoning and visual structure comprehension for comprehensive time series understanding of MLLMs. However, effective cross-modal integration remains challenging due to fine-grained temporal misalignment across modalities and severe entanglement between shared and modality-specific semantics, which hinder localized interpretation and complementary reasoning. To address these issues, we propose MADI, a multi-modal LLM enhanced with fine-grained alignment and disentangled interaction, featuring (1) Patch-level Alignment, which enforces physically grounded fine-grained correspondence across heterogeneous modalities, (2) Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information, and (3) Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning. Experiments on synthetic and real-world benchmarks show that MADI consistently outperforms general-purpose LLMs and time-series-specialized MLLMs.

</details>


### [332] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型（LLM）安全认证的‘噪声但有效’假设检验框架，通过小规模人工标注校准集估计LLM裁判的TPR/FPR，在大规模自动标注数据上实现有限样本下Type-I错误控制，并在理论和实验上验证了其相对于直接评估与Prediction-Powered Inference的优势。


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge方法虽具可扩展性，但其固有噪声、偏差与不完美性会破坏统计保证，亟需一种能应对裁判不确定性的可靠认证框架。

Method: 构建基于校准集估计TPR/FPR的‘Noisy but Valid’假设检验框架，推导方差修正的临界阈值，实现有限样本下的Type-I误差控制；区别于黑箱式PPI，该框架显式建模裁判行为。

Result: （1）给出噪声检验优于直接评估的精确条件；（2）在Jigsaw、Hate Speech与SafeRLHF数据集上实证验证；（3）揭示并量化了实际方法与‘Oracle’（已知真参数）间的性能差距（Oracle Gap）。

Conclusion: 本工作为LLM裁判场景下的统计评估提供了首个系统化、具解释性且带理论保证的诊断工具，厘清了裁判质量、数据规模与认证要求对评估效能的影响机制及权衡关系。

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [333] [Shaping capabilities with token-level data filtering](https://arxiv.org/abs/2601.21571)
*Neil Rathi,Alec Radford*

Main category: cs.LG

TL;DR: 本文提出在预训练阶段通过数据过滤（尤其是token级过滤）来有效、鲁棒且低成本地消除语言模型的不期望能力（如医疗能力），效果随模型规模增大而增强，并支持后续在被遗忘领域上的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有减少语言模型不期望能力的方法多为后处理，易被对抗者绕过；因此需在预训练阶段主动塑造模型能力。

Method: 在预训练数据中进行文档级和token级过滤，结合稀疏自编码器标注token、蒸馏轻量分类器，并在不同规模模型上验证效果与鲁棒性。

Result: token级过滤比文档级更高效，在最大模型上实现被遗忘领域7000倍计算减速；过滤后的模型仍可在该领域完成对齐；方法对噪声标签具有鲁棒性。

Conclusion: 在预训练中实施细粒度（token级）数据过滤是一种高效、可扩展且鲁棒的‘能力遗忘’策略，为可控模型训练提供了新范式。

Abstract: Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.

</details>


### [334] [Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer](https://arxiv.org/abs/2601.21323)
*Achraf Hsain,Ahmed Abdelkader,Emmanuel Baldwin Mbaya,Hamoud Aljamaan*

Main category: cs.LG

TL;DR: 本文研究了对抗样本在深度神经网络（DNN）与基于HOG的手工特征分类器之间的迁移性，发现传统ML模型同样易受攻击，且FGSM比PGD更具破坏力，表明对抗脆弱性是图像分类系统的固有属性。


<details>
  <summary>Details</summary>
Motivation: 探究经典机器学习流水线（使用手工特征）是否继承深度神经网络的对抗脆弱性，特别是当通过神经代理模型发起攻击时；验证特征工程是否能通过信息瓶颈过滤对抗信号。

Method: 以VGG16为代理模型，在CIFAR-10上生成FGSM和PGD对抗样本，测试其迁移到四种经典分类器（KNN、决策树、线性SVM、核SVM）及一个浅层神经网络在八种HOG配置下的效果。

Result: 所有经典分类器均出现16.6%–59.1%的相对准确率下降；FGSM在100%情况下比PGD造成更大性能退化；块归一化仅提供有限缓解。

Conclusion: 对抗脆弱性并非端到端可微性的副产品，而是图像分类系统的根本属性，对跨计算范式的安全关键部署具有重要启示。

Abstract: Deep neural networks are vulnerable to adversarial examples--inputs with imperceptible perturbations causing misclassification. While adversarial transfer within neural networks is well-documented, whether classical ML pipelines using handcrafted features inherit this vulnerability when attacked via neural surrogates remains unexplored. Feature engineering creates information bottlenecks through gradient quantization and spatial binning, potentially filtering high-frequency adversarial signals. We evaluate this hypothesis through the first comprehensive study of adversarial transfer from DNNs to HOG-based classifiers. Using VGG16 as a surrogate, we generate FGSM and PGD adversarial examples and test transfer to four classical classifiers (KNN, Decision Tree, Linear SVM, Kernel SVM) and a shallow neural network across eight HOG configurations on CIFAR-10. Our results strongly refute the protective hypothesis: all classifiers suffer 16.6%-59.1% relative accuracy drops, comparable to neural-to-neural transfer. More surprisingly, we discover attack hierarchy reversal--contrary to patterns where iterative PGD dominates FGSM within neural networks, FGSM causes greater degradation than PGD in 100% of classical ML cases, suggesting iterative attacks overfit to surrogate-specific features that don't survive feature extraction. Block normalization provides partial but insufficient mitigation. These findings demonstrate that adversarial vulnerability is not an artifact of end-to-end differentiability but a fundamental property of image classification systems, with implications for security-critical deployments across computational paradigms.

</details>


### [335] [Revisiting Diffusion Model Predictions Through Dimensionality](https://arxiv.org/abs/2601.21419)
*Qing Jin,Chaoyang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种广义预测框架，理论分析了数据几何结构（尤其是本征维度与环境维度的关系）如何影响扩散/流匹配模型中最优预测目标（噪声ε、速度v或直接数据x）的选择，并据此提出k-Diff方法，通过数据驱动方式自动学习最优预测参数k，显著提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 尽管实践中发现高维数据下直接预测数据x优于预测噪声ε或速度v，但缺乏对‘为何最优预测目标依赖于数据特性’的严格理论解释。

Method: 建立一个能统一涵盖ε-、v-、x-预测的广义预测形式；推导数据几何（特别是本征维与环境维之比）与最优预测目标间的解析关系；提出k-Diff框架，用数据驱动方式端到端学习最优预测参数k，避免显式估计难以获取的本征维度。

Result: 理论证明当环境维度远大于数据本征维度时，x-prediction最优；k-Diff在潜空间和像素空间图像生成任务中一致优于固定目标基线（如DDPM、Flow Matching），且适用于不同架构与数据规模。

Conclusion: 数据维度特性是决定最优预测目标的根本因素；k-Diff提供了一种原理清晰、无需人工调参的自动化改进方案，为扩散与流匹配模型的设计提供了新范式。

Abstract: Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.

</details>


### [336] [Lossy Common Information in a Learnable Gray-Wyner Network](https://arxiv.org/abs/2601.21424)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 本文提出了一种受Gray-Wyner信息论启发的可学习三通道编解码器，用于在多视觉任务中分离共享信息与任务特定信息，显著减少冗余并提升性能。


<details>
  <summary>Details</summary>
Motivation: 许多计算机视觉任务存在大量重叠信息，但传统编解码器忽略这一点，导致表示冗余低效。

Method: 基于Gray-Wyner网络思想，构建可学习的三通道编解码器，分离共享与任务特有信息；引入损失下公共信息概念，并设计权衡该分离过程的优化目标。

Result: 在六个视觉基准上的双任务场景中，相比独立编码，所提方法显著降低冗余并持续取得更优性能。

Conclusion: Gray-Wyner理论在现代机器学习中仍具实用价值，能有效桥接经典信息论与面向任务的表征学习。

Abstract: Many computer vision tasks share substantial overlapping information, yet conventional codecs tend to ignore this, leading to redundant and inefficient representations. The Gray-Wyner network, a classical concept from information theory, offers a principled framework for separating common and task-specific information. Inspired by this idea, we develop a learnable three-channel codec that disentangles shared information from task-specific details across multiple vision tasks. We characterize the limits of this approach through the notion of lossy common information, and propose an optimization objective that balances inherent tradeoffs in learning such representations. Through comparisons of three codec architectures on two-task scenarios spanning six vision benchmarks, we demonstrate that our approach substantially reduces redundancy and consistently outperforms independent coding. These results highlight the practical value of revisiting Gray-Wyner theory in modern machine learning contexts, bridging classic information theory with task-driven representation learning.

</details>


### [337] [Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking](https://arxiv.org/abs/2601.21619)
*Yiming Wang,Zhuosheng Zhang,Rui Wang*

Main category: cs.LG

TL;DR: 本文提出T2方法，通过利用潜在表示在解码前估计每个样本的最优并行度，以解决大语言模型中因样本异质性导致的‘过度扩展诅咒’问题，从而在保持性能的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 由于样本异质性，固定全局高并行度（N）会导致部分样本资源浪费（即‘过度扩展诅咒’），影响推理效率。

Method: 提出轻量级方法T2，利用隐层表征预估每个样本所需的最优并行度N'，实现动态、样本级的并行度分配。

Result: 实验表明，T2在维持相近性能的同时显著降低计算成本，提升并行推理效率。

Conclusion: ‘过度扩展诅咒’普遍存在且严重；T2为高效、自适应的并行推理提供了可行路径。

Abstract: Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.

</details>


### [338] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 本文提出Repository-Centric Learning（RCL）新范式，通过深度学习单个代码仓库的内在结构（“physics”），训练出轻量级但高度专业化的代码模型SWE-Spot-4B，在多项软件工程任务上超越更大尺寸开源模型及部分商用小模型。


<details>
  <summary>Details</summary>
Motivation: 现有小语言模型（SLMs）在隐私敏感和资源受限场景中部署需求迫切，但其推理时泛化能力弱，难以应对陌生复杂代码库；传统任务中心学习（TCL）范式无法弥补该能力缺口。

Method: 提出以代码仓库为中心的学习（RCL）范式，强调对单个仓库的纵向深度理解而非跨任务广度覆盖；设计四单元‘Repository-Centric Experience’，将静态代码库转化为交互式学习信号；据此训练SWE-Spot-4B系列小型专家模型。

Result: SWE-Spot-4B在多个SWE任务上优于更大开源模型（如CWM、Qwen3-Coder-30B）及效率导向商用模型（如GPT-4.1-mini、GPT-5-nano）；同时具备更高训练样本效率与更低推理开销。

Conclusion: 仓库级掌握是构建高效智能不可或缺的新维度，应与通用编码能力互补；RCL范式为小模型在真实软件环境中的实用化提供了新路径。

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [339] [Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities](https://arxiv.org/abs/2601.21702)
*Tien Dang,The-Hai Nguyen,Dinh Mai Phuong,Nguyen Minh Phuong,Hoang Thanh-Tung,Le-Minh Nguyen,Naoya Inoue*

Main category: cs.LG

TL;DR: 本文提出并研究了表示误导（RM）这一类大语言模型（LLM）遗忘方法，通过操纵遗忘样本的潜在表示（forget-representations）实现遗忘；基于线性表示假设，发现用于RM的目标向量不仅能实现遗忘，还能激发可控的行为调控与能力增强等副作用，并揭示其潜在风险与应用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管表示误导（RM）是一类重要的LLM遗忘方法，但其中目标向量所起的作用尚未被充分探索。本文旨在从线性表示假设出发，重新审视RM中目标向量的深层作用。

Method: 基于线性表示假设，将高阶概念建模为一维表示向量，并在forget-representation空间中进行线性操作；通过实证分析验证RM在行为控制（如真实性、情感、拒绝倾向）和能力增强（如上下文学习能力）方面的副作用。

Result: 发现RM不仅实现遗忘，还一致引发可控的侧向行为与更强的高阶能力；该现象在多种任务上被广泛验证。

Conclusion: RM中的目标向量具有双重效应：既带来潜在滥用风险，也提供了一种可利用机制，用于构建具备更强能力和可控行为的新一代模型。

Abstract: We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models' truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models' in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.

</details>


### [340] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 本文提出了一种面向多模态大语言模型（MLLMs）的视觉引导关键词正则化方法（ViKeR），通过利用无关视觉输入预测目标词级分布，结合信息熵定义关键token，并通过梯度重加权增强关键token更新，从而在图像级遗忘任务中实现高效、低遗忘、高连贯性的未学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM遗忘方法直接套用LLM方法，忽视了多模态特性：未区分答案中各token的重要性，且仅关注语言模态而忽略视觉线索对关键token的指示作用。

Method: 提出Visual-Guided Key-Token Regularization (ViKeR)：1）形式化MLLM多模态问答中的遗忘问题；2）利用无关视觉输入预测理想遗忘后token分布并用于正则化；3）基于信息熵定义关键token；4）通过token级梯度重加权放大关键token的参数更新。

Result: 在MLLMU和CLEAR基准上验证，ViKeR在有效遗忘目标图像的同时，显著缓解了知识遗忘（forgetting）并保持了回答的语义连贯性。

Conclusion: 视觉线索可有效指导MLLM中关键答案token的识别与正则化，ViKeR通过多模态协同建模提升了遗忘的精准性与模型整体稳定性。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [341] [GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization](https://arxiv.org/abs/2601.22095)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Chi Wang,Yuehao Wang,Jing Xiong,Liliang Ren,Bo Peng,Qingmei Wang,Xiaoran Shang,Mac Schwager,Anderson Schneider,Yuriy Nevmyvaka,Xiaodong Liu*

Main category: cs.LG

TL;DR: 本文提出GeoNorm，一种基于流形优化视角的新型归一化方法，通过在流形上进行测地线更新，并引入层间更新衰减机制，显著提升Transformer性能且几乎不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: Pre-Norm与Post-Norm在Transformer中的放置方式尚无定论，作者从流形优化角度重新审视归一化层的作用。

Method: 将FFN和注意力层输出解释为优化方向，用测地线更新替代标准归一化，并设计层间更新衰减机制。

Result: GeoNorm在各类Transformer模型中持续优于现有归一化方法，且可无缝集成、计算开销极小。

Conclusion: GeoNorm提供了一种更符合优化本质的归一化范式，验证了流形视角对Transformer架构设计的有效指导价值。

Abstract: The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.

</details>


### [342] [Discovering Hidden Gems in Model Repositories](https://arxiv.org/abs/2601.22157)
*Jonathan Kahana,Eliahu Horwitz,Yedid Hoshen*

Main category: cs.LG

TL;DR: 本文发现大量被忽视的高性能微调模型（'hidden gems'），提出基于多臂老虎机和改进Sequential Halving算法的高效模型发现方法，显著降低搜索成本。


<details>
  <summary>Details</summary>
Motivation: 探究开源社区中模型使用高度集中于少数基础模型的现象，是市场高效选择还是优质模型被系统性忽视。

Method: 对2000多个模型进行大规模评估；将模型发现建模为多臂老虎机问题；改进Sequential Halving算法，采用共享查询集和激进淘汰策略。

Result: 在Llama-3.1-8B系列中发现下载量极少但数学性能从83.2%提升至96.0%的模型；新方法仅需每候选模型约50次查询，加速发现超50倍。

Conclusion: 存在大量未被关注的高性能微调模型；所提搜索方法可大幅提高发现效率，推动更优模型的实际应用。

Abstract: Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.

</details>


### [343] [Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization](https://arxiv.org/abs/2601.20868)
*Rongzheng Wang,Yihong Huang,Muquan Li,Jiakai Li,Di Liang,Bob Simons,Pei Ke,Shuang Liang,Ke Qin*

Main category: cs.LG

TL;DR: 本文提出DASH框架，通过动态感知的求解器启发式设计和配置文件库检索（PLR），在组合优化问题中显著提升运行效率与解质量，并大幅降低大语言模型适应成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的启发式设计（LHD）框架存在终点评估单一、忽略收敛过程与运行效率，以及分布偏移时适应成本过高的问题。

Method: 提出Dynamics-Aware Solver Heuristics（DASH）框架，联合优化求解器搜索机制与运行调度策略，并引入Profiled Library Retrieval（PLR）机制，在进化过程中同步归档专用求解器以支持异构分布下的高效热启动。

Result: 在四个组合优化问题上，DASH将运行效率提升3倍以上，解质量超越现有最优基线；同时降低90%以上的LLM适应成本，并在不同分布下保持高精度。

Conclusion: DASH通过融合动态收敛感知与可复用求解器库，有效解决了LHD方法在效率、泛化性与适应成本方面的关键瓶颈，为LLM驱动的组合优化提供了更实用、可扩展的新范式。

Abstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.

</details>


### [344] [Finetune-Informed Pretraining Boosts Downstream Performance](https://arxiv.org/abs/2601.20884)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Finetune-Informed Pretraining (FIP) 的模型无关预训练方法，旨在通过增强目标模态的掩码难度、损失权重和解码器容量，使多模态预训练更偏向下游微调中实际使用的关键模态，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准多模态预训练对所有模态一视同仁，但在实际部署中往往只重点使用一种模态，导致该模态表征未被充分优化。

Method: FIP在不修改共享编码器、不增加监督信号的前提下，针对目标模态采用更高掩码难度、更强损失加权和更大解码器容量。

Result: 在无线信号星座图的掩码建模任务上，FIP持续提升了下游微调性能，且无需额外数据或计算开销。

Conclusion: FIP是一种简单、兼容性强、可广泛应用于多模态掩码建模流程的预训练优化方法。

Abstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.

</details>


### [345] [The Depth Delusion: Why Transformers Should Be Wider, Not Deeper](https://arxiv.org/abs/2601.20994)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 本文提出了架构条件下的神经缩放定律，揭示了在参数总量固定时，模型宽度应比深度增长更快（2.8倍），并发现存在一个临界深度：超过该深度后继续增加层数反而会损害性能（'深度幻觉'），该结论在17M至7B参数的30种Transformer架构上得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律将模型架构视为可互换，忽略深度与宽度对性能的差异化影响，无法指导最优架构设计。

Method: 提出架构条件缩放律，理论推导最优深度D* ~ C^0.12和最优宽度W* ~ C^0.34，并识别临界深度D_crit ~ W^0.44；在30种Transformer架构（17M–7B参数）上进行高算力训练与实证分析。

Result: 验证了宽度增长快于深度的规律（2.8×），发现并命名'深度幻觉'现象；在7B规模下，64层模型（6.38B参数）性能反低于32层模型（6.86B参数），损失高出0.12 nats；整体拟合R² = 0.922。

Conclusion: 模型深度与宽度不可简单互换；存在架构依赖的最优缩放关系，且该关系在大规模生产模型中依然成立，对高效大模型设计具有直接指导意义。

Abstract: Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.

</details>


### [346] [Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research](https://arxiv.org/abs/2601.21008)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.LG

TL;DR: 本文提出了两个新的运筹学（OR）基准测试ORDebug和ORBias，强调将求解器纳入评估循环，以评估模型在调试不可行模型和行为理性方面的表现。实验表明，领域特定的强化学习训练可显著提升小规模模型（8B）在修复率、诊断准确率和解决速度上的性能，超越了前沿API；同时，课程学习能有效减少系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准仅评估运筹学问题的一次性代码生成能力，忽略了实际中反复调试不可行模型这一核心工作流，缺乏对诊断与修复过程的评估。

Method: 构建两个新基准：ORDebug（含5000+多错误类型问题，每次修复后调用求解器并重新计算IIS以提供确定性反馈）和ORBias（2000个报童问题，评估模型偏离最优策略的系统性偏差）；采用领域定制的RLVR训练和课程学习策略，在26个模型上进行大规模评估。

Result: 8B模型经RLVR训练后，在ORDebug上修复率达95.3%（+9.1%）、诊断准确率62.4%（+14.6%）、解决步数减少1.7倍；在ORBias上实现ID→OOD负向偏移（-9.6%），系统性偏差降低48%。

Conclusion: 引入可验证oracle的过程级评估范式，配合针对性训练，可使中小规模模型在运筹学任务中显著超越更大规模通用模型，证明‘方法优于规模’。

Abstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \textbf{solver in the evaluation loop}. \textbf{\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \IIS{} recomputation, providing deterministic, verifiable feedback. \textbf{\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\% vs 86.2\% recovery rate (+9.1\%), 62.4\% vs 47.8\% diagnostic accuracy (+14.6\%), and 2.25 vs 3.78 steps to resolution (1.7$\times$ faster). On \ORBias{}, curriculum training achieves the only negative ID$\rightarrow$OOD bias drift among models evaluated (-9.6\%), reducing systematic bias by 48\% (from 20.0\% to 10.4\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.

</details>


### [347] [Conditional Denoising Model as a Physical Surrogate Model](https://arxiv.org/abs/2601.21021)
*José Afonso,Pedro Viegas,Rodrigo Ventura,Vasco Guerra*

Main category: cs.LG

TL;DR: 本文提出条件去噪模型（CDM），通过学习物理流形几何结构，以去噪方式隐式满足物理约束，无需显式引入物理方程，在等离子体基准测试中展现出更高参数与数据效率及更强物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有物理一致的代理建模方法难以严格满足控制方程：软约束易失效，后处理无法内化解的几何结构。

Method: 提出条件去噪模型（CDM），训练网络从加噪状态恢复干净状态，学习指向有效解子空间的向量场；采用时不变公式，将推理转化为确定性不动点迭代，实现向平衡流形的投影。

Result: 在低温等离子体物理与化学基准上，CDM比物理一致基线方法具有更高的参数和数据效率；即使训练中未接触控制方程，其物理约束满足度仍优于使用显式物理损失训练的基线。

Conclusion: 去噪目标可作为强隐式正则器，使模型无需显式物理监督即可内化并严格遵循物理规律，为物理驱动建模提供了新范式。

Abstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

</details>


### [348] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: 本文提出了一种名为SIGMA-PPG的生成式基础模型，通过统计先验引导的对抗掩码机制和语义一致性约束（向量量化），有效应对PPG信号冗余与噪声问题，在12个下游任务中性能优于5种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有PPG信号基础模型受限于信号固有的冗余性和噪声；标准掩码建模易得平凡解，对比学习缺乏形态学精度。

Method: 提出统计先验引导的生成式掩码架构（SIGMA-PPG），包含强化学习驱动的先验引导对抗掩码机制和基于向量量化的语义一致性约束，以提升生理波形映射的鲁棒性与代码本语义密度。

Result: 在超12万小时PPG数据上预训练后，SIGMA-PPG在12个下游任务中平均性能优于5种前沿基线方法。

Conclusion: SIGMA-PPG通过融合统计先验、对抗掩码与语义量化，显著提升了PPG基础模型的表征能力与泛化性，为生理信号建模提供了新范式。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [349] [SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection](https://arxiv.org/abs/2601.21050)
*Haokun Zhou*

Main category: cs.LG

TL;DR: 本文提出SMKC框架，通过特征哈希和混合核图像处理传感器动态变化的多变量时间序列异常检测问题，无需梯度更新即可实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 传统多变量时间序列异常检测假设观测变量集合静态不变，但实际监控系统中传感器常发生增删改（sensor churn），导致输入维度动态变化且存在训练未见值，现有方法难以应对。

Method: 提出SMKC框架：1）使用排列不变的特征哈希将原始可变长输入映射为固定长度状态序列；2）构建混合核图像以捕获序列及其导数的全局时序结构；3）采用掩码重建与师生预测目标学习正常模式。

Result: 实验表明鲁棒对数距离通道提供主要判别信号，余弦表示效果较差；基于SMKC表征的随机投影+近邻检测器无需梯度更新即能媲美全训练基线。

Conclusion: SMKC成功解耦动态输入结构与检测器，其表征本身具有强判别力，支持无训练冷启动部署，适用于资源受限场景。

Abstract: Conventional anomaly detection in multivariate time series relies on the assumption that the set of observed variables remains static. In operational environments, however, monitoring systems frequently experience sensor churn. Signals may appear, disappear, or be renamed, creating data windows where the cardinality varies and may include values unseen during training. To address this challenge, we propose SMKC, a framework that decouples the dynamic input structure from the anomaly detector. We first employ permutation-invariant feature hashing to sketch raw inputs into a fixed size state sequence. We then construct a hybrid kernel image to capture global temporal structure through pairwise comparisons of the sequence and its derivatives. The model learns normal patterns using masked reconstruction and a teacher-student prediction objective. Our evaluation reveals that robust log-distance channels provide the primary discriminative signal, whereas cosine representations often fail to capture sufficient contrast. Notably, we find that a detector using random projections and nearest neighbors on the SMKC representation performs competitively with fully trained baselines without requiring gradient updates. This highlights the effectiveness of the representation itself and offers a practical cold-start solution for resource-constrained deployments.

</details>


### [350] [Textual Equilibrium Propagation for Deep Compound AI Systems](https://arxiv.org/abs/2601.21064)
*Minghui Chen,Wenlong Deng,James Zou,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 本文提出Textual Equilibrium Propagation（TEP），一种受能量模型中平衡传播启发的局部学习方法，用于优化深度复合AI系统中的长程工作流，缓解全局文本反馈传播导致的梯度爆炸与消失问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于全局文本反馈传播的方法（如TextGrad）在系统深度增加时性能显著下降，存在文本梯度爆炸和消失两大深度扩展失效模式。

Method: 提出Textual Equilibrium Propagation（TEP），包含自由相（本地LLM批评者迭代优化提示至平衡态）和扰动相（以有界强度进行邻近提示编辑，通过前向信号而非反向链式反馈传播任务目标）。

Result: 在长程问答基准和多智能体工具使用数据集上，TEP相比TextGrad等全局传播方法持续提升准确率与效率，且增益随系统深度增长，同时保持黑盒LLM组件的实用性。

Conclusion: TEP通过局部优化与受控适应机制，有效克服了深度复合AI系统中全局文本反向传播的计算开销与信号衰减问题，为可扩展的端到端提示优化提供了新范式。

Abstract: Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.

</details>


### [351] [LOCUS: Low-Dimensional Model Embeddings for Efficient Model Exploration, Comparison, and Selection](https://arxiv.org/abs/2601.21082)
*Shivam Patel,William Cocke,Gauri Joshi*

Main category: cs.LG

TL;DR: LOCUS是一种无需重训练即可生成语言模型能力低维嵌入的方法，并能结合查询编码预测模型正确性，实现高效模型路由与下游应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）生态迅速扩张，导致模型池庞大且动态变化，亟需一种高效、可扩展的模型能力表征与管理方法。

Method: LOCUS采用基于注意力机制的编码器，通过确定性前向传播将查询编码和评估分数映射为低维模型嵌入；同时训练一个基于嵌入与查询编码的正确性预测器用于路由。

Result: LOCUS在嵌入生成中比基线方法最多减少4.8倍的查询评估样本量；其嵌入空间具有几何意义（邻近即相似），支持模型比较、聚类、组合选择及失效模型代理等下游任务；路由准确率达SOTA。

Conclusion: LOCUS提供了一种轻量、可扩展、无需重训练的模型能力表征框架，显著提升模型池管理效率与灵活性，并拓展了嵌入空间的语义与实用价值。

Abstract: The rapidly growing ecosystem of Large Language Models (LLMs) makes it increasingly challenging to manage and utilize the vast and dynamic pool of models effectively. We propose LOCUS, a method that produces low-dimensional vector embeddings that compactly represent a language model's capabilities across queries. LOCUS is an attention-based approach that generates embeddings by a deterministic forward pass over query encodings and evaluation scores via an encoder model, enabling seamless incorporation of new models to the pool and refinement of existing model embeddings without having to perform any retraining. We additionally train a correctness predictor that uses model embeddings and query encodings to achieve state-of-the-art routing accuracy on unseen queries. Experiments show that LOCUS needs up to 4.8x fewer query evaluation samples than baselines to produce informative and robust embeddings. Moreover, the learned embedding space is geometrically meaningful: proximity reflects model similarity, enabling a range of downstream applications including model comparison and clustering, model portfolio selection, and resilient proxies of unavailable models.

</details>


### [352] [Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed](https://arxiv.org/abs/2601.21094)
*Minjae Kwon,Josephine Lamp,Lu Feng*

Main category: cs.LG

TL;DR: 本文研究了安全强化学习算法在糖尿病管理中的部署安全性，发现训练时的安全保证在分布偏移下难以泛化，提出测试时屏蔽机制有效恢复安全性，并在多种算法和患者群体上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习算法通常在固定训练条件下评估，但其在实际部署中面对分布偏移（如不同患者）时的安全性泛化能力尚不明确，亟需在真实安全关键场景中检验。

Method: 在统一临床模拟器GlucoSim上对八种安全RL算法进行基准测试，分析其在不同糖尿病类型与年龄组患者上的安全泛化能力；引入基于学习动力学模型的测试时动作屏蔽机制以恢复安全性。

Result: 发现显著的安全泛化差距：训练中满足约束的策略在未见患者上频繁违反安全要求；测试时屏蔽机制使PPO-Lag、CPO等强基线算法Time-in-Range提升13–14%，同时降低临床风险指数和血糖变异性。

Conclusion: 训练时的安全性不能自然泛化至分布偏移下的部署场景，而测试时屏蔽是一种通用且有效的安全增强手段，为安全关键控制任务提供了可扩展的保障框架。

Abstract: Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.

</details>


### [353] [Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement](https://arxiv.org/abs/2601.21149)
*Maria Despoina Siampou,Shushman Choudhury,Shang-Ling Hsu,Neha Arora,Cyrus Shahabi*

Main category: cs.LG

TL;DR: 本文提出ME-POIs框架，通过融合语言模型的POI文本嵌入与大规模人类移动数据，学习以POI功能为核心的、上下文无关的通用表征，并在多项地图增强任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有POI表征方法主要依赖静态文本元数据或轨迹上下文，忽略了反映真实使用方式的POI功能这一关键信号。

Method: 提出ME-POIs框架：将单次访问建模为时序上下文化嵌入，通过对比学习对齐到可学习的POI表征；并设计多尺度空间邻近POI间的时间访问模式传播机制以缓解长尾稀疏问题。

Result: 在五个新提出的地图增强任务上，ME-POIs显著优于纯文本和纯移动基线；仅用移动数据训练的ME-POIs在部分任务上甚至超越纯文本模型。

Conclusion: POI功能是构建准确、通用POI表征不可或缺的核心要素，ME-POIs成功将其融入表征学习过程。

Abstract: Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.

</details>


### [354] [Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations](https://arxiv.org/abs/2601.21150)
*Henry Kvinge,Andrew Aguilar,Nayda Farnsworth,Grace O'Brien,Robert Jasper,Sarah Scullen,Helen Jenne*

Main category: cs.LG

TL;DR: 本文探讨了AI在数学研究中的应用，超越了传统的问答式范式，通过训练神经网络预测群运算（如模运算或置换组合），检验模型是否能学习到抽象代数结构（如交换性、子群等），实验表明小规模神经网络确实能捕捉部分抽象数学性质，但尚不能提取如单位元等概念。


<details>
  <summary>Details</summary>
Motivation: 现有AI在数学中的应用多局限于考试式问答，而真实数学研究是开放式的、以探索新结构和模式为导向；本文旨在探索AI模型能否在完成特定数学任务时，自发学习并表征更广泛的数学结构。

Method: 以群运算预测（如模运算、置换合成）为任务，训练窄域神经网络，并设计一系列测试来评估模型是否隐含学习到群论概念（如单位元、交换性、子群）；通过线性分类器、表示空间分析等手段检验模型内部表征。

Result: 实验发现模型能捕捉某些抽象代数性质，例如模运算的交换性；可训练线性分类器区分特定子群元素（无子群标签监督）；但无法可靠提取单位元等基本概念。

Conclusion: 即使小型神经网络的表征也蕴含可被提取的有趣抽象数学结构，表明AI有望辅助数学发现，但当前能力仍有限，需进一步研究如何引导模型学习更深层的数学本质。

Abstract: While a real-world research program in mathematics may be guided by a motivating question, the process of mathematical discovery is typically open-ended. Ideally, exploration needed to answer the original question will reveal new structures, patterns, and insights that are valuable in their own right. This contrasts with the exam-style paradigm in which the machine learning community typically applies AI to math. To maximize progress in mathematics using AI, we will need to go beyond simple question answering. With this in mind, we explore the extent to which narrow models trained to solve a fixed mathematical task learn broader mathematical structure that can be extracted by a researcher or other AI system. As a basic test case for this, we use the task of training a neural network to predict a group operation (for example, performing modular arithmetic or composition of permutations). We describe a suite of tests designed to assess whether the model captures significant group-theoretic notions such as the identity element, commutativity, or subgroups. Through extensive experimentation we find evidence that models learn representations capable of capturing abstract algebraic properties. For example, we find hints that models capture the commutativity of modular arithmetic. We are also able to train linear classifiers that reliably distinguish between elements of certain subgroups (even though no labels for these subgroups are included in the data). On the other hand, we are unable to extract notions such as the concept of the identity element. Together, our results suggest that in some cases the representations of even small neural networks can be used to distill interesting abstract structure from new mathematical objects.

</details>


### [355] [AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection](https://arxiv.org/abs/2601.21171)
*Kamal Berahmand,Saman Forouzandeh,Mehrnoush Mohammadi,Parham Moradi,Mahdi Jalili*

Main category: cs.LG

TL;DR: 本文提出AC2L-GAD框架，通过主动反事实对比学习解决图异常检测中标签稀缺与类别极度不平衡问题，兼顾语义一致性与难负样本构建，显著降低计算开销并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 图异常检测面临标签稀缺和极端类别不平衡的挑战，而现有图对比学习方法存在正样本语义不一致和负样本信息量不足的问题。

Method: 提出AC2L-GAD框架，结合信息论驱动的主动选择与反事实生成：主动识别结构复杂节点，生成保持异常特性的正增强样本和具有判别力的正常负样本，并仅对精选子集执行高成本反事实生成。

Result: 在九个基准数据集（含GADBench真实金融交易图）上实验表明，AC2L-GAD性能优于或媲美现有SOTA方法，尤其在属性-结构交互复杂的异常上提升显著；计算开销比全图反事实生成降低约65%。

Conclusion: AC2L-GAD通过引入主动反事实对比机制，在保证检测质量的同时大幅降低计算成本，为无监督图异常检测提供了更高效、鲁棒的新范式。

Abstract: Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.

</details>


### [356] [Rethinking Refinement: Correcting Generative Bias without Noise Injection](https://arxiv.org/abs/2601.21182)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 本文提出了一种无需噪声注入或多步重采样的后处理式偏差校正框架Bi-stage Flow Refinement（BFR），通过潜空间对齐和数据空间增强两种策略，在保持原始ODE轨迹的前提下提升生成样本质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散模型和流模型）在高维场景下常存在系统性偏差，导致样本质量下降，亟需有效且高效的后处理校正方法。

Method: 提出基于流匹配的两阶段流精炼（BFR）框架：第一阶段为潜空间对齐（适用于近似可逆生成器），第二阶段为带轻量增强的数据空间精炼；所有修正均为确定性、单步、不扰动原ODE轨迹。

Result: 在MNIST、CIFAR-10和256×256 FFHQ上验证了FID与覆盖率的一致提升；MNIST上从FID 3.95提升至1.46（SOTA），仅需1次额外函数评估（1-NFE）且保持多样性。

Conclusion: BFR证明了简单、确定性、低开销的后处理精炼能显著缓解生成模型偏差，为高效高质量生成提供了新范式。

Abstract: Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.

</details>


### [357] [A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models](https://arxiv.org/abs/2601.21207)
*Chuan-Shen Hu*

Main category: cs.LG

TL;DR: 本文提出了一种基于细胞层论（cellular sheaf theory）的框架，用于建模和分析图神经网络中节点特征与边权重的局部一致性与调和性，并结合TDA思想扩展为多尺度方法，以刻画几何/拓扑深度学习模型中结构与信号的联合特性。


<details>
  <summary>Details</summary>
Motivation: 现有几何与拓扑深度学习（GDL/TDL）模型中，特征在训练过程中的分布与扩散行为尚缺乏系统分析，这一基础问题亟待探索。

Method: 引入细胞层论建模图结构上的局部特征对齐与一致性；结合拓扑数据分析（TDA）思想，构建多尺度扩展以捕获图模型中的层次化特征交互。

Result: 实现了对GDL/TDL架构的联合表征——既涵盖其底层几何/拓扑结构，也包含定义其上的学习信号；为节点分类、子结构检测、社区发现等任务提供新的理论洞察。

Conclusion: 细胞层论为理解GDL/TDL中特征传播与聚合提供了统一的拓扑视角，多尺度扩展增强了对层级特征关系的刻画能力，推动了该方向的理论深化与可解释性研究。

Abstract: Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.

</details>


### [358] [Temporal Context and Architecture: A Benchmark for Naturalistic EEG Decoding](https://arxiv.org/abs/2601.21215)
*Mehmet Ergezer*

Main category: cs.LG

TL;DR: 本文研究了模型架构与时间上下文在自然化EEG解码中的相互作用，比较了CNN、LSTM、EEGXF、S4和S5五种模型在不同时间长度下的性能，并分析了它们在鲁棒性、参数效率和不确定性校准方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 探究不同模型架构与时间上下文长度对自然化EEG解码性能的影响，并评估其在真实场景（如跨被试、跨频率、分布外任务）下的鲁棒性。

Method: 在HBN电影观看数据集上，对CNN、LSTM、EEGXF、S4和S5五种模型进行基准测试，任务为4分类，段长从8秒到128秒；并开展零样本跨频率迁移、跨任务OOD测试和留一被试泛化实验。

Result: S5在64秒段长下达到98.7%±0.6准确率，参数量仅为CNN的约1/20；S5跨被试泛化更强但OOD下过于自信；EEGXF在频率偏移下更稳健且保守，但分布内校准较差。

Conclusion: 存在实际的效率-鲁棒性权衡：S5适合追求高精度与低参数量的场景；EEGXF更适合强调鲁棒性和保守不确定性估计的应用。

Abstract: We study how model architecture and temporal context interact in naturalistic EEG decoding. Using the HBN movie-watching dataset, we benchmark five architectures, CNN, LSTM, a stabilized Transformer (EEGXF), S4, and S5, on a 4-class task across segment lengths from 8s to 128s. Accuracy improves with longer context: at 64s, S5 reaches 98.7%+/-0.6 and CNN 98.3%+/-0.3, while S5 uses ~20x fewer parameters than CNN. To probe real-world robustness, we evaluate zero-shot cross-frequency shifts, cross-task OOD inputs, and leave-one-subject-out generalization. S5 achieves stronger cross-subject accuracy but makes over-confident errors on OOD tasks; EEGXF is more conservative and stable under frequency shifts, though less calibrated in-distribution. These results reveal a practical efficiency-robustness trade-off: S5 for parameter-efficient peak accuracy; EEGXF when robustness and conservative uncertainty are critical.

</details>


### [359] [Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks](https://arxiv.org/abs/2601.21242)
*Luwei Sun,Dongrui Shen,Jianfe Li,Yulong Zhao,Han Feng*

Main category: cs.LG

TL;DR: 本文提出了一种基于SignReLU激活函数的深度神经网络理论框架，用于近似条件生成建模中的比率型函数（f1/f2），并在扩散模型（如DDPM）中给出了逆向过程估计器及KL风险界，分解了泛化误差为逼近误差与估计误差。


<details>
  <summary>Details</summary>
Motivation: 针对条件生成建模中目标条件密度呈比率形式（f1/f2）的挑战，尤其在基于扩散的生成模型中该比率对应结构化边缘密度，亟需理论支撑。

Method: 利用SignReLU激活函数的分段结构特性，构建深度神经网络逼近比率型函数；在标准正则性假设下，推导L^p(Omega)逼近误差界与收敛速率；针对DDPM设计SignReLU神经估计器，并分析其KL风险分解。

Result: 获得了比率型函数的L^p逼近界与收敛率；为DDPM逆向过程构造了SignReLU估计器；导出了生成分布与真实分布间超额KL风险的上界，并将其分解为逼近误差和估计误差两部分。

Conclusion: 该理论框架为扩散类生成模型的有限样本训练提供了泛化保证，揭示了SignReLU网络在比率密度估计中的有效性与可分析性。

Abstract: Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.

</details>


### [360] [Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences](https://arxiv.org/abs/2601.21246)
*Namkyung Yoon,Sanghong Kim,Hwangnam Kim*

Main category: cs.LG

TL;DR: 本文提出了一种基于峰感知条件生成模型的人工智能判别框架，用于提升干扰条件下GC-MS检测的可靠性。该框架通过突出特征峰、融合化学与溶剂信息生成高质量合成数据，从而增强AI判别模型的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: GC-MS在存在干扰物质时易出现非特异性峰、保留时间偏移和背景噪声增加，导致灵敏度下降和误报，亟需提升其在复杂环境下的测量可靠性。

Method: 提出峰感知条件生成对抗网络（CGAN），引入峰感知机制突出GC-MS特征峰，并将化学与溶剂信息编码至隐向量中，以生成符合实验条件的合成GC-MS信号；利用合成数据训练AI判别模型。

Result: 生成数据与真实数据在余弦相似度和皮尔逊相关系数上均超0.9，有效保持峰数量多样性并显著降低误报率。

Conclusion: 所提峰感知条件生成框架能高效生成高质量GC-MS仿真数据，显著提升AI判别模型在干扰条件下的准确性与稳定性，为有限实测场景下的化学物质识别提供新范式。

Abstract: Gas chromatography-mass spectrometry (GC-MS) is a widely used analytical method for chemical substance detection, but measurement reliability tends to deteriorate in the presence of interfering substances. In particular, interfering substances cause nonspecific peaks, residence time shifts, and increased background noise, resulting in reduced sensitivity and false alarms. To overcome these challenges, in this paper, we propose an artificial intelligence discrimination framework based on a peak-aware conditional generative model to improve the reliability of GC-MS measurements under interference conditions. The framework is learned with a novel peak-aware mechanism that highlights the characteristic peaks of GC-MS data, allowing it to generate important spectral features more faithfully. In addition, chemical and solvent information is encoded in a latent vector embedded with it, allowing a conditional generative adversarial neural network (CGAN) to generate a synthetic GC-MS signal consistent with the experimental conditions. This generates an experimental dataset that assumes indirect substance situations in chemical substance data, where acquisition is limited without conducting real experiments. These data are used for the learning of AI-based GC-MS discrimination models to help in accurate chemical substance discrimination. We conduct various quantitative and qualitative evaluations of the generated simulated data to verify the validity of the proposed framework. We also verify how the generative model improves the performance of the AI discrimination framework. Representatively, the proposed method is shown to consistently achieve cosine similarity and Pearson correlation coefficient values above 0.9 while preserving peak number diversity and reducing false alarms in the discrimination model.

</details>


### [361] [DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher](https://arxiv.org/abs/2601.21283)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为DUET的新型蒸馏式大模型遗忘方法，通过让学生模型模仿受提示引导的教师模型的行为，在高效实现特定知识遗忘的同时，保留通用领域知识，显著提升了遗忘效果与实用性，并大幅提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有大模型遗忘方法存在计算开销大、易灾难性遗忘（调优式）或易受提示移除/逆向工程攻击（上下文式）等问题，亟需兼顾安全性、效率与效果的新方法。

Method: 提出Distilled Unlearning from an Efficient Teacher (DUET)，采用知识蒸馏框架：以一个经提示引导、能拒绝生成不良知识但保持领域能力的教师模型为指导，训练学生模型模仿其行为，实现轻量、鲁棒的遗忘。

Result: 在多个基准和增强评估协议上，DUET在遗忘效果（forgetting）和效用保持（utility preservation）两方面均优于现有最先进方法，且数据效率提升数量级。

Conclusion: DUET成功融合调优式与上下文式遗忘的优势，为构建可信AI提供了一种高效、鲁棒、数据友好的大模型遗忘新范式。

Abstract: LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.

</details>


### [362] [PILD: Physics-Informed Learning via Diffusion](https://arxiv.org/abs/2601.21284)
*Tianyi Zeng,Tianyi Wang,Jiaru Zhang,Zimo Zeng,Feiyang Zhang,Yiming Xu,Sikai Chen,Yajie Zou,Yangyang Wang,Junfeng Jiao,Christian Claudel,Xinbo Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为PILD的物理信息扩散模型框架，通过引入拉普拉斯分布的虚拟残差观测和多层物理条件嵌入模块，在扩散过程中融合物理定律，显著提升了生成结果的准确性、稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽强大但纯数据驱动，难以满足需遵循物理定律的工程与科学实际问题。

Method: 提出Physics-Informed Learning via Diffusion（PILD）框架：1）引入拉普拉斯分布采样的虚拟残差观测监督训练；2）设计条件嵌入模块，在去噪网络多层注入物理信息。

Result: 在车辆轨迹估计、轮胎力预测、达西流与等离子体动力学等多个任务上，PILD显著优于现有物理信息与扩散模型基线，提升精度、稳定性与泛化性。

Conclusion: PILD是一种简洁、模块化且广泛适用的框架，可有效融合扩散建模与第一性原理物理约束，适用于含常微分方程、偏微分方程及代数/不等式约束的问题。

Abstract: Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.

</details>


### [363] [Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation](https://arxiv.org/abs/2601.21285)
*Ruifeng Zhang,Zexi Huang,Zikai Wang,Ke Sun,Bohang Zheng,Zhen Ouyang,Huimin Xie,Phil Shen,Junlin Zhang,Wentao Guo,Qinglei Wang*

Main category: cs.LG

TL;DR: 本文提出了Zenith，一种可扩展且高效的推荐系统排序架构，通过Token Fusion和Token Boost模块处理高维Prime Tokens，在保持低推理延迟的同时提升特征交互建模能力，并在TikTok Live上验证了其显著的线上效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模多粒度特征交互时，往往忽视了高效特征处理与模型容量扩展之间的平衡，导致推理延迟过高，难以满足实际大规模推荐场景需求。

Method: 提出Zenith架构，包含Token Fusion和Token Boost模块，专门处理少量高维Prime Tokens，增强token异质性以改善缩放规律。

Result: 在TikTok Live上线A/B测试中，Zenith提升CTR AUC达+1.05%，Logloss降低1.10%，优质观看会话/用户提升9.93%，优质观看时长/用户提升8.11%。

Conclusion: Zenith在不显著增加推理开销的前提下，实现了更优的特征交互建模与线上指标提升，验证了其作为高效可扩展排序架构的有效性与实用性。

Abstract: Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature interactions, relatively little attention has been paid to efficient feature handling and scaling model capacity without incurring excessive inference latency. In this paper, we address this by presenting Zenith, a scalable and efficient ranking architecture that learns complex feature interactions with minimal runtime overhead. Zenith is designed to handle a few high-dimensional Prime Tokens with Token Fusion and Token Boost modules, which exhibits superior scaling laws compared to other state-of-the-art ranking methods, thanks to its improved token heterogeneity. Its real-world effectiveness is demonstrated by deploying the architecture to TikTok Live, a leading online livestreaming platform that attracts billions of users globally. Our A/B test shows that Zenith achieves +1.05%/-1.10% in online CTR AUC and Logloss, and realizes +9.93% gains in Quality Watch Session / User and +8.11% in Quality Watch Duration / User.

</details>


### [364] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为Physics-Guided Tiny-Mamba Transformer（PG-TMT）的紧凑型三支路编码器模型，用于旋转机械的可靠性中心化预测与健康管理。该模型融合物理先验（如故障阶次带对齐）与数据驱动建模（Tiny-Mamba + 轻量Transformer），并结合极值理论（EVT）校准告警阈值，以在非平稳工况、跨域迁移和严重类别不平衡下实现高精度、低且可控误报率的在线早期故障预警。


<details>
  <summary>Details</summary>
Motivation: 旋转机械的可靠性中心化预测需在非平稳运行条件、跨工况/传感器域偏移及严重类别不平衡下，提供准确、低且可预测误报率的早期预警信号。

Method: 提出PG-TMT三支路编码器：1）深度可分离卷积主干捕获微瞬态；2）Tiny-Mamba状态空间分支建模近线性长程动态；3）轻量局部Transformer编码跨通道共振。引入解析时-频映射，将注意力谱与轴承故障阶次带对齐，生成物理可解释的带对齐分数；采用极值理论（EVT）建模健康分超限，设定目标误报强度的阈值，并辅以双阈值迟滞机制抑制抖动。

Result: 在CWRU、Paderborn、XJTU-SY及工业试点数据集上，PG-TMT在泄漏无损流式协议下取得更高Precision-Recall AUC（不平衡场景主指标）、相当或更优ROC AUC、更短平均检测时间（匹配误报强度下），并具备强跨域泛化能力。

Conclusion: PG-TMT通过物理对齐表征与EVT校准决策规则相结合，实现了可校准、可解释、即插即用的早期预警系统，适用于可靠性导向的预测性维护与健康管理。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [365] [Grounding and Enhancing Informativeness and Utility in Dataset Distillation](https://arxiv.org/abs/2601.21296)
*Shaobo Wang,Yantai Yang,Guo Chen,Peiru Li,Kaixin Li,Yufa Zhou,Zhaorun Chen,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出InfoUtil框架，通过信息量（Informativeness）和效用（Utility）的理论建模与优化，实现更优的数据集蒸馏，在ImageNet-1K上超越SOTA 6.1%。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法多依赖启发式策略，缺乏对原始数据与合成数据间根本关系的理论刻画。

Method: 定义Informativeness（样本内关键信息）与Utility（训练集中关键样本），构建最优蒸馏的数学形式；提出InfoUtil框架：①基于Shapley值的信息量最大化（博弈论视角），②基于梯度范数的效用最大化（选择全局影响大的样本）。

Result: 在ImageNet-1K数据集上使用ResNet-18，性能比此前SOTA提升6.1%。

Conclusion: 从信息与效用双维度建模数据集蒸馏具有理论合理性与实践优越性，InfoUtil为DD提供了可解释、可优化的新范式。

Abstract: Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.

</details>


### [366] [The Surprising Difficulty of Search in Model-Based Reinforcement Learning](https://arxiv.org/abs/2601.21306)
*Wei-Di Chang,Mikael Henaff,Brandon Amos,Gregory Dudek,Scott Fujimoto*

Main category: cs.LG

TL;DR: 本文挑战了模型强化学习中搜索即插即用的传统观点，指出即使模型高度准确，搜索也可能损害性能；关键在于缓解分布偏移，而非提升模型或价值函数的准确性，并据此提出关键技术实现多基准领域的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为长期预测和误差累积是模型强化学习的主要障碍，本文旨在挑战这一观点，并探究搜索在模型强化学习中的真实作用与限制。

Method: 通过实验分析搜索在模型强化学习中的影响，识别导致性能下降的关键因素（如分布偏移），并基于此提出改进搜索有效性的关键技术。

Result: 发现搜索并非即插即用，甚至在模型高度准确时仍可能损害性能；缓解分布偏移比提高模型或价值函数精度更重要；所提方法在多个主流基准领域达到最先进性能。

Conclusion: 搜索的有效性取决于对分布偏移的控制，而非单纯依赖模型精度；应重新审视搜索在模型强化学习中的角色，并聚焦于分布匹配等核心问题。

Abstract: This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.

</details>


### [367] [Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation](https://arxiv.org/abs/2601.21315)
*Seonghwi Kim,Sung Ho Jo,Wooseok Ha,Minwoo Chae*

Main category: cs.LG

TL;DR: 本文提出了一种新的分布鲁棒学习框架，用于解决无监督域自适应（UDA）中目标域数据稀缺和源域存在虚假相关性的问题，通过建模协变量分布与条件标签分布的不确定性，提升模型在目标域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法在目标域标注数据极少或源域存在强虚假相关性时表现不佳，亟需更鲁棒的方法。

Method: 提出一种分布鲁棒学习框架，同时建模协变量分布和条件标签分布的不确定性；设计可与现有UDA方法无缝集成的高效学习算法。

Result: 在多种分布偏移场景下的大量实验表明，该方法显著优于强基线，尤其在目标数据极度稀缺时优势明显。

Conclusion: 所提框架兼具理论合理性与实践通用性，适用于单源与多源UDA场景，有效提升了UDA在现实受限条件下的鲁棒性与实用性。

Abstract: Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.

</details>


### [368] [Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21316)
*Aoyu Pang,Maonan Wang,Zifan Sha,Wenwei Yue,Changle Li,Chung Shue Chen,Man-On Pun*

Main category: cs.LG

TL;DR: 本文提出了一种统一的空地交通协同优化模型与框架（UAGMC），结合深度强化学习与V2X通信，实现动态垂直起降机场选择与空中出租车路径规划，显著降低平均出行时间。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对空地一体化出行系统中乘客最优路径规划的系统性探索，亟需整合低空UAM与地面交通以实现高效门到门出行。

Method: 构建统一优化模型，融合空地交通策略选择，刻画多模态网络动态特性与实时交通及乘客行为；在此基础上设计基于深度强化学习和V2X通信的UAGMC框架，用于动态优化vertiport选择与空中路径。

Result: 实验表明UAGMC相较传统比例分配方法可降低34%的平均出行时间，提升了整体出行效率。

Conclusion: 该工作为通过空地交通协同推动智能城市出行提供了坚实基础，并为多模态交通系统集成优化提供了新思路。

Abstract: Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.

</details>


### [369] [Memorization Control in Diffusion Models from Denoising-centric Perspective](https://arxiv.org/abs/2601.21348)
*Thuy Phuong Vu,Mai Viet Hoang Do,Minhhuy Le,Dinh-Cuong Hoang,Phan Xuan Tan*

Main category: cs.LG

TL;DR: 本文从去噪中心视角研究扩散模型中的记忆化问题，提出一种新的时间步采样策略，通过调整置信区间宽度直接控制记忆化与泛化之间的权衡，实验证明该方法能有效减少记忆化并提升生成数据与训练分布的一致性。


<details>
  <summary>Details</summary>
Motivation: 控制扩散模型的记忆化对需要生成数据紧密匹配训练分布的应用至关重要；现有方法主要关注数据或模型层面的修改，而忽略了去噪过程本身的影响。

Method: 提出一种基于去噪轨迹的时间步采样策略，通过调节置信区间宽度，显式控制学习发生在去噪过程的哪个阶段。

Result: 在图像和1D信号生成任务上的实验表明，将学习重点向后期去噪步骤偏移可一致地减少记忆化，并提升生成分布与训练分布的对齐程度。

Conclusion: 去噪过程中的时间步采样策略对记忆化具有显著影响，所提方法提供了一种通用且有效的记忆化-泛化权衡调控机制。

Abstract: Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.

</details>


### [370] [L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.21349)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 本文提出了一种名为L2R的新型路由框架，通过低秩潜在空间和饱和内积评分（SIPS）来提升MoE模型中路由器的判别力与稳定性，并增强专家表达能力，在语言和视觉任务上均取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型多采用原始高维空间中的线性路由器，易受表征失配、角度集中和尺度敏感打分等问题影响，导致路由判别力弱、专家专业化不稳定。

Method: 提出低秩与Lipschitz约束路由（L2R）：（1）将专家分配映射到共享低秩潜在路由空间；（2）引入饱和内积评分（SIPS）显式控制路由函数的Lipschitz性质；（3）设计参数高效的多锚点路由机制以提升专家表达能力。

Result: 在大规模语言MoE模型和ImageNet上的视觉MoE设置中，L2R显著提升了路由稳定性、专家专业化程度及整体模型性能。

Conclusion: L2R通过协同优化路由空间结构与打分几何，为MoE模型提供了更鲁棒、可扩展且高效的路由范式。

Abstract: Mixture-of-Experts (MoE) models scale neural networks by conditionally activating a small subset of experts, where the router plays a central role in determining expert specialization and overall model performance. However, many modern MoE systems still adopt linear routers in raw high-dimensional representation spaces, where representation mismatch, angular concentration, and scale-sensitive scoring can jointly undermine routing discriminability and stable expert specialization. In this work, we propose Low-rank \& Lipschitz-controlled Routing (L2R), a unified routing framework that reshapes both the routing space and scoring geometry. L2R performs expert assignment in a shared low-rank latent routing space and introduces Saturated Inner-Product Scoring (SIPS) to explicitly control the Lipschitz behavior of routing functions, yielding smoother and more stable routing geometry. In addition, L2R incorporates a parameter-efficient multi-anchor routing mechanism to enhance expert expressiveness. Extensive experiments on a large-scale language MoE model and a vision MoE setting on ImageNet demonstrate that L2R consistently improves routing stability, expert specialization, and overall model performance.

</details>


### [371] [Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving](https://arxiv.org/abs/2601.21351)
*Chendong Song,Meixuan Wang,Hang Zhou,Hong Liang,Yuan Lyu,Zixi Chen,Yuwei Fan,Zijie Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种针对LLM解码的注意力-前馈网络分离（AFD）架构的资源配比分析框架，通过概率工作负载建模，推导出最优Attention/FFN资源配置比，显著降低设备空闲时间并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: AFD架构虽能独立扩展内存与计算资源，但其性能对Attention/FFN的资源配置比高度敏感；错误配比会导致步级阻塞和设备空闲，亟需理论指导进行合理定容。

Method: 构建适用于rA-1F拓扑的可处理解析框架，采用概率工作负载模型刻画非平稳的Attention端任务（受动态上下文长度和随机请求长度影响）与稳定的FFN端任务，进而推导最大化系统平均吞吐量的最优A/F比闭式规则。

Result: 所提闭式规则在多种负载下与仿真结果误差<10%，且一致减少设备空闲时间；经真实trace校准的AFD模拟器验证了该理论的有效性。

Conclusion: 基于非平稳-平稳混合负载建模的解析方法，为AFD等异构计算架构提供了高效、可泛化的资源定容理论基础。

Abstract: Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute resources, its performance is highly sensitive to the Attention/FFN provisioning ratio: mis-sizing induces step-level blocking and costly device idle time. We develop a tractable analytical framework for sizing AFD bundles in an $r$A-$1$F topology, where the key difficulty is that Attention-side work is nonstationary-token context grows and requests are continuously replenished with random lengths-while FFN work is stable given the aggregated batch. Using a probabilistic workload model, we derive closed-form rules for the optimal A/F ratio that maximize average throughput per instance across the system. A trace-calibrated AFD simulator validates the theory: across workloads, the theoretical optimal A/F ratio matches the simulation-optimal within 10%, and consistently reduces idle time.

</details>


### [372] [Sim-MSTNet: sim2real based Multi-task SpatioTemporal Network Traffic Forecasting](https://arxiv.org/abs/2601.21384)
*Hui Ma,Qingzhong Li,Jin Wang,Jie Wu,Shaoyu Dou,Li Feng,Xinjun Pei*

Main category: cs.LG

TL;DR: 本文提出Sim-MSTNet，一种基于sim2real的多任务时空网络流量预测模型，通过仿真器生成合成数据、域随机化缩小仿真与真实数据分布差异，并结合注意力机制和动态损失加权缓解任务不平衡与负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量预测方法在数据稀缺时泛化能力差；多任务学习易受任务不平衡和负迁移影响，尤其在建模多种服务类型时。

Method: 提出Sim-MSTNet：1）利用仿真器生成合成数据缓解数据稀缺；2）采用域随机化与双层优化（样本加权+模型训练）缩小仿真-真实数据分布差距；3）引入注意力机制实现任务间选择性知识共享；4）使用动态损失加权平衡多任务目标。

Result: 在两个开源数据集上的实验表明，Sim-MSTNet持续优于SOTA基线，精度和泛化能力均提升。

Conclusion: Sim-MSTNet有效解决了小样本下多任务网络流量预测中的泛化差、任务不平衡与负迁移问题，验证了sim2real范式在该领域的有效性。

Abstract: Network traffic forecasting plays a crucial role in intelligent network operations, but existing techniques often perform poorly when faced with limited data. Additionally, multi-task learning methods struggle with task imbalance and negative transfer, especially when modeling various service types. To overcome these challenges, we propose Sim-MSTNet, a multi-task spatiotemporal network traffic forecasting model based on the sim2real approach. Our method leverages a simulator to generate synthetic data, effectively addressing the issue of poor generalization caused by data scarcity. By employing a domain randomization technique, we reduce the distributional gap between synthetic and real data through bi-level optimization of both sample weighting and model training. Moreover, Sim-MSTNet incorporates attention-based mechanisms to selectively share knowledge between tasks and applies dynamic loss weighting to balance task objectives. Extensive experiments on two open-source datasets show that Sim-MSTNet consistently outperforms state-of-the-art baselines, achieving enhanced accuracy and generalization.

</details>


### [373] [Intrinsic Reward Policy Optimization for Sparse-Reward Environments](https://arxiv.org/abs/2601.21391)
*Minjae Cho,Huy Trong Tran*

Main category: cs.LG

TL;DR: 本文提出了一种名为内在奖励策略优化（IRPO）的新框架，通过利用多种内在奖励直接优化外部奖励策略，无需预训练子策略，在稀疏奖励环境中提升了性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励环境下，传统探索策略（如噪声注入）效果有限；结合内在奖励的方法存在信用分配不稳定或样本效率低、次优等问题。

Method: 提出IRPO算法，采用代理策略梯度（surrogate policy gradient）作为更有效的学习信号，直接基于多种内在奖励优化外部奖励策略，避免预训练子策略。

Result: 在离散与连续环境中，IRPO相较基线方法展现出更好的性能和样本效率，并对所解优化问题进行了形式化分析。

Conclusion: IRPO为稀疏奖励强化学习提供了一种稳定、高效且无需分层预训练的内在奖励利用新范式。

Abstract: Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.

</details>


### [374] [Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.21418)
*Qian Wan,Ziao Xu,Luona Wei,Xiaoxuan Shen,Jianwen Sun*

Main category: cs.LG

TL;DR: 本文提出Difficulty-aware Policy Optimization (DiPO)，一种基于强化学习的大型推理模型（LRM）训练框架，旨在缓解LRM在简单任务上的'过度思考'问题。DiPO通过让模型自主建模任务难度，并设计难度感知的奖励函数，在减少冗余推理的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在简单任务中因模仿人类深度思考而产生冗余推理（即'过度思考'），现有方法忽视了任务难度感知，难以实现推理资源的有效分配。

Method: 提出Difficulty-aware Policy Optimization (DiPO)框架：1）基于模型自推理的难度建模方法，降低人工标注依赖；2）设计难度信号增强的奖励函数，兼顾推理性能、输出格式并惩罚冗长推理。

Result: 实验表明，DiPO使模型能自发调节推理开销，显著减少冗余token，且未因思维压缩而损失性能。

Conclusion: 任务难度感知对优化LRM推理效率至关重要；DiPO提供了一种无需大量人工标注、兼顾效率与性能的强化学习训练新范式。

Abstract: Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.

</details>


### [375] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: 本文提出SAGE框架，通过序列级信号解耦和非对称自适应动力学，解决OneRec中词汇独立及梯度优化的对称保守性问题，提升冷启动推荐效果与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统（如OneRec）依赖独立词表，维护成本高、扩展性差；其优化方法GBPO存在‘对称保守性’问题，难以兼顾冷启动项更新与噪声环境下的多样性保持。

Method: 提出SAGE（Sequence-level Adaptive Gradient Evolution）统一优化框架：（1）序列级信号解耦——结合几何平均重要性比与解耦多目标优势，消除token级方差；（2）非对称自适应动力学——构建动态梯度流形，对冷启动项施加‘提升因子’实现超线性更新，并引入‘熵感知惩罚’打破信息茧房。

Result: 理论分析与实验表明，SAGE能有效缓解冷启动问题、维持推荐多样性，同时保留GBPO的数值稳定性。

Conclusion: SAGE实现了无需独立词表的开源LLM架构复用，并通过创新的梯度调控机制，在生成式列表推荐中兼顾性能、稳定性和泛化能力。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [376] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: 本文提出HER框架，通过双层思维建模（角色第一人称思考 vs. LLM第三人称推理）、逆向工程构建高质量推理数据、以及基于人类偏好的奖励建模，显著提升了大模型在角色扮演中对内在认知过程的模拟能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演虽能模仿语气与知识，但难以真实模拟角色的‘内在思想’；其瓶颈在于缺乏高质量推理轨迹数据和符合人类偏好的可靠奖励信号。

Method: 提出HER统一框架：1）引入双层思考机制区分角色内省与模型推理；2）通过逆向工程构建推理增强型角色扮演数据；3）设计人类对齐的原则体系与奖励模型；4）在Qwen3-32B上联合使用监督学习与强化学习进行训练。

Result: 在CoSER基准提升30.26，在Minimax Role-Play Bench提升14.97，显著超越Qwen3-32B基线；并开源数据集、原则与模型。

Conclusion: HER有效推动了LLM从行为表征迈向认知模拟，为可信赖、有深度的角色扮演提供了新范式与基础资源。

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [377] [L$^3$: Large Lookup Layers](https://arxiv.org/abs/2601.21461)
*Albert Tseng,Christopher De Sa*

Main category: cs.LG

TL;DR: 本文提出了一种新型稀疏化方法Large Lookup Layer（L³），通过静态token路由和上下文相关的嵌入聚合，在解码层中引入新的稀疏维度，兼顾硬件效率与建模能力，并在语言建模和下游任务中显著优于密集模型和等稀疏MoE模型。


<details>
  <summary>Details</summary>
Motivation: 现有MoE动态硬路由存在硬件效率低、训练不稳定等问题；而传统词嵌入表虽天然稀疏但缺乏上下文信息。本文旨在探索一种兼具高效性与上下文建模能力的新稀疏范式。

Method: 提出Large Lookup Layer（L³），包含两部分：(1) 系统友好的静态token路由架构，支持快速训练与CPU卸载推理；(2) 基于信息论的嵌入分配算法，优化速度与质量权衡。

Result: 在高达2.6B活跃参数的Transformer上验证，L³在语言建模和下游任务中均显著优于同规模密集模型和等稀疏MoE模型。

Conclusion: L³为稀疏语言模型提供了一种新范式，在保持高硬件效率的同时增强上下文建模能力，是嵌入表思想向解码层的有效泛化。

Abstract: Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.

</details>


### [378] [More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)](https://arxiv.org/abs/2601.21522)
*Sagi Meir,Tommer D. Keidar,Noam Levi,Shlomi Reuveni,Barak Hirshberg*

Main category: cs.LG

TL;DR: 本文提出了一种新的LLM查询方法Reset-and-Discard（ReD），以提升在固定预算下的coverage@cost（单位成本覆盖问题数），并能基于pass@k预测节省的尝试次数或推断其幂律指数。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标pass@k在固定预算下无法有效反映LLM对多样问题的实际覆盖能力；需要更合适的指标coverage@cost及相应优化方法。

Method: 提出Reset-and-Discard（ReD）查询策略，通过重置与丢弃机制提升coverage@cost，并支持从pass@k定量预测成本节省或反推幂律指数。

Result: 在HumanEval上实验表明，ReD显著减少了达成目标coverage所需的尝试次数、token量和美元成本，并可高效估计模型推理的幂律特性。

Conclusion: ReD是一种通用、高效且可预测的LLM查询优化方法，提升了单位成本下的问题覆盖效率，并为LLM推理能力量化分析提供了新工具。

Abstract: The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.

</details>


### [379] [Multi-Modal Time Series Prediction via Mixture of Modulated Experts](https://arxiv.org/abs/2601.21547)
*Lige Zhang,Ali Maatouk,Jialin Chen,Leandros Tassiulas,Rex Ying*

Main category: cs.LG

TL;DR: 本文提出Expert Modulation新范式，通过文本信号直接调控专家路由与计算，提升多模态时间序列预测性能，避免了传统token级融合在数据稀缺与时间序列异质性高时的对齐难题。


<details>
  <summary>Details</summary>
Motivation: 现有基于token级融合的多模态时间序列预测方法在高质量时序-文本配对稀缺、且时间序列尺度与特性差异大时，难以实现有效跨模态对齐；同时，多数MoE架构仍依赖此类融合，限制了其潜力。

Method: 提出Expert Modulation：利用文本信号动态调节MoE中专家的路由决策和各专家内部的计算过程，实现文本对时序建模的直接、高效跨模态控制，不依赖共享嵌入空间的token级混合。

Result: 通过理论分析与大量实验验证，该方法在多模态时间序列预测任务上取得显著性能提升。

Conclusion: Expert Modulation是一种更鲁棒、更灵活的多模态时间序列预测新范式，尤其适用于低配对质量与高异质性场景，为MoE与多模态时序建模的结合提供了新思路。

Abstract: Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at https://github.com/BruceZhangReve/MoME

</details>


### [380] [SAL: Selective Adaptive Learning for Backpropagation-Free Training with Sparsification](https://arxiv.org/abs/2601.21561)
*Fanping Liu,Hua Yang,Jiasi Zou*

Main category: cs.LG

TL;DR: 本文提出了一种名为Selective Adaptive Learning (SAL) 的新训练方法，通过选择性参数激活与自适应区域划分，缓解反向传播中权重对称性和梯度干扰问题，并在多个基准和大规模模型上验证了其有效性与生物合理性。


<details>
  <summary>Details</summary>
Motivation: 标准深度学习依赖反向传播（BP），存在生物不可行的权重对称性约束和密集表征中的显著梯度干扰问题。

Method: SAL将参数空间分解为互斥且样本依赖的区域，结合选择性参数激活与自适应区域划分，并采用改进的反馈对齐机制消除显式权重对称性要求。

Result: SAL在10个标准基准上实现有竞争力的收敛速度与分类性能；在深层网络（达128层）和大模型（达10亿参数）中仍保持数值一致性与高精度。

Conclusion: SAL提供了一种受生物启发、可扩展且更符合神经科学原理的替代训练范式，有助于推动可扩展神经网络训练的研究。

Abstract: Standard deep learning relies on Backpropagation (BP), which is constrained by biologically implausible weight symmetry and suffers from significant gradient interference within dense representations. To mitigate these bottlenecks, we propose Selective Adaptive Learning (SAL), a training method that combines selective parameter activation with adaptive area partitioning. Specifically, SAL decomposes the parameter space into mutually exclusive, sample-dependent regions. This decoupling mitigates gradient interference across divergent semantic patterns and addresses explicit weight symmetry requirements through our refined feedback alignment. Empirically, SAL demonstrates competitive convergence rates, leading to improved classification performance across 10 standard benchmarks. Additionally, SAL achieves numerical consistency and competitive accuracy even in deep regimes (up to 128 layers) and large-scale models (up to 1B parameters). Our approach is loosely inspired by biological learning mechanisms, offering a plausible alternative that contributes to the study of scalable neural network training.

</details>


### [381] [Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks](https://arxiv.org/abs/2601.21572)
*Jinhao Li,Yuhao Sun,Zhiyuan Ma,Hao He,Xinche Zhang,Xing Chen,Jin Li,Sen Song*

Main category: cs.LG

TL;DR: 本文提出了一种名为Signal-Adaptive Trust Regions (SATR)的新方法，用于稳定和高效地训练高维、长时序强化学习任务中的循环脉冲神经网络（RSNNs），通过基于信号能量自适应调节KL散度约束的信任区域，并结合二值化实现加速。


<details>
  <summary>Details</summary>
Motivation: RSNNs在能效控制策略中潜力巨大，但其在高维、长时序强化学习任务中的训练仍具挑战性；现有基于群体的无梯度优化因估计梯度方差大，易导致不稳定更新。

Method: 提出SATR——一种分布式的更新规则，通过将KL散度归一化为估计信号能量来约束策略更新的相对变化；自动根据信号强弱扩展或收缩信任区域；针对伯努利连接分布实例化SATR，并引入位集（bitset）实现二值脉冲与二值权重以加速训练。

Result: 在多个高维连续控制基准任务上，SATR在有限种群规模下显著提升训练稳定性，并达到与PPO-LSTM等强基线相当的性能；位集实现大幅降低训练耗时，支持大规模RSNN策略搜索。

Conclusion: SATR为RSNN的可扩展、鲁棒强化学习训练提供了新范式，兼顾理论合理性（信任区域+信号感知）与工程实用性（二值化加速）。

Abstract: Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.

</details>


### [382] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练、无需验证器的推理分布锐化方法，通过 token 级低温度缩放近似全局幂分布，在不依赖外部奖励的前提下，性能媲美甚至超越 GRPO，且推理延迟比 MCMC 降低 10 倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有 RL 后训练提升 LLM 推理能力的效果主要源于分布锐化而非新能力习得；MCMC 采样虽可实现类似锐化且无需奖励，但计算开销过大，缺乏实用性。

Method: 从理论上推导出全局幂分布可由 token 级低温度分布加未来轨迹质量感知的缩放因子近似；据此设计一种自回归式、训练-free、verifier-free 的生成分布锐化算法。

Result: 在数学、问答和代码任务上，于四个 LLM 上验证：该方法不使用外部奖励即达到或超过 one-shot GRPO 性能，且推理延迟比 MCMC 降低超 10 倍。

Conclusion: 分布锐化是提升 LLM 推理性能的关键机制；本文提出的轻量级、理论可解释的锐化方法为替代 RL 后训练与 MCMC 提供了高效可行的新范式。

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [383] [Dynamics Reveals Structure: Challenging the Linear Propagation Assumption](https://arxiv.org/abs/2601.21601)
*Hoyeon Chang,Bálint Mucsányi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 本文探讨了神经网络中一阶参数更新是否能保持逻辑一致性，通过关系代数形式化分析线性传播假设（LPA）的几何限制，发现negation和converse可由张量分解支持，但composition因与negation不兼容而导致特征映射坍缩，揭示了知识编辑、反转诅咒和多跳推理失败的共同结构根源。


<details>
  <summary>Details</summary>
Motivation: 神经网络依赖一阶参数更新进行适应，但此类更新能否保持逻辑一致性尚不清楚；本文旨在探究线性传播假设（LPA）在逻辑操作（否定、逆、复合）下的几何可行性边界。

Method: 采用关系代数建模三类核心关系操作（negation、converse、composition），分别分析其在一阶方向无关传播下的数学约束；对negation和converse证明所需张量分解结构，对composition则通过归约为合取并论证双线性必要性，导出与negation的不兼容性。

Result: negation和converse可在特定张量分解下实现方向无关的一阶传播；而composition必然导致双线性合取，该性质与negation冲突，迫使特征映射退化为常数；从而揭示LPA存在本质结构性缺陷。

Conclusion: LPA无法同时支持否定、逆与复合等基本逻辑操作，其内在结构性限制是知识编辑失败、反转诅咒及多跳推理困难的共同根源；需超越线性传播范式以实现逻辑一致的神经符号整合。

Abstract: Neural networks adapt through first-order parameter updates, yet it remains unclear whether such updates preserve logical coherence. We investigate the geometric limits of the Linear Propagation Assumption (LPA), the premise that local updates coherently propagate to logical consequences. To formalize this, we adopt relation algebra and study three core operations on relations: negation flips truth values, converse swaps argument order, and composition chains relations. For negation and converse, we prove that guaranteeing direction-agnostic first-order propagation necessitates a tensor factorization separating entity-pair context from relation content. However, for composition, we identify a fundamental obstruction. We show that composition reduces to conjunction, and prove that any conjunction well-defined on linear features must be bilinear. Since bilinearity is incompatible with negation, this forces the feature map to collapse. These results suggest that failures in knowledge editing, the reversal curse, and multi-hop reasoning may stem from common structural limitations inherent to the LPA.

</details>


### [384] [Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification](https://arxiv.org/abs/2601.21615)
*Jiaxin Zhang,Yiqi Wang,Siwei Wang,Xihong Yang,Yu Shi,Xinwang Liu,En Zhu*

Main category: cs.LG

TL;DR: 本文提出TTReFT框架，通过在测试时对图神经网络的潜在表示进行微调，而非直接微调模型参数，以缓解分布外（OOD）场景下的性能下降问题。该方法包含不确定性引导的节点选择、低秩表示干预和干预感知的掩码自编码器三个关键创新，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在分布外测试场景下性能显著下降，而现有的测试时训练（TTT）方法（如参数微调PaFT）存在灾难性遗忘问题，限制了其实际应用。

Method: 提出TTReFT框架，将适应目标从模型参数转向潜在表示；具体包括：(1) 不确定性引导的节点选择机制；(2) 保持预训练知识的低秩表示干预；(3) 动态适配节点选择策略的干预感知掩码自编码器。

Result: 在五个基准数据集上的大量实验表明，TTReFT在OOD设置下实现了持续且优越的性能；理论分析也为其提供了保障。

Conclusion: TTReFT确立了表示微调作为图TTT的新范式，兼具理论基础与实用价值，适用于真实场景部署。

Abstract: Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.

</details>


### [385] [HeRo-Q: A General Framework for Stable Low Bit Quantization via Hessian Conditioning](https://arxiv.org/abs/2601.21626)
*Jinhao Zhang Yunquan Zhang,Zicheng yan,Boyang Zhang,Jun Sun,Daning Cheng*

Main category: cs.LG

TL;DR: 本文提出Hessian Robust Quantization (HeRo Q)算法，通过在量化前对权重空间施加可学习的旋转-压缩矩阵，重塑损失曲面以降低Hessian最大特征值，从而提升对量化噪声的鲁棒性；该方法无需修改模型结构、开销极小，且在W4A8和极具挑战性的W3A16设置下均显著优于GPTQ、AWQ和SpinQuant等SOTA方法。


<details>
  <summary>Details</summary>
Motivation: Post Training Quantization (PTQ)常导致'低误差、高损失'现象，根本原因在于大语言模型损失曲面的Hessian矩阵存在少数高曲率方向，对扰动极度敏感。

Method: 提出Hessian Robust Quantization (HeRo Q)算法，在量化前引入轻量级、可学习的旋转-压缩矩阵对权重空间进行变换，联合优化以降低Hessian最大特征值，重塑损失曲面。

Result: 在Llama和Qwen模型上实验表明，HeRo Q在W4A8标准设置及W3A16超低位设置下均显著优于GPTQ、AWQ和SpinQuant；在Llama3 8B上将GSM8K准确率提升至70.15%，并有效避免逻辑崩溃。

Conclusion: HeRo Q是一种无需架构修改、计算开销极小、易于集成到现有PTQ流程中的新方法，通过Hessian感知的权重空间变换显著提升了量化鲁棒性与下游任务性能。

Abstract: Post Training Quantization (PTQ), a mainstream model compression technique, often leads to the paradoxical 'low error, high loss' phenomenon because it focuses solely on minimizing quantization error. The root cause lies in the Hessian matrix of the LLM loss landscape: a few high curvature directions are extremely sensitive to perturbations. To address this, we propose the Hessian Robust Quantization (HeRo Q) algorithm, which applies a lightweight, learnable rotation-compression matrix to the weight space prior to quantization. This joint framework reshapes the loss landscape by reducing the largest Hessian eigenvalue and reducing its max eigenvalue, thereby significantly enhancing robustness to quantization noise. HeRo-Q requires no architectural modifications, incurs negligible computational overhead, and integrates seamlessly into existing PTQ pipelines. Experiments on Llama and Qwen models show that HeRo Q consistently outperforms state of the art methods including GPTQ, AWQ, and SpinQuant not only achieving superior performance under standard W4A8 settings, but also excelling in the highly challenging W3A16 ultra low bit regime, where it boosts GSM8K accuracy on Llama3 8B to 70.15\% and effectively avoids the logical collapse commonly seen in aggressive quantization.

</details>


### [386] [Seg-MoE: Multi-Resolution Segment-wise Mixture-of-Experts for Time Series Forecasting Transformers](https://arxiv.org/abs/2601.21641)
*Evandro S. Ortigossa,Eran Segal*

Main category: cs.LG

TL;DR: 本文提出Seg-MoE，一种面向时间序列预测的段级稀疏MoE架构，通过按连续时间步段而非单个token进行路由，更好捕捉时序局部性与连续性，在多个长时序预测基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoE的时间序列预测模型采用token-wise路由，忽视时间序列固有的局部性和连续性，难以高效建模长期动态；需设计更契合时序结构的稀疏机制。

Method: 提出Seg-MoE：将输入时间序列划分为连续的时间段，对每个段整体进行专家路由和处理，使每个专家能直接建模段内交互；将其嵌入时间序列Transformer中。

Result: 在多个多变量长时序预测基准上，Seg-MoE持续超越密集Transformer及已有token-wise MoE模型，取得SOTA精度；消融实验证实段级路由是性能提升的关键。

Conclusion: 将MoE路由粒度与时间序列内在结构对齐是一种强归纳偏置，为序列数据建模中的条件稀疏架构开辟了新方向。

Abstract: Transformer-based models have recently made significant advances in accurate time-series forecasting, but even these architectures struggle to scale efficiently while capturing long-term temporal dynamics. Mixture-of-Experts (MoE) layers are a proven solution to scaling problems in natural language processing. However, existing MoE approaches for time-series forecasting rely on token-wise routing mechanisms, which may fail to exploit the natural locality and continuity of temporal data. In this work, we introduce Seg-MoE, a sparse MoE design that routes and processes contiguous time-step segments rather than making independent expert decisions. Token segments allow each expert to model intra-segment interactions directly, naturally aligning with inherent temporal patterns. We integrate Seg-MoE layers into a time-series Transformer and evaluate it on multiple multivariate long-term forecasting benchmarks. Seg-MoE consistently achieves state-of-the-art forecasting accuracy across almost all prediction horizons, outperforming both dense Transformers and prior token-wise MoE models. Comprehensive ablation studies confirm that segment-level routing is the key factor driving these gains. Our results show that aligning the MoE routing granularity with the inherent structure of time series provides a powerful, yet previously underexplored, inductive bias, opening new avenues for conditionally sparse architectures in sequential data modeling.

</details>


### [387] [Gauge-invariant representation holonomy](https://arxiv.org/abs/2601.21653)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 本文提出了一种名为'表示全息性（representation holonomy）'的新度量，用于刻画深度网络内部表征沿输入路径变化的几何特性，弥补了CKA、SVCCA等仅关注点对点相似性的不足；该度量具有规范不变性，能反映表征的隐含曲率，并与模型鲁棒性和训练动态密切相关。


<details>
  <summary>Details</summary>
Motivation: 现有相似性度量（如CKA、SVCCA）只捕捉激活集合间的点对点重叠，无法反映表征如何随输入路径变化，因而难以解释模型在扰动或对抗攻击下的不同响应。

Method: 提出‘表示全息性’——一种规范不变的统计量，通过小闭环上的特征平行移动所累积的‘扭转’来量化路径依赖性；估计器采用全局白化固定规范、基于共享子空间和仅旋转Procrustes对齐邻域，并将结果嵌入完整特征空间。

Result: 理论证明其对正交（及仿射，白化后）变换不变，仿射层下为线性零假设，小半径时全息性趋零；实验表明其随环半径增大而增加，能区分CKA下相似但鲁棒性不同的模型，并与对抗/污染鲁棒性相关，且可追踪训练中特征的形成与稳定过程。

Conclusion: 表示全息性是一种实用、可扩展的诊断工具，可用于深入探究学习表征的几何结构，超越传统点对点相似性分析。

Abstract: Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the "twist" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.

</details>


### [388] [SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework](https://arxiv.org/abs/2601.21664)
*Xingyue Zhang,Yuxuan Bao,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SENDAI是一种分层稀疏测量数据同化框架，通过结合仿真先验与学习到的差异校正，在极稀疏传感器观测下重建时空场，显著优于传统和最新基线方法。


<details>
  <summary>Details</summary>
Motivation: 弥合数据丰富训练与观测稀疏部署之间的差距，尤其在目标域存在分布偏移、异质结构和多尺度动态时。

Method: 提出SENDAI框架，结合仿真先验与学习的差异校正，实现从超稀疏传感器观测中重建完整空间状态。

Result: 在MODIS植被指数重建任务中，SSIM较传统基线提升185%，较高频方法提升36%；有效保持诊断相关结构（如地形拓扑、土地覆盖不连续性、空间梯度）。

Conclusion: SENDAI是一种轻量级、可操作性强的稀疏测量重建框架，适用于物理驱动推断、资源受限部署及实时监测控制。

Abstract: Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available training data. We present SENDAI, a hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework that reconstructs full spatial states from hyper sparse sensor observations by combining simulation-derived priors with learned discrepancy corrections. We demonstrate the performance on satellite remote sensing, reconstructing MODIS (Moderate Resolution Imaging Spectroradiometer) derived vegetation index fields across six globally distributed sites. Using seasonal periods as a proxy for domain shift, the framework consistently outperforms established baselines that require substantially denser observations -- SENDAI achieves a maximum SSIM improvement of 185% over traditional baselines and a 36% improvement over recent high-frequency-based methods. These gains are particularly pronounced for landscapes with sharp boundaries and sub-seasonal dynamics; more importantly, the framework effectively preserves diagnostically relevant structures -- such as field topologies, land cover discontinuities, and spatial gradients. By yielding corrections that are more structurally and spectrally separable, the reconstructed fields are better suited for downstream inference of indirectly observed variables. The results therefore highlight a lightweight and operationally viable framework for sparse-measurement reconstruction that is applicable to physically grounded inference, resource-limited deployment, and real-time monitor and control.

</details>


### [389] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文揭示了强化学习中结果层面模式崩溃的根本原因在于期望回报目标函数本身，并提出了一种通过逆概率缩放修正目标函数的方法（IPS-GRPO），在不增加探索启发式或模型复杂度的前提下，有效缓解多模态策略优化中的模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 标准的期望回报最大化方法在多终端解任务中常导致结果层面的模式崩溃，传统归因于探索不足或正则化弱，但作者指出这是目标函数本身的结构性缺陷。

Method: 理论分析揭示期望回报目标中概率乘子导致结果频率被放大，进而引发指数级比率发散；提出逆概率缩放（IPS）修正学习信号，并将其融入Group Relative Policy Optimization（GRPO）框架，形成无需额外模型或结构改动的IPS-GRPO算法。

Result: 在多种推理与分子生成任务上，IPS-GRPO显著降低结果层面模式崩溃，同时保持或超越基线性能。

Conclusion: 修正目标函数比添加探索启发式更能从根本上解决多模态策略优化中的模式崩溃问题，逆概率缩放是一种简洁、普适且有效的解决方案。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [390] [XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision](https://arxiv.org/abs/2601.21688)
*Alexandre Myara,Nicolas Bourriez,Thomas Boyer,Thomas Lemercier,Ihab Bendidi,Auguste Genovesio*

Main category: cs.LG

TL;DR: 本文提出了XFactors，一种弱监督的VAE框架，通过对比学习和KL正则化实现对选定因素的显式解耦与控制，无需对抗训练或辅助分类器，在多个数据集上达到SOTA解耦性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督解耦方法在真实数据上难以恢复语义因子，而监督方法因依赖对抗目标或辅助分类器而不稳定、难扩展。

Method: 基于解耦信息瓶颈视角，将隐空间分解为残差子空间S和K个因子特定子空间T_i；对每个目标因子，用InfoNCE对比损失拉近相同因子值的隐向量、推开不同值对；同时用KL正则化约束S和各T_i服从高斯分布，以组织几何结构并避免对抗训练。

Result: 在多个数据集（含CelebA）上以固定超参取得SOTA解耦指标，子空间中因子对齐稳定，支持通过隐变量替换实现可控因子交换，并验证了随隐变量容量增加的可扩展性。

Conclusion: XFactors提供了一种稳定、可扩展、无需对抗训练的弱监督解耦新范式，兼顾语义可解释性与实际可控性。

Abstract: Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.
  We introduce \textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\mathcal{S}$ and factor-specific subspaces $\mathcal{T}_1,\ldots,\mathcal{T}_K$ and a residual subspace $\mathcal{S}$. Each target factor is encoded in its assigned $\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.
  Across multiple datasets, with constant hyperparameters, \textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.

</details>


### [391] [Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics](https://arxiv.org/abs/2601.21698)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.LG

TL;DR: 本文研究课程学习（curriculum learning）对语言模型预训练的影响，发现其主要作用是稳定各学习阶段内的优化过程，而非改变整体学习轨迹或引入新阶段。


<details>
  <summary>Details</summary>
Motivation: 课程学习改变了预训练数据的顺序，但尚不清楚它是否真正改变了模型的学习轨迹，还是仅仅重新安排了对固定轨迹的数据暴露顺序。

Method: 在三种基于语言学动机的课程（词频、习得年龄、动词变体）和随机顺序下，训练不同规模（14M–410M参数）的Pythia模型，并分析训练动态、梯度噪声、输出头谱饱和度及最终准确率；辅以基于梯度方差控制的理想化理论分析。

Result: 所有顺序下训练均遵循相同的潜在学习阶段序列；课程主要影响阶段内的数据暴露；小模型中随机顺序导致更高梯度噪声与更强谱饱和、更低准确率；课程可缓解这些现象；大模型中差异减弱；理论分析支持难度调控通过降低梯度方差提升优化稳定性。

Conclusion: 课程学习的核心益处在于稳定各学习阶段内的优化过程，而非重塑学习轨迹或催生新阶段；其效果随模型规模增大而减弱。

Abstract: Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.

</details>


### [392] [When does predictive inverse dynamics outperform behavior cloning?](https://arxiv.org/abs/2601.21718)
*Lukas Schäfer,Pallavi Choudhury,Abdelhak Lemkhenter,Chris Lovett,Somjit Nath,Luis França,Matheus Ribeiro Furtado de Mendonça,Alex Lamb,Riashat Islam,Siddhartha Sen,John Langford,Katja Hofmann,Sergio Valcarcel Macua*

Main category: cs.LG

TL;DR: 本文提出PIDM（预测性逆动力学模型）在行为克隆（BC）数据有限时表现更优，并从理论角度解释其优势源于引入了偏差-方差权衡：未来状态预测带来偏差，但以该预测为条件的逆动力学模型可显著降低方差；理论条件和实证结果均表明PIDM具有更低预测误差与更高样本效率。


<details>
  <summary>Details</summary>
Motivation: 行为克隆（BC）在专家演示数据有限时性能不佳，而近期提出的PIDM架构虽表现更好，但其优势机理尚不明确，本文旨在提供理论解释。

Method: 建立PIDM的理论分析框架，推导其偏差-方差分解，给出状态预测器偏差需满足的条件，以保证PIDM相比BC具有更低预测误差和更高样本效率，并在2D导航和3D游戏环境中进行实证验证。

Result: 理论证明了PIDM通过偏差-方差权衡提升性能的机制，并在2D任务中显示PIDM仅需BC 20%-33%的演示样本，在3D视觉复杂环境中也节省超66%样本。

Conclusion: PIDM的优势本质在于可控偏差换取显著方差下降，其理论条件为设计更鲁棒的离线模仿学习方法提供了指导。

Abstract: Behavior cloning (BC) is a practical offline imitation learning method, but it often fails when expert demonstrations are limited. Recent works have introduced a class of architectures named predictive inverse dynamics models (PIDM) that combine a future state predictor with an inverse dynamics model (IDM). While PIDM often outperforms BC, the reasons behind its benefits remain unclear. In this paper, we provide a theoretical explanation: PIDM introduces a bias-variance tradeoff. While predicting the future state introduces bias, conditioning the IDM on the prediction can significantly reduce variance. We establish conditions on the state predictor bias for PIDM to achieve lower prediction error and higher sample efficiency than BC, with the gap widening when additional data sources are available. We validate the theoretical insights empirically in 2D navigation tasks, where BC requires up to five times (three times on average) more demonstrations than PIDM to reach comparable performance; and in a complex 3D environment in a modern video game with high-dimensional visual inputs and stochastic transitions, where BC requires over 66\% more samples than PIDM.

</details>


### [393] [Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle](https://arxiv.org/abs/2601.21739)
*Alberto Fernández-Hernández,Cristian Pérez-Corral,Jose I. Mestre,Manuel F. Dolz,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 本文揭示了Adam优化器中动量参数β₁=β₂时验证分数和训练行为更优的现象，并通过提出'梯度尺度不变性'概念，从理论上证明了该现象的根源。


<details>
  <summary>Details</summary>
Motivation: 尽管Adam已被广泛使用近十年，但其动量参数β₁=β₂时性能更优这一经验事实尚未得到理论解释。

Method: 作者形式化定义了'梯度尺度不变性'，并从理论上证明Adam在一阶意义上具备该性质当且仅当β₁=β₂；同时在多个视觉、语言任务及不同网络结构上进行了实验验证。

Result: 理论证明与实验均表明，当β₁=β₂时，Adam具有梯度尺度不变性，使得梯度缩放对更新的影响更平滑，从而提升训练稳定性和泛化性能。

Conclusion: β₁=β₂不仅是一种经验选择，更是满足梯度尺度不变性的必要条件，为理解Adam行为提供了统一解释，并为未来优化器设计提供了新原则。

Abstract: Adam has been at the core of large-scale training for almost a decade, yet a simple empirical fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy $β_{1}=β_{2}$. Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely tied to a structural property that we refer to as \textit{gradient scale invariance}. We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if $β_{1}=β_{2}$. This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-robust updates. The theory is supported by experiments across vision and language tasks, and across different architectural families, in which rescaling the gradient has a markedly smoother effect on the update when $β_{1}=β_{2}$. Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers.

</details>


### [394] [Temporal Sepsis Modeling: a Fully Interpretable Relational Way](https://arxiv.org/abs/2601.21747)
*Vincent Lemaire,Nédra Meloulli,Pierre Jaquet*

Main category: cs.LG

TL;DR: 本文提出了一种基于关系数据建模的可解释机器学习框架，用于脓毒症早期预测，通过命题化技术将EMR时序数据转化为可解释特征，并结合选择性朴素贝叶斯分类器实现高可解释性预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在脓毒症早期预测中性能良好，但缺乏可解释性且忽略患者潜在亚型。

Method: 将EMR时序数据建模为关系型结构，采用基于经典聚合/选择函数的命题化技术提取可解释特征，再使用选择性朴素贝叶斯分类器进行分类。

Result: 实验验证表明该方法兼具预测有效性与极高的可解释性，支持单变量、全局、局部和反事实四类解释。

Conclusion: 关系建模与命题化为脓毒症预测提供了新范式，在保持性能的同时显著提升临床可解释性与实用性。

Abstract: Sepsis remains one of the most complex and heterogeneous syndromes in intensive care, characterized by diverse physiological trajectories and variable responses to treatment. While deep learning models perform well in the early prediction of sepsis, they often lack interpretability and ignore latent patient sub-phenotypes. In this work, we propose a machine learning framework by opening up a new avenue for addressing this issue: a relational approach. Temporal data from electronic medical records (EMRs) are viewed as multivariate patient logs and represented in a relational data schema. Then, a propositionalisation technique (based on classic aggregation/selection functions from the field of relational data) is applied to construct interpretable features to "flatten" the data. Finally, the flattened data is classified using a selective naive Bayesian classifier. Experimental validation demonstrates the relevance of the suggested approach as well as its extreme interpretability. The interpretation is fourfold: univariate, global, local, and counterfactual.

</details>


### [395] [ECSEL: Explainable Classification via Signomial Equation Learning](https://arxiv.org/abs/2601.21789)
*Adia Lumadjeng,Ilker Birbil,Erman Acar*

Main category: cs.LG

TL;DR: ECSEL是一种可解释的分类方法，通过学习符号形式的signomial方程实现分类与解释一体化，在准确率、效率和可解释性上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 许多符号回归基准问题具有紧凑的signomial结构，因此提出直接学习此类形式化表达以兼顾分类性能与可解释性。

Method: ECSEL直接构建结构化的闭式signomial方程作为分类器和解释器，不依赖黑箱模型。

Result: 在符号回归基准上恢复目标方程的比例更高、计算开销更小；分类准确率媲美主流机器学习模型；具备良好的全局特征行为、决策边界分析和局部归因性质；在电商与欺诈检测案例中揭示数据偏差、支持反事实推理并提供可操作洞见。

Conclusion: ECSEL实现了高精度、高效率与强可解释性的统一，为可信AI提供了新的建模范式。

Abstract: We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.

</details>


### [396] [Effective LoRA Adapter Routing using Task Representations](https://arxiv.org/abs/2601.21795)
*Akash Dhasade,Anne-Marie Kermarrec,Igor Pavlovic,Diana Petrescu,Rafael Pires,Mathis Randl,Martijn de Vos*

Main category: cs.LG

TL;DR: LORAUTER是一种新颖的LoRA适配器路由框架，通过任务嵌入而非适配器特征进行选择与组合，在任务级别实现高效可扩展路由，显著提升性能并具备强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA适配器池规模迅速扩大，但缺乏高效、可扩展的路由机制来为查询选择和组合合适的适配器；同时，多数方法依赖适配器训练数据，泛化性和实用性受限。

Method: 提出LORAUTER框架，利用小验证集构建任务嵌入表示，将查询路由到任务空间而非直接映射到具体适配器，从而实现无需适配器训练数据的、以任务为中心的适配器选择与组合。

Result: 在多任务实验中持续超越基线方法：当存在任务对齐适配器时达到Oracle性能（101.2%）；在未见任务上提升5.2分；且在超大（>1500个）、含噪适配器池中仍保持鲁棒性。

Conclusion: LORAUTER通过任务级路由范式，解决了大规模LoRA适配器池中的高效选择与泛化组合问题，为参数高效微调提供了可扩展、实用的新路径。

Abstract: Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.

</details>


### [397] [MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts](https://arxiv.org/abs/2601.21866)
*Evandro S. Ortigossa,Guy Lutsker,Eran Segal*

Main category: cs.LG

TL;DR: 本文提出MoHETS模型，通过异构专家混合（MoHE）和外生信息融合，提升多变量时间序列的长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列具有复杂的多尺度结构（如全局趋势、局部周期性、非平稳性），传统稀疏MoE方法使用同质MLP专家难以有效建模多样化的时序动态。

Method: 提出MoHETS：1）引入稀疏的异构专家混合（MoHE）层，包含共享深度卷积专家（建模序列连续性）和路由式傅里叶专家（建模局部周期性）；2）通过协变量patch嵌入的交叉注意力增强对非平稳动态的鲁棒性；3）用轻量卷积patch解码器替代线性投影头，提升参数效率与泛化能力。

Result: 在7个多元基准数据集和多个预测步长上达到SOTA，平均MSE较强基线降低12%。

Conclusion: MoHETS通过异构专家分工、外生信息建模和高效解码设计，有效提升了多变量长时序预测的准确性、鲁棒性与泛化性。

Abstract: Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, they typically rely on homogeneous MLP experts that poorly capture the diverse temporal dynamics of time series data. We address these limitations with MoHETS, an encoder-only Transformer that integrates sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks, combining a shared depthwise-convolution expert for sequence-level continuity with routed Fourier-based experts for patch-level periodic structures. MoHETS further improves robustness to non-stationary dynamics by incorporating exogenous information via cross-attention over covariate patch embeddings. Finally, we replace parameter-heavy linear projection heads with a lightweight convolutional patch decoder, improving parameter efficiency, reducing training instability, and allowing a single model to generalize across arbitrary forecast horizons. We validate across seven multivariate benchmarks and multiple horizons, with MoHETS consistently achieving state-of-the-art performance, reducing the average MSE by $12\%$ compared to strong recent baselines, demonstrating effective heterogeneous specialization for long-term forecasting.

</details>


### [398] [Robust Multimodal Representation Learning in Healthcare](https://arxiv.org/abs/2601.21941)
*Xiaoguang Zhu,Linxiao Gong,Lianlong Sun,Yang Liu,Haoyu Wang,Jing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种双流特征去相关框架，利用结构因果分析识别并处理医疗多模态数据中的系统性偏差，通过解耦因果特征与虚假相关性来提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实医疗数据集常含多源系统性偏差，影响多模态表征学习的泛化能力；现有方法侧重融合而忽视偏差特征。

Method: 提出双流特征去相关框架，结合结构因果分析、广义交叉熵损失和互信息最小化，在双流神经网络中实现因果特征与虚假相关性的解耦。

Result: 在MIMIC-IV、eICU和ADNI数据集上实验表明该框架能持续提升性能，且具有模型无关性和可集成性。

Conclusion: 该框架有效缓解医疗多模态学习中的偏差问题，增强模型鲁棒性与临床适用性。

Abstract: Medical multimodal representation learning aims to integrate heterogeneous data into unified patient representations to support clinical outcome prediction. However, real-world medical datasets commonly contain systematic biases from multiple sources, which poses significant challenges for medical multimodal representation learning. Existing approaches typically focus on effective multimodal fusion, neglecting inherent biased features that affect the generalization ability. To address these challenges, we propose a Dual-Stream Feature Decorrelation Framework that identifies and handles the biases through structural causal analysis introduced by latent confounders. Our method employs a causal-biased decorrelation framework with dual-stream neural networks to disentangle causal features from spurious correlations, utilizing generalized cross-entropy loss and mutual information minimization for effective decorrelation. The framework is model-agnostic and can be integrated into existing medical multimodal learning methods. Comprehensive experiments on MIMIC-IV, eICU, and ADNI datasets demonstrate consistent performance improvements.

</details>


### [399] [Geometry of Drifting MDPs with Path-Integral Stability Certificates](https://arxiv.org/abs/2601.21991)
*Zuyuan Zhang,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 本文提出了一种基于同伦路径几何视角的非平稳强化学习新框架，通过建模环境变化的长度-曲率-拐点特征来刻画内在复杂性，并设计了HT-RL和HT-MCTS算法实现自适应跟踪。


<details>
  <summary>Details</summary>
Motivation: 现实中的强化学习环境常是非平稳的（如奖励与动力学漂移、加速、振荡、突变），而现有理论仅粗粒度地衡量变化量，忽视了局部变化模式（如加速度、动作间隙近似）对策略抖动和跟踪误差的关键影响。

Method: 将非平稳折扣MDP建模为可微同伦路径，追踪最优Bellman不动点的运动轨迹，提取长度-曲率-拐点签名；推导求解器无关的路径积分稳定性界与间隙安全可行域；在此基础上设计HT-RL和HT-MCTS两个轻量级在线自适应算法，利用回放数据估计路径特征并动态调节学习/规划强度。

Result: 实验表明，HT-RL和HT-MCTS在跟踪性能与动态遗憾上优于静态基线，尤其在振荡与易切换环境中增益最大。

Conclusion: 几何化建模非平稳性可揭示更精细的内在复杂性结构，所提出的同伦跟踪框架及其算法为鲁棒、自适应的非平稳RL提供了新范式。

Abstract: Real-world reinforcement learning is often \emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \emph{how much} the environment changes, not \emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \textit{Homotopy-Tracking RL (HT-RL)} and \textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.

</details>


### [400] [Latent Adversarial Regularization for Offline Preference Optimization](https://arxiv.org/abs/2601.22083)
*Enyi Jiang,Yibo Jacky Zhang,Yinglun Xu,Andreas Haupt,Nancy Amato,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出GANPO方法，通过对抗学习在潜在空间中对语言模型进行偏好优化，以解决传统基于token级正则化在语义不一致性上的局限性，实验证明其在多种架构和任务上具有稳定提升，并在分布偏移和噪声下提供更鲁棒的结构反馈。


<details>
  <summary>Details</summary>
Motivation: 传统基于token级相似性的偏好优化难以反映语义或行为相似性，导致语言模型优化效果受限。

Method: 提出GANPO方法，利用对抗生成网络（GAN）思想，在策略模型与参考模型的内部表征（latent representations）之间施加正则化，最小化其潜在空间差异；将GANPO作为正则项嵌入现有离线偏好优化目标中。

Result: 在多个模型架构和任务上验证了GANPO带来一致性能提升；相比token级正则化，GANPO在分布偏移和噪声下展现出更强的鲁棒性，同时保持相近下游性能和较低计算开销。

Conclusion: 潜在空间正则化（尤其是GANPO）是一种更符合语言模型语义结构的偏好优化范式，能提供比token级正则化更稳健、更具结构性的反馈。

Abstract: Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.

</details>


### [401] [Value-Based Pre-Training with Downstream Feedback](https://arxiv.org/abs/2601.22108)
*Shuqi Ke,Giulia Fanti*

Main category: cs.LG

TL;DR: 本文提出V-Pretraining方法，利用少量下游任务反馈信息（无需标签）来引导基础模型的自监督预训练，提升下游任务性能并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自监督预训练使用固定代理目标（如下一词预测），可能将计算资源错配到与下游任务无关的能力上，导致效率低下。

Method: V-Pretraining是一种基于价值、模态无关的可控持续预训练方法：由轻量级任务设计者动态选择预训练任务（如数据增强策略），使预训练损失梯度与下游任务梯度对齐，从而引导模型学习相关能力，且不直接使用下游标签更新模型。

Result: 在语言模型（0.5B–7B）上，仅用12%的GSM8K样本作为反馈，推理准确率（GSM8K Pass@1）相对提升达18%；在视觉自监督学习中，ADE20K mIoU提升1.07，NYUv2 RMSE降低，ImageNet线性评估准确率提升，并验证了更高的token效率。

Conclusion: V-Pretraining能以极少量下游反馈高效引导预训练方向，在多个模态和任务上显著提升下游性能与训练效率，为可控、目标导向的基础模型预训练提供了新范式。

Abstract: Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.

</details>


### [402] [StepShield: When, Not Whether to Intervene on Rogue Agents](https://arxiv.org/abs/2601.22136)
*Gloria Felicia,Michael Eniolade,Jinfeng He,Zitha Sasindran,Hemant Kumar,Milan Hussain Angati,Sandeep Bandarupalli*

Main category: cs.LG

TL;DR: 本文提出StepShield，首个评估违规行为检测时机（而非仅是否检测到）的基准，引入三个新时间指标，并揭示LLM裁判在早期干预率上显著优于静态分析器，且早期检测可大幅降低监控成本。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全基准仅报告二元准确率，无法区分早期干预与事后分析，而这一区别对实际安全至关重要。

Method: 构建包含9213条代码代理轨迹的StepShield基准（含1278对标注训练样本和7935条测试轨迹），定义Early Intervention Rate (EIR)、Intervention Gap和Tokens Saved三个新型时间评估指标，并设计级联HybridGuard检测器。

Result: LLM裁判EIR达59%，静态分析器仅26%，标准准确率完全掩盖该差距；HybridGuard使监控成本降低75%，五年预计节省1.08亿美元。

Conclusion: StepShield通过聚焦‘何时检测’而非‘是否检测’，为构建更安全、更具经济可行性的AI代理提供了新评估基础。

Abstract: Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.

</details>


### [403] [PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training](https://arxiv.org/abs/2601.22137)
*Shenghao Yang,Zhichao Wang,Oleg Balabanov,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: PRISM是一种通过自适应多项式逼近与随机草图技术加速矩阵函数（如平方根、正交化）迭代计算的通用框架，无需先验谱信息，适用于GPU，并在Shampoo和Muon优化器中验证了训练加速效果。


<details>
  <summary>Details</summary>
Motivation: 矩阵函数（如平方根、逆根、正交化）在预处理梯度法中至关重要，但传统方法依赖特征分解，效率低；需高效、免特征分解、适配GPU的迭代算法。

Method: 提出PRISM框架：结合自适应多项式拟合与随机草图技术，在每次迭代中通过草图最小二乘拟合当前谱，动态调整多项式 surrogate，用于加速Newton-Schulz类迭代。

Result: PRISM无需显式谱界或奇异值估计，能自动适配演化中的谱；在Shampoo和Muon优化器中集成后实证提升了神经网络训练速度。

Conclusion: PRISM为矩阵函数计算提供了高效、自适应、硬件友好的新范式，显著提升预处理优化器的实际性能。

Abstract: Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.

</details>
