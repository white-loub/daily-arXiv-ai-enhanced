<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 353]
- [cs.CL](#cs.CL) [Total: 199]
- [cs.GR](#cs.GR) [Total: 15]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 295]
- [cs.AI](#cs.AI) [Total: 56]
- [cs.RO](#cs.RO) [Total: 108]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Pathological Truth Bias in Vision-Language Models](https://arxiv.org/abs/2509.22674)
*Yash Thube*

Main category: cs.CV

TL;DR: 本文提出了MATS（多模态真实性空间化审计）工具，用于检测视觉语言模型在面对视觉矛盾陈述时的系统性失败，并引入了SCS和IAR两个指标。实验发现生成式VLM存在一致性差的问题，而对比学习模型更鲁棒，同时通过激活补丁定位了失败原因并提出修复路径。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽然发展迅速，但标准基准测试难以揭示其在真实场景中的系统性缺陷，影响实际可信度，因此需要一种更精细的行为审计方法来评估模型的空间推理与事实一致性能力。

Method: 提出MATS审计框架，包含Spatial Consistency Score (SCS) 和 Incorrect Agreement Rate (IAR) 两个指标，用以衡量模型对视觉矛盾语句的拒绝能力；采用激活补丁技术进行因果归因，定位导致失败的关键模型组件。

Result: 指令调优的生成式VLM（如LLaVA 1.5、QwenVLchat）表现出较低的SCS和较高的IAR，说明其容易接受视觉上矛盾的描述；而对比编码器（如CLIP、SigLIP）则表现更稳健；激活补丁分析表明，生成模型的中后期跨注意力机制是失败的关键所在，对比模型的问题集中在池化投影部分。

Conclusion: MATS提供了一种有效的诊断工具，揭示了不同类型VLM在空间一致性上的显著差异，激活补丁不仅帮助定位问题模块，还为模型修复提供了可行路径，有助于提升VLM在现实应用中的可靠性。

Abstract: Vision Language Models (VLMs) are improving quickly, but standard benchmarks
can hide systematic failures that reduce real world trust. We introduce MATS
(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that
measures whether models reject visually contradicted statements, and two
metrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).
Instruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS
and high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.
Activation patching causally localizes failure loci (mid to late cross
attention for generative models, pooled projection components for contrastive
models) and suggests concrete repair paths.

</details>


### [2] [Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method](https://arxiv.org/abs/2509.22686)
*Shinji Yamashita,Yuma Kinoshita,Hitoshi Kiya*

Main category: cs.CV

TL;DR: 本文提出了一种基于对数极坐标傅里叶变换和互相关最大化的高效算法，能够以亚像素精度联合估计两幅图像之间的尺度和旋转变化。


<details>
  <summary>Details</summary>
Motivation: 传统相位相关方法在处理平移估计上有效，但难以应对由相机缩放或旋转引起的尺度和旋转变化，因此需要一种更精确的图像配准方法。

Method: 该方法结合了对数极坐标下的傅里叶变换与基于辅助函数法的亚像素级互相关最大化策略，实现尺度和旋转的联合估计。

Result: 实验结果表明，所提方法在尺度和旋转估计上的平均误差低于传统的基于离散互相关的傅里叶变换技术。

Conclusion: 该算法在图像配准中能更精确地估计尺度和旋转变化，具有优于传统方法的性能表现。

Abstract: This paper introduces a highly efficient algorithm capable of jointly
estimating scale and rotation between two images with sub-pixel precision.
Image alignment serves as a critical process for spatially registering images
captured from different viewpoints, and finds extensive use in domains such as
medical imaging and computer vision. Traditional phase-correlation techniques
are effective in determining translational shifts; however, they are inadequate
when addressing scale and rotation changes, which often arise due to camera
zooming or rotational movements. In this paper, we propose a novel algorithm
that integrates scale and rotation estimation based on the Fourier transform in
log-polar coordinates with a cross-correlation maximization strategy,
leveraging the auxiliary function method. By incorporating sub-pixel-level
cross-correlation our method enables precise estimation of both scale and
rotation. Experimental results demonstrate that the proposed method achieves
lower mean estimation errors for scale and rotation than conventional Fourier
transform-based techniques that rely on discrete cross-correlation.

</details>


### [3] [Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization](https://arxiv.org/abs/2509.22688)
*Xu Jia*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的框架，结合课程学习和难度感知过滤，提升了多模态大模型在结构化感知任务中的定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言推理中表现优异，但在需要精确局部化和鲁棒性的结构化感知任务上存在困难。

Method: 采用增强的组相对策略优化（GRPO）框架，引入基于课程的数据调度和难度感知过滤机制，以应对稀疏、噪声奖励下的优化问题。

Result: 在自动驾驶基准测试中显著提升了检测精度和鲁棒性，消融实验验证了奖励设计、KL正则化和课程节奏对收敛稳定性和泛化能力的重要性。

Conclusion: 强化学习驱动的优化结合结构化数据课程是实现鲁棒、可解释的多模态检测的可扩展路径。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language reasoning
but often struggle with structured perception tasks requiring precise
localization and robustness. We propose a reinforcement learning framework that
augments Group Relative Policy Optimization (GRPO) with curriculum-based data
scheduling and difficulty-aware filtering. This approach stabilizes
optimization under sparse, noisy rewards and enables progressive adaptation to
complex samples. Evaluations on autonomous driving benchmarks demonstrate
substantial improvements in detection accuracy and robustness. Ablation studies
confirm the importance of reward design, KL regularization, and curriculum
pacing for convergence stability and generalization. Our findings highlight
reinforcement-driven optimization with structured data curricula as a scalable
path toward robust and interpretable multimodal detection.

</details>


### [4] [Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.22689)
*Ha-Hieu Pham,Minh Le,Han Huynh,Nguyen Quoc Khanh Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 提出了一种名为拓扑图一致性（TGC）的半监督语义分割框架，通过图论约束提升病理图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖像素级一致性，易传播噪声伪标签并产生碎片化或拓扑无效的分割结果。

Method: 引入图谱对齐、组件计数和邻接统计等图论约束，在预测图与参考图之间实现拓扑一致性。

Result: 在GlaS和CRAG数据集上达到5-10%标注下的最先进性能，显著缩小了与全监督的差距。

Conclusion: TGC能有效保持全局拓扑结构，提升半监督语义分割的准确性和鲁棒性。

Abstract: Semi-supervised semantic segmentation (SSSS) is vital in computational
pathology, where dense annotations are costly and limited. Existing methods
often rely on pixel-level consistency, which propagates noisy pseudo-labels and
produces fragmented or topologically invalid masks. We propose Topology Graph
Consistency (TGC), a framework that integrates graph-theoretic constraints by
aligning Laplacian spectra, component counts, and adjacency statistics between
prediction graphs and references. This enforces global topology and improves
segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC
achieves state-of-the-art performance under 5-10% supervision and significantly
narrows the gap to full supervision. Code is available at
https://github.com/hieuphamha19/TGC.

</details>


### [5] [A review of Recent Techniques for Person Re-Identification](https://arxiv.org/abs/2509.22690)
*Andrea Asperti,Salvatore Fiorilla,Simone Nardi,Lorenzo Orsini*

Main category: cs.CV

TL;DR: 本文综述了人员重识别（ReID）领域中监督和无监督方法的最新进展，指出监督方法已接近性能瓶颈，而无监督方法在近三年取得显著进展，性能差距正在缩小。


<details>
  <summary>Details</summary>
Motivation: 监督方法依赖大量标注数据，存在标注成本高和可扩展性差的问题，因此研究转向利用未标注数据的无监督ReID方法。

Method: 对监督和无监督人员ReID的重要文献进行系统回顾与分类，分析技术演进趋势，并比较两类方法的性能发展。

Result: 监督ReID已接近性能上限，改进空间有限；无监督ReID近年来表现突出，性能逐渐逼近监督方法，显示出收敛趋势。

Conclusion: 无监督人员ReID具有巨大潜力，未来可能在性能上接近甚至达到监督方法的水平，推动ReID技术向更实用、可扩展的方向发展。

Abstract: Person re-identification (ReId), a crucial task in surveillance, involves
matching individuals across different camera views. The advent of Deep
Learning, especially supervised techniques like Convolutional Neural Networks
and Attention Mechanisms, has significantly enhanced person Re-ID. However, the
success of supervised approaches hinges on vast amounts of annotated data,
posing scalability challenges in data labeling and computational costs. To
address these limitations, recent research has shifted towards unsupervised
person re-identification. Leveraging abundant unlabeled data, unsupervised
methods aim to overcome the need for pairwise labelled data. Although
traditionally trailing behind supervised approaches, unsupervised techniques
have shown promising developments in recent years, signalling a narrowing
performance gap. Motivated by this evolving landscape, our survey pursues two
primary objectives. First, we review and categorize significant publications in
supervised person re-identification, providing an in-depth overview of the
current state-of-the-art and emphasizing little room for further improvement in
this domain. Second, we explore the latest advancements in unsupervised person
re-identification over the past three years, offering insights into emerging
trends and shedding light on the potential convergence of performance between
supervised and unsupervised paradigms. This dual-focus survey aims to
contribute to the evolving narrative of person re-identification, capturing
both the mature landscape of supervised techniques and the promising outcomes
in the realm of unsupervised learning.

</details>


### [6] [Sequential Token Merging: Revisiting Hidden States](https://arxiv.org/abs/2509.22691)
*Yan Wen,Peng Ye,Lin Zhang,Baopu Li,Jiakang Yuan,Yaoxin Yang,Tao Chen*

Main category: cs.CV

TL;DR: 提出Sequential Token Merging (STM) 方法，通过双向最近邻合并和隐藏状态保护，在减少视觉Mamba模型token数量的同时保持高性能，实现最先进的效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了Vision Mambas中的有限方向序列依赖性（LDSD），且其效率受图像分辨率下二次方token增长的限制。

Method: 基于Mamba的选择性扫描机制，提出STM，包括双向最近邻token合并以保留序列依赖性，以及保护类token周围的隐藏状态以增强稳定性。

Result: 实验显示，ViM-Ti在减少20% token时仅下降1.0%准确率，ViM-S在减少40% token时仅下降1.4%，显著优于现有方法。

Conclusion: STM有效利用Mamba的层间收敛特性，将时间遗忘转化为稳定性，在极低复杂度下实现了卓越的性能与效率平衡。

Abstract: Vision Mambas (ViMs) achieve remarkable success with sub-quadratic
complexity, but their efficiency remains constrained by quadratic token scaling
with image resolution. While existing methods address token redundancy, they
overlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a
critical information flow mechanism revealed in our analysis. We further
identify Mamba's selective scan enables gradual information aggregation in
hidden states. Based on these insights, we propose Sequential Token Merging
(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve
sequential dependencies through symmetric spatial aggregation, and 2) Hidden
states protection to stabilize the hidden states around the class token. STM
strategically leverages Mamba's layer-wise loss convergence to convert temporal
forgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%
accuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for
ViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with
minimal complexity, while providing new insights into state-space model
dynamics. Codes will be released soon.

</details>


### [7] [Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2509.22692)
*Le Zhang,Ao Li,Qibin Hou,Ce Zhu,Yonina C. Eldar*

Main category: cs.CV

TL;DR: 本文对超分辨率（SR）技术进行了全面综述，涵盖单图像、视频、立体和光场超分辨率方法，分析了超过150种SISR、近70种VSR及约30种SSR和LFSR方法，提供了方法、数据集、评估协议和复杂性分析，并建立了基于主干结构的分类体系，指出了该领域尚未充分研究的开放问题。


<details>
  <summary>Details</summary>
Motivation: 现有综述多集中于特定子领域，缺乏对超分辨率整体领域的系统性梳理，因此需要一个涵盖广泛方法和技术进展的综合性回顾以指导研究人员。

Method: 对各类超分辨率方法进行系统性文献回顾与分类，按主干网络结构建立分类体系，分析不同方法的技术路线、常用数据集、评估指标及计算复杂度。

Result: 总结了超过150种SISR、近70种VSR以及约30种SSR和LFSR方法，提出了基于主干结构的分类法，识别出若干有价值但研究不足的开放问题，并建立了公开资源库。

Conclusion: 该综述为超分辨率领域提供了全面的技术概览和结构化分类，有助于推动未来研究方向，并为研究人员提供了宝贵的参考资源和开源工具支持。

Abstract: Super-resolution (SR) has garnered significant attention within the computer
vision community, driven by advances in deep learning (DL) techniques and the
growing demand for high-quality visual applications. With the expansion of this
field, numerous surveys have emerged. Most existing surveys focus on specific
domains, lacking a comprehensive overview of this field. Here, we present an
in-depth review of diverse SR methods, encompassing single image
super-resolution (SISR), video super-resolution (VSR), stereo super-resolution
(SSR), and light field super-resolution (LFSR). We extensively cover over 150
SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR
and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical
results, and complexity. In addition, we conducted a taxonomy based on each
backbone structure according to the diverse purposes. We also explore valuable
yet under-studied open issues in the field. We believe that this work will
serve as a valuable resource and offer guidance to researchers in this domain.
To facilitate access to related work, we created a dedicated repository
available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.

</details>


### [8] [Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment](https://arxiv.org/abs/2509.22697)
*Abhiroop Chatterjee,Susmita Ghosh*

Main category: cs.CV

TL;DR: 本文提出了一种用于高光谱图像场景理解的视觉-语言模型优化方法，采用CLIP风格的对比学习框架，通过仅更新0.07%的参数实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像具有高维3D体素结构，传统视觉和语言模型难以有效对齐其跨模态信息，亟需一种高效且轻量化的跨模态对齐方法。

Method: 采用CLIP-style对比训练框架，将视觉骨干网络的体素级嵌入映射到冻结的大嵌入模型（LEM）的潜在空间，并通过可训练探针对齐视觉特征与文本标记表示；引入描述性提示作为HSI嵌入的结构化锚点，并在硬负样本和半硬负样本上施加对比损失以增强对齐效果。

Result: 在Indian Pines数据集上比现有基线提升+0.92总体准确率（OA）和+1.60 Kappa，在Pavia University数据集上提升+0.69 OA和+0.90 Kappa，且参数量仅为DCTN的1/50、SS-TMNet的1/90。

Conclusion: 所提方法以极低的可调参数量实现了高光谱图像跨模态理解的最先进性能，验证了基于对比学习与提示引导的轻量化对齐策略的有效性。

Abstract: As data requirements continue to grow, efficient learning increasingly
depends on the curation and distillation of high-value data rather than
brute-force scaling of model sizes. In the case of a hyperspectral image (HSI),
the challenge is amplified by the high-dimensional 3D voxel structure, where
each spatial location is associated with hundreds of contiguous spectral
channels. While vision and language models have been optimized effectively for
natural image or text tasks, their cross-modal alignment in the hyperspectral
domain remains an open and underexplored problem. In this article, we make an
attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene
understanding by exploiting a CLIP-style contrastive training framework. Our
framework maps voxel-level embeddings from a vision backbone onto the latent
space of a frozen large embedding model (LEM), where a trainable probe aligns
vision features with the model's textual token representations. The two
modalities are aligned via a contrastive loss restricted to a curated set of
hard (closest wrong classes) and semi-hard (random distractors) negatives,
along with positive pairs. To further enhance alignment, descriptive prompts
that encode class semantics are introduced and act as structured anchors for
the HSI embeddings. It is seen that the proposed method updates only 0.07
percent of the total parameters, yet yields state-of-the-art performance. For
example, on Indian Pines (IP) the model produces better results over unimodal
and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa
($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA
and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters,
nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet.

</details>


### [9] [Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning](https://arxiv.org/abs/2509.22700)
*Zhuang Qi,Pan Yu,Lei Meng,Sijin Zhou,Han Yu,Xiaoxiao Li,Xiangxu Meng*

Main category: cs.CV

TL;DR: 提出GPR-NIAM方法，通过非干扰注意力掩码和跨孤岛协同优化，在一次性联邦提示学习中实现更强的跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习方法依赖多轮通信且缺乏跨任务泛化能力，尤其在一次性通信场景下表现不足。

Method: 设计注意力隔离模块以抑制提示令牌对原始文本的过度关注，并通过跨孤岛协同优化模块融合多源视觉知识，进行跨模态对齐以校准全局提示。

Result: 在十个多模态基准数据集上验证，GPR-NIAM在类级和域级泛化方面均优于八种前沿方法。

Conclusion: GPR-NIAM有效提升了联邦提示学习在单次通信下的性能与跨任务泛化能力，缓解了数据异质性带来的影响。

Abstract: Federated Prompt Learning (FPL) enables communication-efficient adaptation by
tuning lightweight prompts on top of frozen pre-trained models. Existing FPL
methods typically rely on global information, which is only available after the
second training round, to facilitate collaboration among client models.
Therefore, they are inherently dependent on multi-round communication to fully
exhibit their strengths. Moreover, existing one-shot federated learning methods
typically focus on fitting seen tasks, but lack cross-task generalization. To
bridge this gap, we propose the Global Prompt Refinement with Non-Interfering
Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to
design a masking mechanism that restricts excessive interaction between the
original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves
this through the collaboration of two key modules. Firstly, the attention
isolation module suppresses attention from the learnable prompt tokens to the
original text tokens, and reweights the reverse attention which preserves
generalization across tasks. Secondly, the cross-silo collaborative refinement
module integrates decentralized visual knowledge into a unified base and
calibrates the global prompt through multi-source cross-modal knowledge
alignment, further mitigating the inconsistency caused by data heterogeneity.
Extensive experiments conducted on ten benchmark datasets under two tasks show
that GPR-NIAM outperforms eight state-of-the-art methods in both class-level
and domain-level generalization.

</details>


### [10] [GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot](https://arxiv.org/abs/2509.22708)
*Ahed Alboody*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mixture-of-Experts的生成式零样本学习模型GZSL-MoE，用于3D点云语义分割，特别是在人类-机器人协作环境下的COVERED数据集上，通过结合MoE与生成模型提升对可见和不可见类别的分割性能。


<details>
  <summary>Details</summary>
Motivation: 在3D点云语义分割中，难以获取所有类别物体的完整训练数据，因此需要一种能够在没有标注数据的情况下识别不可见类别的方法。

Method: 将Mixture-of-Experts（MoE）引入生成式零样本学习框架，在生成器和判别器中使用MoE层，生成更接近真实特征的伪特征，并利用预训练KPConv提取可见类特征进行对抗训练。

Result: GZSL-MoE在COVERED数据集上显著提升了对可见类和不可见类的语义分割性能，验证了其在复杂3D环境中应用的潜力。

Conclusion: 结合MoE机制的GZSL模型能有效增强3D点云语义分割中的零样本学习能力，为缺乏全面标注数据的实际场景提供了可行解决方案。

Abstract: Generative Zero-Shot Learning approach (GZSL) has demonstrated significant
potential in 3D point cloud semantic segmentation tasks. GZSL leverages
generative models like GANs or VAEs to synthesize realistic features (real
features) of unseen classes. This allows the model to label unseen classes
during testing, despite being trained only on seen classes. In this context, we
introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts
(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to
generate fake features that closely resemble real features extracted using a
pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main
contribution of this paper is the integration of Mixture-of-Experts into the
Generator and Discriminator components of the Generative Zero-Shot Learning
model for 3D point cloud semantic segmentation, applied to the COVERED dataset
(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)
environments. By combining the Generative Zero-Shot Learning model with
Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides
a promising solution for understanding complex 3D environments, especially when
comprehensive training data for all object classes is unavailable. The
performance evaluation of the GZSL-MoE model highlights its ability to enhance
performance on both seen and unseen classes. Keywords Generalized Zero-Shot
Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot
Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,
Mixture-of Experts

</details>


### [11] [IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism](https://arxiv.org/abs/2509.22719)
*Adithya Giri*

Main category: cs.CV

TL;DR: 提出了具有归纳偏置的图像Transformer（IBiT），通过引入学习到的掩码，在小数据集上显著提升Vision Transformer的性能，同时保持其可解释性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer缺乏CNN的归纳偏置，导致在小数据集上表现不佳，本文旨在解决这一问题。

Method: 通过引入基于学习的掩码将卷积的归纳偏置引入Vision Transformer，提出Inductively Biased Image Transformers (IBiT)。

Result: IBiT在小数据集上显著优于标准Vision Transformer，且无需知识蒸馏，同时保持了模型的可解释性。

Conclusion: 引入归纳偏置可以有效提升Vision Transformer在小规模数据上的学习能力，IBiT为数据有限场景下的Transformer应用提供了新思路。

Abstract: In recent years, Transformer-based architectures have become the dominant
method for Computer Vision applications. While Transformers are explainable and
scale well with dataset size, they lack the inductive biases of Convolutional
Neural Networks. While these biases may be learned on large datasets, we show
that introducing these inductive biases through learned masks allow Vision
Transformers to learn on much smaller datasets without Knowledge Distillation.
These Transformers, which we call Inductively Biased Image Transformers (IBiT),
are significantly more accurate on small datasets, while retaining the
explainability Transformers.

</details>


### [12] [LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning](https://arxiv.org/abs/2509.22720)
*Zezhong Fan,Xiaohan Li,Luyi Ma,Kai Zhao,Liang Peng,Topojoy Biswas,Evren Korpeoglu,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: 本文提出了一种名为LayoutAgent的智能体框架，结合视觉-语言推理与组合扩散模型，用于生成语义合理且空间上逼真的多物体场景布局。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏显式空间推理能力，导致生成的物体布局不真实；而传统空间规划方法难以捕捉视觉场景的语义丰富性。因此需要一种能同时兼顾语义关系与几何合理性的新方法。

Method: 首先使用视觉语言模型对输入图像进行分割、物体尺寸估计、场景图构建和提示重写；然后利用组合扩散方法根据场景图中的对象关系生成符合空间逻辑的边界框；最后通过前景条件图像生成器将物体渲染到规划好的布局中形成完整场景。

Result: 实验表明，LayoutAgent在布局连贯性、空间真实感和美学对齐方面优于现有的最先进布局生成模型。

Conclusion: LayoutAgent有效融合了视觉-语言理解与机器人领域的空间规划技术，实现了更真实、语义一致的多物体场景布局生成。

Abstract: Designing realistic multi-object scenes requires not only generating images,
but also planning spatial layouts that respect semantic relations and physical
plausibility. On one hand, while recent advances in diffusion models have
enabled high-quality image generation, they lack explicit spatial reasoning,
leading to unrealistic object layouts. On the other hand, traditional spatial
planning methods in robotics emphasize geometric and relational consistency,
but they struggle to capture semantic richness in visual scenes. To bridge this
gap, in this paper, we propose LayoutAgent, an agentic framework that unifies
vision-language reasoning with compositional diffusion for layout generation.
Given multiple input images with target objects in them, our method first
employs visual-language model to preprocess the inputs through segmentation,
object size estimation, scene graph construction, and prompt rewriting. Then we
leverage compositional diffusion-a method traditionally used in robotics-to
synthesize bounding boxes that respect object relations encoded in the scene
graph for spatial layouts. In the end, a foreground-conditioned image generator
composes the complete scene by rendering the objects into the planned layout
guided by designed prompts. Experiments demonstrate that LayoutAgent
outperforms other state-of-the-art layout generation models in layout
coherence, spatial realism and aesthetic alignment.

</details>


### [13] [CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.22737)
*Jie Cai,Kangning Yang,Lan Fu,Jiaming Ding,Jinlong Li,Huiming Sun,Daitao Xing,Jinglin Shen,Zibo Meng*

Main category: cs.CV

TL;DR: CompareBench是一个用于评估视觉-语言模型视觉比较推理能力的新基准，揭示了现有模型在时间、空间、数量和几何比较任务中的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 视觉比较推理是视觉-语言模型的一项基本但研究不足的能力，当前模型在此类任务上的表现缺乏系统评估。

Method: 构建包含1000个问答对的CompareBench基准，涵盖数量、时间、几何和空间四类任务，并基于TallyBench和HistCaps两个辅助数据集进行评估，测试多个闭源和开源模型的表现。

Result: 实验显示现有模型在时间排序和空间关系判断上表现差，常在基础计数和几何比较中出错，显示出明显的系统盲点。

Conclusion: 视觉比较推理仍是当前视觉-语言模型的薄弱环节，CompareBench为改进多模态推理提供了可控、多样且具有诊断性的评估基础。

Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison
reasoning in vision-language models (VLMs), a fundamental yet understudied
skill. CompareBench consists of 1000 QA pairs across four tasks: quantity
(600), temporal (100), geometric (200), and spatial (100). It is derived from
two auxiliary datasets that we constructed: TallyBench (2000 counting images
with QA) and HistCaps (515 historical images with bilingual captions). We
evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source
models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but
also reveal critical limitations: even the strongest models consistently fail
at temporal ordering and spatial relations, and they often make mistakes in
basic counting and geometric comparisons that are trivial for humans. These
findings demonstrate that visual comparison remains a systematic blind spot for
current VLMs. By providing controlled, diverse, and diagnostic evaluation,
CompareBench establishes a foundation for advancing more reliable multimodal
reasoning.

</details>


### [14] [MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning](https://arxiv.org/abs/2509.22761)
*Yapeng Mi,Hengli Li,Yanpeng Zhao,Chenxi Li,Huimin Wu,Xiaojian Ma,Song-Chun Zhu,Ying Nian Wu,Qing Li*

Main category: cs.CV

TL;DR: 提出MILR方法，一种在测试时联合对图像和文本进行推理的跨模态图像生成方法，在多个基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的图像生成方法受限于单模态推理或依赖高质量推理数据进行微调，难以实现有效的跨模态联合推理。

Method: 在统一的潜在向量空间中，通过离散图像和文本token的向量表示进行搜索式推理，使用策略梯度方法并由图像质量critic引导，在MUG框架内实现测试时联合推理。

Result: 在GenEval、T2I-CompBench和WISE上均取得最先进的结果，尤其在知识密集型WISE上整体得分0.63，比基线提升80%；定性分析显示其具备时间与文化推理能力。

Conclusion: 统一潜在空间中的联合跨模态推理是提升图像生成性能的关键，MILR在无需训练的情况下显著增强生成质量与语义一致性。

Abstract: Reasoning-augmented machine learning systems have shown improved performance
in various domains, including image generation. However, existing
reasoning-based methods for image generation either restrict reasoning to a
single modality (image or text) or rely on high-quality reasoning data for
fine-tuning. To tackle these limitations, we propose MILR, a test-time method
that jointly reasons over image and text in a unified latent vector space.
Reasoning in MILR is performed by searching through vector representations of
discrete image and text tokens. Practically, this is implemented via the policy
gradient method, guided by an image quality critic. We instantiate MILR within
the unified multimodal understanding and generation (MUG) framework that
natively supports language reasoning before image synthesis and thus
facilitates cross-modal reasoning. The intermediate model outputs, which are to
be optimized, serve as the unified latent space, enabling MILR to operate
entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE,
achieving state-of-the-art results on all benchmarks. Notably, on
knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over
the baseline by 80%. Our further analysis indicates that joint reasoning in the
unified latent space is the key to its strong performance. Moreover, our
qualitative studies reveal MILR's non-trivial ability in temporal and cultural
reasoning, highlighting the efficacy of our reasoning method.

</details>


### [15] [UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation](https://arxiv.org/abs/2509.22763)
*Tangqi Shi,Pietro Lio*

Main category: cs.CV

TL;DR: 提出UESA-Net，一种具有多方向收缩注意力的U形网络，用于提升乳腺和甲状腺超声图像中病灶分割的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡超声图像中的全局语义与局部细节，且受斑点噪声和结构重叠影响，导致分割性能受限。

Method: 设计了一种编码器-解码器结构的UESA-Net，在编码器中引入沿水平、垂直和深度方向的注意力模块，并结合收缩策略融合先验知识；解码器采用成对收缩机制，整合低层物理线索与编码器特征以增强上下文建模。

Result: 在TN3K和BUSI两个公开数据集上，UESA-Net分别取得了0.8487和0.6495的IoU分数，达到当前最优性能。

Conclusion: UESA-Net能有效融合多方向空间信息与先验知识，在乳腺和甲状腺超声图像分割任务中表现出优于现有方法的性能，具有良好的鲁棒性和应用潜力。

Abstract: Background: Breast and thyroid cancers pose an increasing public-health
burden. Ultrasound imaging is a cost-effective, real-time modality for lesion
detection and segmentation, yet suffers from speckle noise, overlapping
structures, and weak global-local feature interactions. Existing networks
struggle to reconcile high-level semantics with low-level spatial details. We
aim to develop a segmentation framework that bridges the semantic gap between
global context and local detail in noisy ultrasound images.
  Methods: We propose UESA-Net, a U-shaped network with multidirectional
shrinkage attention. The encoder-decoder architecture captures long-range
dependencies and fine-grained structures of lesions. Within each encoding
block, attention modules operate along horizontal, vertical, and depth
directions to exploit spatial details, while a shrinkage (threshold) strategy
integrates prior knowledge and local features. The decoder mirrors the encoder
but applies a pairwise shrinkage mechanism, combining prior low-level physical
cues with corresponding encoder features to enhance context modeling.
  Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) -
UESA-Net achieved state-of-the-art performance with intersection-over-union
(IoU) scores of 0.8487 and 0.6495, respectively.
  Conclusions: UESA-Net effectively aggregates multidirectional spatial
information and prior knowledge to improve robustness and accuracy in breast
and thyroid ultrasound segmentation, demonstrating superior performance to
existing methods on multiple benchmarks.

</details>


### [16] [UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation](https://arxiv.org/abs/2509.25079)
*Guanjun Wu,Jiemin Fang,Chen Yang,Sikuang Li,Taoran Yi,Jia Lu,Zanwei Zhou,Jiazhong Cen,Lingxi Xie,Xiaopeng Zhang,Wei Wei,Wenyu Liu,Xinggang Wang,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniLat3D的统一框架，通过将几何和外观编码到单一潜在空间中，实现高质量3D资产的单阶段生成，显著提升了生成效率和几何-纹理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成模型多采用两阶段扩散方法，存在几何与纹理错位及计算成本高的问题，因此需要一种更高效、一体化的生成方案。

Method: 提出统一的VAE（Unified VAE）将几何与外观信息压缩为紧凑的潜在表示UniLat，并基于此使用单一流匹配模型直接从噪声生成UniLat，实现端到端单阶段3D生成。

Result: 在公开数据集上训练的UniLat3D可在数秒内从单张图像生成高质量3D资产，在几何质量和外观保真度方面优于现有方法。

Conclusion: UniLat3D通过统一的潜在空间实现了高效、高质量的单阶段3D生成，解决了传统两阶段方法中的对齐问题和冗余计算，具有良好的实用性和扩展性。

Abstract: High-fidelity 3D asset generation is crucial for various industries. While
recent 3D pretrained models show strong capability in producing realistic
content, most are built upon diffusion models and follow a two-stage pipeline
that first generates geometry and then synthesizes appearance. Such a decoupled
design tends to produce geometry-texture misalignment and non-negligible cost.
In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation. Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat. UniLat integrates structural and visual information into a dense
low-resolution latent, which can be efficiently decoded into diverse 3D
formats, e.g., 3D Gaussians and meshes. Based on this unified representation,
we train a single flow-matching model to map Gaussian noise directly into
UniLat, eliminating redundant stages. Trained solely on public datasets,
UniLat3D produces high-quality 3D assets in seconds from a single image,
achieving superior appearance fidelity and geometric quality. More demos \&
code are available at https://unilat3d.github.io/

</details>


### [17] [PartCo: Part-Level Correspondence Priors Enhance Category Discovery](https://arxiv.org/abs/2509.22769)
*Fernando Julio Cendra,Kai Han*

Main category: cs.CV

TL;DR: 本文提出了PartCo，一种基于部件级对应先验的广义类别发现（GCD）新框架，通过引入部件级视觉特征对应关系，增强了对细粒度语义结构的捕捉能力，显著提升了现有GCD方法的性能，实现了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有的GCD方法主要依赖语义标签和全局图像表示，忽略了对区分相似类别至关重要的部件级细节线索，因此需要一种能够利用更精细视觉结构的方法来提升类别发现效果。

Method: 提出PartCo框架，通过建模部件级视觉特征的对应关系，捕捉更细粒度的语义结构，并可无缝集成到现有GCD方法中，无需大幅修改。

Result: 在多个基准数据集上的实验表明，PartCo显著提升了现有GCD方法的性能，弥合了语义标签与部件级视觉组成之间的差距，取得了当前最优的结果。

Conclusion: PartCo通过引入部件级对应先验，有效增强了广义类别发现中的细粒度识别能力，为GCD任务设定了新的标杆，并具有良好的通用性和扩展性。

Abstract: Generalized Category Discovery (GCD) aims to identify both known and novel
categories within unlabeled data by leveraging a set of labeled examples from
known categories. Existing GCD methods primarily depend on semantic labels and
global image representations, often overlooking the detailed part-level cues
that are crucial for distinguishing closely related categories. In this paper,
we introduce PartCo, short for Part-Level Correspondence Prior, a novel
framework that enhances category discovery by incorporating part-level visual
feature correspondences. By leveraging part-level relationships, PartCo
captures finer-grained semantic structures, enabling a more nuanced
understanding of category relationships. Importantly, PartCo seamlessly
integrates with existing GCD methods without requiring significant
modifications. Our extensive experiments on multiple benchmark datasets
demonstrate that PartCo significantly improves the performance of current GCD
approaches, achieving state-of-the-art results by bridging the gap between
semantic labels and part-level visual compositions, thereby setting new
benchmarks for GCD. Project page: https://visual-ai.github.io/partco

</details>


### [18] [DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models](https://arxiv.org/abs/2509.22793)
*Komal Kumar,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Ivan Laptev,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出了一种名为DEFT的高效微调框架，通过将预训练文本到图像模型的权重更新分解为两个低秩矩阵组件，在保持指令遵循能力和可编辑性的同时，实现了在多种任务上的优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的高效微调方法在概念个性化、任务统一和生成可控性之间难以平衡，尤其在参数量少、计算资源有限的情况下表现受限。因此，需要一种既能适应新概念又能保留原始模型能力的微调方法。

Method: DEFT将权重更新分解为两个部分：一个投影到由低秩矩阵张成子空间的正交补空间，另一个是在该子空间内的低秩更新。通过两个可训练的低秩矩阵分别控制子空间和子空间内的适应，从而实现高效的参数调整。

Result: 在Dreambooth、Dreambench Plus、InsDet和VisualCloze等多个数据集上验证了DEFT的有效性，在个性化生成、对象场景适应和视觉上下文学习任务中均达到或优于现有方法的表现，同时保持良好的提示对齐和编辑能力。

Conclusion: DEFT提供了一种通用且高效的微调范式，能够在极小参数开销下实现强大的适应能力与模型可控性，展现出高效微调的涌现特性。

Abstract: Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves
adjusting the model to suit a particular task or dataset while minimizing
computational resources and limiting the number of trainable parameters.
However, it often faces challenges in striking a trade-off between aligning
with the target distribution: learning a novel concept from a limited image for
personalization and retaining the instruction ability needed for unifying
multiple tasks, all while maintaining editability (aligning with a variety of
prompts or in-context generation). In this work, we introduce DEFT,
Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that
adapts a pre-trained weight matrix by decomposing its update into two
components with two trainable matrices: (1) a projection onto the complement of
a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.
The single trainable low-rank matrix defines the subspace, while the other
trainable low-rank matrix enables flexible parameter adaptation within that
subspace. We conducted extensive experiments on the Dreambooth and Dreambench
Plus datasets for personalization, the InsDet dataset for object and scene
adaptation, and the VisualCloze dataset for a universal image generation
framework through visual in-context learning with both Stable Diffusion and a
unified model. Our results demonstrated state-of-the-art performance,
highlighting the emergent properties of efficient fine-tuning. Our code is
available on \href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.

</details>


### [19] [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
*Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang,Wenhao Huang,Wenhu Chen*

Main category: cs.CV

TL;DR: 本文提出了VideoScore2，一个多维度、可解释且与人类评价对齐的视频生成评估框架，能够细粒度地评估视觉质量、文本-视频对齐以及物理/常识一致性，并通过大规模标注数据和强化学习方法提升分析鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评估方法多依赖单一评分，缺乏可解释性和细粒度分析，难以全面衡量视频质量。因此需要一个能提供多维度、可解释评估并支持可控生成的框架。

Method: 提出VideoScore2框架，采用两阶段训练：先在包含27,168个带评分和推理链的人工标注视频的大规模数据集VideoFeedback2上进行监督微调，再使用组相对策略优化（GRPO）进行强化学习，以增强分析能力。

Result: 在领域内基准VideoScore-Bench-v2上达到44.35的准确率（+5.94），在四个跨领域基准上平均得分为50.37（+4.32），显著优于现有方法，并能生成可解释的评估结果，支持Best-of-N采样中的奖励建模。

Conclusion: VideoScore2实现了多维度、可解释且与人类对齐的视频生成评估，在性能和实用性上均优于现有方法，为视频生成模型的评估与优化提供了有效工具。

Abstract: Recent advances in text-to-video generation have produced increasingly
realistic and diverse content, yet evaluating such videos remains a fundamental
challenge due to their multi-faceted nature encompassing visual quality,
semantic alignment, and physical consistency. Existing evaluators and reward
models are limited to single opaque scores, lack interpretability, or provide
only coarse analysis, making them insufficient for capturing the comprehensive
nature of video quality assessment. We present VideoScore2, a
multi-dimensional, interpretable, and human-aligned framework that explicitly
evaluates visual quality, text-to-video alignment, and physical/common-sense
consistency while producing detailed chain-of-thought rationales. Our model is
trained on a large-scale dataset VideoFeedback2 containing 27,168
human-annotated videos with both scores and reasoning traces across three
dimensions, using a two-stage pipeline of supervised fine-tuning followed by
reinforcement learning with Group Relative Policy Optimization (GRPO) to
enhance analytical robustness. Extensive experiments demonstrate that
VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our
in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance
across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),
while providing interpretable assessments that bridge the gap between
evaluation and controllable generation through effective reward modeling for
Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/

</details>


### [20] [TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses](https://arxiv.org/abs/2509.22813)
*Sahar Dastani,Ali Bahri,Gustavo Adolfo Vargas Hakim,Moslem Yazdanpanah,Mehrdad Noori,David Osowiechi,Samuel Barbeau,Ismail Ben Ayed,Herve Lombaert,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出TRUST，一种基于不确定性引导的SSM遍历的测试时适应方法，利用多种遍历顺序生成输入图像的多重视角，提升状态空间模型在分布偏移下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSM）在视觉任务中表现高效，但在分布偏移下泛化性能显著下降，现有测试时适应方法未能充分利用SSM的结构特性。

Method: 提出TRUST方法，通过多样化的遍历顺序生成多个因果视角，使用模型预测作为伪标签更新Mamba特有参数，并对多次遍历的适应权重进行平均融合。

Result: 在七个基准上实验表明，TRUST consistently 提升了模型鲁棒性，并优于现有的TTA方法。

Conclusion: TRUST是首个显式利用SSM架构特性进行测试时适应的方法，有效提升了SSM在分布偏移下的泛化能力。

Abstract: State Space Models (SSMs) have emerged as efficient alternatives to Vision
Transformers (ViTs), with VMamba standing out as a pioneering architecture
designed for vision tasks. However, their generalization performance degrades
significantly under distribution shifts. To address this limitation, we propose
TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel
test-time adaptation (TTA) method that leverages diverse traversal permutations
to generate multiple causal perspectives of the input image. Model predictions
serve as pseudo-labels to guide updates of the Mamba-specific parameters, and
the adapted weights are averaged to integrate the learned information across
traversal scans. Altogether, TRUST is the first approach that explicitly
leverages the unique architectural properties of SSMs for adaptation.
Experiments on seven benchmarks show that TRUST consistently improves
robustness and outperforms existing TTA methods.

</details>


### [21] [MMPB: It's Time for Multi-Modal Personalization](https://arxiv.org/abs/2509.22820)
*Jaeik Kim,Woojin Kim,Woohyeon Park,Jaeyoung Do*

Main category: cs.CV

TL;DR: 本文提出了MMPB，首个用于评估视觉语言模型（VLM）在个性化方面表现的大规模基准，包含10k图像-查询对和111个可个性化概念。通过对23个主流VLM的评估，发现大多数模型在对话一致性、用户偏好处理和视觉线索适应方面存在困难，揭示了当前VLM在个性化上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型在用户个性化方面的适应能力尚未被充分探索，而在智能家庭、医疗等用户导向场景中，模型与用户特定概念的对齐至关重要。因此，需要一个系统性的基准来评估和推动VLM的个性化能力。

Method: 构建了MMPB基准，包含四个类别（人、动物、物体、角色）共111个可个性化概念，并设计了三个任务类型，通过三阶段协议（概念注入、多轮对话、个性化查询）评估23个VLM的个性化性能。

Result: 实验表明大多数VLM（包括部分闭源模型）在个性化任务中表现不佳，尤其在维持对话一致性、处理用户偏好和响应视觉线索方面存在明显缺陷，同时观察到拒绝行为和长上下文遗忘等问题。

Conclusion: MMPB为评估VLM的个性化能力提供了可扩展的基准，揭示了现有模型的关键不足，为未来实现真正个性化的多模态AI研究奠定了基础。

Abstract: Visual personalization is essential in user-facing AI systems such as smart
homes and healthcare, where aligning model behavior with user-centric concepts
is critical. However, recent large Vision-Language Models (VLMs), despite their
broad applicability, remain underexplored in their ability to adapt to
individual users. In this paper, we introduce MMPB, the first extensive
benchmark for evaluating VLMs on personalization. MMPB comprises 10k
image-query pairs and includes 111 personalizable concepts across four
categories: humans, animals, objects, and characters, with the human category
enriched with preference-grounded queries. We structure personalization into
three main task types, each highlighting a different key property of VLMs.
Using 23 widely used VLMs including both open- and closed-source models, we
evaluate personalization performance via a three-stage protocol: concept
injection, multi-turn dialogue, and personalized querying. Our findings
indicate that most VLMs (including some closed-source models) struggle with
personalization, particularly in maintaining consistency over dialogue,
handling user preferences, and adapting to visual cues. Our analysis reveals
that the challenges in VLM personalization (such as refusal behaviors and
long-context forgetting) highlight substantial room for improvement. By
identifying these limitations and offering a scalable benchmark, MMPB offers
valuable insights and a solid foundation for future research toward truly
personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB

</details>


### [22] [Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN](https://arxiv.org/abs/2509.22836)
*Roie Kazoom,Alon Goldberg,Hodaya Cohen,Ofer Hadar*

Main category: cs.CV

TL;DR: 提出一种新型的可完全控制的对抗补丁生成框架，结合生成式U-Net设计与Grad-CAM引导的补丁放置，在保持视觉真实感的同时实现高攻击成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗补丁攻击多依赖白盒假设、无目标攻击或产生明显可见的补丁，限制了在现实场景中的应用，因此需要一种兼具真实性、目标可控性和黑盒适用性的攻击方法。

Method: 采用生成式U-Net结构结合Grad-CAM指导补丁位置，实现语义感知的定位，允许攻击者自由选择输入图像和目标类别，生成视觉逼真且高度有效的对抗补丁。

Result: 在多种CNN和Vision Transformer模型上实验显示，攻击成功率（ASR）和目标类成功率（TCS）均超过99%，在白盒、黑盒及跨模型迁移场景下均达到最先进水平。

Conclusion: 该框架在真实性、目标控制和黑盒攻击之间取得了良好平衡，为基于补丁的对抗攻击设立了新基准，缩小了理论攻击强度与实际隐蔽性之间的差距。

Abstract: Adversarial patch attacks pose a severe threat to deep neural networks, yet
most existing approaches rely on unrealistic white-box assumptions, untargeted
objectives, or produce visually conspicuous patches that limit real-world
applicability. In this work, we introduce a novel framework for fully
controllable adversarial patch generation, where the attacker can freely choose
both the input image x and the target class y target, thereby dictating the
exact misclassification outcome. Our method combines a generative U-Net design
with Grad-CAM-guided patch placement, enabling semantic-aware localization that
maximizes attack effectiveness while preserving visual realism. Extensive
experiments across convolutional networks (DenseNet-121, ResNet-50) and vision
transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach
achieves state-of-the-art performance across all settings, with attack success
rates (ASR) and target-class success (TCS) consistently exceeding 99%.
  Importantly, we show that our method not only outperforms prior white-box
attacks and untargeted baselines, but also surpasses existing non-realistic
approaches that produce detectable artifacts. By simultaneously ensuring
realism, targeted control, and black-box applicability-the three most
challenging dimensions of patch-based attacks-our framework establishes a new
benchmark for adversarial robustness research, bridging the gap between
theoretical attack strength and practical stealthiness.

</details>


### [23] [Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention](https://arxiv.org/abs/2509.22839)
*Ibrahim Delibasoglu,Fredrik Heintz*

Main category: cs.CV

TL;DR: 本文提出了一种名为CrossScaleNet的新架构，结合基于patch的交叉注意力机制和多尺度处理，在时间序列预测中实现了高性能与良好的时间可解释性。


<details>
  <summary>Details</summary>
Motivation: 提高时间序列预测模型的透明度和决策支持能力，解决现有可解释方法计算成本高、难以有效识别时间显著性的问题。

Method: 引入一种嵌入训练过程的patch-based交叉注意力机制，并结合多尺度处理，实现对时间显著性的内在可解释性。

Result: 在合成数据集和公开基准上验证了模型在时间显著性检测和预测精度上的优越性能，显著优于多数基于Transformer的模型。

Conclusion: CrossScaleNet在不牺牲预测准确性的前提下，有效提升了时间序列预测的时间可解释性，弥合了可解释性与高性能之间的差距。

Abstract: Explainability in time series forecasting is essential for improving model
transparency and supporting informed decision-making. In this work, we present
CrossScaleNet, an innovative architecture that combines a patch-based
cross-attention mechanism with multi-scale processing to achieve both high
performance and enhanced temporal explainability. By embedding attention
mechanisms into the training process, our model provides intrinsic
explainability for temporal saliency, making its decision-making process more
transparent. Traditional post-hoc methods for temporal saliency detection are
computationally expensive, particularly when compared to feature importance
detection. While ablation techniques may suffice for datasets with fewer
features, identifying temporal saliency poses greater challenges due to its
complexity. We validate CrossScaleNet on synthetic datasets with known saliency
ground truth and on established public benchmarks, demonstrating the robustness
of our method in identifying temporal saliency. Experiments on real-world
datasets for forecasting task show that our approach consistently outperforms
most transformer-based models, offering better explainability without
sacrificing predictive accuracy. Our evaluations demonstrate superior
performance in both temporal saliency detection and forecasting accuracy.
Moreover, we highlight that existing models claiming explainability often fail
to maintain strong performance on standard benchmarks. CrossScaleNet addresses
this gap, offering a balanced approach that captures temporal saliency
effectively while delivering state-of-the-art forecasting performance across
datasets of varying complexity.

</details>


### [24] [Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging](https://arxiv.org/abs/2509.22841)
*Yi Luo,Yike Guo,Hamed Hooshangnejad,Rui Zhang,Xue Feng,Quan Chen,Wil Ngwa,Kai Ding*

Main category: cs.CV

TL;DR: 本研究提出一种基于迁移学习和多模态交互感知网络（结合MAMBA和切片交互模块SIM）的方法，用于提升PET/CT图像中肺部肿瘤内部大体肿瘤体积（IGTV）的分割精度，显著优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于标注的IGTV数据集稀缺以及肿瘤边界处PET信号弱，肺部移动肿瘤的IGTV精确分割面临挑战，影响放疗计划的准确性。

Method: 采用在大规模GTV数据集上预训练、并在私有IGTV数据集（LUCID的PET/CT子集）上微调的迁移学习策略；构建2.5D分割框架，并引入包含通道与空间注意力及深度卷积的切片交互模块（SIM），以增强切片间关系建模。

Result: 在私有IGTV数据集上，该方法Dice分数达到0.609，显著高于传统基线的0.385，验证了其在弱PET信号区域的有效性。

Conclusion: 结合迁移学习、多模态技术和切片交互模块可显著提升IGTV分割性能，有助于提高肺癌放疗计划的可靠性和临床实用性。

Abstract: Lung cancer remains the leading cause of cancerrelated deaths globally.
Accurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is
pivotal for optimal radiation therapy in mobile tumors such as lung cancer to
account for tumor motion, yet is hindered by the limited availability of
annotated IGTV datasets and attenuated PET signal intensity at tumor
boundaries. In this study, we present a transfer learningbased methodology
utilizing a multimodal interactive perception network with MAMBA, pre-trained
on extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a
private IGTV cohort. This cohort constitutes the PET/CT subset of the
Lung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the
challenge of weak PET intensities in IGTV peripheral slices, we introduce a
slice interaction module (SIM) within a 2.5D segmentation framework to
effectively model inter-slice relationships. Our proposed module integrates
channel and spatial attention branches with depthwise convolutions, enabling
more robust learning of slice-to-slice dependencies and thereby improving
overall segmentation performance. A comprehensive experimental evaluation
demonstrates that our approach achieves a Dice of 0.609 on the private IGTV
dataset, substantially surpassing the conventional baseline score of 0.385.
This work highlights the potential of transfer learning, coupled with advanced
multimodal techniques and a SIM to enhance the reliability and clinical
relevance of IGTV segmentation for lung cancer radiation therapy planning.

</details>


### [25] [ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models](https://arxiv.org/abs/2509.22864)
*Yixuan Hu,Yuxuan Xue,Simon Klenk,Daniel Cremers,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的生成方法ControlEvents，用于合成高质量的事件数据，并通过多种控制信号（如文本标签、2D骨架、3D姿态）进行引导，显著降低标注数据的获取成本。


<details>
  <summary>Details</summary>
Motivation: 由于事件相机数据标注困难且昂贵，缺乏大规模标注数据制约了基于事件的视觉任务的发展，因此需要一种高效生成标注事件数据的方法。

Method: 利用Stable Diffusion等基础模型的扩散先验，设计了一种基于扩散的生成模型ControlEvents，通过少量微调和有限标注数据实现高质量事件数据的生成，并支持多种控制信号输入。

Result: 在视觉识别、2D骨架估计和3D姿态估计任务中验证了生成数据的有效性，合成数据能提升模型性能，并可生成训练时未见过的文本标签对应的事件数据。

Conclusion: ControlEvents能够高效生成高质量、多模态控制的事件数据，显著降低数据标注成本，同时展现出强大的零样本文本生成能力，为事件相机应用提供了可行的数据生成方案。

Abstract: In recent years, event cameras have gained significant attention due to their
bio-inspired properties, such as high temporal resolution and high dynamic
range. However, obtaining large-scale labeled ground-truth data for event-based
vision tasks remains challenging and costly. In this paper, we present
ControlEvents, a diffusion-based generative model designed to synthesize
high-quality event data guided by diverse control signals such as class text
labels, 2D skeletons, and 3D body poses. Our key insight is to leverage the
diffusion prior from foundation models, such as Stable Diffusion, enabling
high-quality event data generation with minimal fine-tuning and limited labeled
data. Our method streamlines the data generation process and significantly
reduces the cost of producing labeled event datasets. We demonstrate the
effectiveness of our approach by synthesizing event data for visual
recognition, 2D skeleton estimation, and 3D body pose estimation. Our
experiments show that the synthesized labeled event data enhances model
performance in all tasks. Additionally, our approach can generate events based
on unseen text labels during training, illustrating the powerful text-based
generation capabilities inherited from foundation models.

</details>


### [26] [Learning KAN-based Implicit Neural Representations for Deformable Image Registration](https://arxiv.org/abs/2509.22874)
*Nikita Drozdov,Marat Zinovev,Dmitry Sorokin*

Main category: cs.CV

TL;DR: 本文提出了KAN-IDIR和RandKAN-IDIR，首次将Kolmogorov-Arnold网络（KAN）引入基于隐式神经表示（INR）的可变形图像配准，通过随机基函数采样策略在降低计算成本的同时保持高精度和学习稳定性，在多种医学影像数据上优于现有INR方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的图像配准方法依赖大量标注数据且在某些器官或模态上精度不足，而经典迭代方法虽精确但耗时；INR提供连续映射优势，但存在实例特定优化带来的计算效率与训练稳定性问题，因此需要更高效稳定的INR配准框架。

Method: 提出KAN-IDIR和RandKAN-IDIR，利用KAN建模从空间坐标到位移向量的连续变形场，并设计随机基函数采样策略以减少所需基函数数量，提升计算效率与跨随机种子的训练稳定性。

Result: 在肺部CT、脑部MRI和心脏MRI三个数据集上验证，KAN-IDIR和RandKAN-IDIR在所有评估模态和解剖结构中均达到最高精度的INR方法，计算开销低，训练稳定性好；RandKAN-IDIR略优于可学习基索引模型，且避免了额外的训练复杂性。

Conclusion: KAN与INR的结合为可变形图像配准提供了高效、稳定且高精度的新范式，RandKAN-IDIR通过随机化基采样在性能与效率之间实现了更优平衡，具有广泛的医学图像分析应用潜力。

Abstract: Deformable image registration (DIR) is a cornerstone of medical image
analysis, enabling spatial alignment for tasks like comparative studies and
multi-modal fusion. While learning-based methods (e.g., CNNs, transformers)
offer fast inference, they often require large training datasets and struggle
to match the precision of classical iterative approaches on some organ types
and imaging modalities. Implicit neural representations (INRs) have emerged as
a promising alternative, parameterizing deformations as continuous mappings
from coordinates to displacement vectors. However, this comes at the cost of
requiring instance-specific optimization, making computational efficiency and
seed-dependent learning stability critical factors for these methods. In this
work, we propose KAN-IDIR and RandKAN-IDIR, the first integration of
Kolmogorov-Arnold Networks (KANs) into deformable image registration with
implicit neural representations (INRs). Our proposed randomized basis sampling
strategy reduces the required number of basis functions in KAN while
maintaining registration quality, thereby significantly lowering computational
costs. We evaluated our approach on three diverse datasets (lung CT, brain MRI,
cardiac MRI) and compared it with competing instance-specific learning-based
approaches, dataset-trained deep learning models, and classical registration
approaches. KAN-IDIR and RandKAN-IDIR achieved the highest accuracy among
INR-based methods across all evaluated modalities and anatomies, with minimal
computational overhead and superior learning stability across multiple random
seeds. Additionally, we discovered that our RandKAN-IDIR model with randomized
basis sampling slightly outperforms the model with learnable basis function
indices, while eliminating its additional training-time complexity.

</details>


### [27] [Convolutional Set Transformer](https://arxiv.org/abs/2509.22889)
*Federico Chinello,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 提出卷积集变换器（CST），可直接处理任意数量且视觉异构但共享高层语义的图像集，兼具特征提取与上下文建模能力，性能优于现有方法，并支持可解释性与迁移学习。


<details>
  <summary>Details</summary>
Motivation: 现有集输入网络无法直接处理3D图像张量，需依赖CNN提取特征后再建模图像间关系，限制了特征提取与上下文建模的协同优化。

Method: 设计CST架构，直接在3D图像张量上操作，结合卷积与变换器机制，同步进行特征提取和集合内图像关系建模。

Result: 在集合分类、集合异常检测任务中表现优于现有方法，支持Grad-CAM等可解释技术，并可通过迁移学习适应新任务；发布预训练模型CST-15。

Conclusion: CST实现了图像集的端到端处理，提升了性能与可解释性，支持迁移学习，推动了集输入数据的通用模型发展。

Abstract: We introduce the Convolutional Set Transformer (CST), a novel neural
architecture designed to process image sets of arbitrary cardinality that are
visually heterogeneous yet share high-level semantics - such as a common
category, scene, or concept. Existing set-input networks, e.g., Deep Sets and
Set Transformer, are limited to vector inputs and cannot directly handle 3D
image tensors. As a result, they must be cascaded with a feature extractor,
typically a CNN, which encodes images into embeddings before the set-input
network can model inter-image relationships. In contrast, CST operates directly
on 3D image tensors, performing feature extraction and contextual modeling
simultaneously, thereby enabling synergies between the two processes. This
design yields superior performance in tasks such as Set Classification and Set
Anomaly Detection and further provides native compatibility with CNN
explainability methods such as Grad-CAM, unlike competing approaches that
remain opaque. Finally, we show that CSTs can be pre-trained on large-scale
datasets and subsequently adapted to new domains and tasks through standard
Transfer Learning schemes. To support further research, we release CST-15, a
CST backbone pre-trained on ImageNet
(https://github.com/chinefed/convolutional-set-transformer).

</details>


### [28] [TY-RIST: Tactical YOLO Tricks for Real-time Infrared Small Target Detection](https://arxiv.org/abs/2509.22909)
*Abdulkarim Atrash,Omar Moured,Yufan Chen,Jiaming Zhang,Seyda Ertekin,Omur Ugur*

Main category: cs.CV

TL;DR: 提出TY-RIST，一种基于YOLOv12n的红外小目标检测方法，通过改进骨干网络、检测头、注意力机制和分支剪枝策略，在降低25.5%计算成本的同时提升检测性能，实现高效实时检测。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临特征少、背景复杂、显著性低和计算成本高等挑战，现有方法难以兼顾精度与效率。

Method: 设计了步长感知的骨干网络、高分辨率检测头、级联坐标注意力模块，并采用分支剪枝策略优化计算开销；引入NWD损失提升回归稳定性。

Result: 在四个基准数据集上mAP@0.5提升7.9%，Precision提升3%，Recall提升10.2%，单GPU可达123 FPS，跨数据集验证显示良好泛化能力。

Conclusion: TY-RIST在精度、速度和计算成本之间取得良好平衡，显著优于现有方法，适用于实际红外小目标检测任务。

Abstract: Infrared small target detection (IRSTD) is critical for defense and
surveillance but remains challenging due to (1) target loss from minimal
features, (2) false alarms in cluttered environments, (3) missed detections
from low saliency, and (4) high computational costs. To address these issues,
we propose TY-RIST, an optimized YOLOv12n architecture that integrates (1) a
stride-aware backbone with fine-grained receptive fields, (2) a high-resolution
detection head, (3) cascaded coordinate attention blocks, and (4) a branch
pruning strategy that reduces computational cost by about 25.5% while
marginally improving accuracy and enabling real-time inference. We also
incorporate the Normalized Gaussian Wasserstein Distance (NWD) to enhance
regression stability. Extensive experiments on four benchmarks and across 20
different models demonstrate state-of-the-art performance, improving mAP at 0.5
IoU by +7.9%, Precision by +3%, and Recall by +10.2%, while achieving up to 123
FPS on a single GPU. Cross-dataset validation on a fifth dataset further
confirms strong generalization capability. Additional results and resources are
available at https://www.github.com/moured/TY-RIST

</details>


### [29] [Learning Unified Representation of 3D Gaussian Splatting](https://arxiv.org/abs/2509.22917)
*Yuelin Xin,Yuheng Liu,Xiaohui Xie,Xinke Li*

Main category: cs.CV

TL;DR: 提出基于连续子流形场的3D高斯点阵嵌入表示方法，以解决其参数化非唯一性和异质性问题。


<details>
  <summary>Details</summary>
Motivation: 直接使用原始高斯参数作为特征会导致模型高度依赖数据，且无法处理参数化的非唯一和异质特性。

Method: 设计一种基于连续子流形场的嵌入表示，封装高斯基元的内在信息，保持颜色和几何结构，同时实现唯一映射和通道同质性。

Result: 该表示方法能更好地保留3D高斯点阵的结构信息，提升神经网络中的学习效率和泛化能力。

Conclusion: 所提出的嵌入表示为基于3D高斯点阵的学习系统提供了更原则性的特征表达方式。

Abstract: A well-designed vectorized representation is crucial for the learning systems
natively based on 3D Gaussian Splatting. While 3DGS enables efficient and
explicit 3D reconstruction, its parameter-based representation remains hard to
learn as features, especially for neural-network-based models. Directly feeding
raw Gaussian parameters into learning frameworks fails to address the
non-unique and heterogeneous nature of the Gaussian parameterization, yielding
highly data-dependent models. This challenge motivates us to explore a more
principled approach to represent 3D Gaussian Splatting in neural networks that
preserves the underlying color and geometric structure while enforcing unique
mapping and channel homogeneity. In this paper, we propose an embedding
representation of 3DGS based on continuous submanifold fields that encapsulate
the intrinsic information of Gaussian primitives, thereby benefiting the
learning of 3DGS.

</details>


### [30] [Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings](https://arxiv.org/abs/2509.22925)
*Yuanzhi Zhu,Xi Wang,Stéphane Lathuilière,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出软嵌入（soft embeddings）以解决单步生成器在从掩码扩散模型蒸馏过程中存在的建模偏差和梯度不可导问题，通过将离散token替换为其期望嵌入，实现端到端训练并支持GAN精调、基于奖励的优化和测试时嵌入优化。


<details>
  <summary>Details</summary>
Motivation: 单步生成器继承教师模型的偏差且输出离散token导致梯度中断，限制了后续优化方法的应用。

Method: 引入软嵌入，用生成器输出分布下的期望嵌入替代离散token，结合Di[M]O框架形成Soft-Di[M]O，实现完全可微的连续代理表示。

Result: 在多个MDM教师模型上，Soft-Di[M]O实现了最先进的单步生成性能：ImageNet-256上FID达1.56（结合GAN精调），文本到图像生成中GenEval和HPS得分更高，并通过TTEO进一步提升效果。

Conclusion: 软嵌入有效克服了单步生成器的局限性，使其可兼容多种下游优化技术，显著提升生成质量。

Abstract: One-step generators distilled from Masked Diffusion Models (MDMs) compress
multiple sampling steps into a single forward pass, enabling efficient text and
image synthesis. However, they suffer two key limitations: they inherit
modeling bias from the teacher, and their discrete token outputs block gradient
flow, preventing post-distillation refinements such as adversarial training,
reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this
work, we introduce soft embeddings, a simple relaxation that replaces discrete
tokens with the expected embeddings under the generator's output distribution.
Soft embeddings preserve representation fidelity for one-step discrete
generator while providing a fully differentiable continuous surrogate that is
compatible with teacher backbones and tokenizer decoders. Integrating soft
embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes
one-step generators end-to-end trainable and enables straightforward
application of GAN-based refinement, differentiable reward fine-tuning, and
TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),
Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image
performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,
along with higher GenEval and HPS scores on text-to-image with reward
fine-tuning, and further gains from TTEO.

</details>


### [31] [FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning](https://arxiv.org/abs/2509.22930)
*Chenghan Yang,Peng Zhou,Dong-Sheng Zhang,Yueyun Wang,Hong-Bin Shen,Xiaoyong Pan*

Main category: cs.CV

TL;DR: FishAI 2.0 是一个结合多模态少样本深度学习与图像生成的数据增强框架，用于提升稀有海洋鱼类的识别准确率，在家族、属和种级别均表现出高精度。


<details>
  <summary>Details</summary>
Motivation: 传统海洋生物图像识别面临数据集不完整和模型精度不足的问题，尤其在稀有物种的少样本条件下表现更差。

Method: 利用分层海洋鱼类基准数据集训练 FishAI 2.0 模型；使用大语言模型 DeepSeek 生成文本描述，并通过 Stable Diffusion 2 采用分层扩散策略进行图像增强，构建多模态特征空间；将增强后的图文数据输入基于 CLIP 的模型实现少样本图像识别。

Result: FishAI 2.0 在家族级别达到 91.67% 的 Top-1 准确率和 97.97% 的 Top-5 准确率，在属和种级别分别达到 87.58% 和 85.42% 的 Top-1 准确率，显著优于基线模型。

Conclusion: FishAI 2.0 提升了海洋鱼类识别的效率与准确性，为海洋生态监测与保护提供了可扩展的技术方案，具有科学价值与实际应用前景。

Abstract: Traditional marine biological image recognition faces challenges of
incomplete datasets and unsatisfactory model accuracy, particularly for
few-shot conditions of rare species where data scarcity significantly hampers
the performance. To address these issues, this study proposes an intelligent
marine fish recognition framework, FishAI 2.0, integrating multimodal few-shot
deep learning techniques with image generation for data augmentation. First, a
hierarchical marine fish benchmark dataset, which provides a comprehensive data
foundation for subsequent model training, is utilized to train the FishAI 2.0
model. To address the data scarcity of rare classes, the large language model
DeepSeek was employed to generate high-quality textual descriptions, which are
input into Stable Diffusion 2 for image augmentation through a hierarchical
diffusion strategy that extracts latent encoding to construct a multimodal
feature space. The enhanced visual-textual datasets were then fed into a
Contrastive Language-Image Pre-Training (CLIP) based model, enabling robust
few-shot image recognition. Experimental results demonstrate that FishAI 2.0
achieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent
at the family level, outperforming baseline CLIP and ViT models with a
substantial margin for the minority classes with fewer than 10 training
samples. To better apply FishAI 2.0 to real-world scenarios, at the genus and
species level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58
percent and 85.42 percent, demonstrating practical utility. In summary, FishAI
2.0 improves the efficiency and accuracy of marine fish identification and
provides a scalable technical solution for marine ecological monitoring and
conservation, highlighting its scientific value and practical applicability.

</details>


### [32] [Brain Tumor Classification from MRI Scans via Transfer Learning and Enhanced Feature Representation](https://arxiv.org/abs/2509.22956)
*Ahta-Shamul Hoque Emran,Hafija Akter,Abdullah Al Shiam,Abu Saleh Musa Miah,Anichur Rahman,Fahmid Al Farid,Hezerul Abdul Karim*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的脑肿瘤检测框架，结合预训练ResNet50、全局平均池化和新型Dense-Dropout序列，并构建了新的MMCBT数据集以解决MRI数据不足和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的早期检测对改善患者预后至关重要，但现有MRI数据集资源有限且存在类别不平衡问题。

Method: 采用预训练ResNet50进行特征提取，结合全局平均池化和线性投影获取高级图像表示，引入新型Dense-Dropout序列增强非线性特征学习并减少过拟合。

Result: 构建了包含209名受试者的MMCBT数据集（共16944张经临床验证的图像），通过数据增强实现类别平衡，提升了模型鲁棒性和检测性能。

Conclusion: 所提框架在脑肿瘤检测中表现出高效性和准确性，新数据集为相关研究提供了可靠资源。

Abstract: Brain tumors are abnormal cell growths in the central nervous system (CNS),
and their timely detection is critical for improving patient outcomes. This
paper proposes an automatic and efficient deep-learning framework for brain
tumor detection from magnetic resonance imaging (MRI) scans. The framework
employs a pre-trained ResNet50 model for feature extraction, followed by Global
Average Pooling (GAP) and linear projection to obtain compact, high-level image
representations. These features are then processed by a novel Dense-Dropout
sequence, a core contribution of this work, which enhances non-linear feature
learning, reduces overfitting, and improves robustness through diverse feature
transformations. Another major contribution is the creation of the Mymensingh
Medical College Brain Tumor (MMCBT) dataset, designed to address the lack of
reliable brain tumor MRI resources. The dataset comprises MRI scans from 209
subjects (ages 9 to 65), including 3671 tumor and 13273 non-tumor images, all
clinically verified under expert supervision. To overcome class imbalance, the
tumor class was augmented, resulting in a balanced dataset well-suited for deep
learning research.

</details>


### [33] [Hemorica: A Comprehensive CT Scan Dataset for Automated Brain Hemorrhage Classification, Segmentation, and Detection](https://arxiv.org/abs/2509.22993)
*Kasra Davoodi,Mohammad Hoseyni,Javad Khoramdel,Reza Barati,Reihaneh Mortazavi,Amirhossein Nikoofard,Mahdi Aliyari-Shoorehdeli,Jaber Hatam Parikhan*

Main category: cs.CV

TL;DR: 本文介绍了Hemorica，一个包含372例头部CT扫描的公开数据集，用于颅内出血（ICH）的AI辅助诊断。数据集包含五种ICH亚型的精细标注，并建立了基准模型验证其质量，为多任务学习和AI辅助检测系统提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 颅内出血的及时诊断至关重要，但现有AI解决方案受限于分散且不完整的公开数据。因此，需要一个高质量、细粒度标注的公开数据集来推动研究发展。

Method: 收集了2012至2024年的372例头部CT扫描，采用双人标注流程并结合神经外科医生仲裁，对五种ICH亚型进行患者级和切片级分类、边界框、二维像素掩码及三维体素掩码标注。使用卷积和Transformer模型进行二分类和分割任务的基准测试。

Result: 数据集展现出低评分者间变异性和高临床真实性。轻量模型MobileViT-XS在二分类中达到87.8%的F1分数，U-Net结合DenseNet161编码器在病变分割中取得85.5%的Dice分数，验证了数据质量和样本量的充分性。

Conclusion: Hemorica是一个高质量、统一且细粒度标注的公开数据集，支持多任务学习、课程学习，并有助于向大规模弱标注数据迁移，为开发AI辅助的ICH检测与量化系统提供了可靠基准。

Abstract: Timely diagnosis of Intracranial hemorrhage (ICH) on Computed Tomography (CT)
scans remains a clinical priority, yet the development of robust Artificial
Intelligence (AI) solutions is still hindered by fragmented public data. To
close this gap, we introduce Hemorica, a publicly available collection of 372
head CT examinations acquired between 2012 and 2024. Each scan has been
exhaustively annotated for five ICH subtypes-epidural (EPH), subdural (SDH),
subarachnoid (SAH), intraparenchymal (IPH), and intraventricular (IVH)-yielding
patient-wise and slice-wise classification labels, subtype-specific bounding
boxes, two-dimensional pixel masks and three-dimensional voxel masks. A
double-reading workflow, preceded by a pilot consensus phase and supported by
neurosurgeon adjudication, maintained low inter-rater variability.
Comprehensive statistical analysis confirms the clinical realism of the
dataset. To establish reference baselines, standard convolutional and
transformer architectures were fine-tuned for binary slice classification and
hemorrhage segmentation. With only minimal fine-tuning, lightweight models such
as MobileViT-XS achieved an F1 score of 87.8% in binary classification, whereas
a U-Net with a DenseNet161 encoder reached a Dice score of 85.5% for binary
lesion segmentation that validate both the quality of the annotations and the
sufficiency of the sample size. Hemorica therefore offers a unified,
fine-grained benchmark that supports multi-task and curriculum learning,
facilitates transfer to larger but weakly labelled cohorts, and facilitates the
process of designing an AI-based assistant for ICH detection and quantification
systems.

</details>


### [34] [ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View](https://arxiv.org/abs/2509.23008)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为ARSS的新框架，使用GPT风格的解码器-only自回归模型，结合视频分词器和相机编码器，从单张图像生成新视角，在公共数据集上表现优于或媲美基于扩散模型的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在世界建模任务（如从稀疏输入生成新视角）中存在非因果生成导致的视图不一致问题，限制了其应用。而自回归模型具有因果性，能够逐步生成并累积信息，更适合此类任务。

Method: 提出ARSS框架：1）使用视频分词器将图像序列映射为离散token；2）设计相机编码器将相机轨迹转化为3D位置引导；3）引入保持时间顺序、随机排列空间顺序的自回归Transformer模块以提升生成质量。

Result: 在多个公开数据集上进行了定性和定量实验，结果表明ARSS在新视角合成任务上表现优于或相当于当前最先进的扩散模型方法。

Conclusion: ARSS通过结合自回归建模与3D感知的相机引导，在保持生成一致性的同时实现了高质量的新视角合成，为世界建模任务提供了新的有效方案。

Abstract: Despite their exceptional generative quality, diffusion models have limited
applicability to world modeling tasks, such as novel view generation from
sparse inputs. This limitation arises because diffusion models generate outputs
in a non-causal manner, often leading to distortions or inconsistencies across
views, and making it difficult to incrementally adapt accumulated knowledge to
new queries. In contrast, autoregressive (AR) models operate in a causal
fashion, generating each token based on all previously generated tokens. In
this work, we introduce \textbf{ARSS}, a novel framework that leverages a
GPT-style decoder-only AR model to generate novel views from a single image,
conditioned on a predefined camera trajectory. We employ a video tokenizer to
map continuous image sequences into discrete tokens and propose a camera
encoder that converts camera trajectories into 3D positional guidance. Then to
enhance generation quality while preserving the autoregressive structure, we
propose a autoregressive transformer module that randomly permutes the spatial
order of tokens while maintaining their temporal order. Extensive qualitative
and quantitative experiments on public datasets demonstrate that our method
performs comparably to, or better than, state-of-the-art view synthesis
approaches based on diffusion models. Our code will be released upon paper
acceptance.

</details>


### [35] [Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition](https://arxiv.org/abs/2509.23009)
*Masato Kobayashi,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: 提出一种通过分离时间动态信息和静态场景信息来减少动作识别模型中静态偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 动作识别模型过度依赖静态线索而非动态人体运动，导致在真实场景和零样本动作识别中表现不佳。

Method: 采用统计独立性损失分离有偏和无偏流，并结合场景预测损失。

Result: 实验证明该方法能有效减少静态偏差，并验证了场景预测损失的重要性。

Conclusion: 所提出的方法能够有效缓解静态偏差，提升动作识别模型的泛化能力。

Abstract: Action recognition models rely excessively on static cues rather than dynamic
human motion, which is known as static bias. This bias leads to poor
performance in real-world applications and zero-shot action recognition. In
this paper, we propose a method to reduce static bias by separating temporal
dynamic information from static scene information. Our approach uses a
statistical independence loss between biased and unbiased streams, combined
with a scene prediction loss. Our experiments demonstrate that this method
effectively reduces static bias and confirm the importance of scene prediction
loss.

</details>


### [36] [Desensitizing for Improving Corruption Robustness in Point Cloud Classification through Adversarial Training](https://arxiv.org/abs/2509.23010)
*Zhiqiang Tian,Weigang Li,Chunhua Deng,Junwei Hu,Yongqiang Wang,Wenping Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Desensitized Adversarial Training (DesenAT) 的方法，通过特征去敏感化和自蒸馏框架来提升点云DNN模型对 corrupted 数据的鲁棒性，同时保持在干净数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于场景复杂性、传感器误差和处理不精确，点云数据不可避免地会受到损坏。过度依赖输入特征是DNN脆弱性的根源，但尚不清楚这一问题是否存在于3D点云任务中，以及减少对这些特征的依赖是否能增强模型鲁棒性。

Method: 使用Shapley值量化DNN对点云特征的敏感性，并设计DesenAT方法：通过消除高贡献点、空间变换生成对抗样本进行对抗训练；结合自蒸馏机制将干净样本的知识迁移到对抗样本中，缓解信息损失。

Result: 在ModelNet-C和PointCloud-C上的实验表明，该方法能有效提升模型在多种腐蚀场景下的鲁棒性，同时不损害其在干净数据集上的性能。

Conclusion: 减少模型对高敏感特征的依赖并通过自蒸馏增强对抗训练，可显著提高3D点云DNN的鲁棒性，验证了去敏感化训练的有效性。

Abstract: Due to scene complexity, sensor inaccuracies, and processing imprecision,
point cloud corruption is inevitable. Over-reliance on input features is the
root cause of DNN vulnerabilities. It remains unclear whether this issue exists
in 3D tasks involving point clouds and whether reducing dependence on these
features can enhance the model's robustness to corrupted point clouds. This
study attempts to answer these questions. Specifically, we quantified the
sensitivity of the DNN to point cloud features using Shapley values and found
that models trained using traditional methods exhibited high sensitivity values
for certain features. Furthermore, under an equal pruning ratio, prioritizing
the pruning of highly sensitive features causes more severe damage to model
performance than random pruning. We propose `Desensitized Adversarial Training'
(DesenAT), generating adversarial samples using feature desensitization and
conducting training within a self-distillation framework, which aims to
alleviate DNN's over-reliance on point clouds features by smoothing
sensitivity. First, data points with high contribution components are
eliminated, and spatial transformation is used to simulate corruption scenes,
generate adversarial samples, and conduct adversarial training on the model.
Next, to compensate for information loss in adversarial samples, we use the
self-distillation method to transfer knowledge from clean samples to
adversarial samples, and perform adversarial training in a distillation
manner.Extensive experiments on ModelNet-C and PointCloud-C demonstrate show
that the propose method can effectively improve the robustness of the model
without reducing the performance of clean data sets. This code is publicly
available at
\href{https://github.com/JerkyT/DesenAT/tree/master}{https://github.com/JerkyT/DesenAT}.

</details>


### [37] [Geometry-Aware Losses for Structure-Preserving Text-to-Sign Language Generation](https://arxiv.org/abs/2509.23011)
*Zetian Wu,Tianshuo Zhou,Stefan Lee,Liang Huang*

Main category: cs.CV

TL;DR: 提出一种新的手语翻译方法，通过建模骨骼关节间的几何约束和运动动态，显著提升生成动作的自然性和解剖合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略人体骨骼运动的解剖学约束与协调模式，导致生成的动作僵硬或不符合生物力学规律。

Method: 引入关节位置、骨长和运动动态的几何约束，采用父关节相对重加权机制增强手指灵活性，并使用骨姿态损失和骨长约束保证解剖一致性。

Result: 与之前最优方法相比，性能差距缩小56.51%，骨长和运动方差差异分别减少18.76%和5.48%。

Conclusion: 该方法显著提升了手语视频生成的解剖真实性和动作自然度，有效改善了文本到手语翻译的质量。

Abstract: Sign language translation from text to video plays a crucial role in enabling
effective communication for Deaf and hard--of--hearing individuals. A major
challenge lies in generating accurate and natural body poses and movements that
faithfully convey intended meanings. Prior methods often neglect the anatomical
constraints and coordination patterns of human skeletal motion, resulting in
rigid or biomechanically implausible outputs. To address this, we propose a
novel approach that explicitly models the relationships among skeletal
joints--including shoulders, arms, and hands--by incorporating geometric
constraints on joint positions, bone lengths, and movement dynamics. During
training, we introduce a parent-relative reweighting mechanism to enhance
finger flexibility and reduce motion stiffness. Additionally, bone-pose losses
and bone-length constraints enforce anatomically consistent structures. Our
method narrows the performance gap between the previous best and the
ground-truth oracle by 56.51%, and further reduces discrepancies in bone length
and movement variance by 18.76% and 5.48%, respectively, demonstrating
significant gains in anatomical realism and motion naturalness.

</details>


### [38] [Planning with Unified Multimodal Models](https://arxiv.org/abs/2509.23014)
*Yihao Sun,Zhilong Zhang,Yang Yu,Pierre-Luc Bacon*

Main category: cs.CV

TL;DR: 本文提出了一种基于统一多模态模型（UMMs）的规划框架Uni-Plan，利用生成视觉内容进行推理，在长视界规划任务中显著优于基于VLM的方法，且具备良好的数据可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言推理的决策方法受限于单一模态，缺乏对多模态信息的有效利用，限制了模型的推理与决策能力。

Method: 提出Uni-Plan框架，利用支持多模态输入输出的UMM，将同一模型同时作为策略、动态模型和价值函数；引入自判别过滤机制，利用生成模型自身过滤无效的动态预测以减少幻觉。

Result: 在长视界规划任务中，Uni-Plan相比VLM方法显著提升成功率，无需专家示范，且在相同训练数据下表现更优，展现出强数据可扩展性。

Conclusion: Uni-Plan展示了统一多模态模型在决策与规划中的巨大潜力，为未来基于UMM的推理与决策研究奠定了基础。

Abstract: With the powerful reasoning capabilities of large language models (LLMs) and
vision-language models (VLMs), many recent works have explored using them for
decision-making. However, most of these approaches rely solely on
language-based reasoning, which limits their ability to reason and make
informed decisions. Recently, a promising new direction has emerged with
unified multimodal models (UMMs), which support both multimodal inputs and
outputs. We believe such models have greater potential for decision-making by
enabling reasoning through generated visual content. To this end, we propose
Uni-Plan, a planning framework built on UMMs. Within this framework, a single
model simultaneously serves as the policy, dynamics model, and value function.
In addition, to avoid hallucinations in dynamics predictions, we present a
novel approach self-discriminated filtering, where the generative model serves
as a self-discriminator to filter out invalid dynamics predictions. Experiments
on long-horizon planning tasks show that Uni-Plan substantially improves
success rates compared to VLM-based methods, while also showing strong data
scalability, requiring no expert demonstrations and achieving better
performance under the same training-data size. This work lays a foundation for
future research in reasoning and decision-making with UMMs.

</details>


### [39] [Copyright Infringement Detection in Text-to-Image Diffusion Models via Differential Privacy](https://arxiv.org/abs/2509.23022)
*Xiafeng Man,Zhipeng Wei,Jingjing Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于差分隐私思想的版权侵权检测框架DPM，用于检测文本到图像扩散模型中的版权侵犯内容，无需访问原始训练数据或文本提示。


<details>
  <summary>Details</summary>
Motivation: 现有的版权侵权检测方法缺乏鲁棒性且缺少严谨的理论基础，同时大模型可能未经授权记忆并复现受版权保护的内容，引发法律与伦理问题。

Method: 通过引入条件敏感性度量，并从差分隐私角度形式化版权侵权检测；提出D-Plus-Minus（DPM）框架，利用正向学习与反向遗忘的微调模拟数据点的加入与移除，结合正交提示分布上的统计指标计算置信度得分，以识别侵权内容。

Result: DPM在无需访问原始训练数据或提示的情况下，能够可靠地检测出侵权内容；同时构建了版权侵权检测数据集CIDD，支持跨类别评估。

Conclusion: DPM为生成式AI时代的知识产权保护提供了一个可解释且实用的解决方案，具有良好的鲁棒性和理论基础。

Abstract: The widespread deployment of large vision models such as Stable Diffusion
raises significant legal and ethical concerns, as these models can memorize and
reproduce copyrighted content without authorization. Existing detection
approaches often lack robustness and fail to provide rigorous theoretical
underpinnings. To address these gaps, we formalize the concept of copyright
infringement and its detection from the perspective of Differential Privacy
(DP), and introduce the conditional sensitivity metric, a concept analogous to
sensitivity in DP, that quantifies the deviation in a diffusion model's output
caused by the inclusion or exclusion of a specific training data point. To
operationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc
detection framework that identifies copyright infringement in text-to-image
diffusion models. Specifically, DPM simulates inclusion and exclusion processes
by fine-tuning models in two opposing directions: learning or unlearning.
Besides, to disentangle concept-specific influence from the global parameter
shifts induced by fine-tuning, DPM computes confidence scores over orthogonal
prompt distributions using statistical metrics. Moreover, to facilitate
standardized benchmarking, we also construct the Copyright Infringement
Detection Dataset (CIDD), a comprehensive resource for evaluating detection
across diverse categories. Our results demonstrate that DPM reliably detects
infringement content without requiring access to the original training dataset
or text prompts, offering an interpretable and practical solution for
safeguarding intellectual property in the era of generative AI.

</details>


### [40] [Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement](https://arxiv.org/abs/2509.23025)
*Gabriel A. Viana,Luis F. Alves Pereira,Tsang Ing Ren,George D. C. Cavalcanti,Jan Sijbers*

Main category: cs.CV

TL;DR: 本文提出了一种评估感知损失设计对低剂量CT图像增强性能影响的系统性框架，引入了“感知影响力”这一新指标，并通过实验验证了优化感知损失设计可显著提升图像去噪效果和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素的损失函数（如均方误差）在低剂量CT图像增强中容易导致过度平滑，丢失临床细节；而现有感知损失的设计选择缺乏系统研究，影响其性能发挥。

Method: 引入‘感知影响力’作为量化感知损失贡献的指标，构建一个原则性框架来评估特征表示层次、编码器预训练数据集及感知损失权重等设计因素的影响，并通过系统性实验比较不同配置的效果。

Result: 实验表明，文献中常用的感知损失配置表现不佳，而经过优化的设计能显著改善CT图像的噪声抑制和结构保真度，且无需更改网络架构；并通过统计分析提供了有效的设计指南。

Conclusion: 合理的感知损失设计对LDCT图像质量提升至关重要，本文提出的框架和指南为未来使用感知损失进行医学图像增强提供了可靠依据。

Abstract: Perceptual losses have emerged as powerful tools for training networks to
enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to
traditional pixel-wise losses such as Mean Squared Error, which often lead to
over-smoothed reconstructions and loss of clinically relevant details in LDCT
images. The perceptual losses operate in a latent feature space defined by a
pretrained encoder and aim to preserve semantic content by comparing high-level
features rather than raw pixel values. However, the design of perceptual losses
involves critical yet underexplored decisions, including the feature
representation level, the dataset used to pretrain the encoder, and the
relative importance assigned to the perceptual component during optimization.
In this work, we introduce the concept of perceptual influence (a metric that
quantifies the relative contribution of the perceptual loss term to the total
loss) and propose a principled framework to assess the impact of the loss
design choices on the model training performance. Through systematic
experimentation, we show that the widely used configurations in the literature
to set up a perceptual loss underperform compared to better-designed
alternatives. Our findings show that better perceptual loss designs lead to
significant improvements in noise reduction and structural fidelity of
reconstructed CT images, without requiring any changes to the network
architecture. We also provide objective guidelines, supported by statistical
analysis, to inform the effective use of perceptual losses in LDCT denoising.
Our source code is available at
https://github.com/vngabriel/perceptual-influence.

</details>


### [41] [Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across SAR and Multispectral Modalities](https://arxiv.org/abs/2509.23035)
*Tomohiro Tanaka,Narumasa Tsutsumida*

Main category: cs.CV

TL;DR: 提出一种基于轻量级多模态预训练Transformer（Presto）的传感器灵活洪水检测方法，可利用SAR、多光谱或二者融合数据进行洪水制图，具有高效、鲁棒和实用性强的优点。


<details>
  <summary>Details</summary>
Motivation: 现有洪水监测方法受限于单一传感器的数据可用性问题或多传感器融合的高计算成本与数据需求，难以满足灾害应急响应中快速、灵活的制图需求。

Method: 通过微调一个轻量级（约0.4M参数）的多模态预训练Transformer模型Presto，实现对SAR和多光谱数据的像素级洪水检测，并支持SAR-only、MS-only及SAR+MS三种输入模式。

Result: 在Sen1Floods11数据集上，该方法在融合模式下F1达0.896，mIoU达0.886，优于Prithvi-100M基线；在MS-only模式下F1为0.893，SAR-only下为0.718，表现出良好鲁棒性。

Conclusion: 所提出的参数高效、传感器灵活的方法能在不同数据条件下稳定工作，适用于实际灾害场景中不受传感器限制的快速洪水范围评估。

Abstract: Floods are increasingly frequent natural disasters causing extensive human
and economic damage, highlighting the critical need for rapid and accurate
flood inundation mapping. While remote sensing technologies have advanced flood
monitoring capabilities, operational challenges persist: single-sensor
approaches face weather-dependent data availability and limited revisit
periods, while multi-sensor fusion methods require substantial computational
resources and large-scale labeled datasets. To address these limitations, this
study introduces a novel sensor-flexible flood detection methodology by
fine-tuning Presto, a lightweight ($\sim$0.4M parameters) multi-modal
pre-trained transformer that processes both Synthetic Aperture Radar (SAR) and
multispectral (MS) data at the pixel level. Our approach uniquely enables flood
mapping using SAR-only, MS-only, or combined SAR+MS inputs through a single
model architecture, addressing the critical operational need for rapid response
with whatever sensor data becomes available first during disasters. We
evaluated our method on the Sen1Floods11 dataset against the large-scale
Prithvi-100M baseline ($\sim$100M parameters) across three realistic data
availability scenarios. The proposed model achieved superior performance with
an F1 score of 0.896 and mIoU of 0.886 in the optimal sensor-fusion scenario,
outperforming the established baseline. Crucially, the model demonstrated
robustness by maintaining effective performance in MS-only scenarios (F1:
0.893) and functional capabilities in challenging SAR-only conditions (F1:
0.718), confirming the advantage of multi-modal pre-training for operational
flood mapping. Our parameter-efficient, sensor-flexible approach offers an
accessible and robust solution for real-world disaster scenarios requiring
immediate flood extent assessment regardless of sensor availability
constraints.

</details>


### [42] [GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization](https://arxiv.org/abs/2509.23038)
*Jingxing Li,Yongjae Lee,Deliang Fan*

Main category: cs.CV

TL;DR: GeLoc3r提出了一种通过几何一致性正则化（GCR）增强相对相机位姿估计的方法，在训练过程中引入几何知识，使回归网络在保持25ms快速推理的同时接近MASt3R级别的精度。


<details>
  <summary>Details</summary>
Motivation: 尽管Prior ReLoc3R速度快、性能好，但其内部表示存在几何不一致性，限制了精度提升；而基于对应关系的方法（如MASt3R）虽精度高但速度慢。因此需要一种兼顾速度与精度的新方法。

Method: 提出GeLoc3r，采用几何一致性正则化（GCR），在训练时利用真值深度生成密集3D-2D对应关系，通过FusionTransformer加权，并用加权RANSAC计算几何一致的位姿，构建一致性损失来提升回归网络的几何理解能力；推理时仅使用回归头，不进行几何计算。

Result: 在CO3Dv2、RealEstate10K和MegaDepth1500等多个挑战性基准上均优于ReLoc3R，例如在CO3Dv2上AUC@5°达到40.45%（相对提升16%），同时保持25ms快速推理。

Conclusion: GeLoc3r通过在训练中注入几何一致性，实现了回归方法的速度与对应方法精度的结合，代表了神经网络学习相机几何的新范式。

Abstract: Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and
state-of-the-art regression accuracy, yet our analysis reveals subtle geometric
inconsistencies in its internal representations that prevent reaching the
precision ceiling of correspondence-based methods like MASt3R (which require
300ms per pair). In this work, we present GeLoc3r, a novel approach to relative
camera pose estimation that enhances pose regression methods through Geometric
Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma
by training regression networks to produce geometrically consistent poses
without inference-time geometric computation. During training, GeLoc3r
leverages ground-truth depth to generate dense 3D-2D correspondences, weights
them using a FusionTransformer that learns correspondence importance, and
computes geometrically-consistent poses via weighted RANSAC. This creates a
consistency loss that transfers geometric knowledge into the regression
network. Unlike FAR method which requires both regression and geometric solving
at inference, GeLoc3r only uses the enhanced regression head at test time,
maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On
challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving
significant improvements including 40.45% vs. 34.85% AUC@5{\deg} on the CO3Dv2
dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\deg} on
RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric
consistency during training rather than enforcing it at inference, GeLoc3r
represents a paradigm shift in how neural networks learn camera geometry,
achieving both the speed of regression and the geometric understanding of
correspondence methods.

</details>


### [43] [MMeViT: Multi-Modal ensemble ViT for Post-Stroke Rehabilitation Action Recognition](https://arxiv.org/abs/2509.23044)
*Ye-eun Kim,Suhyeon Lim,Andrew J. Choi*

Main category: cs.CV

TL;DR: 本研究针对中风患者居家上肢日常生活活动的远程监测，提出了一种基于IMU传感器和RGB-D相机的多模态动作识别系统，并设计了适用于中风患者动作特征的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别研究主要面向非残疾个体，难以适用于中风患者；同时中风患者的康复治疗面临医疗资源短缺问题，亟需有效的远程监测技术。

Method: 采用IMU传感器和RGB-D相机采集中风患者上肢动作数据，构建多模态数据集，提出一种适用于该类数据的深度学习模型，并进行数据预处理与分析。

Result: 发现中风患者的动作数据比非残疾个体更难聚类，所提出的模型能有效学习难以聚类数据中的标签趋势，在动作识别方面表现良好。

Conclusion: 该研究验证了深度学习模型在中风患者动作识别中的可行性，未来可扩展至家庭康复评估与反馈系统。

Abstract: Rehabilitation therapy for stroke patients faces a supply shortage despite
the increasing demand. To address this issue, remote monitoring systems that
reduce the burden on medical staff are emerging as a viable alternative. A key
component of these remote monitoring systems is Human Action Recognition (HAR)
technology, which classifies actions. However, existing HAR studies have
primarily focused on non-disable individuals, making them unsuitable for
recognizing the actions of stroke patients. HAR research for stroke has largely
concentrated on classifying relatively simple actions using machine learning
rather than deep learning. In this study, we designed a system to monitor the
actions of stroke patients, focusing on domiciliary upper limb Activities of
Daily Living (ADL). Our system utilizes IMU (Inertial Measurement Unit) sensors
and an RGB-D camera, which are the most common modalities in HAR. We directly
collected a dataset through this system, investigated an appropriate preprocess
and proposed a deep learning model suitable for processing multimodal data. We
analyzed the collected dataset and found that the action data of stroke
patients is less clustering than that of non-disabled individuals.
Simultaneously, we found that the proposed model learns similar tendencies for
each label in data with features that are difficult to clustering. This study
suggests the possibility of expanding the deep learning model, which has
learned the action features of stroke patients, to not only simple action
recognition but also feedback such as assessment contributing to domiciliary
rehabilitation in future research. The code presented in this study is
available at https://github.com/ye-Kim/MMeViT.

</details>


### [44] [Activation Matching for Explanation Generation](https://arxiv.org/abs/2509.23051)
*Pirzada Suhail,Aditya Anand,Amit Sethi*

Main category: cs.CV

TL;DR: 本文提出一种基于激活匹配的轻量级自编码器方法，通过生成二值掩码来提取图像中对分类决策关键的最小且可信的解释区域。


<details>
  <summary>Details</summary>
Motivation: 为了提升深度学习模型决策的可解释性，需要生成既简洁又忠实于模型推理过程的视觉解释。

Method: 训练一个轻量级自动编码器生成二值掩码，使掩码后的输入在多层激活分布、预测标签上与原输入一致，并引入L1正则、二值化惩罚和总变分等先验约束掩码的稀疏性和紧凑性，同时使用溯因约束保证解释的忠实性和必要性。

Result: 该方法能生成极简、紧凑且人类可读的掩码，有效保留模型预测行为的同时去除无关区域，在多个数据集上实现了优于现有方法的解释效果。

Conclusion: 所提出的激活匹配结合多目标优化框架能够生成最小且可信的解释，为理解预训练分类器的决策机制提供了实用工具。

Abstract: In this paper we introduce an activation-matching--based approach to generate
minimal, faithful explanations for the decision-making of a pretrained
classifier on any given image. Given an input image \(x\) and a frozen model
\(f\), we train a lightweight autoencoder to output a binary mask \(m\) such
that the explanation \(e = m \odot x\) preserves both the model's prediction
and the intermediate activations of \(x\). Our objective combines: (i)
multi-layer activation matching with KL divergence to align distributions and
cross-entropy to retain the top-1 label for both the image and the explanation;
(ii) mask priors -- L1 area for minimality, a binarization penalty for crisp
0/1 masks, and total variation for compactness; and (iii) abductive constraints
for faithfulness and necessity. Together, these objectives yield small,
human-interpretable masks that retain classifier behavior while discarding
irrelevant input regions, providing practical and faithful minimalist
explanations for the decision making of the underlying model.

</details>


### [45] [Mask What Matters: Controllable Text-Guided Masking for Self-Supervised Medical Image Analysis](https://arxiv.org/abs/2509.23054)
*Ruilang Wang,Shuotong Xu,Bowen Liu,Runlin Huang,Donglong Chen,Weifeng Su*

Main category: cs.CV

TL;DR: 提出了一种可控的文本引导掩码框架Mask What Matters，用于自监督医学图像分析，通过视觉-语言模型实现语义对齐的区域定位，显著提升多种医学影像任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督掩码图像建模方法多依赖随机高比例掩码，语义对齐差，且区域感知变体常需监督信号或重建启发式策略，限制了跨任务和模态的适应性。

Method: 利用视觉-语言模型进行基于提示的区域定位，对诊断相关区域进行差异化掩码，强调关键区域并减少背景冗余，实现可控的掩码策略。

Result: 在脑MRI、胸部CT和肺部X光等多种医学影像模态上优于现有MIM方法（如SparK），分类准确率最高提升+3.1%，BoxAP提升+1.3，MaskAP提升+1.1，且整体掩码比例更低（如40% vs 70%）。

Conclusion: 可控的文本驱动掩码能实现语义对齐的自监督学习，有效提升医学图像分析中视觉模型的鲁棒性和泛化能力。

Abstract: The scarcity of annotated data in specialized domains such as medical imaging
presents significant challenges to training robust vision models. While
self-supervised masked image modeling (MIM) offers a promising solution,
existing approaches largely rely on random high-ratio masking, leading to
inefficiency and poor semantic alignment. Moreover, region-aware variants
typically depend on reconstruction heuristics or supervised signals, limiting
their adaptability across tasks and modalities. We propose Mask What Matters, a
controllable text-guided masking framework for self-supervised medical image
analysis. By leveraging vision-language models for prompt-based region
localization, our method flexibly applies differentiated masking to emphasize
diagnostically relevant regions while reducing redundancy in background areas.
This controllable design enables better semantic alignment, improved
representation learning, and stronger cross-task generalizability.
Comprehensive evaluation across multiple medical imaging modalities, including
brain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently
outperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1
percentage points in classification accuracy, +1.3 in box average precision
(BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it
achieves these improvements with substantially lower overall masking ratios
(e.g., 40\% vs. 70\%). This work demonstrates that controllable, text-driven
masking can enable semantically aligned self-supervised learning, advancing the
development of robust vision models for medical image analysis.

</details>


### [46] [FMC-DETR: Frequency-Decoupled Multi-Domain Coordination for Aerial-View Object Detection](https://arxiv.org/abs/2509.23056)
*Ben Liang,Yuan Liu,Bingwen Qiu,Yihong Wang,Xiubao Sui,Qian Chen*

Main category: cs.CV

TL;DR: 本文提出FMC-DETR，一种用于航拍图像小目标检测的新型框架，通过频域解耦融合提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在航拍图像小目标检测中因上下文融合延迟和非线性建模不足，难以有效利用全局信息优化浅层特征，导致性能瓶颈。

Method: 提出FMC-DETR框架，包括Wavelet Kolmogorov-Arnold Transformer（WeKat）骨干网络、Cross-stage Partial Fusion（CPF）模块和Multi-Domain Feature Coordination（MDFC）模块，实现频域解耦融合、增强全局低频感知与多尺度特征交互。

Result: 在VisDrone等航拍数据集上达到SOTA性能，相比基线模型AP提升6.5%，AP50提升8.2%，且参数更少。

Conclusion: FMC-DETR通过频域解耦融合有效提升了航拍图像中小目标的检测能力，具有较强的实用性和扩展性。

Abstract: Aerial-view object detection is a critical technology for real-world
applications such as natural resource monitoring, traffic management, and
UAV-based search and rescue. Detecting tiny objects in high-resolution aerial
imagery presents a long-standing challenge due to their limited visual cues and
the difficulty of modeling global context in complex scenes. Existing methods
are often hampered by delayed contextual fusion and inadequate non-linear
modeling, failing to effectively use global information to refine shallow
features and thus encountering a performance bottleneck. To address these
challenges, we propose FMC-DETR, a novel framework with frequency-decoupled
fusion for aerial-view object detection. First, we introduce the Wavelet
Kolmogorov-Arnold Transformer (WeKat) backbone, which applies cascaded wavelet
transforms to enhance global low-frequency context perception in shallow
features while preserving fine-grained details, and employs Kolmogorov-Arnold
networks to achieve adaptive non-linear modeling of multi-scale dependencies.
Next, a lightweight Cross-stage Partial Fusion (CPF) module reduces redundancy
and improves multi-scale feature interaction. Finally, we introduce the
Multi-Domain Feature Coordination (MDFC) module, which unifies spatial,
frequency, and structural priors to to balance detail preservation and global
enhancement. Extensive experiments on benchmark aerial-view datasets
demonstrate that FMC-DETR achieves state-of-the-art performance with fewer
parameters. On the challenging VisDrone dataset, our model achieves
improvements of 6.5% AP and 8.2% AP50 over the baseline, highlighting its
effectiveness in tiny object detection. The code can be accessed at
https://github.com/bloomingvision/FMC-DETR.

</details>


### [47] [Follow-Your-Preference: Towards Preference-Aligned Image Inpainting](https://arxiv.org/abs/2509.23082)
*Yutao Shen,Junkun Yuan,Toru Aonishi,Hideki Nakayama,Yue Ma*

Main category: cs.CV

TL;DR: 本文研究了图像修复中的偏好对齐问题，通过使用直接偏好优化方法和公开的奖励模型构建偏好训练数据集，在多个基准和模型上进行了实验。研究发现大多数奖励模型能够提供有效的评分，但存在亮度、构图和色彩等方面的偏差，可能导致奖励欺骗；而简单的模型集成可有效缓解这些偏差，提升对齐效果。基于这些发现，作者在不改变模型结构或使用新数据的情况下，显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 解决图像修复中偏好对齐的基本问题，探究现有奖励模型的有效性与潜在偏差，建立可靠且通用的对齐方法。

Method: 采用直接偏好优化（DPO）进行对齐训练，利用九个公开奖励模型构建偏好数据集，并在不同结构和生成算法的基线模型上开展实验，分析候选和样本扩展趋势，识别奖励模型偏差，并提出模型集成策略以减少偏差影响。

Result: 实验证明大多数奖励模型可用于构建偏好数据；偏好数据在不同模型和基准下表现出良好的扩展性；发现了奖励模型在亮度、构图和颜色上的系统性偏见；简单集成方法能有效抑制偏差，提升性能；所提对齐模型在标准指标、GPT-4评估和人工评价中均显著超越先前方法。

Conclusion: 通过对奖励模型的再审视和简单集成策略，可在不修改模型结构或引入新数据的前提下实现更优的偏好对齐效果，为图像修复中的对齐任务建立了简单而坚实的基线。

Abstract: This paper investigates image inpainting with preference alignment. Instead
of introducing a novel method, we go back to basics and revisit fundamental
problems in achieving such alignment. We leverage the prominent direct
preference optimization approach for alignment training and employ public
reward models to construct preference training datasets. Experiments are
conducted across nine reward models, two benchmarks, and two baseline models
with varying structures and generative algorithms. Our key findings are as
follows: (1) Most reward models deliver valid reward scores for constructing
preference data, even if some of them are not reliable evaluators. (2)
Preference data demonstrates robust trends in both candidate scaling and sample
scaling across models and benchmarks. (3) Observable biases in reward models,
particularly in brightness, composition, and color scheme, render them
susceptible to cause reward hacking. (4) A simple ensemble of these models
yields robust and generalizable results by mitigating such biases. Built upon
these observations, our alignment models significantly outperform prior models
across standard metrics, GPT-4 assessments, and human evaluations, without any
changes to model structures or the use of new datasets. We hope our work can
set a simple yet solid baseline, pushing this promising frontier. Our code is
open-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.

</details>


### [48] [Streamline pathology foundation model by cross-magnification distillation](https://arxiv.org/abs/2509.23097)
*Ziyu Su,Abdul Rehman Akbar,Usama Sajjad,Anil V. Parwani,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: XMAG是一种轻量级基础模型，通过跨放大倍数蒸馏技术，将20x教师模型的知识迁移到5x学生模型，显著降低计算需求，实现快速、准确的病理图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型因参数量大、依赖高倍率图像处理，在临床部署中计算成本过高，限制了其实际应用。

Method: 提出XMAG模型，采用紧凑骨干网络并在5x放大倍率下运行；通过双级知识蒸馏框架，对齐全局表示和局部空间token映射，从20x教师模型向5x学生模型迁移知识。

Result: 在六个临床病理分析任务中，XMAG的诊断准确率与更大的基础模型相差不到1%，处理速度达8.8张全切片图像/分钟，比现有方法快30倍，且仅需1/11.3的图像块数量。跨机构验证显示其具有良好的泛化能力。

Conclusion: 跨放大倍数蒸馏是实现基础模型在资源受限临床环境中高效部署的有效途径，有望推动实时AI病理诊断的落地应用。

Abstract: Foundation models (FM) have transformed computational pathology but remain
computationally prohibitive for clinical deployment due to their massive
parameter counts and high-magnification processing requirements. Here, we
introduce XMAG, a lightweight FM developed through corss-magnification
distillation that transfers knowledge from state-of-the-art 20x magnification
teacher to an efficient 5x magnification student architecture. XMAG employs a
compact backbone and operates entirely at 5x, requiring 11.3 times fewer
patches per whole slide image (WSI) compared to existing approaches. Our Novel
distillation framework incorporates dual-level knowledge transfer, aligning
both global image representations and local spatial token mapping. We trained
XMAG on 3.49 million images curated from publicly available datasets and
evaluated performance across six clinically relevant histopathology analysis
tasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within
1% of substantially larger foundation models while delivering 30-fold
processing acceleration, reaching 8.8 WSIs per minute processing speed. Our
cross-institutional validation confirmed robust generalization. Further, we
developed an end-to-end training strategy to further boost our model's
performance to approach the larger FMs' performance. These results establish
cross-magnification distillation as a viable approach for deploying FM
capabilities in resource-constrained clinical environments, potentially
enabling real-time pathology AI integration.

</details>


### [49] [CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP](https://arxiv.org/abs/2509.23098)
*Na Min An,Inha Kang,Minhyun Lee,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出CoPatch，一种零样本指代表达分割框架，通过增强文本和图像模态中的空间表征来提升空间定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（如CLIP）在理解空间关系上存在不足，且文本特征提取常忽略上下文词元，导致指代表达分割性能受限。

Method: 在语言流中构建包含携带空间线索上下文词元的混合文本特征；在视觉流中从中间层发现新路径提取能更好保留空间结构的patch级图像特征，并将两者融合为聚类的图像-文本相似性图（CoMap）以实现精确掩码选择。

Result: 在RefCOCO、RefCOCO+、RefCOCOg和PhraseCut数据集上实现零样本性能提升，mIoU提高2-7个百分点。

Conclusion: 通过挖掘和利用VLM内部隐含的空间知识，可显著提升零样本指代表达分割中的空间定位能力，为该领域提供了新方向。

Abstract: Spatial grounding is crucial for referring image segmentation (RIS), where
the goal of the task is to localize an object described by language. Current
foundational vision-language models (VLMs), such as CLIP, excel at aligning
images and text but struggle with understanding spatial relationships. Within
the language stream, most existing methods often focus on the primary noun
phrase when extracting local text features, undermining contextual tokens.
Within the vision stream, CLIP generates similar features for images with
different spatial layouts, resulting in limited sensitivity to spatial
structure. To address these limitations, we propose \textsc{CoPatch}, a
zero-shot RIS framework that leverages internal model components to enhance
spatial representations in both text and image modalities. For language,
\textsc{CoPatch} constructs hybrid text features by incorporating context
tokens carrying spatial cues. For vision, it extracts patch-level image
features using our novel path discovered from intermediate layers, where
spatial structure is better preserved. These enhanced features are fused into a
clustered image-text similarity map, \texttt{CoMap}, enabling precise mask
selection. As a result, \textsc{CoPatch} significantly improves spatial
grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+
2--7 mIoU) without requiring any additional training. Our findings underscore
the importance of recovering and leveraging the untapped spatial knowledge
inherently embedded in VLMs, thereby paving the way for opportunities in
zero-shot RIS.

</details>


### [50] [Deep Learning for Oral Health: Benchmarking ViT, DeiT, BEiT, ConvNeXt, and Swin Transformer](https://arxiv.org/abs/2509.23100)
*Ajo Babu George,Sadhvik Bathini,Niranjana S R*

Main category: cs.CV

TL;DR: 本研究系统评估了五种先进的Transformer架构在多类牙科疾病分类中的性能，发现ConvNeXt、Swin Transformer和BEiT表现最佳，尤其在处理数据不平衡问题上具有优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中常被忽视的真实世界数据不平衡问题，提升牙科疾病分类模型的实用性。

Method: 使用Oral Diseases数据集训练和验证ViT、DeiT、ConvNeXt、Swin Transformer和BEiT五种模型，比较其准确率、精确率、召回率和F1分数。

Result: ConvNeXt以81.06%的验证准确率表现最优，BEiT和Swin Transformer紧随其后；ViT和DeiT在龋齿相关类别上表现较差。

Conclusion: ConvNeXt、Swin Transformer和BEiT在牙科疾病分类中表现出可靠的诊断性能，适合临床应用，为未来AI驱动的口腔疾病诊断工具提供了模型选择依据。

Abstract: Objective: The aim of this study was to systematically evaluate and compare
the performance of five state-of-the-art transformer-based architectures -
Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), ConvNeXt,
Swin Transformer, and Bidirectional Encoder Representation from Image
Transformers (BEiT) - for multi-class dental disease classification. The study
specifically focused on addressing real-world challenges such as data
imbalance, which is often overlooked in existing literature.
  Study Design: The Oral Diseases dataset was used to train and validate the
selected models. Performance metrics, including validation accuracy, precision,
recall, and F1-score, were measured, with special emphasis on how well each
architecture managed imbalanced classes.
  Results: ConvNeXt achieved the highest validation accuracy at 81.06, followed
by BEiT at 80.00 and Swin Transformer at 79.73, all demonstrating strong
F1-scores. ViT and DeiT achieved accuracies of 79.37 and 78.79, respectively,
but both struggled particularly with Caries-related classes.
  Conclusions: ConvNeXt, Swin Transformer, and BEiT showed reliable diagnostic
performance, making them promising candidates for clinical application in
dental imaging. These findings provide guidance for model selection in future
AI-driven oral disease diagnostic tools and highlight the importance of
addressing data imbalance in real-world scenarios

</details>


### [51] [HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing](https://arxiv.org/abs/2509.23103)
*Emadeldeen Hamdan,Ahmet Enis Cetin*

Main category: cs.CV

TL;DR: 提出HTMA-Net，结合Hadamard变换与无乘法SRAM存内计算，显著减少深度神经网络中的乘法操作，降低计算复杂度和参数量，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的边缘设备上高效部署深度神经网络，亟需降低乘法运算带来的计算成本。

Method: 引入HTMA-Net框架，选择性地用基于Hadamard变换的混合层替代中间卷积层，并在SRAM存内计算中实现无乘法操作的卷积运算。

Result: 在ResNet-18等模型上验证，相比基线最多减少52%的乘法操作，计算复杂度和参数量显著降低，且保持相当的准确率。

Conclusion: 结合结构化Hadamard变换与存内无乘法计算是构建高效深度学习架构的有效途径。

Abstract: Reducing the cost of multiplications is critical for efficient deep neural
network deployment, especially in energy-constrained edge devices. In this
work, we introduce HTMA-Net, a novel framework that integrates the Hadamard
Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing
to reduce arithmetic complexity while maintaining accuracy. Unlike prior
methods that only target multiplications in convolutional layers or focus
solely on in-memory acceleration, HTMA-Net selectively replaces intermediate
convolutions with Hybrid Hadamard-based transform layers whose internal
convolutions are implemented via multiplication-avoiding in-memory operations.
We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet,
and provide a detailed comparison against regular, MF-only, and HT-only
variants. Results show that HTMA-Net eliminates up to 52\% of multiplications
compared to baseline ResNet-18, ResNet-20, and ResNet-50 models, while
achieving comparable accuracy in evaluation and significantly reducing
computational complexity and the number of parameters. Our results demonstrate
that combining structured Hadamard transform layers with SRAM-based in-memory
computing multiplication-avoiding operators is a promising path towards
efficient deep learning architectures.

</details>


### [52] [Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM](https://arxiv.org/abs/2509.23105)
*Junxiao Xue,Quan Deng,Xuecheng Wu,Kelu Yao,Xinyi Yin,Fei Yu,Wei Zhou,Yanfei Zhong,Yang Liu,Dingkang Yang*

Main category: cs.CV

TL;DR: 本文提出了一个名为ChangeIMTI的大规模多任务遥感变化理解数据集，并设计了一种具有双粒度感知的视觉引导视觉语言模型（ChangeVG），在四个变化理解任务上实现了优越性能，尤其在变化描述任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感变化理解数据集在变化描述、计数和定位等多任务交互方面缺乏深度理解和多样性，难以支持全面的环境变化分析，因此需要构建更全面、互动性强的多任务数据集和模型。

Method: 构建了包含变化描述、二分类、计数和定位四个任务的ChangeIMTI数据集，并提出ChangeVG模型，采用双分支视觉引导模块融合细粒度空间特征与高层语义信息，作为辅助提示指导大型视觉语言模型进行指令微调，实现层次化跨模态学习。

Result: 在四个任务上实验表明，所提方法在变化描述任务中比Semantic-CC方法在综合S*m指标上高出1.39点，且消融研究验证了模型关键组件的有效性。

Conclusion: ChangeIMTI数据集和ChangeVG模型有效推动了遥感变化理解的多任务协同与深度交互，通过双粒度感知和视觉引导机制显著提升了跨模态学习性能。

Abstract: Remote sensing change understanding (RSCU) is essential for analyzing remote
sensing images and understanding how human activities affect the environment.
However, existing datasets lack deep understanding and interactions in the
diverse change captioning, counting, and localization tasks. To tackle these
gaps, we construct ChangeIMTI, a new large-scale interactive multi-task
instruction dataset that encompasses four complementary tasks including change
captioning, binary change classification, change counting, and change
localization. Building upon this new dataset, we further design a novel
vision-guided vision-language model (ChangeVG) with dual-granularity awareness
for bi-temporal remote sensing images (i.e., two remote sensing images of the
same area at different times). The introduced vision-guided module is a
dual-branch architecture that synergistically combines fine-grained spatial
feature extraction with high-level semantic summarization. These enriched
representations further serve as the auxiliary prompts to guide large
vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning,
thereby facilitating the hierarchical cross-modal learning. We extensively
conduct experiments across four tasks to demonstrate the superiority of our
approach. Remarkably, on the change captioning task, our method outperforms the
strongest method Semantic-CC by 1.39 points on the comprehensive S*m metric,
which integrates the semantic similarity and descriptive accuracy to provide an
overall evaluation of change caption. Moreover, we also perform a series of
ablation studies to examine the critical components of our method.

</details>


### [53] [Stochastic Interpolants via Conditional Dependent Coupling](https://arxiv.org/abs/2509.23122)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.CV

TL;DR: 提出了一种基于条件依赖耦合策略的统一多阶段生成框架，通过单个Diffusion Transformer实现端到端优化，在保证高保真度的同时提升了生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在计算成本与生成质量之间存在权衡问题，基于VAE的方法存在信息损失和无法端到端训练的问题，而像素空间方法计算代价高昂，级联模型难以有效优化。

Method: 提出条件依赖耦合策略，将生成过程分解为多阶段插值轨迹，并采用统一的Diffusion Transformer建模整个过程，实现端到端训练和知识共享。

Result: 实验表明该方法在多种分辨率下均实现了高保真度和高效率，优于现有模型。

Conclusion: 所提出的统一多阶段生成框架有效解决了计算成本、信息损失和端到端优化之间的矛盾，为高质量图像生成提供了新思路。

Abstract: Existing image generation models face critical challenges regarding the
trade-off between computation and fidelity. Specifically, models relying on a
pretrained Variational Autoencoder (VAE) suffer from information loss, limited
detail, and the inability to support end-to-end training. In contrast, models
operating directly in the pixel space incur prohibitive computational cost.
Although cascade models can mitigate computational cost, stage-wise separation
prevents effective end-to-end optimization, hampers knowledge sharing, and
often results in inaccurate distribution learning within each stage. To address
these challenges, we introduce a unified multistage generative framework based
on our proposed Conditional Dependent Coupling strategy. It decomposes the
generative process into interpolant trajectories at multiple stages, ensuring
accurate distribution learning while enabling end-to-end optimization.
Importantly, the entire process is modeled as a single unified Diffusion
Transformer, eliminating the need for disjoint modules and also enabling
knowledge sharing. Extensive experiments demonstrate that our method achieves
both high fidelity and efficiency across multiple resolutions.

</details>


### [54] [Benchmarking DINOv3 for Multi-Task Stroke Analysis on Non-Contrast CT](https://arxiv.org/abs/2509.23132)
*Donghao Zhang,Yimin Chen,Kauê TN Duarte,Taha Aslan,Mohamed AlShamrani,Brij Karmur,Yan Wan,Shengcai Chen,Bo Hu,Bijoy K Menon,Wu Qiu*

Main category: cs.CV

TL;DR: 本研究利用先进的自监督视觉Transformer模型DINOv3，提升非增强CT（NCCT）在卒中分析任务中的表现，包括梗死和出血分割、异常分类、出血亚型分类及ASPECTS评分分类，建立了多项任务的强基准，并分析了该方法的优势与局限性。


<details>
  <summary>Details</summary>
Motivation: NCCT在快速诊断卒中方面至关重要，但受限于低对比度和信噪比，现有方法在特征表示上仍有不足，因此需要更强大的模型来提升自动化卒中诊断性能。

Method: 采用DINOv3这一前沿的自监督视觉Transformer模型，提取NCCT图像的深层特征，并应用于多种卒中分析任务，包括分割、分类等，在多个公开和私有数据集上进行综合评估。

Result: 在梗死/出血分割、异常分类、出血亚型分类和ASPECTS分类等多个任务上建立了强有力的基准性能，验证了DINOv3在提升NCCT分析效果方面的潜力。

Conclusion: DINOv3能够生成强大的特征表示，显著提升NCCT在多种卒中分析任务中的自动化诊断性能，展示了自监督学习在临床影像分析中的应用前景，但也存在计算成本和泛化能力等方面的限制。

Abstract: Non-contrast computed tomography (NCCT) is essential for rapid stroke
diagnosis but is limited by low image contrast and signal to noise ratio. We
address this challenge by leveraging DINOv3, a state-of-the-art self-supervised
vision transformer, to generate powerful feature representations for a
comprehensive set of stroke analysis tasks. Our evaluation encompasses infarct
and hemorrhage segmentation, anomaly classification (normal vs. stroke and
normal vs. infarct vs. hemorrhage), hemorrhage subtype classification (EDH,
SDH, SAH, IPH, IVH), and dichotomized ASPECTS classification (<=6 vs. >6) on
multiple public and private datasets. This study establishes strong benchmarks
for these tasks and demonstrates the potential of advanced self-supervised
models to improve automated stroke diagnosis from NCCT, providing a clear
analysis of both the advantages and current constraints of the approach. The
code is available at https://github.com/Zzz0251/DINOv3-stroke.

</details>


### [55] [Earth-Agent: Unlocking the Full Landscape of Earth Observation with Agents](https://arxiv.org/abs/2509.23141)
*Peilin Feng,Zhutao Lv,Junyan Ye,Xiaolei Wang,Xinjie Huo,Jinhua Yu,Wanghan Xu,Wenlong Zhang,Lei Bai,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出了Earth-Agent，首个结合RGB和光谱地球观测数据的代理框架，通过MCP工具生态系统实现跨模态、多步骤和定量时空推理，并提出包含248个专家设计任务的Earth-Bench评估基准，推动遥感领域向基于科学推理的下一代大模型应用发展。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理需要多步推理和专业工具的复杂地球观测任务时能力有限，且当前基于代理的方法局限于RGB感知、浅层推理，缺乏系统性评估机制。

Method: 构建Earth-Agent框架，集成RGB与光谱数据，基于MCP架构动态调用专家工具进行跨模态推理；设计Earth-Bench基准，包含13,729张图像和双层评估协议，覆盖多种模态与科学任务。

Result: 实验表明Earth-Agent在不同LLM主干、通用代理框架及遥感基准上的表现均优于现有方法，能有效支持地物参数反演和定量时空分析等复杂任务。

Conclusion: Earth-Agent建立了地球观测分析的新范式，推动LLM在遥感领域向科学化、工具增强和深度推理方向发展。

Abstract: Earth observation (EO) is essential for understanding the evolving states of
the Earth system. Although recent MLLMs have advanced EO research, they still
lack the capability to tackle complex tasks that require multi-step reasoning
and the use of domain-specific tools. Agent-based methods offer a promising
direction, but current attempts remain in their infancy, confined to RGB
perception, shallow reasoning, and lacking systematic evaluation protocols. To
overcome these limitations, we introduce Earth-Agent, the first agentic
framework that unifies RGB and spectral EO data within an MCP-based tool
ecosystem, enabling cross-modal, multi-step, and quantitative spatiotemporal
reasoning beyond pretrained MLLMs. Earth-Agent supports complex scientific
tasks such as geophysical parameter retrieval and quantitative spatiotemporal
analysis by dynamically invoking expert tools and models across modalities. To
support comprehensive evaluation, we further propose Earth-Bench, a benchmark
of 248 expert-curated tasks with 13,729 images, spanning spectrum, products and
RGB modalities, and equipped with a dual-level evaluation protocol that
assesses both reasoning trajectories and final outcomes. We conduct
comprehensive experiments varying different LLM backbones, comparisons with
general agent frameworks, and comparisons with MLLMs on remote sensing
benchmarks, demonstrating both the effectiveness and potential of Earth-Agent.
Earth-Agent establishes a new paradigm for EO analysis, moving the field toward
scientifically grounded, next-generation applications of LLMs in Earth
observation. Our code and dataset will be publicly released.

</details>


### [56] [WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning](https://arxiv.org/abs/2509.23150)
*Wenxuan Fang,Jiangwei Weng,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为WeatherCycle的无监督图像恢复框架，通过亮度-色度分解和退化感知课程正则化，实现多天气条件下的图像去噪与增强。


<details>
  <summary>Details</summary>
Motivation: 现有的图像恢复方法通常依赖于特定任务的物理先验，难以泛化到多种真实天气场景，因此需要一个统一且可扩展的无监督框架。

Method: 提出WeatherCycle框架，采用亮度-色度分解策略解耦退化与内容；引入Lumina Degradation Guidance Module（LDGM）通过频域幅度调制建模光照退化；设计Difficulty-Aware Contrastive Regularization（DACR）模块，利用CLIP分类器识别困难样本并进行对比对齐以提升语义一致性。

Result: 在多个多天气数据集上实验表明，该方法在无监督方法中达到最先进的性能，并展现出对复杂天气退化的强泛化能力。

Conclusion: WeatherCycle提供了一个通用、无需配对数据的图像恢复解决方案，有效应对多天气条件下的退化问题，显著提升了恢复质量与语义保真度。

Abstract: Unsupervised image restoration under multi-weather conditions remains a
fundamental yet underexplored challenge. While existing methods often rely on
task-specific physical priors, their narrow focus limits scalability and
generalization to diverse real-world weather scenarios. In this work, we
propose \textbf{WeatherCycle}, a unified unpaired framework that reformulates
weather restoration as a bidirectional degradation-content translation cycle,
guided by degradation-aware curriculum regularization. At its core,
WeatherCycle employs a \textit{lumina-chroma decomposition} strategy to
decouple degradation from content without modeling complex weather, enabling
domain conversion between degraded and clean images. To model diverse and
complex degradations, we propose a \textit{Lumina Degradation Guidance Module}
(LDGM), which learns luminance degradation priors from a degraded image pool
and injects them into clean images via frequency-domain amplitude modulation,
enabling controllable and realistic degradation modeling. Additionally, we
incorporate a \textit{Difficulty-Aware Contrastive Regularization (DACR)}
module that identifies hard samples via a CLIP-based classifier and enforces
contrastive alignment between hard samples and restored features to enhance
semantic consistency and robustness. Extensive experiments across serve
multi-weather datasets, demonstrate that our method achieves state-of-the-art
performance among unsupervised approaches, with strong generalization to
complex weather degradations.

</details>


### [57] [Sparse2Dense: A Keypoint-driven Generative Framework for Human Video Compression and Vertex Prediction](https://arxiv.org/abs/2509.23169)
*Bolin Chen,Ru-Ling Liao,Yan Ye,Jie Chen,Shanzhi Yin,Xinrui Ju,Shiqi Wang,Yibo Fan*

Main category: cs.CV

TL;DR: 提出Sparse2Dense框架，利用极稀疏3D关键点实现超低比特率人体视频压缩与精确顶点预测。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的多媒体应用中，如何同时实现超低比特率视频压缩和准确的人体顶点预测是一个关键挑战。

Method: 采用关键点驱动的生成框架，通过多任务学习和关键点感知的深度生成模型，用稀疏3D关键点编码人体运动，并估计密集运动以合成具时间连贯性和真实纹理的视频；同时联合优化顶点预测器以保持视觉与几何一致性。

Result: 实验表明，Sparse2Dense在人体视频压缩性能上优于传统和生成式视频编解码器，同时能精确预测人体顶点几何结构。

Conclusion: Sparse2Dense可有效支持带宽高效的人体中心媒体传输，适用于实时动作分析、虚拟人动画和沉浸式娱乐等场景。

Abstract: For bandwidth-constrained multimedia applications, simultaneously achieving
ultra-low bitrate human video compression and accurate vertex prediction
remains a critical challenge, as it demands the harmonization of dynamic motion
modeling, detailed appearance synthesis, and geometric consistency. To address
this challenge, we propose Sparse2Dense, a keypoint-driven generative framework
that leverages extremely sparse 3D keypoints as compact transmitted symbols to
enable ultra-low bitrate human video compression and precise human vertex
prediction. The key innovation is the multi-task learning-based and
keypoint-aware deep generative model, which could encode complex human motion
via compact 3D keypoints and leverage these sparse keypoints to estimate dense
motion for video synthesis with temporal coherence and realistic textures.
Additionally, a vertex predictor is integrated to learn human vertex geometry
through joint optimization with video generation, ensuring alignment between
visual content and geometric structure. Extensive experiments demonstrate that
the proposed Sparse2Dense framework achieves competitive compression
performance for human video over traditional/generative video codecs, whilst
enabling precise human vertex prediction for downstream geometry applications.
As such, Sparse2Dense is expected to facilitate bandwidth-efficient
human-centric media transmission, such as real-time motion analysis, virtual
human animation, and immersive entertainment.

</details>


### [58] [TRAX: TRacking Axles for Accurate Axle Count Estimation](https://arxiv.org/abs/2509.23171)
*Avinash Rai,Sandeep Jana,Vishal Vijay*

Main category: cs.CV

TL;DR: 提出了一种基于视频的端到端车辆轴数计数方法，结合YOLO-OBB和YOLO检测车辆与轮胎，并通过TRAX算法跟踪轴特征，有效解决了密集和遮挡场景下的轴数误检问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集、遮挡或长车辆场景下轴数检测准确性不足，限制了交通管理与收费系统的自动化水平。

Method: 采用YOLO-OBB检测并分类车辆，使用YOLO检测轮胎，并通过提出的TRAX（Tire and Axle Tracking）算法跨帧跟踪轮胎与轴特征，实现轮胎与对应车辆的智能匹配。

Result: 显著降低了误报率，提升了长车辆和复杂场景下的轴数检测准确性和系统鲁棒性，在真实交通视频中表现良好。

Conclusion: 该方法推动了可扩展的AI驱动轴数检测系统的发展，有望以机器视觉替代传统道路基础设施。

Abstract: Accurate counting of vehicle axles is essential for traffic control, toll
collection, and infrastructure development. We present an end-to-end,
video-based pipeline for axle counting that tackles limitations of previous
works in dense environments. Our system leverages a combination of YOLO-OBB to
detect and categorize vehicles, and YOLO to detect tires. Detected tires are
intelligently associated to their respective parent vehicles, enabling accurate
axle prediction even in complex scenarios. However, there are a few challenges
in detection when it comes to scenarios with longer and occluded vehicles. We
mitigate vehicular occlusions and partial detections for longer vehicles by
proposing a novel TRAX (Tire and Axle Tracking) Algorithm to successfully track
axle-related features between frames. Our method stands out by significantly
reducing false positives and improving the accuracy of axle-counting for long
vehicles, demonstrating strong robustness in real-world traffic videos. This
work represents a significant step toward scalable, AI-driven axle counting
systems, paving the way for machine vision to replace legacy roadside
infrastructure.

</details>


### [59] [Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift](https://arxiv.org/abs/2509.23176)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.CV

TL;DR: CalSAM 是一种轻量级的适应框架，通过特征Fisher信息惩罚和置信度错配惩罚，提升 SAM 在医学图像（尤其是脑 MRI）中的领域泛化能力和不确定性校准，仅微调掩码解码器即可显著改善分割性能与可靠性。


<details>
  <summary>Details</summary>
Motivation: SAM 在自然图像上表现良好，但在医学体积数据上存在领域偏移和过度自信问题，限制了其在临床场景中的可靠性，因此需要一种有效且高效的校准与适应方法。

Method: 提出 CalSAM 框架，引入两种新损失：基于 3D 特征图的特征Fisher信息惩罚（FIP）以减少编码器对领域偏移的敏感性，以及置信度错配惩罚（CMP）以抑制体素级预测的过度自信；整体损失仅用于微调掩码解码器，保持 SAM 编码器冻结。

Result: 在跨中心和扫描仪迁移任务中，CalSAM 显著提升了性能与校准效果，例如在 BraTS 扫描仪分割任务中 DSC 提升 7.4%，HD95 降低 26.9%，ECE 降低 39.5%；在 ATLAS-C 数据上 DSC 提升 5.3%，ECE 降低 32.6%；消融实验表明 FIP 与 CMP 具有互补作用，Fisher 惩罚带来约 15% 的训练时间开销。

Conclusion: CalSAM 在不破坏 SAM 编码器预训练知识的前提下，有效提升了其在医学图像上的领域适应性与预测校准能力，是一种高效、实用的 SAM 零样本迁移方案。

Abstract: The Segment Anything Model (SAM) exhibits strong zero-shot performance on
natural images but suffers from domain shift and overconfidence when applied to
medical volumes. We propose \textbf{CalSAM}, a lightweight adaptation framework
that (i) reduces encoder sensitivity to domain shift via a \emph{Feature Fisher
Information Penalty} (FIP) computed on 3D feature maps and (ii) penalizes
overconfident voxel-wise errors through a \emph{Confidence Misalignment
Penalty} (CMP). The combined loss, \(\mathcal{L}_{\mathrm{CalSAM}}\) fine-tunes
only the mask decoder while keeping SAM's encoders frozen. On cross-center and
scanner-shift evaluations, CalSAM substantially improves accuracy and
calibration: e.g., on the BraTS scanner split (Siemens$\to$GE) CalSAM shows a
$+7.4\%$ relative improvement in $\mathrm{DSC}$ (80.1\% vs.\ 74.6\%), a
$-26.9\%$ reduction in $\mathrm{HD95}$ (4.6 mm vs.\ 6.3 mm), and a $-39.5\%$
reduction in $\mathrm{ECE}$ (5.2\% vs.\ 8.6\%). On ATLAS-C (motion
corruptions), CalSAM achieves a $+5.3\%$ relative improvement in $\mathrm{DSC}$
(75.9\%) and a $-32.6\%$ reduction in $\mathrm{ECE}$ (5.8\%). Ablations show
FIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty
incurs a modest $\sim$15\% training-time overhead. CalSAM therefore delivers
improved domain generalization and better-calibrated uncertainty estimates for
brain MRI segmentation, while retaining the computational benefits of freezing
SAM's encoder.

</details>


### [60] [Unsupervised Online 3D Instance Segmentation with Synthetic Sequences and Dynamic Loss](https://arxiv.org/abs/2509.23194)
*Yifan Zhang,Wei Zhang,Chuangxin He,Zhonghua Miao,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了一种新的无监督在线3D实例分割框架，通过合成点云序列生成、灵活的时间采样策略和动态加权损失，提升了分割精度和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于训练多样性不足、时间采样僵化和对噪声伪标签的依赖，难以在无标注数据条件下保持跨LiDAR扫描的物体身份一致性。

Method: 通过生成合成点云序列来增加训练分布的多样性；采用灵活采样策略利用相邻和非相邻帧捕捉长短时态依赖；引入动态加权损失以强调高置信度和信息丰富的样本。

Result: 在SemanticKITTI、nuScenes和PandaSet数据集上实验表明，该方法在分割准确性和时间关联鲁棒性方面均优于UNIT及其他无监督基线方法。

Conclusion: 所提出的方法有效提升了无监督在线3D实例分割性能，为无需人工标注和仿真引擎的数据增强与学习提供了新思路。

Abstract: Unsupervised online 3D instance segmentation is a fundamental yet challenging
task, as it requires maintaining consistent object identities across LiDAR
scans without relying on annotated training data. Existing methods, such as
UNIT, have made progress in this direction but remain constrained by limited
training diversity, rigid temporal sampling, and heavy dependence on noisy
pseudo-labels. We propose a new framework that enriches the training
distribution through synthetic point cloud sequence generation, enabling
greater diversity without relying on manual labels or simulation engines. To
better capture temporal dynamics, our method incorporates a flexible sampling
strategy that leverages both adjacent and non-adjacent frames, allowing the
model to learn from long-range dependencies as well as short-term variations.
In addition, a dynamic-weighting loss emphasizes confident and informative
samples, guiding the network toward more robust representations. Through
extensive experiments on SemanticKITTI, nuScenes, and PandaSet, our method
consistently outperforms UNIT and other unsupervised baselines, achieving
higher segmentation accuracy and more robust temporal associations. The code
will be publicly available at github.com/Eaphan/SFT3D.

</details>


### [61] [Real-World Transferable Adversarial Attack on Face-Recognition Systems](https://arxiv.org/abs/2509.23198)
*Andrey Kaznacheev,Matvey Mikhalchuk,Andrey Kuznetsov,Aleksandr Petiushko,Anton Razzhigaev*

Main category: cs.CV

TL;DR: 提出了一种名为GaP（Gaussian Patch）的新型方法，可在严格黑盒设置下生成通用且物理可迁移的对抗性补丁，仅通过约10,000次查询即可在ArcFace和未见过的FaceNet模型上实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击大多局限于数字域或需要白盒访问，缺乏在严格黑盒条件下仍具物理可迁移性的有效攻击方法。

Method: 采用查询效率高、零阶贪婪算法迭代构建前额区域的对称灰度模式，通过不断添加高斯斑点，并利用代理FR模型的余弦相似度分数进行优化。

Result: 在约10,000次查询下，GaP在数字和真实物理场景中均实现了高攻击成功率，并展现出强迁移性，可成功欺骗未见过的FaceNet模型。

Conclusion: 证明了即使对目标系统了解有限，仍可构造出鲁棒且可迁移的对抗攻击，揭示了人脸识别系统在现实世界中的严重安全漏洞。

Abstract: Adversarial attacks on face recognition (FR) systems pose a significant
security threat, yet most are confined to the digital domain or require
white-box access. We introduce GaP (Gaussian Patch), a novel method to generate
a universal, physically transferable adversarial patch under a strict black-box
setting. Our approach uses a query-efficient, zero-order greedy algorithm to
iteratively construct a symmetric, grayscale pattern for the forehead. The
patch is optimized by successively adding Gaussian blobs, guided only by the
cosine similarity scores from a surrogate FR model to maximally degrade
identity recognition. We demonstrate that with approximately 10,000 queries to
a black-box ArcFace model, the resulting GaP achieves a high attack success
rate in both digital and real-world physical tests. Critically, the attack
shows strong transferability, successfully deceiving an entirely unseen FaceNet
model. Our work highlights a practical and severe vulnerability, proving that
robust, transferable attacks can be crafted with limited knowledge of the
target system.

</details>


### [62] [UltraUNet: Real-Time Ultrasound Tongue Segmentation for Diverse Linguistic and Imaging Conditions](https://arxiv.org/abs/2509.23225)
*Alisher Myrgyyassov,Zhen Song,Yu Sun,Bruce Xiao Wang,Min Ney Wong,Yongping Zheng*

Main category: cs.CV

TL;DR: UltraUNet是一种轻量级编码器-解码器网络，用于超声图像中舌轮廓的实时分割，具有高精度和高计算效率。


<details>
  <summary>Details</summary>
Motivation: 由于信噪比低、成像变异性和计算需求高，超声舌图像中舌轮廓的实时分割面临挑战。

Method: 提出UltraUNet，采用轻量级Squeeze-and-Excitation模块、群组归一化和基于求和的跳跃连接，并结合超声特定的数据增强方法。

Result: 实现250帧/秒的处理速度，单数据集Dice系数为0.855，MSD为0.993px，跨数据集Dice平均为0.734和0.761。

Conclusion: UltraUNet为语音研究、临床诊断和言语运动障碍分析提供了快速且准确的解决方案。

Abstract: Ultrasound tongue imaging (UTI) is a non-invasive and cost-effective tool for
studying speech articulation, motor control, and related disorders. However,
real-time tongue contour segmentation remains challenging due to low
signal-to-noise ratios, imaging variability, and computational demands. We
propose UltraUNet, a lightweight encoder-decoder architecture optimized for
real-time segmentation of tongue contours in ultrasound images. UltraUNet
incorporates domain-specific innovations such as lightweight
Squeeze-and-Excitation blocks, Group Normalization for small-batch stability,
and summation-based skip connections to reduce memory and computational
overhead. It achieves 250 frames per second and integrates ultrasound-specific
augmentations like denoising and blur simulation. Evaluations on 8 datasets
demonstrate high accuracy and robustness, with single-dataset Dice = 0.855 and
MSD = 0.993px, and cross-dataset Dice averaging 0.734 and 0.761. UltraUNet
provides a fast, accurate solution for speech research, clinical diagnostics,
and analysis of speech motor disorders.

</details>


### [63] [Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers](https://arxiv.org/abs/2509.23235)
*Seongsoo Heo,Dong-Wan Choi*

Main category: cs.CV

TL;DR: 本文提出了一种新的模型反演方法Patch Rebirth Inversion (PRI)，用于提升Vision Transformers在无数据学习中的反演效率，通过渐进式稀疏化合成图像，在保持高精度的同时显著加速反演过程。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏模型反演（SMI）方法通过剪枝不重要的图像块来提升反演效率，但过早丢弃这些块会抑制类别无关特征的提取，影响知识迁移。作者发现即使是随机选择的图像块也能在持续反演中获得可迁移知识，因此提出应避免提前丢弃。

Method: 提出Patch Rebirth Inversion (PRI)方法，在反演过程中逐步分离最重要的图像块以构建稀疏合成图像，同时让其余图像块继续演化以供后续选择，实现渐进式稀疏化。

Result: PRI比标准的Dense Model Inversion (DMI)快10倍，比SMI快2倍，且准确率优于SMI，与DMI相当。

Conclusion: PRI通过保留并持续优化非关键图像块，实现了效率与性能的平衡，揭示了‘重生效应’在知识迁移中的重要性，为高效的无数据模型反演提供了新思路。

Abstract: Model inversion is a widely adopted technique in data-free learning that
reconstructs synthetic inputs from a pretrained model through iterative
optimization, without access to original training data. Unfortunately, its
application to state-of-the-art Vision Transformers (ViTs) poses a major
computational challenge, due to their expensive self-attention mechanisms. To
address this, Sparse Model Inversion (SMI) was proposed to improve efficiency
by pruning and discarding seemingly unimportant patches, which were even
claimed to be obstacles to knowledge transfer. However, our empirical findings
suggest the opposite: even randomly selected patches can eventually acquire
transferable knowledge through continued inversion. This reveals that
discarding any prematurely inverted patches is inefficient, as it suppresses
the extraction of class-agnostic features essential for knowledge transfer,
along with class-specific features. In this paper, we propose Patch Rebirth
Inversion (PRI), a novel approach that incrementally detaches the most
important patches during the inversion process to construct sparse synthetic
images, while allowing the remaining patches to continue evolving for future
selection. This progressive strategy not only improves efficiency, but also
encourages initially less informative patches to gradually accumulate more
class-relevant knowledge, a phenomenon we refer to as the Re-Birth effect,
thereby effectively balancing class-agnostic and class-specific knowledge.
Experimental results show that PRI achieves up to 10x faster inversion than
standard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently
outperforming SMI in accuracy and matching the performance of DMI.

</details>


### [64] [Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection](https://arxiv.org/abs/2509.23236)
*Mingfei Han,Haihong Hao,Jinxing Zhou,Zhihui Li,Yuhui Zheng,Xueqing Deng,Linjie Yang,Xiaojun Chang*

Main category: cs.CV

TL;DR: 提出一种基于自洽性的新框架，通过长回答与短答案的一致性生成偏好对训练数据，无需人工标注或外部监督，有效减少视觉-语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量人工标注或强外部模型监督来缓解视觉-语言模型的幻觉问题，成本高且难以扩展，因此需要一种更高效、可扩展的无监督解决方案。

Method: 设计一个自反思流程，利用模型对简短短二元问题的高可靠性回答，与详细回答进行对比，通过不一致性信号自动构建高质量训练数据，并基于自洽性优化模型输出。

Result: 在多个基准（如AMBER、MultiObject-Hal、Object HalBench、MMHal-Bench）上显著提升了事实准确性和输出可靠性，同时在LLaVA-Bench和MMBench上保持了良好的指令遵循能力。

Conclusion: 该方法仅依赖模型自身的一致性，无需外部标注或更强模型监督，提供了一种可扩展、高效的方案来减少幻觉，提升视觉-语言模型的可信度。

Abstract: Vision-language models often hallucinate details, generating non-existent
objects or inaccurate attributes that compromise output reliability. Existing
methods typically address these issues via extensive human annotations or
external supervision from more powerful models. In this work, we present a
novel framework that leverages the model's self-consistency between long
responses and short answers to generate preference pairs for training. We
observe that short binary questions tend to yield highly reliable responses,
which can be used to query the target model to evaluate and rank its generated
responses. Specifically, we design a self-reflection pipeline where detailed
model responses are compared against concise binary answers, and inconsistency
signals are utilized to automatically curate high-quality training data without
human annotations or external model-based supervision. By relying solely on
self-consistency rather than external supervision, our method offers a scalable
and efficient solution that effectively reduces hallucinations using unlabeled
data. Extensive experiments on multiple benchmarks, i.e., AMBER,
MultiObject-Hal (ROPE), Object HalBench, and MMHal-Bench, demonstrate
significant improvements in factual grounding and reliability. Moreover, our
approach maintains robust instruction-following ability, as evidenced by
enhanced performance on LLaVA-Bench and MMBench.

</details>


### [65] [TATTOO: Training-free AesTheTic-aware Outfit recOmmendation](https://arxiv.org/abs/2509.23242)
*Yuntian Wu,Xiaonan Hu,Ziqi Zhou,Hao Lu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的时尚穿搭推荐方法TATTOO，利用多模态大模型生成目标商品描述，并通过美学思维链提取图像的结构化美学特征，结合动态熵门控机制实现高质量、审美感知的搭配推荐。


<details>
  <summary>Details</summary>
Motivation: 现有时尚搭配推荐方法依赖大规模标注数据和特定任务训练，且缺乏对人类审美的显式建模，限制了其应用与效果。

Method: 提出TATTOO方法，利用多模态大语言模型（MLLMs）生成目标项描述，通过美学思维链将图像提炼为包含颜色、风格、场合等维度的结构化美学特征，并采用动态熵门控机制融合视觉摘要、文本描述和美学向量，在共享嵌入空间中进行候选项目排序。

Result: 在真实数据集Aesthetic-100上达到SOTA性能，在标准Polyvore数据集上展示了强大的零样本检索能力。

Conclusion: TATTOO实现了无需训练的时尚搭配推荐新范式，兼具高性能与审美感知能力，为时尚电商中的智能推荐提供了更高效、可扩展的解决方案。

Abstract: The global fashion e-commerce market relies significantly on intelligent and
aesthetic-aware outfit-completion tools to promote sales. While previous
studies have approached the problem of fashion outfit-completion and
compatible-item retrieval, most of them require expensive, task-specific
training on large-scale labeled data, and no effort is made to guide outfit
recommendation with explicit human aesthetics. In the era of Multimodal Large
Language Models (MLLMs), we show that the conventional training-based pipeline
could be streamlined to a training-free paradigm, with better recommendation
scores and enhanced aesthetic awareness. We achieve this with TATTOO, a
Training-free AesTheTic-aware Outfit recommendation approach. It first
generates a target-item description using MLLMs, followed by an aesthetic
chain-of-thought used to distill the images into a structured aesthetic profile
including color, style, occasion, season, material, and balance. By fusing the
visual summary of the outfit with the textual description and aesthetics
vectors using a dynamic entropy-gated mechanism, candidate items can be
represented in a shared embedding space and be ranked accordingly. Experiments
on a real-world evaluation set Aesthetic-100 show that TATTOO achieves
state-of-the-art performance compared with existing training-based methods.
Another standard Polyvore dataset is also used to measure the advanced
zero-shot retrieval capability of our training-free method.

</details>


### [66] [Increasing the Diversity in RGB-to-Thermal Image Translation for Automotive Applications](https://arxiv.org/abs/2509.23243)
*Kaili Wang,Leonardo Ravaglia,Roberto Longo,Lore Goetschalckx,David Van Hamme,Julie Moeyersoms,Ben Stoffelen,Tom De Schepper*

Main category: cs.CV

TL;DR: 提出一种基于CoAdaIN的多模态RGB到热成像转换框架，实现更真实、多样化的热图像生成，以解决ADAS中热数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于真实驾驶场景中热成像数据集有限且在模拟器中表征不足，现有ADAS研究面临挑战，需要有效方法生成多样化热图像。

Method: 提出一种多模态图像翻译框架，引入组件感知的自适应实例归一化（CoAdaIN），该方法能针对不同图像组件（如天空、车辆、行人）进行局部风格迁移，而非全局应用，从而实现一对多的RGB到热成像转换。

Result: 实验表明，所提方法生成的热图像更加逼真且多样性显著提升，在视觉质量和下游任务表现上优于现有单映射方法。

Conclusion: CoAdaIN能够有效提升RGB-to-thermal图像翻译的真实性和多样性，为ADAS中的热感知研究提供了可靠的数据生成解决方案。

Abstract: Thermal imaging in Advanced Driver Assistance Systems (ADAS) improves road
safety with superior perception in low-light and harsh weather conditions
compared to traditional RGB cameras. However, research in this area faces
challenges due to limited dataset availability and poor representation in
driving simulators. RGB-to-thermal image translation offers a potential
solution, but existing methods focus on one-to-one mappings. We propose a
one-to-many mapping using a multi-modal translation framework enhanced with our
Component-aware Adaptive Instance Normalization (CoAdaIN). Unlike the original
AdaIN, which applies styles globally, CoAdaIN adapts styles to different image
components individually. The result, as we show, is more realistic and diverse
thermal image translations. This is the accepted author manuscript of the paper
published in IEEE Sensors Conference 2024. The final published version is
available at 10.1109/SENSORS60989.2024.10785056.

</details>


### [67] [LiDAR-based Human Activity Recognition through Laplacian Spectral Analysis](https://arxiv.org/abs/2509.23255)
*Sasan Sharifipour,Constantino Álvarez Casado,Le Nguyen,Tharindu Ekanayake,Manuel Lage Cañellas,Nhi Nguyen,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 提出一种基于图谱分析的LiDAR点云人体活动识别方法，利用拉普拉斯谱特征实现高精度分类，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 为解决摄像头在隐私和光照敏感性方面的问题，探索LiDAR点云在人体活动识别中的应用。

Method: 将每个LiDAR帧映射为邻近图（epsilon-图），计算其拉普拉斯谱，提取特征值及特征向量统计特征作为姿态描述符，并通过滑动窗口的时间统计生成固定长度向量，结合支持向量机和随机森林进行分类。

Result: 在MM-Fi数据集上，采用严格被试独立协议，13类康复活动中达到94.4%准确率，27类活动中达到90.3%，超过基于骨架的基线方法。

Conclusion: 该方法提供了一种紧凑、可解释且高效的特征表示，是端到端深度学习之外的优良替代方案。

Abstract: Human Activity Recognition supports applications in healthcare,
manufacturing, and human-machine interaction. LiDAR point clouds offer a
privacy-preserving alternative to cameras and are robust to illumination. We
propose a HAR method based on graph spectral analysis. Each LiDAR frame is
mapped to a proximity graph (epsilon-graph) and the Laplacian spectrum is
computed. Eigenvalues and statistics of eigenvectors form pose descriptors, and
temporal statistics over sliding windows yield fixed vectors for classification
with support vector machines and random forests. On the MM-Fi dataset with 40
subjects and 27 activities, under a strict subject-independent protocol, the
method reaches 94.4% accuracy on a 13-class rehabilitation set and 90.3% on all
27 activities. It also surpasses the skeleton-based baselines reported for
MM-Fi. The contribution is a compact and interpretable feature set derived
directly from point cloud geometry that provides an accurate and efficient
alternative to end-to-end deep learning.

</details>


### [68] [OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2509.23258)
*Atakan Topaloglu,Kunyi Li,Michael Niemeyer,Nassir Navab,A. Murat Tekalp,Federico Tombari*

Main category: cs.CV

TL;DR: 本文提出OracleGS，一种结合生成完整性与回归保真性的稀疏视图高斯点阵化新框架，通过“提出-验证”机制利用3D感知扩散模型生成完整场景，并借助MVS模型的注意力图提供不确定性信号来指导优化，有效抑制幻觉伪影，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图新视角合成因几何歧义严重而具有病态性，现有方法在几何保真度与场景完整性之间存在权衡问题。

Method: 提出“提出-验证”框架：首先使用预训练的3D感知扩散模型生成新颖视图以构建完整场景；然后将多视图立体（MVS）模型作为3D感知oracle，利用其注意力图评估生成视图的3D不确定性；最后通过不确定性加权损失引导3D高斯点阵化模型的优化。

Result: 该方法在Mip-NeRF 360和NeRF Synthetic等多个数据集上优于当前最先进的方法，能够有效过滤幻觉伪影，同时保留欠约束区域中的合理补全。

Conclusion: OracleGS成功融合了生成模型的完整性与回归模型的几何保真性，通过引入基于多视图证据的不确定性验证机制，提升了稀疏视图条件下的新视角合成质量。

Abstract: Sparse-view novel view synthesis is fundamentally ill-posed due to severe
geometric ambiguity. Current methods are caught in a trade-off: regressive
models are geometrically faithful but incomplete, whereas generative models can
complete scenes but often introduce structural inconsistencies. We propose
OracleGS, a novel framework that reconciles generative completeness with
regressive fidelity for sparse view Gaussian Splatting. Instead of using
generative models to patch incomplete reconstructions, our
"propose-and-validate" framework first leverages a pre-trained 3D-aware
diffusion model to synthesize novel views to propose a complete scene. We then
repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the
3D uncertainties of generated views, using its attention maps to reveal regions
where the generated views are well-supported by multi-view evidence versus
where they fall into regions of high uncertainty due to occlusion, lack of
texture, or direct inconsistency. This uncertainty signal directly guides the
optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss.
Our approach conditions the powerful generative prior on multi-view geometric
evidence, filtering hallucinatory artifacts while preserving plausible
completions in under-constrained regions, outperforming state-of-the-art
methods on datasets including Mip-NeRF 360 and NeRF Synthetic.

</details>


### [69] [Learning Regional Monsoon Patterns with a Multimodal Attention U-Net](https://arxiv.org/abs/2509.23267)
*Swaib Ilias Mazumder,Manish Kumar,Aparajita Khan*

Main category: cs.CV

TL;DR: 提出了一种基于多模态深度学习的高分辨率降水分类框架，利用卫星和地球观测数据，构建了覆盖印度五邦、分辨率达1公里的新数据集，并结合注意力引导的U-Net架构与复合损失函数，在极端降雨预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于地面观测稀疏且区域变化复杂，印度季风降水预测仍具挑战性，现有模型多基于较粗网格（5-50公里），难以满足农业、水资源管理和气候风险规划对高精度降水预测的需求。

Method: 构建了一个融合七种地理空间模态（地表温度、植被指数、土壤湿度、相对湿度、风速、高程和土地利用）的1公里分辨率数据集，采用注意力引导的U-Net架构捕捉多模态时空特征，并使用焦点损失和Dice损失函数解决印度气象局定义的降雨类别不平衡问题。

Result: 所提多模态框架在各类降雨预测任务中均优于单模态基线和其他深度学习方法，尤其在极端降雨类别上性能显著提升，实现了区域季风预测的最先进水平。

Conclusion: 该研究为印度区域季风预报提供了可扩展的框架、基准数据集和最先进的结果，推动了气候韧性建设与地理空间人工智能的应用。

Abstract: Accurate monsoon rainfall prediction is vital for India's agriculture, water
management, and climate risk planning, yet remains challenging due to sparse
ground observations and complex regional variability. We present a multimodal
deep learning framework for high-resolution precipitation classification that
leverages satellite and Earth observation data. Unlike previous rainfall
prediction models based on coarse 5-50 km grids, we curate a new 1 km
resolution dataset for five Indian states, integrating seven key geospatial
modalities: land surface temperature, vegetation (NDVI), soil moisture,
relative humidity, wind speed, elevation, and land use, covering the
June-September 2024 monsoon season. Our approach uses an attention-guided U-Net
architecture to capture spatial patterns and temporal dependencies across
modalities, combined with focal and dice loss functions to handle rainfall
class imbalance defined by the India Meteorological Department (IMD).
Experiments demonstrate that our multimodal framework consistently outperforms
unimodal baselines and existing deep learning methods, especially in extreme
rainfall categories. This work contributes a scalable framework, benchmark
dataset, and state-of-the-art results for regional monsoon forecasting, climate
resilience, and geospatial AI applications in India.

</details>


### [70] [SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction](https://arxiv.org/abs/2509.23273)
*Yihao Ding,Soyeon Caren Han,Yanbei Jiang,Yan Li,Zechuan Li,Yifan Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为SynDoc的新框架，结合判别式与生成式模型，通过合成数据生成和自适应指令调优，提升领域特定视觉丰富文档的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在处理医学、金融等领域的复杂敏感文档时存在幻觉、领域适应不足和依赖大量标注数据的问题。

Method: SynDoc采用结构信息提取和领域特定查询生成来创建高质量合成数据，并通过自适应指令调优和递归推理机制优化判别与生成模型。

Result: 该框架在关键信息提取任务中表现出更高的准确性、稳定性和可扩展性，有效平衡了领域特定知识与通用世界知识的融合。

Conclusion: SynDoc为领域特定的视觉丰富文档理解提供了一个高效、精确且无需大量真实标注数据的解决方案。

Abstract: Domain-specific Visually Rich Document Understanding (VRDU) presents
significant challenges due to the complexity and sensitivity of documents in
fields such as medicine, finance, and material science. Existing Large
(Multimodal) Language Models (LLMs/MLLMs) achieve promising results but face
limitations such as hallucinations, inadequate domain adaptation, and reliance
on extensive fine-tuning datasets. This paper introduces SynDoc, a novel
framework that combines discriminative and generative models to address these
challenges. SynDoc employs a robust synthetic data generation workflow, using
structural information extraction and domain-specific query generation to
produce high-quality annotations. Through adaptive instruction tuning, SynDoc
improves the discriminative model's ability to extract domain-specific
knowledge. At the same time, a recursive inferencing mechanism iteratively
refines the output of both models for stable and accurate predictions. This
framework demonstrates scalable, efficient, and precise document understanding
and bridges the gap between domain-specific adaptation and general world
knowledge for document key information extraction tasks.

</details>


### [71] [Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing](https://arxiv.org/abs/2509.23279)
*Rohit Chowdhury,Aniruddha Bala,Rohan Jaiswal,Siddharth Roheda*

Main category: cs.CV

TL;DR: 本文提出了一种名为Vid-Freeze的新型对抗攻击方法，通过向图像添加精心设计的对抗性扰动来抑制视频生成模型中的注意力机制，从而有效阻止运动合成，同时保持图像语义不变。


<details>
  <summary>Details</summary>
Motivation: 随着图像到视频生成模型的快速发展，恶意或欺骗性视频内容的生成风险增加，现有防御方法在阻止运动生成方面仍不足，因此需要一种更有效且原理明确的保护机制。

Method: 提出Vid-Freeze方法，通过针对I2V模型的注意力机制施加对抗性扰动，抑制其运动合成能力，使生成的视频静止或接近静态，同时保持原始图像的语义完整性。

Result: 实验表明，Vid-Freeze能有效阻断多种I2V模型的运动生成，生成的视频几乎无动态变化，同时图像视觉质量保持良好，显示出对注意力机制攻击的高有效性。

Conclusion: Vid-Freeze为抵御I2V模型滥用提供了一种强有力且具有前瞻性的防御手段，揭示了基于注意力抑制的对抗攻击在多媒体安全中的潜力。

Abstract: The rapid progress of image-to-video (I2V) generation models has introduced
significant risks, enabling video synthesis from static images and facilitating
deceptive or malicious content creation. While prior defenses such as I2VGuard
attempt to immunize images, effective and principled protection to block motion
remains underexplored. In this work, we introduce Vid-Freeze - a novel
attention-suppressing adversarial attack that adds carefully crafted
adversarial perturbations to images. Our method explicitly targets the
attention mechanism of I2V models, completely disrupting motion synthesis while
preserving semantic fidelity of the input image. The resulting immunized images
generate stand-still or near-static videos, effectively blocking malicious
content creation. Our experiments demonstrate the impressive protection
provided by the proposed approach, highlighting the importance of attention
attacks as a promising direction for robust and proactive defenses against
misuse of I2V generation models.

</details>


### [72] [Seeing Through the Blur: Unlocking Defocus Maps for Deepfake Detection](https://arxiv.org/abs/2509.23289)
*Minsun Jeon,Simon S. Woo*

Main category: cs.CV

TL;DR: 提出一种基于散焦模糊的可解释深度伪造检测框架，利用光学成像中自然产生的散焦模糊特征来区分真实与合成图像，具有良好的泛化性和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，合成图像越来越逼真，传统检测方法难以应对，急需基于物理一致性的可靠检测手段。

Method: 构建散焦模糊图作为判别特征，通过分析图像中深度相关的散焦模糊特性差异来检测伪造内容，并进行三项深入的特征分析验证其有效性。

Result: 实验证明散焦模糊是一种可靠且可解释的取证线索，能有效识别多种AI生成图像，在不同数据集上表现出良好性能。

Conclusion: 基于物理成像规律的散焦模糊特征为深度伪造检测提供了鲁棒、通用且可解释的新方向，有助于提升视觉媒体的真实性验证能力。

Abstract: The rapid advancement of generative AI has enabled the mass production of
photorealistic synthetic images, blurring the boundary between authentic and
fabricated visual content. This challenge is particularly evident in deepfake
scenarios involving facial manipulation, but also extends to broader
AI-generated content (AIGC) cases involving fully synthesized scenes. As such
content becomes increasingly difficult to distinguish from reality, the
integrity of visual media is under threat. To address this issue, we propose a
physically interpretable deepfake detection framework and demonstrate that
defocus blur can serve as an effective forensic signal. Defocus blur is a
depth-dependent optical phenomenon that naturally occurs in camera-captured
images due to lens focus and scene geometry. In contrast, synthetic images
often lack realistic depth-of-field (DoF) characteristics. To capture these
discrepancies, we construct a defocus blur map and use it as a discriminative
feature for detecting manipulated content. Unlike RGB textures or
frequency-domain signals, defocus blur arises universally from optical imaging
principles and encodes physical scene structure. This makes it a robust and
generalizable forensic cue. Our approach is supported by three in-depth feature
analyses, and experimental results confirm that defocus blur provides a
reliable and interpretable cue for identifying synthetic images. We aim for our
defocus-based detection pipeline and interpretability tools to contribute
meaningfully to ongoing research in media forensics. The implementation is
publicly available at:
https://github.com/irissun9602/Defocus-Deepfake-Detection

</details>


### [73] [Seeing the Unseen in Low-light Spike Streams](https://arxiv.org/abs/2509.23304)
*Liwen Hu,Yang Li,Mianzhi Liu,Yijia Guo,Shenghao Xie,Ziluo Ding,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出了Diff-SPK，首个基于扩散模型的脉冲相机重建方法，有效利用生成先验补充低光条件下的纹理信息，并构建了首个真实低光脉冲流重建基准。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低光高速场景下难以处理严重噪声和稀疏信息的脉冲流，需更有效的重建方法。

Method: 提出Diff-SPK，采用ETFI模块聚合低光脉冲流中的稀疏信息，并作为ControlNet的条件输入；引入ETFI特征融合模块提升生成质量。

Result: 在自建的首个大规模低光脉冲流基准上验证了方法优越性，显著提升了低光高速场景下的重建质量。

Conclusion: Diff-SPK结合生成先验与条件控制生成，在低光脉冲流重建中表现出色，推动了脉冲相机在高动态视觉任务中的应用。

Abstract: Spike camera, a type of neuromorphic sensor with high-temporal resolution,
shows great promise for high-speed visual tasks. Unlike traditional cameras,
spike camera continuously accumulates photons and fires asynchronous spike
streams. Due to unique data modality, spike streams require reconstruction
methods to become perceptible to the human eye.
  However, lots of methods struggle to handle spike streams in low-light
high-speed scenarios due to severe noise and sparse information. In this work,
we propose Diff-SPK, the first diffusion-based reconstruction method for spike
camera. Diff-SPK effectively leverages generative priors to supplement texture
information in low-light conditions. Specifically, it first employs an
\textbf{E}nhanced \textbf{T}exture \textbf{f}rom Inter-spike \textbf{I}nterval
(ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI
serves as a conditioning input for ControlNet to generate the high-speed
scenes. To improve the quality of results, we introduce an ETFI-based feature
fusion module during the generation process.
  Moreover, we establish the first bona fide benchmark for the low-light spike
stream reconstruction task. It significantly surpasses existing reconstruction
datasets in scale and provides quantitative illumination information. The
performance on real low-light spike streams demonstrates the superiority of
Diff-SPK.

</details>


### [74] [Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2509.23310)
*Hao Liu,Yongjie Zheng,Yuhan Kang,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种平衡扩散引导融合（BDGF）框架，用于多模态遥感数据的分类，通过自适应模态掩码策略和多层次扩散特征引导机制解决预训练中模态不平衡问题，并在四个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练多模态DDPM存在模态不平衡问题，且如何有效利用扩散模型特征来指导互补性特征提取仍不明确。

Method: 提出BDGF框架：采用自适应模态掩码策略实现模态平衡；利用扩散特征在CNN、Mamba和Transformer分支中进行分层引导，结合特征融合、组通道注意力和交叉注意力机制；引入互学习策略以对齐各子网络的概率熵和特征相似性，增强分支协作。

Result: 在四个多模态遥感数据集上的实验表明，该方法在土地覆盖分类任务中显著优于现有方法，展现出更强的特征提取与融合能力。

Conclusion: BDGF有效解决了多模态DDPM中的模态不平衡问题，并通过扩散特征引导的多分支协同学习提升了分类性能，具有良好的应用前景。

Abstract: Deep learning-based techniques for the analysis of multimodal remote sensing
data have become popular due to their ability to effectively integrate
complementary spatial, spectral, and structural information from different
sensors. Recently, denoising diffusion probabilistic models (DDPMs) have
attracted attention in the remote sensing community due to their powerful
ability to capture robust and complex spatial-spectral distributions. However,
pre-training multimodal DDPMs may result in modality imbalance, and effectively
leveraging diffusion features to guide complementary diversity feature
extraction remains an open question. To address these issues, this paper
proposes a balanced diffusion-guided fusion (BDGF) framework that leverages
multimodal diffusion features to guide a multi-branch network for land-cover
classification. Specifically, we propose an adaptive modality masking strategy
to encourage the DDPMs to obtain a modality-balanced rather than spectral
image-dominated data distribution. Subsequently, these diffusion features
hierarchically guide feature extraction among CNN, Mamba, and transformer
networks by integrating feature fusion, group channel attention, and
cross-attention mechanisms. Finally, a mutual learning strategy is developed to
enhance inter-branch collaboration by aligning the probability entropy and
feature similarity of individual subnetworks. Extensive experiments on four
multimodal remote sensing datasets demonstrate that the proposed method
achieves superior classification performance. The code is available at
https://github.com/HaoLiu-XDU/BDGF.

</details>


### [75] [Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning](https://arxiv.org/abs/2509.23311)
*Haorui Yu,Qiufeng Yi,Yijia Chu,Yang Zhao*

Main category: cs.CV

TL;DR: 提出诊断框架以评估视觉语言模型在火主题文化图像中的分类与解释能力，揭示模型对西方节日识别较好，但对非西方传统和紧急场景存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型看似具备文化理解能力，实则依赖表面模式匹配，缺乏真实的文化推理能力，可能导致误判风险。

Method: 构建包含西方节日、非西方传统和紧急场景的测试集，通过分类和生成解释的方式分析多个模型的表现。

Result: 模型能正确识别显著的西方节日，但在代表性不足的文化活动上表现差，常给出模糊标签或将紧急情况误判为庆祝活动。

Conclusion: 暴露了模型依赖符号捷径的风险，表明仅用准确率评估文化理解不足，需发展更全面、可解释且公平的多模态评估方法。

Abstract: Vision-Language Models (VLMs) often appear culturally competent but rely on
superficial pattern matching rather than genuine cultural understanding. We
introduce a diagnostic framework to probe VLM reasoning on fire-themed cultural
imagery through both classification and explanation analysis. Testing multiple
models on Western festivals, non-Western traditions, and emergency scenes
reveals systematic biases: models correctly identify prominent Western
festivals but struggle with underrepresented cultural events, frequently
offering vague labels or dangerously misclassifying emergencies as
celebrations. These failures expose the risks of symbolic shortcuts and
highlight the need for cultural evaluation beyond accuracy metrics to ensure
interpretable and fair multimodal systems.

</details>


### [76] [C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection](https://arxiv.org/abs/2509.23316)
*Siheng Wang,Zhengdao Li,Yanshu Li,Canran Xiao,Haibo Zhan,Zhengtao Yao,Xuzhi Zhang,Jiale Kang,Linshan Li,Weiming Liu,Zhikang Dong,Jifeng Shen,Junhao Dong,Qiang Sun,Piotr Koniusz*

Main category: cs.CV

TL;DR: 本文提出了一种名为C3-OWD的课程式跨模态对比学习框架，旨在同时提升目标检测在不可见类别上的泛化能力和恶劣环境下的鲁棒性。通过两个阶段分别利用RGBT数据增强鲁棒性以及视觉-语言对齐提升泛化能力，并引入指数移动平均（EMA）机制缓解灾难性遗忘问题。实验表明该方法在多个基准上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放世界检测中难以同时兼顾对未见类别的泛化能力和在极端环境下的鲁棒性，本文旨在解决这一权衡问题。

Method: 提出C3-OWD框架：第一阶段使用RGBT数据进行预训练以增强鲁棒性；第二阶段通过视觉-语言对齐提升对新类别的泛化能力；引入EMA机制防止跨阶段学习中的性能退化。

Result: 在FLIR、OV-COCO和OV-LVIS数据集上验证了方法的有效性，分别取得80.1 AP^50、48.6 AP^50_Novel和35.7 mAP_r的性能，显著优于现有方法。

Conclusion: C3-OWD成功融合了可见光-红外检测的鲁棒性和开放世界检测的泛化能力，为实际应用中复杂场景下的目标检测提供了有效解决方案。

Abstract: Object detection has advanced significantly in the closed-set setting, but
real-world deployment remains limited by two challenges: poor generalization to
unseen categories and insufficient robustness under adverse conditions. Prior
research has explored these issues separately: visible-infrared detection
improves robustness but lacks generalization, while open-world detection
leverages vision-language alignment strategy for category diversity but
struggles under extreme environments. This trade-off leaves robustness and
diversity difficult to achieve simultaneously. To mitigate these issues, we
propose \textbf{C3-OWD}, a curriculum cross-modal contrastive learning
framework that unifies both strengths. Stage~1 enhances robustness by
pretraining with RGBT data, while Stage~2 improves generalization via
vision-language alignment. To prevent catastrophic forgetting between two
stages, we introduce an Exponential Moving Average (EMA) mechanism that
theoretically guarantees preservation of pre-stage performance with bounded
parameter lag and function consistency. Experiments on FLIR, OV-COCO, and
OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$
AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\text{Novel}}$ on OV-COCO, and $35.7$
mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness
and diversity evaluations. Code available at:
https://github.com/justin-herry/C3-OWD.git.

</details>


### [77] [Spatial-Spectral Binarized Neural Network for Panchromatic and Multi-spectral Images Fusion](https://arxiv.org/abs/2509.23321)
*Yizhen Jiang,Mengting Ma,Anqi Zhu,Xiaowen Ma,Jiaxin Li,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的二值化神经网络方法S2BNet用于遥感图像全色锐化，通过设计空间-光谱二值化卷积（S2B-Conv），有效缓解了光谱失真和空间特征退化问题，在保证高性能的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在全色锐化中性能优异但计算复杂度高，难以部署于资源受限设备；现有二值化方法在处理PAN和LR-MS图像时易导致光谱失真和轮廓退化，因此需设计专用的轻量高效二值化网络。

Method: 提出定制化的空间-光谱二值化卷积（S2B-Conv），包含光谱重分布机制（SRM）和Gabor空间特征放大器（GSFA）：SRM通过动态学习的仿射变换调整光谱分布，缓解光谱失真；GSFA利用多频率、多角度Gabor滤波增强多尺度各向异性空间特征。多个S2B-Conv构成S2BNet网络。

Result: 实验表明，S2BNet在多个数据集上实现了良好的定性和定量结果，相比传统二值化模型显著减少了光谱畸变和轮廓模糊，同时保持了极高的计算效率，适合在资源受限设备上部署。

Conclusion: S2BNet验证了二值神经网络在遥感全色锐化任务中的可行性，通过专门设计的S2B-Conv模块有效解决了二值化带来的光谱与空间特征退化问题，为轻量化遥感图像融合提供了新思路。

Abstract: Remote sensing pansharpening aims to reconstruct spatial-spectral properties
during the fusion of panchromatic (PAN) images and low-resolution
multi-spectral (LR-MS) images, finally generating the high-resolution
multi-spectral (HR-MS) images. Although deep learning-based models have
achieved excellent performance, they often come with high computational
complexity, which hinder their applications on resource-limited devices. In
this paper, we explore the feasibility of applying the binary neural network
(BNN) to pan-sharpening. Nevertheless, there are two main issues with
binarizing pan-sharpening models: (i) the binarization will cause serious
spectral distortion due to the inconsistent spectral distribution of the
PAN/LR-MS images; (ii) the common binary convolution kernel is difficult to
adapt to the multi-scale and anisotropic spatial features of remote sensing
objects, resulting in serious degradation of contours. To address the above
issues, we design the customized spatial-spectral binarized convolution
(S2B-Conv), which is composed of the Spectral-Redistribution Mechanism (SRM)
and Gabor Spatial Feature Amplifier (GSFA). Specifically, SRM employs an affine
transformation, generating its scaling and bias parameters through a dynamic
learning process. GSFA, which randomly selects different frequencies and angles
within a preset range, enables to better handle multi-scale and-directional
spatial features. A series of S2B-Conv form a brand-new binary network for
pan-sharpening, dubbed as S2BNet. Extensive quantitative and qualitative
experiments have shown our high-efficiency binarized pan-sharpening method can
attain a promising performance.

</details>


### [78] [Decoupling Reasoning and Perception: An LLM-LMM Framework for Faithful Visual Reasoning](https://arxiv.org/abs/2509.23322)
*Hongrui Jia,Chaoya Jiang,Shikun Zhang,Wei Ye*

Main category: cs.CV

TL;DR: 提出一种无需训练的视觉推理框架，通过分离推理与感知过程，利用大语言模型主导推理并动态查询多模态模型获取视觉信息，有效减少脱离图像内容的错误推理，提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在长推理链中过度依赖文本逻辑，逐渐脱离视觉信息，导致推理偏离图像内容，产生错误结论。

Method: 将推理与感知过程解耦：由大语言模型负责高层推理，通过提问方式向多模态模型按需获取视觉信息；多模态模型仅作为视觉问答引擎提供感知细节，整个框架无需训练或架构修改。

Result: 实验表明该方法显著减少了脱离视觉依据的推理步骤，提升了推理过程的保真度和最终准确性，在多个评测中表现出色。

Conclusion: 所提出的轻量级、即插即用框架能有效控制视觉推理过程，增强模型对图像内容的依赖，改善长链视觉推理的可靠性。

Abstract: Significant advancements in the reasoning capabilities of Large Language
Models (LLMs) are now driven by test-time scaling laws, particularly those
leveraging extended Chain-of-Thought (CoT) reasoning. Inspired by these
breakthroughs, researchers have extended these paradigms to Large Multimodal
Models (LMMs). However, a critical limitation emerges: as their reasoning
chains extend, LMMs increasingly rely on textual logic, progressively losing
grounding in the underlying visual information. This leads to reasoning paths
that diverge from the image content, culminating in erroneous conclusions. To
address this, we introduce a strikingly simple yet effective training-free
visual-reasoning pipeline. The core concept is to decouple the reasoning and
perception processes. A powerful LLM orchestrates the high-level reasoning,
strategically interrogating a LMM to extract specific visual information
required for its logical chain. The LMM, in turn, functions exclusively as a
visual question-answering engine, supplying the necessary perceptual details on
demand. This lightweight, plug-and-play approach requires no additional
training or architectural changes. Comprehensive evaluations validate that our
framework effectively governs the visual reasoning process, leading to a
significant reduction in visually-unfounded reasoning steps and a substantial
improvement in reasoning fidelity.

</details>


### [79] [DDP: Dual-Decoupled Prompting for Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2509.23335)
*Kaile Du,Zihan Ye,Junzhou Xie,Fan Lyu,Yixi Shen,Yuyang Li,Miaoxuan Zhu,Fuyuan Hu,Ling Shao,Guangcan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Dual-Decoupled Prompting (DDP) 的框架，用于解决多标签类增量学习中的语义混淆和真假阳性混淆问题，无需回放且参数高效，在标准基准上首次超过80% mAP和70% F1。


<details>
  <summary>Details</summary>
Motivation: 由于共现类别导致的语义混淆和部分标注导致的真阴性-假阳性混淆，基于提示的方法在多标签类增量学习中表现不佳。

Method: DDP为每个类别分配特定的正负提示以解耦语义，并引入渐进置信度解耦（PCD）策略来抑制假阳性；冻结过去的提示作为知识锚点，并采用层间提示提升效率。

Result: 在MS-COCO和PASCAL VOC数据集上，DDP consistently outperforms prior methods，在标准MS-COCO B40-C10基准下成为首个mAP超过80%、F1超过70%的无回放MLCIL方法。

Conclusion: DDP有效解决了MLCIL中的关键挑战，实现了高效、无需回放的多标签类增量学习，性能显著优于现有方法。

Abstract: Prompt-based methods have shown strong effectiveness in single-label
class-incremental learning, but their direct extension to multi-label
class-incremental learning (MLCIL) performs poorly due to two intrinsic
challenges: semantic confusion from co-occurring categories and
true-negative-false-positive confusion caused by partial labeling. We propose
Dual-Decoupled Prompting (DDP), a replay-free and parameter-efficient framework
that explicitly addresses both issues. DDP assigns class-specific
positive-negative prompts to disentangle semantics and introduces Progressive
Confidence Decoupling (PCD), a curriculum-inspired decoupling strategy that
suppresses false positives. Past prompts are frozen as knowledge anchors, and
interlayer prompting enhances efficiency. On MS-COCO and PASCAL VOC, DDP
consistently outperforms prior methods and is the first replay-free MLCIL
approach to exceed 80% mAP and 70% F1 under the standard MS-COCO B40-C10
benchmark.

</details>


### [80] [LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning](https://arxiv.org/abs/2509.23339)
*Bin Wu,Yahui Liu,Chi Zhang,Yao Zhao,Wei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于在线强化学习的盲脸修复框架LRPO，通过复合奖励函数、真实标签引导的可能性正则化和噪声级别优势分配，显著提升了修复质量和保真度。


<details>
  <summary>Details</summary>
Motivation: 盲脸修复面临解空间大、细节缺失和身份模糊等问题，现有方法难以平衡感知质量和保真度。

Method: 提出Likelihood-Regularized Policy Optimization (LRPO)框架，结合在线强化学习，采用复合奖励函数、真实标签引导的可能性正则化和噪声级别优势分配策略优化修复过程。

Result: 实验表明，LRPO在低质量输入上显著优于基线方法，实现了最先进的面部修复性能。

Conclusion: LRPO有效解决了强化学习在盲脸修复中的不兼容问题，兼顾了生成质量与真实性，推动了BFR的发展。

Abstract: Blind Face Restoration (BFR) encounters inherent challenges in exploring its
large solution space, leading to common artifacts like missing details and
identity ambiguity in the restored images. To tackle these challenges, we
propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the
first to apply online reinforcement learning (RL) to the BFR task. LRPO
leverages rewards from sampled candidates to refine the policy network,
increasing the likelihood of high-quality outputs while improving restoration
performance on low-quality inputs. However, directly applying RL to BFR creates
incompatibility issues, producing restoration results that deviate
significantly from the ground truth. To balance perceptual quality and
fidelity, we propose three key strategies: 1) a composite reward function
tailored for face restoration assessment, 2) ground-truth guided likelihood
regularization, and 3) noise-level advantage assignment. Extensive experiments
demonstrate that our proposed LRPO significantly improves the face restoration
quality over baseline methods and achieves state-of-the-art performance.

</details>


### [81] [DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice](https://arxiv.org/abs/2509.23344)
*Zijie Meng,Jin Hao,Xiwei Dai,Yang Feng,Jiaxiang Liu,Bin Feng,Huikai Wu,Xiaotang Gai,Hengchuan Zhu,Tianxiang Hu,Yangyang Wu,Hongxia Xu,Jin Li,Jun Xiao,Xiaoqiang Liu,Joey Tianyi Zhou,Fudong Zhu,Zhihe Zhao,Lunguo Xia,Bing Fang,Jimeng Sun,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DentVLM的多模态视觉-语言模型，用于专家级口腔疾病诊断，基于大规模双语数据集训练，在多种成像模式和诊断任务中显著优于现有模型，并在临床研究中表现出超越初级甚至部分高级牙医的诊断能力，同时提升诊疗效率。


<details>
  <summary>Details</summary>
Motivation: 现有的AI模型通常只能处理单一任务，难以满足临床牙科实践中复杂的多模态需求，因此需要开发一种能够综合理解多种口腔影像并进行准确诊断的通用模型。

Method: 开发了DentVLM，一个基于110,447张图像和246万对视觉问答数据的大规模双语多模态视觉-语言模型，可解析七种2D口腔影像模态，涵盖36项诊断任务，并在真实临床环境中与25名牙医对比评估其性能。

Result: DentVLM在口腔疾病诊断准确率上比现有领先模型高出19.6%，在错颌畸形诊断上高出27.9%；在涉及1,946名患者和3,105个问答对的临床研究中，其表现超过13名初级牙医中的21项任务和12名高级牙医中的12项任务；与牙医协作时，可将初级牙医水平提升至高级，并减少15-22%的诊断时间。

Conclusion: DentVLM是一种强大的临床决策支持工具，有望提升基层牙科诊疗水平，缓解医患资源不平衡，并推动专业医疗知识的普及。

Abstract: Diagnosing and managing oral diseases necessitate advanced visual
interpretation across diverse imaging modalities and integrated information
synthesis. While current AI models excel at isolated tasks, they often fall
short in addressing the complex, multimodal requirements of comprehensive
clinical dental practice. Here we introduce DentVLM, a multimodal
vision-language model engineered for expert-level oral disease diagnosis.
DentVLM was developed using a comprehensive, large-scale, bilingual dataset of
110,447 images and 2.46 million visual question-answering (VQA) pairs. The
model is capable of interpreting seven 2D oral imaging modalities across 36
diagnostic tasks, significantly outperforming leading proprietary and
open-source models by 19.6% higher accuracy for oral diseases and 27.9% for
malocclusions. In a clinical study involving 25 dentists, evaluating 1,946
patients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic
performance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12
senior dentists on 12 of 36 tasks. When integrated into a collaborative
workflow, DentVLM elevated junior dentists' performance to senior levels and
reduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM
exhibited promising performance across three practical utility scenarios,
including home-based dental health management, hospital-based intelligent
diagnosis and multi-agent collaborative interaction. These findings establish
DentVLM as a robust clinical decision support tool, poised to enhance primary
dental care, mitigate provider-patient imbalances, and democratize access to
specialized medical expertise within the field of dentistry.

</details>


### [82] [Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling](https://arxiv.org/abs/2509.23352)
*Xiaolong Fu,Lichen Ma,Zipeng Guo,Gaojing Zhou,Chongxiao Wang,ShiPing Dong,Shizhe Zhou,Shizhe Zhou,Ximan Liu,Jingling Fu,Tan Lit Sin,Yu Shi,Zhen Chen,Junshi Huang,Jason Li*

Main category: cs.CV

TL;DR: 本文提出Dynamic-TreeRPO，通过树结构搜索与动态噪声强度的滑动窗口采样策略，提升文本到图像生成中强化学习的探索效率和生成质量，并结合监督微调与强化学习范式提出LayerTuning-RL，在保持计算成本不变的同时显著提升语义一致性、视觉保真度和人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的文本到图像生成方法因采样组变化小导致探索不充分且采样效率低，需提高探索多样性并降低计算开销。

Method: 提出Dynamic-TreeRPO，采用树结构搜索与动态噪声强度的滑动窗口采样，共享前缀路径以分摊计算开销；设计GRPO引导优化与受限SDE采样；将监督微调损失重构为动态加权进度奖励模型，结合自适应裁剪边界形成LayerTuning-RL框架。

Result: 在HPS-v2.1、PickScore和ImageReward基准上分别超越当前最优方法4.9%、5.91%和8.66%，训练效率提升近50%。

Conclusion: Dynamic-TreeRPO与LayerTuning-RL有效提升了文本到图像生成的质量与训练效率，实现了更高效、多样化的策略探索与多目标优化的融合。

Abstract: The integration of Reinforcement Learning (RL) into flow matching models for
text-to-image (T2I) generation has driven substantial advances in generation
quality. However, these gains often come at the cost of exhaustive exploration
and inefficient sampling strategies due to slight variation in the sampling
group. Building on this insight, we propose Dynamic-TreeRPO, which implements
the sliding-window sampling strategy as a tree-structured search with dynamic
noise intensities along depth. We perform GRPO-guided optimization and
constrained Stochastic Differential Equation (SDE) sampling within this tree
structure. By sharing prefix paths of the tree, our design effectively
amortizes the computational overhead of trajectory search. With well-designed
noise intensities for each tree layer, Dynamic-TreeRPO can enhance the
variation of exploration without any extra computational cost. Furthermore, we
seamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within
Dynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the
loss function of SFT as a dynamically weighted Progress Reward Model (PRM)
rather than a separate pretraining method. By associating this weighted PRM
with dynamic-adaptive clipping bounds, the disruption of exploration process in
Dynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and
the LayerTuning-RL paradigm, our model dynamically explores a diverse search
space along effective directions. Compared to existing baselines, our approach
demonstrates significant superiority in terms of semantic consistency, visual
fidelity, and human preference alignment on established benchmarks, including
HPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA
by $4.9\%$, $5.91\%$, and $8.66\%$ on those benchmarks, respectively, while
improving the training efficiency by nearly $50\%$.

</details>


### [83] [Test-time Uncertainty Estimation for Medical Image Registration via Transformation Equivariance](https://arxiv.org/abs/2509.23355)
*Lin Tian,Xiaoling Hu,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出了一种无需重新训练的测试时不确定性估计框架，适用于任何预训练的医学图像配准网络，通过变换等变性分析预测方差，分解为内在离散和偏差抖动，有效识别配准误差高的区域。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性估计方法需要修改架构或重新训练，难以应用于已有的预训练配准网络，限制了其在临床中的安全使用。

Method: 基于配准的变换等变性，通过输入空间扰动下网络预测的方差分析，推导出扰动-based不确定性分解，包含内在离散和偏差抖动两项。

Result: 在四种解剖结构和多个配准模型上验证，不确定性图与配准误差高度相关，能有效标记需警惕的区域。

Conclusion: 该框架可将任何预训练配准网络转化为测试时的风险感知工具，推动医学图像配准向临床安全应用迈进。

Abstract: Accurate image registration is essential for downstream applications, yet
current deep registration networks provide limited indications of whether and
when their predictions are reliable. Existing uncertainty estimation
strategies, such as Bayesian methods, ensembles, or MC dropout, require
architectural changes or retraining, limiting their applicability to pretrained
registration networks. Instead, we propose a test-time uncertainty estimation
framework that is compatible with any pretrained networks. Our framework is
grounded in the transformation equivariance property of registration, which
states that the true mapping between two images should remain consistent under
spatial perturbations of the input. By analyzing the variance of network
predictions under such perturbations, we derive a theoretical decomposition of
perturbation-based uncertainty in registration. This decomposition separates
into two terms: (i) an intrinsic spread, reflecting epistemic noise, and (ii) a
bias jitter, capturing how systematic error drifts under perturbations. Across
four anatomical structures (brain, cardiac, abdominal, and lung) and multiple
registration models (uniGradICON, SynthMorph), the uncertainty maps correlate
consistently with registration errors and highlight regions requiring caution.
Our framework turns any pretrained registration network into a risk-aware tool
at test time, placing medical image registration one step closer to safe
deployment in clinical and large-scale research settings.

</details>


### [84] [GRAPE: Let GPRO Supervise Query Rewriting by Ranking for Retrieval](https://arxiv.org/abs/2509.23370)
*Zhaohua Zhang,Jianhuan Zhuo,Muxi Chen,Chenchen Zhao,Wenyu Jiang,Tianwen Jiang,Mingyang Chen,Yu Tang,Qiuyong Xiao,Jihong Zhang,Zhixun Su*

Main category: cs.CV

TL;DR: 本文提出了GRAPE，一种即插即用的检索引导查询重写方法，通过引入排序感知的策略优化来增强CLIP在分布偏移场景下的检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在输入分布偏离训练数据时表现不佳，现有基于大语言模型的查询重写方法因缺乏监督信号难以生成最优查询。

Method: 提出GRAPE方法，利用排序感知的策略优化（GRPO），结合语义相似度和基于语料库相对排序的奖励机制，指导大语言模型生成更匹配检索器训练分布的查询。

Result: 在多种分布偏移场景（多语言、长文本、跨模态）下实验表明，GRAPE显著提升检索性能，在Recall@10上平均提高4.9%。

Conclusion: GRAPE有效缓解了CLIP在分布偏移下的性能下降问题，通过引入排序信号解决了评分膨胀问题，具有良好的通用性和实用性。

Abstract: The CLIP model has become a cornerstone of large-scale retrieval systems by
aligning text and image data in a unified embedding space. Despite its
simplicity and efficiency, CLIP struggles when applied to tasks whose input
distributions diverge from its training corpus, such as queries with
multilingual, long-form, or multimodal differences. To avoid costly retraining,
existing methods mainly adopt query-rewriting strategies with large language
models (LLMs), aiming to mitigate distribution gaps at the query level.
However, due to the lack of supervision signals, LLMs fail to generate the
optimal one that fits the training distribution. We address this challenge with
GRAPE (Grouped Ranking-Aware Policy Optimization Enhancement), a plug-and-play
enhancement approach that incorporates ranking signals into retrieval-guided
query rewriting with LLMs. Intuitively, GRAPE proposes to leverage GRPO to
bridge distributional differences -- including length, multilingual, and
modality shifts -- by transforming queries into forms better aligned with the
retriever's training distribution. However, our preliminary experiment finds
that naively finetuning LLM with similarity scores can lead to score inflation,
where nearly all candidates are assigned unexpectedly high scores regardless of
their true relevance. To address score inflation, we propose a corpus-relative
ranking-based reward, which explicitly aligns optimization with ranking metrics
while suppressing spurious score inflation. Extensive experiments demonstrate
that GRAPE consistently improves retrieval performance under distributional
shifts -- including multilingual differences (Flickr30k-CN, CVLUE, XM3600),
length differences (Wikipedia), and multimodal differences (CIRR) -- achieving
an average improvement of 4.9\% in Recall\@10. The code is available at
https://github.com/Chinese0123456/GRAPE.git

</details>


### [85] [CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation](https://arxiv.org/abs/2509.23375)
*Yifan Yang,Yuxiang Yan,Boda Liu,Jian Pu*

Main category: cs.CV

TL;DR: 提出了一种名为CasPoinTr的级联网络框架，结合知识蒸馏技术，用于点云补全，有效提升不完整点云的整体形状恢复和细节重建性能。


<details>
  <summary>Details</summary>
Motivation: 现实环境中采集的点云常因传感器限制、遮挡等因素而不完整，需有效方法进行补全。

Method: 采用级联网络结构，分为形状重建和融合补全两个阶段，并引入知识蒸馏，由在稠密点云上训练的教师模型向学生模型传递补全知识。

Result: 在ShapeNet-55数据集不同难度设置下实验表明，CasPoinTr在形状恢复和细节保持方面优于现有方法。

Conclusion: 级联结构与知识蒸馏的结合能有效提升点云补全中全局形状感知与局部细节重建能力。

Abstract: Point clouds collected from real-world environments are often incomplete due
to factors such as limited sensor resolution, single viewpoints, occlusions,
and noise. These challenges make point cloud completion essential for various
applications. A key difficulty in this task is predicting the overall shape and
reconstructing missing regions from highly incomplete point clouds. To address
this, we introduce CasPoinTr, a novel point cloud completion framework using
cascaded networks and knowledge distillation. CasPoinTr decomposes the
completion task into two synergistic stages: Shape Reconstruction, which
generates auxiliary information, and Fused Completion, which leverages this
information alongside knowledge distillation to generate the final output.
Through knowledge distillation, a teacher model trained on denser point clouds
transfers incomplete-complete associative knowledge to the student model,
enhancing its ability to estimate the overall shape and predict missing
regions. Together, the cascaded networks and knowledge distillation enhance the
model's ability to capture global shape context while refining local details,
effectively bridging the gap between incomplete inputs and complete targets.
Experiments on ShapeNet-55 under different difficulty settings demonstrate that
CasPoinTr outperforms existing methods in shape recovery and detail
preservation, highlighting the effectiveness of our cascaded structure and
distillation strategy.

</details>


### [86] [UniPose: Unified Cross-modality Pose Prior Propagation towards RGB-D data for Weakly Supervised 3D Human Pose Estimation](https://arxiv.org/abs/2509.23376)
*Jinghong Zheng,Changlong Jiang,Jiaqi Li,Haohong Kuang,Hang Xu,Tingbing Yan*

Main category: cs.CV

TL;DR: UniPose是一种用于弱监督3D人体姿态估计的统一跨模态姿态先验传播方法，利用未标注的单视图RGB-D序列，通过自监督学习将2D姿态标注从大规模RGB数据集迁移到3D领域，无需繁琐的3D关键点标注。


<details>
  <summary>Details</summary>
Motivation: 解决3D人体姿态估计中缺乏标注数据的问题，避免多视角相机标定和合成到真实数据迁移的挑战。

Method: 利用现成的2D姿态估计作为点云网络的弱监督信号，结合时空约束（如身体对称性和关节运动），并通过2D到3D反投影损失和跨模态交互增强训练；采用锚点到关节预测方法进行3D提升。

Result: 在CMU Panoptic和ITOP数据集上，UniPose性能与全监督方法相当；结合大规模无标签数据（如NTU RGB+D 60）可进一步提升在复杂条件下的表现，并在3D提升任务上达到最先进水平。

Conclusion: UniPose有效实现了从2D到3D姿态估计的知识迁移，无需3D标注，具有良好的实用性和扩展潜力。

Abstract: In this paper, we present UniPose, a unified cross-modality pose prior
propagation method for weakly supervised 3D human pose estimation (HPE) using
unannotated single-view RGB-D sequences (RGB, depth, and point cloud data).
UniPose transfers 2D HPE annotations from large-scale RGB datasets (e.g., MS
COCO) to the 3D domain via self-supervised learning on easily acquired RGB-D
sequences, eliminating the need for labor-intensive 3D keypoint annotations.
This approach bridges the gap between 2D and 3D domains without suffering from
issues related to multi-view camera calibration or synthetic-to-real data
shifts. During training, UniPose leverages off-the-shelf 2D pose estimations as
weak supervision for point cloud networks, incorporating spatial-temporal
constraints like body symmetry and joint motion. The 2D-to-3D back-projection
loss and cross-modality interaction further enhance this process. By treating
the point cloud network's 3D HPE results as pseudo ground truth, our
anchor-to-joint prediction method performs 3D lifting on RGB and depth
networks, making it more robust against inaccuracies in 2D HPE results compared
to state-of-the-art methods. Experiments on CMU Panoptic and ITOP datasets show
that UniPose achieves comparable performance to fully supervised methods.
Incorporating large-scale unlabeled data (e.g., NTU RGB+D 60) enhances its
performance under challenging conditions, demonstrating its potential for
practical applications. Our proposed 3D lifting method also achieves
state-of-the-art results.

</details>


### [87] [Generative Modeling of Shape-Dependent Self-Contact Human Poses](https://arxiv.org/abs/2509.23393)
*Takehiko Ohkawa,Jihyun Lee,Shunsuke Saito,Jason Saragih,Fabian Prado,Yichen Xu,Shoou-I Yu,Ryosuke Furuta,Yoichi Sato,Takaaki Shiratori*

Main category: cs.CV

TL;DR: 提出了首个包含精确身体形状注册的大规模自接触数据集Goliath-SC，并基于身体部分的潜在扩散模型引入了依赖于身体形状参数的自接触先验，提升了单视角下人体姿态估计中自接触的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自接触数据集缺乏多样的自接触姿态和精确的身体形状信息，限制了对自接触姿态与身体形状关系的深入分析。

Method: 构建了包含383K自接触姿态和130个受试者的大规模数据集Goliath-SC，提出基于身体部分的潜在扩散模型，并引入自注意力机制，将身体形状参数作为条件建模自接触先验，进而将其融入单视角人体姿态估计中以优化接触姿态。

Result: 实验证明，结合身体形状条件能有效提升自接触姿态分布的建模效果，并显著改善单视角姿态估计中的自接触精度。

Conclusion: 身体形状是建模自接触姿态的关键因素，所提出的形状条件自接触先验有助于更准确地估计真实场景中的自接触人体姿态。

Abstract: One can hardly model self-contact of human poses without considering
underlying body shapes. For example, the pose of rubbing a belly for a person
with a low BMI leads to penetration of the hand into the belly for a person
with a high BMI. Despite its relevance, existing self-contact datasets lack the
variety of self-contact poses and precise body shapes, limiting conclusive
analysis between self-contact poses and shapes. To address this, we begin by
introducing the first extensive self-contact dataset with precise body shape
registration, Goliath-SC, consisting of 383K self-contact poses across 130
subjects. Using this dataset, we propose generative modeling of self-contact
prior conditioned by body shape parameters, based on a body-part-wise latent
diffusion with self-attention. We further incorporate this prior into
single-view human pose estimation while refining estimated poses to be in
contact. Our experiments suggest that shape conditioning is vital to the
successful modeling of self-contact pose distribution, hence improving
single-view pose estimation in self-contact.

</details>


### [88] [WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving](https://arxiv.org/abs/2509.23402)
*Ziyue Zhu,Zhanqian Wu,Zhenxin Zhu,Lijun Zhou,Haiyang Sun,Bing Wan,Kun Ma,Guang Chen,Hangjun Ye,Jin Xie,jian Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为WorldSplat的前馈框架，用于生成4D驾驶场景，能够生成高质量、时空一致的多视角驾驶视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法在驾驶场景生成中存在3D一致性不足和视点覆盖稀疏的问题，而重建方法缺乏生成能力，因此需要一种兼顾生成与高质量新视角合成的方法。

Method: 首先引入4D感知的潜在扩散模型生成像素对齐的4D高斯表示，然后使用增强的视频扩散模型优化渲染出的新视角视频。

Result: 在基准数据集上的实验表明，WorldSplat能有效生成高保真、时空一致的多视角驾驶视频。

Conclusion: WorldSplat成功结合了生成与重建的优势，为自动驾驶提供了可扩展且可控的高质量训练数据生成方案。

Abstract: Recent advances in driving-scene generation and reconstruction have
demonstrated significant potential for enhancing autonomous driving systems by
producing scalable and controllable training data. Existing generation methods
primarily focus on synthesizing diverse and high-fidelity driving videos;
however, due to limited 3D consistency and sparse viewpoint coverage, they
struggle to support convenient and high-quality novel-view synthesis (NVS).
Conversely, recent 3D/4D reconstruction approaches have significantly improved
NVS for real-world driving scenes, yet inherently lack generative capabilities.
To overcome this dilemma between scene generation and reconstruction, we
propose \textbf{WorldSplat}, a novel feed-forward framework for 4D
driving-scene generation. Our approach effectively generates consistent
multi-track videos through two key steps: ((i)) We introduce a 4D-aware latent
diffusion model integrating multi-modal information to produce pixel-aligned 4D
Gaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel
view videos rendered from these Gaussians using a enhanced video diffusion
model. Extensive experiments conducted on benchmark datasets demonstrate that
\textbf{WorldSplat} effectively generates high-fidelity, temporally and
spatially consistent multi-track novel view driving videos.

</details>


### [89] [Enhanced Fracture Diagnosis Based on Critical Regional and Scale Aware in YOLO](https://arxiv.org/abs/2509.23408)
*Yuyang Sun,Junchuan Yu,Cuiming Zou*

Main category: cs.CV

TL;DR: 本文提出了一种改进的YOLO模型Fracture-YOLO，用于骨折检测，通过引入CRSelector注意力机制和Scale-Aware头结构，显著提升了检测精度，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统骨折诊断依赖医生经验，速度和准确性受限，亟需一种高效、准确的自动化检测方法。

Method: 在YOLO框架基础上引入CRSelector模块以利用全局纹理信息关注骨折关键区域，并设计Scale-Aware（ScA）头动态调整多尺度特征权重，提升多尺度骨折检测能力。

Result: 实验结果显示，相比基线模型，Fracture-YOLO的mAP50和mAP50-95分别提升了4和3，显著提高了检测精度。

Conclusion: Fracture-YOLO通过引入CRSelector和ScA模块，在骨折检测任务中表现出优越性能，达到了当前最优水平，具有临床辅助诊断的应用潜力。

Abstract: Fracture detection plays a critical role in medical imaging analysis,
traditional fracture diagnosis relies on visual assessment by experienced
physicians, however the speed and accuracy of this approach are constrained by
the expertise. With the rapid advancements in artificial intelligence, deep
learning models based on the YOLO framework have been widely employed for
fracture detection, demonstrating significant potential in improving diagnostic
efficiency and accuracy. This study proposes an improved YOLO-based model,
termed Fracture-YOLO, which integrates novel Critical-Region-Selector Attention
(CRSelector) and Scale-Aware (ScA) heads to further enhance detection
performance. Specifically, the CRSelector module utilizes global texture
information to focus on critical features of fracture regions. Meanwhile, the
ScA module dynamically adjusts the weights of features at different scales,
enhancing the model's capacity to identify fracture targets at multiple scales.
Experimental results demonstrate that, compared to the baseline model,
Fracture-YOLO achieves a significant improvement in detection precision, with
mAP50 and mAP50-95 increasing by 4 and 3, surpassing the baseline model and
achieving state-of-the-art (SOTA) performance.

</details>


### [90] [FracDetNet: Advanced Fracture Detection via Dual-Focus Attention and Multi-scale Calibration in Medical X-ray Imaging](https://arxiv.org/abs/2509.23416)
*Yuyang Sun,Cuiming Zou*

Main category: cs.CV

TL;DR: 本文提出了一种名为FracDetNet的先进骨折检测框架，结合双焦点注意力（DFA）和多尺度校准（MC）模块，显著提升了医学图像中细微和形态多样骨折的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不同成像角度和图像质量不佳的情况下，难以准确检测细微且形态多样的骨折，影响临床诊断效率。

Method: FracDetNet引入了Dual-Focus Attention（DFA）模块以同时捕获局部细节和全局上下文，并采用Multi-scale Calibration（MC）模块自适应优化特征表示，从而提升检测精度。

Result: 在GRAZPEDWRI-DX数据集上的实验表明，FracDetNet的mAP$_{50-95}$达到40.0%，较基线模型提升7.5%；mAP$_{50}$达63.9%，提升4.2%，特定骨折检测准确率提高2.9%。

Conclusion: FracDetNet通过DFA和MC模块有效提升了复杂条件下骨折检测的准确性，具有良好的临床应用潜力。

Abstract: In this paper, an advanced fracture detection framework, FracDetNet, is
proposed to address challenges in medical imaging, as accurate fracture
detection is essential for enhancing diagnostic efficiency in clinical
practice. Despite recent advancements, existing methods still struggle with
detecting subtle and morphologically diverse fractures due to variable imaging
angles and suboptimal image quality. To overcome these limitations, FracDetNet
integrates Dual-Focus Attention (DFA) and Multi-scale Calibration (MC).
Specifically, the DFA module effectively captures detailed local features and
comprehensive global context through combined global and local attention
mechanisms. Additionally, the MC adaptively refines feature representations to
enhance detection performance. Experimental evaluations on the publicly
available GRAZPEDWRI-DX dataset demonstrate state-of-the-art performance, with
FracDetNet achieving a mAP$_{50-95}$ of 40.0\%, reflecting a \textbf{7.5\%}
improvement over the baseline model. Furthermore, the mAP$_{50}$ reaches
63.9\%, representing an increase of \textbf{4.2\%}, with fracture-specific
detection accuracy also enhanced by \textbf{2.9\%}.

</details>


### [91] [SPIKE-RL: Video-LLMs meet Bayesian Surprise](https://arxiv.org/abs/2509.23433)
*Sahithya Ravi,Aditya Chinchure,Raymond T. Ng,Leonid Sigal,Vered Shwartz*

Main category: cs.CV

TL;DR: 本文提出了SPIKE框架，通过量化贝叶斯惊奇来识别视频中的关键瞬间，并结合SPIKE-RL优化信念假设，实现更有效的非均匀帧采样，提升了多个下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLM通常采用均匀采样处理视频，容易错过定义视频叙事的关键时刻。作者希望构建能感知并响应视觉信息突变的模型，以更好捕捉视频中的惊奇事件。

Method: 提出SPIKE框架，利用贝叶斯惊奇衡量视频流中新视觉证据引发的信念更新；开发SPIKE-RL，使用GRPO根据视频字幕奖励信号优化信念假设；实现查询无关的惊奇加权帧采样策略。

Result: SPIKE在正向（FunQA）和负向（Oops!）惊奇基准上与人类判断高度相关；结合SPIKE和SPIKE-RL的方法在五个下游任务中均优于均匀采样。

Conclusion: 通过使Video-LLM能够跟踪信念并注册惊奇，该工作为能动态修正理解的更鲁棒视频模型提供了新路径。

Abstract: Real-world videos often show routine activities punctuated by memorable,
surprising events. However, most Video-LLMs process videos by sampling frames
uniformly, likely missing critical moments that define a video's narrative. We
introduce SPIKE, an inference-time framework that quantifies Bayesian Surprise
as the belief update triggered by new visual evidence in the video stream,
identifying moments where new visual evidence conflicts with prior beliefs.
SPIKE effectively localizes surprise in videos, strongly correlated with humans
on positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs
of zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which
leverages GRPO to optimize belief hypotheses based on a reward signal from the
video caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame
sampling, which allocates more frames to interesting moments in the video. With
this strategy, we achieve consistent performance gains on five downstream
benchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and
register surprise, our work paves the way for more robust models that can
revise their understanding in response to new information.

</details>


### [92] [FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation](https://arxiv.org/abs/2509.23438)
*Mohammed Alsakabi,Wael Mobeirek,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 提出FM-SIREN和FM-FINER，通过Nyquist指导的神经元特定频率乘子减少特征冗余，提升隐式神经表示的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于周期激活的隐式神经表示网络存在隐藏特征冗余问题，限制了MLP的表达能力。

Method: 受离散正弦变换启发，为周期性激活分配Nyquist指导的、神经元特定的频率乘子，引入频率多样性。

Result: 特征冗余减少近50%，在1D音频、2D图像、3D形状拟合及NeRF合成等任务中一致提升信号重建性能。

Conclusion: 该方法无需调参或增加网络深度，简单且有效，优于基线模型并保持高效性。

Abstract: Existing periodic activation-based implicit neural representation (INR)
networks, such as SIREN and FINER, suffer from hidden feature redundancy, where
neurons within a layer capture overlapping frequency components due to the use
of a fixed frequency multiplier. This redundancy limits the expressive capacity
of multilayer perceptrons (MLPs). Drawing inspiration from classical signal
processing methods such as the Discrete Sine Transform (DST), we propose
FM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency
multipliers to periodic activations. Unlike existing approaches, our design
introduces frequency diversity without requiring hyperparameter tuning or
additional network depth. This simple yet principled modification reduces the
redundancy of features by nearly 50% and consistently improves signal
reconstruction across diverse INR tasks, including fitting 1D audio, 2D image
and 3D shape, and synthesis of neural radiance fields (NeRF), outperforming
their baseline counterparts while maintaining efficiency.

</details>


### [93] [FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing](https://arxiv.org/abs/2509.23452)
*Tanawan Premsri,Parisa Kordjamshidi*

Main category: cs.CV

TL;DR: 本文提出了FoR-SALE，一种基于参考系引导的空间调整方法，用于改进文本到图像生成中的空间理解，特别是在非相机视角下的空间描述。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在处理非相机视角的空间描述时表现不佳，缺乏对参考系的有效建模。

Method: 在SLD框架基础上扩展，利用视觉模块提取图像空间结构，并将空间表达映射到对应相机视角，通过统一视角评估图文对齐性，检测错位后生成并应用编辑操作，采用新颖的潜在空间操作调整图像朝向和深度。

Result: 在两个专为评估带参考系空间理解设计的基准上，FoR-SALE使最先进T2I模型性能提升最高达5.3%（仅一轮修正）。

Conclusion: FoR-SALE有效整合了参考系概念，显著提升了T2I模型在复杂空间描述下的生成准确性，推动了多模态模型的空间推理能力发展。

Abstract: Frame of Reference (FoR) is a fundamental concept in spatial reasoning that
humans utilize to comprehend and describe space. With the rapid progress in
Multimodal Language models, the moment has come to integrate this
long-overlooked dimension into these models. In particular, in text-to-image
(T2I) generation, even state-of-the-art models exhibit a significant
performance gap when spatial descriptions are provided from perspectives other
than the camera. To address this limitation, we propose Frame of
Reference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),
an extension of the Self-correcting LLM-controlled Diffusion (SLD) framework
for T2I. For-Sale evaluates the alignment between a given text and an initially
generated image, and refines the image based on the Frame of Reference
specified in the spatial expressions. It employs vision modules to extract the
spatial configuration of the image, while simultaneously mapping the spatial
expression to a corresponding camera perspective. This unified perspective
enables direct evaluation of alignment between language and vision. When
misalignment is detected, the required editing operations are generated and
applied. FoR-SALE applies novel latent-space operations to adjust the facing
direction and depth of the generated images. We evaluate FoR-SALE on two
benchmarks specifically designed to assess spatial understanding with FoR. Our
framework improves the performance of state-of-the-art T2I models by up to 5.3%
using only a single round of correction.

</details>


### [94] [3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras](https://arxiv.org/abs/2509.23455)
*Tharindu Ekanayake,Constantino Álvarez Casado,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本文提出了一种紧凑且与估计器无关的模块3DPCNet，用于将单目3D姿态转换为一致的、以身体为中心的规范坐标系，从而消除视角变化的影响。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计生成的是以相机为中心的骨骼结构，导致运动学信号依赖于视角，限制了在健康和体育科学等需要跨视角比较分析的应用中的使用。

Method: 3DPCNet直接在3D关节坐标上操作，采用混合编码器融合图卷积网络提取的局部骨骼特征和Transformer提供的全局上下文，通过门控交叉注意力机制进行整合，并预测一个连续的6D旋转，映射为SO(3)矩阵以对齐姿态。模型在MM-Fi数据集上通过合成旋转姿态进行自监督训练，结合复合损失函数优化旋转精度和姿态重建。

Result: 在MM-Fi基准测试中，3DPCNet将平均旋转误差从20°以上降低至3.4°，平均每个关节位置误差从约64毫米降至47毫米；在TotalCapture数据集上的定性评估显示，该方法生成的加速度信号与真实IMU传感器数据具有良好的视觉一致性。

Conclusion: 3DPCNet能有效消除视角变异性，生成物理上合理的运动分析结果，适用于多种下游应用。

Abstract: Monocular 3D pose estimators produce camera-centered skeletons, creating
view-dependent kinematic signals that complicate comparative analysis in
applications such as health and sports science. We present 3DPCNet, a compact,
estimator-agnostic module that operates directly on 3D joint coordinates to
rectify any input pose into a consistent, body-centered canonical frame. Its
hybrid encoder fuses local skeletal features from a graph convolutional network
with global context from a transformer via a gated cross-attention mechanism.
From this representation, the model predicts a continuous 6D rotation that is
mapped to an $SO(3)$ matrix to align the pose. We train the model in a
self-supervised manner on the MM-Fi dataset using synthetically rotated poses,
guided by a composite loss ensuring both accurate rotation and pose
reconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error
from over 20$^{\circ}$ to 3.4$^{\circ}$ and the Mean Per Joint Position Error
from ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations
on the TotalCapture dataset further demonstrate that our method produces
acceleration signals from video that show strong visual correspondence to
ground-truth IMU sensor data, confirming that our module removes viewpoint
variability to enable physically plausible motion analysis.

</details>


### [95] [No Concept Left Behind: Test-Time Optimization for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.23457)
*Mohammad Hossein Sameti,Amir M. Mansourian,Arash Marioriyad,Soheil Fadaee Oshyani,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度的测试时优化框架，通过分解提示语义概念并在全局和概念级别评估对齐性，提升文本到图像生成的组合保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在复杂提示下常遗漏或错误表示对象和属性，缺乏对各个语义概念的精细控制。

Method: 将输入提示分解为语义概念，利用改进的CLIP计算概念级匹配度，并在迭代过程中结合大语言模型进行提示优化。

Result: 在DrawBench和CompBench上实验表明，该方法显著提升了概念覆盖率和人类评价的生成保真度。

Conclusion: 所提框架能有效提高T2I模型对复杂提示的理解与生成准确性，实现更完整的语义对齐。

Abstract: Despite recent advances in text-to-image (T2I) models, they often fail to
faithfully render all elements of complex prompts, frequently omitting or
misrepresenting specific objects and attributes. Test-time optimization has
emerged as a promising approach to address this limitation by refining
generation without the need for retraining. In this paper, we propose a
fine-grained test-time optimization framework that enhances compositional
faithfulness in T2I generation. Unlike most of prior approaches that rely
solely on a global image/text similarity score, our method decomposes the input
prompt into semantic concepts and evaluates alignment at both the global and
concept levels. A fine-grained variant of CLIP is used to compute concept-level
correspondence, producing detailed feedback on missing or inaccurate concepts.
This feedback is fed into an iterative prompt refinement loop, enabling the
large language model to propose improved prompts. Experiments on DrawBench and
CompBench prompts demonstrate that our method significantly improves concept
coverage and human-judged faithfulness over both standard test-time
optimization and the base T2I model. Code is available at:
https://github.com/AmirMansurian/NoConceptLeftBehind

</details>


### [96] [Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling Missing Modalities, Noisy Pseudo-Labels, and Model Degradation](https://arxiv.org/abs/2509.23475)
*Ming-Tsung Hsu,Fang-Yu Hsu,Yi-Ting Lin,Kai-Heng Chien,Jun-Ren Chen,Cheng-Hsiang Su,Yi-Chen Ou,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态面部反欺骗框架MFAS-DANet，用于解决域适应场景下的三个主要挑战：缺失模态、噪声伪标签和模型退化。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态FAS模型在面对新目标域中的未见攻击时表现不佳，且域适应方法在多模态FAS中尚未被探索。

Method: 提出MFAS-DANet框架：通过从其他模态提取互补特征来应对缺失模态；利用跨模态预测不确定性生成可靠的伪标签以减少噪声影响；设计自适应机制动态调整损失权重以防止模型退化。

Result: 大量实验表明，MFAS-DANet在多模态FAS的域适应场景下具有有效性，并达到最先进的性能。

Conclusion: MFAS-DANet有效解决了多模态FAS在域适应中的关键问题，显著提升了对未见攻击的泛化能力。

Abstract: Recent multi-modal face anti-spoofing (FAS) methods have investigated the
potential of leveraging multiple modalities to distinguish live and spoof
faces. However, pre-adapted multi-modal FAS models often fail to detect unseen
attacks from new target domains. Although a more realistic domain adaptation
(DA) scenario has been proposed for single-modal FAS to learn specific spoof
attacks during inference, DA remains unexplored in multi-modal FAS methods. In
this paper, we propose a novel framework, MFAS-DANet, to address three major
challenges in multi-modal FAS under the DA scenario: missing modalities, noisy
pseudo labels, and model degradation. First, to tackle the issue of missing
modalities, we propose extracting complementary features from other modalities
to substitute missing modality features or enhance existing ones. Next, to
reduce the impact of noisy pseudo labels during model adaptation, we propose
deriving reliable pseudo labels by leveraging prediction uncertainty across
different modalities. Finally, to prevent model degradation, we design an
adaptive mechanism that decreases the loss weight during unstable adaptations
and increasing it during stable ones. Extensive experiments demonstrate the
effectiveness and state-of-the-art performance of our proposed MFAS-DANet.

</details>


### [97] [RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation](https://arxiv.org/abs/2509.23480)
*Shourya Verma,Mengbo Wang,Nadia Atallah Lanman,Ananth Grama*

Main category: cs.CV

TL;DR: 提出了一种名为'RestoRect'的新型潜在修正流特征蒸馏方法，用于图像恢复，通过生成式学习在潜在空间中合成教师模型质量的特征，结合Retinex理论、可学习各向异性扩散和三角颜色空间极化，在多个数据集和任务上实现了更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法无法有效捕捉现代Transformer架构动态生成特征的过程，且高性能图像恢复模型通常速度慢，快速模型则效果差，亟需一种兼顾效率与质量的新方法。

Method: 提出RestoRect，将修正流应用于特征蒸馏，构建生成式学习框架；结合Retinex理论进行物理分解，引入可学习各向异性扩散约束和三角颜色空间极化；设计特征层提取损失，通过交叉归一化Transformer特征对齐与基于百分位的异常检测实现跨架构知识迁移。

Result: 在15个图像恢复数据集、4项任务和8个指标上验证了方法的优越性，表现出更好的训练稳定性、更快的收敛与推理速度，同时保持高质量的恢复效果。

Conclusion: RestoRect通过生成式特征蒸馏框架有效解决了图像恢复中速度与性能的权衡问题，为高效高质量模型压缩与知识迁移提供了新思路。

Abstract: Current approaches for restoration of degraded images face a critical
trade-off: high-performance models are too slow for practical use, while fast
models produce poor results. Knowledge distillation transfers teacher knowledge
to students, but existing static feature matching methods cannot capture how
modern transformer architectures dynamically generate features. We propose
'RestoRect', a novel Latent Rectified Flow Feature Distillation method for
restoring degraded images. We apply rectified flow to reformulate feature
distillation as a generative process where students learn to synthesize
teacher-quality features through learnable trajectories in latent space. Our
framework combines Retinex theory for physics-based decomposition with
learnable anisotropic diffusion constraints, and trigonometric color space
polarization. We introduce a Feature Layer Extraction loss for robust knowledge
transfer between different network architectures through cross-normalized
transformer feature alignment with percentile-based outlier detection.
RestoRect achieves better training stability, and faster convergence and
inference while preserving restoration quality. We demonstrate superior results
across 15 image restoration datasets, covering 4 tasks, on 8 metrics.

</details>


### [98] [Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos](https://arxiv.org/abs/2509.23492)
*Junyi Wu,Jiachen Tao,Haoxuan Wang,Gaowen Liu,Ramana Rao Kompella,Yan Yan*

Main category: cs.CV

TL;DR: 提出了一种基于方向锚定的高斯点阵化方法（OriGS），用于从随意拍摄的单目视频中实现高质量4D重建，通过引入场景方向的超维表示，有效建模复杂动态场景中的局部变形。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯点阵化的动态场景重建方法通常依赖低秩假设，难以准确建模无约束动态下的区域特定复杂形变。

Method: 提出全局方向场来传播时空中的主前进方向，并设计方向感知的超高斯表示，将时间、空间、几何和方向信息统一为概率状态，通过条件切片推断区域特定形变。

Result: 实验表明，OriGS在真实世界复杂动态场景中的重建保真度优于主流方法。

Conclusion: OriGS通过方向锚定的超维建模，显著提升了单目视频4D重建的质量，尤其在处理复杂局部动态时表现优越。

Abstract: We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework
for high-quality 4D reconstruction from casually captured monocular videos.
While recent advances extend 3D Gaussian Splatting to dynamic scenes via
various motion anchors, such as graph nodes or spline control points, they
often rely on low-rank assumptions and fall short in modeling complex,
region-specific deformations inherent to unconstrained dynamics. OriGS
addresses this by introducing a hyperdimensional representation grounded in
scene orientation. We first estimate a Global Orientation Field that propagates
principal forward directions across space and time, serving as stable
structural guidance for dynamic modeling. Built upon this, we propose
Orientation-aware Hyper-Gaussian, a unified formulation that embeds time,
space, geometry, and orientation into a coherent probabilistic state. This
enables inferring region-specific deformation through principled conditioned
slicing, adaptively capturing diverse local dynamics in alignment with global
motion intent. Experiments demonstrate the superior reconstruction fidelity of
OriGS over mainstream methods in challenging real-world dynamic scenes.

</details>


### [99] [Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional](https://arxiv.org/abs/2509.23499)
*Divyam Madaan,Varshan Muhunthan,Kyunghyun Cho,Sumit Chopra*

Main category: cs.CV

TL;DR: 本研究对23个视觉问答基准进行了大规模实证分析，量化了模态内和模态间依赖性，发现许多旨在减少文本偏差的基准反而增强了图像依赖，且大模型常依赖单模态线索掩盖了多模态推理的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习中，模态内与模态间依赖性的本质及其相互作用在基准评估中缺乏清晰刻画，影响了对模型真正多模态推理能力的准确评估。

Method: 利用多模态大语言模型（MLLMs），在涵盖通用知识、专业知识、OCR和文档理解等领域的23个VQA基准上，系统量化模态内（如视觉或文本单独贡献）和模态间（如视觉-文本交互）依赖性。

Result: 发现不同基准间和内部对视觉、文本及其交互的依赖差异显著；许多试图减轻文本偏差的基准反而增强了图像依赖；这种单模态依赖现象在不同规模模型中普遍存在，导致高表现掩盖了多模态推理的缺失。

Conclusion: 提供了多模态数据集的定量刻画方法，揭示了现有基准的设计缺陷，为未来更合理的多模态基准设计与评估提供了原则性指导。

Abstract: Understanding the interplay between intra-modality dependencies (the
contribution of an individual modality to a target task) and inter-modality
dependencies (the relationships between modalities and the target task) is
fundamental to advancing multi-modal learning. However, the nature of and
interaction between these dependencies within current benchmark evaluations
remains poorly characterized. In this work, we present a large-scale empirical
study to quantify these dependencies across 23 visual question-answering
benchmarks using multi-modal large language models (MLLMs) covering domains
such as general and expert knowledge reasoning, optical character recognition,
and document understanding. Our findings show that the reliance on vision,
question (text), and their interaction varies significantly, both across and
within benchmarks. We discover that numerous benchmarks intended to mitigate
text-only biases have inadvertently amplified image-only dependencies. This
characterization persists across model sizes, as larger models often use these
intra-modality dependencies to achieve high performance that mask an underlying
lack of multi-modal reasoning. We provide a quantitative characterization of
multi-modal datasets, enabling a principled approach to multi-modal benchmark
design and evaluation.

</details>


### [100] [Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update](https://arxiv.org/abs/2509.23502)
*Fatemeh Salahi Chashmi,Roya Sotoudeh*

Main category: cs.CV

TL;DR: 提出一种结合动态核机制和全局编码器注意力模块的新型框架，用于提高结肠息肉分割的准确性和效率，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于息肉在医学图像中形状、大小多样且边界对比度低，结肠息肉分割仍具挑战性。

Method: 引入由全局上下文向量初始化的动态核机制，并结合全局编码器注意力模块聚合多尺度特征，解码器中采用统一通道适配（UCA）标准化特征维度。

Result: 在Kvasir-SEG和CVC-ClinicDB数据集上取得更高的Dice和IoU分数，同时降低计算成本。

Conclusion: 该方法为结肠息肉分割提供了鲁棒且高效的解决方案，具有临床和自动化诊断系统的应用潜力。

Abstract: Polyp segmentation is a critical step in colorectal cancer detection, yet it
remains challenging due to the diverse shapes, sizes, and low contrast
boundaries of polyps in medical imaging. In this work, we propose a novel
framework that improves segmentation accuracy and efficiency by integrating a
Dynamic Kernel (DK) mechanism with a global Encoder Attention module. The DK
mechanism, initialized by a global context vector from the EA module,
iteratively refines segmentation predictions across decoding stages, enabling
the model to focus on and accurately delineate complex polyp boundaries. The EA
module enhances the network's ability to capture critical lesion features by
aggregating multi scale information from all encoder layers. In addition, we
employ Unified Channel Adaptation (UCA) in the decoder to standardize feature
dimensions across stages, ensuring consistent and computationally efficient
information fusion. Our approach extends the lesion-aware kernel framework by
introducing a more flexible, attention driven kernel initialization and a
unified decoder design. Extensive experiments on the KvasirSEG and CVC ClinicDB
benchmark datasets demonstrate that our model outperforms several state of the
art segmentation methods, achieving superior Dice and Intersection over Union
scores. Moreover, UCA simplifies the decoder structure, reducing computational
cost without compromising accuracy. Overall, the proposed method provides a
robust and adaptable solution for polyp segmentation, with promising
applications in clinical and automated diagnostic systems.

</details>


### [101] [Evaluating point-light biological motion in multimodal large language models](https://arxiv.org/abs/2509.23517)
*Akila Kadambi,Marco Iacoboni,Lisa Aziz-Zadeh,Srini Narayanan*

Main category: cs.CV

TL;DR: 本文提出了ActPLD，首个用于评估多模态大语言模型（MLLMs）在人类点光源显示（PLDs）下动作理解能力的基准。实验结果表明，现有模型在动作识别和时空理解方面表现普遍较差，暴露出显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 由于人类能从极简视觉线索（如点光源显示）中提取丰富语义信息，且这种能力源于具身经验，因此研究模型对PLDs的理解有助于揭示当前MLLMs在动作认知上的局限性。

Method: 构建了包含单人动作与社交互动场景的PLD数据集，作为测试多模态大语言模型动作处理能力的基准（ActPLD），并在最先进的专有和开源模型上进行评估。

Result: 所有测试模型在ActPLD基准上的表现均显著偏低，显示出在动作识别和时空动态理解方面的根本性不足。

Conclusion: 当前的多模态大语言模型在仅依赖身体运动线索的动作理解任务上存在严重缺陷，说明其在模拟人类具身认知方面仍有巨大差距。

Abstract: Humans can extract rich semantic information from minimal visual cues, as
demonstrated by point-light displays (PLDs), which consist of sparse sets of
dots localized to key joints of the human body. This ability emerges early in
development and is largely attributed to human embodied experience. Since PLDs
isolate body motion as the sole source of meaning, they represent key stimuli
for testing the constraints of action understanding in these systems. Here we
introduce ActPLD, the first benchmark to evaluate action processing in MLLMs
from human PLDs. Tested models include state-of-the-art proprietary and
open-source systems on single-actor and socially interacting PLDs. Our results
reveal consistently low performance across models, introducing fundamental gaps
in action and spatiotemporal understanding.

</details>


### [102] [Imaging-Based Mortality Prediction in Patients with Systemic Sclerosis](https://arxiv.org/abs/2509.23530)
*Alec K. Peltekian,Karolina Senkow,Gorkem Durak,Kevin M. Grudzinski,Bradford C. Bemiss,Jane E. Dematte,Carrie Richardson,Nikolay S. Markov,Mary Carns,Kathleen Aren,Alexandra Soriano,Matthew Dapas,Harris Perlman,Aaron Gundersheimer,Kavitha C. Selvan,John Varga,Monique Hinchcliff,Krishnan Warrior,Catherine A. Gao,Richard G. Wunderink,GR Scott Budinger,Alok N. Choudhary,Anthony J. Esposito,Alexander V. Misharin,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 本研究利用放射组学和深度学习方法，基于2125例系统性硬化症（SSc）患者的胸部CT图像，构建纵向分析框架，预测与肺部并发症相关的死亡率。采用ResNet-18、DenseNet-121和Swin Transformer模型，在1年、3年和5年死亡预测中分别达到0.769、0.801和0.709的AUC，表明该方法在早期检测和风险评估中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 系统性硬化症相关间质性肺病（SSc-ILD）是导致患者发病和死亡的主要原因，尽管胸部CT常用于诊断和监测，但其在疾病进展和死亡预测中的作用尚不明确，因此需要更有效的预测工具。

Method: 收集来自西北硬皮病登记库的2125例SSc患者的CT扫描，使用ResNet-18、DenseNet-121和Swin Transformer等预训练深度学习模型进行微调，并结合放射组学方法，对1年、3年和5年死亡率进行预测分析。

Result: 模型在1年、3年和5年死亡预测中AUC分别为0.769、0.801和0.709；数据集中分别有181、326和428例CT扫描来自对应时间窗内死亡的患者。

Conclusion: 放射组学与深度学习相结合的方法能有效提升SSc-ILD的早期检出与风险评估能力，为利用影像数据预测SSc患者预后提供了重要进展。

Abstract: Interstitial lung disease (ILD) is a leading cause of morbidity and mortality
in systemic sclerosis (SSc). Chest computed tomography (CT) is the primary
imaging modality for diagnosing and monitoring lung complications in SSc
patients. However, its role in disease progression and mortality prediction has
not yet been fully clarified. This study introduces a novel, large-scale
longitudinal chest CT analysis framework that utilizes radiomics and deep
learning to predict mortality associated with lung complications of SSc. We
collected and analyzed 2,125 CT scans from SSc patients enrolled in the
Northwestern Scleroderma Registry, conducting mortality analyses at one, three,
and five years using advanced imaging analysis techniques. Death labels were
assigned based on recorded deaths over the one-, three-, and five-year
intervals, confirmed by expert physicians. In our dataset, 181, 326, and 428 of
the 2,125 CT scans were from patients who died within one, three, and five
years, respectively. Using ResNet-18, DenseNet-121, and Swin Transformer we use
pre-trained models, and fine-tuned on 2,125 images of SSc patients. Models
achieved an AUC of 0.769, 0.801, 0.709 for predicting mortality within one-,
three-, and five-years, respectively. Our findings highlight the potential of
both radiomics and deep learning computational methods to improve early
detection and risk assessment of SSc-related interstitial lung disease, marking
a significant advancement in the literature.

</details>


### [103] [Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis](https://arxiv.org/abs/2509.23535)
*Ibne Farabi Shihab,Weiheng Chai,Jiyang Wang,Sanjeda Akter,Senem Velipasalar Gursoy,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出一种资源感知的自适应超分辨率框架，优化模型校准和关键事件的精确率-召回率，显著提升驾驶员监控系统在安全关键场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 低分辨率训练虽精度高但置信度校准差，难以满足安全关键场景对可靠性的要求。

Method: 采用自适应超分辨率框架，结合轻量级伪影检测器过滤超分产生的幻觉，优化模型校准与关键事件的精度-召回率。

Result: 在安全性指标上达到最优：ECE为5.8%，drowsiness检测AUPR达0.78，phone use检测精确率-召回率为0.74，均优于基线模型。

Conclusion: 该自适应框架在安全关键应用中相比低分辨率训练模型具有更优的可靠性和性能，代表当前最优解决方案。

Abstract: Driver monitoring systems require not just high accuracy but reliable,
well-calibrated confidence scores for safety-critical deployment. While direct
low-resolution training yields high overall accuracy, it produces poorly
calibrated predictions that can be dangerous in safety-critical scenarios. We
propose a resource-aware adaptive super-resolution framework that optimizes for
model calibration and high precision-recall on critical events. Our approach
achieves state-of-the-art performance on safety-centric metrics: best
calibration (ECE of 5.8\% vs 6.2\% for LR-trained baselines), highest AUPR for
drowsiness detection (0.78 vs 0.74), and superior precision-recall for phone
use detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters,
5.2ms overhead) provides additional safety by filtering SR-induced
hallucinations. While LR-trained video models serve as strong general-purpose
baselines, our adaptive framework represents the state-of-the-art solution for
safety-critical applications where reliability is paramount.

</details>


### [104] [OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction](https://arxiv.org/abs/2509.23541)
*Hongyang Li,Jinyuan Qu,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了OVSeg3R，一种利用2D感知模型和3D重建进行开放词汇3D实例分割的训练方案，通过视图级实例划分和2D边界感知超点显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D实例分割方法在开放词汇设置下表现不佳，且依赖昂贵的手动标注；同时，3D重建中的几何细节丢失和2D到3D标注投影带来的噪声限制了性能。

Method: OVSeg3R利用2D视频重建的3D场景作为输入，通过2D-3D对应关系将2D开放词汇模型的实例掩码投影到3D生成监督标签；提出视图级实例划分算法以减少因部分标注引入的误报，并设计2D实例边界感知超点，利用2D掩码约束超点聚类，避免跨越实例边界。

Result: 在ScanNet200基准上比现有闭集方法提升+2.3 mAP，在标准开放词汇设置下对新类别超过先前方法约+7.1 mAP，显著缩小了长尾分布中尾部与头部类别的性能差距。

Conclusion: OVSeg3R有效实现了从2D开放词汇模型到3D实例分割的知识迁移，无需人工标注，兼具实用性与高性能，推动了开放词汇3D感知的发展。

Abstract: In this paper, we propose a training scheme called OVSeg3R to learn
open-vocabulary 3D instance segmentation from well-studied 2D perception models
with the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes
from 2D videos as input, avoiding costly manual adjustment while aligning input
with real-world applications. By exploiting the 2D to 3D correspondences
provided by 3D reconstruction models, OVSeg3R projects each view's 2D instance
mask predictions, obtained from an open-vocabulary 2D model, onto 3D to
generate annotations for the view's corresponding sub-scene. To avoid
incorrectly introduced false positives as supervision due to partial
annotations from 2D to 3D, we propose a View-wise Instance Partition algorithm,
which partitions predictions to their respective views for supervision,
stabilizing the training process. Furthermore, since 3D reconstruction models
tend to over-smooth geometric details, clustering reconstructed points into
representative super-points based solely on geometry, as commonly done in
mainstream 3D segmentation methods, may overlook geometrically non-salient
objects. We therefore introduce 2D Instance Boundary-aware Superpoint, which
leverages 2D masks to constrain the superpoint clustering, preventing
superpoints from violating instance boundaries. With these designs, OVSeg3R not
only extends a state-of-the-art closed-vocabulary 3D instance segmentation
model to open-vocabulary, but also substantially narrows the performance gap
between tail and head classes, ultimately leading to an overall improvement of
+2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard
open-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP
on the novel classes, further validating its effectiveness.

</details>


### [105] [From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations](https://arxiv.org/abs/2509.23555)
*Javed Ahmad,Penggang Gao,Donatien Delehelle,Mennuti Canio,Nikhil Deshpande,Jesús Ortiz,Darwin G. Caldwell,Yonas Teodros Tefera*

Main category: cs.CV

TL;DR: 本文综述了3D高斯点阵（3DGS）在多个领域中的应用，探讨其相较于NeRF的技术优势、适应性及现存局限，强调其在光逼真度、几何保真度和计算效率之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨3DGS为何逐渐取代NeRF成为主流神经场景表示方法，并总结其在SLAM、远程呈现、机器人操作和3D内容生成等领域的共性需求与发展趋势。

Method: 通过系统比较不同领域的3DGS应用流程，围绕统一的研究问题进行分析：技术优势、输入模态适应性与限制因素。

Result: 发现3DGS在渲染质量、优化速度和任务驱动理解方面优于NeRF，且能有效支持多种应用场景的需求。

Conclusion: 3DGS凭借其高效性与灵活性，正成为神经渲染与场景理解的核心工具，未来有望推动虚实环境中的感知、交互与内容创作发展。

Abstract: Neural scene representations such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have transformed how 3D environments are modeled,
rendered, and interpreted. NeRF introduced view-consistent photorealism via
volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient
alternative that supports high-quality rendering, faster optimization, and
integration into hybrid pipelines for enhanced photorealism and task-driven
scene understanding. This survey examines how 3DGS is being adopted across
SLAM, telepresence and teleoperation, robotic manipulation, and 3D content
generation. Despite their differences, these domains share common goals:
photorealistic rendering, meaningful 3D structure, and accurate downstream
tasks. We organize the review around unified research questions that explain
why 3DGS is increasingly displacing NeRF-based approaches: What technical
advantages drive its adoption? How does it adapt to different input modalities
and domain-specific constraints? What limitations remain? By systematically
comparing domain-specific pipelines, we show that 3DGS balances photorealism,
geometric fidelity, and computational efficiency. The survey offers a roadmap
for leveraging neural rendering not only for image synthesis but also for
perception, interaction, and content creation across real and virtual
environments.

</details>


### [106] [Pancreas Part Segmentation under Federated Learning Paradigm](https://arxiv.org/abs/2509.23562)
*Ziliang Hong,Halil Ertugrul Aktas,Andrea Mia Bejar,Katherine Wu,Hongyi Pan,Gorkem Durak,Zheyuan Zhang,Sait Kayali,Temel Tirkes,Federica Proietto Salanitri,Concetto Spampinato,Michael Goggins,Tamas Gonda,Candice Bolan,Raj Keswani,Frank Miller,Michael Wallace,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出了一种用于MRI胰腺头部、体部和尾部分割的首个联邦学习（FL）方法，解决了胰腺疾病区域异质性带来的临床挑战。通过在七个医疗机构间进行协作训练，使用711例T1加权和726例T2加权MRI扫描数据，实现了隐私保护下的模型训练。研究系统评估了三种先进的分割架构与两种FL算法的组合，并提出一种基于解剖学信息的损失函数，提升了对胰腺各部分的分割精度。


<details>
  <summary>Details</summary>
Motivation: 胰腺疾病在不同区域表现出显著异质性，如癌症多发于头部，慢性胰腺炎导致尾部组织丢失，因此精确分割胰腺各部分对诊断和治疗至关重要。但由于MRI中形态多变、软组织对比度差及患者间解剖差异，分割极具挑战。此外，数据稀缺也限制了现有方法的发展。

Method: 提出一种隐私保护的联邦学习框架，联合七个医学中心的数据进行模型训练，采用U-Net、Attention U-Net和Swin UNETR三种分割架构与FedAvg、FedProx两种FL算法结合，并引入一种新的解剖学感知损失函数，强调MRI中区域特异性纹理对比。

Result: 实验表明，Attention U-Net结合FedAvg在处理胰腺异质性方面表现最优；所提损失函数有效提升分割性能，在分布式、异构数据集上仍达到可接受的临床精度。

Conclusion: 该研究首次将联邦学习应用于胰腺分区分割，有效应对数据孤岛和隐私问题，同时提高模型泛化能力，为多中心医学影像分析提供了可行方案。

Abstract: We present the first federated learning (FL) approach for pancreas part(head,
body and tail) segmentation in MRI, addressing a critical clinical challenge as
a significant innovation. Pancreatic diseases exhibit marked regional
heterogeneity cancers predominantly occur in the head region while chronic
pancreatitis causes tissue loss in the tail, making accurate segmentation of
the organ into head, body, and tail regions essential for precise diagnosis and
treatment planning. This segmentation task remains exceptionally challenging in
MRI due to variable morphology, poor soft-tissue contrast, and anatomical
variations across patients. Our novel contribution tackles two fundamental
challenges: first, the technical complexity of pancreas part delineation in
MRI, and second the data scarcity problem that has hindered prior approaches.
We introduce a privacy-preserving FL framework that enables collaborative model
training across seven medical institutions without direct data sharing,
leveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key
innovations include: (1) a systematic evaluation of three state-of-the-art
segmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two
FL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as
optimal for pancreatic heterogeneity, which was never been done before; (2) a
novel anatomically-informed loss function prioritizing region-specific texture
contrasts in MRI. Comprehensive evaluation demonstrates that our approach
achieves clinically viable performance despite training on distributed,
heterogeneous datasets.

</details>


### [107] [Towards Interpretable Visual Decoding with Attention to Brain Representations](https://arxiv.org/abs/2509.23566)
*Pinyuan Feng,Hossein Adeli,Wenxuan Guo,Fan Cheng,Ethan Hwang,Nikolaus Kriegeskorte*

Main category: cs.CV

TL;DR: 提出NeuroAdapter，一种直接基于脑信号条件化潜在扩散模型的视觉解码框架，无需中间特征空间，提升了重建质量和对脑区贡献的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将脑信号映射到图像或文本中间特征空间进行视觉解码，掩盖了不同脑区对生成结果的影响，缺乏透明度和解释性。

Method: 提出NeuroAdapter框架，直接将脑表示作为潜在扩散模型的条件输入，并构建IBBI双向解释框架，分析扩散去噪过程中跨注意力机制，揭示皮层区域如何影响生成过程。

Result: 在公开fMRI数据集上实现了与先前方法相当甚至更优的视觉重建质量，同时通过IBBI揭示了不同脑区在生成过程中的动态作用。

Conclusion: NeuroAdapter实现了端到端的脑到图像解码，增强了模型可解释性，为通过视觉神经科学视角理解扩散模型提供了新路径。

Abstract: Recent work has demonstrated that complex visual stimuli can be decoded from
human brain activity using deep generative models, helping brain science
researchers interpret how the brain represents real-world scenes. However, most
current approaches leverage mapping brain signals into intermediate image or
text feature spaces before guiding the generative process, masking the effect
of contributions from different brain areas on the final reconstruction output.
In this work, we propose NeuroAdapter, a visual decoding framework that
directly conditions a latent diffusion model on brain representations,
bypassing the need for intermediate feature spaces. Our method demonstrates
competitive visual reconstruction quality on public fMRI datasets compared to
prior work, while providing greater transparency into how brain signals shape
the generation process. To this end, we contribute an Image-Brain
BI-directional interpretability framework (IBBI) which investigates
cross-attention mechanisms across diffusion denoising steps to reveal how
different cortical areas influence the unfolding generative trajectory. Our
results highlight the potential of end-to-end brain-to-image decoding and
establish a path toward interpreting diffusion models through the lens of
visual neuroscience.

</details>


### [108] [RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization](https://arxiv.org/abs/2509.23582)
*Kaicheng Yang,Xun Zhang,Haotong Qin,Yucheng Lin,Kaisen Yang,Xianglong Yan,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对Diffusion Transformers（DiTs）的系统性量化感知训练框架RobuQ，解决了低比特激活量化中的关键瓶颈，首次实现了在ImageNet-1K等大数据集上稳定且高性能的2比特平均激活量化图像生成。


<details>
  <summary>Details</summary>
Motivation: DiTs在图像生成中表现优异，但计算和内存成本高，难以部署。现有量化方法在DiTs上效果受限，尤其是低比特激活量化面临激活分布复杂和敏感性问题。

Method: 提出RobuQ框架，包括强效的三值权重基线（W1.58A4）、RobustQuantizer（利用Hadamard变换改善激活分布）以及AMPN（首个用于DiTs的仅激活混合精度网络），实现逐层动态分配激活位宽。

Result: 在低于4比特的配置下，RobuQ在无条件和条件图像生成任务中均达到SOTA性能，平均激活2比特时仍可在ImageNet-1K上实现稳定且具竞争力的生成效果。

Conclusion: RobuQ有效解决了DiTs在极低比特下的量化难题，推动了其在资源受限场景下的实际应用，为高效扩散模型提供了新方向。

Abstract: Diffusion Transformers (DiTs) have recently emerged as a powerful backbone
for image generation, demonstrating superior scalability and performance over
U-Net architectures. However, their practical deployment is hindered by
substantial computational and memory costs. While Quantization-Aware Training
(QAT) has shown promise for U-Nets, its application to DiTs faces unique
challenges, primarily due to the sensitivity and distributional complexity of
activations. In this work, we identify activation quantization as the primary
bottleneck for pushing DiTs to extremely low-bit settings. To address this, we
propose a systematic QAT framework for DiTs, named RobuQ. We start by
establishing a strong ternary weight (W1.58A4) DiT baseline. Building upon
this, we propose RobustQuantizer to achieve robust activation quantization. Our
theoretical analyses show that the Hadamard transform can convert unknown
per-token distributions into per-token normal distributions, providing a strong
foundation for this method. Furthermore, we propose AMPN, the first
Activation-only Mixed-Precision Network pipeline for DiTs. This method applies
ternary weights across the entire network while allocating different activation
precisions to each layer to eliminate information bottlenecks. Through
extensive experiments on unconditional and conditional image generation, our
RobuQ framework achieves state-of-the-art performance for DiT quantization in
sub-4-bit quantization configuration. To the best of our knowledge, RobuQ is
the first achieving stable and competitive image generation on large datasets
like ImageNet-1K with activations quantized to average 2 bits. The code and
models will be available at https://github.com/racoonykc/RobuQ .

</details>


### [109] [VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement](https://arxiv.org/abs/2509.23584)
*Shulian Zhang,Yong Guo,Long Peng,Ziyang Wang,Ye Chen,Wenbo Li,Xiao Zhang,Yulun Zhang,Jian Chen*

Main category: cs.CV

TL;DR: 提出VividFace，一种新颖高效的单步扩散框架，用于视频人脸增强，通过预训练模型和联合潜在-像素面部聚焦训练策略，在减少推理时间的同时提升感知质量、身份保持和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频人脸增强方法在面部纹理建模、模型泛化能力和推理效率方面存在挑战，需要更高效且鲁棒的方法。

Method: 基于预训练的WANX视频生成模型，采用单步流匹配范式，并引入联合潜在-像素面部聚焦训练策略和MLLM驱动的数据整理管道。

Result: VividFace在感知质量、身份保持和时间稳定性方面达到最先进水平，同时显著降低推理时间。

Conclusion: VividFace有效解决了视频人脸增强中的关键挑战，兼具高性能与高效率，具有广泛的应用前景。

Abstract: Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions
from degraded video sequences, a capability that underpins numerous
applications including video conferencing, film restoration, and surveillance.
Despite substantial progress in the field, current methods that primarily rely
on video super-resolution and generative frameworks continue to face three
fundamental challenges: (1) faithfully modeling intricate facial textures while
preserving temporal consistency; (2) restricted model generalization due to the
lack of high-quality face video training data; and (3) low efficiency caused by
repeated denoising steps during inference. To address these challenges, we
propose VividFace, a novel and efficient one-step diffusion framework for video
face enhancement. Built upon the pretrained WANX video generation model, our
method leverages powerful spatiotemporal priors through a single-step flow
matching paradigm, enabling direct mapping from degraded inputs to high-quality
outputs with significantly reduced inference time. To further boost efficiency,
we propose a Joint Latent-Pixel Face-Focused Training strategy that employs
stochastic switching between facial region optimization and global
reconstruction, providing explicit supervision in both latent and pixel spaces
through a progressive two-stage training process. Additionally, we introduce an
MLLM-driven data curation pipeline for automated selection of high-quality
video face datasets, enhancing model generalization. Extensive experiments
demonstrate that VividFace achieves state-of-the-art results in perceptual
quality, identity preservation, and temporal stability, while offering
practical resources for the research community.

</details>


### [110] [From Static to Dynamic: a Survey of Topology-Aware Perception in Autonomous Driving](https://arxiv.org/abs/2509.23641)
*Yixiao Chen,Ruining Yang,Xin Chen,Jia He,Dongliang Xu,Yue Yao*

Main category: cs.CV

TL;DR: 该论文综述了实现自动驾驶的关键——拓扑感知感知，涵盖矢量化地图构建、拓扑结构建模、先验知识融合和基于语言模型的感知四个方向，指出研究正从静态地图向动态传感器驱动感知转变。


<details>
  <summary>Details</summary>
Motivation: 传统静态地图成本高、更新慢、泛化能力差，难以满足自动驾驶对实时性与可扩展性的需求。

Method: 系统回顾并分析了四个核心研究方向的技术进展，强调动态感知范式的兴起及其优势。

Result: 动态感知利用车载传感器实现实时地图构建与拓扑推理，在空间建模、语义关系推理、知识融合和多模态理解方面取得进展。

Conclusion: 动态、传感器驱动的拓扑感知是未来自动驾驶系统实现自适应、可扩展和可解释性的关键路径。

Abstract: The key to achieving autonomous driving lies in topology-aware perception,
the structured understanding of the driving environment with an emphasis on
lane topology and road semantics. This survey systematically reviews four core
research directions under this theme: vectorized map construction, topological
structure modeling, prior knowledge fusion, and language model-based
perception. Across these directions, we observe a unifying trend: a paradigm
shift from static, pre-built maps to dynamic, sensor-driven perception.
Specifically, traditional static maps have provided semantic context for
autonomous systems. However, they are costly to construct, difficult to update
in real time, and lack generalization across regions, limiting their
scalability. In contrast, dynamic representations leverage on-board sensor data
for real-time map construction and topology reasoning. Each of the four
research directions contributes to this shift through compact spatial modeling,
semantic relational reasoning, robust domain knowledge integration, and
multimodal scene understanding powered by pre-trained language models.
Together, they pave the way for more adaptive, scalable, and explainable
autonomous driving systems.

</details>


### [111] [Multi-Level Heterogeneous Knowledge Transfer Network on Forward Scattering Center Model for Limited Samples SAR ATR](https://arxiv.org/abs/2509.23596)
*Chenxi Zhao,Daochang Wang,Siqian Zhang,Gangyao Kuang*

Main category: cs.CV

TL;DR: 本文提出了一种基于前向散射中心模型（FSCM）的多级异构知识迁移网络（MHKT），用于提升合成孔径雷达（SAR）目标识别性能，通过特征、分布和类别三个层次的知识迁移，有效解决了模拟数据中冗余信息干扰和数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟图像的SAR目标识别方法包含大量无关信息（如背景、噪声），影响知识迁移质量，且存在模拟与实测数据不平衡导致的优化偏差问题。

Method: 提出MHKT网络，包含任务关联信息选择器（TAIS）、目标通用知识迁移（TGKT）模块和类别关系知识迁移（CRKT）模块，分别在特征、分布和类别层级迁移FSCM中的关键目标知识，并引入最大判别散度（MDD）度量函数提升可迁移知识的判别性。

Result: 在两个由FSCM数据和实测SAR图像构建的新数据集上进行了广泛实验，结果表明所提方法显著优于现有方法，验证了其在小样本条件下的优越识别性能。

Conclusion: 通过从模拟数据中提取具有物理意义的FSCM并实现多层次纯净知识迁移，所提MHKT网络能有效提升SAR目标识别精度，为跨域知识迁移提供了可解释性强的新思路。

Abstract: Simulated data-assisted SAR target recognition methods are the research
hotspot currently, devoted to solving the problem of limited samples. Existing
works revolve around simulated images, but the large amount of irrelevant
information embedded in the images, such as background, noise, etc., seriously
affects the quality of the migrated information. Our work explores a new
simulated data to migrate purer and key target knowledge, i.e., forward
scattering center model (FSCM) which models the actual local structure of the
target with strong physical meaning and interpretability. To achieve this
purpose, multi-level heterogeneous knowledge transfer (MHKT) network is
proposed, which fully migrates FSCM knowledge from the feature, distribution
and category levels, respectively. Specifically, we permit the more suitable
feature representations for the heterogeneous data and separate non-informative
knowledge by task-associated information selector (TAIS), to complete purer
target feature migration. In the distribution alignment, the new metric
function maximum discrimination divergence (MDD) in target generic knowledge
transfer (TGKT) module perceives transferable knowledge efficiently while
preserving discriminative structure about classes. Moreover, category relation
knowledge transfer (CRKT) module leverages the category relation consistency
constraint to break the dilemma of optimization bias towards simulation data
due to imbalance between simulated and measured data. Such stepwise knowledge
selection and migration will ensure the integrity of the migrated FSCM
knowledge. Notably, extensive experiments on two new datasets formed by FSCM
data and measured SAR images demonstrate the superior performance of our
method.

</details>


### [112] [Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices](https://arxiv.org/abs/2509.23647)
*Xingjian Yang,Ashis G. Banerjee*

Main category: cs.CV

TL;DR: 提出一种用于边缘设备的统一框架，通过共享的光照不变颜色对特征表示，实现鲁棒的6D姿态估计与高效实时跟踪。


<details>
  <summary>Details</summary>
Motivation: 在复杂光照条件下对新物体进行鲁棒6D姿态估计仍具挑战性，通常需在初始姿态估计精度和实时跟踪效率之间权衡。

Method: 设计一个统一框架，结合基于鲁棒初始估计模块和快速运动跟踪器，采用共享的光照不变颜色对特征表示，用于RGB-D图像与3D网格配准及时间对应验证。

Result: 在基准数据集上的实验表明，该方法在保持高精度跟踪的同时，具有良好的姿态估计准确性，尤其在剧烈姿态变化下表现稳定。

Conclusion: 所提方法在边缘设备上实现了高效、鲁棒的6D姿态估计与跟踪，兼顾准确性与实时性。

Abstract: Robust 6D pose estimation of novel objects under challenging illumination
remains a significant challenge, often requiring a trade-off between accurate
initial pose estimation and efficient real-time tracking. We present a unified
framework explicitly designed for efficient execution on edge devices, which
synergizes a robust initial estimation module with a fast motion-based tracker.
The key to our approach is a shared, lighting-invariant color-pair feature
representation that forms a consistent foundation for both stages. For initial
estimation, this feature facilitates robust registration between the live RGB-D
view and the object's 3D mesh. For tracking, the same feature logic validates
temporal correspondences, enabling a lightweight model to reliably regress the
object's motion. Extensive experiments on benchmark datasets demonstrate that
our integrated approach is both effective and robust, providing competitive
pose estimation accuracy while maintaining high-fidelity tracking even through
abrupt pose changes.

</details>


### [113] [VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration](https://arxiv.org/abs/2509.23601)
*Han Hu,Zhuoran Zheng,Liang Li,Chen Lyu*

Main category: cs.CV

TL;DR: 本文提出了VAMamba，一种视觉自适应Mamba框架，通过QCLAM和GPS-SS2D两个关键创新提升图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mamba的图像恢复方法受限于固定的扫描模式和低效的特征利用，难以应对多样的退化情况。

Method: 提出QCLAM（基于队列缓存的低秩自适应记忆）和GPS-SS2D（贪婪路径扫描SS2D），前者通过历史特征缓存与相似性匹配实现动态特征融合，后者利用Vision Transformer生成像素重要性图并采用贪婪策略确定最优扫描路径。

Result: 在多种图像恢复任务中，VAMamba在恢复质量和计算效率方面均优于现有方法。

Conclusion: VAMamba实现了对退化区域的自适应聚焦，在保持高效计算的同时显著提升了图像恢复性能，为自适应图像恢复设立了新基准。

Abstract: Recent Mamba-based image restoration methods have achieved promising results
but remain
  limited by fixed scanning patterns and inefficient feature utilization.
Conventional Mamba
  architectures rely on predetermined paths that cannot adapt to diverse
degradations, constraining
  both restoration performance and computational efficiency. To overcome these
limitations, we
  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.
First,
  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha
  FIFO cache that stores historical representations. Similarity between current
LoRA-adapted and
  cached features guides intelligent fusion, enabling dynamic reuse while
effectively controlling
  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.
A
  Vision Transformer generates score maps to estimate pixel importance, and a
greedy strategy de termines optimal forward and backward scanning paths. These
learned trajectories replace rigid
  patterns, enabling SS2D to perform targeted feature extraction. The
integration of QCLAM and
  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while
maintaining high
  computational efficiency. Extensive experiments across diverse restoration
tasks demonstrate
  that VAMamba consistently outperforms existing approaches in both restoration
quality and
  efficiency, establishing new benchmarks for adaptive image restoration. Our
code is available
  at https://github.com/WaterHQH/VAMamba.

</details>


### [114] [FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention](https://arxiv.org/abs/2509.23733)
*Hangtian Zhao,Xiang Chen,Yizhe Li,Qianhao Wang,Haibo Lu,Fei Gao*

Main category: cs.CV

TL;DR: 提出FastViDAR框架，利用四个鱼眼相机输入生成360度深度图，引入AHA机制和ERP融合方法，在嵌入式硬件上实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效、实时的360度深度估计，尤其是在资源受限的嵌入式平台上。

Method: 采用四个鱼眼相机输入，引入替代性层次化注意力（AHA）机制进行跨视角特征融合，并通过ERP投影融合多视角深度估计。

Result: 在HM3D和2D3D-S数据集上生成ERP图像-深度对，实现了高达20 FPS的推理速度，并在真实数据集上表现出具有竞争力的零样本性能。

Conclusion: FastViDAR通过AHA和ERP融合策略，有效平衡了精度与效率，适用于实际部署于嵌入式平台的全景深度估计任务。

Abstract: In this paper we propose FastViDAR, a novel framework that takes four fisheye
camera inputs and produces a full $360^\circ$ depth map along with per-camera
depth, fusion depth, and confidence estimates. Our main contributions are: (1)
We introduce Alternative Hierarchical Attention (AHA) mechanism that
efficiently fuses features across views through separate intra-frame and
inter-frame windowed self-attention, achieving cross-view feature mixing with
reduced overhead. (2) We propose a novel ERP fusion approach that projects
multi-view depth estimates to a shared equirectangular coordinate system to
obtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D
and 2D3D-S datasets for comprehensive evaluation, demonstrating competitive
zero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA
Orin NX embedded hardware. Project page:
\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}

</details>


### [115] [Deep Taxonomic Networks for Unsupervised Hierarchical Prototype Discovery](https://arxiv.org/abs/2509.23602)
*Zekun Wang,Ethan Haarer,Zhiyi Dai,Tianyi Zhu,Christopher J. MacLellan*

Main category: cs.CV

TL;DR: 提出深度分类网络（deep taxonomic networks），一种基于变分推断的深度潜在变量模型，通过构建二叉树结构的高斯混合先验来自动生成层次化聚类结构，无需预设类别数量，有效利用中间层原型信息，在图像数据上实现了优越的层次聚类性能和可解释的语义层次发现。


<details>
  <summary>Details</summary>
Motivation: 现有深度层次聚类方法通常将结构绑定于类别数量，且未能充分利用中间层次的原型信息，缺乏对层次结构与原型协同学习的有效建模。

Method: 提出深度分类网络，采用完全二叉树结构的混合高斯潜在变量作为先验，在变分推断框架下优化ELBO，通过最大化证据下界来促进原型间层次关系的学习，实现从无标签数据中自动发现层次化分类结构和原型簇。

Result: 在多个图像分类数据集上优于基线方法，提出的评估机制利用所有层次的原型簇进行评价，实验表明该方法在层次聚类性能上表现优异，定性结果展示了其能发现细粒度视觉差异与粗粒度语义类别兼具的可解释层次结构。

Conclusion: 深度分类网络能够有效克服现有方法对类别数依赖和原型信息利用不足的问题，通过潜在层次化先验自动发现丰富且可解释的分类体系，为无监督层次知识组织提供了新思路。

Abstract: Inspired by the human ability to learn and organize knowledge into
hierarchical taxonomies with prototypes, this paper addresses key limitations
in current deep hierarchical clustering methods. Existing methods often tie the
structure to the number of classes and underutilize the rich prototype
information available at intermediate hierarchical levels. We introduce deep
taxonomic networks, a novel deep latent variable approach designed to bridge
these gaps. Our method optimizes a large latent taxonomic hierarchy,
specifically a complete binary tree structured mixture-of-Gaussian prior within
a variational inference framework, to automatically discover taxonomic
structures and associated prototype clusters directly from unlabeled data
without assuming true label sizes. We analytically show that optimizing the
ELBO of our method encourages the discovery of hierarchical relationships among
prototypes. Empirically, our learned models demonstrate strong hierarchical
clustering performance, outperforming baselines across diverse image
classification datasets using our novel evaluation mechanism that leverages
prototype clusters discovered at all hierarchical levels. Qualitative results
further reveal that deep taxonomic networks discover rich and interpretable
hierarchical taxonomies, capturing both coarse-grained semantic categories and
fine-grained visual distinctions.

</details>


### [116] [GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State](https://arxiv.org/abs/2509.23737)
*Guole Shen,Tianchen Deng,Yanbo Wang,Yongtao Chen,Yilin Shen,Jiuming Liu,Jingchuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于DUSt3R的端到端SLAM框架GRS-SLAM3R，通过引入空间记忆和全局一致性优化，实现了无需先验知识的稠密场景重建与位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于图像对估计点云，缺乏对空间记忆和全局一致性的考虑，导致重建结果不够准确和连续。

Method: 采用序列化输入，增量式估计全局坐标系下的度量尺度点云；设计基于Transformer的门控更新模块维护空间记忆，并通过子图划分与局部对齐结合相对约束实现全局地图一致性。

Result: 在多个数据集上实验表明，该方法在保持实时性的同时，显著提升了重建精度。

Conclusion: GRS-SLAM3R有效解决了现有DUSt3R方法在全局一致性和空间记忆方面的不足，实现了高精度、实时的稠密SLAM。

Abstract: DUSt3R-based end-to-end scene reconstruction has recently shown promising
results in dense visual SLAM. However, most existing methods only use image
pairs to estimate pointmaps, overlooking spatial memory and global
consistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework
for dense scene reconstruction and pose estimation from RGB images without any
prior knowledge of the scene or camera parameters. Unlike existing DUSt3R-based
frameworks, which operate on all image pairs and predict per-pair point maps in
local coordinate frames, our method supports sequentialized input and
incrementally estimates metric-scale point clouds in the global coordinate. In
order to improve consistent spatial correlation, we use a latent state for
spatial memory and design a transformer-based gated update module to reset and
update the spatial memory that continuously aggregates and tracks relevant 3D
information across frames. Furthermore, we partition the scene into submaps,
apply local alignment within each submap, and register all submaps into a
common world frame using relative constraints, producing a globally consistent
map. Experiments on various datasets show that our framework achieves superior
reconstruction accuracy while maintaining real-time performance.

</details>


### [117] [MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and High-Quality Low-Dose CT Image Denoising](https://arxiv.org/abs/2509.23603)
*Tangtangfang Fang,Jingxi Hu,Xiangjian He,Jiaqi Yang*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散增强的多阶段去噪网络MAN，用于高效高质量低剂量CT图像去噪，显著提升推理速度并保持优秀重建质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在低剂量CT去噪中表现优异，但因推理时间过长限制了临床应用，亟需高效的解决方案。

Method: 通过感知优化的自编码器在压缩潜在空间中进行操作，采用基于注意力机制的条件U-Net实现快速确定性条件去噪扩散过程。

Result: 在LDCT和投影数据集上，模型感知质量优于CNN/GAN方法，接近DDPM等重型扩散模型，推理速度提升60倍以上，PSNR/SSIM指标仍具竞争力。

Conclusion: 所提方法在保证高重建质量的同时大幅降低计算成本，为生成模型在医学影像中的临床应用提供了可行路径。

Abstract: While diffusion models have set a new benchmark for quality in Low-Dose
Computed Tomography (LDCT) denoising, their clinical adoption is critically
hindered by extreme computational costs, with inference times often exceeding
thousands of seconds per scan. To overcome this barrier, we introduce MAN, a
Latent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and
High-Quality Low-Dose CT Image Denoising task. Our method operates in a
compressed latent space via a perceptually-optimized autoencoder, enabling an
attention-based conditional U-Net to perform the fast, deterministic
conditional denoising diffusion process with drastically reduced overhead. On
the LDCT and Projection dataset, our model achieves superior perceptual
quality, surpassing CNN/GAN-based methods while rivaling the reconstruction
fidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most
critically, in the inference stage, our model is over 60x faster than
representative pixel space diffusion denoisers, while remaining competitive on
PSNR/SSIM scores. By bridging the gap between high fidelity and clinical
viability, our work demonstrates a practical path forward for advanced
generative models in medical imaging.

</details>


### [118] [VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis](https://arxiv.org/abs/2509.23605)
*Zeren Xiong,Yue Yu,Zedong Zhang,Shuo Chen,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的视觉融合框架VMDiff，用于生成融合两个输入图像特征的连贯新图像，有效解决了共存生成和语义偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像到图像生成方法在融合多源视觉信息时存在对象简单并置（共存生成）和语义不平衡导致的主导性偏差（偏置生成）问题，难以实现真正融合。

Method: 提出Visual Mixing Diffusion (VMDiff)，通过在噪声和潜在空间层面融合双输入图像，结合引导去噪、反转和球面插值的混合采样过程，并设计基于相似度评分的自适应参数调整模块，实现结构感知且语义平衡的融合。

Result: 在包含780个概念对的基准上实验表明，该方法在视觉质量、语义一致性和人工评价的创造力方面均优于强基线方法。

Conclusion: VMDiff能有效生成语义融合良好且高质量的新图像，为多源视觉内容创作提供了实用且高效的解决方案。

Abstract: Creating novel images by fusing visual cues from multiple sources is a
fundamental yet underexplored problem in image-to-image generation, with broad
applications in artistic creation, virtual reality and visual media. Existing
methods often face two key challenges: coexistent generation, where multiple
objects are simply juxtaposed without true integration, and bias generation,
where one object dominates the output due to semantic imbalance. To address
these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet
effective diffusion-based framework that synthesizes a single, coherent object
by integrating two input images at both noise and latent levels. Our approach
comprises: (1) a hybrid sampling process that combines guided denoising,
inversion, and spherical interpolation with adjustable parameters to achieve
structure-aware fusion, mitigating coexistent generation; and (2) an efficient
adaptive adjustment module, which introduces a novel similarity-based score to
automatically and adaptively search for optimal parameters, countering semantic
bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that
our method outperforms strong baselines in visual quality, semantic
consistency, and human-rated creativity.

</details>


### [119] [DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving through Real-to-Simulation](https://arxiv.org/abs/2509.23922)
*Haibao Yu,Wenxian Yang,Ruiyang Hao,Chuanye Wang,Jiaru Zhong,Ping Luo,Zaiqing Nie*

Main category: cs.CV

TL;DR: 提出了一种结合真实世界驾驶场景的闭环评估框架，通过基础设施协作将现实交通场景和数字孪生技术集成到CARLA仿真器中，提升了自动驾驶评估的真实性与挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CARLA的闭环评测依赖人工配置交通场景，偏离真实世界情况，难以反映实际驾驶性能，因此需要更贴近真实世界的评估方法。

Method: 从100小时的基础设施摄像头视频数据中提取800个动态交通场景，构建15个真实路口的静态数字孪生资产，并将其集成到CARLA仿真器中，实现高保真、可视一致的闭环评估环境。

Result: 构建了一个包含多样化驾驶行为、地点、天气和时段的复杂城市交叉口闭环评测基准，显著提升了仿真环境的真实性和测试挑战性。

Conclusion: 该框架有效弥合了仿真与现实之间的差距，为端到端自动驾驶模型提供了更可靠、更具代表性的闭环评估平台。

Abstract: Closed-loop evaluation is increasingly critical for end-to-end autonomous
driving. Current closed-loop benchmarks using the CARLA simulator rely on
manually configured traffic scenarios, which can diverge from real-world
conditions, limiting their ability to reflect actual driving performance. To
address these limitations, we introduce a simple yet challenging closed-loop
evaluation framework that closely integrates real-world driving scenarios into
the CARLA simulator with infrastructure cooperation. Our approach involves
extracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour
video dataset captured by high-mounted infrastructure sensors, and creating
static digital twin assets for 15 real-world intersections with consistent
visual appearance. These digital twins accurately replicate the traffic and
environmental characteristics of their real-world counterparts, enabling more
realistic simulations in CARLA. This evaluation is challenging due to the
diversity of driving behaviors, locations, weather conditions, and times of day
at complex urban intersections. In addition, we provide a comprehensive
closed-loop benchmark for evaluating end-to-end autonomous driving models.
Project URL:
\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.

</details>


### [120] [FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching](https://arxiv.org/abs/2509.23608)
*Liubing Hu,Chen Wu,Anrui Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为FlowLUT的新型端到端图像增强模型，结合了查找表（LUT）的高效性、多先验信息以及流匹配重建图像的参数无关特性，在保持O(1)复杂度的同时实现了自适应色彩校正和细节恢复。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像增强方法在计算效率与表示能力之间存在权衡。传统3D LUT虽高效但缺乏灵活性且依赖固定先验，难以适应不同场景。因此，需要一种兼具高效性与强表示能力的新方法。

Method: 提出FlowLUT模型：首先使用一组可微的、具有不同先验的3D LUT对输入图像进行颜色空间变换；然后通过轻量级内容感知网络动态预测融合权重，实现O(1)复杂度的场景自适应色彩校正；进一步设计迭代流匹配方法恢复局部结构细节并消除伪影；整个模型通过联合优化感知与结构保真度的复合损失函数进行训练。

Result: 在三个基准数据集上进行了广泛实验，结果表明该方法在保持实时处理能力的同时显著提升了图像增强质量，优于现有方法。

Conclusion: FlowLUT成功融合了LUT的高效性与深度学习的表达能力，通过多先验LUT融合与流匹配细节恢复机制，在低复杂度下实现了高质量的自适应图像增强，为实际应用提供了有效解决方案。

Abstract: Deep learning-based image enhancement methods face a fundamental trade-off
between computational efficiency and representational capacity. For example,
although a conventional three-dimensional Look-Up Table (3D LUT) can process a
degraded image in real time, it lacks representational flexibility and depends
solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel
end-to-end model that integrates the efficiency of LUTs, multiple priors, and
the parameter-independent characteristic of flow-matched reconstructed images.
Specifically, firstly, the input image is transformed in color space by a
collection of differentiable 3D LUTs (containing a large number of 3D LUTs with
different priors). Subsequently, a lightweight content-aware dynamically
predicts fusion weights, enabling scene-adaptive color correction with
$\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs
on multiple 3D LUTs, with $\mathcal{O}(1)$ complexity for scene-adaptive color
correction.Furthermore, to address the inherent representation limitations of
LUTs, we design an innovative iterative flow matching method to restore local
structural details and eliminate artifacts. Finally, the entire model is
jointly optimized under a composite loss function enforcing perceptual and
structural fidelity. Extensive experimental results demonstrate the
effectiveness of our method on three benchmarks.

</details>


### [121] [Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.23993)
*Muleilan Pei,Shaoshuai Shi,Shaojie Shen*

Main category: cs.CV

TL;DR: 提出SMART-R1，一种基于R1风格的强化微调范式，通过SFT-RFT-SFT迭代策略提升多智能体交通行为模拟的真实性，在Waymo开放数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模拟器依赖监督学习，在训练与测试间存在分布偏移，导致在新环境中泛化能力不足。

Method: 提出SMART-R1，采用面向指标的策略优化算法和SFT-RFT-SFT迭代训练框架，结合监督微调与强化微调，提升行为分布对齐。

Result: 在Waymo Open Motion Dataset上验证有效，在WOSAC挑战赛中取得0.7858的真实感元得分，排名第一。

Conclusion: SMART-R1显著提升了交通模拟中代理行为的真实性与泛化能力，为自动驾驶仿真提供了高效、可扩展的新范式。

Abstract: Scalable and realistic simulation of multi-agent traffic behavior is critical
for advancing autonomous driving technologies. Although existing data-driven
simulators have made significant strides in this domain, they predominantly
rely on supervised learning to align simulated distributions with real-world
driving scenarios. A persistent challenge, however, lies in the distributional
shift that arises between training and testing, which often undermines model
generalization in unseen environments. To address this limitation, we propose
SMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for
next-token prediction models to better align agent behavior with human
preferences and evaluation metrics. Our approach introduces a metric-oriented
policy optimization algorithm to improve distribution alignment and an
iterative "SFT-RFT-SFT" training strategy that alternates between Supervised
Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance
gains. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) validate the effectiveness of this simple yet powerful R1-style training
framework in enhancing foundation models. The results on the Waymo Open Sim
Agents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art
performance with an overall realism meta score of 0.7858, ranking first on the
leaderboard at the time of submission.

</details>


### [122] [InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects](https://arxiv.org/abs/2509.23612)
*Xinhao Cai,Minghang Zheng,Xin Jin,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一个用于3D场景中可移动物体的文本控制人-物交互生成的新任务，并构建了InteractMove数据集，提出了一种包含3D视觉定位、手-物联合可操作性学习和局部场景建模的 pipeline 方法，实现了物理合理且符合文本描述的交互生成。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中人-场景交互类别不足，且多局限于静态物体交互，难以捕捉真实环境中可移动物体的复杂交互；收集包含可动物体的真实交互数据成本高、难度大，因此需要构建新的数据集与方法以推动该领域发展。

Method: 首先利用3D视觉定位模型识别目标交互物体；然后通过手-物联合可操作性学习预测手部关节与物体部分的接触区域；最后结合局部场景建模与碰撞避免约束优化交互过程，确保动作的物理合理性。

Result: 实验表明，所提方法在生成符合文本描述且物理合理的交互方面优于现有方法，能够准确识别交互对象、处理不同大小和类别的物体，并有效避免物体与场景之间的碰撞。

Conclusion: 本文提出的InteractMove数据集和方法为文本驱动的可移动人-物交互生成提供了有效解决方案，显著提升了在复杂3D场景中生成多样化、物理可信交互的能力。

Abstract: We propose a novel task of text-controlled human object interaction
generation in 3D scenes with movable objects. Existing human-scene interaction
datasets suffer from insufficient interaction categories and typically only
consider interactions with static objects (do not change object positions), and
the collection of such datasets with movable objects is difficult and costly.
To address this problem, we construct the InteractMove dataset for Movable
Human-Object Interaction in 3D Scenes by aligning existing human object
interaction data with scene contexts, featuring three key characteristics: 1)
scenes containing multiple movable objects with text-controlled interaction
specifications (including same-category distractors requiring spatial and 3D
scene context understanding), 2) diverse object types and sizes with varied
interaction patterns (one-hand, two-hand, etc.), and 3) physically plausible
object manipulation trajectories. With the introduction of various movable
objects, this task becomes more challenging, as the model needs to identify
objects to be interacted with accurately, learn to interact with objects of
different sizes and categories, and avoid collisions between movable objects
and the scene. To tackle such challenges, we propose a novel pipeline solution.
We first use 3D visual grounding models to identify the interaction object.
Then, we propose a hand-object joint affordance learning to predict contact
regions for different hand joints and object parts, enabling accurate grasping
and manipulation of diverse objects. Finally, we optimize interactions with
local-scene modeling and collision avoidance constraints, ensuring physically
plausible motions and avoiding collisions between objects and the scene.
Comprehensive experiments demonstrate our method's superiority in generating
physically plausible, text-compliant interactions compared to existing
approaches.

</details>


### [123] [Gaze Estimation for Human-Robot Interaction: Analysis Using the NICO Platform](https://arxiv.org/abs/2509.24001)
*Matej Palider,Omar Eldardeer,Viktor Kocur*

Main category: cs.CV

TL;DR: 本文评估了在共享工作空间场景下的人机交互（HRI）中当前的凝视估计方法，使用NICO机器人平台收集了一个新的标注数据集，并评估了四种最先进的凝视估计模型。结果表明，尽管角度误差接近通用基准报告的水平，但在共享工作空间中的距离误差最佳中位数为16.48厘米，揭示了当前方法的实际局限性。最后，文章讨论了这些局限性，并提出了如何最好地将凝视估计作为HRI系统中的一种模态进行集成的建议。


<details>
  <summary>Details</summary>
Motivation: 为了评估现有凝视估计方法在人机协作共享工作空间中的实际性能，并识别其在真实应用场景中的局限性。

Method: 引入了一个基于NICO机器人平台采集的新标注数据集，对四种先进的凝视估计模型进行了评估，采用角度误差和在共享工作空间中的距离误差作为评价指标。

Result: 模型的角度误差接近通用基准，但在实际空间中的最佳中位距离误差为16.48厘米，显示出在实际应用中的显著偏差。

Conclusion: 当前的凝视估计方法在实际HRI场景中存在明显的空间定位误差，需结合上下文信息和多模态融合策略来提升实用性，文中提出了相应的集成建议。

Abstract: This paper evaluates the current gaze estimation methods within an HRI
context of a shared workspace scenario. We introduce a new, annotated dataset
collected with the NICO robotic platform. We evaluate four state-of-the-art
gaze estimation models. The evaluation shows that the angular errors are close
to those reported on general-purpose benchmarks. However, when expressed in
terms of distance in the shared workspace the best median error is 16.48 cm
quantifying the practical limitations of current methods. We conclude by
discussing these limitations and offering recommendations on how to best
integrate gaze estimation as a modality in HRI systems.

</details>


### [124] [BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images](https://arxiv.org/abs/2509.23617)
*Cheng Huang,Weizheng Xie,Fan Gao,Yutong Liu,Ruoling Wu,Zeyu Han,Jingxi Qiu,Xiangxiang Wang,Zhenglin Yang,Hao Wang,Yongbin Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为BioVessel-Net的无监督生成框架，用于光学相干断层扫描血管造影（OCTA）中的视网膜血管结构分割，无需标注数据即可实现高精度、可解释的血管提取，并发布了包含2D和3D OCTA图像的新基准数据集RetinaMix。


<details>
  <summary>Details</summary>
Motivation: 现有的视网膜血管分割方法依赖大量人工标注，成本高且易出错，尤其在OCTA中难以获取标注数据，因此需要一种无需标签、高效且可靠的分割方法。

Method: 提出BioVessel-Net，结合血管生物统计特征、对抗性优化和半径引导的分割策略，直接建模血管结构而非像素点；同时构建新数据集RetinaMix用于训练与评估。

Result: BioVessel-Net在RetinaMix及现有数据集上实现了接近完美的分割精度，显著优于当前最先进的有监督和半监督方法，且无需标注数据或高性能计算。

Conclusion: BioVessel-Net与RetinaMix共同提供了一种无需标签、计算高效且临床可解释的视网膜血管分析方案，在青光眼监测、血流建模和疾病进展预测中具有广泛应用前景。

Abstract: Structural changes in retinal blood vessels are critical biomarkers for the
onset and progression of glaucoma and other ocular diseases. However, current
vessel segmentation approaches largely rely on supervised learning and
extensive manual annotations, which are costly, error-prone, and difficult to
obtain in optical coherence tomography angiography. Here we present
BioVessel-Net, an unsupervised generative framework that integrates vessel
biostatistics with adversarial refinement and a radius-guided segmentation
strategy. Unlike pixel-based methods, BioVessel-Net directly models vascular
structures with biostatistical coherence, achieving accurate and explainable
vessel extraction without labeled data or high-performance computing. To
support training and evaluation, we introduce RetinaMix, a new benchmark
dataset of 2D and 3D OCTA images with high-resolution vessel details from
diverse populations. Experimental results demonstrate that BioVessel-Net
achieves near-perfect segmentation accuracy across RetinaMix and existing
datasets, substantially outperforming state-of-the-art supervised and
semi-supervised methods. Together, BioVessel-Net and RetinaMix provide a
label-free, computationally efficient, and clinically interpretable solution
for retinal vessel analysis, with broad potential for glaucoma monitoring,
blood flow modeling, and progression prediction. Code and dataset are
available: https://github.com/VikiXie/SatMar8.

</details>


### [125] [DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation](https://arxiv.org/abs/2509.23624)
*Wei Pan,Huiguo He,Hiuyi Cheng,Yilin Shi,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出DiffInk，首个基于潜在扩散Transformer的全行手写生成框架，结合InkVAE和InkDiT，实现高效、高保真的文本到手写轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于字符或词级生成，导致整行文本生成效率低且缺乏整体结构建模。

Method: 设计InkVAE（带OCR和风格分类损失的序列VAE）实现内容与风格解耦；构建InkDiT（潜在扩散Transformer）生成连贯笔迹轨迹。

Result: 实验表明DiffInk在字形准确性和风格保真度上优于现有最先进方法，且生成效率显著提升。

Conclusion: DiffInk为全行在线手写生成提供了高效且高质量的解决方案，推动了文本到手写合成的发展。

Abstract: Deep generative models have advanced text-to-online handwriting generation
(TOHG), which aims to synthesize realistic pen trajectories conditioned on
textual input and style references. However, most existing methods still
primarily focus on character- or word-level generation, resulting in
inefficiency and a lack of holistic structural modeling when applied to full
text lines. To address these issues, we propose DiffInk, the first latent
diffusion Transformer framework for full-line handwriting generation. We first
introduce InkVAE, a novel sequential variational autoencoder enhanced with two
complementary latent-space regularization losses: (1) an OCR-based loss
enforcing glyph-level accuracy, and (2) a style-classification loss preserving
writing style. This dual regularization yields a semantically structured latent
space where character content and writer styles are effectively disentangled.
We then introduce InkDiT, a novel latent diffusion Transformer that integrates
target text and reference styles to generate coherent pen trajectories.
Experimental results demonstrate that DiffInk outperforms existing
state-of-the-art methods in both glyph accuracy and style fidelity, while
significantly improving generation efficiency. Code will be made publicly
available.

</details>


### [126] [RIV: Recursive Introspection Mask Diffusion Vision Language Model](https://arxiv.org/abs/2509.23625)
*YuQian Li,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了RIV模型，通过引入内省训练和递归推理机制，使基于掩码扩散的视觉语言模型具备自我纠正能力，显著提升了多模态理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散视觉语言模型无法纠正生成过程中的错误，缺乏自我修正能力，限制了其在复杂任务中的表现。

Method: 提出递归内省掩码扩散视觉语言模型（RIV），包括两个新机制：一是内省训练，引入内省模型识别生成序列中的语法、拼写和逻辑错误；二是递归推理，在标准解掩码后，利用内省模型检测并重新掩码错误，循环执行解掩码-内省-再掩码直至结果可靠。

Result: 在多个基准测试上的实验结果表明，RIV达到了最先进的性能，优于大多数现有的掩码扩散视觉语言模型。

Conclusion: RIV通过内省训练和递归推理有效实现了自我纠正能力，显著提升了生成质量与模型可靠性，为未来多模态模型的发展提供了新方向。

Abstract: Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable
progress in multimodal understanding tasks. However, these models are unable to
correct errors in generated tokens, meaning they lack self-correction
capability. In this paper, we propose Recursive Introspection Mask Diffusion
Vision Language Model (RIV), which equips the model with self-correction
ability through two novel mechanisms. The first is Introspection Training,
where an Introspection Model is introduced to identify errors within generated
sequences. Introspection Training enables the model to detect not only
grammatical and spelling mistakes, but more importantly, logical errors. The
second is Recursive Inference. Beginning with the standard unmasking step, the
learned Introspection Model helps to identify errors in the output sequence and
remask them. This alternating
($\text{unmask}\rightarrow\text{introspection}\rightarrow\text{remask}$)
process is repeated recursively until reliable results are obtained.
Experimental results on multiple benchmarks demonstrate that the proposed RIV
achieves state-of-the-art performance, outperforming most existing MDVLMs.

</details>


### [127] [FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation](https://arxiv.org/abs/2509.24241)
*Seungwook Kim,Seunghyeon Lee,Minsu Cho*

Main category: cs.CV

TL;DR: 提出两种无需训练、在推理时使用的技术，通过显式动作参数提升基于扩散模型的机器人视频生成效果，显著提高动作连贯性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地构建世界模型和机器人基础模型，需要生成与显式动作轨迹高度一致的逼真机器人视频，现有方法对动作参数的利用不足。

Method: 提出两种训练-free的方法：1）动作缩放的无分类器指导，根据动作幅度动态调整引导强度；2）动作缩放的噪声截断，调整初始噪声分布以匹配期望的运动动态。

Result: 在真实机器人操作数据集上的实验表明，所提方法显著提升了生成视频的动作连贯性和视觉质量，适用于多种机器人环境。

Conclusion: 通过主动利用显式动作参数指导扩散模型的生成过程，可以在不进行额外训练的情况下有效提升机器人视频生成的可控性和真实性。

Abstract: Generating realistic robot videos from explicit action trajectories is a
critical step toward building effective world models and robotics foundation
models. We introduce two training-free, inference-time techniques that fully
exploit explicit action parameters in diffusion-based robot video generation.
Instead of treating action vectors as passive conditioning signals, our methods
actively incorporate them to guide both the classifier-free guidance process
and the initialization of Gaussian latents. First, action-scaled
classifier-free guidance dynamically modulates guidance strength in proportion
to action magnitude, enhancing controllability over motion intensity. Second,
action-scaled noise truncation adjusts the distribution of initially sampled
noise to better align with the desired motion dynamics. Experiments on real
robot manipulation datasets demonstrate that these techniques significantly
improve action coherence and visual quality across diverse robot environments.

</details>


### [128] [Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision Foundation Models](https://arxiv.org/abs/2509.23626)
*Beomseok Kang,Niluthpol Chowdhury Mithun,Mikhail Sizintsev,Han-Pang Chiu,Supun Samarasekera*

Main category: cs.CV

TL;DR: 本文提出了FAMDA，一种基于视觉基础模型的多任务无监督域自适应框架，通过自训练范式生成高质量伪标签，显著提升了跨域多任务密集预测性能，并在效率和准确性之间实现了优越平衡。


<details>
  <summary>Details</summary>
Motivation: 现有多任务无监督域自适应方法主要依赖对抗学习，效果不如最新的自训练技术；同时在机器人应用中面临域偏移问题，需要更有效的方法来提升模型在新环境中的泛化能力。

Method: 提出FAMDA框架，利用视觉基础模型（VFMs）作为教师模型，在自训练范式中融合语义分割和深度估计基础模型，为目标域生成高质量伪标签，并将其知识蒸馏到一个高效的学生网络中。

Result: FAMDA在标准的合成到真实的多任务UDA基准和新的昼夜适应任务上均达到最先进水平；轻量级版本模型体积比基础模型小10倍以上，仍保持SOTA精度。

Conclusion: FAMDA有效弥合了多任务UDA中对抗学习与自训练之间的差距，结合基础模型的强大泛化能力与轻量学生网络的高效性，为资源受限的机器人应用提供了高性能、可部署的解决方案。

Abstract: Multi-task dense prediction, which aims to jointly solve tasks like semantic
segmentation and depth estimation, is crucial for robotics applications but
suffers from domain shift when deploying models in new environments. While
unsupervised domain adaptation (UDA) addresses this challenge for single tasks,
existing multi-task UDA methods primarily rely on adversarial learning
approaches that are less effective than recent self-training techniques. In
this paper, we introduce FAMDA, a simple yet effective UDA framework that
bridges this gap by leveraging Vision Foundation Models (VFMs) as powerful
teachers. Our approach integrates Segmentation and Depth foundation models into
a self-training paradigm to generate high-quality pseudo-labels for the target
domain, effectively distilling their robust generalization capabilities into a
single, efficient student network. Extensive experiments show that FAMDA
achieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA
multi-task learning (MTL) benchmarks and a challenging new day-to-night
adaptation task. Our framework enables the training of highly efficient models;
a lightweight variant achieves SOTA accuracy while being more than 10$\times$
smaller than foundation models, highlighting FAMDA's suitability for creating
domain-adaptive and efficient models for resource-constrained robotics
applications.

</details>


### [129] [MotionVerse: A Unified Multimodal Framework for Motion Comprehension, Generation and Editing](https://arxiv.org/abs/2509.23635)
*Ruibing Hou,Mingshuang Luo,Hongyu Pan,Hong Chang,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文提出了MotionVerse，一个利用大语言模型（LLM）来理解、生成和编辑单人及多人场景下人体运动的统一框架。


<details>
  <summary>Details</summary>
Motivation: 为了实现对人体运动的高效建模，并解决运动与语言模态间的干扰问题，需要一种能够有效整合多模态信息并保持计算效率的统一框架。

Method: 采用带残差量化的运动分词器将连续运动序列转换为多流离散标记，并提出“延迟并行”建模策略以捕捉流间依赖关系；设计了具有模态特定参数的双塔架构以减轻模态干扰。

Result: 消融实验验证了各组件的有效性，大量实验表明MotionVerse在多种运动相关任务中表现出优越性能。

Conclusion: MotionVerse通过创新的多流离散表示、延迟并行建模和双塔架构，成功实现了对复杂人体运动的理解与生成，展现出强大的多场景适应能力和应用潜力。

Abstract: This paper proposes MotionVerse, a unified framework that harnesses the
capabilities of Large Language Models (LLMs) to comprehend, generate, and edit
human motion in both single-person and multi-person scenarios. To efficiently
represent motion data, we employ a motion tokenizer with residual quantization,
which converts continuous motion sequences into multi-stream discrete tokens.
Furthermore, we introduce a \textit{Delay Parallel} Modeling strategy, which
temporally staggers the encoding of residual token streams. This design enables
LLMs to effectively capture inter-stream dependencies while maintaining
computational efficiency comparable to single-stream modeling. Moreover, to
alleviate modality interference between motion and language, we design a
\textit{dual-tower architecture} with modality-specific parameters, ensuring
stable integration of motion information for both comprehension and generation
tasks. Comprehensive ablation studies demonstrate the effectiveness of each
component in MotionVerse, and extensive experiments showcase its superior
performance across a wide range of motion-relevant tasks.

</details>


### [130] [SCOPE: Semantic Conditioning for Sim2Real Category-Level Object Pose Estimation in Robotics](https://arxiv.org/abs/2509.24572)
*Peter Hönig,Stefan Thalhammer,Jean-Baptiste Weibel,Matthias Hirschmanner,Markus Vincze*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的类别级物体位姿估计方法SCOPE，利用DINOv2特征作为连续语义先验，无需离散类别标签，显著缩小了仿真到现实的差距，并在已知和未知物体类别上实现了优异的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，机器人需要处理未知物体，传统方法依赖离散类别标签，难以泛化到已知类别之外。因此，需要一种能够结合语义理解并具备跨类别泛化能力的位姿估计方法。

Method: 提出SCOPE，一种基于扩散模型的类别级物体位姿估计方法，使用DINOv2特征作为连续语义先验，通过交叉注意力机制融合特征，并结合逼真的合成训练数据和点法线噪声模型来缩小Sim2Real差距，学习跨实例的规范化物体坐标系。

Result: SCOPE在合成数据训练下超越现有最先进方法，在5°5cm指标上相对提升31.9%；在两个实例级数据集上的实验表明其能泛化到未知类别，在未见物体上的抓取成功率高达100%。

Conclusion: SCOPE通过引入连续语义先验和扩散模型，有效解决了类别级位姿估计中的Sim2Real差距问题，并展现出对未知类别物体的强大泛化能力，为开放环境中的机器人操作提供了可靠解决方案。

Abstract: Object manipulation requires accurate object pose estimation. In open
environments, robots encounter unknown objects, which requires semantic
understanding in order to generalize both to known categories and beyond. To
resolve this challenge, we present SCOPE, a diffusion-based category-level
object pose estimation model that eliminates the need for discrete category
labels by leveraging DINOv2 features as continuous semantic priors. By
combining these DINOv2 features with photorealistic training data and a noise
model for point normals, we reduce the Sim2Real gap in category-level object
pose estimation. Furthermore, injecting the continuous semantic priors via
cross-attention enables SCOPE to learn canonicalized object coordinate systems
across object instances beyond the distribution of known categories. SCOPE
outperforms the current state of the art in synthetically trained
category-level object pose estimation, achieving a relative improvement of
31.9\% on the 5$^\circ$5cm metric. Additional experiments on two instance-level
datasets demonstrate generalization beyond known object categories, enabling
grasping of unseen objects from unknown categories with a success rate of up to
100\%. Code available: https://github.com/hoenigpeter/scope.

</details>


### [131] [LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders](https://arxiv.org/abs/2509.23639)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Kangli Zi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级方法LightFair，通过微调文本嵌入来实现公平的文本到图像扩散模型，有效减轻了训练和采样负担，并在Stable Diffusion v1.5上实现了最先进的去偏效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在去偏过程中通常带来沉重的训练或采样负担，且性能不佳，因此需要一种更高效的方法来缓解文本编码器带来的偏见问题。

Method: 提出一种协作式距离约束去偏策略，通过微调文本嵌入来平衡不同属性间的嵌入距离，并设计两阶段文本引导采样策略以保持生成质量。

Result: 在Stable Diffusion v1.5上验证了方法的有效性，仅需1/4的训练成本即达到SOTA去偏效果，且几乎不增加采样负担。

Conclusion: LightFair是一种高效、低负担的公平文本到图像生成方法，能够在不依赖辅助网络的情况下显著提升模型公平性。

Abstract: This paper explores a novel lightweight approach LightFair to achieve fair
text-to-image diffusion models (T2I DMs) by addressing the adverse effects of
the text encoder. Most existing methods either couple different parts of the
diffusion model for full-parameter training or rely on auxiliary networks for
correction. They incur heavy training or sampling burden and unsatisfactory
performance. Since T2I DMs consist of multiple components, with the text
encoder being the most fine-tunable and front-end module, this paper focuses on
mitigating bias by fine-tuning text embeddings. To validate feasibility, we
observe that the text encoder's neutral embedding output shows substantial
skewness across image embeddings of various attributes in the CLIP space. More
importantly, the noise prediction network further amplifies this imbalance. To
finetune the text embedding, we propose a collaborative distance-constrained
debiasing strategy that balances embedding distances to improve fairness
without auxiliary references. However, mitigating bias can compromise the
original generation quality. To address this, we introduce a two-stage
text-guided sampling strategy to limit when the debiased text encoder
intervenes. Extensive experiments demonstrate that LightFair is effective and
efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA
debiasing at just $1/4$ of the training burden, with virtually no increase in
sampling burden. The code is available at https://github.com/boyuh/LightFair.

</details>


### [132] [EfficientMIL: Efficient Linear-Complexity MIL Method for WSI Classification](https://arxiv.org/abs/2509.23640)
*Chengying She,Ben Wang,Xinran Zhang,Dongjie Fan,Jialu Zhang,Chengwei Chen,Lizhuang Liu*

Main category: cs.CV

TL;DR: 提出了一种线性复杂度的MIL方法EfficientMIL，用于全切片图像分类，通过自适应补丁选择模块（APS）和高效序列模型（如GRU、LSTM、Mamba）替代传统的自注意力机制，在多个病理数据集上实现了更优性能和更高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力机制的MIL方法在处理大量图像补丁时计算复杂度高（二次复杂度），限制了其在全切片图像分类中的效率和可扩展性。

Method: 设计了EfficientMIL，采用线性复杂度的序列模型（如GRU、LSTM和Mamba）替代Transformer中的自注意力机制，并引入自适应补丁选择模块（APS）优化补丁选择过程。

Result: 在TCGA-Lung数据集上，EfficientMIL-Mamba达到0.976 AUC和0.933准确率；在CAMELYON16数据集上，EfficientMIL-GRU达到0.990 AUC和0.975准确率，均超越现有SOTA方法。APS模块也被证明比传统选择策略更有效。

Conclusion: EfficientMIL通过降低计算复杂度并提升补丁选择效率，在全切片图像分类任务中实现了更高效且更准确的性能，具有良好的应用前景。

Abstract: Whole slide images (WSIs) classification represents a fundamental challenge
in computational pathology, where multiple instance learning (MIL) has emerged
as the dominant paradigm. Current state-of-the-art (SOTA) MIL methods rely on
attention mechanisms, achieving good performance but requiring substantial
computational resources due to quadratic complexity when processing hundreds of
thousands of patches. To address this computational bottleneck, we introduce
EfficientMIL, a novel linear-complexity MIL approach for WSIs classification
with the patches selection module Adaptive Patch Selector (APS) that we
designed, replacing the quadratic-complexity self-attention mechanisms in
Transformer-based MIL methods with efficient sequence models including
RNN-based GRU, LSTM, and State Space Model (SSM) Mamba. EfficientMIL achieves
significant computational efficiency improvements while outperforming other MIL
methods across multiple histopathology datasets. On TCGA-Lung dataset,
EfficientMIL-Mamba achieved AUC of 0.976 and accuracy of 0.933, while on
CAMELYON16 dataset, EfficientMIL-GRU achieved AUC of 0.990 and accuracy of
0.975, surpassing previous state-of-the-art methods. Extensive experiments
demonstrate that APS is also more effective for patches selection than
conventional selection strategies.

</details>


### [133] [Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic Environments](https://arxiv.org/abs/2509.24731)
*Luis F. W. Batista,Tom Bourbon,Cedric Pradalier*

Main category: cs.CV

TL;DR: 该研究利用偏振成像技术缓解水面反光对漂浮物语义分割的影响，在PoTATO数据集上评估了多模态融合网络的性能，发现偏振信息可提升低对比度物体的检测精度并减少反射引起的误检，但会增加计算负担和模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 水面反光和变化的户外光照常影响漂浮物分割的准确性，传统RGB图像难以有效应对这些问题，因此需要探索能抑制反光、提升分割性能的新方法。

Method: 采用偏振成像技术获取包含偏振信息的图像，在公开的PoTATO数据集（含内陆水道中塑料瓶的偏振图像）上 benchmark 当前先进的多模态融合网络，并与基于传统模型的单图像基线方法进行比较。

Result: 实验结果表明，偏振信息有助于恢复低对比度目标并抑制由反射引起的误检，相比RGB输入提升了平均交并比（mIoU）并降低了轮廓误差；然而，额外的偏振通道增加了模型体积和计算负荷，并可能引入新的误检。

Conclusion: 偏振成像有助于提升水上漂浮物的分割精度，尤其在处理反光和低对比度场景时优势明显，但需权衡其带来的计算成本和模型复杂性；通过提供可复现的基准和开源代码，帮助研究者判断偏振相机是否适用于其应用场景，并推动相关研究发展。

Abstract: Accurate segmentation of floating debris on water is often compromised by
surface glare and changing outdoor illumination. Polarimetric imaging offers a
single-sensor route to mitigate water-surface glare that disrupts semantic
segmentation of floating objects. We benchmark state-of-the-art fusion networks
on PoTATO, a public dataset of polarimetric images of plastic bottles in inland
waterways, and compare their performance with single-image baselines using
traditional models. Our results indicate that polarimetric cues help recover
low-contrast objects and suppress reflection-induced false positives, raising
mean IoU and lowering contour error relative to RGB inputs. These sharper masks
come at a cost: the additional channels enlarge the models increasing the
computational load and introducing the risk of new false positives. By
providing a reproducible, diagnostic benchmark and publicly available code, we
hope to help researchers choose if polarized cameras are suitable for their
applications and to accelerate related research.

</details>


### [134] [ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation](https://arxiv.org/abs/2509.24878)
*Jiuhong Xiao,Roshan Nayak,Ning Zhang,Daniel Tortei,Giuseppe Loianno*

Main category: cs.CV

TL;DR: 本文提出了ThermalGen，一种基于自适应流的生成模型，用于RGB到热成像（RGB-T）图像转换，解决了同步配对数据稀缺的问题，并在多种条件下实现了高质量的热图像合成。


<details>
  <summary>Details</summary>
Motivation: 由于同步且校准的RGB-热成像图像对稀缺，阻碍了视觉-热传感器融合与跨模态任务的发展，因此需要有效的RGB-T图像转换方法来生成训练所需的热图像。

Method: 提出ThermalGen模型，采用基于流的生成架构，结合RGB图像条件化设计和风格解耦机制，并利用八个公开及三个新构建的大规模卫星-航空与地面RGB-T配对数据集进行训练。

Result: 在多个RGB-T基准上的实验表明，ThermalGen在翻译性能上达到或优于现有的GAN和扩散模型方法，能够合成在视角、传感器特性和环境条件方面具有显著变化的热图像。

Conclusion: ThermalGen是首个能够在多样化现实条件下生成高质量热图像的RGB-T转换模型，为多模态感知任务提供了有效的数据增强解决方案。

Abstract: Paired RGB-thermal data is crucial for visual-thermal sensor fusion and
cross-modality tasks, including important applications such as multi-modal
image alignment and retrieval. However, the scarcity of synchronized and
calibrated RGB-thermal image pairs presents a major obstacle to progress in
these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image
translation has emerged as a promising solution, enabling the synthesis of
thermal images from abundant RGB datasets for training purposes. In this study,
we propose ThermalGen, an adaptive flow-based generative model for RGB-T image
translation, incorporating an RGB image conditioning architecture and a
style-disentangled mechanism. To support large-scale training, we curated eight
public satellite-aerial, aerial, and ground RGB-T paired datasets, and
introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day,
Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor
types, and geographic regions. Extensive evaluations across multiple RGB-T
benchmarks demonstrate that ThermalGen achieves comparable or superior
translation performance compared to existing GAN-based and diffusion-based
methods. To our knowledge, ThermalGen is the first RGB-T image translation
model capable of synthesizing thermal images that reflect significant
variations in viewpoints, sensor characteristics, and environmental conditions.
Project page: http://xjh19971.github.io/ThermalGen

</details>


### [135] [Griffin: Generative Reference and Layout Guided Image Composition](https://arxiv.org/abs/2509.23643)
*Aryan Mikaeili,Amirhossein Alimohammadi,Negar Hassanpour,Ali Mahdavi-Amiri,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的多图像布局控制方法，通过图像而非文本指定内容，并精确控制每个元素的位置，实现了对象和部件级别的简单且显式的组合控制。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在生成逼真图像方面取得了进展，但基于文本的控制在需要更明确指导时存在局限性，因此需要一种能够精确定义内容及其位置的方法以实现更精细的控制。

Method: 该方法采用基于图像的参考输入，每个参考仅需一张图像，通过非训练方式实现多图像布局控制，提供对对象和部件级组成的显式控制。

Result: 实验表明该方法在多种图像合成任务中均表现出有效性，能够实现精确的内容放置和灵活的组合控制。

Conclusion: 该方法为文本到图像生成提供了更直观和精确的控制手段，尤其适用于需要细粒度布局控制的应用场景。

Abstract: Text-to-image models have achieved a level of realism that enables the
generation of highly convincing images. However, text-based control can be a
limiting factor when more explicit guidance is needed. Defining both the
content and its precise placement within an image is crucial for achieving
finer control. In this work, we address the challenge of multi-image layout
control, where the desired content is specified through images rather than
text, and the model is guided on where to place each element. Our approach is
training-free, requires a single image per reference, and provides explicit and
simple control for object and part-level composition. We demonstrate its
effectiveness across various image composition tasks.

</details>


### [136] [Sparse-Up: Learnable Sparse Upsampling for 3D Generation with High-Fidelity Textures](https://arxiv.org/abs/2509.23646)
*Lu Xiao,Jiale Zhang,Yang Liu,Taicheng Huang,Xin Tian*

Main category: cs.CV

TL;DR: 提出Sparse-Up框架，通过稀疏体素、表面锚定和视域分割来高效建模高保真纹理，保留高频细节并减少内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持跨视角一致性与高分辨率细节之间难以兼顾，导致纹理撕裂或细节丢失。

Method: 采用稀疏体素引导纹理重建，结合表面锚定将体素约束在网格表面，并利用图像块指导的体素划分策略进行局部监督和梯度反传。

Result: 减少了70%以上的冗余体素，显著降低高分辨率训练中的内存消耗，同时保持几何一致性和纹理高频细节。

Conclusion: Sparse-Up在保证多视角一致性的同时突破了分辨率限制，实现了内存高效且高保真的纹理建模。

Abstract: The creation of high-fidelity 3D assets is often hindered by a 'pixel-level
pain point': the loss of high-frequency details. Existing methods often trade
off one aspect for another: either sacrificing cross-view consistency,
resulting in torn or drifting textures, or remaining trapped by the resolution
ceiling of explicit voxels, forfeiting fine texture detail. In this work, we
propose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework
that effectively preserves high-frequency details. We use sparse voxels to
guide texture reconstruction and ensure multi-view consistency, while
leveraging surface anchoring and view-domain partitioning to break through
resolution constraints. Surface anchoring employs a learnable upsampling
strategy to constrain voxels to the mesh surface, eliminating over 70% of
redundant voxels present in traditional voxel upsampling. View-domain
partitioning introduces an image patch-guided voxel partitioning scheme,
supervising and back-propagating gradients only on visible local patches.
Through these two strategies, we can significantly reduce memory consumption
during high-resolution voxel training without sacrificing geometric
consistency, while preserving high-frequency details in textures.

</details>


### [137] [Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events](https://arxiv.org/abs/2509.25146)
*Richeek Das,Kostas Daniilidis,Pratik Chaudhari*

Main category: cs.CV

TL;DR: 本文提出了一种名为Fast Feature Field（F³）的事件相机数据表示方法，通过从过去事件预测未来事件来学习，保持场景结构和运动信息，具有高效、鲁棒和适用于多种下游任务的优点。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据稀疏且具有高时间分辨率，传统方法难以有效利用其时空信息，因此需要一种能够高效建模并保留结构与运动信息的新表示方法。

Method: 提出Fast Feature Field（F³），结合多分辨率哈希编码和深度集（deep sets）技术，将事件流表示为连续的时空多通道图像，并通过自监督方式预测未来事件进行训练。

Result: F³在光流估计、语义分割和单目深度估计任务上达到SOTA性能，支持HD分辨率120Hz和VGA分辨率440Hz的高效计算，并在多种平台、环境和传感器条件下验证了泛化能力。

Conclusion: F³是一种高效、鲁棒且通用的事件相机数据表示方法，能有效支持多种高帧率视觉任务，具有广泛的机器人应用前景。

Abstract: This paper develops a mathematical argument and algorithms for building
representations of data from event-based cameras, that we call Fast Feature
Field ($\text{F}^3$). We learn this representation by predicting future events
from past events and show that it preserves scene structure and motion
information. $\text{F}^3$ exploits the sparsity of event data and is robust to
noise and variations in event rates. It can be computed efficiently using ideas
from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and
440 Hz at VGA resolutions. $\text{F}^3$ represents events within a contiguous
spatiotemporal volume as a multi-channel image, enabling a range of downstream
tasks. We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates). Our implementations can predict these tasks at
25-75 Hz at HD resolution.

</details>


### [138] [ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis](https://arxiv.org/abs/2509.23652)
*Congzhi Zhang,Zhibin Wang,Yinchao Ma,Jiawei Peng,Yihan Wang,Qiang Zhou,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了ReWatch数据集和ReWatch-R1模型，以推动基于强化学习与可验证奖励的复杂视频推理。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏用于有效启动强化学习与可验证奖励（RLVR）的多跳复杂问题和高质量、基于视频的思维链（CoT）数据，限制了其在复杂视频推理中的应用。

Method: 提出一个多阶段合成流程，构建包含ReWatch-Caption、ReWatch-QA和ReWatch-CoT三部分的大规模数据集；设计基于多智能体ReAct框架的CoT合成方法，模拟人类“重看”过程生成基于视频的推理轨迹；采用监督微调（SFT）和RLVR框架结合观察与推理（O&R）奖励机制训练ReWatch-R1模型。

Result: ReWatch-R1在五个具有挑战性的视频推理基准上取得了最先进的平均性能。

Conclusion: ReWatch数据集和O&R奖励机制有效提升了LVLM在复杂视频多步推理任务中的表现，显著减少了幻觉问题，推动了RLVR在视频理解中的应用。

Abstract: While Reinforcement Learning with Verifiable Reward (RLVR) significantly
advances image reasoning in Large Vision-Language Models (LVLMs), its
application to complex video reasoning remains underdeveloped. This gap stems
primarily from a critical data bottleneck: existing datasets lack the
challenging, multi-hop questions and high-quality, video-grounded
Chain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address
this, we introduce ReWatch, a large-scale dataset built to foster advanced
video reasoning. We propose a novel multi-stage synthesis pipeline to
synthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT.
A core innovation is our Multi-Agent ReAct framework for CoT synthesis, which
simulates a human-like "re-watching" process to generate video-grounded
reasoning traces by explicitly modeling information retrieval and verification.
Building on this dataset, we develop ReWatch-R1 by post-training a strong
baseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This
framework incorporates a novel Observation \& Reasoning (O\&R) reward mechanism
that evaluates both the final answer's correctness and the reasoning's
alignment with video content, directly penalizing hallucination. Our
experiments show that ReWatch-R1 achieves state-of-the-art average performance
on five challenging video reasoning benchmarks.

</details>


### [139] [LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training](https://arxiv.org/abs/2509.23661)
*Xiang An,Yin Xie,Kaicheng Yang,Wenkang Zhang,Xiuwei Zhao,Zheng Cheng,Yirui Wang,Songcen Xu,Changrui Chen,Chunsheng Wu,Huajie Tan,Chunyuan Li,Jing Yang,Jie Yu,Xiyao Wang,Bin Qin,Yumeng Wang,Zizhen Yan,Ziyong Feng,Ziwei Liu,Bo Li,Jiankang Deng*

Main category: cs.CV

TL;DR: LLaVA-OneVision-1.5 是一个高效、开源且可复现的大型多模态模型系列，通过大规模数据集和低成本训练框架，在多项基准测试中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 构建高性能、低成本且完全从零开始训练的高质量视觉-语言模型，并推动开源社区的发展。

Method: 提出包含85M预训练数据集和26M指令数据集的大规模平衡数据集，采用离线并行数据打包策略实现高效的端到端训练框架。

Result: LLaVA-OneVision-1.5-8B在27个基准中的18个上优于Qwen2.5-VL-7B，LLaVA-OneVision-1.5-4B在所有27个基准上均超越Qwen2.5-VL-3B。

Conclusion: LLaVA-OneVision-1.5 实现了最先进的性能，同时显著降低了计算和财务成本，具备良好的可扩展性和开源潜力。

Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models
(LMMs) that achieve state-of-the-art performance with significantly reduced
computational and financial costs. Different from the existing works,
LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for
building high-quality vision-language models entirely from scratch. The
LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale
Curated Datasets: We construct an 85M concept-balanced pretraining dataset
LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction
dataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed
multimodal tokens. (2) Efficient Training Framework: We develop a complete
end-to-end efficient training framework leveraging an offline parallel data
packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a
$16,000 budget. (3) State-of-the-art Performance: Experimental results
demonstrate that LLaVA-OneVision1.5 yields exceptionally competitive
performance across a broad range of downstream tasks. Specifically,
LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and
LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We
anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community
to await further updates.

</details>


### [140] [HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score](https://arxiv.org/abs/2509.23663)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Peter A. Beerel*

Main category: cs.CV

TL;DR: 提出了一种无需训练的分层视觉令牌剪枝方法HIVTP，通过中间层注意力图评估令牌重要性，显著提升视觉语言模型的推理效率而不损失精度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉编码器输出的大量视觉令牌严重影响推理效率，且许多令牌冗余，需有效剪枝以提升效率。

Method: 利用视觉编码器中间层的注意力图计算令牌重要性得分，将1D令牌序列重塑为2D空间布局，分全局和局部两个阶段保留重要令牌：全局阶段按区域保留高分令牌，局部阶段在小窗口内保留最重要令牌。

Result: 在LLaVA-v1.5-7B和LLaVA-Next-7B上，TTFT最多降低50.0%和55.1%，生成吞吐量提升60.9%和47.3%，且保持甚至提升模型准确率。

Conclusion: HIVTP是一种高效、无需训练的视觉令牌剪枝方法，在显著提升推理速度的同时保持或改善模型性能，优于先前方法。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities on diverse
multimodal tasks. However, the large number of visual tokens output by the
vision encoder severely hinders inference efficiency, and prior studies have
shown that many of these tokens are not important and can therefore be safely
pruned. In this work, we propose HIVTP, a training-free method to improve VLMs
efficiency via hierarchical visual token pruning using a novel
middle-layer-based importance score. Specifically, we utilize attention maps
extracted from the middle layers of the vision encoder, which better reflect
fine-grained and object-level attention, to estimate visual token importance.
Based on this, we propose a hierarchical visual token pruning method to retain
both globally and locally important visual tokens. Specifically, we reshape the
1-D visual token sequence output by the vision encoder into a 2-D spatial
layout. In the global retaining stage, we divide the image into regions and
retain tokens with higher importance scores in each region; in the local
retaining stage, we then divide the image into small windows and retain the
most important token in each local window. Experimental results show that our
proposed method, HIVTP, can reduce the time-to-first-token (TTFT) of
LLaVA-v1.5-7B and LLaVA-Next-7B by up to 50.0% and 55.1%, respectively, and
improve the token generation throughput by up to 60.9% and 47.3%, without
sacrificing accuracy, and even achieving improvements on certain benchmarks.
Compared with prior works, HIVTP achieves better accuracy while offering higher
inference efficiency.

</details>


### [141] [Token Merging via Spatiotemporal Information Mining for Surgical Video Understanding](https://arxiv.org/abs/2509.23672)
*Xixi Jiang,Chen Yang,Dong Zhang,Pingcheng Dong,Xin Yang,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: 提出了一种面向手术视频理解的时空信息挖掘 token 合并方法 STIM-TM，通过解耦的时间和空间维度冗余减少策略，在保持高精度的同时显著提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 现有 Vision Transformer 在处理手术视频时计算开销大，且现有 token 合并方法未充分考虑视频数据的时空结构和信息分布异质性，导致性能次优。

Method: 提出 STIM-TM，采用解耦策略：时间维度上对连续帧中空间对应 token 进行显著性加权合并以保持序列连续性；空间维度上通过时间稳定性分析优先合并静态 token，保护包含关键手术信息的动态区域。该方法无需训练。

Result: STIM-TM 实现了超过 65% 的 GFLOPs 降低，在多个手术视频任务中保持有竞争力的精度，并支持长序列手术视频的高效训练。

Conclusion: STIM-TM 是首个专为手术视频理解设计的 token 合并方法，有效平衡了效率与性能，解决了手术视频分析中的计算瓶颈问题。

Abstract: Vision Transformer models have shown impressive effectiveness in the surgical
video understanding tasks through long-range dependency modeling. However,
current methods suffer from prohibitive computational costs due to processing
massive spatiotemporal tokens across video frames. While prior work on token
merging has advanced model efficiency, they fail to adequately consider the
inherent spatiotemporal structure of video data and overlook the heterogeneous
nature of information distribution, leading to suboptimal performance. In this
paper, we propose a spatiotemporal information mining token merging (STIM-TM)
method, representing the first dedicated approach for surgical video
understanding. STIM-TM introduces a decoupled strategy that reduces token
redundancy along temporal and spatial dimensions independently. Specifically,
the temporal component merges spatially corresponding tokens from consecutive
frames using saliency weighting, preserving critical sequential information and
maintaining continuity. Meanwhile, the spatial component prioritizes merging
static tokens through temporal stability analysis, protecting dynamic regions
containing essential surgical information. Operating in a training-free manner,
STIM-TM achieves significant efficiency gains with over $65\%$ GFLOPs reduction
while preserving competitive accuracy across comprehensive surgical video
tasks. Our method also supports efficient training of long-sequence surgical
videos, addressing computational bottlenecks in surgical applications.

</details>


### [142] [RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks](https://arxiv.org/abs/2509.23673)
*Amit Agarwal,Hitesh Laxmichand Patel,Srikant Panda,Hansa Meghwani,Jyotika Singh,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 本文提出了Region Comprehension Index (RCI)，用于量化多模态基准数据集中全局与局部视觉信息的依赖程度，发现大多数现有基准倾向于局部推理并存在空间偏差，可能影响实际应用，RCI有助于构建更鲁棒的多模态系统。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型评估基准未明确区分模型是依赖全局理解还是局部视觉线索进行推理，缺乏对真实世界推理能力的有效评估。

Method: 提出Region Comprehension Index (RCI)，通过比较参考模型在图像局部区域和完整图像上的表现，系统衡量数据集对全局或局部视觉信息的依赖程度。

Result: 在13个常用多模态基准上应用RCI发现，大多数基准偏向局部推理，存在显著的空间偏差，表明其在真实场景中的可靠性可能受限。

Conclusion: RCI为诊断和缓解多模态基准中的空间偏差提供了有效工具，有助于推动更具鲁棒性和实用性的多模态系统发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.

</details>


### [143] [MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain Segmentation via Multi-scale Self-Distilled Fusion Strategy](https://arxiv.org/abs/2509.23677)
*Dayu Tan,Ziwei Zhang,Yansan Su,Xin Peng,Yike Dai,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D多模态图像分割框架MSD-KMamba，结合双向空间感知与多尺度自蒸馏，在保持高计算效率的同时显著提升了分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有CNN-Transformer混合模型因使用高复杂度全局注意力机制导致计算资源消耗大，且难以兼顾分割精度与效率。

Method: 引入双向空间感知分支以捕获长距离空间依赖，并结合多尺度自蒸馏融合策略增强分层特征表示和语义信息传递。

Result: 在多个标准数据集上实验表明，MSD-KMamba在分割准确性、鲁棒性和泛化性方面优于现有最先进方法，同时有效缓解了体积分割中的二次计算复杂度瓶颈。

Conclusion: MSD-KMamba在保证高效计算和良好可扩展性的同时，解决了全局感知不足的问题，为3D医学图像分割提供了一个高性能且高效的解决方案。

Abstract: Numerous CNN-Transformer hybrid models rely on high-complexity global
attention mechanisms to capture long-range dependencies, which introduces
non-linear computational complexity and leads to significant resource
consumption. Although knowledge distillation and sparse attention mechanisms
can improve efficiency, they often fall short of delivering the high
segmentation accuracy necessary for complex tasks. Balancing model performance
with computational efficiency remains a critical challenge. In this work, we
propose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba,
which integrates bidirectional spatial perception with multi-scale
self-distillation. The bidirectional spatial aware branch effectively captures
long-range spatial context dependencies across brain regions, while also
incorporating a powerful nonlinear feature extraction mechanism that further
enhances the model's ability to learn complex and heterogeneous patterns. In
addition, the proposed multi-scale self-distilled fusion strategy strengthens
hierarchical feature representations and improves the transfer of semantic
information at different resolution levels. By jointly leveraging the
bidirectional spatial perception branch and the multi-scale self-distilled
fusion strategy, our framework effectively mitigates the bottleneck of
quadratic computational complexity in volumetric segmentation, while
simultaneously addressing the limitation of insufficient global perception.
Extensive experiments on multiple standard benchmark datasets demonstrate that
MSD-KMamba consistently outperforms state-of-the-art methods in segmentation
accuracy, robustness, and generalization, while maintaining high computational
efficiency and favorable scalability. The source code of MSD-KMamba is publicly
available at https://github.com/daimao-zhang/MSD-KMamba.

</details>


### [144] [QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification](https://arxiv.org/abs/2509.23681)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Mingqiang Wu,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出QuantSparse框架，结合模型量化与注意力稀疏化，通过多尺度显著注意力蒸馏和二阶梯度稀疏注意力重参数化，在大幅降低存储和计算成本的同时显著提升视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在视频生成上表现优异，但计算和内存开销大；单独使用量化或稀疏化压缩会导致性能严重下降，简单结合效果不佳，需解决稀疏化加剧量化噪声的问题。

Method: 提出QuantSparse框架：1）多尺度显著注意力蒸馏，利用全局结构引导和局部显著性监督缓解量化偏差；2）二阶梯度稀疏注意力重参数化，利用二阶残差的时间稳定性恢复稀疏导致的信息损失。

Result: 在HunyuanVideo-13B上实现20.88 PSNR，优于Q-VDiT的16.85 PSNR，同时存储减少3.68倍，端到端推理加速1.88倍。

Conclusion: QuantSparse有效协同量化与稀疏化，显著提升压缩效率与生成性能，为大规模扩散Transformer的部署提供了可行方案。

Abstract: Diffusion transformers exhibit remarkable video generation capability, yet
their prohibitive computational and memory costs hinder practical deployment.
Model quantization and attention sparsification are two promising directions
for compression, but each alone suffers severe performance degradation under
aggressive compression. Combining them promises compounded efficiency gains,
but naive integration is ineffective. The sparsity-induced information loss
exacerbates quantization noise, leading to amplified attention shifts. To
address this, we propose \textbf{QuantSparse}, a unified framework that
integrates model quantization with attention sparsification. Specifically, we
introduce \textit{Multi-Scale Salient Attention Distillation}, which leverages
both global structural guidance and local salient supervision to mitigate
quantization-induced bias. In addition, we develop \textit{Second-Order Sparse
Attention Reparameterization}, which exploits the temporal stability of
second-order residuals to efficiently recover information lost under sparsity.
Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88
PSNR, substantially outperforming the state-of-the-art quantization baseline
Q-VDiT (16.85 PSNR), while simultaneously delivering a \textbf{3.68$\times$}
reduction in storage and \textbf{1.88$\times$} acceleration in end-to-end
inference. Our code will be released in
https://github.com/wlfeng0509/QuantSparse.

</details>


### [145] [HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection](https://arxiv.org/abs/2509.23690)
*Siyuan Gao,Jiashu Yao,Haoyu Wen,Yuhang Guo,Zeming Liu,Heyan Huang*

Main category: cs.CV

TL;DR: 提出HomeSafeBench，一个包含12,900个数据点的基准，用于评估基于视觉语言模型的具身代理在家庭安全检查任务中的表现，解决了现有基准过度简化和视角静态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准使用文本描述和固定视角，无法准确评估具身代理在家庭安全检查中的真实能力，尤其影响对视觉信息依赖强的VLMs的评估。

Method: 构建HomeSafeBench，提供模拟家庭环境中的动态第一人称视角图像，覆盖五类常见家庭安全隐患，并允许代理自由探索以获取多视角信息。

Result: 主流VLM在HomeSafeBench上的最佳F1分数仅为10.23%，表明当前模型在识别安全隐患和探索策略选择方面存在显著不足。

Conclusion: HomeSafeBench能更真实地评估VLM驱动的具身代理在复杂家庭环境中的安全检测能力，为未来相关研究提供重要参考。

Abstract: Embodied agents can identify and report safety hazards in the home
environments. Accurately evaluating their capabilities in home safety
inspection tasks is curcial, but existing benchmarks suffer from two key
limitations. First, they oversimplify safety inspection tasks by using textual
descriptions of the environment instead of direct visual information, which
hinders the accurate evaluation of embodied agents based on Vision-Language
Models (VLMs). Second, they use a single, static viewpoint for environmental
observation, which restricts the agents' free exploration and cause the
omission of certain safety hazards, especially those that are occluded from a
fixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a
benchmark with 12,900 data points covering five common home safety hazards:
fire, electric shock, falling object, trips, and child safety. HomeSafeBench
provides dynamic first-person perspective images from simulated home
environments, enabling the evaluation of VLM capabilities for home safety
inspection. By allowing the embodied agents to freely explore the room,
HomeSafeBench provides multiple dynamic perspectives in complex environments
for a more thorough inspection. Our comprehensive evaluation of mainstream VLMs
on HomeSafeBench reveals that even the best-performing model achieves an
F1-score of only 10.23%, demonstrating significant limitations in current VLMs.
The models particularly struggle with identifying safety hazards and selecting
effective exploration strategies. We hope HomeSafeBench will provide valuable
reference and support for future research related to home security inspections.
Our dataset and code will be publicly available soon.

</details>


### [146] [Confidence Aware SSD Ensemble with Weighted Boxes Fusion for Weapon Detection](https://arxiv.org/abs/2509.23697)
*Atharva Jadhav,Arush Karekar,Manas Divekar,Shachi Natu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多种特征提取骨干网络的SSD模型集成方法，结合加权框融合（WBF）策略，显著提升了复杂环境下武器检测的鲁棒性和精度，mAP达到0.838，优于单一模型和其他融合方法。


<details>
  <summary>Details</summary>
Motivation: 由于遮挡、光照变化和复杂背景等问题，传统单模型在公共安全监控中的武器检测表现不够鲁棒，亟需提升检测系统的准确性和稳定性。

Method: 采用VGG16、ResNet50、EfficientNet和MobileNetV3作为SSD模型的骨干网络构建多样化集成模型，并使用加权框融合（WBF）方法结合预测结果，特别比较了不同置信度评分策略对融合效果的影响。

Result: 采用'max'置信度评分策略的WBF方法实现了0.838的mAP，相比最佳单模型性能提升了2.948%，且在各种融合策略中表现最优。

Conclusion: 模型集成的多样性与融合策略同样重要，置信度感知的融合机制是提升检测精度的关键，该方法为实时监控场景下的武器检测提供了更鲁棒的解决方案。

Abstract: The safety and security of public spaces is of vital importance, driving the
need for sophisticated surveillance systems capable of accurately detecting
weapons, which are often hampered by issues like partial occlusion, varying
lighting, and cluttered backgrounds. While single-model detectors are advanced,
they often lack robustness in these challenging conditions. This paper presents
the hypothesis that ensemble of Single Shot Multibox Detector (SSD) models with
diverse feature extraction backbones can significantly enhance detection
robustness. To leverage diverse feature representations, individual SSD models
were trained using a selection of backbone networks: VGG16, ResNet50,
EfficientNet, and MobileNetV3. The study is conducted on a dataset consisting
of images of three distinct weapon classes: guns, heavy weapons and knives. The
predictions from these models are combined using the Weighted Boxes Fusion
(WBF) method, an ensemble technique designed to optimize bounding box accuracy.
Our key finding is that the fusion strategy is as critical as the ensemble's
diversity, a WBF approach using a 'max' confidence scoring strategy achieved a
mean Average Precision (mAP) of 0.838. This represents a 2.948% relative
improvement over the best-performing single model and consistently outperforms
other fusion heuristics. This research offers a robust approach to enhancing
real-time weapon detection capabilities in surveillance applications by
demonstrating that confidence-aware fusion is a key mechanism for improving
accuracy metrics of ensembles.

</details>


### [147] [INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception](https://arxiv.org/abs/2509.23700)
*Yunjiang Xu,Lingzhi Li,Jin Wang,Yupeng Ouyang,Benyuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为INSTINCT的新型协作感知框架，通过实例级交互、质量感知过滤和跨代理特征融合，在显著降低通信带宽的同时大幅提升了协同感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法在LiDAR场景下带宽消耗高且性能有限，缺乏高效的实例级交互机制，因此需要一种更高效、低带宽的协作感知架构。

Method: 提出INSTINCT框架，包含三个核心组件：质量感知过滤机制、双分支检测路由方案和跨代理局部实例融合模块，并改进了GT采样技术以支持混合实例特征训练。

Result: 在DAIR-V2X和V2V4Real数据集上，相比最先进方法通信带宽分别降至1/281和1/264，检测精度提升13.23%和33.08%。

Conclusion: INSTINCT有效平衡了协同感知中的通信效率与检测性能，显著优于现有方法，具备实际应用潜力。

Abstract: Collaborative perception systems overcome single-vehicle limitations in
long-range detection and occlusion scenarios by integrating multi-agent sensory
data, improving accuracy and safety. However, frequent cooperative interactions
and real-time requirements impose stringent bandwidth constraints. Previous
works proves that query-based instance-level interaction reduces bandwidth
demands and manual priors, however, LiDAR-focused implementations in
collaborative perception remain underdeveloped, with performance still trailing
state-of-the-art approaches. To bridge this gap, we propose INSTINCT
(INSTance-level INteraCtion ArchiTecture), a novel collaborative perception
framework featuring three core components: 1) a quality-aware filtering
mechanism for high-quality instance feature selection; 2) a dual-branch
detection routing scheme to decouple collaboration-irrelevant and
collaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion
module to aggregate local hybrid instance features. Additionally, we enhance
the ground truth (GT) sampling technique to facilitate training with diverse
hybrid instance features. Extensive experiments across multiple datasets
demonstrate that INSTINCT achieves superior performance. Specifically, our
method achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and
V2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared
to state-of-the-art methods. The code is available at
https://github.com/CrazyShout/INSTINCT.

</details>


### [148] [CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement](https://arxiv.org/abs/2509.23708)
*Boseong Jeon,Junghyuk Lee,Jimin Park,Kwanyoung Kim,Jingi Jung,Sangwon Lee,Hyunbo Shim*

Main category: cs.CV

TL;DR: 提出CrimEdit，通过在统一模型中联合训练去除和插入任务的嵌入，并利用无分类器引导策略，实现高效的对象编辑，包括对象及其效果的去除、可控的效果合成以及单步去噪中的对象移动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理对象影响（如阴影、反射）方面取得进展，但在统一模型中应用无分类器引导来同时处理对象去除和插入任务的效果仍缺乏探索。为此，本文旨在填补这一空白，提升复合编辑的效率。

Method: 提出CrimEdit，联合训练去除与插入任务的嵌入，在单一模型中结合无分类器引导；扩展任务提示以支持空间上不同区域的应用，实现单步去噪中的对象移动。

Result: 实验表明，CrimEdit在对象去除、可控效果插入和对象移动方面表现优越，且无需额外训练或分离的去除与插入阶段。

Conclusion: CrimEdit通过统一建模和引导机制，有效提升了对象编辑的性能与效率，支持多种编辑操作的一体化处理。

Abstract: Recent works on object removal and insertion have enhanced their performance
by handling object effects such as shadows and reflections, using diffusion
models trained on counterfactual datasets. However, the performance impact of
applying classifier-free guidance to handle object effects across removal and
insertion tasks within a unified model remains largely unexplored. To address
this gap and improve efficiency in composite editing, we propose CrimEdit,
which jointly trains the task embeddings for removal and insertion within a
single model and leverages them in a classifier-free guidance scheme --
enhancing the removal of both objects and their effects, and enabling
controllable synthesis of object effects during insertion. CrimEdit also
extends these two task prompts to be applied to spatially distinct regions,
enabling object movement (repositioning) within a single denoising step. By
employing both guidance techniques, extensive experiments show that CrimEdit
achieves superior object removal, controllable effect insertion, and efficient
object movement without requiring additional training or separate removal and
insertion stages.

</details>


### [149] [PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease](https://arxiv.org/abs/2509.23719)
*Shuai Shao,Shu Jiang,Shiyuan Zhao,Di Yang,Yan Wang,Yutong Bai,Jianguo Zhang,Jiangtao Wang*

Main category: cs.CV

TL;DR: 提出了一种端到端的帕金森病自动诊断方法PD-Diag-Net，利用MRI扫描和临床先验知识实现高精度诊断。


<details>
  <summary>Details</summary>
Motivation: 当前帕金森病诊断依赖专家经验，流程复杂，导致早期检测延迟，亟需自动化、准确的诊断方法。

Method: 设计了PD-Diag-Net框架，包括MRI预处理模块，并引入脑区相关性和衰老先验知识，构建特征聚合与诊断模块，从原始MRI直接进行风险评估与辅助诊断。

Result: 在外部测试中准确率达86%，早期诊断准确率超96%，性能优于现有方法20%以上。

Conclusion: PD-Diag-Net有效提升了帕金森病的诊断准确性与可解释性，具有良好的临床应用前景。

Abstract: Parkinson's disease (PD) is a common neurodegenerative disorder that severely
diminishes patients' quality of life. Its global prevalence has increased
markedly in recent decades. Current diagnostic workflows are complex and
heavily reliant on neurologists' expertise, often resulting in delays in early
detection and missed opportunities for timely intervention. To address these
issues, we propose an end-to-end automated diagnostic method for PD, termed
PD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly
from raw MRI scans. This framework first introduces an MRI Pre-processing
Module (MRI-Processor) to mitigate inter-subject and inter-scanner variability
by flexibly integrating established medical imaging preprocessing tools. It
then incorporates two forms of clinical prior knowledge: (1)
Brain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions
strongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior),
which reflects the accelerated aging typically observed in PD-associated
regions. Building on these priors, we design two dedicated modules: the
Relevance-Prior Guided Feature Aggregation Module (Aggregator), which guides
the model to focus on PD-associated regions at the inter-subject level, and the
Age-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps
as auxiliary constraints at the intra-subject level to enhance diagnostic
accuracy and clinical interpretability. Furthermore, we collected external test
data from our collaborating hospital. Experimental results show that
PD-Diag-Net achieves 86\% accuracy on external tests and over 96% accuracy in
early-stage diagnosis, outperforming existing advanced methods by more than
20%.

</details>


### [150] [DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion](https://arxiv.org/abs/2509.23723)
*Zijun Li,Hongyu Yan,Shijie Li,Kunming Luo,Li Lu,Xulei Yang,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的粗到精框架DiffPCN，用于点云补全。该方法通过将部分点云投影为深度图像，利用DepthLDM生成多视角完整深度图以获得粗略点云，再通过去噪和关联感知上采样提升质量，显著提高了补全的几何精度和形状完整性。


<details>
  <summary>Details</summary>
Motivation: 由于点云的非结构化和不规则性，现有扩散模型在点云补全中的应用尚未充分探索。因此，需要一种能有效利用扩散模型强大生成能力并适应点云特性的补全方法。

Method: DiffPCN分为两个阶段：第一阶段将不规则的部分点云投影为结构化深度图像，并作为条件输入至DepthLDM生成多视角完整深度图，进而重建出粗略点云；第二阶段设计点去噪网络去除由扩散模型引入的异常值，并采用关联感知点上采样器，利用输入点云与粗略点之间的局部关联特征进行上采样，得到密集且高保真的完整点云。

Result: 实验结果表明，DiffPCN在几何精度和形状完整性方面均达到最先进水平，显著提升了点云补全的鲁棒性和一致性。

Conclusion: DiffPCN成功地将潜在扩散模型应用于点云补全任务，通过粗到精的两阶段策略有效解决了点云非结构化带来的挑战，在补全质量和稳定性方面表现出色。

Abstract: Latent diffusion models (LDMs) have demonstrated remarkable generative
capabilities across various low-level vision tasks. However, their potential
for point cloud completion remains underexplored due to the unstructured and
irregular nature of point clouds. In this work, we propose DiffPCN, a novel
diffusion-based coarse-to-fine framework for point cloud completion. Our
approach comprises two stages: an initial stage for generating coarse point
clouds, and a refinement stage that improves their quality through point
denoising and upsampling. Specifically, we first project the unordered and
irregular partial point cloud into structured depth images, which serve as
conditions for a well-designed DepthLDM to synthesize completed multi-view
depth images that are used to form coarse point clouds. In this way, our
DiffPCN can yield high-quality and high-completeness coarse point clouds by
leveraging LDM' s powerful generation and comprehension capabilities. Then,
since LDMs inevitably introduce outliers into the generated depth maps, we
design a Point Denoising Network to remove artifacts from the coarse point
cloud by predicting a per-point distance score. Finally, we devise an
Association-Aware Point Upsampler, which guides the upsampling process by
leveraging local association features between the input point cloud and the
corresponding coarse points, further yielding a dense and high-fidelity output.
Experimental results demonstrate that our DiffPCN achieves state-of-the-art
performance in geometric accuracy and shape completeness, significantly
improving the robustness and consistency of point cloud completion.

</details>


### [151] [Video Panels for Long Video Understanding](https://arxiv.org/abs/2509.23724)
*Lars Doorenbos,Federico Spurio,Juergen Gall*

Main category: cs.CV

TL;DR: 提出一种无需训练、无参数的视觉提示策略，通过将多帧组合成单图来提升现有视频语言模型在长视频理解上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在长视频理解上表现仍落后于图像或短视频任务，且改进方法常引入额外复杂性与训练成本。

Method: 设计一种新的视觉提示策略，将多个视频帧作为面板组合成单一图像，以空间细节换取时间分辨率，从而增强长视频的时间建模能力。该方法无需训练、无参数，且适用于各种VLM。

Result: 在五个基准数据集上验证了方法的有效性和一致性，在TimeScope（Long）数据集上问答准确率最高提升19.4%。

Conclusion: 所提方法显著提升了现有视频语言模型在长视频理解任务上的性能，为该领域设定了新的标杆。

Abstract: Recent Video-Language Models (VLMs) achieve promising results on long-video
understanding, but their performance still lags behind that achieved on tasks
involving images or short videos. This has led to great interest in improving
the long context modeling of VLMs by introducing novel modules and additional
complexity. % additional training time. In this paper, we take a different
approach: rather than fine-tuning VLMs with the limited data available, we
attempt to maximize the performance of existing models. To this end, we propose
a novel visual prompting strategy specifically designed for long-video
understanding. By combining multiple frames as panels into one image, we
effectively trade off spatial details for temporal resolution. Our approach is
training-free, parameter-free, and model-agnostic, and can be seamlessly
integrated into existing VLMs. Extensive experiments on five established
benchmarks across a wide range of model architectures, sizes, and context
windows confirm the consistency of our approach. For the TimeScope (Long)
dataset, which has the longest videos, the accuracy for video question
answering is improved by up to 19.4\%. Overall, our method raises the bar for
long video understanding models. We will make our code available upon
acceptance.

</details>


### [152] [M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation](https://arxiv.org/abs/2509.23728)
*Yiheng Zhang,Zhuojiang Cai,Mingdao Wang,Meitong Guo,Tianxiao Li,Li Lin,Yuwang Wang*

Main category: cs.CV

TL;DR: 本文提出了M3DLayout，一个大规模、多源的3D室内布局生成数据集，包含15,080个布局和超过258k个物体实例，整合了真实扫描、CAD设计和程序生成场景，并配有详细的结构化文本描述，显著提升了文本驱动3D场景生成的多样性和细节表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D室内布局生成模型受限于数据集规模小、多样性不足和标注质量低，难以支持复杂语义和空间模式的学习。

Method: 构建了一个融合真实扫描、专业CAD设计和程序生成场景的多源数据集M3DLayout，并为每个布局配备结构化文本描述；基于文本条件扩散模型建立基准进行评估。

Result: 实验表明，M3DLayout能有效提升布局生成模型的性能，尤其通过Inf3DLayout子集增强了小物体的丰富性，生成更复杂精细的场景。

Conclusion: M3DLayout作为一个大规模、高质量、多来源的数据集，有望推动文本驱动3D场景生成的研究发展。

Abstract: In text-driven 3D scene generation, object layout serves as a crucial
intermediate representation that bridges high-level language instructions with
detailed geometric output. It not only provides a structural blueprint for
ensuring physical plausibility but also supports semantic controllability and
interactive editing. However, the learning capabilities of current 3D indoor
layout generation models are constrained by the limited scale, diversity, and
annotation quality of existing datasets. To address this, we introduce
M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.
M3DLayout comprises 15,080 layouts and over 258k object instances, integrating
three distinct sources: real-world scans, professional CAD designs, and
procedurally generated scenes. Each layout is paired with detailed structured
text describing global scene summaries, relational placements of large
furniture, and fine-grained arrangements of smaller items. This diverse and
richly annotated resource enables models to learn complex spatial and semantic
patterns across a wide variety of indoor environments. To assess the potential
of M3DLayout, we establish a benchmark using a text-conditioned diffusion
model. Experimental results demonstrate that our dataset provides a solid
foundation for training layout generation models. Its multi-source composition
enhances diversity, notably through the Inf3DLayout subset which provides rich
small-object information, enabling the generation of more complex and detailed
scenes. We hope that M3DLayout can serve as a valuable resource for advancing
research in text-driven 3D scene synthesis.

</details>


### [153] [LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models](https://arxiv.org/abs/2509.23729)
*Shubhang Bhatnagar,Andy Xu,Kar-Han Tan,Narendra Ahuja*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型（MLLMs）的超低比特（<4位）量化问题，提出了一种分层量化策略LUQ，在保持性能的同时显著降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言任务中表现优异，但部署时需要大量内存和计算资源。现有后训练量化方法在纯语言模型上效果良好，但在MLLM上的应用尚不充分，尤其是超低比特量化面临多模态数据高熵、高方差带来的挑战。

Method: 通过分析发现多模态token及其激活在不同层具有不同的统计特性，部分层对超低比特量化更鲁棒。基于此提出LUQ：分层超低比特量化策略，选择性地对容忍度高的层进行超低比特量化，并使用图文混合token进行后训练量化以提升VQA性能。

Result: 在LLaVA-1.5和Qwen-2.5-VL上评估显示，LUQ模型相比4比特版本分别减少40%和31%内存，且在MME基准上的性能下降小于10%。

Conclusion: LUQ为多模态大语言模型提供了高效的超低比特量化方案，平衡了压缩率与性能，在实际部署中具有重要应用价值。

Abstract: Large Language Models (LLMs) with multimodal capabilities have revolutionized
vision-language tasks, but their deployment often requires huge memory and
computational resources. While post-training quantization (PTQ) has
successfully compressed language models to as low as 1-bit precision without
significant performance loss, its effectiveness for multimodal LLMs (MLLMs)
remains relatively unexplored. In this paper, we present the first study on
ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals
that multimodal tokens and intermediate layer activations produced by them
exhibit significantly higher statistical variance and entropy compared to text
tokens, making them less tolerant to ultra-low bit quantization. However, the
activation distributions of multimodal tokens varies significantly over
different layers, with some layers having lower entropy activation
distributions. We empirically show that such layers in these models can better
tolerate ultra-low bit quantization. Building on these insights, we propose a
novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit
Quantization, which selectively applies ultra-low bit quantization to layers
that are more resilient to it. Additionally, we also show that using a mix of
multimodal tokens (image and text) for PTQ boosts VQA performance in the
ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL
across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less
memory than their 4-bit counterparts, respectively, while exhibiting a
performance degradation of less than 10% on the MME benchmark.

</details>


### [154] [HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation](https://arxiv.org/abs/2509.23736)
*Cong Chen,Ziyuan Huang,Cheng Zou,Muzhi Zhu,Kaixiang Ji,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出了HieraTok，一种基于多尺度Vision Transformer的视觉tokenizer，通过多尺度下采样和尺度因果注意力机制，在图像重建与生成任务中显著优于单尺度方法，实现了更低的rFID和gFID，并加快了生成模型的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统的ViT tokenizer通常仅建模单一尺度的表示，难以兼顾全局语义与局部细节，限制了其在图像重建和生成任务中的性能。因此，需要一种能够有效融合多尺度信息的tokenizer设计。

Method: 提出HieraTok，包含两个核心设计：(1) 对tokenizer编码器生成的token图进行多尺度下采样，得到多尺度token序列；(2) 引入尺度因果注意力机制，使低分辨率的全局语义特征逐步指导高分辨率的结构细节恢复。

Result: 在相同设置下，HieraTok相比单尺度tokenizer将rFID从1.47降至1.07（提升27.2%）；在下游生成任务中，收敛速度提升1.38倍，gFID从16.4降至13.3（提升18.9%）。大规模训练后达到当前最优的rFID（0.45）和gFID（1.82）。

Conclusion: HieraTok是首个引入多尺度设计的ViT-based视觉tokenizer，能有效提升图像重建与生成性能，得益于更平滑且均匀的潜在空间分布，为未来视觉生成任务中的tokenizer设计提供了新方向。

Abstract: In this work, we present HieraTok, a novel multi-scale Vision Transformer
(ViT)-based tokenizer that overcomes the inherent limitation of modeling
single-scale representations. This is realized through two key designs: (1)
multi-scale downsampling applied to the token map generated by the tokenizer
encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal
attention mechanism that enables the progressive flow of information from
low-resolution global semantic features to high-resolution structural details.
Coupling these designs, HieraTok achieves significant improvements in both
image reconstruction and generation tasks. Under identical settings, the
multi-scale visual tokenizer outperforms its single-scale counterpart by a
27.2\% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into
downstream generation frameworks, it achieves a $1.38\times$ faster convergence
rate and an 18.9\% boost in gFID ($16.4 \rightarrow 13.3$), which may be
attributed to the smoother and more uniformly distributed latent space.
Furthermore, by scaling up the tokenizer's training, we demonstrate its
potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To
the best of our knowledge, we are the first to introduce multi-scale ViT-based
tokenizer in image reconstruction and image generation. We hope our findings
and designs advance the ViT-based tokenizers in visual generation tasks.

</details>


### [155] [ResAD++: Towards Class Agnostic Anomaly Detection via Residual Feature Learning](https://arxiv.org/abs/2509.23741)
*Xincheng Yao,Chao Shi,Muming Zhao,Guangtao Zhai,Chongyang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的类无关异常检测框架ResAD及其改进版本ResAD++，通过学习残差特征分布和引入超球面约束来实现特征去相关和尺度一致性，从而在无需重训练的情况下有效泛化到新类别。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法的表征学习仍与类别相关（特征相关性），导致在新类别上的泛化性能不佳。需要一种类无关的方法以提升跨域新类别的检测能力。

Method: 提出残差特征学习框架ResAD：通过匹配并减去正常参考特征构建残差特征，实现特征去相关；引入特征超球面约束以解决尺度相关问题；设计logbarrier双向收缩OCC损失和基于向量量化的特征分布匹配模块，形成ResAD++。

Result: 在八个真实世界异常检测数据集上实验表明，ResAD++在直接应用于新类别时显著优于现有最先进方法，并且性能优于原始ResAD。

Conclusion: ResAD++通过残差特征学习和超球面约束有效实现了类无关异常检测，在跨类别、跨领域场景下具有优异的泛化能力和检测性能。

Abstract: This paper explores the problem of class-agnostic anomaly detection (AD),
where the objective is to train one class-agnostic AD model that can generalize
to detect anomalies in diverse new classes from different domains without any
retraining or fine-tuning on the target data. When applied for new classes, the
performance of current single- and multi-class AD methods is still
unsatisfactory. One fundamental reason is that representation learning in
existing methods is still class-related, namely, feature correlation. To
address this issue, we propose residual features and construct a simple but
effective framework, termed ResAD. Our core insight is to learn the residual
feature distribution rather than the initial feature distribution. Residual
features are formed by matching and then subtracting normal reference features.
In this way, we can effectively realize feature decorrelation. Even in new
classes, the distribution of normal residual features would not remarkably
shift from the learned distribution. In addition, we think that residual
features still have one issue: scale correlation. To this end, we propose a
feature hypersphere constraining approach, which learns to constrain initial
normal residual features into a spatial hypersphere for enabling the feature
scales of different classes as consistent as possible. Furthermore, we propose
a novel logbarrier bidirectional contraction OCC loss and vector quantization
based feature distribution matching module to enhance ResAD, leading to the
improved version of ResAD (ResAD++). Comprehensive experiments on eight
real-world AD datasets demonstrate that our ResAD++ can achieve remarkable AD
results when directly used in new classes, outperforming state-of-the-art
competing methods and also surpassing ResAD. The code is available at
https://github.com/xcyao00/ResAD.

</details>


### [156] [Poivre: Self-Refining Visual Pointing with Reinforcement Learning](https://arxiv.org/abs/2509.23746)
*Wenjie Yang,Zengfeng Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Poivre的自优化方法，通过“指向-可视化-再优化”流程结合强化学习，显著提升了视觉语言模型在视觉指向任务中的性能，Poivre-7B在Point-Bench上超越了现有大模型，达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视觉指向上表现不佳，主要因其需单步完成任务，缺乏反馈机制，难以精确定位目标。

Method: 提出Point, Visualize, then Refine (Poivre)框架，让模型先预测坐标，再通过可视化自身预测结果进行迭代优化；采用强化学习设计奖励机制以驱动自我优化过程。

Result: Poivre-7B在Point-Bench上表现优于Gemini-2.5-Pro和Molmo-72B等大型模型，提升超3%，并实现新的最先进性能。

Conclusion: 引入自优化机制和强化学习可有效提升VLM在视觉指向任务中的准确性，Poivre为该领域提供了高效且可复现的新范式。

Abstract: Visual pointing, which aims to localize a target by predicting its
coordinates on an image, has emerged as an important problem in the realm of
vision-language models (VLMs). Despite its broad applicability, recent
benchmarks show that current VLMs still fall far behind human performance on
this task. A key limitation is that VLMs are typically required to complete the
pointing task in a single step, akin to asking humans to point at an object
without seeing their own fingers. To address this issue, we propose a simple
yet effective self-refining procedure: Point, Visualize, then Refine (Poivre).
This procedure enables a VLM to first mark its estimated point, then
iteratively refine the coordinates if necessary. Inspired by advances of
reasoning models in the natural language domain, we employ reinforcement
learning (RL) to incentivize this self-refining ability. For the RL training,
we design a neat process reward that is not only empirically effective but also
grounded in appealing properties. Our trained model, Poivre-7B, sets a new
state of the art on Point-Bench, outperforming both proprietary models such as
Gemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To
support future research, we release our training and inference code, dataset,
and the Poivre-7B checkpoint.

</details>


### [157] [PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block](https://arxiv.org/abs/2509.23751)
*Arshia Yousefi Nezhad,Helia Aghaei,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出了一种用于结直肠癌息肉分割的新型网络PVTAdpNet，结合Pyramid Vision Transformer和U-Net结构，具有高精度和实时性，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统结肠镜检查因息肉形态多样导致漏检率高，亟需更有效的早期检测方法。

Method: 采用U-Net式编码器-解码器结构，结合Pyramid Vision Transformer主干、新型残差块和基于适配器的跳跃连接，并引入squeeze-and-excitation注意力机制优化特征提取。

Result: 在分布外息肉数据集上达到0.8851的Dice系数和0.8167的mIoU，PolypGen数据集验证了其实时准确性能。

Conclusion: PVTAdpNet在息肉分割任务中表现出色，具备临床应用潜力，适用于实时精准的早期结直肠癌检测。

Abstract: Colorectal cancer ranks among the most common and deadly cancers, emphasizing
the need for effective early detection and treatment. To address the
limitations of traditional colonoscopy, including high miss rates due to polyp
variability, we introduce the Pyramid Vision Transformer Adapter Residual
Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder
structure with a Pyramid Vision Transformer backbone, novel residual blocks,
and adapter-based skip connections. The design enhances feature extraction,
dense prediction, and gradient flow, supported by squeeze-and-excitation
attention for improved channel-wise feature refinement. PVTAdpNet achieves
real-time, accurate polyp segmentation, demonstrating superior performance on
benchmark datasets with high mDice and mIoU scores, making it highly suitable
for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851
and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution
polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's
capability for real-time, accurate performance within familiar distributions.
The source code of our network is available at
https://github.com/ayousefinejad/PVTAdpNet.git

</details>


### [158] [UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception](https://arxiv.org/abs/2509.23760)
*Xinyang Song,Libin Wang,Weining Wang,Shaozhen Liu,Dandan Zheng,Jingdong Chen,Qi Li,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出UniAlignment，一种基于单个扩散变换器的统一多模态生成框架，通过双流扩散训练策略提升跨模态一致性和指令跟随能力，并发布新基准SemGen-Bench用于评估复杂文本指令下的多模态语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖视觉-语言模型或多模块设计，导致架构碎片化和计算效率低下，难以满足复杂语义指令下多模态任务对语义理解一致性的要求。

Method: 提出UniAlignment框架，采用单个扩散变换器和双流扩散训练策略，实现模态内和跨模态的语义对齐，从而增强跨模态一致性和指令跟随鲁棒性。

Result: 在多个任务和基准上实验表明，UniAlignment优于现有基线方法，在图像理解、操作和感知等多模态任务中表现出更强的语义一致性和生成质量。

Conclusion: 扩散模型在统一多模态生成方面具有巨大潜力，UniAlignment通过一体化架构和双流训练策略为多模态语义对齐提供了有效解决方案。

Abstract: The remarkable success of diffusion models in text-to-image generation has
sparked growing interest in expanding their capabilities to a variety of
multi-modal tasks, including image understanding, manipulation, and perception.
These tasks require advanced semantic comprehension across both visual and
textual modalities, especially in scenarios involving complex semantic
instructions. However, existing approaches often rely heavily on
vision-language models (VLMs) or modular designs for semantic guidance, leading
to fragmented architectures and computational inefficiency. To address these
challenges, we propose UniAlignment, a unified multimodal generation framework
within a single diffusion transformer. UniAlignment introduces a dual-stream
diffusion training strategy that incorporates both intrinsic-modal semantic
alignment and cross-modal semantic alignment, thereby enhancing the model's
cross-modal consistency and instruction-following robustness. Additionally, we
present SemGen-Bench, a new benchmark specifically designed to evaluate
multimodal semantic consistency under complex textual instructions. Extensive
experiments across multiple tasks and benchmarks demonstrate that UniAlignment
outperforms existing baselines, underscoring the significant potential of
diffusion models in unified multimodal generation.

</details>


### [159] [GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning](https://arxiv.org/abs/2509.23770)
*Xiaojie Li,Bei Wang,Jianlong Wu,Yue Yu,Liqiang Nie,Min Zhang*

Main category: cs.CV

TL;DR: GenView++ 是一个统一的对比学习框架，通过多源自适应视图生成和质量驱动的对比学习机制，提升正样本对的多样性和语义一致性，并动态调整训练中各样本对的权重，显著改善了视觉与视觉-语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在正样本对构建上多样性不足且易语义失真，同时缺乏对样本对质量的评估机制，导致所有样本对被同等对待，监督效果不佳。

Method: 提出 GenView++，包含两个创新：1）多源自适应视图生成机制，结合图像、文本及图文条件生成多样化且语义一致的视图；2）质量驱动的对比学习机制，评估每对样本的语义对齐与多样性，动态重加权其训练贡献。

Result: 在视觉表征任务中，比 MoCov2 在 ImageNet 线性分类上提升 +2.5%；在视觉-语言任务中，零样本分类平均准确率比 CLIP 提升 +12.31%，比 SLIP 提升 +5.31%，Flickr30k 文本检索 R@5 提升 +3.2%。

Conclusion: GenView++ 有效解决了对比学习中正样本对构建质量低和训练监督不均衡的问题，在多种视觉与视觉-语言任务上表现出显著优势。

Abstract: The success of contrastive learning depends on the construction and
utilization of high-quality positive pairs. However, current methods face
critical limitations on two fronts: on the construction side, both handcrafted
and generative augmentations often suffer from limited diversity and risk
semantic corruption; on the learning side, the absence of a quality assessment
mechanism leads to suboptimal supervision where all pairs are treated equally.
To tackle these challenges, we propose GenView++, a unified framework that
addresses both fronts by introducing two synergistic innovations. To improve
pair construction, GenView++ introduces a multi-source adaptive view generation
mechanism to synthesize diverse yet semantically coherent views by dynamically
modulating generative parameters across image-conditioned, text-conditioned,
and image-text-conditioned strategies. Second, a quality-driven contrastive
learning mechanism assesses each pair's semantic alignment and diversity to
dynamically reweight their training contribution, prioritizing high-quality
pairs while suppressing redundant or misaligned pairs. Extensive experiments
demonstrate the effectiveness of GenView++ across both vision and
vision-language tasks. For vision representation learning, it improves MoCov2
by +2.5% on ImageNet linear classification. For vision-language learning, it
raises the average zero-shot classification accuracy by +12.31% over CLIP and
+5.31% over SLIP across ten datasets, and further improves Flickr30k text
retrieval R@5 by +3.2%. The code is available at
https://github.com/xiaojieli0903/GenViewPlusPlus.

</details>


### [160] [A Modality-Tailored Graph Modeling Framework for Urban Region Representation via Contrastive Learning](https://arxiv.org/abs/2509.23772)
*Yaya Zhao,Kaiqi Zhao,Zixuan Tang,Zhiyuan Liu,Xiaoling Lu,Yalei Du*

Main category: cs.CV

TL;DR: 提出MTGRR，一种针对多模态城市区域表示的图模型框架，通过模态定制的图神经网络和空间感知的融合机制，在多种真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图模型在处理多模态城市数据时，使用相同的图神经网络架构且忽略空间异质性，导致表征能力不足。

Method: 将模态分为聚合级和点级，分别采用混合专家图架构和双层GNN；设计空间感知的多模态融合机制动态生成区域特定的融合权重，并结合联合对比学习优化表示。

Result: 在两个真实世界数据集、六种模态和三项任务上的实验表明，MTGRR consistently 优于最先进的基线方法。

Conclusion: MTGRR能有效捕捉模态特性和空间异质性，显著提升城市区域表示性能。

Abstract: Graph-based models have emerged as a powerful paradigm for modeling
multimodal urban data and learning region representations for various
downstream tasks. However, existing approaches face two major limitations. (1)
They typically employ identical graph neural network architectures across all
modalities, failing to capture modality-specific structures and
characteristics. (2) During the fusion stage, they often neglect spatial
heterogeneity by assuming that the aggregation weights of different modalities
remain invariant across regions, resulting in suboptimal representations. To
address these issues, we propose MTGRR, a modality-tailored graph modeling
framework for urban region representation, built upon a multimodal dataset
comprising point of interest (POI), taxi mobility, land use, road element,
remote sensing, and street view images. (1) MTGRR categorizes modalities into
two groups based on spatial density and data characteristics: aggregated-level
and point-level modalities. For aggregated-level modalities, MTGRR employs a
mixture-of-experts (MoE) graph architecture, where each modality is processed
by a dedicated expert GNN to capture distinct modality-specific
characteristics. For the point-level modality, a dual-level GNN is constructed
to extract fine-grained visual semantic features. (2) To obtain effective
region representations under spatial heterogeneity, a spatially-aware
multimodal fusion mechanism is designed to dynamically infer region-specific
modality fusion weights. Building on this graph modeling framework, MTGRR
further employs a joint contrastive learning strategy that integrates region
aggregated-level, point-level, and fusion-level objectives to optimize region
representations. Experiments on two real-world datasets across six modalities
and three tasks demonstrate that MTGRR consistently outperforms
state-of-the-art baselines, validating its effectiveness.

</details>


### [161] [Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution](https://arxiv.org/abs/2509.23774)
*Qifan Li,Jiale Zou,Jinhua Zhang,Wei Long,Xinyu Zhou,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出了一种基于纹理向量量化和重建感知预测的生成式超分辨率模型，有效降低量化误差并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有VQ方法在视觉先验建模中存在量化误差大、训练监督信号不直接考虑重建误差的问题。

Method: 引入纹理向量量化（TVQ）仅对缺失纹理建模，并采用重建感知预测（RAP）通过直通估计器以图像级监督训练索引预测器。

Result: 模型在低计算成本下实现了高质量、逼真的超分辨率结果。

Conclusion: TVQ&RAP有效提升了VQ-based模型在超分辨率任务中的先验建模精度和重建效果。

Abstract: Vector-quantized based models have recently demonstrated strong potential for
visual prior modeling. However, existing VQ-based methods simply encode visual
features with nearest codebook items and train index predictor with code-level
supervision. Due to the richness of visual signal, VQ encoding often leads to
large quantization error. Furthermore, training predictor with code-level
supervision can not take the final reconstruction errors into consideration,
result in sub-optimal prior modeling accuracy. In this paper we address the
above two issues and propose a Texture Vector-Quantization and a Reconstruction
Aware Prediction strategy. The texture vector-quantization strategy leverages
the task character of super-resolution and only introduce codebook to model the
prior of missing textures. While the reconstruction aware prediction strategy
makes use of the straight-through estimator to directly train index predictor
with image-level supervision. Our proposed generative SR model (TVQ&RAP) is
able to deliver photo-realistic SR results with small computational cost.

</details>


### [162] [GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning](https://arxiv.org/abs/2509.23781)
*Nayeong Kim,Seong Joon Oh,Suha Kwak*

Main category: cs.CV

TL;DR: 提出了一种名为GroupCoOp的参数高效微调方法，用于提升视觉-语言模型在组不平衡数据下的鲁棒性，仅训练0.016%的参数即在五个基准上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在子组不平衡数据下存在虚假相关性问题，导致对少数群体的忽视和类别表征分散。

Method: 设计组特定的文本提示作为各类别的多个分类器，利用视觉-语言模型文本编码器的语义能力发现有效的组提示，从而缓解组不平衡带来的影响。

Result: 在五个基准和五种CLIP架构上均取得最佳结果，有时甚至优于全模型微调的方法。

Conclusion: GroupCoOp是一种简单而有效的去偏微调算法，显著提升了参数高效微调模型在组不平衡场景下的泛化能力和鲁棒性。

Abstract: Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)
excels in various vision tasks thanks to the rich knowledge and generalization
ability of VLMs. However, recent studies revealed that such fine-tuned VLMs are
vulnerable to spurious correlations stemming from the subgroup imbalance in the
fine-tuning datasets. To resolve this issue, we propose Group Context
Optimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm
that enhances the group robustness of fine-tuned VLMs. Its key idea is to
employ group-specific text prompts as group representatives serving as multiple
classifiers for their target class. The rich semantic knowledge of the text
encoder of VLM enables the discovery of effective group prompts even for groups
with a small number of training samples. Leveraging the group prompts for each
class addresses the issues caused by the group-imbalanced training set, such as
the neglect of minority groups and the scattered distribution of each class in
the embedding space. GroupCoOp achieved the best results on five benchmarks
across five CLIP architectures and occasionally outperformed prior methods that
fine-tune the entire network, despite training only 0.016\% of the network's
parameters.

</details>


### [163] [From Unstable to Playable: Stabilizing Angry Birds Levels via Object Segmentation](https://arxiv.org/abs/2509.23787)
*Mahdi Farrokhimaleki,Parsa Rahmati,Richard Zhao*

Main category: cs.CV

TL;DR: 提出一种基于图像的修复方法，用于改进AI生成的游戏关卡的稳定性和可玩性。


<details>
  <summary>Details</summary>
Motivation: 现有的程序化内容生成（PCG）模型可能生成不稳定的关卡，难以保证高质量和行业标准的内容输出。

Method: 利用对象分割和关卡图像的视觉分析来检测结构缺陷，并进行有针对性的修复；在多个对象分割模型中选择最有效的模型构建修复流程。

Result: 实验结果表明，该方法能有效提升AI生成关卡的稳定性和可玩性，且该图像基础的方法具有良好的泛化潜力。

Conclusion: 所提出的方法能够有效识别并修复PCG生成的不稳定关卡，尤其适用于类似Angry Birds的2D游戏，具备广泛的应用前景。

Abstract: Procedural Content Generation (PCG) techniques enable automatic creation of
diverse and complex environments. While PCG facilitates more efficient content
creation, ensuring consistently high-quality, industry-standard content remains
a significant challenge. In this research, we propose a method to identify and
repair unstable levels generated by existing PCG models. We use Angry Birds as
a case study, demonstrating our method on game levels produced by established
PCG approaches. Our method leverages object segmentation and visual analysis of
level images to detect structural gaps and perform targeted repairs. We
evaluate multiple object segmentation models and select the most effective one
as the basis for our repair pipeline. Experimental results show that our method
improves the stability and playability of AI-generated levels. Although our
evaluation is specific to Angry Birds, our image-based approach is designed to
be applicable to a wide range of 2D games with similar level structures.

</details>


### [164] [Controllable Generation of Large-Scale 3D Urban Layouts with Semantic and Structural Guidance](https://arxiv.org/abs/2509.23804)
*Mengyuan Niu,Xinxin Zhuo,Ruizhe Wang,Yuyue Huang,Junyan Yang,Qiao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于几何和语义条件的大规模3D向量城市布局生成可控框架，融合几何与语义属性，支持用户通过修改语义属性直接控制输出。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的方法缺乏几何连续性和可扩展性，而基于图的方法忽略了地块语义，因此需要一种兼顾几何与语义的可控城市建模方法。

Method: 通过融合几何与语义属性、引入边权重并将建筑高度嵌入图结构，将2D布局扩展为3D结构，实现对城市布局的语义控制生成。

Result: 实验表明该方法能生成有效的大规模城市模型，具有良好的几何连续性和语义表达能力。

Conclusion: 该框架为数据驱动的城市规划与设计提供了一个有效的工具，支持可控、可扩展且语义丰富的3D城市建模。

Abstract: Urban modeling is essential for city planning, scene synthesis, and gaming.
Existing image-based methods generate diverse layouts but often lack geometric
continuity and scalability, while graph-based methods capture structural
relations yet overlook parcel semantics. We present a controllable framework
for large-scale 3D vector urban layout generation, conditioned on both geometry
and semantics. By fusing geometric and semantic attributes, introducing edge
weights, and embedding building height in the graph, our method extends 2D
layouts to realistic 3D structures. It also enables users to directly control
the output by modifying semantic attributes. Experiments show that it produces
valid, large-scale urban models, offering an effective tool for data-driven
planning and design.

</details>


### [165] [A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality Control](https://arxiv.org/abs/2509.23815)
*Ali Nazeri,Shashank Mishra,Achim Wagner,Martin Ruskowski,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文提出了一种基于多视角成像与图像融合的新型质量控制模块，有效解决了单视角检测在制造装配中的遮挡与光照问题，显著提升了小零件装配缺陷的检测精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有质量检测方法依赖单视角成像或人工检查，易受遮挡、视角限制和光照变化影响，导致误检并增加产线停机成本。

Method: 设计了一个三摄像头的多视角成像系统，结合定制的图像融合方法与先进目标检测算法，对装配部件进行全方位视觉覆盖，并利用自建多场景标注数据集进行训练与验证。

Result: 实验表明，该方法在识别未正确紧固的小零件（如螺丝）方面显著优于单视角方法，具有更高的精确率和召回率。

Conclusion: 该研究为工业自动化提供了一种可扩展、成本效益高且准确的质量控制方案，克服了单视角检测的局限性，同时公开数据集以促进后续研究。

Abstract: Quality control is a critical aspect of manufacturing, particularly in
ensuring the proper assembly of small components in production lines. Existing
solutions often rely on single-view imaging or manual inspection, which are
prone to errors due to occlusions, restricted perspectives, or lighting
inconsistencies. These limitations require the installation of additional
inspection stations, which could disrupt the assembly line and lead to
increased downtime and costs. This paper introduces a novel multi-view quality
control module designed to address these challenges, integrating a multi-camera
imaging system with advanced object detection algorithms. By capturing images
from three camera views, the system provides comprehensive visual coverage of
components of an assembly process. A tailored image fusion methodology combines
results from multiple views, effectively resolving ambiguities and enhancing
detection reliability. To support this system, we developed a unique dataset
comprising annotated images across diverse scenarios, including varied lighting
conditions, occlusions, and angles, to enhance applicability in real-world
manufacturing environments. Experimental results show that our approach
significantly outperforms single-view methods, achieving high precision and
recall rates in the identification of improperly fastened small assembly parts
such as screws. This work contributes to industrial automation by overcoming
single-view limitations, and providing a scalable, cost-effective, and accurate
quality control mechanism that ensures the reliability and safety of the
assembly line. The dataset used in this study is publicly available to
facilitate further research in this domain.

</details>


### [166] [Assessing Visual Privacy Risks in Multimodal AI: A Novel Taxonomy-Grounded Evaluation of Vision-Language Models](https://arxiv.org/abs/2509.23827)
*Efthymios Tsaprazlis,Tiantian Feng,Anil Ramakrishna,Rahul Gupta,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: 本文提出了一种多层次的视觉隐私分类法，用于评估先进视觉-语言模型在理解与执行隐私原则方面的能力，揭示了当前模型在情境隐私理解上的显著不一致性，并强调了构建更 robust 的隐私感知AI系统的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在隐私概念理解上存在关键缺陷，且缺乏测试资源，因此需要研究这些模型是否以及如何理解和实施隐私原则。

Method: 通过引入一个全面的、多层级的视觉隐私分类法，并基于法律框架来评估多个最先进的视觉-语言模型在隐私任务中的表现。

Result: 评估结果显示现有视觉-语言模型在理解情境隐私方面存在显著不一致，暴露了其在隐私保护能力上的局限性。

Conclusion: 本研究提供了未来研究的基础性分类法和当前模型局限性的关键基准，表明亟需开发更强大、具备隐私意识的AI系统。

Abstract: Artificial Intelligence have profoundly transformed the technological
landscape in recent years. Large Language Models (LLMs) have demonstrated
impressive abilities in reasoning, text comprehension, contextual pattern
recognition, and integrating language with visual understanding. While these
advances offer significant benefits, they also reveal critical limitations in
the models' ability to grasp the notion of privacy. There is hence substantial
interest in determining if and how these models can understand and enforce
privacy principles, particularly given the lack of supporting resources to test
such a task. In this work, we address these challenges by examining how legal
frameworks can inform the capabilities of these emerging technologies. To this
end, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that
captures a wide range of privacy issues, designed to be scalable and adaptable
to existing and future research needs. Furthermore, we evaluate the
capabilities of several state-of-the-art Vision-Language Models (VLMs),
revealing significant inconsistencies in their understanding of contextual
privacy. Our work contributes both a foundational taxonomy for future research
and a critical benchmark of current model limitations, demonstrating the urgent
need for more robust, privacy-aware AI systems.

</details>


### [167] [Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation](https://arxiv.org/abs/2509.23828)
*Hanyu Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了Uni4D-LLM，首个兼具时空感知能力的统一视觉语言模型框架，用于4D场景的理解与生成。通过共享表示和架构设计，实现了理解与生成任务的统一建模。


<details>
  <summary>Details</summary>
Motivation: 现有的3D/4D方法在语义理解和内容生成之间存在范式差异，难以实现统一建模，尤其在需要时空建模的动态4D场景中。因此，亟需一个能同时处理理解与生成任务的统一框架。

Method: 提出Uni4D-LLM，采用自适应交叉注意力融合语义特征、带噪声的外观特征和4D几何线索，构建时空感知的视觉表示；基于Transformer架构，集成自回归（理解）和扩散（生成）模块于同一LLM中，并通过指令微调提升多任务泛化能力。

Result: 在多个4D视觉-语言基准测试上，Uni4D-LLM表现达到或优于当前最先进模型，首次实现了4D场景理解与生成的真正统一。

Conclusion: Uni4D-LLM通过共享表示与架构，成功将4D场景的理解与生成统一于单一Transformer框架下，为物理世界的多模态建模提供了新范式。

Abstract: Vision-language models (VLMs) have demonstrated strong performance in 2D
scene understanding and generation, but extending this unification to the
physical world remains an open challenge. Existing 3D and 4D approaches
typically embed scene geometry into autoregressive model for semantic
understanding and diffusion model for content generation. This paradigm gap
prevents a single model from jointly handling both tasks, especially in dynamic
4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,
the first unified VLM framework with spatiotemporal awareness for 4D scene
understanding and generation. Our design is guided by two key insights: 1)
Unification requires a shared representation. We extract semantic features for
understanding and noisy-injected appearance features for generation,
incorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual
representation through adaptive cross-attention. 2) Unification requires a
shared architecture. Both autoregression and diffusion are built on Transformer
backbones, and this enables integration into a single LLM with task-specific
heads. By aligning visual and linguistic representations, our Uni4D-LLM
produces predictions for both understanding and generation within one
Transformer-based framework. We further apply instruction fine-tuning on
diverse 4D vision-language datasets to improve generalization across tasks.
Extensive experiments on multiple benchmarks demonstrate that Uni4D-LLM
achieves competitive or superior results compared to state-of-the-art models
and offers the first true unification of 4D scene understanding and generation.

</details>


### [168] [2nd Place Report of MOSEv2 Challenge 2025: Concept Guided Video Object Segmentation via SeC](https://arxiv.org/abs/2509.23838)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文评估了Segment Concept (SeC) 框架在复杂视频对象分割v2 (MOSEv2) 数据集上的零样本性能，该框架利用大型视觉-语言模型 (LVLM) 实现对目标的深层语义理解，以提高分割持久性。无需在训练集上进行微调，SeC 在测试集上达到了39.7 \JFn，排名第七届大规模视频对象分割挑战赛复杂VOS赛道第二名。


<details>
  <summary>Details</summary>
Motivation: 现有半监督视频对象分割方法依赖外观模式匹配，在面对剧烈视觉变化、遮挡和场景转换时鲁棒性有限，缺乏对目标的高层概念理解。因此需要引入具备语义理解能力的方法来提升分割稳定性。

Method: 采用Segment Concept (SeC) 框架，结合大型视觉-语言模型 (LVLM)，通过语义层面的理解来指导视频对象分割，实现零样本迁移，未对训练集进行任何微调。

Result: 在MOSEv2数据集上实现了39.7 \JFn的零样本性能，在第七届大规模视频对象分割挑战赛的复杂VOS赛道中排名第二。

Conclusion: SeC框架通过引入高阶语义理解显著提升了视频对象分割的鲁棒性，即使在零样本设置下也能取得优异性能，验证了语义建模在复杂场景下的有效性。

Abstract: Semi-supervised Video Object Segmentation aims to segment a specified target
throughout a video sequence, initialized by a first-frame mask. Previous
methods rely heavily on appearance-based pattern matching and thus exhibit
limited robustness against challenges such as drastic visual changes,
occlusions, and scene shifts. This failure is often attributed to a lack of
high-level conceptual understanding of the target. The recently proposed
Segment Concept (SeC) framework mitigated this limitation by using a Large
Vision-Language Model (LVLM) to establish a deep semantic understanding of the
object for more persistent segmentation. In this work, we evaluate its
zero-shot performance on the challenging coMplex video Object SEgmentation v2
(MOSEv2) dataset. Without any fine-tuning on the training set, SeC achieved
39.7 \JFn on the test set and ranked 2nd place in the Complex VOS track of the
7th Large-scale Video Object Segmentation Challenge.

</details>


### [169] [Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A Two-Stage Rank-Learning Metric](https://arxiv.org/abs/2509.23841)
*Bingyang Cui,Yujie Zhang,Qi Yang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 本文提出了T23D-CompBench，一个用于组合式文本到3D生成的综合性基准，并在此基础上提出Rank2Score评估器，通过两阶段训练实现更贴近人类判断的质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有T23D质量评估基准过时、碎片化且粗粒度，客观指标设计存在局限，导致特征提取不具代表性、鲁棒性差。

Method: 构建包含五个组件、十二个子组件的T23D-CompBench基准，生成3600个纹理网格并收集12.96万条人类评分；提出Rank2Score，采用监督对比回归和课程学习进行两阶段训练。

Result: Rank2Score在多个维度上优于现有指标，并可用于优化生成模型的奖励函数。实验验证了其在下游任务中的有效性。

Conclusion: Rank2Score显著提升了T23D质量评估与人类感知的一致性，T23D-CompBench为未来研究提供了可靠基准。

Abstract: Recent advances in Text-to-3D (T23D) generative models have enabled the
synthesis of diverse, high-fidelity 3D assets from textual prompts. However,
existing challenges restrict the development of reliable T23D quality
assessment (T23DQA). First, existing benchmarks are outdated, fragmented, and
coarse-grained, making fine-grained metric training infeasible. Moreover,
current objective metrics exhibit inherent design limitations, resulting in
non-representative feature extraction and diminished metric robustness. To
address these limitations, we introduce T23D-CompBench, a comprehensive
benchmark for compositional T23D generation. We define five components with
twelve sub-components for compositional prompts, which are used to generate
3,600 textured meshes from ten state-of-the-art generative models. A
large-scale subjective experiment is conducted to collect 129,600 reliable
human ratings across different perspectives. Based on T23D-CompBench, we
further propose Rank2Score, an effective evaluator with two-stage training for
T23DQA. Rank2Score enhances pairwise training via supervised contrastive
regression and curriculum learning in the first stage, and subsequently refines
predictions using mean opinion scores to achieve closer alignment with human
judgments in the second stage. Extensive experiments and downstream
applications demonstrate that Rank2Score consistently outperforms existing
metrics across multiple dimensions and can additionally serve as a reward
function to optimize generative models. The project is available at
https://cbysjtu.github.io/Rank2Score/.

</details>


### [170] [CE-FAM: Concept-Based Explanation via Fusion of Activation Maps](https://arxiv.org/abs/2509.23849)
*Michihiro Kuroki,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 提出一种基于概念的解释方法CE-FAM，通过融合激活图揭示图像分类器学习到的概念、相关区域及其对预测的贡献，利用视觉语言模型知识实现无需标注数据的任意概念解释，并在零样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时揭示图像分类器学到的概念、相关区域及其对预测的贡献，且依赖标注数据，限制了可解释性和泛化能力。

Method: 设计分支网络，共享分类器的激活图，模拟视觉语言模型（VLM）的嵌入；通过概念预测分数的梯度加权激活图得到概念区域，结合对分类得分的影响量化贡献。

Result: 该方法能有效识别概念区域及其贡献，提出新的评估指标验证区域准确性；定性和定量实验表明其优于现有方法，在未见概念的零样本推理中表现突出。

Conclusion: CE-FAM为图像分类提供了通用、无需标注的概念解释框架，结合VLM实现了灵活、准确且可推广的可解释性。

Abstract: Although saliency maps can highlight important regions to explain the
reasoning behind image classification in artificial intelligence (AI), the
meaning of these regions is left to the user's interpretation. In contrast,
conceptbased explanations decompose AI predictions into humanunderstandable
concepts, clarifying their contributions. However, few methods can
simultaneously reveal what concepts an image classifier learns, which regions
are associated with them, and how they contribute to predictions. We propose a
novel concept-based explanation method, Concept-based Explanation via Fusion of
Activation Maps (CE-FAM). It employs a branched network that shares activation
maps with an image classifier and learns to mimic the embeddings of a Vision
and Language Model (VLM). The branch network predicts concepts in an image, and
their corresponding regions are represented by a weighted sum of activation
maps, with weights given by the gradients of the concept prediction scores.
Their contributions are quantified based on their impact on the image
classification score. Our method provides a general framework for identifying
the concept regions and their contributions while leveraging VLM knowledge to
handle arbitrary concepts without requiring an annotated dataset. Furthermore,
we introduce a novel evaluation metric to assess the accuracy of the concept
regions. Our qualitative and quantitative evaluations demonstrate our method
outperforms existing approaches and excels in zero-shot inference for unseen
concepts.

</details>


### [171] [FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction](https://arxiv.org/abs/2509.23859)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出了一种名为FairViT-GAN的新框架，结合CNN和Vision Transformer的优势，并引入对抗去偏机制，以提高面部美感预测的准确性、公平性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有面部美感预测模型存在架构局限性、种族等人口统计学偏差以及缺乏透明度的问题，尤其是CNN难以捕捉全局和谐，而ViT可能忽略细节，且模型容易学习并放大社会偏见。

Method: 提出FairViT-GAN，采用CNN分支提取局部特征，ViT分支建模全局上下文，并通过对抗去偏机制训练特征提取器生成对保护属性（如种族）不变的表示，从而减少算法偏见，同时可视化两个分支的注意力以增强可解释性。

Result: 在SCUT-FBP5500数据集上达到新的SOTA性能，皮尔逊相关系数为0.9230，RMSE为0.2650；族裔子组间性能差距减少了82.9%，对手分类准确率降至52.1%，接近随机水平，显著提升公平性。

Conclusion: FairViT-GAN在保持高预测精度的同时，有效提升了模型的公平性和透明度，为构建负责任的主观视觉评估AI系统提供了可靠范本。

Abstract: Facial Beauty Prediction (FBP) has made significant strides with the
application of deep learning, yet state-of-the-art models often exhibit
critical limitations, including architectural constraints, inherent demographic
biases, and a lack of transparency. Existing methods, primarily based on
Convolutional Neural Networks (CNNs), excel at capturing local texture but
struggle with global facial harmony, while Vision Transformers (ViTs)
effectively model long-range dependencies but can miss fine-grained details.
Furthermore, models trained on benchmark datasets can inadvertently learn and
perpetuate societal biases related to protected attributes like ethnicity. To
address these interconnected challenges, we propose \textbf{FairViT-GAN}, a
novel hybrid framework that synergistically integrates a CNN branch for local
feature extraction and a ViT branch for global context modeling. More
significantly, we introduce an adversarial debiasing mechanism where the
feature extractor is explicitly trained to produce representations that are
invariant to protected attributes, thereby actively mitigating algorithmic
bias. Our framework's transparency is enhanced by visualizing the distinct
focus of each architectural branch. Extensive experiments on the SCUT-FBP5500
benchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in
predictive accuracy, achieving a Pearson Correlation of \textbf{0.9230} and
reducing RMSE to \textbf{0.2650}, but also excels in fairness. Our analysis
reveals a remarkable \textbf{82.9\% reduction in the performance gap} between
ethnic subgroups, with the adversary's classification accuracy dropping to
near-random chance (52.1\%). We believe FairViT-GAN provides a robust,
transparent, and significantly fairer blueprint for developing responsible AI
systems for subjective visual assessment.

</details>


### [172] [Sim-DETR: Unlock DETR for Temporal Sentence Grounding](https://arxiv.org/abs/2509.23867)
*Jiajin Tang,Zhengxuan Wei,Yuchen Zhu,Cheng Shi,Guanbin Li,Liang Lin,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Sim-DETR的简单而强大的基线方法，用于视频中的时序句子定位任务。通过分析发现传统DETR增强策略在此任务中效果不佳，并揭示了两类查询冲突是根本原因。Sim-DETR在解码器中引入两项改进：基于语义和位置重叠约束查询间的自注意力，以及增加查询到帧的对齐机制，以协调全局语义与局部定位。实验表明该方法充分释放了DETR在此任务上的潜力。


<details>
  <summary>Details</summary>
Motivation: 发现现有的DETR增强策略在时序句子定位任务中无法提升性能，甚至可能下降，因此需要深入分析其根本原因并提出更有效的解决方案。

Method: 提出Sim-DETR，在标准DETR的解码器层进行两项修改：一是根据语义和位置重叠约束查询之间的自注意力，二是引入查询到帧的对齐机制以融合全局与局部上下文信息。

Result: 实验结果表明，Sim-DETR有效解决了查询冲突问题，在时序句子定位任务上显著提升了性能，展现出比现有方法更强的基线效果。

Conclusion: Sim-DETR通过简单但关键的改进，成功克服了DETR在时序句子定位中的局限性，为该任务提供了一个强有力的基准模型，并揭示了未来改进方向。

Abstract: Temporal sentence grounding aims to identify exact moments in a video that
correspond to a given textual query, typically addressed with detection
transformer (DETR) solutions. However, we find that typical strategies designed
to enhance DETR do not improve, and may even degrade, its performance in this
task. We systematically analyze and identify the root causes of this abnormal
behavior: (1) conflicts between queries from similar target moments and (2)
internal query conflicts due to the tension between global semantics and local
localization. Building on these insights, we propose a simple yet powerful
baseline, Sim-DETR, which extends the standard DETR with two minor
modifications in the decoder layers: (1) constraining self-attention between
queries based on their semantic and positional overlap and (2) adding
query-to-frame alignment to bridge the global and local contexts. Experiments
demonstrate that Sim-DETR unlocks the full potential of DETR for temporal
sentence grounding, offering a strong baseline for future research.

</details>


### [173] [Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models](https://arxiv.org/abs/2509.23876)
*Ky Dan Nguyen,Hoang Lam Tran,Anh-Dung Dinh,Daochang Liu,Weidong Cai,Xiuying Wang,Chang Xu*

Main category: cs.CV

TL;DR: 提出了一种名为Information-Grounding Guidance (IGG)的新机制，用于解决自回归图像生成模型在渐进式分辨率缩放过程中出现的信息不一致问题，提升了生成图像的清晰度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在图像生成中因逐步缩放分辨率导致跨时间步的patch信息不一致，使引导信号偏离条件信息，影响生成质量。

Method: 引入IGG机制，通过注意力机制将引导信号锚定在语义重要区域，并在采样过程中自适应增强关键信息patch，保持引导与内容的一致性。

Result: 在类条件和文本到图像生成任务中，IGG生成了更清晰、连贯且语义准确的图像，优于现有自回归方法。

Conclusion: IGG有效缓解了AR模型在多尺度生成中的信息漂移问题，为自回归图像生成设定了新基准。

Abstract: Autoregressive (AR) models based on next-scale prediction are rapidly
emerging as a powerful tool for image generation, but they face a critical
weakness: information inconsistencies between patches across timesteps
introduced by progressive resolution scaling. These inconsistencies scatter
guidance signals, causing them to drift away from conditioning information and
leaving behind ambiguous, unfaithful features. We tackle this challenge with
Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance
to semantically important regions through attention. By adaptively reinforcing
informative patches during sampling, IGG ensures that guidance and content
remain tightly aligned. Across both class-conditioned and text-to-image
generation tasks, IGG delivers sharper, more coherent, and semantically
grounded images, setting a new benchmark for AR-based methods.

</details>


### [174] [PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications](https://arxiv.org/abs/2509.23879)
*Hitesh Laxmichand Patel,Amit Agarwal,Srikant Panda,Hansa Meghwani,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 提出Patch Context Robustness Index (PCRI)，首个用于量化多模态大语言模型对视觉上下文粒度变化鲁棒性的系统性指标，评估19个主流MLLM在15个基准上的表现，发现多数模型对背景噪声敏感，仅少数如InternVL2-26B和Qwen2VL-72B表现出强鲁棒性，PCRI为模型设计和选择提供诊断依据。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标未能捕捉多模态大语言模型（MLLM）对无关或干扰性视觉上下文的敏感性，导致其在真实场景中的可靠性受限，因此需要一种系统且可解释的方法来衡量模型对视觉上下文变化的鲁棒性。

Method: 提出Patch Context Robustness Index (PCRI)，通过比较模型在局部图像块输入与完整图像输入下的性能变化，量化其对视觉上下文粒度变化的鲁棒性，并在19个最先进的MLLM和15个视觉-语言基准上进行大规模评估。

Result: 大多数主流MLLM对背景噪声敏感，表现脆弱；仅少数模型（如InternVL2-26B和Qwen2VL-72B）在多种任务中展现出一致的上下文鲁棒性；PCRI揭示了不同架构在视觉上下文处理与整合方面的差异。

Conclusion: PCRI是首个系统评估MLLM视觉上下文鲁棒性的指标，能够支持模型的严谨比较、合理选型，并指导未来更鲁棒的模型架构与训练策略的设计，推动MLLM在现实场景中的可靠部署。

Abstract: The reliability of Multimodal Large Language Models (MLLMs) in real-world
settings is often undermined by sensitivity to irrelevant or distracting visual
context, an aspect not captured by existing evaluation metrics. We introduce
the \textbf{Patch Context Robustness Index (PCRI)}, the first systematic and
interpretable score for quantifying MLLM robustness to variations in visual
context granularity, measuring performance changes between localized image
patches and full-image input.
  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language
benchmarks, we find that most leading models remain brittle to background
noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating
consistent robustness across tasks. PCRI analysis also highlights how different
model architectures handle and integrate visual context, offering actionable
diagnostic insight for both researchers and practitioners.
  PCRI enables rigorous comparison of context robustness, supporting principled
model selection and guiding the development of future architectures and
training strategies for robust, real-world deployment.

</details>


### [175] [Learning Adaptive Pseudo-Label Selection for Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2509.23880)
*Taehun Kong,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督3D目标检测框架，通过可学习的伪标签模块自适应地选择高质量伪标签，结合上下文信息和软监督策略，在KITTI和Waymo数据集上显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有伪标签选择方法依赖人工设定阈值或仅使用部分网络信息，忽略了上下文信息（如物体距离、类别和学习状态），难以准确评估伪标签质量。

Method: 提出一种可学习的伪标签模块，引入两个教师端输出网络进行分数融合与上下文自适应阈值判定，并通过GT边界框对齐监督；同时采用软监督策略增强学生网络对噪声伪标签的鲁棒性。

Result: 在KITTI和Waymo数据集上实验表明，该方法能选择高精度伪标签，覆盖更广的上下文场景，召回率更高，显著优于现有半监督3D检测方法。

Conclusion: 所提出的框架有效提升了半监督3D目标检测中伪标签的选择质量与模型性能，具备更强的鲁棒性和泛化能力。

Abstract: Semi-supervised 3D object detection (SS3DOD) aims to reduce costly 3D
annotations utilizing unlabeled data. Recent studies adopt pseudo-label-based
teacher-student frameworks and demonstrate impressive performance. The main
challenge of these frameworks is in selecting high-quality pseudo-labels from
the teacher's predictions. Most previous methods, however, select pseudo-labels
by comparing confidence scores over thresholds manually set. The latest works
tackle the challenge either by dynamic thresholding or refining the quality of
pseudo-labels. Such methods still overlook contextual information e.g. object
distances, classes, and learning states, and inadequately assess the
pseudo-label quality using partial information available from the networks. In
this work, we propose a novel SS3DOD framework featuring a learnable
pseudo-labeling module designed to automatically and adaptively select
high-quality pseudo-labels. Our approach introduces two networks at the teacher
output level. These networks reliably assess the quality of pseudo-labels by
the score fusion and determine context-adaptive thresholds, which are
supervised by the alignment of pseudo-labels over GT bounding boxes.
Additionally, we introduce a soft supervision strategy that can learn robustly
under pseudo-label noises. This helps the student network prioritize cleaner
labels over noisy ones in semi-supervised learning. Extensive experiments on
the KITTI and Waymo datasets demonstrate the effectiveness of our method. The
proposed method selects high-precision pseudo-labels while maintaining a wider
coverage of contexts and a higher recall rate, significantly improving relevant
SS3DOD methods.

</details>


### [176] [Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction](https://arxiv.org/abs/2509.23885)
*Guoquan Wei,Zekun Zhou,Liu Shi,Wenzhe Shan,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SuperDiff的新型可调泛化扩散模型，用于低剂量CT图像去噪，仅需低剂量投影域数据训练，结合自监督上下文子数据与知识蒸馏，在重建质量和泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型依赖配对数据且泛化性差，扩散模型需学习干净数据分布难以满足临床需求，自监督方法在跨剂量泛化时性能显著下降。

Method: 设计了基于低剂量CT投影域的上下文子数据相似性自适应感知策略，结合知识蒸馏与潜在扩散模型进行细节优化，并提出像素级自校正融合技术实现精细重建，采用双域级联策略实现自监督去噪。

Result: 在多个数据集和真实数据上验证，SuperDiff在定性和定量评估中均优于当前最先进的方法，具备良好的跨剂量甚至未见剂量的泛化能力。

Conclusion: SuperDiff仅需低剂量投影数据即可高效训练，实现了高保真图像重建与强泛化能力，为临床低剂量CT应用提供了可行方案。

Abstract: Current models based on deep learning for low-dose CT denoising rely heavily
on paired data and generalize poorly. Even the more concerned diffusion models
need to learn the distribution of clean data for reconstruction, which is
difficult to satisfy in medical clinical applications. At the same time,
self-supervised-based methods face the challenge of significant degradation of
generalizability of models pre-trained for the current dose to expand to other
doses. To address these issues, this paper proposes a novel method of
tunable-generalization diffusion powered by self-supervised contextual sub-data
for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata
similarity adaptive sensing strategy is designed for denoising centered on the
LDCT projection domain, which provides an initial prior for the subsequent
progress. Subsequently, the initial prior is used to combine knowledge
distillation with a deep combination of latent diffusion models for optimizing
image details. The pre-trained model is used for inference reconstruction, and
the pixel-level self-correcting fusion technique is proposed for fine-grained
reconstruction of the image domain to enhance the image fidelity, using the
initial prior and the LDCT image as a guide. In addition, the technique is
flexibly applied to the generalization of upper and lower doses or even unseen
doses. Dual-domain strategy cascade for self-supervised LDCT denoising,
SuperDiff requires only LDCT projection domain data for training and testing.
Full qualitative and quantitative evaluations on both datasets and real data
show that SuperDiff consistently outperforms existing state-of-the-art methods
in terms of reconstruction and generalization performance.

</details>


### [177] [AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities](https://arxiv.org/abs/2509.23888)
*Tatsuro Banno,Takehiko Ohkawa,Ruicong Liu,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了AssemblyHands-X，首个无标记的3D手-身协同动作识别基准数据集，通过多视角视频构建精确的3D姿态标注，并验证了结合手部与身体姿态可显著提升双手机械活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D动作数据集通常只标注手或身体姿态，缺乏对手-身协调作用的系统评估；基于标记的动作捕捉方法存在视觉伪影，限制模型在自然视频中的泛化能力。

Method: 构建了一个基于同步多视角视频的3D姿态标注流程，结合多视角三角测量与SMPL-X网格拟合，实现对手和上半身的可靠3D注册，并评估不同输入表示（如视频、手部姿态、身体姿态、手-身联合姿态）在基于图卷积或时空注意力机制的动作识别模型上的表现。

Result: 实验表明，基于姿态的动作识别比视频基线更高效准确，且联合建模手部与身体线索优于单独使用任一部分，在双手机械活动中显著提升了识别性能。

Conclusion: 手-身之间的动态交互对理解双手机械活动至关重要，应被共同建模以实现更全面的动作理解。

Abstract: Bimanual human activities inherently involve coordinated movements of both
hands and body. However, the impact of this coordination in activity
understanding has not been systematically evaluated due to the lack of suitable
datasets. Such evaluation demands kinematic-level annotations (e.g., 3D pose)
for the hands and body, yet existing 3D activity datasets typically annotate
either hand or body pose. Another line of work employs marker-based motion
capture to provide full-body pose, but the physical markers introduce visual
artifacts, thereby limiting models' generalization to natural, markerless
videos. To address these limitations, we present AssemblyHands-X, the first
markerless 3D hand-body benchmark for bimanual activities, designed to study
the effect of hand-body coordination for action recognition. We begin by
constructing a pipeline for 3D pose annotation from synchronized multi-view
videos. Our approach combines multi-view triangulation with SMPL-X mesh
fitting, yielding reliable 3D registration of hands and upper body. We then
validate different input representations (e.g., video, hand pose, body pose, or
hand-body pose) across recent action recognition models based on graph
convolution or spatio-temporal attention. Our extensive experiments show that
pose-based action inference is more efficient and accurate than video
baselines. Moreover, joint modeling of hand and body cues improves action
recognition over using hands or upper body alone, highlighting the importance
of modeling interdependent hand-body dynamics for a holistic understanding of
bimanual activities.

</details>


### [178] [LifeCLEF Plant Identification Task 2015](https://arxiv.org/abs/2509.23891)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2015植物识别挑战赛基于超过10万张图像和1000种西欧植物物种，评估大规模植物识别方法，数据来自公众科学平台，总结了参赛团队的方法并分析了结果。


<details>
  <summary>Details</summary>
Motivation: 评估在接近真实世界生物多样性监测条件下大规模植物识别方法的性能。

Method: 使用超过10万张图像和1000种植物物种的数据集，组织国际挑战赛，比较不同研究团队提交的识别系统与方法。

Result: 成功构建了一个大规模、基于公众参与的植物图像数据集，并评估了多种植物识别系统，揭示了当前方法的优势与局限。

Conclusion: 该挑战赛推动了植物自动识别技术的发展，展示了公众科学在生物多样性监测中的潜力。

Abstract: The LifeCLEF plant identification challenge aims at evaluating plant
identification methods and systems at a very large scale, close to the
conditions of a real-world biodiversity monitoring scenario. The 2015
evaluation was actually conducted on a set of more than 100K images
illustrating 1000 plant species living in West Europe. The main originality of
this dataset is that it was built through a large-scale participatory sensing
plateform initiated in 2011 and which now involves tens of thousands of
contributors. This overview presents more precisely the resources and
assessments of the challenge, summarizes the approaches and systems employed by
the participating research groups, and provides an analysis of the main
outcomes.

</details>


### [179] [Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios](https://arxiv.org/abs/2509.23895)
*Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li*

Main category: cs.CV

TL;DR: 提出了一种跨模态对比遗忘框架（CCU），有效解决视觉模态在多模态应用中的隐私泄露问题，同时保持其他模态性能和类别结构稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在视觉遗忘过程中难以保持跨模态知识和保留数据的类内结构稳定性，导致整体性能下降。

Method: 提出CCU框架，包含三个关键组件：选择性视觉遗忘（逆对比学习）、跨模态知识保留（语义一致性）和双集对比分离（隔离扰动）。

Result: 在三个数据集上实验表明，CCU相比基线方法在仅7%遗忘时间下提升7.12%准确率。

Conclusion: CCU能高效实现视觉模态的数据遗忘，同时保护跨模态知识和保留数据的结构完整性，显著提升遗忘效率与模型性能。

Abstract: Visual modality is the most vulnerable to privacy leakage in real-world
multimodal applications like autonomous driving with visual and radar data;
Machine unlearning removes specific training data from pre-trained models to
address privacy leakage, however, existing methods fail to preserve cross-modal
knowledge and maintain intra-class structural stability of retain data, leading
to reduced overall and other modalities' performance during visual unlearning;
to address these challenges, we propose a Cross-modal Contrastive Unlearning
(CCU) framework, which integrates three key components: (a) selective visual
unlearning: employing inverse contrastive learning to dissociate visual
representations from their original semantics, (b) cross-modal knowledge
retention: preserving other modalities' discriminability through semantic
consistency, and (c) dual-set contrastive separation: preserving the model
performance via isolation of structural perturbations between the unlearn set
and retain set; extensive experiments on three datasets demonstrate the
superiority of CCU, and our method achieves a 7.12% accuracy improvement with
only 7% of the unlearning time compared to the top-accuracy baseline.

</details>


### [180] [Q-FSRU: Quantum-Augmented Frequency-Spectral For Medical Visual Question Answering](https://arxiv.org/abs/2509.23899)
*Rakesh Thakur,Yusra Tariq,Rakesh Chandra Joshi*

Main category: cs.CV

TL;DR: 提出Q-FSRU模型，结合频域表示与量子检索增强生成，提升医学视觉问答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决需要图像和文本理解的复杂临床问题仍是医疗AI的重大挑战。

Method: 将医学图像和文本特征通过快速傅里叶变换转至频域，并引入量子启发的检索系统从外部获取医学知识进行融合推理。

Result: 在VQA-RAD数据集上表现优于先前模型，尤其在需图文推理的复杂病例中性能更优。

Conclusion: 频域与量子信息的结合有效提升了医学VQA模型的性能与可解释性，为医生提供更智能、透明的AI工具。

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [181] [LifeCLEF Plant Identification Task 2014](https://arxiv.org/abs/2509.23900)
*Herve Goeau,Alexis Joly,Pierre Bonnet,Souheil Selmi,Jean-Francois Molino,Daniel Barthelemy,Nozha Boujemaa*

Main category: cs.CV

TL;DR: LifeCLEF植物识别任务通过公民科学计划构建了包含500种植物的多模态图像数据集，用于评估植物识别系统。


<details>
  <summary>Details</summary>
Motivation: 推动植物识别技术在真实场景中的应用，促进生物多样性研究。

Method: 利用来自Tela Botanica的公民科学数据，收集七类植物图像（如叶、花、果实等），并组织国际团队进行图像检索方法评测。

Result: 共有来自六个国家的十个团队提交了27种不同方法的运行结果，展示了多媒体检索技术在植物识别中的潜力与挑战。

Conclusion: 该任务验证了基于真实条件的植物识别系统的可行性，并推动了图像检索领域对植物学和生物多样性的持续关注。

Abstract: The LifeCLEFs plant identification task provides a testbed for a
system-oriented evaluation of plant identification about 500 species trees and
herbaceous plants. Seven types of image content are considered: scan and
scan-like pictures of leaf, and 6 kinds of detailed views with unconstrained
conditions, directly photographed on the plant: flower, fruit, stem & bark,
branch, leaf and entire view. The main originality of this data is that it was
specifically built through a citizen sciences initiative conducted by Tela
Botanica, a French social network of amateur and expert botanists. This makes
the task closer to the conditions of a real-world application. This overview
presents more precisely the resources and assessments of task, summarizes the
retrieval approaches employed by the participating groups, and provides an
analysis of the main evaluation results. With a total of ten groups from six
countries and with a total of twenty seven submitted runs, involving distinct
and original methods, this fourth year task confirms Image & Multimedia
Retrieval community interest for biodiversity and botany, and highlights
further challenging studies in plant identification.

</details>


### [182] [EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging](https://arxiv.org/abs/2509.23906)
*Anoushka Harit,William Prew,Zhongtian Sun,Florian Markowetz*

Main category: cs.CV

TL;DR: 提出一种结合类条件扩散回放和弹性权重固化（EWC）的持续学习框架，用于医学图像基础模型的持续适应，在保护隐私的同时有效减少遗忘，性能接近联合训练。


<details>
  <summary>Details</summary>
Motivation: 医学图像模型需要持续适应新数据，但全量重新训练受限于隐私和成本问题，因此需要一种既能避免存储患者样本又能有效缓解灾难性遗忘的方法。

Method: 采用类条件扩散回放生成虚拟样本进行训练，并结合弹性权重固化（EWC）来保护重要模型参数，使用紧凑的Vision Transformer作为骨干网络，在多个医学图像数据集上进行持续学习评估。

Result: 在CheXpert数据集上达到0.851 AUROC，相比DER++相对减少超过30%的遗忘，接近联合训练的0.869 AUROC，同时保持高效和隐私保护；分析表明回放保真度和Fisher加权参数漂移是影响遗忘的关键因素。

Conclusion: 该方法为临床影像模型提供了一条可扩展、隐私友好的持续适应路径，结合回放机制和参数稳定性策略能有效平衡学习新知识与保留旧知识。

Abstract: Medical imaging foundation models must adapt over time, yet full retraining
is often blocked by privacy constraints and cost. We present a continual
learning framework that avoids storing patient exemplars by pairing class
conditional diffusion replay with Elastic Weight Consolidation. Using a compact
Vision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and
CheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by
more than 30\% relative to DER\texttt{++}, and approaches joint training at
0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect
forgetting to two measurable factors: fidelity of replay and Fisher weighted
parameter drift, highlighting the complementary roles of replay diffusion and
synaptic stability. The results indicate a practical route for scalable,
privacy aware continual adaptation of clinical imaging models.

</details>


### [183] [Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation](https://arxiv.org/abs/2509.23907)
*You Zhou,Lijiang Chen,Shuchang Lyu,Guangxia Cui,Wenpei Bai,Zheng Zhou,Meng Li,Guangliang Cheng,Huiyu Zhou,Qi Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦域适应（FedDA）分割训练框架，通过客户端间的特征级对抗学习来对齐特征图，从而提升跨域医学图像分割的模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于医疗资源不平衡、数据损坏或保存不当，不同客户端可能拥有不同模态的医学图像，导致联邦学习中的域偏移问题，影响分割性能。

Method: 提出基于对抗训练机制的特征级对齐方法，在客户端之间进行对抗学习以实现跨域特征对齐，增强模型在多域上的泛化能力。

Result: 在三个医学图像数据集上的实验表明，FedDA在客观和主观评估中均优于现有最先进的联邦聚合算法，实现了跨模态处理能力和稳定的分割性能。

Conclusion: FedDA有效缓解了联邦学习中因模态异质性带来的域偏移问题，显著提升了跨域医学图像分割的性能与鲁棒性。

Abstract: Federated learning enables collaborative training of machine learning models
among different clients while ensuring data privacy, emerging as the mainstream
for breaking data silos in the healthcare domain. However, the imbalance of
medical resources, data corruption or improper data preservation may lead to a
situation where different clients possess medical images of different modality.
This heterogeneity poses a significant challenge for cross-domain medical image
segmentation within the federated learning framework. To address this
challenge, we propose a new Federated Domain Adaptation (FedDA) segmentation
training framework. Specifically, we propose a feature-level adversarial
learning among clients by aligning feature maps across clients through
embedding an adversarial training mechanism. This design can enhance the
model's generalization on multiple domains and alleviate the negative impact
from domain-shift. Comprehensive experiments on three medical image datasets
demonstrate that our proposed FedDA substantially achieves cross-domain
federated aggregation, endowing single modality client with cross-modality
processing capabilities, and consistently delivers robust performance compared
to state-of-the-art federated aggregation algorithms in objective and
subjective assessment. Our code are available at
https://github.com/GGbond-study/FedDA.

</details>


### [184] [EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling](https://arxiv.org/abs/2509.23909)
*Xin Luo,Jiahao Wang,Chenyuan Wu,Shitao Xiao,Xiyan Jiang,Defu Lian,Jiajun Zhang,Dong Liu,Zheng liu*

Main category: cs.CV

TL;DR: 本文提出了一种针对指令引导图像编辑的高保真奖励模型EditScore，通过构建EditReward-Bench基准测试，系统评估并优化奖励模型性能，成功实现基于强化学习的高效图像编辑训练，显著提升了编辑质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有指令引导图像编辑模型在处理复杂指令时表现不佳，且缺乏有效的奖励信号来支持强化学习的应用，限制了其进一步发展。

Method: 构建了EditReward-Bench基准用于评估奖励模型；设计并训练了一系列专用奖励模型EditScore（7B-72B），结合数据筛选与自集成策略提升性能；利用EditScore作为奖励信号进行在线强化学习训练。

Result: EditScore在EditReward-Bench上性能媲美甚至超过GPT-5；相比现有开源VLM，其能有效支持RL训练；应用于OmniGen2基线模型后，显著提升最终模型的编辑性能。

Conclusion: 高质量、领域专用的奖励模型是解锁强化学习在图像编辑中潜力的关键，本文为该领域提供了从评测到建模再到训练的系统性解决方案。

Abstract: Instruction-guided image editing has achieved remarkable progress, yet
current models still face challenges with complex instructions and often
require multiple samples to produce a desired result. Reinforcement Learning
(RL) offers a promising solution, but its adoption in image editing has been
severely hindered by the lack of a high-fidelity, efficient reward signal. In
this work, we present a comprehensive methodology to overcome this barrier,
centered on the development of a state-of-the-art, specialized reward model. We
first introduce EditReward-Bench, a comprehensive benchmark to systematically
evaluate reward models on editing quality. Building on this benchmark, we
develop EditScore, a series of reward models (7B-72B) for evaluating the
quality of instruction-guided image editing. Through meticulous data curation
and filtering, EditScore effectively matches the performance of learning
proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy
tailored for the generative nature of EditScore, our largest variant even
surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity
reward model is the key to unlocking online RL for image editing. Our
experiments show that, while even the largest open-source VLMs fail to provide
an effective learning signal, EditScore enables efficient and robust policy
optimization. Applying our framework to a strong base model, OmniGen2, results
in a final model that shows a substantial and consistent performance uplift.
Overall, this work provides the first systematic path from benchmarking to
reward modeling to RL training in image editing, showing that a high-fidelity,
domain-specialized reward model is the key to unlocking the full potential of
RL in this domain.

</details>


### [185] [MoReact: Generating Reactive Motion from Textual Descriptions](https://arxiv.org/abs/2509.23911)
*Xiyan Xu,Sirui Xu,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本驱动的人类反应动作生成方法MoReact，通过扩散模型分步生成全局轨迹和局部动作，并引入交互损失函数提升生成动作的真实性和与文本描述的对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多人互动动作时难以准确响应多样化的动态场景，且缺乏对语义信息的有效利用，无法实现自适应的反应生成。

Method: 提出MoReact，采用扩散模型先生成全局轨迹再生成局部动作，结合文本描述和对方动作进行条件控制，并设计新的交互损失函数以增强近距离互动的真实感。

Result: 在双人动作数据集上的实验表明，该方法能生成逼真、多样且可控的反应动作，动作与对方运动匹配良好，并符合文本语义描述。

Conclusion: MoReact有效解决了文本驱动下的人类反应动作生成问题，实现了更高的语义一致性与动作协调性，为虚拟角色交互、人机协作等应用提供了新思路。

Abstract: Modeling and generating human reactions poses a significant challenge with
broad applications for computer vision and human-computer interaction. Existing
methods either treat multiple individuals as a single entity, directly
generating interactions, or rely solely on one person's motion to generate the
other's reaction, failing to integrate the rich semantic information that
underpins human interactions. Yet, these methods often fall short in adaptive
responsiveness, i.e., the ability to accurately respond to diverse and dynamic
interaction scenarios. Recognizing this gap, our work introduces an approach
tailored to address the limitations of existing models by focusing on
text-driven human reaction generation. Our model specifically generates
realistic motion sequences for individuals that responding to the other's
actions based on a descriptive text of the interaction scenario. The goal is to
produce motion sequences that not only complement the opponent's movements but
also semantically fit the described interactions. To achieve this, we present
MoReact, a diffusion-based method designed to disentangle the generation of
global trajectories and local motions sequentially. This approach stems from
the observation that generating global trajectories first is crucial for
guiding local motion, ensuring better alignment with given action and text.
Furthermore, we introduce a novel interaction loss to enhance the realism of
generated close interactions. Our experiments, utilizing data adapted from a
two-person motion dataset, demonstrate the efficacy of our approach for this
novel task, which is capable of producing realistic, diverse, and controllable
reactions that not only closely match the movements of the counterpart but also
adhere to the textual guidance. Please find our webpage at
https://xiyan-xu.github.io/MoReactWebPage.

</details>


### [186] [Revisit the Imbalance Optimization in Multi-task Learning: An Experimental Analysis](https://arxiv.org/abs/2509.23915)
*Yihang Guo,Tianyuan Yu,Liang Bai,Yanming Guo,Yirun Ruan,William Li,Weishi Zheng*

Main category: cs.CV

TL;DR: 本文系统分析了多任务学习中的“优化不平衡”问题，发现任务特定梯度的范数与优化不平衡密切相关，并提出通过根据梯度范数缩放任务损失的简单策略，即可达到与耗时的网格搜索相当的性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在训练通用视觉系统方面具有潜力，但常因任务干扰导致的“优化不平衡”问题而表现不佳，本文旨在深入探究该问题的根本原因。

Method: 通过对现有优化方法和架构进行系统的实验分析，研究数据集、模型初始化、数据量等因素的影响，并分析任务梯度范数与性能之间的关系，提出基于梯度范数的任务损失缩放策略。

Result: 发现现有方法在不同数据集上表现不一致，且依赖昂贵的超参数调优；视觉基础模型虽提供良好初始化，但未解决优化不平衡；梯度范数与不平衡高度相关；所提损失缩放策略可媲美网格搜索效果。

Conclusion: 控制梯度动态是实现稳定多任务学习的更直接有效途径，相比复杂方法更具实用性和可扩展性。

Abstract: Multi-task learning (MTL) aims to build general-purpose vision systems by
training a single network to perform multiple tasks jointly. While promising,
its potential is often hindered by "unbalanced optimization", where task
interference leads to subpar performance compared to single-task models. To
facilitate research in MTL, this paper presents a systematic experimental
analysis to dissect the factors contributing to this persistent problem. Our
investigation confirms that the performance of existing optimization methods
varies inconsistently across datasets, and advanced architectures still rely on
costly grid-searched loss weights. Furthermore, we show that while powerful
Vision Foundation Models (VFMs) provide strong initialization, they do not
inherently resolve the optimization imbalance, and merely increasing data
quantity offers limited benefits. A crucial finding emerges from our analysis:
a strong correlation exists between the optimization imbalance and the norm of
task-specific gradients. We demonstrate that this insight is directly
applicable, showing that a straightforward strategy of scaling task losses
according to their gradient norms can achieve performance comparable to that of
an extensive and computationally expensive grid search. Our comprehensive
analysis suggests that understanding and controlling gradient dynamics is a
more direct path to stable MTL than developing increasingly complex methods.

</details>


### [187] [Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives](https://arxiv.org/abs/2509.23917)
*Kuanrong Liu,Siyuan Liang,Cheng Qian,Ming Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文研究了CLIP模型在对抗性扰动下的跨任务迁移行为，发现细粒度任务生成的对抗样本具有更强的迁移能力，并据此提出了一种新的多任务对抗性CLIP框架（MT-AdvCLIP），显著提升了对多种CLIP衍生模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 理解对抗样本在不同任务间的迁移特性对于评估CLIP模型的泛化能力和安全风险至关重要，但其在细粒度任务中的鲁棒性尚未充分探索。

Method: 通过系统性的实证分析，研究CLIP在图像-文本检索、目标检测和语义分割等任务中的对抗迁移行为，并提出MT-AdvCLIP框架，引入任务感知特征聚合损失以增强扰动的跨任务泛化能力。

Result: 实验表明，MT-AdvCLIP在多个公开数据集上显著提高了对抗迁移成功率，平均攻击成功率提升超过39%，且不增加扰动预算。

Conclusion: 该研究揭示了多任务CLIP模型中对抗样本的迁移机制，为多任务鲁棒性评估和对抗样本设计提供了新见解。

Abstract: As a general-purpose vision-language pretraining model, CLIP demonstrates
strong generalization ability in image-text alignment tasks and has been widely
adopted in downstream applications such as image classification and image-text
retrieval. However, it struggles with fine-grained tasks such as object
detection and semantic segmentation. While many variants aim to improve CLIP on
these tasks, its robustness to adversarial perturbations remains underexplored.
Understanding how adversarial examples transfer across tasks is key to
assessing CLIP's generalization limits and security risks. In this work, we
conduct a systematic empirical analysis of the cross-task transfer behavior of
CLIP-based models on image-text retrieval, object detection, and semantic
segmentation under adversarial perturbations. We find that adversarial examples
generated from fine-grained tasks (e.g., object detection and semantic
segmentation) often exhibit stronger transfer potential than those from
coarse-grained tasks, enabling more effective attacks against the original CLIP
model. Motivated by this observation, we propose a novel framework, Multi-Task
Adversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature
aggregation loss and generates perturbations with enhanced cross-task
generalization capability. This design strengthens the attack effectiveness of
fine-grained task models on the shared CLIP backbone. Experimental results on
multiple public datasets show that MT-AdvCLIP significantly improves the
adversarial transfer success rate (The average attack success rate across
multiple tasks is improved by over 39%.) against various CLIP-derived models,
without increasing the perturbation budget. This study reveals the transfer
mechanism of adversarial examples in multi-task CLIP models, offering new
insights into multi-task robustness evaluation and adversarial example design.

</details>


### [188] [Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models](https://arxiv.org/abs/2509.23919)
*Longtao Jiang,Mingfei Han,Lei Chen,Yongqiang Yu,Feng Zhao,Xiaojun Chang,Zhihui Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mask AutoRegressive（MAR）模型的无需训练的文本引导图像修复方法Token Painter，通过双流编码器信息融合和自适应解码器注意力增强，实现了与文本提示高度一致且背景和谐的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在文本引导图像修复中难以兼顾提示细节对齐和背景一致性，因此需要一种更有效的局部可控生成方法。

Method: 提出Token Painter，包含双流编码器信息融合（DEIF）和自适应解码器注意力分数增强（ADAE），在频率域融合文本与背景信息生成指导令牌，并增强关键注意力以提升生成质量。

Result: 实验表明该无需训练的方法在几乎所有指标上均优于先前最先进方法，并提供更优的视觉效果。

Conclusion: Token Painter为文本引导图像修复提供了一种高效、无需训练的解决方案，显著提升了生成内容的文本忠实度与背景协调性。

Abstract: Text-guided image inpainting aims to inpaint masked image regions based on a
textual prompt while preserving the background. Although diffusion-based
methods have become dominant, their property of modeling the entire image in
latent space makes it challenging for the results to align well with prompt
details and maintain a consistent background. To address these issues, we
explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports
image inpainting by generating latent tokens corresponding to mask regions,
enabling better local controllability without altering the background. However,
directly applying MAR to this task makes the inpainting content either ignore
the prompts or be disharmonious with the background context. Through analysis
of the attention maps from the inpainting images, we identify the impact of
background tokens on text tokens during the MAR generation, and leverage this
to design \textbf{Token Painter}, a training-free text-guided image inpainting
method based on MAR. Our approach introduces two key components: (1)
Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and
context information from text and background in frequency domain to produce
novel guidance tokens, allowing MAR to generate text-faithful inpainting
content while keeping harmonious with background context. (2) Adaptive Decoder
Attention Score Enhancing (ADAE), which adaptively enhances attention scores on
guidance tokens and inpainting tokens to further enhance the alignment of
prompt details and the content visual quality. Extensive experiments
demonstrate that our training-free method outperforms prior state-of-the-art
methods across almost all metrics and delivers superior visual results. Codes
will be released.

</details>


### [189] [Learning Encoding-Decoding Direction Pairs to Unveil Concepts of Influence in Deep Vision Networks](https://arxiv.org/abs/2509.23926)
*Alexandros Doumanoglou,Kurt Driessens,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 提出一种新方法，通过方向聚类和概率视角下的信号向量估计编码-解码方向对，揭示深度视觉网络中的概念表示，实现模型理解、解释与修正。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络中的概念表示难以解析，缺乏直接可访问的编码-解码方向对，限制了模型的可解释性与调试能力。

Method: 采用方向聚类识别解码方向，结合信号向量在概率框架下估计编码方向，并提出不确定性区域对齐技术利用网络权重揭示影响预测的可解释方向。

Result: 在合成数据上恢复真实方向对，在真实数据上解码方向对应单一语义概念且优于无监督基线，信号向量准确估计编码方向，并通过激活最大化验证；成功应用于模型行为理解、个体预测解释及反事实生成。

Conclusion: 该方法有效揭示深度网络中的概念结构，提供可解释的方向对，支持模型的理解、诊断与修正，推动黑箱模型的透明化。

Abstract: Empirical evidence shows that deep vision networks represent concepts as
directions in latent space, vectors we call concept embeddings. Each concept
has a latent factor-a scalar-indicating its presence in an input patch. For a
given patch, multiple latent factors are encoded into a compact representation
by linearly combining concept embeddings, with the factors as coefficients.
Since these embeddings enable such encoding, we call them encoding directions.
A latent factor can be recovered via the inner product with a filter, a vector
we call a decoding direction. These encoding-decoding direction pairs are not
directly accessible, but recovering them helps open the black box of deep
networks, enabling understanding, debugging, and improving models. Decoder
directions attribute meaning to latent codes, while encoding directions assess
concept influence on predictions, with both enabling model correction by
unlearning irrelevant concepts. Unlike prior matrix decomposition, autoencoder,
or dictionary learning methods that rely on feature reconstruction, we propose
a new perspective: decoding directions are identified via directional
clustering of activations, and encoding directions are estimated with signal
vectors under a probabilistic view. We further leverage network weights through
a novel technique, Uncertainty Region Alignment, which reveals interpretable
directions affecting predictions. Our analysis shows that (a) on synthetic
data, our method recovers ground-truth direction pairs; (b) on real data,
decoding directions map to monosemantic, interpretable concepts and outperform
unsupervised baselines; and (c) signal vectors faithfully estimate encoding
directions, validated via activation maximization. Finally, we demonstrate
applications in understanding global model behavior, explaining individual
predictions, and intervening to produce counterfactuals or correct errors.

</details>


### [190] [SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing](https://arxiv.org/abs/2509.23927)
*Yi Yang,Xiaokun Zhang,Qingchen Fang,Ziqi Ye,Rui Li,Li Liu,Haipeng Wang*

Main category: cs.CV

TL;DR: 本文提出了首个通用的SAR多模态基础模型SAR-KnowLIP，构建了首个大规模具有完整地理投影属性的SAR数据集SAR-GEOVL-1M，并设计了自洽迭代优化机制，在11个下游任务中表现出领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态AI方法主要针对RGB图像，缺乏对合成孔径雷达（SAR）图像的有效建模，且忽略了地理信息在遥感研究中的重要作用。

Method: 引入地理信息属性，构建大规模SAR多模态数据集SAR-GEOVL-1M；通过层次化认知思维链生成对齐文本；设计自洽迭代优化机制，结合对比、匹配与重建学习提升跨模态对齐。

Result: 建立了包含12万SAR图像、覆盖135座城市的多平台数据集；生成超百万条多维语义标注；在11个下游任务中超越14种主流基础模型，尤其在目标计数和地表覆盖分类中表现突出。

Conclusion: SAR-KnowLIP为SAR多模态建模提供了可复用的数据、模型与评估基准，推动了SAR跨模态智能的发展。

Abstract: Cross-modal artificial intelligence has garnered widespread attention in
recent years, achieving significant progress in the study of natural images.
However, existing methods are mostly designed for RGB imagery, leaving a
significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with
its all-day, all-weather imaging capabilities, plays an irreplaceable role in
remote sensing scene understanding. To address this gap, this paper proposes
SAR-KnowLIP, the first universal SAR multimodal foundational model, along with
reusable data and evaluation baselines. Specifically: (1) This work introduces
the critical yet long-overlooked attribute of geographic information into
remote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR
dataset with complete geographic projection properties), covering multiple
satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured
text is generated through a hierarchical cognitive chain-of-thought (HCoT),
providing more than one million multi-dimensional semantic annotations of
landforms, regional functions, target attributes, and spatial relationships.
(3) We design a Self-Consistent Iterative Optimization mechanism that
continuously enhances cross-modal alignment through a self-supervised closed
loop of contrastive, matching, and reconstruction learning on a transferable
multimodal encoder. (4) A unified evaluation benchmark is established across 11
representative downstream vision and vision-language tasks, with comparisons
against 14 leading foundation models, where SAR-KnowLIP demonstrates leading
performance, particularly in object counting and land-cover classification. We
expect that SAR-KnowLIP's large-scale multimodal data, transferable model
architecture, and comprehensive experimental benchmark will significantly
advance the development of SAR multimodal baseline models.

</details>


### [191] [AutoPrune: Each Complexity Deserves a Pruning Policy](https://arxiv.org/abs/2509.23931)
*Hanshi Wang,Yuhao Xu,Zekun Xu,Jin Gao,Yufan Liu,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的自适应剪枝框架AutoPrune，通过量化视觉与文本令牌间的互信息，并动态调整剪枝策略以适应不同任务和样本的复杂性，在大幅减少计算量的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型中的视觉令牌存在冗余，但固定或启发式剪枝策略无法根据输入和任务的复杂性动态调整，难以匹配模型整体推理过程。受人类视觉认知过程启发，需设计更灵活、自适应的剪枝机制。

Method: AutoPrune通过计算视觉与文本令牌之间的互信息，并将其映射到一个预算约束下的逻辑保留曲线，每条曲线形状由任务复杂度决定，从而实现复杂度自适应的令牌剪枝，且无需额外训练。

Result: 在LLaVA-1.5-7B上剪枝89%的视觉令牌，推理FLOPs减少76.8%，平均保留原始准确率的96.7%，相比PDrop提升9.1%。在多个视觉语言任务及自动驾驶的视觉-语言-动作模型中验证了有效性。

Conclusion: AutoPrune是一种即插即用、无需训练的自适应剪枝方法，能根据任务和样本复杂度动态调整剪枝策略，在显著降低计算成本的同时有效保持模型性能。

Abstract: The established redundancy in visual tokens within large vision-language
models allows pruning to effectively reduce their substantial computational
demands. Previous methods typically employ heuristic layer-specific pruning
strategies where, although the number of tokens removed may differ across
decoder layers, the overall pruning schedule is fixed and applied uniformly to
all input samples and tasks, failing to align token elimination with the
model's holistic reasoning trajectory. Cognitive science indicates that human
visual processing often begins with broad exploration to accumulate evidence
before narrowing focus as the target becomes distinct. Our experiments reveal
an analogous pattern in these models. This observation suggests that neither a
fixed pruning schedule nor a heuristic layer-wise strategy can optimally
accommodate the diverse complexities inherent in different inputs. To overcome
this limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a
training-free, plug-and-play framework that tailors pruning policies to varying
sample and task complexities. Specifically, AutoPrune quantifies the mutual
information between visual and textual tokens, then projects this signal to a
budget-constrained logistic retention curve. Each such logistic curve, defined
by its unique shape, corresponds to the specific complexity of different tasks
and can guarantee adherence to predefined computational constraints. We
evaluate AutoPrune on standard vision-language tasks and on
Vision-Language-Action models for autonomous driving. Notably, when applied to
LLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference
FLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all
tasks. This corresponds to a 9.1% improvement over the recent work PDrop,
demonstrating the effectiveness. Code is available at
https://github.com/AutoLab-SAI-SJTU/AutoPrune.

</details>


### [192] [CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting](https://arxiv.org/abs/2509.23947)
*Dragoş-Andrei Chileban,Andrei-Ştefan Bulzan,Cosmin Cernǎzanu-Glǎvan*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯点阵（3D-GS）的单视角汽车损伤检测与分割方法，通过将2D掩码提升至3D并结合深度与不透明度的正态分布模型进行高斯投影与过滤，实现了对难以在多视角下一致检测的小损伤的有效识别。


<details>
  <summary>Details</summary>
Motivation: 现有自动车损检测多依赖2D图像分析，缺乏对3D几何信息的利用；而传统3D重建方法在单视角或稀疏视角下表现有限，因此需要一种能在有限视角下实现精确3D损伤分割的方法。

Method: 提出一种无需学习的单视图3D-GS分割方法：利用SfM获取相机参数，将3D高斯投影到图像平面，并结合Z缓冲和深度/不透明度的正态分布模型进行过滤，从而实现从2D掩码到3D损伤的提升。

Result: 实验表明该方法在仅单视角可见的细微损伤（如划痕、小凹陷）检测中效果显著，优于依赖多视角一致性的方法，验证了其在实际复杂场景中的有效性。

Conclusion: 所提方法有效结合了3D高斯点阵与几何先验，在无需训练的情况下实现了高精度的单视角3D车损分割，为自动保险定损提供了实用且高效的解决方案。

Abstract: Automatic car damage detection has been a topic of significant interest for
the auto insurance industry as it promises faster, accurate, and cost-effective
damage assessments. However, few works have gone beyond 2D image analysis to
leverage 3D reconstruction methods, which have the potential to provide a more
comprehensive and geometrically accurate representation of the damage.
Moreover, recent methods employing 3D representations for novel view synthesis,
particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to
generate accurate and coherent 3D reconstructions from a limited number of
views. In this work we introduce an automatic car damage detection pipeline
that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we
propose a simple yet effective learning-free approach for single-view 3D-GS
segmentation. Specifically, Gaussians are projected onto the image plane using
camera parameters obtained via Structure from Motion (SfM). They are then
filtered through an algorithm that utilizes Z-buffering along with a normal
distribution model of depth and opacities. Through experiments we found that
this method is particularly effective for challenging scenarios like car damage
detection, where target objects (e.g., scratches, small dents) may only be
clearly visible in a single view, making multi-view consistency approaches
impractical or impossible. The code is publicly available at:
https://github.com/DragosChileban/CrashSplat.

</details>


### [193] [HunyuanImage 3.0 Technical Report](https://arxiv.org/abs/2509.23951)
*Siyu Cao,Hangting Chen,Peng Chen,Yiji Cheng,Yutao Cui,Xinchi Deng,Ying Dong,Kipper Gong,Tianpeng Gu,Xiusen Gu,Tiankai Hang,Duojun Huang,Jie Jiang,Zhengkai Jiang,Weijie Kong,Changlin Li,Donghao Li,Junzhe Li,Xin Li,Yang Li,Zhenxi Li,Zhimin Li,Jiaxin Lin,Linus,Lucaz Liu,Shu Liu,Songtao Liu,Yu Liu,Yuhong Liu,Yanxin Long,Fanbin Lu,Qinglin Lu,Yuyang Peng,Yuanbo Peng,Xiangwei Shen,Yixuan Shi,Jiale Tao,Yangyu Tao,Qi Tian,Pengfei Wan,Chunyu Wang,Kai Wang,Lei Wang,Linqing Wang,Lucas Wang,Qixun Wang,Weiyan Wang,Hao Wen,Bing Wu,Jianbing Wu,Yue Wu,Senhao Xie,Fang Yang,Miles Yang,Xiaofeng Yang,Xuan Yang,Zhantao Yang,Jingmiao Yu,Zheng Yuan,Chao Zhang,Jian-Wei Zhang,Peizhen Zhang,Shi-Xue Zhang,Tao Zhang,Weigang Zhang,Yepeng Zhang,Yingfang Zhang,Zihao Zhang,Zijian Zhang,Penghao Zhao,Zhiyuan Zhao,Xuefei Zhe,Jianchen Zhu,Zhao Zhong*

Main category: cs.CV

TL;DR: HunyuanImage 3.0 是一个统一多模态理解和生成的自回归框架模型，具备超过800亿参数的MoE结构，其中推理时激活130亿参数，是目前最大且最强大的开源图像生成模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一多模态理解与生成，并推动开源社区发展，构建动态且活跃的多模态生态系统。

Method: 采用精细的数据筛选、先进的架构设计、原生思维链（Chain-of-Thoughts）方案、渐进式预训练、激进的后训练以及高效的大规模训练与推理基础设施。

Result: 成功训练出包含800亿参数的MoE模型，在文本-图像对齐和视觉质量方面经过自动与人工评估，性能媲美先前的最先进模型。

Conclusion: HunyuanImage 3.0 在规模和性能上达到当前开源图像生成模型的领先水平，其代码与权重已公开，有助于促进多模态研究与应用的发展。

Abstract: We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available. The achievement of
HunyuanImage 3.0 relies on several key components, including meticulous data
curation, advanced architecture design, a native Chain-of-Thoughts schema,
progressive model pre-training, aggressive model post-training, and an
efficient infrastructure that enables large-scale training and inference. With
these advancements, we successfully trained a Mixture-of-Experts (MoE) model
comprising over 80 billion parameters in total, with 13 billion parameters
activated per token during inference, making it the largest and most powerful
open-source image generative model to date. We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem. All
open source assets are publicly available at
https://github.com/Tencent-Hunyuan/HunyuanImage-3.0

</details>


### [194] [ColLab: A Collaborative Spatial Progressive Data Engine for Referring Expression Comprehension and Generation](https://arxiv.org/abs/2509.23955)
*Shilan Zhang,Jirui Huang,Ruilin Yao,Cong Wang,Yaxiong Chen,Peng Xu,Shengwu Xiong*

Main category: cs.CV

TL;DR: 本文提出了ColLab，一种无需人工监督的全自动指代表达理解（REC）和生成（REG）数据生成引擎，通过协作多模态模型交互（CMMI）策略和空间渐进增强（SPA）模块，显著提升了数据生成速度与表达质量。


<details>
  <summary>Details</summary>
Motivation: 现有REC和REG数据集依赖耗时费力的人工标注，难以大规模扩展，因此需要一种自动化、可扩展的数据生成方法。

Method: 提出ColLab框架，结合多模态大语言模型（MLLMs）和大语言模型（LLMs）的语义理解能力，采用协作多模态模型交互（CMMI）策略生成描述，并设计空间渐进增强（SPA）模块以提升重复实例间的空间表达能力。

Result: 实验表明，ColLab显著加快了REC和REG的标注过程，同时提高了生成表达的质量和区分性；该框架已被部分应用于ICCV 2025 MARS2挑战赛的数据生成中，增强了数据集的多样性和挑战性。

Conclusion: ColLab实现了高效、高质量的REC和REG数据自动化生成，为多模态理解任务提供了可扩展的数据解决方案，并在实际竞赛数据构建中验证了其有效性。

Abstract: Referring Expression Comprehension (REC) and Referring Expression Generation
(REG) are fundamental tasks in multimodal understanding, supporting precise
object localization through natural language. However, existing REC and REG
datasets rely heavily on manual annotation, which is labor-intensive and
difficult to scale. In this paper, we propose ColLab, a collaborative spatial
progressive data engine that enables fully automated REC and REG data
generation without human supervision. Specifically, our method introduces a
Collaborative Multimodal Model Interaction (CMMI) strategy, which leverages the
semantic understanding of multimodal large language models (MLLMs) and large
language models (LLMs) to generate descriptions. Furthermore, we design a
module termed Spatial Progressive Augmentation (SPA) to enhance spatial
expressiveness among duplicate instances. Experiments demonstrate that ColLab
significantly accelerates the annotation process of REC and REG while improving
the quality and discriminability of the generated expressions. In addition to
the core methodological contribution, our framework was partially adopted in
the data generation pipeline of the ICCV 2025 MARS2 Challenge on Multimodal
Reasoning, enriching the dataset with diverse and challenging samples that
better reflect real-world reasoning demands.

</details>


### [195] [Reinforcement Learning with Inverse Rewards for World Model Post-training](https://arxiv.org/abs/2509.23958)
*Yang Ye,Tianyu He,Shuo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: 提出了一种名为RLIR的后训练框架，通过逆向动力学模型从生成视频中恢复输入动作，以提供可验证的奖励信号，从而提升视频世界模型的动作跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在准确建模人类指定动作方面的能力尚不充分，且传统的强化学习后训练方法因标注成本高和难以构建基于规则的视频验证器而不适用。

Method: 提出Reinforcement Learning with Inverse Rewards (RLIR)，利用逆向动力学模型将高维视频映射到低维动作空间，生成可验证的奖励信号，并结合Group Relative Policy Optimization进行优化。

Result: 在自回归和扩散模型上实验显示，动作跟随能力提升5-10%，视觉质量提高高达10%，并获得更高的人类偏好评分。

Conclusion: RLIR是首个专门用于增强视频世界模型动作跟随能力的后训练方法，有效解决了奖励信号不可靠和验证困难的问题。

Abstract: World models simulate dynamic environments, enabling agents to interact with
diverse input modalities. Although recent advances have improved the visual
quality and temporal consistency of video world models, their ability of
accurately modeling human-specified actions remains under-explored.
Reinforcement learning presents a promising approach for directly improving the
suboptimal action-following capability of pre-trained models, assuming that an
appropriate reward function can be defined. However, transferring reinforcement
learning post-training methods to world model is impractical due to the
prohibitive cost of large-scale preference annotations and the infeasibility of
constructing rule-based video verifiers. To address this gap, we propose
Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework
that derives verifiable reward signals by recovering input actions from
generated videos using an Inverse Dynamics Model. By mapping high-dimensional
video modality to a low-dimensional action space, RLIR provides an objective
and verifiable reward for optimization via Group Relative Policy Optimization.
Experiments across autoregressive and diffusion paradigms demonstrate 5-10%
gains in action-following, up to 10% improvements in visual quality, and higher
human preference scores, establishing RLIR as the first post-training method
specifically designed to enhance action-following in video world models.

</details>


### [196] [A Novel Hybrid Deep Learning and Chaotic Dynamics Approach for Thyroid Cancer Classification](https://arxiv.org/abs/2509.23968)
*Nada Bouchekout,Abdelkrim Boukabou,Morad Grimes,Yassine Habchi,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 提出一种结合自适应CNN与小波-混沌系统的甲状腺超声图像分类方法，在多个指标上达到SOTA性能，具有良好的泛化能力和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 为应对甲状腺癌发病率上升的问题，需提高超声诊断的准确性与及时性，现有方法在特征提取和判别能力上仍有提升空间。

Method: 将自适应卷积神经网络（CNN）与CDF9/7小波变换结合，并利用n-scroll混沌系统调制小波细节系数以增强判别性特征，构建wavelet-chaos-CNN分类 pipeline。

Result: 在DDTI数据集上实现98.17%准确率、98.76%敏感度、97.58%特异度、97.55% F1分数和0.9912 AUC；优于EfficientNetV2-S等主流模型；消融实验显示混沌调制带来+8.79%的准确率提升；跨数据集测试（TCIA和ISIC）表现出良好泛化能力；推理速度为28.7ms/图像，峰值显存1,125MB。

Conclusion: 该wavelet-chaos-CNN框架在甲状腺超声分类中表现优异，兼具高性能、强鲁棒性和低计算开销，具备临床部署潜力。

Abstract: Timely and accurate diagnosis is crucial in addressing the global rise in
thyroid cancer, ensuring effective treatment strategies and improved patient
outcomes. We present an intelligent classification method that couples an
Adaptive Convolutional Neural Network (CNN) with Cohen-Daubechies-Feauveau
(CDF9/7) wavelets whose detail coefficients are modulated by an n-scroll
chaotic system to enrich discriminative features. We evaluate on the public
DDTI thyroid ultrasound dataset (n = 1,638 images; 819 malignant / 819 benign)
using 5-fold cross-validation, where the proposed method attains 98.17%
accuracy, 98.76% sensitivity, 97.58% specificity, 97.55% F1-score, and an AUC
of 0.9912. A controlled ablation shows that adding chaotic modulation to CDF9/7
improves accuracy by +8.79 percentage points over a CDF9/7-only CNN (from
89.38% to 98.17%). To objectively position our approach, we trained
state-of-the-art backbones on the same data and splits: EfficientNetV2-S
(96.58% accuracy; AUC 0.987), Swin-T (96.41%; 0.986), ViT-B/16 (95.72%; 0.983),
and ConvNeXt-T (96.94%; 0.987). Our method outperforms the best of these by
+1.23 points in accuracy and +0.0042 in AUC, while remaining computationally
efficient (28.7 ms per image; 1,125 MB peak VRAM). Robustness is further
supported by cross-dataset testing on TCIA (accuracy 95.82%) and transfer to an
ISIC skin-lesion subset (n = 28 unique images, augmented to 2,048; accuracy
97.31%). Explainability analyses (Grad-CAM, SHAP, LIME) highlight clinically
relevant regions. Altogether, the wavelet-chaos-CNN pipeline delivers
state-of-the-art thyroid ultrasound classification with strong generalization
and practical runtime characteristics suitable for clinical integration.

</details>


### [197] [VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic Diffusion](https://arxiv.org/abs/2509.23971)
*Kargi Chauhan,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 提出Validity-First Spatial Intelligence (VFSI) 方法，通过能量引导在扩散采样过程中强制执行物理约束，显著提升交通仿真中的物理有效性，同时改善生成轨迹的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成的交通轨迹常违反基本物理规律（如碰撞、驶出道路等），缺乏对物理有效性的保证，需将物理约束作为架构设计的必要条件。

Method: 引入基于能量的引导机制，在扩散模型采样过程中融入碰撞避免和运动学约束作为能量函数，无需重新训练模型即可引导去噪过程生成符合物理规律的轨迹。

Result: 在Waymo Open Motion Dataset的200个城市场景中，VFSI将碰撞率从24.6%降至8.1%（下降67%），整体物理有效性从50.3%提升至94.2%（提升87%），同时ADE从1.34m改善到1.21m，提升了生成质量。

Conclusion: 推理阶段显式施加物理约束是实现高物理有效性和真实感交通仿真的必要且充分条件，VFSI作为一种模型无关方法，为扩散模型在安全关键型应用中的部署提供了可靠解决方案。

Abstract: Modern diffusion models generate realistic traffic simulations but
systematically violate physical constraints. In a large-scale evaluation of
SceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of
generated trajectories violate basic physical laws - vehicles collide, drive
off roads, and spawn inside buildings. This reveals a fundamental limitation:
current models treat physical validity as an emergent property rather than an
architectural requirement. We propose Validity-First Spatial Intelligence
(VFSI), which enforces constraints through energy-based guidance during
diffusion sampling, without model retraining. By incorporating collision
avoidance and kinematic constraints as energy functions, we guide the denoising
process toward physically valid trajectories. Across 200 urban scenarios from
the Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to
8.1%) and improves overall validity by 87% (50.3% to 94.2%), while
simultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our
model-agnostic approach demonstrates that explicit constraint enforcement
during inference is both necessary and sufficient for physically valid traffic
simulation.

</details>


### [198] [Towards Redundancy Reduction in Diffusion Models for Efficient Video Super-Resolution](https://arxiv.org/abs/2509.23980)
*Jinpei Guo,Yifei Ji,Zheng Chen,Yufei Wang,Sizhuo Ma,Yong Guo,Yulun Zhang,Jian Wang*

Main category: cs.CV

TL;DR: 提出了一种名为OASIS的一次性扩散模型，用于真实世界视频超分辨率，通过注意力专业化路由和渐进式训练策略实现了高效且高性能的视频超分辨率。


<details>
  <summary>Details</summary>
Motivation: 直接将生成式扩散模型应用于视频超分辨率会导致冗余，因为低质量视频已包含大量内容信息，这增加了计算开销和学习负担。

Method: 提出OASIS模型，采用注意力专业化路由机制，根据不同模式的内在行为分配注意力头，并结合渐进式训练策略，从时间一致的退化逐步过渡到不一致的退化设置。

Result: 在合成和真实世界数据集上均达到最先进的性能，相比SeedVR2等一次性扩散基线方法实现6.2倍的推理速度提升。

Conclusion: OASIS通过减少冗余操作和有效利用预训练知识，显著提升了视频超分辨率的效率和性能，适用于复杂退化场景。

Abstract: Diffusion models have recently shown promising results for video
super-resolution (VSR). However, directly adapting generative diffusion models
to VSR can result in redundancy, since low-quality videos already preserve
substantial content information. Such redundancy leads to increased
computational overhead and learning burden, as the model performs superfluous
operations and must learn to filter out irrelevant information. To address this
problem, we propose OASIS, an efficient $\textbf{o}$ne-step diffusion model
with $\textbf{a}$ttention $\textbf{s}$pecialization for real-world
v$\textbf{i}$deo $\textbf{s}$uper-resolution. OASIS incorporates an attention
specialization routing that assigns attention heads to different patterns
according to their intrinsic behaviors. This routing mitigates redundancy while
effectively preserving pretrained knowledge, allowing diffusion models to
better adapt to VSR and achieve stronger performance. Moreover, we propose a
simple yet effective progressive training strategy, which starts with
temporally consistent degradations and then shifts to inconsistent settings.
This strategy facilitates learning under complex degradations. Extensive
experiments demonstrate that OASIS achieves state-of-the-art performance on
both synthetic and real-world datasets. OASIS also provides superior inference
speed, offering a $\textbf{6.2$\times$}$ speedup over one-step diffusion
baselines such as SeedVR2. The code will be available at
\href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.

</details>


### [199] [RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization](https://arxiv.org/abs/2509.23991)
*Dongki Jung,Jaehoon Choi,Yonghan Lee,Dinesh Manocha*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的鲁棒360度单目深度估计方法RPG360，利用透视基础模型和图优化技术，通过六面立方体贴图表示和深度尺度对齐，实现了跨数据集的优异性能，并在下游任务中展现出有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模标注数据集，360度图像的深度估计面临挑战，现有方法依赖于大量训练数据，难以泛化。

Method: 将360度图像转换为六面立方体贴图，使用透视基础模型估计每面的深度和法线，并引入基于图优化的深度尺度对齐方法，通过每面的尺度参数实现跨面深度一致性。

Result: 在Matterport3D、Stanford2D3D和360Loc等多个数据集上表现出优越的零样性能，并在特征匹配（提升3.2~5.4%）和运动结构恢复（AUC@5提升0.2~9.7%）等下游任务中验证了其有效性。

Conclusion: RPG360是一种无需训练且具有强泛化能力的360度深度估计方法，通过结合基础模型与图优化，有效解决了立方体贴图中的深度尺度不一致问题，适用于多种应用场景。

Abstract: The increasing use of 360 images across various domains has emphasized the
need for robust depth estimation techniques tailored for omnidirectional
images. However, obtaining large-scale labeled datasets for 360 depth
estimation remains a significant challenge. In this paper, we propose RPG360, a
training-free robust 360 monocular depth estimation method that leverages
perspective foundation models and graph optimization. Our approach converts 360
images into six-face cubemap representations, where a perspective foundation
model is employed to estimate depth and surface normals. To address depth scale
inconsistencies across different faces of the cubemap, we introduce a novel
depth scale alignment technique using graph-based optimization, which
parameterizes the predicted depth and normal maps while incorporating an
additional per-face scale parameter. This optimization ensures depth scale
consistency across the six-face cubemap while preserving 3D structural
integrity. Furthermore, as foundation models exhibit inherent robustness in
zero-shot settings, our method achieves superior performance across diverse
datasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate
the versatility of our depth estimation approach by validating its benefits in
downstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion
0.2 ~ 9.7% in AUC@5.

</details>


### [200] [TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute Coronary Syndrome Treatment Prediction](https://arxiv.org/abs/2509.23999)
*Diane Kim,Minh Nguyen Nhat To,Sherif Abdalla,Teresa S. M. Tsang,Purang Abolmaesumi,and Christina Luong*

Main category: cs.CV

TL;DR: TREAT-Net是一个基于超声心动图视频和临床记录的多模态深度学习模型，用于非侵入性预测急性冠脉综合征（ACS）治疗，准确率优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉造影作为ACS诊断金标准具有侵入性和资源密集型缺点，可能导致诊断延迟和治疗推迟，亟需一种非侵入、高效的替代方案。

Method: 提出TREAT-Net，采用表格引导的交叉注意力机制增强视频理解，并通过晚期融合整合多模态信息，利用超过9000例ACS病例数据进行训练。

Result: 模型在平衡准确率上达到67.6%，AUROC为71.1%，跨模态一致性分析显示干预预测准确率达88.6%。

Conclusion: TREAT-Net有望成为一种有效的非侵入性工具，用于ACS患者的及时分诊，尤其适用于缺乏冠状动脉造影资源的医疗欠发达地区。

Abstract: Coronary angiography remains the gold standard for diagnosing Acute Coronary
Syndrome (ACS). However, its resource-intensive and invasive nature can expose
patients to procedural risks and diagnostic delays, leading to postponed
treatment initiation. In this work, we introduce TREAT-Net, a multimodal deep
learning framework for ACS treatment prediction that leverages non-invasive
modalities, including echocardiography videos and structured clinical records.
TREAT-Net integrates tabular-guided cross-attention to enhance video
interpretation, along with a late fusion mechanism to align predictions across
modalities. Trained on a dataset of over 9000 ACS cases, the model outperforms
unimodal and non-fused baselines, achieving a balanced accuracy of 67.6% and an
AUROC of 71.1%. Cross-modality agreement analysis demonstrates 88.6% accuracy
for intervention prediction. These findings highlight the potential of
TREAT-Net as a non-invasive tool for timely and accurate patient triage,
particularly in underserved populations with limited access to coronary
angiography.

</details>


### [201] [SIE3D: Single-image Expressive 3D Avatar generation via Semantic Embedding and Perceptual Expression Loss](https://arxiv.org/abs/2509.24004)
*Zhiqi Huang,Dulongkai Cui,Jinglu Hu*

Main category: cs.CV

TL;DR: 本文提出SIE3D，一种从单张图像和描述性文本生成高保真、可控制表情的3D头像的新框架，通过新颖的条件机制和感知表情损失函数，在单个消费级GPU上实现了优于现有方法的表情准确性和身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏通过文本对3D头像表情进行细粒度、直观控制的能力，且难以保证生成表情与文本描述的一致性。

Method: SIE3D融合图像中的身份特征与文本的语义嵌入，采用新颖的条件机制实现细粒度控制，并引入基于预训练表情分类器的感知表情损失函数来正则化生成过程，确保表情准确性。

Result: 实验表明，SIE3D在表情可控性、身份保持和表情保真度方面显著优于竞争方法，且可在单个消费级GPU上高效运行。

Conclusion: SIE3D有效实现了文本驱动的精细表情控制与高保真3D头像生成，为个性化虚拟形象创建提供了实用且高效的解决方案。

Abstract: Generating high-fidelity 3D head avatars from a single image is challenging,
as current methods lack fine-grained, intuitive control over expressions via
text. This paper proposes SIE3D, a framework that generates expressive 3D
avatars from a single image and descriptive text. SIE3D fuses identity features
from the image with semantic embedding from text through a novel conditioning
scheme, enabling detailed control. To ensure generated expressions accurately
match the text, it introduces an innovative perceptual expression loss
function. This loss uses a pre-trained expression classifier to regularize the
generation process, guaranteeing expression accuracy. Extensive experiments
show SIE3D significantly improves controllability and realism, outperforming
competitive methods in identity preservation and expression fidelity on a
single consumer-grade GPU. Project page:
https://blazingcrystal1747.github.io/SIE3D/

</details>


### [202] [FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.24008)
*Haonan Ge,Yiwei Wang,Kai-Wei Chang,Hang Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了FrameMind，一种基于强化学习的端到端框架，通过动态帧采样和文本推理与视觉感知交替进行的FiCOT机制，提升视频理解模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型采用固定的帧采样策略，无法根据具体问题的推理需求自适应地获取视觉信息，导致在需要广泛时间覆盖或精细空间细节的任务中表现不佳。

Method: 提出FrameMind框架，结合Frame-Interleaved Chain-of-Thought (FiCOT) 机制，使模型在多轮交互中交替进行文本推理和主动视觉感知，并通过工具提取关键帧或视频片段；引入Dynamic Resolution Frame Sampling (DRFS) 和 DRFS-GRPO 算法，以增强模型对时空权衡的学习能力，并通过结果奖励训练采样策略，无需帧级标注。

Result: 在MLVU和VideoMME等具有挑战性的基准上，FrameMind显著优于现有模型，实现了更灵活、高效的视频理解。

Conclusion: FrameMind通过动态请求视觉信息和强化学习策略，实现了更智能的视频理解，推动了该领域的技术进步。

Abstract: Current video understanding models rely on fixed frame sampling strategies,
processing predetermined visual inputs regardless of the specific reasoning
requirements of each question. This static approach limits their ability to
adaptively gather visual evidence, leading to suboptimal performance on tasks
that require either broad temporal coverage or fine-grained spatial detail. In
this paper, we introduce FrameMind, an end-to-end framework trained with
reinforcement learning that enables models to dynamically request visual
information during reasoning through Frame-Interleaved Chain-of-Thought
(FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns
where the model alternates between textual reasoning and active visual
perception, using tools to extract targeted frames or video clips based on
identified knowledge gaps. To train effective dynamic sampling policies, we
propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to
diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a
group-relative policy optimization algorithm that learns from outcome-based
rewards without requiring frame-level annotations. Extensive experiments on
challenging benchmarks like MLVU and VideoMME demonstrate that our method
significantly outperforms existing models, advancing the state of the art in
flexible and efficient video understanding.

</details>


### [203] [Generalized Category Discovery in Hyperspectral Images via Prototype Subspace Modeling](https://arxiv.org/abs/2509.24017)
*Xianlu Li,Nicolas Nadisic,Shaoguang Huang,Aleksandra Pizurica*

Main category: cs.CV

TL;DR: 本文提出了首个针对高光谱图像的广义类别发现（GCD）框架，通过原型子空间建模更好地捕捉类别结构，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法主要针对RGB图像，难以有效处理高光谱图像的高维性和复杂光谱结构。

Method: 提出基于子空间的原型建模方法，用一组基向量表示每个类别，并引入基正交性和重构性两个约束来指导学习。

Result: 在真实高光谱数据上实验表明，该方法显著优于当前最先进的GCD方法。

Conclusion: 所提出的子空间建模范式为高光谱图像中的广义类别发现提供了有效且强大的解决方案。

Abstract: Generalized category discovery~(GCD) seeks to jointly identify both known and
novel categories in unlabeled data. While prior works have mainly focused on
RGB images, their assumptions and modeling strategies do not generalize well to
hyperspectral images~(HSI), which are inherently high-dimensional and exhibit
complex spectral structures. In this paper, we propose the first GCD framework
tailored for HSI, introducing a prototype subspace modeling model to better
capture class structure. Instead of learning a single prototype vector for each
category as in existing methods such as SimGCD, we model each category using a
set of basis vectors, forming a subspace representation that enables greater
expressiveness and discrimination in a high-dimensional feature space. To guide
the learning of such bases, we enforce two key constraints: (1) a basis
orthogonality constraint that promotes inter-class separability, and (2) a
reconstruction constraint that ensures each prototype basis can effectively
reconstruct its corresponding class samples. Experimental results on real-world
HSI demonstrate that our method significantly outperforms state-of-the-art GCD
methods, establishing a strong foundation for generalized category discovery in
hyperspectral settings.

</details>


### [204] [Hazy Pedestrian Trajectory Prediction via Physical Priors and Graph-Mamba](https://arxiv.org/abs/2509.24020)
*Jian Chen,Zhuoran Zheng,Han Hu,Guijuan Zhang,Dianjie Lu,Liang Li,Chen Lyu*

Main category: cs.CV

TL;DR: 提出了一种结合大气散射物理先验与行人关系拓扑建模的深度学习模型，用于雾天下的行人轨迹预测，显著提升了预测精度与推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决雾天条件下行人轨迹预测中物理信息退化和行人交互建模无效的问题。

Method: 构建可微的大气散射模型以解耦雾霾浓度与光照退化；设计自适应扫描状态空间模型（改进Mamba）进行特征提取；采用异构图注意力网络建模多粒度行人交互，并结合时空融合模块捕捉运动协同演化模式。

Result: 在浓雾场景下（能见度<30m），相比SOTA模型minADE和minFDE分别降低37.2%和41.5%；推理速度较原Mamba提升78%，同时保持长程依赖建模能力。

Conclusion: 该方法通过融合物理先验与拓扑建模，为恶劣环境下的智能交通系统提供了可靠的感知新范式。

Abstract: To address the issues of physical information degradation and ineffective
pedestrian interaction modeling in pedestrian trajectory prediction under hazy
weather conditions, we propose a deep learning model that combines physical
priors of atmospheric scattering with topological modeling of pedestrian
relationships. Specifically, we first construct a differentiable atmospheric
scattering model that decouples haze concentration from light degradation
through a network with physical parameter estimation, enabling the learning of
haze-mitigated feature representations. Second, we design an adaptive scanning
state space model for feature extraction. Our adaptive Mamba variant achieves a
78% inference speed increase over native Mamba while preserving long-range
dependency modeling.
  Finally, to efficiently model pedestrian relationships, we develop a
heterogeneous graph attention network, using graph matrices to model
multi-granularity interactions between pedestrians and groups, combined with a
spatio-temporal fusion module to capture the collaborative evolution patterns
of pedestrian movements. Furthermore, we constructed a new pedestrian
trajectory prediction dataset based on ETH/UCY to evaluate the effectiveness of
the proposed method. Experiments show that our method reduces the minADE /
minFDE metrics by 37.2% and 41.5%, respectively, compared to the SOTA models in
dense haze scenarios (visibility < 30m), providing a new modeling paradigm for
reliable perception in intelligent transportation systems in adverse
environments.

</details>


### [205] [$\mathbf{R}^3$: Reconstruction, Raw, and Rain: Deraining Directly in the Bayer Domain](https://arxiv.org/abs/2509.24022)
*Nate Rothschild,Moshe Kimhi,Avi Mendelson,Chaim Baskin*

Main category: cs.CV

TL;DR: 本文提出在原始Bayer域进行图像去雨重建优于传统的sRGB域方法，通过构建首个公开的真实雨天场景Bayer与sRGB配对数据集Raw-Rain，并引入颜色不变的评价指标ICS，验证了ISP-last范式在低层视觉任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 传统图像重建网络多在经过ISP处理的sRGB图像上训练，但ISP过程会导致颜色混合、动态范围压缩和细节模糊等不可逆损失。本文旨在探索在原始Bayer数据上直接学习是否可避免这些损失，从而提升重建质量。

Method: 以去雨任务为用例，比较sRGB域与Bayer域的重建性能；构建包含真实雨天场景的Raw-Rain数据集（同时提供12位Bayer和位深匹配的sRGB图像）；提出信息守恒评分（ICS），一种更贴近人类感知的颜色不变性评估指标；设计适用于Bayer域的重建模型。

Result: 在测试集上，基于Bayer域的模型相比sRGB域方法PSNR最高提升+0.99 dB，ICS提升+1.2%，且运行速度更快，计算量仅为一半（GFLOPs减半）。

Conclusion: 在原始Bayer域进行图像重建能有效避免ISP带来的信息损失，实现更优的重建效果，支持“ISP-last”范式，为端到端可学习相机成像管线提供了可能。

Abstract: Image reconstruction from corrupted images is crucial across many domains.
Most reconstruction networks are trained on post-ISP sRGB images, even though
the image-signal-processing pipeline irreversibly mixes colors, clips dynamic
range, and blurs fine detail. This paper uses the rain degradation problem as a
use case to show that these losses are avoidable, and demonstrates that
learning directly on raw Bayer mosaics yields superior reconstructions. To
substantiate the claim, we (i) evaluate post-ISP and Bayer reconstruction
pipelines, (ii) curate Raw-Rain, the first public benchmark of real rainy
scenes captured in both 12-bit Bayer and bit-depth-matched sRGB, and (iii)
introduce Information Conservation Score (ICS), a color-invariant metric that
aligns more closely with human opinion than PSNR or SSIM. On the test split,
our raw-domain model improves sRGB results by up to +0.99 dB PSNR and +1.2%
ICS, while running faster with half of the GFLOPs. The results advocate an
ISP-last paradigm for low-level vision and open the door to end-to-end
learnable camera pipelines.

</details>


### [206] [Joint Superpixel and Self-Representation Learning for Scalable Hyperspectral Image Clustering](https://arxiv.org/abs/2509.24027)
*Xianlu Li,Nicolas Nadisic,Shaoguang Huang,Aleksandra Pizurica*

Main category: cs.CV

TL;DR: 提出了一种端到端的联合优化超像素分割与子空间聚类的框架，通过反馈机制实现聚类感知的分割，显著提升高光谱图像分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有超像素方法独立于聚类任务进行分割，导致分割结果与聚类目标不一致，限制了子空间聚类在高光谱图像中的可扩展性。

Method: 设计了一个基于展开ADMM的自表示网络作为模型驱动信号，通过反馈机制指导可微超像素模块，实现超像素分割与子空间聚类的联合优化，并引入每个超像素独立的紧凑性参数学习。

Result: 在多个高光谱图像基准数据集上实验表明，该方法在聚类精度上优于现有最先进方法，同时提高了计算效率和分割质量。

Conclusion: 所提出的联合优化框架能够生成兼顾光谱和空间结构的聚类感知超像素，有效提升高光谱图像无监督分析的性能。

Abstract: Subspace clustering is a powerful unsupervised approach for hyperspectral
image (HSI) analysis, but its high computational and memory costs limit
scalability. Superpixel segmentation can improve efficiency by reducing the
number of data points to process. However, existing superpixel-based methods
usually perform segmentation independently of the clustering task, often
producing partitions that do not align with the subsequent clustering
objective. To address this, we propose a unified end-to-end framework that
jointly optimizes superpixel segmentation and subspace clustering. Its core is
a feedback mechanism: a self-representation network based on unfolded
Alternating Direction Method of Multipliers (ADMM) provides a model-driven
signal to guide a differentiable superpixel module. This joint optimization
yields clustering-aware partitions that preserve both spectral and spatial
structure. Furthermore, our superpixel network learns a unique compactness
parameter for each superpixel, enabling more flexible and adaptive
segmentation. Extensive experiments on benchmark HSI datasets demonstrate that
our method consistently achieves superior accuracy compared with
state-of-the-art clustering approaches.

</details>


### [207] [A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer](https://arxiv.org/abs/2509.24066)
*Leonardo Iurada,Beatrice Occhiena,Tatiana Tommasi*

Main category: cs.CV

TL;DR: 本文研究了在预训练视觉模型上初始化时剪枝的方法，发现即使在没有任务特定数据的情况下，剪枝也能保持模型在未见任务上的零样本性能，并且微调后可恢复被保留任务的性能，这归因于大规模预训练带来的有利损失景观。


<details>
  <summary>Details</summary>
Motivation: 由于预训练视觉模型的计算和存储成本较高，限制了其实际部署，而传统的剪枝方法需要任务特定数据，当下游任务未知时带来了挑战。因此，本文旨在探索在无任务特定数据情况下如何有效剪枝预训练模型。

Method: 通过在不同任务上进行初始化剪枝实验，分析剪枝后模型在未见任务上的零样本性能以及微调后的表现，探究数据对剪枝效果的影响，并结合损失景观分析其背后原因。

Result: 实验表明，在一个任务上剪枝能保持模型在其他未见任务上的零样本性能；进一步微调不仅能提升原任务性能，还能恢复被保留任务的性能。这一现象得益于大规模预训练形成的平滑且鲁棒的损失景观。

Conclusion: 预训练模型的广泛迁移能力使其在初始化剪枝时无需依赖特定任务数据，即可实现跨任务的有效压缩与适应，为下游任务未知场景下的模型压缩提供了可行方案。

Abstract: The widespread availability of pre-trained vision models has enabled numerous
deep learning applications through their transferable representations. However,
their computational and storage costs often limit practical deployment.
Pruning-at-Initialization has emerged as a promising approach to compress
models before training, enabling efficient task-specific adaptation. While
conventional wisdom suggests that effective pruning requires task-specific
data, this creates a challenge when downstream tasks are unknown in advance. In
this paper, we investigate how data influences the pruning of pre-trained
vision models. Surprisingly, pruning on one task retains the model's zero-shot
performance also on unseen tasks. Furthermore, fine-tuning these pruned models
not only improves performance on original seen tasks but can recover held-out
tasks' performance. We attribute this phenomenon to the favorable loss
landscapes induced by extensive pre-training on large-scale datasets.

</details>


### [208] [Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding](https://arxiv.org/abs/2509.24072)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文提出了“Grounding IDs”的概念，解释了外部视觉线索如何通过隐含标识符增强多模态模型中图像与文本的对齐，提升跨模态定位并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在多模态任务上表现良好，但在结构化推理和精确对齐方面存在局限。研究旨在理解外部视觉结构（如分区和标注）提升性能的内在机制。

Method: 通过表征分析识别由外部线索诱导的“Grounding IDs”，并在嵌入空间中检验其对齐特性；结合因果干预实验验证这些标识符在对象与符号线索绑定中的中介作用。

Result: 发现Grounding IDs在嵌入空间中形成强健的组内对齐，缩小图文模态差距；因果干预证实其在绑定中的关键作用；该机制增强了相关组件间的注意力，改善了跨模态对齐并减少了幻觉现象。

Conclusion: Grounding IDs是一种关键的符号机制，解释了外部线索如何提升多模态模型的绑定能力，为模型提供了可解释性，并带来了鲁棒性的实际提升。

Abstract: Large vision-language models (LVLMs) show strong performance across
multimodal benchmarks but remain limited in structured reasoning and precise
grounding. Recent work has demonstrated that adding simple visual structures,
such as partitions and annotations, improves accuracy, yet the internal
mechanisms underlying these gains remain unclear. We investigate this
phenomenon and propose the concept of Grounding IDs, latent identifiers induced
by external cues that bind objects to their designated partitions across
modalities. Through representation analysis, we find that these identifiers
emerge as robust within-partition alignment in embedding space and reduce the
modality gap between image and text. Causal interventions further confirm that
these identifiers mediate binding between objects and symbolic cues. We show
that Grounding IDs strengthen attention between related components, which in
turn improves cross-modal grounding and reduces hallucinations. Taken together,
our results identify Grounding IDs as a key symbolic mechanism explaining how
external cues enhance multimodal binding, offering both interpretability and
practical improvements in robustness.

</details>


### [209] [Autoregressive Video Generation beyond Next Frames Prediction](https://arxiv.org/abs/2509.24081)
*Sucheng Ren,Chen Chen,Zhenbang Wang,Liangchen Song,Xiangxin Zhu,Alan Yuille,Yinfei Yang,Jiasen Lu*

Main category: cs.CV

TL;DR: 提出VideoAR框架，使用时空立方体作为预测单元，优于传统的逐帧视频生成方法。


<details>
  <summary>Details</summary>
Motivation: 质疑视频生成中以帧为基本预测单位的合理性，探索更优的预测单元。

Method: 设计支持多种预测单元（如全帧、关键细节帧、多尺度细化和时空立方体）的统一框架VideoAR，并采用时空立方体进行自回归建模。

Result: 基于立方体的预测在生成质量、速度和时间连贯性上均优于现有方法，突破帧间约束，实现更快推理和长达分钟级视频序列的生成。

Conclusion: 时空立方体是更优的视频生成预测单位，该工作推动对视频等时空领域序列分解方式的重新思考。

Abstract: Autoregressive models for video generation typically operate frame-by-frame,
extending next-token prediction from language to video's temporal dimension. We
question that unlike word as token is universally agreed in language if frame
is a appropriate prediction unit? To address this, we present VideoAR, a
unified framework that supports a spectrum of prediction units including full
frames, key-detail frames, multiscale refinements, and spatiotemporal cubes.
Among these designs, we find model video generation using
\textit{spatiotemporal} cubes as prediction units, which allows autoregressive
models to operate across both spatial and temporal dimensions simultaneously.
This approach eliminates the assumption that frames are the natural atomic
units for video autoregression. We evaluate VideoAR across diverse prediction
strategies, finding that cube-based prediction consistently delivers superior
quality, speed, and temporal coherence. By removing the frame-by-frame
constraint, our video generator surpasses state-of-the-art baselines on VBench
while achieving faster inference and enabling seamless scaling to minute-long
sequences. We hope this work will motivate rethinking sequence decomposition in
video and other spatiotemporal domains.

</details>


### [210] [Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow](https://arxiv.org/abs/2509.24099)
*Prerit Gupta,Shourya Verma,Ananth Grama,Aniket Bera*

Main category: cs.CV

TL;DR: DualFlow是一个用于多模态双人动作生成的统一高效框架，能够基于文本、音乐和先验动作序列生成3D动作，采用修正流实现快速推理，并通过检索增强生成模块提升语义对齐和互动协调性。


<details>
  <summary>Details</summary>
Motivation: 生成逼真且上下文感知的双人动作在计算机图形学和人机交互中仍具挑战，现有方法在多模态条件下的生成质量、同步性和效率方面存在局限。

Method: 提出DualFlow框架，利用修正流实现从噪声到数据的确定性直线采样路径；引入检索增强生成（RAG）模块，结合音乐特征和基于大语言模型的文本分解检索动作范例；设计对比学习目标和同步损失函数以增强条件对齐和双人协作。

Result: 在文本到动作、音乐到动作及多模态交互任务上表现优异，显著提升动作质量、响应速度和时间连贯性，实现节奏同步，推理时间短且误差累积少。

Conclusion: DualFlow在多模态人体动作生成任务中达到最先进水平，具备高效、高质量和强语义对齐的优势，适用于复杂双人互动动作合成。

Abstract: Generating realistic, context-aware two-person motion conditioned on diverse
modalities remains a central challenge in computer graphics, animation, and
human-computer interaction. We introduce DualFlow, a unified and efficient
framework for multi-modal two-person motion generation. DualFlow conditions 3D
motion synthesis on diverse inputs, including text, music, and prior motion
sequences. Leveraging rectified flow, it achieves deterministic straight-line
sampling paths between noise and data, reducing inference time and mitigating
error accumulation common in diffusion-based models. To enhance semantic
grounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that
retrieves motion exemplars using music features and LLM-based text
decompositions of spatial relations, body movements, and rhythmic patterns. We
use contrastive objective that further strengthens alignment with conditioning
signals and introduce synchronization loss that improves inter-person
coordination. Extensive evaluations across text-to-motion, music-to-motion, and
multi-modal interactive benchmarks show consistent gains in motion quality,
responsiveness, and efficiency. DualFlow produces temporally coherent and
rhythmically synchronized motions, setting state-of-the-art in multi-modal
human motion generation.

</details>


### [211] [SVAC: Scaling Is All You Need For Referring Video Object Segmentation](https://arxiv.org/abs/2509.24109)
*Li Zhang,Haoxiang Gao,Zhihao Zhang,Luoxiao Huang,Tao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SVAC的统一模型，通过增加输入帧和分割标记的数量来提升指代表视频对象分割（RVOS）性能，并引入基于锚点的时空压缩模块（ASTC）和片段特定分配策略（CSA）以解决计算开销和复杂时序动态问题，在多个基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RVOS方法在利用多模态大语言模型（MLLMs）先验知识、处理长时间视频的计算成本以及时序动态建模方面仍存在不足。

Method: 提出SVAC模型，采用Anchor-Based Spatio-Temporal Compression（ASTC）模块压缩视觉标记并保留关键时空结构，结合Clip-Specific Allocation（CSA）策略优化不同视频片段中动态对象的处理。

Result: SVAC在多个RVOS基准上达到最先进的性能，同时具备较高的计算效率。

Conclusion: SVAC通过增强视频-语言交互与高效压缩机制，有效提升了RVOS的精度与实用性，为后续研究提供了可扩展且高效的框架。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment target objects in
video sequences based on natural language descriptions. While recent advances
in Multi-modal Large Language Models (MLLMs) have improved RVOS performance
through enhanced text-video understanding, several challenges remain, including
insufficient exploitation of MLLMs' prior knowledge, prohibitive computational
and memory costs for long-duration videos, and inadequate handling of complex
temporal dynamics. In this work, we propose SVAC, a unified model that improves
RVOS by scaling up input frames and segmentation tokens to enhance
video-language interaction and segmentation precision. To address the resulting
computational challenges, SVAC incorporates the Anchor-Based Spatio-Temporal
Compression (ASTC) module to compress visual tokens while preserving essential
spatio-temporal structure. Moreover, the Clip-Specific Allocation (CSA)
strategy is introduced to better handle dynamic object behaviors across video
clips. Experimental results demonstrate that SVAC achieves state-of-the-art
performance on multiple RVOS benchmarks with competitive efficiency. Our code
is available at https://github.com/lizhang1998/SVAC.

</details>


### [212] [GANji: A Framework for Introductory AI Image Generation](https://arxiv.org/abs/2509.24128)
*Chandon Hamel,Mike Busch*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级框架GANji，用于基准测试基础AI图像生成技术，使用10,314个日本汉字字符的数据集系统比较了变分自编码器（VAE）、生成对抗网络（GAN）和去噪扩散概率模型（DDPM）的性能。结果显示，尽管DDPM在图像保真度方面表现最佳（FID得分为26.2），但其采样时间比其他模型慢2000多倍。GANji框架有效地揭示了模型架构、计算成本和视觉质量之间的基本权衡，非常适合教育和研究目的。


<details>
  <summary>Details</summary>
Motivation: 生成模型的比较研究通常需要大量的计算资源，这为研究人员和实践者设置了障碍。因此，本文旨在开发一个轻量级的框架来降低这一门槛，使更多人能够进行生成模型的基准测试。

Method: 本文引入了GANji框架，并使用包含10,314个日本汉字字符的数据集，系统地比较了三种生成模型：变分自编码器（VAE）、生成对抗网络（GAN）和去噪扩散概率模型（DDPM）。评估指标包括Fréchet Inception Distance (FID) 和采样时间。

Result: 实验结果表明，DDPM在图像保真度方面表现最好，FID得分为26.2，但其采样速度比VAE和GAN慢超过2000倍。相比之下，VAE和GAN在保持合理图像质量的同时具有显著更快的采样速度。

Conclusion: GANji是一个有效且易于访问的工具，能够揭示不同生成模型在架构、计算成本和视觉质量之间的根本权衡，适用于教育和研究场景。

Abstract: The comparative study of generative models often requires significant
computational resources, creating a barrier for researchers and practitioners.
This paper introduces GANji, a lightweight framework for benchmarking
foundational AI image generation techniques using a dataset of 10,314 Japanese
Kanji characters. It systematically compares the performance of a Variational
Autoencoder (VAE), a Generative Adversarial Network (GAN), and a Denoising
Diffusion Probabilistic Model (DDPM). The results demonstrate that while the
DDPM achieves the highest image fidelity, with a Fr\'echet Inception Distance
(FID) score of 26.2, its sampling time is over 2,000 times slower than the
other models. The GANji framework is an effective and accessible tool for
revealing the fundamental trade-offs between model architecture, computational
cost, and visual quality, making it ideal for both educational and research
purposes.

</details>


### [213] [Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding](https://arxiv.org/abs/2509.24133)
*Zhecheng Li,Guoxian Song,Yiwei Wang,Zhen Xiong,Junsong Yuan,Yujun Cai*

Main category: cs.CV

TL;DR: 提出GMS框架，结合通用视觉语言模型和特定任务模型，通过粗到细的协同机制显著提升GUI grounding性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI grounding方法在跨应用和系统时表现不佳，难以同时准确理解多样化的UI元素并预测操作的空间坐标。

Method: 设计了一个五阶段的协同框架GMS，其中通用视觉语言模型作为'Scanner'定位感兴趣区域，专用接地模型作为'Locator'在区域内精确输出坐标，并引入分层搜索与跨模态通信机制。

Result: 在ScreenSpot-Pro数据集上，独立的'Scanner'和'Locator'准确率分别为2.0%和3.7%，而GMS框架整体准确率达到35.7%，性能提升超过10倍，且优于多种强基线方法。

Conclusion: GMS通过模拟人类视觉注意机制，有效融合通用与专用模型的优势，在GUI grounding任务中展现出卓越的性能和鲁棒性，具有广泛的应用潜力。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs)
presents a challenging task that requires models to comprehend diverse UI
elements across various applications and systems, while also accurately
predicting the spatial coordinates for the intended operation. To tackle this
problem, we propose GMS: Generalist Scanner Meets Specialist Locator, a
synergistic coarse-to-fine framework that effectively improves GUI grounding
performance. GMS leverages the complementary strengths of general
vision-language models (VLMs) and small, task-specific GUI grounding models by
assigning them distinct roles within the framework. Specifically, the general
VLM acts as a 'Scanner' to identify potential regions of interest, while the
fine-tuned grounding model serves as a 'Locator' that outputs precise
coordinates within these regions. This design is inspired by how humans perform
GUI grounding, where the eyes scan the interface and the brain focuses on
interpretation and localization. Our whole framework consists of five stages
and incorporates hierarchical search with cross-modal communication to achieve
promising prediction results. Experimental results on the ScreenSpot-Pro
dataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\%$
and $3.7\%$ accuracy respectively when used independently, their integration
within GMS framework yields an overall accuracy of $35.7\%$, representing a $10
\times$ improvement. Additionally, GMS significantly outperforms other strong
baselines under various settings, demonstrating its robustness and potential
for general-purpose GUI grounding.

</details>


### [214] [EYE-DEX: Eye Disease Detection and EXplanation System](https://arxiv.org/abs/2509.24136)
*Youssef Sabiri,Walid Houmaidi,Amine Abouaomar*

Main category: cs.CV

TL;DR: 本研究提出了一种名为EYE-DEX的自动化框架，用于分类10种视网膜疾病，基于包含21,577张眼底图像的大规模数据集，采用微调的VGG16模型实现了92.36%的准确率，并结合Grad-CAM技术提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 视网膜疾病的诊断对预防视力丧失和减轻社会经济负担至关重要，传统人工分级方法耗时且主观，亟需高效、客观的自动化诊断工具。

Method: 采用三种预训练的卷积神经网络（VGG16、VGG19、ResNet50）进行基准测试，并对VGG16进行微调以优化性能；同时集成Grad-CAM技术生成可视化解释，突出显示病变区域。

Result: 微调后的VGG16模型在大规模视网膜疾病数据集上达到了92.36%的测试准确率，为当前最优结果，并通过Grad-CAM提供了可解释的视觉支持。

Conclusion: EYE-DEX框架在多类视网膜疾病分类中表现出高准确性与良好可解释性，有助于提升临床医生对AI辅助诊断的信任，推动其在实际医疗中的应用。

Abstract: Retinal disease diagnosis is critical in preventing vision loss and reducing
socioeconomic burdens. Globally, over 2.2 billion people are affected by some
form of vision impairment, resulting in annual productivity losses estimated at
$411 billion. Traditional manual grading of retinal fundus images by
ophthalmologists is time-consuming and subjective. In contrast, deep learning
has revolutionized medical diagnostics by automating retinal image analysis and
achieving expert-level performance. In this study, we present EYE-DEX, an
automated framework for classifying 10 retinal conditions using the large-scale
Retinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three
pre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and
ResNet50--with our finetuned VGG16 achieving a state-of-the-art global
benchmark test accuracy of 92.36%. To enhance transparency and explainability,
we integrate the Gradient-weighted Class Activation Mapping (Grad-CAM)
technique to generate visual explanations highlighting disease-specific
regions, thereby fostering clinician trust and reliability in AI-assisted
diagnostics.

</details>


### [215] [Analysis of Bias in Deep Learning Facial Beauty Regressors](https://arxiv.org/abs/2509.24138)
*Chandon Hamel,Mike Busch*

Main category: cs.CV

TL;DR: 该研究发现，即使使用看似平衡的数据集，AI面部美学预测模型仍存在显著的种族偏见，表明当前AI美容技术在跨种族群体中缺乏公平性，并提出了可能的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 揭示AI系统在塑造审美标准中的潜在偏见问题，特别是在种族层面的不公平现象，以促进更公正的AI美容技术发展。

Method: 通过在SCUT-FBP5500和MEBeauty数据集上训练的模型进行比较分析，并使用Kruskal-Wallis H检验和事后Dunn分析等统计方法，在公平性数据集FairFace上评估不同族裔群体间的预测差异。

Result: 两个模型在族裔群体间均表现出显著的预测差异（p < 0.001），且跨数据集验证显示算法放大了社会审美偏见；仅有4.8-9.5%的组间比较满足分布一致性标准，表明现有方法在实现公平性方面严重不足。

Conclusion: 当前的AI面部美学预测方法存在系统性种族偏见，无法实现预测公平，需引入更有效的去偏策略以避免加剧社会不平等。

Abstract: Bias can be introduced to AI systems even from seemingly balanced sources,
and AI facial beauty prediction is subject to ethnicity-based bias. This work
sounds warnings about AI's role in shaping aesthetic norms while providing
potential pathways toward equitable beauty technologies through comparative
analysis of models trained on SCUT-FBP5500 and MEBeauty datasets. Employing
rigorous statistical validation (Kruskal-Wallis H-tests, post hoc Dunn
analyses). It is demonstrated that both models exhibit significant prediction
disparities across ethnic groups $(p < 0.001)$, even when evaluated on the
balanced FairFace dataset. Cross-dataset validation shows algorithmic
amplification of societal beauty biases rather than mitigation based on
prediction and error parity. The findings underscore the inadequacy of current
AI beauty prediction approaches, with only 4.8-9.5\% of inter-group comparisons
satisfying distributional parity criteria. Mitigation strategies are proposed
and discussed in detail.

</details>


### [216] [Asymmetric VAE for One-Step Video Super-Resolution Acceleration](https://arxiv.org/abs/2509.24142)
*Jianze Li,Yong Guo,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频超分辨率方法FastVSR，通过高压缩率VAE（f16）和优化的训练策略显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在视频超分辨率中虽已实现单步采样，但在推理效率上仍有优化空间。

Method: 设计了空间压缩比为16的f16 VAE结构，结合像素洗牌和通道复制进行上采样，并提出下界引导的训练策略以提高训练稳定性。

Result: 实验结果显示，FastVSR相比多步模型提速111.9倍，相比现有单步模型提速3.92倍。

Conclusion: FastVSR在保持性能的同时大幅降低计算成本，显著提升视频超分辨率的推理效率。

Abstract: Diffusion models have significant advantages in the field of real-world video
super-resolution and have demonstrated strong performance in past research. In
recent diffusion-based video super-resolution (VSR) models, the number of
sampling steps has been reduced to just one, yet there remains significant room
for further optimization in inference efficiency. In this paper, we propose
FastVSR, which achieves substantial reductions in computational cost by
implementing a high compression VAE (spatial compression ratio of 16, denoted
as f16). We design the structure of the f16 VAE and introduce a stable training
framework. We employ pixel shuffle and channel replication to achieve
additional upsampling. Furthermore, we propose a lower-bound-guided training
strategy, which introduces a simpler training objective as a lower bound for
the VAE's performance. It makes the training process more stable and easier to
converge. Experimental results show that FastVSR achieves speedups of 111.9
times compared to multi-step models and 3.92 times compared to existing
one-step models. We will release code and models at
https://github.com/JianzeLi-114/FastVSR.

</details>


### [217] [Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework](https://arxiv.org/abs/2509.24149)
*Walid Houmaidi,Youssef Sabiri,Salmane El Mansour Billah,Amine Abouaomar*

Main category: cs.CV

TL;DR: 本文提出了一种名为BrainFusion的脑肿瘤分析方法，结合细调的CNN模型（如VGG16、ResNet50、Xception）与YOLOv8进行肿瘤分类与精确定位，在MRI数据上实现了99.86%的测试准确率，显著超越先前方法，并通过可解释AI提升临床可信度。


<details>
  <summary>Details</summary>
Motivation: 早期准确分类脑肿瘤对指导治疗和改善患者预后至关重要，现有方法在准确性和定位能力上仍有提升空间。

Method: 采用细调的VGG16、ResNet50和Xception等CNN模型进行肿瘤分类，结合YOLOv8实现肿瘤精确定位，并利用可解释AI技术增强模型输出的可解释性。

Result: 在Brain Tumor MRI数据集上，细调后的VGG16模型达到99.86%的测试准确率，优于以往基准，且系统具备高精度分类与定位能力。

Conclusion: BrainFusion通过深度学习与可解释AI的融合，展现出在脑肿瘤诊断中提高速度与可靠性的巨大潜力，有助于提升患者诊疗效果和生存率。

Abstract: The early and accurate classification of brain tumors is crucial for guiding
effective treatment strategies and improving patient outcomes. This study
presents BrainFusion, a significant advancement in brain tumor analysis using
magnetic resonance imaging (MRI) by combining fine-tuned convolutional neural
networks (CNNs) for tumor classification--including VGG16, ResNet50, and
Xception--with YOLOv8 for precise tumor localization with bounding boxes.
Leveraging the Brain Tumor MRI Dataset, our experiments reveal that the
fine-tuned VGG16 model achieves test accuracy of 99.86%, substantially
exceeding previous benchmarks. Beyond setting a new accuracy standard, the
integration of bounding-box localization and explainable AI techniques further
enhances both the clinical interpretability and trustworthiness of the system's
outputs. Overall, this approach underscores the transformative potential of
deep learning in delivering faster, more reliable diagnoses, ultimately
contributing to improved patient care and survival rates.

</details>


### [218] [LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis](https://arxiv.org/abs/2509.24165)
*Moxin Zhao,Nan Meng,Jason Pui Yin Cheung,Chris Yuk Kwan Tang,Chenxi Yu,Wenting Zhong,Pengyu Lu,Chang Shi,Yipeng Zhuang,Teng Zhang*

Main category: cs.CV

TL;DR: 提出LatXGen框架，通过RGBD图像生成侧位脊柱X光片，实现无辐射的脊柱矢状面形态评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究在冠状面无辐射评估方面进展显著，但矢状面的无辐射精准评估仍缺乏可靠方法。

Method: 提出双阶段生成框架LatXGen，结合注意力机制的FFC模块和空间形变网络SDN，从后表面RGBD图像合成侧位X光片，并构建包含3264对数据的大规模配对数据集。

Result: LatXGen在视觉保真度和定量指标上优于现有GAN方法，能生成解剖结构准确的X光图像。

Conclusion: LatXGen为青少年特发性脊柱侧弯的矢状面评估提供了有前景的无辐射解决方案，推动了全面的脊柱形态评估。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal
deformity, and accurate morphological assessment requires evaluating both
coronal and sagittal alignment. While previous research has made significant
progress in developing radiation-free methods for coronal plane assessment,
reliable and accurate evaluation of sagittal alignment without ionizing
radiation remains largely underexplored. To address this gap, we propose
LatXGen, a novel generative framework that synthesizes realistic lateral spinal
radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed
backs. This enables accurate, radiation-free estimation of sagittal spinal
alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal
morphology changes from a lateral perspective based on posteroanterior surface
geometry, and (2) performing cross-modality translation from RGBD input to the
radiographic domain. The framework adopts a dual-stage architecture that
progressively estimates lateral spinal structure and synthesizes corresponding
radiographs. To enhance anatomical consistency, we introduce an attention-based
Fast Fourier Convolution (FFC) module for integrating anatomical features from
RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model
morphological variations in the lateral view. Additionally, we construct the
first large-scale paired dataset for this task, comprising 3,264 RGBD and
lateral radiograph pairs. Experimental results demonstrate that LatXGen
produces anatomically accurate radiographs and outperforms existing GAN-based
methods in both visual fidelity and quantitative metrics. This study offers a
promising, radiation-free solution for sagittal spine assessment and advances
comprehensive AIS evaluation.

</details>


### [219] [High-Order Progressive Trajectory Matching for Medical Image Dataset Distillation](https://arxiv.org/abs/2509.24177)
*Le Dong,Jinghao Bian,Jingyang Hou,Jingliang Hu,Yilei Shi,Weisheng Dong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 提出一种基于形状势能和渐进匹配策略的轨迹匹配方法，用于医学图像数据集蒸馏，有效提升蒸馏性能并保持隐私与模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法主要关注优化过程的终端状态，忽略了中间状态的关键信息，导致蒸馏效果受限。

Method: 引入形状势能以捕捉参数轨迹的几何结构，并采用从易到难的渐进式匹配策略，逐步优化参数匹配过程。

Result: 在医学图像分类任务中，该方法显著提升了蒸馏性能，在保护隐私的同时，模型准确率接近使用原始数据集训练的结果。

Conclusion: 所提出的方法通过更好地利用优化轨迹中的几何信息，有效改进了医学图像数据的蒸馏质量，具有实际应用潜力。

Abstract: Medical image analysis faces significant challenges in data sharing due to
privacy regulations and complex institutional protocols. Dataset distillation
offers a solution to address these challenges by synthesizing compact datasets
that capture essential information from real, large medical datasets.
Trajectory matching has emerged as a promising methodology for dataset
distillation; however, existing methods primarily focus on terminal states,
overlooking crucial information in intermediate optimization states. We address
this limitation by proposing a shape-wise potential that captures the geometric
structure of parameter trajectories, and an easy-to-complex matching strategy
that progressively addresses parameters based on their complexity. Experiments
on medical image classification tasks demonstrate that our method improves
distillation performance while preserving privacy and maintaining model
accuracy comparable to training on the original datasets. Our code is available
at https://github.com/Bian-jh/HoP-TM.

</details>


### [220] [Combining Discrepancy-Confusion Uncertainty and Calibration Diversity for Active Fine-Grained Image Classification](https://arxiv.org/abs/2509.24181)
*Yinghao Jin,Xi Yang*

Main category: cs.CV

TL;DR: 提出一种结合差异-混淆不确定性和校准多样性的新方法（DECERN），用于主动细粒度图像分类，有效评估样本信息量，在7个数据集的26种设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在细粒度图像分类中，由于类别间差异细微，传统主动学习难以准确评估样本的信息量，因此需要更有效的样本选择策略。

Method: 提出DECERN方法，结合差异-混淆不确定性与校准多样性：首先量化局部特征融合过程中的类别方向性和结构稳定性；然后通过不确定性加权聚类增强多样性；最后校准多样性以兼顾全局多样性和局部代表性。

Result: 在7个细粒度图像数据集、26种实验设置下的广泛实验表明，该方法在性能上显著优于现有主流方法。

Conclusion: DECERN能更有效地识别细粒度图像中的关键样本，提升标注效率和模型性能，为主动学习在细粒度分类中的应用提供了新思路。

Abstract: Active learning (AL) aims to build high-quality labeled datasets by
iteratively selecting the most informative samples from an unlabeled pool under
limited annotation budgets. However, in fine-grained image classification,
assessing this informativeness is especially challenging due to subtle
inter-class differences. In this paper, we introduce a novel method, combining
discrepancy-confusion uncertainty and calibration diversity for active
fine-grained image classification (DECERN), to effectively perceive the
distinctiveness between fine-grained images and evaluate the sample value.
DECERN introduces a multifaceted informativeness measure that combines
discrepancy-confusion uncertainty and calibration diversity. The
discrepancy-confusion uncertainty quantifies the category directionality and
structural stability of fine-grained unlabeled data during local feature
fusion. Subsequently, uncertainty-weighted clustering is performed to diversify
the uncertainty samples. Then we calibrate the diversity to maximize the global
diversity of the selected sample while maintaining its local
representativeness. Extensive experiments conducted on 7 fine-grained image
datasets across 26 distinct experimental settings demonstrate that our method
achieves superior performance compared to state-of-the-art methods.

</details>


### [221] [Tumor Synthesis conditioned on Radiomics](https://arxiv.org/abs/2509.24182)
*Jonghun Kim,Inye Na,Eun Sook Ko,Hyunjin Park*

Main category: cs.CV

TL;DR: 提出一种基于放射组学特征的肿瘤生成模型，结合GAN和扩散模型生成3D医学图像中的肿瘤掩码和纹理，支持肿瘤的编辑与多样化合成，适用于多种器官和模态，有助于下游任务训练和治疗规划。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，获取大规模3D医学影像数据集具有挑战性，现有生成模型在输出多样性方面受限，难以准确表示3D医学图像。

Method: 采用GAN生成肿瘤掩码，扩散模型基于放射组学特征生成肿瘤纹理，利用放射组学特征（如大小、形状、纹理）作为生成条件，实现肿瘤的生成、移除、操控和重定位。

Result: 在肾脏、肺、乳腺和脑四种器官的CT和MRI数据上验证了模型有效性，生成图像能有效辅助下游任务训练，并通过专家评估验证了真实性。

Conclusion: 该方法可生成多样化的逼真肿瘤图像，支持医生可视化不同放射组学特征下的肿瘤变化，具有在医学图像分析和治疗规划中应用的潜力。

Abstract: Due to privacy concerns, obtaining large datasets is challenging in medical
image analysis, especially with 3D modalities like Computed Tomography (CT) and
Magnetic Resonance Imaging (MRI). Existing generative models, developed to
address this issue, often face limitations in output diversity and thus cannot
accurately represent 3D medical images. We propose a tumor-generation model
that utilizes radiomics features as generative conditions. Radiomics features
are high-dimensional handcrafted semantic features that are biologically
well-grounded and thus are good candidates for conditioning. Our model employs
a GAN-based model to generate tumor masks and a diffusion-based approach to
generate tumor texture conditioned on radiomics features. Our method allows the
user to generate tumor images according to user-specified radiomics features
such as size, shape, and texture at an arbitrary location. This enables the
physicians to easily visualize tumor images to better understand tumors
according to changing radiomics features. Our approach allows for the removal,
manipulation, and repositioning of tumors, generating various tumor types in
different scenarios. The model has been tested on tumors in four different
organs (kidney, lung, breast, and brain) across CT and MRI. The synthesized
images are shown to effectively aid in training for downstream tasks and their
authenticity was also evaluated through expert evaluations. Our method has
potential usage in treatment planning with diverse synthesized tumors.

</details>


### [222] [Simulating Post-Neoadjuvant Chemotherapy Breast Cancer MRI via Diffusion Model with Prompt Tuning](https://arxiv.org/abs/2509.24185)
*Jonghun Kim,Hyunjin Park*

Main category: cs.CV

TL;DR: 本研究利用DCE-MRI的最大强度投影图像，结合扩散模型和提示调优，从治疗前图像预测乳腺癌新辅助化疗（NAC）后的肿瘤变化，生成图像在质量和反映病理完全缓解（pCR）相关肿瘤大小变化方面优于其他生成模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测乳腺癌患者对新辅助化疗（NAC）的响应有助于个性化治疗规划，而现有方法在生成反映肿瘤动态变化的高质量影像方面仍有不足。

Method: 采用DCE-MRI的最大强度投影图像，基于扩散模型从治疗前图像生成NAC后3或12周的预测图像，并引入提示调优以整合影响NAC响应的临床因素。

Result: 所提模型在图像质量指标上优于其他生成模型，且在生成反映pCR相关肿瘤大小变化的图像方面表现更佳，消融实验验证了方法设计的有效性。

Conclusion: 该方法有望提升NAC响应预测的准确性，为个体化精准医疗提供支持。

Abstract: Neoadjuvant chemotherapy (NAC) is a common therapy option before the main
surgery for breast cancer. Response to NAC is monitored using follow-up dynamic
contrast-enhanced magnetic resonance imaging (DCE-MRI). Accurate prediction of
NAC response helps with treatment planning. Here, we adopt maximum intensity
projection images from DCE-MRI to generate post-treatment images (i.e., 3 or 12
weeks after NAC) from pre-treatment images leveraging the emerging diffusion
model. We introduce prompt tuning to account for the known clinical factors
affecting response to NAC. Our model performed better than other generative
models in image quality metrics. Our model was better at generating images that
reflected changes in tumor size according to pCR compared to other models.
Ablation study confirmed the design choices of our method. Our study has the
potential to help with precision medicine.

</details>


### [223] [Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection](https://arxiv.org/abs/2509.24192)
*Sojung An,Kwanyong Park,Yong Jae Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为TaSe的框架，通过将文本表示分解为核心成分（对象、属性、关系）并以层次化方式重新组合，提升了视觉语言模型在复杂查询下的语言感知能力，在OmniLabel基准上实现了24%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理包含描述性属性和关系子句的复杂语言查询时表现有限，主要由于文本编码器无法有效分离目标对象与其描述信息，导致大量误检。

Method: 提出TaSe框架，包括三个部分：分层合成字幕数据集、'Talk in Pieces'模块（通过新设计的解耦损失函数将文本嵌入分解为子空间表示）、'See in Whole'模块（通过层次化目标聚合解耦后的成分）。

Result: 在OmniLabel基准上，相比现有方法性能提升24%，验证了语言组合性对细粒度多模态表示的重要性。

Conclusion: 通过引入语言结构的归纳偏置，TaSe框架显著增强了视觉语言模型对复杂查询的理解能力，为语言驱动的目标检测提供了新思路。

Abstract: While vision-language models (VLMs) have made significant progress in
multimodal perception (e.g., open-vocabulary object detection) with simple
language queries, state-of-the-art VLMs still show limited ability to perceive
complex queries involving descriptive attributes and relational clauses. Our
in-depth analysis shows that these limitations mainly stem from text encoders
in VLMs. Such text encoders behave like bags-of-words and fail to separate
target objects from their descriptive attributes and relations in complex
queries, resulting in frequent false positives. To address this, we propose
restructuring linguistic representations according to the hierarchical
relations within sentences for language-based object detection. A key insight
is the necessity of disentangling textual tokens into core components-objects,
attributes, and relations ("talk in pieces")-and subsequently aggregating them
into hierarchically structured sentence-level representations ("see in whole").
Building on this principle, we introduce the TaSe framework with three main
contributions: (1) a hierarchical synthetic captioning dataset spanning three
tiers from category names to descriptive sentences; (2) Talk in Pieces, the
three-component disentanglement module guided by a novel disentanglement loss
function, transforms text embeddings into subspace compositions; and (3) See in
Whole, which learns to aggregate disentangled components into hierarchically
structured embeddings with the guide of proposed hierarchical objectives. The
proposed TaSe framework strengthens the inductive bias of hierarchical
linguistic structures, resulting in fine-grained multimodal representations for
language-based object detection. Experimental results under the OmniLabel
benchmark show a 24% performance improvement, demonstrating the importance of
linguistic compositionality.

</details>


### [224] [An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation](https://arxiv.org/abs/2509.24194)
*Zach Eidex,Mojtaba Safari,Jie Ding,Richard Qiu,Justin Roper,David Yu,Hui-Kuo Shu,Zhen Tian,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出一种基于3D潜在空间修正流（T1C-RFlow）的深度学习框架，用于从无对比剂的多参数MRI生成高质量T1增强图像，性能优于现有模型且推理速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 钆对比剂在MRI中存在安全风险和使用不一致问题，需开发无需对比剂即可生成T1增强图像的方法。

Method: 采用预训练自编码器提取T1w和T2-FLAIR图像的潜在空间表示，并在该空间上构建3D修正流扩散模型（T1C-RFlow），在BraTS 2024胶质瘤、脑膜瘤和转移瘤数据集上进行训练与验证。

Result: T1C-RFlow在肿瘤重建质量上优于pix2pix、DDPM和DiT-3D等基准模型，NMSE和SSIM指标更优，且推理时间仅6.9秒/体积，远快于传统DDPM模型。

Conclusion: T1C-RFlow能高效生成逼真的合成T1增强图像，有望实现无需钆对比剂的脑肿瘤MRI成像，具有临床应用潜力。

Abstract: Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed
with T1w MRI to enhance lesion visualization but are restricted in patients at
risk of nephrogenic systemic fibrosis and variations in GBCA administration can
introduce imaging inconsistencies. This study develops an efficient 3D
deep-learning framework to generate T1-contrast enhanced images (T1C) from
pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified
flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and
T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient
latent space representation. A rectified flow diffusion model is then trained
in this latent space representation. The T1C-RFlow model was trained on a
curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients),
meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets.
Selected patients were split into train (N=2860), validation (N=612), and test
(N=614) sets. Results: Both qualitative and quantitative results demonstrate
that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM,
Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow
achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/-
0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/-
0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction
performance and significantly faster denoising times (6.9 s/volume, 200 steps)
than conventional DDPM models in both latent space (37.7s, 1000 steps) and
patch-based in image space (4.3 hr/volume). Significance: Our proposed method
generates synthetic T1C images that closely resemble ground truth T1C in much
less time than previous diffusion models. Further development may permit a
practical method for contrast-agent-free MRI for brain tumors.

</details>


### [225] [UniVid: The Open-Source Unified Video Model](https://arxiv.org/abs/2509.24200)
*Jiabin Luo,Junhui Lin,Zeyu Zhang,Biao Wu,Meng Fang,Ling Chen,Hao Tang*

Main category: cs.CV

TL;DR: 提出UniVid，一种结合MLLM与扩散解码器的统一视频建模架构，通过轻量适配器实现视频理解和生成，引入温度模态对齐和金字塔反射机制，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频生成中语义保真度不足（由于文本-视觉标记不平衡）和跨模态注意力局限性的问题，同时避免从图像MLLM扩展到视频时的高成本重训练。

Method: 设计UniVid架构，将MLLM与扩散解码器通过轻量适配器连接；提出温度模态对齐以增强提示遵循能力，以及金字塔反射机制通过动态关键帧选择实现高效时序推理。

Result: 在VBench-Long上比EasyAnimateV5.1提升2.2%，在MSVD-QA和ActivityNet-QA上相比最优7B基线分别提升1.0%和3.3%准确率。

Conclusion: UniVid实现了高效的统一视频理解与生成，解决了语义不一致和训练成本高的问题，在多项任务中达到领先性能。

Abstract: Unified video modeling that combines generation and understanding
capabilities is increasingly important but faces two key challenges:
maintaining semantic faithfulness during flow-based generation due to
text-visual token imbalance and the limitations of uniform cross-modal
attention across the flow trajectory, and efficiently extending image-centric
MLLMs to video without costly retraining. We present UniVid, a unified
architecture that couples an MLLM with a diffusion decoder through a
lightweight adapter, enabling both video understanding and generation. We
introduce Temperature Modality Alignment to improve prompt adherence and
Pyramid Reflection for efficient temporal reasoning via dynamic keyframe
selection. Extensive experiments on standard benchmarks demonstrate
state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total
score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA
and ActivityNet-QA, respectively, compared with the best prior 7B baselines.

</details>


### [226] [BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation](https://arxiv.org/abs/2509.24204)
*Zelin Liu,Sicheng Dong,Bocheng Li,Yixuan Yang,Jiacheng Ruan,Chenxu Zhou,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出BALR-SAM，一种用于医学图像分割的边界感知低秩自适应框架，在仅微调1.8%参数的情况下超越多种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: Vision基础模型（如SAM）在大规模自然图像上预训练后，因缺乏领域适配而在医学图像分割中表现不佳，且临床中高效微调模型并保持高性能仍具挑战。

Method: 提出BALR-SAM，包含三个组件：(1) 使用深度可分离卷积和多尺度融合的互补细节增强网络（CDEN）以捕捉边界敏感特征；(2) 在SAM的ViT块中引入低秩适配器，优化医学场景下的特征表示与注意力机制，并大幅减少参数量；(3) 在掩码解码器中采用低秩张量注意力机制，降低75%内存占用并提升推理速度。

Result: 在标准医学分割数据集上实验表明，BALR-SAM无需提示即可优于包括完全微调MedSAM在内的多种SOTA方法，仅更新1.8%（1170万）参数。

Conclusion: BALR-SAM通过边界感知设计和低秩适配策略，实现了高效、轻量且高性能的医学图像分割模型适配，适用于资源受限的临床环境。

Abstract: Vision foundation models like the Segment Anything Model (SAM), pretrained on
large-scale natural image datasets, often struggle in medical image
segmentation due to a lack of domain-specific adaptation. In clinical practice,
fine-tuning such models efficiently for medical downstream tasks with minimal
resource demands, while maintaining strong performance, is challenging. To
address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation
framework that enhances SAM for medical imaging. It combines three tailored
components: (1) a Complementary Detail Enhancement Network (CDEN) using
depthwise separable convolutions and multi-scale fusion to capture
boundary-sensitive features essential for accurate segmentation; (2) low-rank
adapters integrated into SAM's Vision Transformer blocks to optimize feature
representation and attention for medical contexts, while simultaneously
significantly reducing the parameter space; and (3) a low-rank tensor attention
mechanism in the mask decoder, cutting memory usage by 75% and boosting
inference speed. Experiments on standard medical segmentation datasets show
that BALR-SAM, without requiring prompts, outperforms several state-of-the-art
(SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8%
(11.7M) of its parameters.

</details>


### [227] [Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos](https://arxiv.org/abs/2509.24209)
*Yingdong Hu,Yisheng He,Jinnan Chen,Weihao Yuan,Kejie Qiu,Zehong Lin,Siyu Zhu,Zilong Dong,Jun Zhang*

Main category: cs.CV

TL;DR: 提出Forge4D，一种前馈式4D人体重建与插值模型，能够从未经校准的稀疏视角视频中高效重建并支持新视角和新时间点合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态3D人体重建中存在速度慢或无法生成新时间表示的问题，限制了实际应用。

Method: 将4D重建与插值简化为流式3D高斯重建与密集运动预测的联合任务；使用可学习状态令牌保证时序一致性，并设计运动预测模块与遮挡感知融合实现任意时刻插值；通过自监督重定向损失和光流损失进行训练。

Result: 在域内和域外数据集上均表现出色，实现了高效、高质量的4D人体重建与时间插值。

Conclusion: Forge4D有效解决了稀疏视角下动态人体重建的效率与时序一致性问题，支持新颖视角和时间点的合成，具有广泛的应用前景。

Abstract: Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view
videos is critical for numerous downstream applications. Existing methods,
however, are either limited by the slow reconstruction speeds or incapable of
generating novel-time representations. To address these challenges, we propose
Forge4D, a feed-forward 4D human reconstruction and interpolation model that
efficiently reconstructs temporally aligned representations from uncalibrated
sparse-view videos, enabling both novel view and novel time synthesis. Our
model simplifies the 4D reconstruction and interpolation problem as a joint
task of streaming 3D Gaussian reconstruction and dense motion prediction. For
the task of streaming 3D Gaussian reconstruction, we first reconstruct static
3D Gaussians from uncalibrated sparse-view images and then introduce learnable
state tokens to enforce temporal consistency in a memory-friendly manner by
interactively updating shared information across different timestamps. For
novel time synthesis, we design a novel motion prediction module to predict
dense motions for each 3D Gaussian between two adjacent frames, coupled with an
occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at
arbitrary timestamps. To overcome the lack of the ground truth for dense motion
supervision, we formulate dense motion prediction as a dense point matching
task and introduce a self-supervised retargeting loss to optimize this module.
An additional occlusion-aware optical flow loss is introduced to ensure motion
consistency with plausible human movement, providing stronger regularization.
Extensive experiments demonstrate the effectiveness of our model on both
in-domain and out-of-domain datasets. Project page and code at:
https://zhenliuzju.github.io/huyingdong/Forge4D.

</details>


### [228] [Scalable Audio-Visual Masked Autoencoders for Efficient Affective Video Facial Analysis](https://arxiv.org/abs/2509.24214)
*Xuecheng Wu,Junxiao Xue,Xinyi Yin,Yunyun Shi,Liangyu Fu,Danlei Huang,Yifan Wang,Jia Zhang,Jiayu Nie,Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了AVF-MAE++，一种用于情感视频面部分析（AVFA）的音频-视觉掩码自编码器模型家族，通过双模掩码策略和迭代音视频相关学习模块，有效提升了跨模态关联建模与可扩展性，在17个数据集上实现了多项任务的最先进性能。


<details>
  <summary>Details</summary>
Motivation: AVFA领域受限于数据稀缺，且缺乏对音视频跨模态相关性的有效建模，同时现有自监督方法在可扩展性方面的潜力尚未充分挖掘。

Method: 提出AVF-MAE++框架，采用双模掩码策略、增强的模态编码器设计、迭代音视频相关学习模块以及三阶段渐进式语义注入训练策略，实现更高效的可扩展预训练与跨模态关联建模。

Result: 在17个数据集上验证了模型在三种主要AVFA任务中的优越性，显著优于现有方法，并通过消融实验验证了各组件的有效性。

Conclusion: AVF-MAE++通过系统性设计提升了音视频自监督学习在情感分析中的可扩展性与跨模态建模能力，为AVFA提供了新的基准方案。

Abstract: Affective video facial analysis (AVFA) has emerged as a key research field
for building emotion-aware intelligent systems, yet this field continues to
suffer from limited data availability. In recent years, the self-supervised
learning (SSL) technique of Masked Autoencoders (MAE) has gained momentum, with
growing adaptations in its audio-visual contexts. While scaling has proven
essential for breakthroughs in general multi-modal learning domains, its
specific impact on AVFA remains largely unexplored. Another core challenge in
this field is capturing both intra- and inter-modal correlations through
scalable audio-visual representations. To tackle these issues, we propose
AVF-MAE++, a family of audio-visual MAE models designed to efficiently
investigate the scaling properties in AVFA while enhancing cross-modal
correlation modeling. Our framework introduces a novel dual masking strategy
across audio and visual modalities and strengthens modality encoders with a
more holistic design to better support scalable pre-training. Additionally, we
present the Iterative Audio-Visual Correlation Learning Module, which improves
correlation learning within the SSL paradigm, bridging the limitations of
previous methods. To support smooth adaptation and reduce overfitting risks, we
further introduce a progressive semantic injection strategy, organizing the
model training into three structured stages. Extensive experiments conducted on
17 datasets, covering three major AVFA tasks, demonstrate that AVF-MAE++
achieves consistent state-of-the-art performance across multiple benchmarks.
Comprehensive ablation studies further highlight the importance of each
proposed component and provide deeper insights into the design choices driving
these improvements. Our code and models have been publicly released at Github.

</details>


### [229] [EVLF-FM: Explainable Vision Language Foundation Model for Medicine](https://arxiv.org/abs/2509.24231)
*Yang Bai,Haoran Cheng,Yang Zhou,Jun Zhou,Arun Thirunavukarasu,Yuhe Ke,Jie Yao,Kanae Fukutsu,Chrystie Wan Ning Quek,Ashley Hong,Laura Gutierrez,Zhen Ling Teo,Darren Shu Jeng Ting,Brian T. Soetikno,Christopher S. Nielsen,Tobias Elze,Zengxiang Li,Linh Le Dinh,Hiok Hong Chan,Victor Koh,Marcus Tan,Kelvin Z. Li,Leonard Yip,Ching Yu Cheng,Yih Chung Tham,Gavin Siew Wei Tan,Leopold Schmetterer,Marcus Ang,Rahat Hussain,Jod Mehta,Tin Aung,Lionel Tim-Ee Cheng,Tran Nguyen Tuan Anh,Chee Leong Cheng,Tien Yin Wong,Nan Liu,Iain Beehuat Tan,Soon Thye Lim,Eyal Klang,Tony Kiat Hon Lim,Rick Siow Mong Goh,Yong Liu,Daniel Shu Wei Ting*

Main category: cs.CV

TL;DR: EVLF-FM是一个多模态视觉-语言基础模型，统一了广泛的诊断能力与细粒度的可解释性，在多种医学影像模态和临床专科中表现出色，具备像素级视觉定位和推理能力，通过混合训练策略实现最先进的准确性和逐步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI基础模型局限于特定模态且缺乏透明推理过程，阻碍了临床应用。因此需要一个具备跨模态诊断能力和可解释性的统一模型。

Method: 提出EVLF-FM，采用多模态视觉-语言架构，结合监督学习与视觉强化微调的混合训练策略，支持多疾病诊断、视觉问答及像素级视觉定位与推理。

Result: 在内部验证中，EVLF-FM平均准确率达0.858，F1-score为0.797，优于主流通用和专用模型；在九种模态中mIOU达0.743，Acc@0.5为0.837；外部验证显示其在少样本和零样本设置下表现优异。

Conclusion: EVLF-FM是一种具备可解释性和推理能力的多疾病VLM模型，有助于提升基础模型在真实临床环境中的可信度与应用前景。

Abstract: Despite the promise of foundation models in medical AI, current systems
remain limited - they are modality-specific and lack transparent reasoning
processes, hindering clinical adoption. To address this gap, we present
EVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify
broad diagnostic capability with fine-grain explainability. The development and
testing of EVLF-FM encompassed over 1.3 million total samples from 23 global
datasets across eleven imaging modalities related to six clinical specialties:
dermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology.
External validation employed 8,884 independent test samples from 10 additional
datasets across five imaging modalities. Technically, EVLF-FM is developed to
assist with multiple disease diagnosis and visual question answering with
pixel-level visual grounding and reasoning capabilities. In internal validation
for disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858)
and F1-score (0.797), outperforming leading generalist and specialist models.
In medical visual grounding, EVLF-FM also achieved stellar performance across
nine modalities with average mIOU of 0.743 and Acc@0.5 of 0.837. External
validations further confirmed strong zero-shot and few-shot performance, with
competitive F1-scores despite a smaller model size. Through a hybrid training
strategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not
only achieves state-of-the-art accuracy but also exhibits step-by-step
reasoning, aligning outputs with visual evidence. EVLF-FM is an early
multi-disease VLM model with explainability and reasoning capabilities that
could advance adoption of and trust in foundation models for real-world
clinical deployment.

</details>


### [230] [Latent Visual Reasoning](https://arxiv.org/abs/2509.24251)
*Bangzheng Li,Ximeng Sun,Jiang Liu,Ze Wang,Jialian Wu,Xiaodong Yu,Hao Chen,Emad Barsoum,Muhao Chen,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态大语言模型推理范式Latent Visual Reasoning (LVR)，在视觉嵌入空间中进行自回归推理，显著提升了细粒度视觉理解和感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将推理局限于语言空间，视觉信息仅为静态前提，限制了多模态模型的感知能力。

Method: 通过视觉编码器将图像映射到与语言模型共享的语义空间中的视觉token，训练语言模型生成关键视觉token的潜在状态，实现潜视觉推理，并与文本生成交错进行。

Result: 在MMVP任务上达到71.67%的性能，优于Qwen2.5-VL的66.67%，并在感知密集型VQA任务中取得显著提升。

Conclusion: LVR实现了在视觉嵌入空间中的直接推理，增强了模型对视觉细节的理解和感知能力，为多模态推理提供了新方向。

Abstract: Multimodal Large Language Models (MLLMs) have achieved notable gains in
various tasks by incorporating Chain-of-Thought (CoT) reasoning in language
spaces. Recent work extends this direction by leveraging external tools for
visual editing, thereby enhancing the visual signal along the reasoning
trajectories. Nevertheless, these approaches remain fundamentally constrained:
reasoning is still confined to the language space, with visual information
treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a
new paradigm that enables autoregressive reasoning directly in the visual
embedding space. A visual encoder first projects images into visual tokens
within a joint semantic space shared with the language model. The language
model is then trained to generate latent states that reconstruct key visual
tokens critical for answering the query, constituting the process of latent
visual reasoning. By interleaving LVR with standard text generation, our model
achieves substantial gains on perception-intensive visual question answering
tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement
learning on latent reasoning, further balancing LVR and textual generation. We
show that LVR substantially improves fine-grained visual understanding and
perception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code
base and model weights will be released later.

</details>


### [231] [When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs](https://arxiv.org/abs/2509.24258)
*Jinming Liu,Zhaoyang Jia,Jiahao Li,Bin Li,Xin Jin,Wenjun Zeng,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态大语言模型（MLLMs）的图像编解码器CoTAM，通过自适应保护多级特征来优化压缩性能，在保持下游任务表现的同时显著降低比特率。


<details>
  <summary>Details</summary>
Motivation: 传统图像编解码器针对人类视觉系统优化，不适合MLLM在多种下游任务中对不同层次特征的需求；现有压缩方法会引入影响MLLM性能的失真。

Method: 基于CLIP浅层注意力生成重要性图用于比特分配，并设计带有多级损失函数的轻量级适配解码器，以同时保留低级细节和高级语义信息。

Result: 实验表明，相比现有神经编解码器，该方法在保持MLLM任务性能的同时，最高可节省35.99%的比特率。

Conclusion: CoTAM能有效适应MLLM对多级特征的需求，是首个专为MLLM设计的图像编解码框架，显著提升了压缩效率与任务兼容性。

Abstract: The increasing deployment of powerful Multimodal Large Language Models
(MLLMs), typically hosted on cloud platforms, urgently requires effective
compression techniques to efficiently transmit signal inputs (e.g., images,
videos) from edge devices with minimal bandwidth usage. However, conventional
image codecs are optimized for fidelity to serve the Human Visual System (HVS)
and ill-suited for MLLMs, in which diverse downstream tasks are jointly
considered. In this paper, we first systematically analyze the impact of
compression artifacts on several mainstream MLLMs. We find that: Compression
distortion unevenly impacts different-level image features, leading to varying
effects on MLLMs' downstream tasks depending on their feature-level reliance.
Motivated by this discovery, we propose an image Codec TAilored to MLLMs
(CoTAM) designed to adaptively protect multi-level features and suit different
demands of downstream tasks. The encoder leverages CLIP's shallow-layer
attention to generate an importance map for bit allocation, preserving critical
semantic regions. Concurrently, the decoder integrates a lightweight adapter
with a multi-level loss function to ensure the faithful reconstruction both of
low-level details and high-level semantic context for robust synthesis of
cross-level features. Extensive experiments validate that our method achieves
up to 35.99\% bitrate saving while maintaining the same performance on the MLLM
tasks, outperforming previous SOTA neural codecs.

</details>


### [232] [S$^2$NN: Sub-bit Spiking Neural Networks](https://arxiv.org/abs/2509.24266)
*Wenjie Wei,Malu Zhang,Jieyuan Zhang,Ammar Belatreche,Shuai Wang,Yimeng Shan,Hanwen Liu,Honglin Cao,Guoqing Wang,Yang Yang,Haizhou Li*

Main category: cs.CV

TL;DR: 本文提出了亚比特脉冲神经网络（S^2NNs），通过使用少于1比特的权重表示来压缩和加速SNN，结合OS-Quant和MPFD方法，在保持高性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 为了进一步压缩和加速脉冲神经网络（SNNs），解决二值SNN在大规模部署时仍存在的存储和计算开销问题。

Method: 提出S^2NN框架，利用训练好的二值SNN中核的聚类模式建立基线；设计OS-Quant方法，通过识别并自适应缩放异常值来优化码字选择；引入基于膜电位的特征蒸馏（MPFD）方法，提升高度压缩模型的性能。

Result: 在视觉与非视觉任务上实验表明，S^2NN在性能和效率方面优于现有的量化SNN方法。

Conclusion: S^2NN通过亚比特权重表示有效平衡了模型压缩与性能，具有在边缘计算场景中部署的良好潜力。

Abstract: Spiking Neural Networks (SNNs) offer an energy-efficient paradigm for machine
intelligence, but their continued scaling poses challenges for resource-limited
deployment. Despite recent advances in binary SNNs, the storage and
computational demands remain substantial for large-scale networks. To further
explore the compression and acceleration potential of SNNs, we propose Sub-bit
Spiking Neural Networks (S$^2$NNs) that represent weights with less than one
bit. Specifically, we first establish an S$^2$NN baseline by leveraging the
clustering patterns of kernels in well-trained binary SNNs. This baseline is
highly efficient but suffers from \textit{outlier-induced codeword selection
bias} during training. To mitigate this issue, we propose an
\textit{outlier-aware sub-bit weight quantization} (OS-Quant) method, which
optimizes codeword selection by identifying and adaptively scaling outliers.
Furthermore, we propose a \textit{membrane potential-based feature
distillation} (MPFD) method, improving the performance of highly compressed
S$^2$NN via more precise guidance from a teacher model. Extensive results on
vision and non-vision tasks reveal that S$^2$NN outperforms existing quantized
SNNs in both performance and efficiency, making it promising for edge computing
applications.

</details>


### [233] [Cycle Diffusion Model for Counterfactual Image Generation](https://arxiv.org/abs/2509.24267)
*Fangrui Huang,Alan Wang,Binxu Li,Bailey Trang,Ridvan Yesiloglu,Tianyu Hua,Wei Peng,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出了一种基于循环训练的扩散模型（CDM），用于提升医学图像生成中的条件保真度和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像生成方法在条件保真度和生成质量方面存在不足，尤其在直接生成和反事实生成中难以保证一致性。

Method: 引入循环训练框架，通过加入循环约束来微调扩散模型，确保生成图像与原始图像之间的一致性，从而提高条件遵循能力和图像真实性。

Result: 在包含多个3D脑部MRI数据集的联合数据上实验表明，该方法提升了条件准确性，并在FID和SSIM指标上改善了生成图像质量。

Conclusion: CDM的循环策略能有效改进基于扩散模型的医学图像生成，适用于数据增强、反事实分析和疾病进展建模等应用。

Abstract: Deep generative models have demonstrated remarkable success in medical image
synthesis. However, ensuring conditioning faithfulness and high-quality
synthetic images for direct or counterfactual generation remains a challenge.
In this work, we introduce a cycle training framework to fine-tune diffusion
models for improved conditioning adherence and enhanced synthetic image
realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency
between generated and original images by incorporating cycle constraints,
enabling more reliable direct and counterfactual generation. Experiments on a
combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and
PPMI) show that our method improves conditioning accuracy and enhances image
quality as measured by FID and SSIM. The results suggest that the cycle
strategy used in CDM can be an effective method for refining diffusion-based
medical image generation, with applications in data augmentation,
counterfactual, and disease progression modeling.

</details>


### [234] [Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds](https://arxiv.org/abs/2509.24273)
*Yongqiang Wang,Weigang Li,Wenping Liu,Zhiqiang Tian,Jinling Li*

Main category: cs.CV

TL;DR: 提出了一种基于骨架的鲁棒点云配准框架（SRRF），通过结合点云及其骨架的对齐变换，并引入分布距离损失函数，提升了在密度失真、噪声和几何形变等复杂情况下的配准精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准方法对噪声、密度失真和几何变形敏感，难以在真实场景中保持高精度，因此需要一种更具鲁棒性的配准方法。

Method: 提出SRRF框架，利用骨架表示增强配准的稳定性，联合优化原始点云和骨架的对齐结果，并设计分布距离损失函数以保持源点云与目标点云骨架的一致性。

Result: 在多种受污染数据集上的实验表明，SRRF在不同退化场景下均优于当前最先进的配准方法，表现出更强的鲁棒性和更高的配准精度。

Conclusion: SRRF通过融合骨架结构信息有效提升了点云配准在复杂退化条件下的性能，适用于实际3D感知任务。

Abstract: Point cloud registration is fundamental in 3D vision applications, including
autonomous driving, robotics, and medical imaging, where precise alignment of
multiple point clouds is essential for accurate environment reconstruction.
However, real-world point clouds are often affected by sensor limitations,
environmental noise, and preprocessing errors, making registration challenging
due to density distortions, noise contamination, and geometric deformations.
Existing registration methods rely on direct point matching or surface feature
extraction, which are highly susceptible to these corruptions and lead to
reduced alignment accuracy. To address these challenges, a skeleton-based
robust registration framework is presented, which introduces a
corruption-resilient skeletal representation to improve registration robustness
and accuracy. The framework integrates skeletal structures into the
registration process and combines the transformations obtained from both the
corrupted point cloud alignment and its skeleton alignment to achieve optimal
registration. In addition, a distribution distance loss function is designed to
enforce the consistency between the source and target skeletons, which
significantly improves the registration performance. This framework ensures
that the alignment considers both the original local geometric features and the
global stability of the skeleton structure, resulting in robust and accurate
registration results. Experimental evaluations on diverse corrupted datasets
demonstrate that SRRF consistently outperforms state-of-the-art registration
methods across various corruption scenarios, including density distortions,
noise contamination, and geometric deformations. The results confirm the
robustness of SRRF in handling corrupted point clouds, making it a potential
approach for 3D perception tasks in real-world scenarios.

</details>


### [235] [Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context](https://arxiv.org/abs/2509.24275)
*Yongqiang Wang,Weigang Li,Wenping Liu,Zhe Xu,Zhiqiang Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为CEGC的统一、基于置信度的框架，用于鲁棒的部分3D点云配准。该方法通过联合建模重叠置信度和对应关系可靠性，在复杂场景中实现精确对齐，并在多个数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 部分点云配准在自主感知和3D场景理解中至关重要，但由于结构模糊性、部分可见性和噪声等问题仍具挑战性。

Method: 提出CEGC框架，包含混合重叠置信度估计模块（结合语义描述符和几何相似性）和上下文感知匹配策略（利用全局注意力分配软置信度分数），并通过可微加权SVD求解器计算精确变换。

Result: 在ModelNet40、ScanObjectNN和7Scenes数据集上的实验表明，CEGC在配准精度、鲁棒性和泛化能力方面优于当前最先进的方法。

Conclusion: CEGC提供了一种可解释且可扩展的解决方案，有效应对挑战性条件下的部分点云配准问题。

Abstract: Partial point cloud registration is essential for autonomous perception and
3D scene understanding, yet it remains challenging owing to structural
ambiguity, partial visibility, and noise. We address these issues by proposing
Confidence Estimation under Global Context (CEGC), a unified, confidence-driven
framework for robust partial 3D registration. CEGC enables accurate alignment
in complex scenes by jointly modeling overlap confidence and correspondence
reliability within a shared global context. Specifically, the hybrid overlap
confidence estimation module integrates semantic descriptors and geometric
similarity to detect overlapping regions and suppress outliers early. The
context-aware matching strategy smitigates ambiguity by employing global
attention to assign soft confidence scores to correspondences, improving
robustness. These scores guide a differentiable weighted singular value
decomposition solver to compute precise transformations. This tightly coupled
pipeline adaptively down-weights uncertain regions and emphasizes contextually
reliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D
vision datasets demonstrate that CEGC outperforms state-of-the-art methods in
accuracy, robustness, and generalization. Overall, CEGC offers an interpretable
and scalable solution to partial point cloud registration under challenging
conditions.

</details>


### [236] [ASIA: Adaptive 3D Segmentation using Few Image Annotations](https://arxiv.org/abs/2509.24288)
*Sai Raj Kishore Perla,Aditya Vora,Sauradip Nag,Ali Mahdavi-Amiri,Hao Zhang*

Main category: cs.CV

TL;DR: ASIA是一种新颖的3D分割框架，通过少量野外图像标注实现对非语义和难以文本描述的3D“部分”进行可控分割。


<details>
  <summary>Details</summary>
Motivation: 现有的3D分割方法依赖多视角图像或复杂的3D标注，成本高且不灵活；而文本描述可能模糊不清。因此需要一种更实用、精确且易于使用的3D分割方法。

Method: 利用Stable Diffusion等文生图扩散模型的先验知识，将图像空间中的分割结果迁移到3D；训练时优化每个片段的文本token，并使用新的跨视角部件对应损失进行微调；推理时对3D网格的多视图渲染进行分割，在UV空间融合标签，通过噪声优化技术 refine，并映射回网格。

Result: ASIA在语义与非语义3D分割任务中均优于现有方法，定量与定性评估均有显著提升。

Conclusion: ASIA提供了一种实用且可泛化的3D分割解决方案，仅需少量图像标注即可实现高精度、可控的分割，适用于几何或结构差异较大的对象。

Abstract: We introduce ASIA (Adaptive 3D Segmentation using few Image Annotations), a
novel framework that enables segmentation of possibly non-semantic and
non-text-describable "parts" in 3D. Our segmentation is controllable through a
few user-annotated in-the-wild images, which are easier to collect than
multi-view images, less demanding to annotate than 3D models, and more precise
than potentially ambiguous text descriptions. Our method leverages the rich
priors of text-to-image diffusion models, such as Stable Diffusion (SD), to
transfer segmentations from image space to 3D, even when the annotated and
target objects differ significantly in geometry or structure. During training,
we optimize a text token for each segment and fine-tune our model with a novel
cross-view part correspondence loss. At inference, we segment multi-view
renderings of the 3D mesh, fuse the labels in UV-space via voting, refine them
with our novel Noise Optimization technique, and finally map the UV-labels back
onto the mesh. ASIA provides a practical and generalizable solution for both
semantic and non-semantic 3D segmentation tasks, outperforming existing methods
by a noticeable margin in both quantitative and qualitative evaluations.

</details>


### [237] [SVGThinker: Instruction-Aligned and Reasoning-Driven Text-to-SVG Generation](https://arxiv.org/abs/2509.24299)
*Hanqi Chen,Zhongyin Zhao,Ye Chen,Zhujin Liang,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出SVGThinker，一种基于推理的文本到SVG生成框架，通过逐步生成和多模态标注提升生成质量与指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SVG生成方法存在泛化能力弱和指令遵循差的问题。

Method: 构建一个推理驱动的框架，逐个渲染SVG基本图元并利用多模态模型标注图像与代码，通过监督微调训练LLM暴露其思维链。

Result: 实验表明，SVGThinker相比最先进基线能生成更稳定、可编辑且质量更高的SVG，同时保持矢量图形的结构优势。

Conclusion: SVGThinker有效提升了文本到SVG生成的鲁棒性与准确性，支持精确分层编辑，为设计与自动化图形生成提供了新方向。

Abstract: Scalable Vector Graphics (SVG) is a code-based representation for 2D visuals.
Leveraging recent advances in large language models (LLMs), we study
text-to-SVG generation and address two persistent gaps: weak generalization and
poor adherence to input instructions. We present SVGThinker, a reasoning-driven
framework that aligns the production of SVG code with the visualization process
and supports the full set of SVG primitives. Our pipeline first renders each
primitive in sequence and uses a multimodal model to annotate the image and
code; we then build stepwise updates that mirror the incremental addition of
primitives. On this data, we train an LLM with supervised fine-tuning that
exposes its chain-of-thought as intermediate reasoning, improving robustness
and reducing errors and hallucinations. Experiments against state-of-the-art
baselines show that SVGThinker produces more stable, editable, and
higher-quality SVGs while preserving the structural advantages of vector
graphics. Unlike image-based methods, our outputs enable precise and
hierarchical editing, opening new directions for design, content creation, and
automated graphics generation.

</details>


### [238] [FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting](https://arxiv.org/abs/2509.24304)
*Zefeng He,Xiaoye Qu,Yafu Li,Siyuan Huang,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出了FrameThinker框架，通过迭代查询视频内容实现长视频推理，采用两阶段训练策略（SFT+强化学习），在显著减少处理帧数的同时，在多个长视频理解基准上实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在长视频理解中受限于均匀帧采样和静态文本推理，效率低下且难以应对视觉密集型任务，因此需要一种更高效的长视频推理机制。

Method: 提出FrameThinker框架，引入视频内容的迭代查询机制；采用两阶段训练：先用监督微调（SFT）赋予模型基本操作能力，再用强化学习（RL）优化决策策略，并深入设计各动作的奖励函数。

Result: 在Video-Holmes、LongVideo-Reason等多个基准上平均提升+10.4%，7B模型在LongVideo-Reason上以仅20.6帧的平均数量达到76.1%准确率，超过LongVILA-R1（72.0%）且使用帧数减少20倍以上。

Conclusion: FrameThinker通过引入动态帧选择与强化学习驱动的推理机制，显著提升了长视频理解的效率与准确性，为LVLMs在长视频任务中的应用提供了新范式。

Abstract: While Large Vision-Language Models (LVLMs) have achieved substantial progress
in video understanding, their application to long video reasoning is hindered
by uniform frame sampling and static textual reasoning, which are inefficient
and struggle to handle visually intensive video tasks. To overcome these
challenges, in this paper, we introduce the concept of thinking with long
videos and propose a novel framework FrameThinker. Within this framework, LVLMs
are able to iteratively interrogate video content. Developing such video
reasoning capabilities in LVLMs presents notable challenges, particularly in
adapting the model to new video actions (e.g. select frame), and designing
reward functions to guide LVLMs to adopt the newly introduced action. To solve
these challenges, we propose a two-phase training strategy, first employing
Supervised Fine-Tuning (SFT) to instill fundamental action capabilities,
followed by Reinforcement Learning (RL) to optimize a strategic decision-making
policy.Notably, in this RL phase, we conduct an in-depth and comprehensive
exploration of the reward design for each action and format reward. Extensive
experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and
long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and
LVBench, demonstrate that FrameThinker achieves a significant average
improvement of +10.4% over baselines while drastically reducing the number of
processed frames. Most notably, our 7B model, FrameThinker establishes a new
state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average
of only 20.6 frames. This not only outperforms the competitive LongVILA-R1
(72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating
unparalleled efficiency and effectiveness.

</details>


### [239] [OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction](https://arxiv.org/abs/2509.24308)
*Yuhang Cao,Haojun Yan,Danya Yao*

Main category: cs.CV

TL;DR: 本文提出OMeGa，一种联合优化显式三角网格和2D高斯点的端到端框架，通过灵活绑定策略和引入网格约束与单目法线监督，显著提升无纹理室内区域的重建精度，在挑战性室内基准上性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点渲染方法在无纹理室内区域存在几何重建不准确问题，且网格提取与优化脱节，未能利用网格几何指导点优化。

Method: 提出OMeGa框架，采用灵活绑定策略将高斯点的空间属性表达在网格坐标系中，纹理属性保留在点上；联合优化网格与点；引入网格约束和单目法线监督正则化几何学习；设计启发式迭代网格 refinement 策略，分割高误差面片并剪枝不可靠部分。

Result: 在室内重建基准上达到SOTA性能，Chamfer-L1比2DGS基线降低47.3%，同时保持有竞争力的新视图渲染质量。

Conclusion: OMeGa有效解决了以往方法在无纹理室内重建中的局限性，通过联合优化和几何引导显著提升了重建精度。

Abstract: Neural rendering with Gaussian splatting has advanced novel view synthesis,
and most methods reconstruct surfaces via post-hoc mesh extraction. However,
existing methods suffer from two limitations: (i) inaccurate geometry in
texture-less indoor regions, and (ii) the decoupling of mesh extraction from
optimization, thereby missing the opportunity to leverage mesh geometry to
guide splat optimization. In this paper, we present OMeGa, an end-to-end
framework that jointly optimizes an explicit triangle mesh and 2D Gaussian
splats via a flexible binding strategy, where spatial attributes of Gaussian
Splats are expressed in the mesh frame and texture attributes are retained on
splats. To further improve reconstruction accuracy, we integrate mesh
constraints and monocular normal supervision into the optimization, thereby
regularizing geometry learning. In addition, we propose a heuristic, iterative
mesh-refinement strategy that splits high-error faces and prunes unreliable
ones to further improve the detail and accuracy of the reconstructed mesh.
OMeGa achieves state-of-the-art performance on challenging indoor
reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\% over the 2DGS
baseline while maintaining competitive novel-view rendering quality. The
experimental results demonstrate that OMeGa effectively addresses prior
limitations in indoor texture-less reconstruction.

</details>


### [240] [Towards Foundation Models for Cryo-ET Subtomogram Analysis](https://arxiv.org/abs/2509.24311)
*Runmin Jiang,Wanyue Feng,Yuntian Yang,Shriya Pingulkar,Hong Wang,Xi Xiao,Xiaoyu Cao,Genpei Zhang,Xiao Wang,Xiaolong Wu,Tianyang Wang,Yang Liu,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了用于冷冻电子断层扫描（cryo-ET）子断层图像分析的首个基础模型框架，包括大规模合成数据生成器CryoEngine、自适应相位分词增强的视觉Transformer（APT-ViT）和抗噪对比学习策略（NRCL），在多种任务上实现了最先进的性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于标注稀缺、噪声严重和模型泛化能力差，现有的cryo-ET子断层分析面临挑战，亟需更鲁棒和可扩展的方法。

Method: 提出CryoEngine生成大规模合成数据；设计APT-ViT引入自适应相位分词以增强对几何和语义变化的不变性；采用NRCL策略提升在高噪声下的表征学习稳定性。

Result: 在24个合成与真实数据集上验证，该方法在分类、对齐和平均三大子断层任务中均达到最先进性能，并展现出对未见数据集的强泛化能力。

Conclusion: 所提出的基础模型框架显著提升了cryo-ET子断层分析的鲁棒性与可扩展性，为结构生物学中的原位结构解析提供了有力工具。

Abstract: Cryo-electron tomography (cryo-ET) enables in situ visualization of
macromolecular structures, where subtomogram analysis tasks such as
classification, alignment, and averaging are critical for structural
determination. However, effective analysis is hindered by scarce annotations,
severe noise, and poor generalization. To address these challenges, we take the
first step towards foundation models for cryo-ET subtomograms. First, we
introduce CryoEngine, a large-scale synthetic data generator that produces over
904k subtomograms from 452 particle classes for pretraining. Second, we design
an Adaptive Phase Tokenization-enhanced Vision Transformer (APT-ViT), which
incorporates adaptive phase tokenization as an equivariance-enhancing module
that improves robustness to both geometric and semantic variations. Third, we
introduce a Noise-Resilient Contrastive Learning (NRCL) strategy to stabilize
representation learning under severe noise conditions. Evaluations across 24
synthetic and real datasets demonstrate state-of-the-art (SOTA) performance on
all three major subtomogram tasks and strong generalization to unseen datasets,
advancing scalable and robust subtomogram analysis in cryo-ET.

</details>


### [241] [Similarity-Aware Selective State-Space Modeling for Semantic Correspondence](https://arxiv.org/abs/2509.24318)
*Seungwook Kim,Minsu Cho*

Main category: cs.CV

TL;DR: MambaMatcher是一种新的方法，利用选择性状态空间模型高效建模高维相关性，克服了传统特征度量方法和现有相关度量方法的局限性，在标准语义对应基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统特征度量方法可能忽略复杂的相互关系，而现有的相关度量方法因处理4D相关图而计算成本高。

Method: 引入MambaMatcher，采用来自Mamba线性复杂度算法的相似性感知选择扫描机制，有效优化4D相关图，同时保持特征图分辨率和感受野。

Result: 在标准语义对应基准上的实验表明，MambaMatcher实现了最先进的性能。

Conclusion: MambaMatcher通过高效建模高维相关性，显著提升了语义图像匹配的性能和效率。

Abstract: Establishing semantic correspondences between images is a fundamental yet
challenging task in computer vision. Traditional feature-metric methods enhance
visual features but may miss complex inter-correlation relationships, while
recent correlation-metric approaches are hindered by high computational costs
due to processing 4D correlation maps. We introduce MambaMatcher, a novel
method that overcomes these limitations by efficiently modeling
high-dimensional correlations using selective state-space models (SSMs). By
implementing a similarity-aware selective scan mechanism adapted from Mamba's
linear-complexity algorithm, MambaMatcher refines the 4D correlation map
effectively without compromising feature map resolution or receptive field.
Experiments on standard semantic correspondence benchmarks demonstrate that
MambaMatcher achieves state-of-the-art performance.

</details>


### [242] [TP-MVCC: Tri-plane Multi-view Fusion Model for Silkie Chicken Counting](https://arxiv.org/abs/2509.24329)
*Sirui Chen,Yuhong Feng,Yifeng Wang,Jianghai Liao,Qi Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于三平面多视角的鸡群计数模型TP-MVCC，通过几何投影和特征融合实现高精度计数。


<details>
  <summary>Details</summary>
Motivation: 在拥挤场景中由于遮挡和视角受限，准确的动物计数在智能养殖中具有挑战性。

Method: 采用多视角相机输入，利用几何投影与三平面融合将多视角特征统一到地面平面，并通过空间变换对齐特征，解码生成场景级密度图进行计数。

Result: 在真实养殖条件下构建了首个丝羽乌骨鸡多视角数据集，实验显示TP-MVCC达到95.1%的准确率，显著优于单视角和传统融合方法，且在密集遮挡场景中表现出强鲁棒性。

Conclusion: TP-MVCC有效解决了多视角鸡群计数中的遮挡问题，具备在智能农业中实际应用的潜力。

Abstract: Accurate animal counting is essential for smart farming but remains difficult
in crowded scenes due to occlusions and limited camera views. To address this,
we propose a tri-plane-based multi-view chicken counting model (TP-MVCC), which
leverages geometric projection and tri-plane fusion to integrate features from
multiple cameras onto a unified ground plane. The framework extracts
single-view features, aligns them via spatial transformation, and decodes a
scene-level density map for precise chicken counting. In addition, we construct
the first multi-view dataset of silkie chickens under real farming conditions.
Experiments show that TP-MVCC significantly outperforms single-view and
conventional fusion comparisons, achieving 95.1\% accuracy and strong
robustness in dense, occluded scenarios, demonstrating its practical potential
for intelligent agriculture.

</details>


### [243] [Hyperspherical Latents Improve Continuous-Token Autoregressive Generation](https://arxiv.org/abs/2509.24335)
*Guolin Ke,Hui Xue*

Main category: cs.CV

TL;DR: SphereAR通过将自回归模型的输入输出约束在固定半径的超球面上，解决了VAE潜在空间中异方差性导致的方差崩溃问题，在ImageNet图像生成上取得了优于扩散模型和掩码生成模型的效果。


<details>
  <summary>Details</summary>
Motivation: 连续token的自回归图像生成模型在分类器自由引导（CFG）下存在潜在空间方差不一致的问题，导致生成过程中的方差崩溃，限制了其性能。

Method: 提出SphereAR，利用超球面VAE将所有自回归输入输出（包括经过CFG后的）约束在固定半径的超球面上，消除尺度分量的影响，从而稳定解码过程。

Result: 在ImageNet生成任务中，SphereAR-H（943M）达到FID 1.34，SphereAR-L（479M）达到FID 1.54，SphereAR-B（208M）达到FID 1.92，均超越或匹敌更大规模的基线模型。

Conclusion: SphereAR有效解决了AR图像生成中的方差崩溃问题，首次使纯逐token、光栅顺序的自回归模型在同等参数规模下超过扩散和掩码生成模型。

Abstract: Autoregressive (AR) models are promising for image generation, yet
continuous-token AR variants often trail latent diffusion and masked-generation
models. The core issue is heterogeneous variance in VAE latents, which is
amplified during AR decoding, especially under classifier-free guidance (CFG),
and can cause variance collapse. We propose SphereAR to address this issue. Its
core design is to constrain all AR inputs and outputs -- including after CFG --
to lie on a fixed-radius hypersphere (constant $\ell_2$ norm), leveraging
hyperspherical VAEs. Our theoretical analysis shows that hyperspherical
constraint removes the scale component (the primary cause of variance
collapse), thereby stabilizing AR decoding. Empirically, on ImageNet
generation, SphereAR-H (943M) sets a new state of the art for AR models,
achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54
and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger
baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,
this is the first time a pure next-token AR image generator with raster order
surpasses diffusion and masked-generation models at comparable parameter
scales.

</details>


### [244] [Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA](https://arxiv.org/abs/2509.24350)
*Yan Ke,Xin Yu,Heming Du,Scott Chapman,Helen Huang*

Main category: cs.CV

TL;DR: 提出一种自反思、自改进的多智能体框架，用于解决多图像农业视觉问答中的上下文不足和推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对单图像或文本查询，难以应对需要多图像输入和实时外部知识的真实农业场景。

Method: 设计包含检索者、反思者、回答者和改进者的多智能体协作框架，实现上下文扩展、反思推理、并行应答生成与迭代优化。

Result: 在AgMMU基准测试中表现出色，显著提升多图像农业问答性能。

Conclusion: 该框架有效增强了农业视觉问答系统对复杂、不完整证据的适应能力，具备良好的应用前景。

Abstract: Agricultural visual question answering is essential for providing farmers and
researchers with accurate and timely knowledge. However, many existing
approaches are predominantly developed for evidence-constrained settings such
as text-only queries or single-image cases. This design prevents them from
coping with real-world agricultural scenarios that often require multi-image
inputs with complementary views across spatial scales, and growth stages.
Moreover, limited access to up-to-date external agricultural context makes
these systems struggle to adapt when evidence is incomplete. In addition, rigid
pipelines often lack systematic quality control. To address this gap, we
propose a self-reflective and self-improving multi-agent framework that
integrates four roles, the Retriever, the Reflector, the Answerer, and the
Improver. They collaborate to enable context enrichment, reflective reasoning,
answer drafting, and iterative improvement.
  A Retriever formulates queries and gathers external information, while a
Reflector assesses adequacy and triggers sequential reformulation and renewed
retrieval. Two Answerers draft candidate responses in parallel to reduce bias.
The Improver refines them through iterative checks while ensuring that
information from multiple images is effectively aligned and utilized.
Experiments on the AgMMU benchmark show that our framework achieves competitive
performance on multi-image agricultural QA.

</details>


### [245] [NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis](https://arxiv.org/abs/2509.24353)
*Yixuan Ren,Hanyu Wang,Hao Chen,Bo He,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: NeRV-Diffusion是一种隐式潜在视频扩散模型，通过生成神经网络权重来合成视频，将权重重组为卷积神经网络参数，形成隐式神经表示（INR），并以帧索引为输入解码为视频。


<details>
  <summary>Details</summary>
Motivation: 传统视频生成方法依赖逐帧特征图编码，存在效率低和跨帧注意力计算复杂的问题。NeRV-Diffusion旨在通过整体压缩和生成视频，提升生成效率和质量。

Method: 该框架包含两个阶段：1）基于超网络的tokenizer，将原始视频从像素空间编码到神经参数空间，瓶颈层的潜在变量作为INR权重；2）隐式扩散变换器，在潜在INR权重上进行去噪。采用高斯分布的INR权重设计、SNR自适应损失加权和调度采样策略进行有效训练。

Result: NeRV-Diffusion在UCF-101和Kinetics-600等真实视频基准上，视频生成质量优于以往基于INR的模型，并与最新的非隐式模型性能相当。同时实现了帧间或视频间的平滑插值。

Conclusion: NeRV-Diffusion通过将视频生成转化为神经网络权重生成，实现了高效、高质量的视频合成，避免了传统方法中的时序跨帧注意力机制，展现出优越的生成性能和平滑的隐空间特性。

Abstract: We present NeRV-Diffusion, an implicit latent video diffusion model that
synthesizes videos via generating neural network weights. The generated weights
can be rearranged as the parameters of a convolutional neural network, which
forms an implicit neural representation (INR), and decodes into videos with
frame indices as the input. Our framework consists of two stages: 1) A
hypernetworkbased tokenizer that encodes raw videos from pixel space to neural
parameter space, where the bottleneck latent serves as INR weights to decode.
2) An implicit diffusion transformer that denoises on the latent INR weights.
In contrast to traditional video tokenizers that encode videos into frame-wise
feature maps, NeRV-Diffusion compresses and generates a video holistically as a
unified neural network. This enables efficient and high-quality video synthesis
via obviating temporal cross-frame attentions in the denoiser and decoding
video latent with dedicated decoders. To achieve Gaussian-distributed INR
weights with high expressiveness, we reuse the bottleneck latent across all
NeRV layers, as well as reform its weight assignment, upsampling connection and
input coordinates. We also introduce SNR-adaptive loss weighting and scheduled
sampling for effective training of the implicit diffusion model. NeRV-Diffusion
reaches superior video generation quality over previous INR-based models and
comparable performance to most recent state-of-the-art non-implicit models on
real-world video benchmarks including UCF-101 and Kinetics-600. It also brings
a smooth INR weight space that facilitates seamless interpolations between
frames or videos.

</details>


### [246] [An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation](https://arxiv.org/abs/2509.24358)
*Dayu Tan,Cheng Kong,Yansen Su,Hai Chen,Dongliang Yang,Junfeng Xia,Chunhou Zheng*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习网络LamFormer，用于多器官医学图像分割，结合线性注意力Mamba和改进的金字塔编码器，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用Transformer进行长距离依赖建模时计算成本高且局部细节提取不足。

Method: 设计了LamFormer网络，包含Linear Attention Mamba（LAM）模块、增强型金字塔编码器、并行分层特征聚合（PHFA）模块和简化版Transformer（RT）模块。

Result: 在七个复杂多样的数据集上超越现有分割方法，实现了性能与模型复杂度的良好平衡。

Conclusion: LamFormer有效解决了高计算成本和局部细节缺失问题，适用于精细的多器官分割任务。

Abstract: In the field of multi-organ medical image segmentation, recent methods
frequently employ Transformers to capture long-range dependencies from image
features. However, these methods overlook the high computational cost of
Transformers and their deficiencies in extracting local detailed information.
To address high computational costs and inadequate local detail information, we
reassess the design of feature extraction modules and propose a new
deep-learning network called LamFormer for fine-grained segmentation tasks
across multiple organs. LamFormer is a novel U-shaped network that employs
Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture
multi-scale long-range dependencies. We construct the Parallel Hierarchical
Feature Aggregation (PHFA) module to aggregate features from different layers
of the encoder, narrowing the semantic gap among features while filtering
information. Finally, we design the Reduced Transformer (RT), which utilizes a
distinct computational approach to globally model up-sampled features. RRT
enhances the extraction of detailed local information and improves the
network's capability to capture long-range dependencies. LamFormer outperforms
existing segmentation methods on seven complex and diverse datasets,
demonstrating exceptional performance. Moreover, the proposed network achieves
a balance between model performance and model complexity.

</details>


### [247] [DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense](https://arxiv.org/abs/2509.24359)
*Amira Guesmi,Muhammad Shafique*

Main category: cs.CV

TL;DR: 本文提出了一种名为DRIFT的新方法，通过引入梯度不一致性来增强深度神经网络对对抗样本的鲁棒性，有效抵御多种攻击并在保持低计算开销的同时显著提升防御性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易受到对抗样本的影响，现有防御方法在梯度可估计时往往失效，因此需要一种能够从根本上降低对抗样本可迁移性的新机制。

Method: 作者提出了“梯度共识”概念，并设计了DRIFT——一个由轻量级可学习滤波器组成的随机集成模型，通过最大化雅可比空间和logit空间中的响应差异来破坏梯度共识，同时保持正常输入的预测一致性。

Result: DRIFT在ImageNet上的CNN和Vision Transformer模型上均表现出优异的鲁棒性，优于现有的预处理、对抗训练和基于扩散的防御方法，在自适应白盒、迁移和无梯度攻击下均有显著提升，且运行时间和内存开销极低。

Conclusion: DRIFT通过主动制造梯度不一致性，验证了梯度发散作为一种实用且可推广的对抗防御原则的有效性，为未来防御机制的设计提供了新方向。

Abstract: Deep neural networks remain highly vulnerable to adversarial examples, and
most defenses collapse once gradients can be reliably estimated. We identify
\emph{gradient consensus} -- the tendency of randomized transformations to
yield aligned gradients -- as a key driver of adversarial transferability.
Attackers exploit this consensus to construct perturbations that remain
effective across transformations. We introduce \textbf{DRIFT} (Divergent
Response in Filtered Transformations), a stochastic ensemble of lightweight,
learnable filters trained to actively disrupt gradient consensus. Unlike prior
randomized defenses that rely on gradient masking, DRIFT enforces
\emph{gradient dissonance} by maximizing divergence in Jacobian- and
logit-space responses while preserving natural predictions. Our contributions
are threefold: (i) we formalize gradient consensus and provide a theoretical
analysis linking consensus to transferability; (ii) we propose a
consensus-divergence training strategy combining prediction consistency,
Jacobian separation, logit-space separation, and adversarial robustness; and
(iii) we show that DRIFT achieves substantial robustness gains on ImageNet
across CNNs and Vision Transformers, outperforming state-of-the-art
preprocessing, adversarial training, and diffusion-based defenses under
adaptive white-box, transfer-based, and gradient-free attacks. DRIFT delivers
these improvements with negligible runtime and memory cost, establishing
gradient divergence as a practical and generalizable principle for adversarial
defense.

</details>


### [248] [UI-UG: A Unified MLLM for UI Understanding and Generation](https://arxiv.org/abs/2509.24361)
*Hao Yang,Weijie Qiu,Ru Zhang,Zhou Fang,Ruichao Mao,Xiaoyu Lin,Maji Huang,Zhaosong Huang,Teng Guo,Shuoyang Liu,Hai Rao*

Main category: cs.CV

TL;DR: 本文提出了UI-UG，一个用于用户界面理解和生成的统一多模态大语言模型，通过SFT、GRPO和DPO等方法在理解与生成任务上均取得SOTA性能，并展示了任务集成对双向提升的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在特定领域（如用户界面）的理解准确性和生成质量方面仍存在挑战。

Method: 采用监督微调（SFT）结合组相对策略优化（GRPO）提升UI理解能力；使用直接偏好优化（DPO）改进UI生成质量；并设计了一套工业级高效的工作流程，包括领域特定语言（DSL）、训练策略、渲染过程和评估指标。

Result: 在理解任务上达到SOTA，优于更大规模的通用MLLM和同规模专用模型；在生成任务上性能相当但计算成本更低；集成理解与生成任务可相互提升性能。

Conclusion: UI-UG通过统一架构和优化策略，在UI理解和生成任务中实现了高效且高质量的表现，具备实际工业应用价值。

Abstract: Although Multimodal Large Language Models (MLLMs) have been widely applied
across domains, they are still facing challenges in domain-specific tasks, such
as User Interface (UI) understanding accuracy and UI generation quality. In
this paper, we introduce UI-UG (a unified MLLM for UI Understanding and
Generation), integrating both capabilities. For understanding tasks, we employ
Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization
(GRPO) to enhance fine-grained understanding on the modern complex UI data. For
generation tasks, we further use Direct Preference Optimization (DPO) to make
our model generate human-preferred UIs. In addition, we propose an industrially
effective workflow, including the design of an LLM-friendly domain-specific
language (DSL), training strategies, rendering processes, and evaluation
metrics. In experiments, our model achieves state-of-the-art (SOTA) performance
on understanding tasks, outperforming both larger general-purpose MLLMs and
similarly-sized UI-specialized models. Our model is also on par with these
larger MLLMs in UI generation performance at a fraction of the computational
cost. We also demonstrate that integrating understanding and generation tasks
can improve accuracy and quality for both tasks.

</details>


### [249] [Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models](https://arxiv.org/abs/2509.24365)
*Jitai Hao,Hao Liu,Xinyan Xiao,Qiang Huang,Jun Yu*

Main category: cs.CV

TL;DR: 提出Uni-X架构，通过分离两端、共享中间层的X形设计解决多模态模型中视觉与文本的梯度冲突，显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 发现统一多模态模型在共享Transformer中存在严重的视觉与文本梯度冲突，尤其在浅层和深层，限制了模型训练效率与性能。

Method: 提出Uni-X，采用两端分离、中间共享的X形架构，初始和末尾层进行模态特异性处理，中间层共享参数以实现高层语义融合。

Result: 实验表明，Uni-X在相同训练条件下优于强基线，3B参数模型性能媲美或超越7B参数的自回归统一多模态模型，在GenEval图像生成任务上得分为82，并在文本与视觉理解任务中表现优异。

Conclusion: Uni-X有效缓解了多模态训练中的梯度冲突，是一种参数高效且可扩展的统一多模态建模基础架构。

Abstract: Unified Multimodal Models (UMMs) built on shared autoregressive (AR)
transformers are attractive for their architectural simplicity. However, we
identify a critical limitation: when trained on multimodal inputs,
modality-shared transformers suffer from severe gradient conflicts between
vision and text, particularly in shallow and deep layers. We trace this issue
to the fundamentally different low-level statistical properties of images and
text, while noting that conflicts diminish in middle layers where
representations become more abstract and semantically aligned. To overcome this
challenge, we propose Uni-X, a two-end-separated, middle-shared architecture.
Uni-X dedicates its initial and final layers to modality-specific processing,
while maintaining shared parameters in the middle layers for high-level
semantic fusion. This X-shaped design not only eliminates gradient conflicts at
both ends but also further alleviates residual conflicts in the shared layers.
Extensive experiments validate the effectiveness of Uni-X. Under identical
training conditions, Uni-X achieves superior training efficiency compared to
strong baselines. When scaled to 3B parameters with larger training data, Uni-X
matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for
image generation alongside strong performance in text and vision understanding
tasks. These results establish Uni-X as a parameter-efficient and scalable
foundation for future unified multimodal modeling. Our code is available at
https://github.com/CURRENTF/Uni-X

</details>


### [250] [Real-Aware Residual Model Merging for Deepfake Detection](https://arxiv.org/abs/2509.24367)
*Jinhee Park,Guisik Kim,Choongsang Cho,Junseok Kwon*

Main category: cs.CV

TL;DR: 提出了一种无需训练的参数空间模型融合框架R²M，用于高效应对快速演化的Deepfake检测，具有良好的泛化性和可组合性。


<details>
  <summary>Details</summary>
Motivation: Deepfake生成器快速演化，传统方法需重复收集数据和重新训练，不切实际；而现有模型合并方法在共享二元决策任务下未能充分利用各专家模型的共性。

Method: 提出Real-aware Residual Model Merging（R²M），通过低秩分解提取共享的Real成分，将各专家模型分解为Real对齐部分和Fake残差，利用逐层秩截断去噪残差，并通过任务级范数匹配聚合残差，实现训练-free的模型融合。

Result: R²M在分布内、跨数据集和未见数据集上均优于联合训练和其他合并基线，且具备可组合性：面对新伪造家族，仅需微调一个专家并重新合并，无需整体重训。

Conclusion: R²M是一种高效、可扩展的Deepfake检测框架，通过解耦真实特征与伪造残差，在无需重新训练的情况下持续集成新专家模型，显著提升检测鲁棒性与实用性。

Abstract: Deepfake generators evolve quickly, making exhaustive data collection and
repeated retraining impractical. We argue that model merging is a natural fit
for deepfake detection: unlike generic multi-task settings with disjoint
labels, deepfake specialists share the same binary decision and differ in
generator-specific artifacts. Empirically, we show that simple weight averaging
preserves Real representations while attenuating Fake-specific cues. Building
upon these findings, we propose Real-aware Residual Model Merging (R$^2$M), a
training-free parameter-space merging framework. R$^2$M estimates a shared Real
component via a low-rank factorization of task vectors, decomposes each
specialist into a Real-aligned part and a Fake residual, denoises residuals
with layerwise rank truncation, and aggregates them with per-task norm matching
to prevent any single generator from dominating. A concise rationale explains
why a simple head suffices: the Real component induces a common separation
direction in feature space, while truncated residuals contribute only minor
off-axis variations. Across in-distribution, cross-dataset, and unseen-dataset,
R$^2$M outperforms joint training and other merging baselines. Importantly,
R$^2$M is also composable: when a new forgery family appears, we fine-tune one
specialist and re-merge, eliminating the need for retraining.

</details>


### [251] [From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis](https://arxiv.org/abs/2509.24369)
*Khawlah Bajbaa,Abbas Anwar,Muhammad Saqib,Hafeez Anwar,Nabin Sharma,Muhammad Usman*

Main category: cs.CV

TL;DR: 提出一种融合扩散模型和条件生成对抗网络的混合框架，从卫星图像生成地理一致的街景图像，在CVUSA数据集上表现优于纯扩散方法，并与最先进的GAN方法相当。


<details>
  <summary>Details</summary>
Motivation: 由于卫星图像与街景图像在外观和视角上存在显著差异，直接合成高质量、地理一致的街景图像具有挑战性，因此需要更强大的生成模型。

Method: 采用多阶段训练策略，结合Stable Diffusion和条件GAN，设计双分支架构与融合策略，以提升生成图像的几何一致性和视觉质量。

Result: 在CVUSA数据集上，该方法在多个指标上优于仅使用扩散模型的方法，生成的街景图像具有高 realism、良好的地理一致性及丰富的细节（如道路标线、次级道路和云层等）。

Conclusion: 所提出的混合框架有效结合了扩散模型和GAN的优势，能够从卫星图像生成高质量、几何一致的全景街景图像，为跨视角图像合成提供了新的解决方案。

Abstract: Street view imagery has become an essential source for geospatial data
collection and urban analytics, enabling the extraction of valuable insights
that support informed decision-making. However, synthesizing street-view images
from corresponding satellite imagery presents significant challenges due to
substantial differences in appearance and viewing perspective between these two
domains. This paper presents a hybrid framework that integrates diffusion-based
models and conditional generative adversarial networks to generate
geographically consistent street-view images from satellite imagery. Our
approach uses a multi-stage training strategy that incorporates Stable
Diffusion as the core component within a dual-branch architecture. To enhance
the framework's capabilities, we integrate a conditional Generative Adversarial
Network (GAN) that enables the generation of geographically consistent
panoramic street views. Furthermore, we implement a fusion strategy that
leverages the strengths of both models to create robust representations,
thereby improving the geometric consistency and visual quality of the generated
street-view images. The proposed framework is evaluated on the challenging
Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image
synthesis. Experimental results demonstrate that our hybrid approach
outperforms diffusion-only methods across multiple evaluation metrics and
achieves competitive performance compared to state-of-the-art GAN-based
methods. The framework successfully generates realistic and geometrically
consistent street-view images while preserving fine-grained local details,
including street markings, secondary roads, and atmospheric elements such as
clouds.

</details>


### [252] [DINOReg: Strong Point Cloud Registration with Vision Foundation Model](https://arxiv.org/abs/2509.24370)
*Congjia Chen,Yufu Qu*

Main category: cs.CV

TL;DR: 本文提出DINOReg，一种充分利用视觉和几何信息的点云配准网络。通过引入DINOv2从图像中提取丰富的纹理与语义特征，并在patch级别融合视觉与几何特征，结合混合位置编码，显著提升了配准性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖几何信息，或虽引入颜色但未充分挖掘图像中的纹理与语义信息，且特征融合方式有损，限制了性能。

Method: 采用DINOv2提取图像视觉特征，在patch级别融合视觉与几何特征，并设计混合位置编码以统一图像与点云空间的位置信息。

Result: 在RGBD-3DMatch和RGBD-3DLoMatch数据集上实验表明，相比现有最先进方法，patch内点率提升14.2%，配准召回率提升15.7%。

Conclusion: DINOReg有效融合视觉与几何模态，在多模态点云配准中展现出优越性能，验证了利用视觉基础模型提升配准效果的潜力。

Abstract: Point cloud registration is a fundamental task in 3D computer vision. Most
existing methods rely solely on geometric information for feature extraction
and matching. Recently, several studies have incorporated color information
from RGB-D data into feature extraction. Although these methods achieve
remarkable improvements, they have not fully exploited the abundant texture and
semantic information in images, and the feature fusion is performed in an
image-lossy manner, which limit their performance. In this paper, we propose
DINOReg, a registration network that sufficiently utilizes both visual and
geometric information to solve the point cloud registration problem. Inspired
by advances in vision foundation models, we employ DINOv2 to extract
informative visual features from images, and fuse visual and geometric features
at the patch level. This design effectively combines the rich texture and
global semantic information extracted by DINOv2 with the detailed geometric
structure information captured by the geometric backbone. Additionally, a mixed
positional embedding is proposed to encode positional information from both
image space and point cloud space, which enhances the model's ability to
perceive spatial relationships between patches. Extensive experiments on the
RGBD-3DMatch and RGBD-3DLoMatch datasets demonstrate that our method achieves
significant improvements over state-of-the-art geometry-only and multi-modal
registration methods, with a 14.2% increase in patch inlier ratio and a 15.7%
increase in registration recall. The code is publicly available at
https://github.com/ccjccjccj/DINOReg.

</details>


### [253] [Mask Clustering-based Annotation Engine for Large-Scale Submeter Land Cover Mapping](https://arxiv.org/abs/2509.24374)
*Hao Chen,Fang Xu,Tamer Saleh,Weifeng Hao,Gui-Song Xia*

Main category: cs.CV

TL;DR: 提出了一种基于掩码聚类的注释引擎（MCAE），用于高效生成亚米级分辨率遥感影像的城市级土地覆盖标注数据集HiCity-LC，显著提升标注效率并保持高质量，支持大范围高精度土地覆盖制图。


<details>
  <summary>Details</summary>
Motivation: 现有亚米级遥感影像标注数据稀缺且成本高昂，传统方法依赖于低质量先验产品或耗时的手动标注，难以满足大规模高分辨率土地覆盖制图的需求。

Method: 基于空间自相关原理，将语义一致的掩码组作为最小标注单元，通过掩码聚类实现多个实例的同步标注，构建名为MCAE的自动化标注引擎。

Result: 实现了140亿像素的高质量标注数据集HiCity-LC，覆盖五个中国主要城市，分类精度超过85%，标注效率提升一到两个数量级。

Conclusion: MCAE为亚米级分辨率遥感影像的大规模土地覆盖制图提供了可扩展、高效的解决方案，HiCity-LC是首个公开的城市级亚米分辨率土地覆盖基准数据集。

Abstract: Recent advances in remote sensing technology have made submeter resolution
imagery increasingly accessible, offering remarkable detail for fine-grained
land cover analysis. However, its full potential remains underutilized -
particularly for large-scale land cover mapping - due to the lack of
sufficient, high-quality annotated datasets. Existing labels are typically
derived from pre-existing products or manual annotation, which are often
unreliable or prohibitively expensive, particularly given the rich visual
detail and massive data volumes of submeter imagery. Inspired by the spatial
autocorrelation principle, which suggests that objects of the same class tend
to co-occur with similar visual features in local neighborhoods, we propose the
Mask Clustering-based Annotation Engine (MCAE), which treats semantically
consistent mask groups as the minimal annotating units to enable efficient,
simultaneous annotation of multiple instances. It significantly improves
annotation efficiency by one to two orders of magnitude, while preserving label
quality, semantic diversity, and spatial representativeness. With MCAE, we
build a high-quality annotated dataset of about 14 billion labeled pixels,
referred to as HiCity-LC, which supports the generation of city-scale land
cover maps across five major Chinese cities with classification accuracies
above 85%. It is the first publicly available submeter resolution city-level
land cover benchmark, highlighting the scalability and practical utility of
MCAE for large-scale, submeter resolution mapping. The dataset is available at
https://github.com/chenhaocs/MCAE

</details>


### [254] [REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2509.24382)
*Soumyadeep Chandra,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文提出REALIGN，一种基于正则化融合部分Gromov-Wasserstein最优传输的自监督框架，用于从程序视频中学习表征，能有效处理无关帧、重复动作和非单调步骤顺序。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如OPEL）依赖Kantorovich最优传输，仅基于特征相似性，难以捕捉任务的高阶时间结构，且对背景、重复动作和非顺序步骤鲁棒性差。

Method: 提出REALIGN框架，采用R-FPGWOT联合建模视觉对应关系和时间关系，并结合序列间对比学习稳定训练过程，避免退化解。

Result: 在多个基准（EgoProceL、ProceL、CrossTask）上，REALIGN平均F1分数提升达18.9%，时间IoU提高超30%，并生成更可解释的传输图。

Conclusion: REALIGN通过联合优化视觉与时间对齐，在复杂程序视频中实现了更鲁棒、准确且可解释的自监督学习。

Abstract: Learning from procedural videos remains a core challenge in self-supervised
representation learning, as real-world instructional data often contains
background segments, repeated actions, and steps presented out of order. Such
variability violates the strong monotonicity assumptions underlying many
alignment methods. Prior state-of-the-art approaches, such as OPEL, leverage
Kantorovich Optimal Transport (KOT) to build frame-to-frame correspondences,
but rely solely on feature similarity and fail to capture the higher-order
temporal structure of a task. In this paper, we introduce REALIGN, a
self-supervised framework for procedure learning based on Regularized Fused
Partial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT,
our formulation jointly models visual correspondences and temporal relations
under a partial alignment scheme, enabling robust handling of irrelevant
frames, repeated actions, and non-monotonic step orders common in instructional
videos. To stabilize training, we integrate FPGWOT distances with
inter-sequence contrastive learning, avoiding the need for multiple
regularizers and preventing collapse to degenerate solutions. Across egocentric
(EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves
up to 18.9% average F1-score improvements and over 30% temporal IoU gains,
while producing more interpretable transport maps that preserve key-step
orderings and filter out noise.

</details>


### [255] [Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA](https://arxiv.org/abs/2509.24445)
*Jianxin Liang,Tan Yue,Yuxuan Wang,Yueqian Wang,Zhihan Yin,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种通过生成更丰富监督信号来提升视频问答（VideoQA）模型性能的新框架，采用问题导向的改写（QBP）和问题导向的描述生成（QBC）两种策略，将孤立的事实问答对转化为连贯叙述和细粒度视觉依据，从而实现更深层次的视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA模型依赖孤立的事实性问答对进行监督，缺乏对视频事件叙事和因果结构的理解，导致模型理解浅层化。因此需要一种能捕捉视频内在逻辑的新型监督方式。

Method: 提出QBP和QBC两种合成监督信号的方法：QBP将多个问答对重构为叙述性段落，还原事件结构；QBC为每个答案生成基于视频内容的细粒度视觉推理依据。利用大模型生成的数据，统一以next-token预测目标训练VideoQA模型。

Result: 在STAR和NExT-QA数据集上取得显著性能提升，3B模型在STAR上达到72.5%（+4.9%），7B模型在NExT-QA上达到80.8%，均为新SOTA。同时提升了跨数据集泛化能力，QBP使收敛速度加快2.5倍以上。

Conclusion: 将监督信号从孤立事实转向叙事一致性和有据可依的推理，能够构建更准确、高效且泛化的VideoQA训练范式。

Abstract: The performance of Video Question Answering (VideoQA) models is fundamentally
constrained by the nature of their supervision, which typically consists of
isolated, factual question-answer pairs. This "bag-of-facts" approach fails to
capture the underlying narrative and causal structure of events, limiting
models to a shallow understanding of video content. To move beyond this
paradigm, we introduce a framework to synthesize richer supervisory signals. We
propose two complementary strategies: Question-Based Paraphrasing (QBP), which
synthesizes the diverse inquiries (what, how, why) from a video's existing set
of question-answer pairs into a holistic narrative paragraph that reconstructs
the video's event structure; and Question-Based Captioning (QBC), which
generates fine-grained visual rationales, grounding the answer to each question
in specific, relevant evidence. Leveraging powerful generative models, we use
this synthetic data to train VideoQA models under a unified next-token
prediction objective. Extensive experiments on STAR and NExT-QA validate our
approach, demonstrating significant accuracy gains and establishing new
state-of-the-art results, such as improving a 3B model to 72.5\% on STAR
(+4.9\%) and a 7B model to 80.8\% on NExT-QA. Beyond accuracy, our analysis
reveals that both QBP and QBC substantially enhance cross-dataset
generalization, with QBP additionally accelerating model convergence by over
2.5x. These results demonstrate that shifting data synthesis from isolated
facts to narrative coherence and grounded rationales yields a more accurate,
efficient, and generalizable training paradigm.

</details>


### [256] [Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy](https://arxiv.org/abs/2509.24385)
*Haijier Chen,Bo Xu,Shoujian Zhang,Haoze Liu,Jiaxuan Lin,Jingrong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的3D多模态大语言模型Vid-LLM，无需依赖外部3D数据，通过几何先验和跨任务适配器提升3D场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D多模态大语言模型依赖3D输入数据，限制了其可扩展性和泛化能力，因此需要一种更实用、可部署于真实场景的解决方案。

Method: 提出Vid-LLM，利用视频输入结合几何先验；设计跨任务适配器（CTA）融合3D几何信息与视觉-语言表征；引入度量深度模型恢复真实尺度几何，并采用两阶段蒸馏策略进行优化训练。

Result: 在多个基准上验证了方法的有效性，在3D问答、3D密集描述和3D视觉定位任务中表现出优越的多任务性能。

Conclusion: Vid-LLM无需3D输入即可实现高效的3D场景理解，具备良好的实用性与扩展性，为3D多模态理解提供了新思路。

Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have
significantly improved Vision-Language (VL) reasoning in 2D domains. However,
extending these capabilities to 3D scene understanding remains a major
challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend
on 3D data inputs, which limits scalability and generalization. To address this
limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes
video inputs without requiring external 3D data, making it practical for
real-world deployment. In our method, the geometric prior are directly used to
improve the performance of the sceen perception. To integrate the geometric
cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to
align the 3D geometric priors with the vision-language representations. To
ensure geometric consistency and integrity, we introduce a Metric Depth Model
that recovers real-scale geometry from the reconstruction outputs. Finally, the
model is fine-tuned with a two-stage distillation optimization strategy,
realizing fast convergence and stabilizes training. Extensive experiments
across diverse benchmarks verified the effectiveness of our method on 3D
Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks,
demonstrating the superior multi-task capabilities.

</details>


### [257] [Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks](https://arxiv.org/abs/2509.24473)
*Shijie Lian,Changti Wu,Laurence Tianruo Yang,Hang Yuan,Bin Yu,Lei Zhang,Kai Chen*

Main category: cs.CV

TL;DR: 本文提出通过欧几里得几何问题求解作为代理任务，提升多模态大语言模型的空间智能。为此构建了包含约3万道几何题的Euclid30K数据集，并采用GRPO方法对Qwen2.5VL和RoboBrain2.0系列模型进行微调，使其具备识别形状、计数、关系推理和多步演绎能力。实验表明，微调后的模型在多个空间推理基准上零样本性能显著提升，其中RoboBrain2.0-Euclid-7B在VSI-Bench上达到49.6%的准确率，超越先前最优模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在空间智能方面表现不足，缺乏系统性训练机制来掌握如形状变换、空间关系推理等能力。因此，需要一种有效的方法来提升模型的通用空间推理能力。

Method: 构建了一个包含约30K平面与立体几何题的多模态数据集Euclid30K；采用Group Relative Policy Optimization（GRPO）对Qwen2.5VL和RoboBrain2.0系列模型进行微调，使其学习并应用欧几里得几何原理进行多步推理。

Result: 微调后的模型在Super-CLEVR、Omni3DBench、VSI-Bench和MindCube四个空间推理基准上实现了显著的零样本性能提升；所有模型在VSI-Bench上的平均准确率从34.5%提升至40.5%；RoboBrain2.0-Euclid-7B达到49.6%的准确率，超过此前SOTA模型Spatial-MLLM。

Conclusion: 这是首个系统性研究表明，以几何为中心的微调能够赋予视觉-语言模型广泛可迁移的空间推理能力，为提升多模态模型的空间智能提供了有效路径。

Abstract: Spatial intelligence spans a rich suite of abilities, including visualising
and transforming shapes, mentally rotating objects, judging relational
positions and containment, and estimating numerosity. However, it still remains
a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To
fill this gap, we propose to treat Euclidean geometry problem-solving as a
surrogate task. Specifically, we meticulously constructed a curated multimodal
dataset, called Euclid30K, comprising approximately 30K plane and solid
geometry problems. To enable the model to acquire and apply Euclidean
principles from these geometry problems, we employed Group Relative Policy
Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,
inspiring the models to identify shapes, count, and relate entities, and
perform multi-step deductive reasoning using Euclidean principles. Our
experiments demonstrate that the resulting models achieve substantial zero-shot
gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,
VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after
training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models
rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,
RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous
state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first
systematic study showing that geometry-centric fine-tuning can confer
vision-language models with broadly transferable spatial skills. Code and
Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.

</details>


### [258] [PCICF: A Pedestrian Crossing Identification and Classification Framework](https://arxiv.org/abs/2509.24386)
*Junyi Gu,Beatriz Cabrero-Daniel,Ali Nouri,Lydia Armini,Christian Berger*

Main category: cs.CV

TL;DR: 本文提出了一种名为PCICF的框架，用于系统识别和分类脆弱道路使用者（如行人）的交通场景，以支持自动驾驶出租车在特定运行设计域中的安全分析。作者基于合成数据集SMIRK构建了更丰富的MoreSMIRK数据集，并利用空间填充曲线（SFC）将多维场景特征转化为可匹配的模式，在真实世界数据集PIE上验证了该方法的有效性，能够准确识别复杂的行人过街行为，且具备车载实时应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着robotaxis在城市环境中的商业化部署，其必须可靠地检测脆弱道路使用者（VRUs）。现有方法在处理复杂、多变的交通场景时存在不足，尤其是在多行人交互场景中。因此，需要一个系统性的框架来识别和分类这些关键场景，以支持自动驾驶系统在操作设计域（ODD）之外的安全评估与训练。

Method: 提出PCICF框架，基于扩展的MoreSMIRK数据集（由原SMIRK数据集扩展而来，包含多行人过街场景），采用空间填充曲线（SFC）将多维场景特征映射为一维特征模式，进而实现对行人穿越行为的识别与分类，并在真实数据集PIE上进行评估。

Result: PCICF能够在真实视频数据中成功识别和分类复杂的行人穿越场景，包括行人组的合并与分离；实验表明该方法具有高准确性，并且由于使用了计算高效的SFC，具备在robotaxis上实时运行的潜力。

Conclusion: PCICF为自动驾驶系统提供了一种有效的工具，用于系统化分析脆弱道路使用者的行为场景，特别是在多行人交叉口环境中，可用于提升ODD内外的安全评估能力，并具备实际部署的可行性。

Abstract: We have recently observed the commercial roll-out of robotaxis in various
countries. They are deployed within an operational design domain (ODD) on
specific routes and environmental conditions, and are subject to continuous
monitoring to regain control in safety-critical situations. Since ODDs
typically cover urban areas, robotaxis must reliably detect vulnerable road
users (VRUs) such as pedestrians, bicyclists, or e-scooter riders. To better
handle such varied traffic situations, end-to-end AI, which directly compute
vehicle control actions from multi-modal sensor data instead of only for
perception, is on the rise. High quality data is needed for systematically
training and evaluating such systems within their OOD. In this work, we propose
PCICF, a framework to systematically identify and classify VRU situations to
support ODD's incident analysis. We base our work on the existing synthetic
dataset SMIRK, and enhance it by extending its single-pedestrian-only design
into the MoreSMIRK dataset, a structured dictionary of multi-pedestrian
crossing situations constructed systematically. We then use space-filling
curves (SFCs) to transform multi-dimensional features of scenarios into
characteristic patterns, which we match with corresponding entries in
MoreSMIRK. We evaluate PCICF with the large real-world dataset PIE, which
contains more than 150 manually annotated pedestrian crossing videos. We show
that PCICF can successfully identify and classify complex pedestrian crossings,
even when groups of pedestrians merge or split. By leveraging computationally
efficient components like SFCs, PCICF has even potential to be used onboard of
robotaxis for OOD detection for example. We share an open-source replication
package for PCICF containing its algorithms, the complete MoreSMIRK dataset and
dictionary, as well as our experiment results presented in:
https://github.com/Claud1234/PCICF

</details>


### [259] [RapidMV: Leveraging Spatio-Angular Representations for Efficient and Consistent Text-to-Multi-View Synthesis](https://arxiv.org/abs/2509.24410)
*Seungwook Kim,Yichun Shi,Kejie Li,Minsu Cho,Peng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RapidMV的新型文本到多视图生成模型，能够在约5秒内生成32个多视角合成图像，并通过新颖的时空角潜在空间提升效率和多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 为了高效生成具有高一致性的多视角合成图像，作为生成3D资产的重要桥梁。

Method: 提出一种新的时空角潜在空间，将空间外观和角度视角偏差编码到单一潜在表示中，并采用分阶段策略训练RapidMV。

Result: RapidMV在多视角一致性和延迟方面优于现有方法，同时保持了竞争力的图像质量和文本对齐能力。

Conclusion: RapidMV是一种高效且一致的文本到多视图图像生成模型，为合成3D资产提供了实用的解决方案。

Abstract: Generating synthetic multi-view images from a text prompt is an essential
bridge to generating synthetic 3D assets. In this work, we introduce RapidMV, a
novel text-to-multi-view generative model that can produce 32 multi-view
synthetic images in just around 5 seconds. In essence, we propose a novel
spatio-angular latent space, encoding both the spatial appearance and angular
viewpoint deviations into a single latent for improved efficiency and
multi-view consistency. We achieve effective training of RapidMV by
strategically decomposing our training process into multiple steps. We
demonstrate that RapidMV outperforms existing methods in terms of consistency
and latency, with competitive quality and text-image alignment.

</details>


### [260] [CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers](https://arxiv.org/abs/2509.24416)
*Kai Liu,Shaoqiu Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CLQ的跨层引导正交量化方法，用于扩散变换器（DiTs）的高效后训练量化，能够在W4A4精度下实现显著的内存节省和推理加速，同时几乎不损失视觉生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着扩散变换器（DiTs）模型规模和复杂度的增加，其在边缘设备上的部署受到内存和计算资源限制，亟需高效的模型压缩技术以支持实际应用。

Method: CLQ包含三个关键设计：1）跨块校准（CBC），通过更准确的校准数据提升量化指导效果；2）基于正交的平滑（OBS），利用块Hadamard矩阵对通道异常值进行低开销平滑；3）跨层参数搜索（CLPS），优化量化参数。

Result: 在图像和视频生成模型上验证了CLQ的有效性，实现了W4A4量化下的极小性能下降，获得3.98倍内存节省和3.95倍推理加速。

Conclusion: CLQ是一种高效的DiTs后训练量化方法，显著提升了模型在边缘设备上的部署可行性，同时保持了高质量的视觉生成能力。

Abstract: Visual generation quality has been greatly promoted with the rapid advances
in diffusion transformers (DiTs), which is attributed to the scaling of model
size and complexity. However, these attributions also hinder the practical
deployment of DiTs on edge devices, limiting their development and application.
Serve as an efficient model compression technique, model post-training
quantization (PTQ) can reduce the memory consumption and speed up the
inference, with inevitable performance degradation. To alleviate the
degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization
method for DiTs. To be specific, CLQ consists of three key designs. First, we
observe that the calibration data used by most of the PTQ methods can not
honestly represent the distribution of the activations. Therefore, we propose
cross-block calibration (CBC) to obtain accurate calibration data, with which
the quantization can be better guided. Second, we propose orthogonal-based
smoothing (OBS), which quantifies the outlier score of each channel and
leverages block Hadamard matrix to smooth the outliers with negligible
overhead. Third, we propose cross-layer parameter searching (CLPS) to search.
We evaluate CLQ with both image generation and video generation models and
successfully compress the model into W4A4 with negligible degradation in visual
quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our
code is available at
\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.

</details>


### [261] [NeMo: Needle in a Montage for Video-Language Understanding](https://arxiv.org/abs/2509.24563)
*Zi-Yuan Hu,Shuo Liang,Duo Zheng,Yanyang Li,Yeyao Tao,Shijia Huang,Wei Feng,Jia Qin,Jianguang Yu,Jing Huang,Meng Fang,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一个名为NeMo（Montage中的针）的新任务，用于评估视频大语言模型在复杂时序推理、长上下文回忆和时间定位方面的能力，并构建了自动化数据生成流程和包含超过3万QA对的NeMoBench基准。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型需要新的评估协议和基准来测试其在复杂时序推理方面的能力，而受LLM中‘针在 haystack 中’测试的启发，作者希望设计一种类似方法来评估模型的关键推理能力。

Method: 提出NeMo任务和自动化数据生成流程，基于该流程构建NeMoBench视频语言基准，包含13,486个不同长度的视频及其31,378个自动生成的问答对，并对20种最先进模型进行评估。

Result: 实验证明所提出的管道能可靠地自动生成高质量评估数据，NeMoBench可持续更新；对20个SOTA模型的评测提供了关于其能力与局限性的深入洞察。

Conclusion: NeMoBench为视频语言模型提供了一个可扩展、可持续更新的评估基准，有效推动了对模型时序理解与推理能力的深入研究。

Abstract: Recent advances in video large language models (VideoLLMs) call for new
evaluation protocols and benchmarks for complex temporal reasoning in
video-language understanding. Inspired by the needle in a haystack test widely
used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed
to assess VideoLLMs' critical reasoning capabilities, including long-context
recall and temporal grounding. To generate video question answering data for
our task, we develop a scalable automated data generation pipeline that
facilitates high-quality data synthesis. Built upon the proposed pipeline, we
present NeMoBench, a video-language benchmark centered on our task.
Specifically, our full set of NeMoBench features 31,378 automatically generated
question-answer (QA) pairs from 13,486 videos with various durations ranging
from seconds to hours. Experiments demonstrate that our pipeline can reliably
and automatically generate high-quality evaluation data, enabling NeMoBench to
be continuously updated with the latest videos. We evaluate 20 state-of-the-art
models on our benchmark, providing extensive results and key insights into
their capabilities and limitations. Our project page is available at:
https://lavi-lab.github.io/NeMoBench.

</details>


### [262] [A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models](https://arxiv.org/abs/2509.24420)
*Pei-Han Chen,Szu-Chi Chung*

Main category: cs.CV

TL;DR: 本研究系统评估了图像数据集质量对模型性能的影响，并提出了一种结合CleanVision和Fastdup工具的改进流程，通过自动阈值选择等增强方法显著提升了低质量图像检测和去重的F1分数。


<details>
  <summary>Details</summary>
Motivation: 随着模型架构的成熟，数据质量成为影响机器学习性能的关键因素，但图像领域中关于数据集质量评估的系统性研究仍然有限。

Method: 基于CIFAKE数据集识别常见质量问题，分析并整合CleanVision与Fastdup工具，提出自动阈值选择和去重策略以提升检测性能。

Result: 在单一扰动下F1分数从0.6794提升至0.9468，双扰动下从0.7447提升至0.8557；去重F1分数从0.4576提升至0.7928。模型对模糊和严重下采样等遮挡关键特征的质量问题更为敏感。

Conclusion: 所提出的流程有效提升了图像数据集质量评估的准确性，为图像机器学习中的数据质量保障提供了可行方案。

Abstract: In machine learning, research has traditionally focused on model development,
with relatively less attention paid to training data. As model architectures
have matured and marginal gains from further refinements diminish, data quality
has emerged as a critical factor. However, systematic studies on evaluating and
ensuring dataset quality in the image domain remain limited.
  This study investigates methods for systematically assessing image dataset
quality and examines how various image quality factors influence model
performance. Using the publicly available and relatively clean CIFAKE dataset,
we identify common quality issues and quantify their impact on training.
Building on these findings, we develop a pipeline that integrates two
community-developed tools, CleanVision and Fastdup. We analyze their underlying
mechanisms and introduce several enhancements, including automatic threshold
selection to detect problematic images without manual tuning.
  Experimental results demonstrate that not all quality issues exert the same
level of impact. While convolutional neural networks show resilience to certain
distortions, they are particularly vulnerable to degradations that obscure
critical visual features, such as blurring and severe downscaling. To assess
the performance of existing tools and the effectiveness of our proposed
enhancements, we formulate the detection of low-quality images as a binary
classification task and use the F1 score as the evaluation metric. Our
automatic thresholding method improves the F1 score from 0.6794 to 0.9468 under
single perturbations and from 0.7447 to 0.8557 under dual perturbations. For
near-duplicate detection, our deduplication strategy increases the F1 score
from 0.4576 to 0.7928. These results underscore the effectiveness of our
workflow and provide a foundation for advancing data quality assessment in
image-based machine learning.

</details>


### [263] [Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh](https://arxiv.org/abs/2509.24421)
*Yuanyuan Gao,Yuning Gong,Yifei Liu,Li Jingfeng,Zhihang Zhong,Dingwen Zhang,Yanci Zhang,Dan Xu,Xiao Sun*

Main category: cs.CV

TL;DR: 本文提出了Proxy-GS，一种通过引入代理机制实现高斯溅射的遮挡感知方法，显著提升了渲染效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯溅射方法在大规模场景中存在计算开销大和遮挡感知不足的问题，导致冗余和渲染质量下降。

Method: 提出了一种快速代理系统，生成精确的遮挡深度图，并用于引导高斯点的剔除和训练过程中的密度化。

Result: 在严重遮挡的场景下（如MatrixCity Streets数据集），Proxy-GS相比Octree-GS实现了超过2.5倍的速度提升，并显著提高了渲染质量。

Conclusion: Proxy-GS通过引入遮挡感知机制，在加速渲染的同时改善了MLP-based高斯溅射的视觉保真度，适用于大规模复杂场景。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for
achieving photorealistic rendering. Recent MLP-based variants further improve
visual fidelity but introduce substantial decoding overhead during rendering.
To alleviate computation cost, several pruning strategies and level-of-detail
(LOD) techniques have been introduced, aiming to effectively reduce the number
of Gaussian primitives in large-scale scenes. However, our analysis reveals
that significant redundancy still remains due to the lack of occlusion
awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a
proxy to introduce Gaussian occlusion awareness from any view. At the core of
our approach is a fast proxy system capable of producing precise occlusion
depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles:
first, it guides the culling of anchors and Gaussians to accelerate rendering
speed. Second, it guides the densification towards surfaces during training,
avoiding inconsistencies in occluded regions, and improving the rendering
quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset,
Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering
capability but also achieves faster rendering speed. Specifically, it achieves
more than 2.5x speedup over Octree-GS, and consistently delivers substantially
higher rendering quality. Code will be public upon acceptance.

</details>


### [264] [Rethinking Unsupervised Cross-modal Flow Estimation: Learning from Decoupled Optimization and Consistency Constraint](https://arxiv.org/abs/2509.24423)
*Runmin Zhang,Jialiang Wang,Si-Yuan Cao,Zhu Yu,Junchen Yu,Guangyi Zhang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DCFlow的无监督跨模态光流估计框架，结合解耦优化策略和跨模态一致性约束，在无需真实光流标签的情况下实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖外观相似性隐式学习光流，难以应对模态差异和几何错位问题，因此需要一种能够显式解决这些挑战的跨模态光流估计算法。

Method: 提出解耦优化策略，协同训练模态转换网络和光流估计网络；设计几何感知的数据合成流程与抗 outlier 损失以提供可靠的运动监督；引入跨模态一致性约束联合优化两个网络。

Result: 在自建的跨模态光流基准上实验表明，DCFlow可集成到多种光流网络中，并在无监督方法中达到最先进的性能。

Conclusion: DCFlow通过解耦优化和跨模态一致性有效解决了跨模态光流估计中的模态差异与几何错位问题，显著提升了无监督条件下的光流预测精度。

Abstract: This work presents DCFlow, a novel unsupervised cross-modal flow estimation
framework that integrates a decoupled optimization strategy and a cross-modal
consistency constraint. Unlike previous approaches that implicitly learn flow
estimation solely from appearance similarity, we introduce a decoupled
optimization strategy with task-specific supervision to address modality
discrepancy and geometric misalignment distinctly. This is achieved by
collaboratively training a modality transfer network and a flow estimation
network. To enable reliable motion supervision without ground-truth flow, we
propose a geometry-aware data synthesis pipeline combined with an
outlier-robust loss. Additionally, we introduce a cross-modal consistency
constraint to jointly optimize both networks, significantly improving flow
prediction accuracy. For evaluation, we construct a comprehensive cross-modal
flow benchmark by repurposing public datasets. Experimental results demonstrate
that DCFlow can be integrated with various flow estimation networks and
achieves state-of-the-art performance among unsupervised approaches.

</details>


### [265] [UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark](https://arxiv.org/abs/2509.24427)
*Ailing Zhang,Lina Lei,Dehong Kong,Zhixin Wang,Jiaqi Xu,Fenglong Song,Chun-Le Guo,Chang Liu,Fan Li,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了UI2V-Bench，一个专注于图像到视频生成模型在语义理解和推理能力方面评估的新基准，填补了现有评价体系的空白。


<details>
  <summary>Details</summary>
Motivation: 现有I2V生成模型的评估主要关注视频质量和时间一致性，忽视了对输入图像语义理解及物理常识和人类常识一致性的评估。

Method: 提出UI2V-Bench，包含空间理解、属性绑定、类别理解和推理四个维度，并设计基于多模态大语言模型的实例级评估流程和反馈式推理评估流程。

Result: 构建了约500个图文对的数据集，评估了多个开源与闭源I2V模型，MLLM-based指标与人工评估结果高度一致。

Conclusion: UI2V-Bench有效弥补了I2V生成模型在语义理解和推理能力评估方面的缺失，为未来研究提供了可靠框架和数据支持。

Abstract: Generative diffusion models are developing rapidly and attracting increasing
attention due to their wide range of applications. Image-to-Video (I2V)
generation has become a major focus in the field of video synthesis. However,
existing evaluation benchmarks primarily focus on aspects such as video quality
and temporal consistency, while largely overlooking the model's ability to
understand the semantics of specific subjects in the input image or to ensure
that the generated video aligns with physical laws and human commonsense. To
address this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V
models with a focus on semantic understanding and reasoning. It introduces four
primary evaluation dimensions: spatial understanding, attribute binding,
category understanding, and reasoning. To assess these dimensions, we design
two evaluation methods based on Multimodal Large Language Models (MLLMs): an
instance-level pipeline for fine-grained semantic understanding, and a
feedback-based reasoning pipeline that enables step-by-step causal assessment
for more accurate evaluation. UI2V-Bench includes approximately 500 carefully
constructed text-image pairs and evaluates a range of both open source and
closed-source I2V models across all defined dimensions. We further incorporate
human evaluations, which show strong alignment with the proposed MLLM-based
metrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by
emphasizing semantic comprehension and reasoning ability, offering a robust
framework and dataset to support future research and model development in the
field.

</details>


### [266] [NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding](https://arxiv.org/abs/2509.24441)
*Yanpeng Zhao,Shanyan Guan,Yunbo Wang,Yanhao Ge,Wei Li,Xiaokang Yang*

Main category: cs.CV

TL;DR: NeoWorld是一个从单张图像生成交互式3D虚拟世界的深度学习框架，采用混合2D/3D场景结构，在保证效率的同时实现高质量的视觉渲染和自然语言驱动的物体控制。


<details>
  <summary>Details</summary>
Motivation: 受科幻小说《Simulacron-3》中按需构建世界概念的启发，旨在解决现有方法在生成大规模虚拟环境时效率与视觉真实性难以兼顾的问题。

Method: 提出一种混合场景结构，将前景关键物体用基于对象的3D表示建模，背景和非交互区域则以2D合成；结合前沿的表征学习与物体到3D技术，支持灵活视角操控和物理合理的动画，并通过自然语言指令控制物体外观与动态。

Result: 在WorldScore基准上显著优于现有的2D和2.5D分层方法，能够随着用户交互逐步展开高细节3D环境，提供动态、沉浸且视觉连贯的探索体验。

Conclusion: NeoWorld通过融合3D对象建模与2D背景合成，在效率与 realism 之间取得良好平衡，推动了单图生成交互式虚拟世界的发展。

Abstract: We introduce NeoWorld, a deep learning framework for generating interactive
3D virtual worlds from a single input image. Inspired by the on-demand
worldbuilding concept in the science fiction novel Simulacron-3 (1964), our
system constructs expansive environments where only the regions actively
explored by the user are rendered with high visual realism through
object-centric 3D representations. Unlike previous approaches that rely on
global world generation or 2D hallucination, NeoWorld models key foreground
objects in full 3D, while synthesizing backgrounds and non-interacted regions
in 2D to ensure efficiency. This hybrid scene structure, implemented with
cutting-edge representation learning and object-to-3D techniques, enables
flexible viewpoint manipulation and physically plausible scene animation,
allowing users to control object appearance and dynamics using natural language
commands. As users interact with the environment, the virtual world
progressively unfolds with increasing 3D detail, delivering a dynamic,
immersive, and visually coherent exploration experience. NeoWorld significantly
outperforms existing 2D and depth-layered 2.5D methods on the WorldScore
benchmark.

</details>


### [267] [MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment](https://arxiv.org/abs/2509.24888)
*Fankai Jia,Daisong Gan,Zhe Zhang,Zhaochi Wen,Chenchen Dan,Dong Liang,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出了一种结合多模态大语言模型与信号处理的MRI质量评估框架MMRQA，在多个基准上实现了最先进的性能，具有强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统MRI质量评估方法在语义理解与可解释性之间存在权衡，且受限于数据稀缺和协议变异。

Method: 结合MRQy增强的鲁棒指标提取、Qwen生成的结构化问答对转换，以及LLaVA-OneVision的低秩适应（LoRA）进行参数高效融合。

Result: 在MR-ART、FastMRI和MyConnectome基准上达到最先进水平，具备强零样本泛化能力和临床可解释输出。

Conclusion: MMRQA通过融合定量分析与语义推理，提升了动态医疗环境中的MRI质量控制效果。

Abstract: Magnetic resonance imaging (MRI) quality assessment is crucial for clinical
decision-making, yet remains challenging due to data scarcity and protocol
variability. Traditional approaches face fundamental trade-offs: signal-based
methods like MRIQC provide quantitative metrics but lack semantic
understanding, while deep learning approaches achieve high accuracy but
sacrifice interpretability. To address these limitations, we introduce the
Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration
of multimodal large language models (MLLMs) with acquisition-aware signal
processing. MMRQA combines three key innovations: robust metric extraction via
MRQy augmented with simulated artifacts, structured transformation of metrics
into question-answer pairs using Qwen, and parameter-efficient fusion through
Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI,
and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with
strong zero-shot generalization, as validated by comprehensive ablation
studies. By bridging quantitative analysis with semantic reasoning, our
framework generates clinically interpretable outputs that enhance quality
control in dynamic medical settings.

</details>


### [268] [Generalist Multi-Class Anomaly Detection via Distillation to Two Heterogeneous Student Networks](https://arxiv.org/abs/2509.24448)
*Hangil Park,Yongmin Seo,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种基于知识蒸馏的双模型集成方法，通过结合Encoder-Decoder和Encoder-Encoder模型，在工业与语义异常检测上均实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在工业检测或语义异常检测之间泛化能力差，且对数据集特定设置敏感，难以兼顾多类与单类任务。

Method: 采用共享DINOv2编码器的双学生模型架构：一个Encoder-Decoder模型用于检测局部缺陷，另一个Encoder-Encoder模型用于语义异常检测；通过知识蒸馏和Noisy-OR联合学习目标融合两个模型的局部与语义异常得分。

Result: 在八个公开基准（包括MVTec-AD、CIFAR-10等）上验证，多类和单类设置下均达到最先进水平，MVTec-AD图像级AUROC达99.7%，CIFAR-10达97.8%。

Conclusion: 该双模型集成框架有效桥接了工业与语义异常检测之间的差距，具备强泛化能力，优于通用及专用模型。

Abstract: Anomaly detection (AD) plays an important role in various real-world
applications. Recent advancements in AD, however, are often biased towards
industrial inspection, struggle to generalize to broader tasks like semantic
anomaly detection and vice versa. Although recent methods have attempted to
address general anomaly detection, their performance remains sensitive to
dataset-specific settings and single-class tasks. In this paper, we propose a
novel dual-model ensemble approach based on knowledge distillation (KD) to
bridge this gap. Our framework consists of a teacher and two student models: an
Encoder-Decoder model, specialized in detecting patch-level minor defects for
industrial AD and an Encoder-Encoder model, optimized for semantic AD. Both
models leverage a shared pre-trained encoder (DINOv2) to extract high-quality
feature representations. The dual models are jointly learned using the Noisy-OR
objective, and the final anomaly score is obtained using the joint probability
via local and semantic anomaly scores derived from the respective models. We
evaluate our method on eight public benchmarks under both single-class and
multi-class settings: MVTec-AD, MVTec-LOCO, VisA and Real-IAD for industrial
inspection and CIFAR-10/100, FMNIST and View for semantic anomaly detection.
The proposed method achieved state-of-the-art accuracies in both domains, in
multi-class as well as single-class settings, demonstrating generalization
across multiple domains of anomaly detection. Our model achieved an image-level
AUROC of 99.7% on MVTec-AD and 97.8% on CIFAR-10, which is significantly better
than the prior general AD models in multi-class settings and even higher than
the best specialist models on individual benchmarks.

</details>


### [269] [LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation](https://arxiv.org/abs/2509.24469)
*Heechang Kim,Gwanghyun Kim,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出了一种基于拉班动作分析（Laban Effort and Shape）的可解释、表达性强的人体运动生成控制方法，通过在推理时优化预训练扩散模型的文本嵌入，实现了无需额外数据的多样化运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到运动生成模型在细粒度表达性控制方面存在挑战，主要由于数据集中缺乏运动风格多样性以及自然语言难以准确描述定量运动特征。因此，需要一种更精确、可解释的运动控制方法。

Method: 将拉班努力与形态成分的量化方法融入文本引导的运动生成模型，提出一种零样本、推理时优化的方法，在采样过程中更新预训练扩散模型的文本嵌入，以实现对运动质量的精细控制。

Result: 该方法能够在不使用额外运动数据的情况下，根据目标拉班标签操控运动属性，生成具有多样化表达性质量的运动，同时保持运动身份的一致性。

Conclusion: 所提出的方法实现了对人体运动生成的可解释且精细的控制，显著提升了生成运动的表达性和多样性，为文本到运动合成提供了新的思路。

Abstract: Diverse human motion generation is an increasingly important task, having
various applications in computer vision, human-computer interaction and
animation. While text-to-motion synthesis using diffusion models has shown
success in generating high-quality motions, achieving fine-grained expressive
motion control remains a significant challenge. This is due to the lack of
motion style diversity in datasets and the difficulty of expressing
quantitative characteristics in natural language. Laban movement analysis has
been widely used by dance experts to express the details of motion including
motion quality as consistent as possible. Inspired by that, this work aims for
interpretable and expressive control of human motion generation by seamlessly
integrating the quantification methods of Laban Effort and Shape components
into the text-guided motion generation models. Our proposed zero-shot,
inference-time optimization method guides the motion generation model to have
desired Laban Effort and Shape components without any additional motion data by
updating the text embedding of pretrained diffusion models during the sampling
step. We demonstrate that our approach yields diverse expressive motion
qualities while preserving motion identity by successfully manipulating motion
attributes according to target Laban tags.

</details>


### [270] [Performance-Efficiency Trade-off for Fashion Image Retrieval](https://arxiv.org/abs/2509.24477)
*Julio Hurtado,Haoran Ni,Duygu Sap,Connor Mattinson,Martin Lotz*

Main category: cs.CV

TL;DR: 本文提出了一种选择性表示框架，通过聚类、核心集选择和基于邻域同质性一致性的异常值去除方法，显著缩小了二手服装图像数据库的规模（降至原始大小的10%），同时保持检索精度，有效提升了检索系统的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 时尚产业产生大量废弃物和排放，推动二手市场发展。机器学习有助于大规模评估二手服装价值，但现有方法在图像检索的可扩展性方面存在挑战，需提高效率并降低成本。

Method: 引入一种选择性表示框架：首先采用聚类和核心集选择方法识别能代表每件服装及其内部变异的关键样本；其次提出一种基于邻域同质性一致性得分的高效异常值去除方法，在选择前过滤非典型样本。

Result: 在DeepFashion Attribute、Con2Shop和DeepFashion2三个公开数据集上验证，该方法将数据库缩减至10%而未牺牲检索准确性；结合异常值去除的聚类技术进一步提升了检索性能，显著降低了计算成本。

Conclusion: 所提出的框架在保持近似最优检索精度的同时，大幅减少了向量数据库中的图像数量，实现了良好的性能-效率权衡，为可扩展的二手服装图像检索提供了有效解决方案。

Abstract: The fashion industry has been identified as a major contributor to waste and
emissions, leading to an increased interest in promoting the second-hand
market. Machine learning methods play an important role in facilitating the
creation and expansion of second-hand marketplaces by enabling the large-scale
valuation of used garments. We contribute to this line of work by addressing
the scalability of second-hand image retrieval from databases. By introducing a
selective representation framework, we can shrink databases to 10% of their
original size without sacrificing retrieval accuracy. We first explore
clustering and coreset selection methods to identify representative samples
that capture the key features of each garment and its internal variability.
Then, we introduce an efficient outlier removal method, based on a
neighbour-homogeneity consistency score measure, that filters out
uncharacteristic samples prior to selection. We evaluate our approach on three
public datasets: DeepFashion Attribute, DeepFashion Con2Shop, and DeepFashion2.
The results demonstrate a clear performance-efficiency trade-off by
strategically pruning and selecting representative vectors of images. The
retrieval system maintains near-optimal accuracy, while greatly reducing
computational costs by reducing the images added to the vector database.
Furthermore, applying our outlier removal method to clustering techniques
yields even higher retrieval performance by removing non-discriminative samples
before the selection.

</details>


### [271] [Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs](https://arxiv.org/abs/2509.24491)
*Yuanshuai Li,Yuping Yan,Junfeng Tang,Yunxuan Li,Zeqi Zheng,Yaochu Jin*

Main category: cs.CV

TL;DR: 提出了一种新的多模态大语言模型对齐框架SCPO，通过语义课程偏好优化有效减少视觉幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型存在视觉幻觉问题，且DPO方法难以捕捉细粒度语义差异并导致捷径学习。

Method: 构建了基于难度排序的细粒度语义对比数据集，并采用渐进式易到难课程学习、动态参考模型和对称双向目标进行训练。

Result: 在多个幻觉基准上显著优于基线模型，幻觉率最高降低62.9%，并在通用视觉-语言基准上保持稳定性能。

Conclusion: SCPO是首个将语义、对称性和课程学习统一用于MLLM对齐的框架，能有效缓解视觉幻觉问题。

Abstract: Multimodal Large Language Models (MLLMs) have significantly improved the
performance of various tasks, but continue to suffer from visual
hallucinations, a critical issue where generated responses contradict visual
evidence. While Direct Preference Optimization(DPO) is widely used for
alignment, its application to MLLMs often fails to capture fine-grained
semantic differences and encourages shortcut learning. To address these
challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a
novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard
curriculum built upon our Semantic Curriculum Preference Pairs dataset, which
provides fine-grained semantic contrasts sorted by difficulty. This curriculum
is trained with a dynamic reference model and a novel symmetric, bidirectional
objective to facilitate simultaneous learning from both textual and visual
preferences. To our knowledge, SCPO is the first framework to unify semantics,
symmetry, and curriculum for MLLMs alignment, effectively mitigating visual
hallucinations. Extensive experiments on LLaVA models across various scales and
versions validate that SCPO demonstrates superior performance compared to
baseline models on multiple hallucination benchmarks, reducing the
hallucination rate by up to 62.9%. Moreover, evaluations on generalized
benchmarks show that SCPO improves factuality while preserving general
capabilities, with its performance remaining stable across general
vision-language benchmarks.

</details>


### [272] [Robust Multimodal Semantic Segmentation with Balanced Modality Contributions](https://arxiv.org/abs/2509.24505)
*Jiaqi Tan,Xu Zheng,Fangyu Li,Yang Liu*

Main category: cs.CV

TL;DR: 提出EQUISeg，一种通过模态平等编码来平衡多模态语义分割中模态贡献的框架，有效缓解模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法常因主导模态退化导致性能显著下降，模态依赖不平衡成为实际应用中的关键挑战。

Method: 构建基于四阶段跨模态Transformer块（CMTB）的EQUISeg框架，并设计自引导模块（SGM）实现模态间的相互指导和自适应贡献调整。

Result: 在多个数据集上实验表明，EQUISeg在多模态分割任务中显著提升性能，并有效减轻模态不平衡带来的负面影响。

Conclusion: EQUISeg通过平等编码和自适应融合机制实现了更鲁棒的多模态语义分割，提升了模型在真实场景中的稳定性。

Abstract: Multimodal semantic segmentation enhances model robustness by exploiting
cross-modal complementarities. However, existing methods often suffer from
imbalanced modal dependencies, where overall performance degrades significantly
once a dominant modality deteriorates in real-world scenarios. Thus, modality
balance has become acritical challenge for practical multimodal segmentation.
To address this issue, we propose EQUISeg, a multimodal segmentation framework
that balances modality contributions through equal encoding of modalities.
Built upon a four-stage Cross-modal Transformer Block(CMTB), EQUISeg enables
efficient multimodal fusion and hierarchical selection. Furthermore, we design
a Self-guided Module(SGM) that mitigates modality imbalance by introducing a
mutual guidance mechanism, enabling each modality to adaptively adjust its
contribution and enhance robustness under degraded conditions. Extensive
experiments on multiple datasets demonstrate that EQUISeg achieves significant
performance gains and effectively alleviates the adverse effects of modality
imbalance in segmentation tasks.

</details>


### [273] [Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency](https://arxiv.org/abs/2509.24514)
*Jiaqi Tan,Fangyu Li,Yang Liu*

Main category: cs.CV

TL;DR: 提出QL-Adapter框架，用于解决指令驱动图像编辑中多物体场景下的数量和布局一致性问题。


<details>
  <summary>Details</summary>
Motivation: 标准CLIP文本编码器在复杂多物体场景中编辑效果不佳，需提升对物体数量、空间布局和类别多样性的控制能力。

Method: 设计两个核心模块：图像-布局融合模块（ILFM）融合布局先验与CLIP图像编码器的ViT块；跨模态增强模块（CMAM）将图像特征注入文本分支以增强文本嵌入。构建了涵盖数量、布局和类别变化的基准数据集QL-Dataset，并定义了数量与布局一致的图像编辑任务（QL-Edit）。

Result: 在QL-Edit任务上，QL-Adapter显著优于现有模型，达到最先进性能。

Conclusion: QL-Adapter通过引入布局感知和跨模态增强机制，有效提升了复杂场景下多物体图像编辑的准确性和一致性。

Abstract: Instruction driven image editing with standard CLIP text encoders often fails
in complex scenes with many objects. We present QL-Adapter, a framework for
multiple object editing that tackles two challenges: enforcing object counts
and spatial layouts, and accommodating diverse categories. QL-Adapter consists
of two core modules: the Image-Layout Fusion Module (ILFM) and the Cross-Modal
Augmentation Module (CMAM). ILFM fuses layout priors with ViT patch tokens from
the CLIP image encoder to strengthen spatial structure understanding. CMAM
injects image features into the text branch to enrich textual embeddings and
improve instruction following. We further build QL-Dataset, a benchmark that
spans broad category, layout, and count variations, and define the task of
quantity and layout consistent image editing (QL-Edit). Extensive experiments
show that QL-Adapter achieves state of the art performance on QL-Edit and
significantly outperforms existing models.

</details>


### [274] [CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models](https://arxiv.org/abs/2509.24526)
*Zheyuan Hu,Chieh-Hsin Lai,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 本文提出了mid-training的概念和方法，通过在预训练和最终训练之间插入一个轻量级的中间阶段（CMT），显著提升了流映射模型（如一致性模型）的训练稳定性与效率，在更少数据和计算资源下实现了SOTA的生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的流映射模型（如CM、MF）虽支持少步生成，但训练不稳定、对超参数敏感且成本高；即使使用预训练扩散模型初始化，仍需将微小步长转换为长跳转映射，无法根本解决不稳定性问题。

Method: 提出Consistency Mid-Training（CMT），在预训练模型生成的求解器轨迹上，训练模型直接从先验样本映射到干净样本，获得轨迹一致且稳定的初始化，用于后续流映射模型的训练。

Result: CMT在CIFAR-10、ImageNet 64x64和512x512上分别实现1.97、1.32和1.84的两步FID，训练数据和GPU时间最多减少98%；在ImageNet 256x256上以约50%训练时间达到1步FID 3.34，优于MF从零训练的结果（FID 3.43）。

Conclusion: CMT是一种原理清晰、高效且通用的流映射模型训练框架，解决了现有方法训练不稳定和高成本的问题，显著提升了少步生成模型的实用性和性能。

Abstract: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable
few-step generation by learning the long jump of the ODE solution of diffusion
models, yet training remains unstable, sensitive to hyperparameters, and
costly. Initializing from a pre-trained diffusion model helps, but still
requires converting infinitesimal steps into a long-jump map, leaving
instability unresolved. We introduce mid-training, the first concept and
practical method that inserts a lightweight intermediate stage between the
(diffusion) pre-training and the final flow map training (i.e., post-training)
for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact
and principled stage that trains a model to map points along a solver
trajectory from a pre-trained model, starting from a prior sample, directly to
the solver-generated clean sample. It yields a trajectory-consistent and stable
initialization. This initializer outperforms random and diffusion-based
baselines and enables fast, robust convergence without heuristics. Initializing
post-training with CMT weights further simplifies flow map learning.
Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10,
1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98%
less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT
reaches 1-step FID 3.34 while cutting total training time by about 50% compared
to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient,
and general framework for training flow map models.

</details>


### [275] [CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D](https://arxiv.org/abs/2509.24528)
*Mohamad Amin Mirzaei,Pantea Amoie,Ali Ekhterachian,Matin Mirzababaei*

Main category: cs.CV

TL;DR: 本文提出了一种改进的3D语义映射方法，通过SemanticSAM与渐进式粒度优化生成更精确的对象级掩码，并结合上下文感知的CLIP编码策略增强语义一致性，在多个3D场景理解任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型的零样本3D语义映射方法因直接使用原始2D掩码导致掩码碎片化和语义分配不准确，难以在复杂环境中有效工作。

Method: 采用SemanticSAM结合渐进式粒度细化生成高质量对象级掩码，并设计上下文感知的CLIP编码策略，融合多个视角的掩码信息并进行加权，以提升3D语义分割和语言查询物体检索性能。

Result: 在多个基准数据集上的实验表明，该方法在3D语义分割和语言驱动的物体检索任务中均显著优于现有方法，有效缓解了过分割问题并增强了语义一致性。

Conclusion: 所提出的方法通过改进掩码生成与上下文编码策略，显著提升了开放词汇、零样本3D语义映射的准确性与鲁棒性，适用于复杂环境下的具身AI与机器人应用。

Abstract: 3D scene understanding is fundamental for embodied AI and robotics,
supporting reliable perception for interaction and navigation. Recent
approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning
embedding vectors to 2D class-agnostic masks generated via vision-language
models (VLMs) and projecting these into 3D. However, these methods often
produce fragmented masks and inaccurate semantic assignments due to the direct
use of raw masks, limiting their effectiveness in complex environments. To
address this, we leverage SemanticSAM with progressive granularity refinement
to generate more accurate and numerous object-level masks, mitigating the
over-segmentation commonly observed in mask generation models such as vanilla
SAM, and improving downstream 3D semantic segmentation. To further enhance
semantic context, we employ a context-aware CLIP encoding strategy that
integrates multiple contextual views of each mask using empirically determined
weighting, providing much richer visual context. We evaluate our approach on
multiple 3D scene understanding tasks, including 3D semantic segmentation and
object retrieval from language queries, across several benchmark datasets.
Experimental results demonstrate significant improvements over existing
methods, highlighting the effectiveness of our approach.

</details>


### [276] [TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.25143)
*Junyi Zhang,Jia-Chen Gu,Wenbo Hu,Yu Zhou,Robinson Piramuthu,Nanyun Peng*

Main category: cs.CV

TL;DR: 本文提出了TemMed-Bench，首个用于分析患者在不同就诊时间点病情变化的医学视觉-语言模型基准，包含视觉问答、报告生成和图像对选择三项任务，并评估了12个大模型的表现，发现多数模型在时序医学图像推理上能力不足，而多模态检索增强可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学推理基准主要基于单次就诊的图像分析，与临床实践中医生依赖患者历史病历进行纵向评估的实际情况脱节，因此需要一个能评估模型对时序医学图像推理能力的新基准。

Method: 构建了TemMed-Bench基准，包含三个任务（VQA、报告生成、图像对选择）和超过17,000个实例的知识库；评估了6个闭源和6个开源视觉-语言大模型；探索了结合检索到的视觉和文本模态信息进行多模态检索增强的方法。

Result: 大多数LVLMs在时序医学图像变化分析上表现不佳，接近随机猜测水平；GPT-4o、GPT-4o-mini和Claude 3.5 Sonnet表现相对较好；多模态检索增强相比无检索或仅文本检索能带来更显著的性能提升，VQA任务平均提升2.59%。

Conclusion: TemMed-Bench填补了真实临床场景下时序医学图像推理评估的空白，揭示了当前大模型在此类任务上的局限性，并表明多模态检索增强是改进模型性能的一个有前景的方向。

Abstract: Existing medical reasoning benchmarks for vision-language models primarily
focus on analyzing a patient's condition based on an image from a single visit.
However, this setting deviates significantly from real-world clinical practice,
where doctors typically refer to a patient's historical conditions to provide a
comprehensive assessment by tracking their changes over time. In this paper, we
introduce TemMed-Bench, the first benchmark designed for analyzing changes in
patients' conditions between different clinical visits, which challenges large
vision-language models (LVLMs) to reason over temporal medical images.
TemMed-Bench consists of a test set comprising three tasks - visual
question-answering (VQA), report generation, and image-pair selection - and a
supplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we
conduct an evaluation of six proprietary and six open-source LVLMs. Our results
show that most LVLMs lack the ability to analyze patients' condition changes
over temporal medical images, and a large proportion perform only at a
random-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini
and Claude 3.5 Sonnet demonstrate comparatively decent performance, though they
have yet to reach the desired level. Furthermore, we explore augmenting the
input with both retrieved visual and textual modalities in the medical domain.
We also show that multi-modal retrieval augmentation yields notably higher
performance gains than no retrieval and textual retrieval alone across most
models on our benchmark, with the VQA task showing an average improvement of
2.59%. Overall, we compose a benchmark grounded on real-world clinical
practice, and it reveals LVLMs' limitations in temporal medical image
reasoning, as well as highlighting the use of multi-modal retrieval
augmentation as a potentially promising direction worth exploring to address
this challenge.

</details>


### [277] [Diffusion Bridge or Flow Matching? A Unifying Framework and Comparative Analysis](https://arxiv.org/abs/2509.24531)
*Kaizhen Zhu,Mokai Pan,Zhechuan Yu,Jingya Wang,Jingyi Yu,Ye Shi*

Main category: cs.CV

TL;DR: 本文首次从随机最优控制和最优传输的角度统一分析了Diffusion Bridge和Flow Matching两种模型，理论上证明Diffusion Bridge具有更低的代价函数和更稳定的轨迹，且在数据量减少时Flow Matching性能下降更明显。通过基于隐式Transformer的统一架构进行实验验证，在多种图像生成任务中结果与理论预测一致。


<details>
  <summary>Details</summary>
Motivation: 尽管Diffusion Bridge和Flow Matching在分布变换中表现良好，但二者在建模假设和实现上的差异导致缺乏统一的理论比较，难以判断其优劣。因此需要一个统一的理论框架来分析两者的性能差异。

Method: 将两种模型框架置于随机最优控制和最优传输理论下进行分析，推导其代价函数并比较性能差异；设计基于隐空间Transformer的统一网络结构，公平比较两者在多种任务和不同数据规模下的表现。

Result: 理论分析表明Diffusion Bridge的代价函数更低，路径更稳定；Flow Matching在小样本下因插值系数失效而性能下降。实验结果显示Diffusion Bridge在多数任务中优于Flow Matching，尤其在低数据量场景下优势更明显。

Conclusion: Diffusion Bridge在理论和实验上均优于Flow Matching，特别是在训练数据有限的情况下更具优势，研究为两种方法的选择提供了明确的指导依据。

Abstract: Diffusion Bridge and Flow Matching have both demonstrated compelling
empirical performance in transformation between arbitrary distributions.
However, there remains confusion about which approach is generally preferable,
and the substantial discrepancies in their modeling assumptions and practical
implementations have hindered a unified theoretical account of their relative
merits. We have, for the first time, provided a unified theoretical and
experimental validation of these two models. We recast their frameworks through
the lens of Stochastic Optimal Control and prove that the cost function of the
Diffusion Bridge is lower, guiding the system toward more stable and natural
trajectories. Simultaneously, from the perspective of Optimal Transport,
interpolation coefficients $t$ and $1-t$ of Flow Matching become increasingly
ineffective when the training data size is reduced. To corroborate these
theoretical claims, we propose a novel, powerful architecture for Diffusion
Bridge built on a latent Transformer, and implement a Flow Matching model with
the same structure to enable a fair performance comparison in various
experiments. Comprehensive experiments are conducted across Image Inpainting,
Super-Resolution, Deblurring, Denoising, Translation, and Style Transfer tasks,
systematically varying both the distributional discrepancy (different
difficulty) and the training data size. Extensive empirical results align
perfectly with our theoretical predictions and allow us to delineate the
respective advantages and disadvantages of these two models. Our code is
available at https://anonymous.4open.science/r/DBFM-3E8E/.

</details>


### [278] [GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts](https://arxiv.org/abs/2509.25160)
*Fan Yuan,Yuchen Yan,Yifan Jiang,Haoran Zhao,Tao Feng,Jinyan Chen,Yanwei Lou,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 本文提出了GSM8K-V，一个纯视觉的多图像数学推理基准，旨在解决现有视觉语言模型在几何以外数学问题和多图像推理上的不足。通过将文本基准GSM8K转化为高质量的视觉样本，作者构建了1,319个测试样例，并评估了多种模型的表现，发现当前模型在该基准上仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉数学推理基准多局限于几何问题，缺乏对数学应用题和多图像推理的覆盖，难以全面评估视觉语言模型的数学推理能力。因此，需要一个更全面、更具挑战性的基准来推动模型发展。

Method: 基于广泛使用的文本基准GSM8K，通过自动化图像生成流程结合人工标注，将其系统性地转换为视觉形式，构建出包含1,319个高质量样本的GSM8K-V数据集，并对多种开源与闭源视觉语言模型进行评估与分析。

Result: 实验结果显示，尽管现有模型在文本版GSM8K上表现接近饱和（如Gemini-2.5-Pro达95.22%准确率），但在GSM8K-V上性能显著下降（同一模型仅46.93%），暴露出当前模型在视觉数学理解与跨图像推理方面的严重不足。

Conclusion: GSM8K-V为视觉数学推理提供了新的评估视角，揭示了现有视觉语言模型在复杂视觉推理任务上的局限性，是一个有助于推动更鲁棒、更通用模型发展的有力基准。

Abstract: Vision language models (VLMs) achieve unified modeling of images and text,
enabling them to accomplish complex real-world tasks through perception,
planning, and reasoning. Among these tasks, reasoning is particularly
representative, with mathematical reasoning serving as a prominent example. It
highlights the high-level capability of VLMs to comprehend mathematical
information in images and to perform sophisticated reasoning. Recently,
numerous visual mathematical reasoning benchmarks have been proposed, but they
are often restricted to geometry, lack coverage of math word problems, and
rarely assess reasoning across multiple images. To address these gaps, we
introduce GSM8K-V, a purely visual multi-image mathematical reasoning
benchmark. GSM8K-V is built by systematically mapping each sample from the
widely used text-based GSM8K into visual form. Through a carefully designed
automated image-generation pipeline combined with meticulous human annotation,
we curate 1,319 high-quality samples. We evaluate a wide range of open-source
and closed-source models on GSM8K-V. Results show that although existing VLMs
have nearly saturated performance on text-based GSM8K, there remains
substantial room for improvement on GSM8K-V. For example, the best-performing
model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on
GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the
limitations of current models as well as potential directions for improvement.
GSM8K-V offers a new perspective on visual mathematical reasoning and
establishes a benchmark to guide the development of more robust and
generalizable VLMs.

</details>


### [279] [Foggy Crowd Counting: Combining Physical Priors and KAN-Graph](https://arxiv.org/abs/2509.24545)
*Yuhao Wang,Zhuoran Zheng,Han Hu,Dianjie Lu,Guijuan Zhang,Chen Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种结合大气散射物理先验的群体计数方法，通过物理机制与数据驱动的协同优化，提升了复杂气象条件下（尤其是雾天）的计数精度。


<details>
  <summary>Details</summary>
Motivation: 针对雾天环境下群体计数中存在的远距离目标模糊、局部特征退化和图像对比度衰减等关键挑战，现有方法难以准确估计密度。因此需要引入物理模型与增强特征表示能力的网络结构来提升鲁棒性。

Method: 1）引入可微分的大气散射模型，结合透射率动态估计与散射参数自适应校准，量化雾对不同景深目标的非线性衰减；2）基于Kolmogorov-Arnold定理设计MSA-KAN模块，采用可学习边缘激活函数与多层渐进结构，增强特征退化区域的非线性表征能力；3）提出天气感知的图卷积网络（GCN），利用MSA-KAN提取的深度特征动态构建空间邻接矩阵。

Result: 在四个公开数据集上的实验表明，在浓雾场景下，该方法相比主流算法MAE指标降低了12.2%–27.5%。

Conclusion: 所提方法通过融合物理先验与深度学习，在雾天复杂环境下的群体计数任务中显著提升了准确性和鲁棒性，验证了机理驱动与数据驱动协同优化的有效性。

Abstract: Aiming at the key challenges of crowd counting in foggy environments, such as
long-range target blurring, local feature degradation, and image contrast
attenuation, this paper proposes a crowd-counting method with a physical a
priori of atmospheric scattering, which improves crowd counting accuracy under
complex meteorological conditions through the synergistic optimization of the
physical mechanism and data-driven.Specifically, first, the method introduces a
differentiable atmospheric scattering model and employs transmittance dynamic
estimation and scattering parameter adaptive calibration techniques to
accurately quantify the nonlinear attenuation laws of haze on targets with
different depths of field.Secondly, the MSA-KAN was designed based on the
Kolmogorov-Arnold Representation Theorem to construct a learnable edge
activation function. By integrating a multi-layer progressive architecture with
adaptive skip connections, it significantly enhances the model's nonlinear
representation capability in feature-degraded regions, effectively suppressing
feature confusion under fog interference.Finally, we further propose a
weather-aware GCN that dynamically constructs spatial adjacency matrices using
deep features extracted by MSA-KAN. Experiments on four public datasets
demonstrate that our method achieves a 12.2\%-27.5\% reduction in MAE metrics
compared to mainstream algorithms in dense fog scenarios.

</details>


### [280] [TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models](https://arxiv.org/abs/2509.24566)
*Zhifang Zhang,Qiqi Tao,Jiaqi Lv,Na Zhao,Lei Feng,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出TokenSwap，一种针对大视觉语言模型的隐蔽后门攻击方法，通过交换文本答案中关键token的语法角色并注入视觉触发器，破坏模型对对象关系的理解，实现高攻击成功率且更难被检测。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLM后门攻击使用固定模式容易被检测，因模型在中毒数据上对目标内容过度自信。为提高隐蔽性和逃避检测，需要新的攻击方式。

Method: 引入TokenSwap攻击：在训练时向样本添加视觉触发器，并交换对应文本回答中关键token的语法角色；采用自适应token加权损失，强化被交换token的学习，使视觉触发与词袋行为关联。

Result: 实验表明，TokenSwap在多个基准和不同LVLM架构上均实现了高攻击成功率，同时具有更强的隐蔽性和抗检测能力。

Conclusion: TokenSwap通过破坏LVLM的对象关系理解能力，实现了一种更隐蔽、更难检测的后门攻击，凸显了当前模型在组合性理解方面的安全漏洞。

Abstract: Large vision-language models (LVLMs) have achieved impressive performance
across a wide range of vision-language tasks, while they remain vulnerable to
backdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim
model to generate a predefined target pattern, which is either inserted into or
replaces the original content. We find that these fixed-pattern attacks are
relatively easy to detect, because the attacked LVLM tends to memorize such
frequent patterns in the training dataset, thereby exhibiting overconfidence on
these targets given poisoned inputs. To address these limitations, we introduce
TokenSwap, a more evasive and stealthy backdoor attack that focuses on the
compositional understanding capabilities of LVLMs. Instead of enforcing a fixed
targeted content, TokenSwap subtly disrupts the understanding of object
relationships in text. Specifically, it causes the backdoored model to generate
outputs that mention the correct objects in the image but misrepresent their
relationships (i.e., bags-of-words behavior). During training, TokenSwap
injects a visual trigger into selected samples and simultaneously swaps the
grammatical roles of key tokens in the corresponding textual answers. However,
the poisoned samples exhibit only subtle differences from the original ones,
making it challenging for the model to learn the backdoor behavior. To address
this, TokenSwap employs an adaptive token-weighted loss that explicitly
emphasizes the learning of swapped tokens, such that the visual triggers and
bags-of-words behavior are associated. Extensive experiments demonstrate that
TokenSwap achieves high attack success rates while maintaining superior
evasiveness and stealthiness across multiple benchmarks and various LVLM
architectures.

</details>


### [281] [BFSM: 3D Bidirectional Face-Skull Morphable Model](https://arxiv.org/abs/2509.24577)
*Zidu Wang,Meng Xu,Miao Xu,Hengyuan Ma,Jiankuo Zhao,Xutao Li,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 提出3D双向人脸-颅骨可变形模型（BFSM），通过密集射线匹配配准方法和共享系数空间实现人脸与颅骨的高精度形状推断，并支持个体化软组织厚度建模，应用于单图三维重建与手术规划。


<details>
  <summary>Details</summary>
Motivation: 由于配对人脸-颅骨数据稀缺、配准精度不足以及临床应用探索有限，尤其是颅面畸形患者常被忽视，现有研究难以构建精确的联合人脸-颅骨模型。

Method: 构建包含200多个样本（含正常与罕见颅面病例）的数据集，每例包含CT颅骨、CT面部和高保真纹理面部扫描；提出新型密集射线匹配配准方法以保证拓扑一致性；建立3D双向人脸-颅骨可变形模型（BFSM），通过共享系数空间实现面部与颅骨间的双向形状推断，并建模软组织厚度变化以支持从同一颅骨生成多个人脸。

Result: 实现了高精度的人脸-颅骨配准与双向形状重建，在单图像3D人脸-颅骨重建和手术规划预测等医疗应用中表现出良好性能，实验验证了方法的鲁棒性与准确性。

Conclusion: BFSM为颅面形态建模提供了新框架，提升了数据包容性与配准精度，支持个性化面部重建与临床应用，推动了远程诊断、医学教育和个体化手术规划的发展。

Abstract: Building a joint face-skull morphable model holds great potential for
applications such as remote diagnostics, surgical planning, medical education,
and physically based facial simulation. However, realizing this vision is
constrained by the scarcity of paired face-skull data, insufficient
registration accuracy, and limited exploration of reconstruction and clinical
applications. Moreover, individuals with craniofacial deformities are often
overlooked, resulting in underrepresentation and limited inclusivity. To
address these challenges, we first construct a dataset comprising over 200
samples, including both normal cases and rare craniofacial conditions. Each
case contains a CT-based skull, a CT-based face, and a high-fidelity textured
face scan. Secondly, we propose a novel dense ray matching registration method
that ensures topological consistency across face, skull, and their tissue
correspondences. Based on this, we introduce the 3D Bidirectional Face-Skull
Morphable Model (BFSM), which enables shape inference between the face and
skull through a shared coefficient space, while also modeling tissue thickness
variation to support one-to-many facial reconstructions from the same skull,
reflecting individual changes such as fat over time. Finally, we demonstrate
the potential of BFSM in medical applications, including 3D face-skull
reconstruction from a single image and surgical planning prediction. Extensive
experiments confirm the robustness and accuracy of our method. BFSM is
available at https://github.com/wang-zidu/BFSM

</details>


### [282] [Comprehensive Benchmarking of YOLOv11 Architectures for Scalable and Granular Peripheral Blood Cell Detection](https://arxiv.org/abs/2509.24595)
*Mohamad Abou Ali,Mariam Abdulfattah,Baraah Al Hussein,Fadi Dornaika,Ali Cherry,Mohamad Hajj-Hassan,Lara Hamawy*

Main category: cs.CV

TL;DR: 本研究提出一个大规模标注的外周血涂片（PBS）细胞检测数据集，包含16,891张图像和298,850个标注细胞，并系统评估了YOLOv11系列模型在细粒度PBS细胞检测中的性能，发现YOLOv11 Medium在精度与计算效率之间达到最佳平衡（mAP@0.5达0.934），且8:1:1数据划分表现更优。


<details>
  <summary>Details</summary>
Motivation: 手动外周血涂片分析费时且主观，深度学习虽具潜力，但缺乏对YOLOv11等先进模型在细粒度血细胞检测中的系统性评估。

Method: 构建了一个大规模标注数据集（16,891张图像，12类白细胞加红细胞，共298,850个细胞），并在此基础上对五种YOLOv11变体（Nano至XLarge）进行系统评估，采用两种数据划分策略（70:20:10和80:10:10），以mAP、精确率、召回率、F1分数和计算效率为评价指标。

Result: YOLOv11 Medium在8:1:1划分下取得最优性能（mAP@0.5为0.934），更大模型提升有限但计算成本显著增加；8:1:1划分在所有模型上均优于7:2:1划分。

Conclusion: YOLOv11 Medium是自动化细粒度PBS细胞检测的高效框架，所发布数据集为血液学研究提供了重要资源。

Abstract: Manual peripheral blood smear (PBS) analysis is labor intensive and
subjective. While deep learning offers a promising alternative, a systematic
evaluation of state of the art models such as YOLOv11 for fine grained PBS
detection is still lacking. In this work, we make two key contributions. First,
we curate a large scale annotated dataset for blood cell detection and
classification, comprising 16,891 images across 12 peripheral blood cell (PBC)
classes, along with the red blood cell class, all carefully re annotated for
object detection tasks. In total, the dataset contains 298,850 annotated cells.
Second, we leverage this dataset to conduct a comprehensive evaluation of five
YOLOv11 variants (ranging from Nano to XLarge). These models are rigorously
benchmarked under two data splitting strategies (70:20:10 and 80:10:10) and
systematically assessed using multiple performance criteria, including mean
Average Precision (mAP), precision, recall, F1 score, and computational
efficiency. Our experiments show that the YOLOv11 Medium variant achieves the
best trade off, reaching a mAP@0.5 of 0.934 under the 8:1:1 split. Larger
models (Large and XLarge) provide only marginal accuracy gains at substantially
higher computational cost. Moreover, the 8:1:1 split consistently outperforms
the 7:2:1 split across all models. These findings highlight YOLOv11,
particularly the Medium variant, as a highly effective framework for automated,
fine grained PBS detection. Beyond benchmarking, our publicly released dataset
(github.com/Mohamad-AbouAli/OI-PBC-Dataset) offers a valuable resource to
advance research on blood cell detection and classification in hematology.

</details>


### [283] [Biomechanical-phase based Temporal Segmentation in Sports Videos: a Demonstration on Javelin-Throw](https://arxiv.org/abs/2509.24606)
*Bikash Kumar Badatya,Vipul Baghel,Jyotirmoy Amin,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出一种基于结构化最优传输（SOT）增强的注意力时空图卷积网络（ASTGCN）的无监督框架，用于精英标枪投掷动作的上下文感知时序分割，无需手动标注即可准确识别运动阶段转换。


<details>
  <summary>Details</summary>
Motivation: 传统体育动作分析依赖人工标注或实验室设备，耗时、昂贵且难以扩展；需要一种自动、精确且可扩展的方法来实现运动阶段的时序分割。

Method: 提出一种结合结构化最优传输（SOT）与注意力时空图卷积网络（ASTGCN）的无监督框架，利用SOT增强ASTGCN对上下文信息的建模能力，实现对标枪投掷关键生物力学阶段（如助跑、发力、投掷、恢复）的自动分割。

Result: 在包含211个专业标枪视频的新数据集上实验表明，该方法在无监督设置下达到71.02%的平均精度均值（mAP）和74.61%的F1分数，显著优于现有方法。同时发布了带帧级标注的公开数据集。

Conclusion: 所提SOT增强ASTGCN框架能有效实现无需人工标注的精细运动阶段分割，为体育运动分析提供了一种高效、可扩展的自动化解决方案。

Abstract: Precise analysis of athletic motion is central to sports analytics,
particularly in disciplines where nuanced biomechanical phases directly impact
performance outcomes. Traditional analytics techniques rely on manual
annotation or laboratory-based instrumentation, which are time-consuming,
costly, and lack scalability. Automatic extraction of relevant kinetic
variables requires a robust and contextually appropriate temporal segmentation.
Considering the specific case of elite javelin-throw, we present a novel
unsupervised framework for such a contextually aware segmentation, which
applies the structured optimal transport (SOT) concept to augment the
well-known Attention-based Spatio-Temporal Graph Convolutional Network
(ASTGCN). This enables the identification of motion phase transitions without
requiring expensive manual labeling. Extensive experiments demonstrate that our
approach outperforms state-of-the-art unsupervised methods, achieving 71.02%
mean average precision (mAP) and 74.61% F1-score on test data, substantially
higher than competing baselines. We also release a new dataset of 211 manually
annotated professional javelin-throw videos with frame-level annotations,
covering key biomechanical phases: approach steps, drive, throw, and recovery.

</details>


### [284] [FreeRet: MLLMs as Training-Free Retrievers](https://arxiv.org/abs/2509.24621)
*Yuhan Zhu,Xiangyu Zeng,Chenting Wang,Xinhao Li,Yicheng Xu,Ziang Yan,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出FreeRet框架，无需额外训练即可将现成的多模态大语言模型（MLLM）转化为强大的两阶段检索器，通过直接提取语义嵌入和利用模型推理能力进行重排序，在多个基准上超越了经过大规模训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在用于检索任务时通常需要大量后处理训练，本文探索是否可以直接利用现成的MLLM进行高效检索，以减少对额外训练的依赖。

Method: FreeRet框架首先从MLLM中直接提取语义准确的嵌入用于快速候选搜索，然后利用模型自身的推理能力进行精确重排序；其关键技术包括绕过词汇对齐层、引入显式先验条件表示生成，以及通过中性选择框架减轻重排序中的框架效应。

Result: 在涵盖46个数据集的MMEB和MMEB-V2基准上，FreeRet显著优于使用数百万对数据训练的专用检索模型，且具有模型无关性，可跨不同规模和家族的MLLM无缝扩展。

Conclusion: 研究表明，经过精心设计的预训练多模态大语言模型无需微调即可成为强大的检索引擎，填补了其作为通用模型在检索任务上的应用空白。

Abstract: Multimodal large language models (MLLMs) are emerging as versatile
foundations for mixed-modality retrieval. Yet, they often require heavy
post-hoc training to convert them into contrastive encoders for retrieval. This
work asks: Can off-the-shelf MLLMs serve as powerful retrievers without
additional training? We present FreeRet, a plug-and-play framework that turns
any MLLM into a two-stage retriever. FreeRet first derives semantically
grounded embeddings directly from the model for fast candidate search, and then
exploits its reasoning ability for precise reranking. The framework contributes
three advances: bypassing lexical alignment layers to obtain semantically
faithful embeddings, conditioning representation generation with explicit
priors, and mitigating framing effect in reranking via neutral choice framing.
On the MMEB and MMEB-V2 benchmarks spanning 46 datasets, FreeRet substantially
outperforms models trained on millions of pairs. Beyond benchmarks, FreeRet is
model-agnostic and scales seamlessly across MLLM families and sizes, preserves
their generative abilities, supports arbitrary modality combinations, and
unifies retrieval, reranking, and generation into end-to-end RAG within a
single model. Our findings demonstrate that pretrained MLLMs, when carefully
harnessed, can serve as strong retrieval engines without training, closing a
critical gap in their role as generalists.

</details>


### [285] [Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs](https://arxiv.org/abs/2509.24640)
*Mohamad Ballout,Okajevo Wilfred,Seyedalireza Yaghoubi,Nohayr Muhammad Abdelmoneim,Julius Mayer,Elia Bruni*

Main category: cs.CV

TL;DR: SPLICE是一个基于COIN数据集的人工整理基准，用于评估多维度事件推理能力，包含3,381个视频和11,423个事件片段，实验显示现有视觉语言模型在视觉推理上显著落后于人类，尤其在上下文和空间推理及专业任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 为了系统评估视觉语言模型在复杂事件推理中的能力，特别是在时间、因果、空间、上下文和常识等多维度上的表现，弥补现有基准的不足。

Method: 构建SPLICE基准，包含人工筛选和分割的视频，并要求人类和VLM对事件片段进行排序以形成连贯序列，比较其性能；同时分析不同子类别和是否使用文本描述对结果的影响。

Result: VLMs显著落后于人类；文本描述能提升模型性能但不影响人类；模型在时间与因果推理、日常任务中表现较好，在空间与上下文推理、专业任务中较差。

Conclusion: 当前VLM在视觉事件推理方面仍严重依赖语言先验，缺乏真正的视觉理解，尤其在复杂场景和专业领域存在明显短板。

Abstract: In this work, we introduce SPLICE, a human-curated benchmark derived from the
COIN instructional video dataset, designed to probe event-based reasoning
across multiple dimensions: temporal, causal, spatial, contextual, and general
knowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories
and 180 sub-categories, such as sports, engineering, and housework. These
videos are segmented into a total of 11,423 event clips. We evaluate both human
participants and state-of-the-art vision-language models (VLMs) on the task of
rearranging these clips into coherent event sequences to assess visual
reasoning capabilities. Results reveal a significant gap: VLMs struggle to
match human performance. While human-annotated textual descriptions improve
model accuracy, they do not affect human performance, suggesting that models
rely more on language priors than on visual understanding. Even with
annotations, VLMs fall short of human-level reasoning, underscoring persistent
challenges in visual reasoning. A deeper analysis across sub-categories shows
that VLMs perform relatively better on videos where temporal and causal
reasoning are dominant, compared to those where contextual and spatial
reasoning are dominant. They also perform better on everyday tasks than on
specialized ones.

</details>


### [286] [RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement](https://arxiv.org/abs/2509.24644)
*Zhu,Libo,Zhou,Zihan,Liu,Xiaoyang,Zhang,Weihang,Shi,Keyu,Fu,Yifan,Zhang,Yulun*

Main category: cs.CV

TL;DR: 本文提出了RIFLE，一种基于扩散模型的框架，用于去除图像中的闪烁条纹（FB），并通过引入闪烁先验估计器和掩码损失，在保持细节的同时有效恢复受FB影响的屏幕截图。


<details>
  <summary>Details</summary>
Motivation: 闪烁条纹（FB）在屏幕拍摄中普遍存在且严重影响可读性和视觉质量，但相关研究较少。现有方法难以有效处理FB问题，因此需要专门的修复任务与模型。

Method: 提出RIFLE框架，结合潜在扩散模型；设计闪烁条纹先验估计器（FPE）预测条纹特征并注入恢复网络；采用掩码损失（ML）聚焦于条纹区域的优化；构建仿真流水线生成带随机抖动的合成FB数据，并加入羽化边界和传感器噪声以增强真实性。

Result: 在真实世界配对数据集上，RIFLE在定量指标和视觉效果上均优于最新的图像重建基线方法，能有效处理从轻微到严重的FB退化。

Conclusion: RIFLE是首个针对FB模拟与去除的研究工作，为后续在数据集构建和去FB模型设计方面奠定了基础，具有重要的研究价值和应用前景。

Abstract: Capturing screens is now routine in our everyday lives. But the photographs
of emissive displays are often influenced by the flicker-banding (FB), which is
alternating bright%u2013dark stripes that arise from temporal aliasing between
a camera's rolling-shutter readout and the display's brightness modulation.
Unlike moire degradation, which has been extensively studied, the FB remains
underexplored despite its frequent and severe impact on readability and
perceived quality. We formulate FB removal as a dedicated restoration task and
introduce Removal of Image Flicker-Banding via Latent Diffusion Enhancement,
RIFLE, a diffusion-based framework designed to remove FB while preserving fine
details. We propose the flicker-banding prior estimator (FPE) that predicts key
banding attributes and injects it into the restoration network. Additionally,
Masked Loss (ML) is proposed to concentrate supervision on banded regions
without sacrificing global fidelity. To overcome data scarcity, we provide a
simulation pipeline that synthesizes FB in the luminance domain with stochastic
jitter in banding angle, banding spacing, and banding width. Feathered
boundaries and sensor noise are also applied for a more realistic simulation.
For evaluation, we collect a paired real-world FB dataset with pixel-aligned
banding-free references captured via long exposure. Across quantitative metrics
and visual comparisons on our real-world dataset, RIFLE consistently
outperforms recent image reconstruction baselines from mild to severe
flicker-banding. To the best of our knowledge, it is the first work to research
the simulation and removal of FB. Our work establishes a great foundation for
subsequent research in both the dataset construction and the removal model
design. Our dataset and code will be released soon.

</details>


### [287] [Learning Object-Centric Representations Based on Slots in Real World Scenarios](https://arxiv.org/abs/2509.24652)
*Adil Kaan Akan*

Main category: cs.CV

TL;DR: 本论文提出了一种基于槽位（slot-based）的轻量级框架SlotAdapt，用于在预训练扩散模型中实现以对象为中心的图像和视频生成与编辑，兼顾场景整体连贯性与对象的独立控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型通常以整体方式处理图像并依赖文本条件，难以实现精细的对象级编辑。本文旨在弥合这一差距，使生成模型能够像人类一样以离散对象的方式理解和操控视觉场景。

Method: 提出SlotAdapt框架，在预训练扩散模型中引入注册令牌（背景/风格）和槽位条件模块（对象）。图像方面通过槽位实现对象发现与编辑；视频方面结合不变性槽注意力（ISA）和时序Transformer聚合器，保持跨帧的对象一致性与动态性。

Result: 在对象发现、分割、图像/视频重建和无监督视频对象分割等任务上达到最先进性能，并支持无需显式监督的对象移除、替换和插入等高级编辑功能。

Conclusion: 该工作建立了一个通用且可扩展的以对象为中心的生成建模方法，桥接了人类对象感知与机器学习，为交互式、结构化的生成工具提供了新设计空间。

Abstract: A central goal in AI is to represent scenes as compositions of discrete
objects, enabling fine-grained, controllable image and video generation. Yet
leading diffusion models treat images holistically and rely on text
conditioning, creating a mismatch for object-level editing. This thesis
introduces a framework that adapts powerful pretrained diffusion models for
object-centric synthesis while retaining their generative capacity.
  We identify a core challenge: balancing global scene coherence with
disentangled object control. Our method integrates lightweight, slot-based
conditioning into pretrained models, preserving their visual priors while
providing object-specific manipulation. For images, SlotAdapt augments
diffusion models with a register token for background/style and
slot-conditioned modules for objects, reducing text-conditioning bias and
achieving state-of-the-art results in object discovery, segmentation,
compositional editing, and controllable image generation.
  We further extend the framework to video. Using Invariant Slot Attention
(ISA) to separate object identity from pose and a Transformer-based temporal
aggregator, our approach maintains consistent object representations and
dynamics across frames. This yields new benchmarks in unsupervised video object
segmentation and reconstruction, and supports advanced editing tasks such as
object removal, replacement, and insertion without explicit supervision.
  Overall, this work establishes a general and scalable approach to
object-centric generative modeling for images and videos. By bridging human
object-based perception and machine learning, it expands the design space for
interactive, structured, and user-driven generative tools in creative,
scientific, and practical domains.

</details>


### [288] [VNODE: A Piecewise Continuous Volterra Neural Network](https://arxiv.org/abs/2509.24659)
*Siddharth Roheda,Aniruddha Bala,Rohit Chowdhury,Rohan Jaiswal*

Main category: cs.CV

TL;DR: 提出了一种结合非线性Volterra滤波与神经微分方程的混合模型VNODE，用于图像分类，在减少参数量的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 受视觉皮层中离散事件处理与连续积分交替机制的启发，旨在提升图像分类模型的效率和表达能力。

Method: 将Volterra神经网络与常微分方程结合，构建分段连续的VNODE模型，交替进行离散特征提取和ODE驱动的状态演化。

Result: 在CIFAR10和Imagenet1K等基准数据集上，VNODE在更低计算复杂度下持续优于现有最先进模型。

Conclusion: VNODE通过融合Volterra滤波与神经ODE，实现了高效且高性能的图像分类，为轻量化深度模型提供了新思路。

Abstract: This paper introduces Volterra Neural Ordinary Differential Equations
(VNODE), a piecewise continuous Volterra Neural Network that integrates
nonlinear Volterra filtering with continuous time neural ordinary differential
equations for image classification. Drawing inspiration from the visual cortex,
where discrete event processing is interleaved with continuous integration,
VNODE alternates between discrete Volterra feature extraction and ODE driven
state evolution. This hybrid formulation captures complex patterns while
requiring substantially fewer parameters than conventional deep architectures.
VNODE consistently outperforms state of the art models with improved
computational complexity as exemplified on benchmark datasets like CIFAR10 and
Imagenet1K.

</details>


### [289] [Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation](https://arxiv.org/abs/2509.24681)
*Hanyu Zhang,Yiming Zhou,Jinxia Zhang*

Main category: cs.CV

TL;DR: 提出了一种以分类器为中心的自适应框架，通过轻量级文本适配器和分层非对称初始化来提升开放词汇伪装目标分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法中分类组件显著影响分割性能，需提高模型对训练时未见类别的泛化能力。

Method: 设计了一个以分类器为核心的框架，引入轻量级文本适配器并采用新的分层非对称初始化策略来增强分类能力。

Result: 在OVCamo基准上相比OVCoser基线显著提升：cIoU从0.443升至0.493，cSm从0.579升至0.658，cMAE从0.336降至0.239。

Conclusion: 针对性增强分类组件是提升开放词汇伪装目标分割性能的有效途径。

Abstract: Open-vocabulary camouflaged object segmentation requires models to segment
camouflaged objects of arbitrary categories unseen during training, placing
extremely high demands on generalization capabilities. Through analysis of
existing methods, it is observed that the classification component
significantly affects overall segmentation performance. Accordingly, a
classifier-centric adaptive framework is proposed to enhance segmentation
performance by improving the classification component via a lightweight text
adapter with a novel layered asymmetric initialization. Through the
classification enhancement, the proposed method achieves substantial
improvements in segmentation metrics compared to the OVCoser baseline on the
OVCamo benchmark: cIoU increases from 0.443 to 0.493, cSm from 0.579 to 0.658,
and cMAE reduces from 0.336 to 0.239. These results demonstrate that targeted
classification enhancement provides an effective approach for advancing
camouflaged object segmentation performance.

</details>


### [290] [Traumatic Brain Injury Segmentation using an Ensemble of Encoder-decoder Models](https://arxiv.org/abs/2509.24684)
*Ghanshyam Dhamat,Vaanathi Sundaresan*

Main category: cs.CV

TL;DR: 本研究开发了一种基于nnUNet框架的自动化管道，用于在T1加权MRI上准确分割中重度创伤性脑损伤（TBI）病灶，结合多种架构和后处理策略，在AIMS-TBI 2025挑战赛中取得了排名前六的表现。


<details>
  <summary>Details</summary>
Motivation: 中重度TBI病灶在大小、数量和位置上高度异质，给图像配准和脑区分割等后续分析带来困难，亟需高精度的自动分割方法以提升神经影像分析的可靠性。

Method: 采用nnUNet框架的多种网络架构进行初始分割，并结合特定的后处理策略优化结果，构建完整的自动化分割流程。

Result: 在AIMS-TBI 2025挑战赛中，该方法整体Dice分数为0.5973，对有可见病灶和无可见病灶图像的Dice分数分别为0.4711和0.8514，准确率达0.8451，位列前六。

Conclusion: 所提出的自动化分割管道在处理高度异质性的TBI病灶方面表现优异，具有良好的应用潜力，且代码已公开，便于后续研究使用。

Abstract: The identification and segmentation of moderate-severe traumatic brain injury
(TBI) lesions pose a significant challenge in neuroimaging. This difficulty
arises from the extreme heterogeneity of these lesions, which vary in size,
number, and laterality, thereby complicating downstream image processing tasks
such as image registration and brain parcellation, reducing the analytical
accuracy. Thus, developing methods for highly accurate segmentation of TBI
lesions is essential for reliable neuroimaging analysis. This study aims to
develop an effective automated segmentation pipeline to automatically detect
and segment TBI lesions in T1-weighted MRI scans. We evaluate multiple
approaches to achieve accurate segmentation of the TBI lesions. The core of our
pipeline leverages various architectures within the nnUNet framework for
initial segmentation, complemented by post-processing strategies to enhance
evaluation metrics. Our final submission to the challenge achieved an accuracy
of 0.8451, Dice score values of 0.4711 and 0.8514 for images with and without
visible lesions, respectively, with an overall Dice score of 0.5973, ranking
among the top-6 methods in the AIMS-TBI 2025 challenge. The Python
implementation of our pipeline is publicly available.

</details>


### [291] [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://arxiv.org/abs/2509.24695)
*Junsong Chen,Yuyang Zhao,Jincheng Yu,Ruihang Chu,Junyu Chen,Shuai Yang,Xianbang Wang,Yicheng Pan,Daquan Zhou,Huan Ling,Haozhe Liu,Hongwei Yi,Hao Zhang,Muyang Li,Yukang Chen,Han Cai,Sanja Fidler,Ping Luo,Song Han,Enze Xie*

Main category: cs.CV

TL;DR: SANA-Video 是一个高效的扩散模型，能够以极低的成本生成高质量、长时长的视频，在720x1280分辨率和分钟级时长下表现优异，并支持在RTX 5090 GPU上快速部署。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型通常计算成本高、内存消耗大，难以高效生成长视频。SANA-Video旨在通过更高效的注意力机制和内存优化策略，实现低成本、高质量、长时长的视频生成。

Method: 提出两种核心技术：(1) Linear DiT，使用线性注意力替代传统注意力，降低计算复杂度；(2) 基于线性注意力累积特性的恒定内存KV缓存，实现块状自回归生成，支持固定内存开销下的长视频生成。同时采用有效的数据过滤和训练策略，大幅降低训练成本。

Result: SANA-Video 在64块H100 GPU上仅用12天完成训练，成本仅为MovieGen的1%。相比同类小模型（如Wan 2.1-1.3B和SkyReel-V2-1.3B），性能相当但推理速度快16倍；在RTX 5090上使用NVFP4精度，生成5秒720p视频从71秒缩短至29秒（2.4倍加速）。

Conclusion: SANA-Video 实现了低成本、高效率、高质量的长视频生成，具备良好的可部署性，推动了小型化视频生成模型的发展。

Abstract: We introduce SANA-Video, a small diffusion model that can efficiently
generate videos up to 720x1280 resolution and minute-length duration.
SANA-Video synthesizes high-resolution, high-quality and long videos with
strong text-video alignment at a remarkably fast speed, deployable on RTX 5090
GPU. Two core designs ensure our efficient, effective and long video
generation: (1) Linear DiT: We leverage linear attention as the core operation,
which is more efficient than vanilla attention given the large number of tokens
processed in video generation. (2) Constant-Memory KV cache for Block Linear
Attention: we design block-wise autoregressive approach for long video
generation by employing a constant-memory state, derived from the cumulative
properties of linear attention. This KV cache provides the Linear DiT with
global context at a fixed memory cost, eliminating the need for a traditional
KV cache and enabling efficient, minute-long video generation. In addition, we
explore effective data filters and model training strategies, narrowing the
training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of
MovieGen. Given its low cost, SANA-Video achieves competitive performance
compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B
and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,
SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating
the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x
speedup). In summary, SANA-Video enables low-cost, high-quality video
generation.

</details>


### [292] [Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility](https://arxiv.org/abs/2509.24702)
*Yutong Hao,Chen Chen,Ajmal Saeed Mian,Chang Xu,Daochang Liu*

Main category: cs.CV

TL;DR: 提出一种无需训练的视频生成框架，通过显式物理推理和新型同步解耦引导（SDG）策略，在推理阶段提升生成视频的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型依赖大规模文本-视频数据隐式学习物理规律，成本高、难以扩展，且易生成违反物理定律的不真实运动。

Method: 构建轻量级物理感知推理模块生成违反物理规律的反事实提示，并提出同步解耦引导（SDG）策略，结合同步方向归一化和轨迹解耦去噪，实时抑制不合理的生成内容。

Result: 在多个物理场景下实验表明，该方法显著提升了生成视频的物理保真度，同时保持视觉质量，且无需额外训练。消融实验验证了物理推理模块和SDG各组件的有效性。

Conclusion: 该工作提出了一种即插即用的物理感知视频生成新范式，通过推理时显式物理建模有效提升生成结果的物理合理性。

Abstract: Diffusion models can generate realistic videos, but existing methods rely on
implicitly learning physical reasoning from large-scale text-video datasets,
which is costly, difficult to scale, and still prone to producing implausible
motions that violate fundamental physical laws. We introduce a training-free
framework that improves physical plausibility at inference time by explicitly
reasoning about implausibility and guiding the generation away from it.
Specifically, we employ a lightweight physics-aware reasoning pipeline to
construct counterfactual prompts that deliberately encode physics-violating
behaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG)
strategy, which leverages these prompts through synchronized directional
normalization to counteract lagged suppression and trajectory-decoupled
denoising to mitigate cumulative trajectory bias, ensuring that implausible
content is suppressed immediately and consistently throughout denoising.
Experiments across different physical domains show that our approach
substantially enhances physical fidelity while maintaining photorealism,
despite requiring no additional training. Ablation studies confirm the
complementary effectiveness of both the physics-aware reasoning component and
SDG. In particular, the aforementioned two designs of SDG are also individually
validated to contribute critically to the suppression of implausible content
and the overall gains in physical plausibility. This establishes a new and
plug-and-play physics-aware paradigm for video generation.

</details>


### [293] [IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?](https://arxiv.org/abs/2509.24709)
*Yang Chen,Minghao Liu,Yufan Shen,Yunwen Li,Tianyuan Huang,Xinyu Fang,Tianyu Zheng,Wenxuan Huang,Cheng Yang,Daocheng Fu,Jianbiao Mei,Rong Wu,Licheng Wen,Xuemeng Yang,Song Mao,Qunshu Lin,Zhi Yu,Yongliang Shen,Yu Qiao,Botian Shi*

Main category: cs.CV

TL;DR: 本文提出了IWR-Bench，一个用于评估大型视觉语言模型（LVLMs）从视频中重建交互式网页能力的新基准。该基准包含来自100个真实网站的113个任务和1,001个动作，涵盖多种交互复杂性、视觉风格和领域，并提供用户交互视频及所有静态资源。通过代理评判框架自动评估生成网页的功能正确性和视觉保真度。在28个LVLM上的实验表明，当前最佳模型总体得分仅为36.35%，功能正确性（24.39%）远低于视觉保真度（64.25%），揭示了现有模型在时序动态理解和事件驱动逻辑生成方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有的网页生成代码任务主要基于静态截图，忽略了现实世界网页应用中的动态交互特性，无法充分评估模型对交互逻辑的理解与实现能力。因此，需要一个能反映真实交互场景的 benchmark 来推动视觉语言模型在动态网页重建方面的发展。

Method: 构建了一个名为 IWR-Bench 的新基准，包含 113 个来自真实网站的任务，每个任务提供交互视频和完整的静态资源（如图片、视频）。设计了两个核心挑战：从多模态输入（视频+资源）进行交互逻辑推理，以及生成可运行的前端代码。采用“代理作为裁判”的自动化评估框架，结合功能正确性（IFS）和视觉保真度（VFS）指标进行量化评价。

Result: 在28个大型视觉语言模型上的实验显示，最佳模型的整体得分为36.35%，其中功能正确性为24.39%，视觉保真度为64.25%，表明当前模型在理解时序交互和生成事件驱动代码方面存在显著缺陷。

Conclusion: IWR-Bench 是首个面向交互式网页重建的综合性基准，揭示了现有 LVLM 在动态交互理解和事件逻辑生成方面的关键局限，为未来视觉语言模型的研究提供了具有挑战性的新方向。

Abstract: The webpage-to-code task requires models to understand visual representations
of webpages and generate corresponding code. However, existing benchmarks
primarily focus on static screenshot-to-code tasks, thereby overlooking the
dynamic interactions fundamental to real-world web applications. To address
this limitation, this paper introduces IWR-Bench, a novel benchmark for
evaluating the capabilities of Large Vision-Language Models (LVLMs) in
interactive webpage reconstruction from video. IWR-Bench comprises 113
meticulously curated tasks from 100 real-world websites, with 1,001 actions and
featuring diverse interaction complexities (e.g., web games), visual styles,
and domains. Aligning with standard web development practices, each task
includes not only user interaction videos but also all crawled static assets
(e.g., images, videos). This benchmark evaluates models on two fundamental
challenges: comprehensive multi-modal reasoning to infer interaction logic from
video and assets, and advanced code generation to translate this logic into
functional code. An agent-as-a-judge framework with a comprehensive metric
system automatically assesses the functional correctness and visual fidelity of
generated webpages. Extensive experiments on 28 LVLMs reveal a significant
challenge: the best model achieves an overall score of only 36.35%, as
functional correctness (24.39% IFS) lags significantly behind visual fidelity
(64.25% VFS). These results highlight critical limitations in current models'
ability to reason about temporal dynamics and synthesize event-driven logic,
establishing IWR-Bench as a challenging frontier for vision-language research.
The benchmark and evaluation code will be made publicly available. Code is
available at https://github.com/L-O-I/IWR-Bench.

</details>


### [294] [Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation](https://arxiv.org/abs/2509.24739)
*Huu Tien Nguyen,Dac Thai Nguyen,The Minh Duc Nguyen,Trung Thanh Nguyen,Thao Nguyen Truong,Huy Hieu Pham,Johan Barthelemy,Minh Quan Tran,Thanh Tam Nguyen,Quoc Viet Hung Nguyen,Quynh Anh Chau,Hong Son Mai,Thanh Trung Nguyen,Phi Le Nguyen*

Main category: cs.CV

TL;DR: 本文介绍了一个新的越南语多模态医学数据集，包含156万对CT-PET图像和2757份完整的临床报告，旨在解决现有视觉-语言模型在医学成像中缺乏PET/CT数据和低资源语言支持的问题。作者还提出了增强VLM训练的框架，并通过实验证明该数据集能显著提升现有模型在医学报告生成和视觉问答等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉-语言模型大多局限于高资源语言和少数影像模态，缺乏对低资源语言（如越南语）和多功能影像（如PET/CT）的支持，限制了其临床适用性和泛化能力。

Method: 构建了一个包含CT-PET图像与越南语临床报告配对的大规模数据集，提出包含数据增强和专家验证测试集的训练框架，并在医学报告生成和视觉问答任务上评估主流视觉-语言模型的性能。

Result: 实验表明，引入该数据集后，现有视觉-语言模型在下游医学任务中的表现显著提升，特别是在处理PET/CT图像和越南语临床文本时展现出更强的能力。

Conclusion: 该数据集和基准测试为推动低资源语言下的医学视觉-语言模型发展提供了重要基础，有助于提升模型在越南医疗环境中的临床相关性和实用性。

Abstract: Vision-Language Foundation Models (VLMs), trained on large-scale multimodal
datasets, have driven significant advances in Artificial Intelligence by
enabling rich cross-modal reasoning. Despite their success in general domains,
applying these models to medical imaging remains challenging due to the limited
availability of diverse imaging modalities and multilingual clinical data. Most
existing medical VLMs are trained on a subset of imaging modalities and focus
primarily on high-resource languages, thus limiting their generalizability and
clinical utility. To address these limitations, we introduce a novel
Vietnamese-language multimodal medical dataset comprising 1,567,062 paired
CT-PET images and corresponding 2,757 full-length clinical reports. This
dataset is designed to fill two pressing gaps in medical AI development: (1)
the lack of PET/CT imaging data in existing VLMs training corpora, which
hinders the development of models capable of handling functional imaging tasks;
and (2) the underrepresentation of low-resource languages, particularly the
Vietnamese language, in medical vision-language research. To the best of our
knowledge, this is the first dataset to provide comprehensive PET/CT-report
pairs in Vietnamese. We further introduce a training framework to enhance VLMs'
learning, including data augmentation and expert-validated test sets. We
conduct comprehensive experiments benchmarking state-of-the-art VLMs on
downstream tasks, including medical report generation and visual question
answering. The experimental results show that incorporating our dataset
significantly improves the performance of existing VLMs. We believe this
dataset and benchmark will serve as a pivotal step in advancing the development
of more robust VLMs for medical imaging, particularly in low-resource
languages, and improving their clinical relevance in Vietnamese healthcare.

</details>


### [295] [Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm](https://arxiv.org/abs/2509.24741)
*Xue-Feng Zhu,Tianyang Xu,Yifan Pan,Jinjie Gu,Xi Li,Jiwen Lu,Xiao-Jun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文提出了一种新的三模态（RGB-Depth-Thermal）目标跟踪任务，并构建了包含500个同步视频的RGBDT500数据集。同时，提出RDTTrack方法，通过正交投影约束融合热红外与深度信息，并以提示学习方式结合预训练RGB跟踪模型，显著提升了复杂场景下的跟踪精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有双模态跟踪方法在复杂场景中因模态有限而表现受限，需引入更多互补模态提升鲁棒性。

Method: 提出RDTTrack，利用预训练RGB跟踪模型和提示学习技术，通过正交投影约束融合热红外与深度信息，并将三模态信息整合为提示输入至基础模型。

Result: 实验表明，RDTTrack在RGBDT500数据集上显著优于现有双模态方法，在跟踪精度和鲁棒性方面均有提升。

Conclusion: 三模态融合（RGB-D-TIR）能有效增强复杂场景下的目标跟踪性能，所提方法和数据集为多模态跟踪提供了新方向。

Abstract: Existing multi-modal object tracking approaches primarily focus on dual-modal
paradigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex
scenarios due to limited input modalities. To address this gap, this work
introduces a novel multi-modal tracking task that leverages three complementary
modalities, including visible RGB, Depth (D), and Thermal Infrared (TIR),
aiming to enhance robustness in complex scenarios. To support this task, we
construct a new multi-modal tracking dataset, coined RGBDT500, which consists
of 500 videos with synchronised frames across the three modalities. Each frame
provides spatially aligned RGB, depth, and thermal infrared images with precise
object bounding box annotations. Furthermore, we propose a novel multi-modal
tracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust
tracking by leveraging a pretrained RGB-only tracking model and prompt learning
techniques. In specific, RDTTrack fuses thermal infrared and depth modalities
under a proposed orthogonal projection constraint, then integrates them with
RGB signals as prompts for the pre-trained foundation tracking model,
effectively harmonising tri-modal complementary cues. The experimental results
demonstrate the effectiveness and advantages of the proposed method, showing
significant improvements over existing dual-modal approaches in terms of
tracking accuracy and robustness in complex scenarios.

</details>


### [296] [ExGS: Extreme 3D Gaussian Compression with Diffusion Priors](https://arxiv.org/abs/2509.24758)
*Jiaqi Chen,Xinhao Ji,Yuanyuan Gao,Hao Li,Yuning Gong,Yifei Liu,Dan Xu,Zhihang Zhong,Dingwen Zhang,Xiao Sun*

Main category: cs.CV

TL;DR: 提出ExGS，一种结合通用高斯压缩（UGC）和GaussPainter的前馈框架，实现3D高斯点阵的极端压缩（超过100倍），同时利用扩散先验恢复高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点阵压缩方法在高压缩比下难以兼顾效率与渲染质量，优化成本高或依赖训练的方法存在速度慢或质量下降问题，需要一种无需重优化且能保持高保真渲染的高效压缩方案。

Method: 提出ExGS框架，包含两部分：UGC模块进行无需重优化的激进剪枝，保留关键高斯图元；GaussPainter模块利用扩散先验和掩码引导修复，在轻量级VAE和单步扩散设计下实时恢复并增强渲染质量。

Result: 实现了超过100倍的压缩比（如354.77 MB降至3.31 MB），在极端压缩下仍保持甚至提升渲染质量，尤其在挑战性场景中显著改善图像质量，且支持实时恢复。

Conclusion: 扩散先验在连接极端压缩与高质量神经渲染方面起核心作用，ExGS为资源受限环境下的高效部署提供了可行解决方案。

Abstract: Neural scene representations, such as 3D Gaussian Splatting (3DGS), have
enabled high-quality neural rendering; however, their large storage and
transmission costs hinder deployment in resource-constrained environments.
Existing compression methods either rely on costly optimization, which is slow
and scene-specific, or adopt training-free pruning and quantization, which
degrade rendering quality under high compression ratios. In contrast, recent
data-driven approaches provide a promising direction to overcome this
trade-off, enabling efficient compression while preserving high rendering
quality. We introduce \textbf{ExGS}, a novel feed-forward framework that
unifies \textbf{Universal Gaussian Compression} (UGC) with
\textbf{GaussPainter} for \textbf{Ex}treme 3D\textbf{GS} compression.
\textbf{UGC} performs re-optimization-free pruning to aggressively reduce
Gaussian primitives while retaining only essential information, whereas
\textbf{GaussPainter} leverages powerful diffusion priors with mask-guided
refinement to restore high-quality renderings from heavily pruned Gaussian
scenes. Unlike conventional inpainting, GaussPainter not only fills in missing
regions but also enhances visible pixels, yielding substantial improvements in
degraded renderings. To ensure practicality, it adopts a lightweight VAE and a
one-step diffusion design, enabling real-time restoration. Our framework can
even achieve over $100\times$ compression (reducing a typical 354.77 MB model
to about 3.31 MB) while preserving fidelity and significantly improving image
quality under challenging conditions. These results highlight the central role
of diffusion priors in bridging the gap between extreme compression and
high-quality neural rendering. Our code repository will be released at
\href{https://github.com/chenttt2001/ExGS}{here}.

</details>


### [297] [VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding](https://arxiv.org/abs/2509.24776)
*Yizhuo Ding,Mingkang Chen,Zhibang Feng,Tong Xiao,Wanying Qu,Wenqi Shao,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出VTPerception-R1框架，通过解耦感知与推理两个阶段，提升多模态大模型的推理准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在基于感知证据进行推理时表现不佳，缺乏有效的感知策略。

Method: 提出VTPerception-R1，包含两个阶段：第一阶段进行感知增强的微调，第二阶段采用感知感知的强化学习，并引入视觉、文本和一致性奖励。

Result: 在四个多模态基准和两个MLLM上验证，该方法显著提升了推理准确性和鲁棒性，尤其对小模型效果更优。

Conclusion: 显式感知（尤其是结合文本线索）效果最佳，VTPerception-R1为感知接地的多模态推理提供了可扩展且可审计的解决方案。

Abstract: Multimodal large language models (MLLMs) often struggle to ground reasoning
in perceptual evidence. We present a systematic study of perception
strategies-explicit, implicit, visual, and textual-across four multimodal
benchmarks and two MLLMs. Our findings show that explicit perception,
especially when paired with textual cues, consistently yields the best
improvements, particularly for smaller models. Based on this insight, we
propose VTPerception-R1, a unified two-stage framework that decouples
perception from reasoning. Stage 1 introduces perception-augmented fine-tuning,
and Stage 2 applies perception-aware reinforcement learning with novel visual,
textual, and consistency rewards. Experiments demonstrate that VTPerception-R1
significantly improves reasoning accuracy and robustness across diverse tasks,
offering a scalable and auditable solution for perception-grounded multimodal
reasoning. Our code is available at:
https://github.com/yizhuoDi/VTPerceprion-R1.

</details>


### [298] [SkyLink: Unifying Street-Satellite Geo-Localization via UAV-Mediated 3D Scene Alignment](https://arxiv.org/abs/2509.24783)
*Hongyang Zhang,Yinhao Liu,Zhenyu Kuang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SkyLink的新方法，用于解决跨视角地理定位中的语义退化问题，通过数据增强、局部特征聚合和3D场景信息对齐，在街景与卫星视图之间实现了鲁棒的特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理极端视角差异时往往忽视语义退化问题，导致跨视角匹配性能下降，因此需要一种更鲁棒的特征检索机制。

Method: 提出SkyLink方法，包括Google检索增强模块以缓解街景遮挡问题，Patch-Aware特征聚合模块强化多局部特征提取，并利用多尺度无人机图像构建的3D场景信息作为街景与卫星视图之间的桥梁，结合自监督与跨视图对比学习实现特征对齐。

Result: 在UAVM2025挑战赛的University-1652数据集上达到25.75%的Recall@1精度，表现出良好的鲁棒性和泛化能力。

Conclusion: SkyLink通过引入3D场景信息和多模块协同设计，有效提升了跨视角地理定位的性能，尤其在复杂城市环境中具有应用潜力。

Abstract: Cross-view geo-localization aims at establishing location correspondences
between different viewpoints. Existing approaches typically learn cross-view
correlations through direct feature similarity matching, often overlooking
semantic degradation caused by extreme viewpoint disparities. To address this
unique problem, we focus on robust feature retrieval under viewpoint variation
and propose the novel SkyLink method. We firstly utilize the Google Retrieval
Enhancement Module to perform data enhancement on street images, which
mitigates the occlusion of the key target due to restricted street viewpoints.
The Patch-Aware Feature Aggregation module is further adopted to emphasize
multiple local feature aggregations to ensure the consistent feature extraction
across viewpoints. Meanwhile, we integrate the 3D scene information constructed
from multi-scale UAV images as a bridge between street and satellite
viewpoints, and perform feature alignment through self-supervised and
cross-view contrastive learning. Experimental results demonstrate robustness
and generalization across diverse urban scenarios, which achieve 25.75$\%$
Recall@1 accuracy on University-1652 in the UAVM2025 Challenge. Code will be
released at https://github.com/HRT00/CVGL-3D.

</details>


### [299] [LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning](https://arxiv.org/abs/2509.24786)
*Shenghao Fu,Qize Yang,Yuan-Ming Li,Xihan Wei,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 提出LOVE-R1模型，通过慢-快自适应帧采样机制，在长视频理解中实现时空信息的平衡，采用多步推理与解耦强化微调提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视频语言模型因均匀采样机制难以兼顾长时间依赖和细节空间感知，导致时空信息权衡不佳。

Method: 设计可自适应缩放的多步推理框架：先用高密度低分辨率帧覆盖时间范围，再对关键片段进行高分辨率‘放大’；使用38k高质量思维链数据微调，并通过解耦强化学习优化每一步推理。

Result: 在4个长视频理解基准上平均超越基线Qwen2.5-VL 3.1%。

Conclusion: LOVE-R1通过自适应帧采样和解耦强化训练，有效平衡了长视频中的时间连贯性与空间细节，显著提升了理解性能。

Abstract: Long video understanding is still challenging for recent Large Video-Language
Models (LVLMs) due to the conflict between long-form temporal understanding and
detailed spatial perception. LVLMs with a uniform frame sampling mechanism,
which samples frames with an equal frame size and fixed sampling rate,
inevitably sacrifice either temporal clues or spatial details, resulting in
suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model
that can adaptively zoom in on a video clip. The model is first provided with
densely sampled frames but in a small resolution. If some spatial details are
needed, the model can zoom in on a clip of interest with a large frame
resolution based on its reasoning until key visual information is obtained. The
whole process is implemented as a multi-step reasoning process. To train the
reasoning ability, we first finetune the model on our collected 38k
high-quality CoT data and enhance it with decoupled reinforcement finetuning.
As outcome rewards can not provide fine-grained process supervision, we
decouple multi-step reasoning into multiple single-step reasoning and optimize
the internal zoom-in ability explicitly. Experiments on long video
understanding benchmarks show that our model with the slow-fast adaptive frame
sampling mechanism achieves a great trade-off between sampling density and
frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an
average of 3.1% points across 4 common long video understanding benchmarks.

</details>


### [300] [Vision Function Layer in Multimodal LLMs](https://arxiv.org/abs/2509.24791)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 该研究提出视觉功能层（VFL）概念，发现多模态大模型中不同视觉功能（如计数、定位、OCR）分布在特定解码层，并揭示其层次顺序与人类认知行为一致。


<details>
  <summary>Details</summary>
Motivation: 理解多模态大语言模型（MLLMs）中视觉信息解码的分层机制，提升模型可解释性与实际应用效率。

Method: 提出视觉Token交换（Visual Token Swapping）分析框架，通过修改KV缓存来精确定位各层功能；基于VFL进行LoRA微调（VFL-LoRA）和数据选择（VFL-select）。

Result: 发现了不同视觉功能集中在2-3个特定层（VFL），且其深度顺序跨模型一致；VFL-LoRA优于全量LoRA并防止功能遗忘；VFL-select仅用20%数据达到全数据98%性能。

Conclusion: VFL概念有助于深入理解MLLM视觉处理机制，为构建更高效、可解释、鲁棒的模型提供新路径。

Abstract: This study identifies that visual-related functional decoding is distributed
across different decoder layers in Multimodal Large Language Models (MLLMs).
Typically, each function, such as counting, grounding, or OCR recognition,
narrows down to two or three layers, which we define as Vision Function Layers
(VFL). Additionally, the depth and its order of different VFLs exhibits a
consistent pattern across different MLLMs, which is well-aligned with human
behaviors (e.g., recognition occurs first, followed by counting, and then
grounding). These findings are derived from Visual Token Swapping, our novel
analytical framework that modifies targeted KV cache entries to precisely
elucidate layer-specific functions during decoding. Furthermore, these insights
offer substantial utility in tailoring MLLMs for real-world downstream
applications. For instance, when LoRA training is selectively applied to VFLs
whose functions align with the training data, VFL-LoRA not only outperform
full-LoRA but also prevent out-of-domain function forgetting. Moreover, by
analyzing the performance differential on training data when particular VFLs
are ablated, VFL-select automatically classifies data by function, enabling
highly efficient data selection to directly bolster corresponding capabilities.
Consequently, VFL-select surpasses human experts in data selection, and
achieves 98% of full-data performance with only 20% of the original dataset.
This study delivers deeper comprehension of MLLM visual processing, fostering
the creation of more efficient, interpretable, and robust models.

</details>


### [301] [Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation](https://arxiv.org/abs/2509.24798)
*Lei Tong,Zhihua Liu,Chaochao Lu,Dino Oglic,Tom Diethe,Philip Teare,Sotirios A. Tsaftaris,Chen Jin*

Main category: cs.CV

TL;DR: Causal-Adapter 是一种模块化框架，用于适配冻结的文本到图像扩散模型，以实现反事实图像生成，通过引入因果结构实现对目标属性的精确控制和高保真图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示工程但缺乏显式的因果结构，难以准确进行反事实编辑和属性控制，因此需要一种能结合因果建模的方法来提升生成质量和语义一致性。

Method: 提出 Causal-Adapter 框架，基于结构因果模型，采用两种属性正则化策略：提示对齐注入（prompt-aligned injection）和条件 token 对比损失，以解耦属性因子并减少虚假相关性。

Result: 在合成和真实数据集上均达到最优性能，Pendulum 数据集上属性控制 MAE 减少 91%，ADNI 数据集上 FID 减少 87%，显著提升反事实编辑的准确性和图像保真度。

Conclusion: Causal-Adapter 能有效实现鲁棒且可泛化的反事实图像编辑，在保持图像身份的同时实现精准的属性修改。

Abstract: We present Causal-Adapter, a modular framework that adapts frozen
text-to-image diffusion backbones for counterfactual image generation. Our
method enables causal interventions on target attributes, consistently
propagating their effects to causal dependents without altering the core
identity of the image. In contrast to prior approaches that rely on prompt
engineering without explicit causal structure, Causal-Adapter leverages
structural causal modeling augmented with two attribute regularization
strategies: prompt-aligned injection, which aligns causal attributes with
textual embeddings for precise semantic control, and a conditioned token
contrastive loss to disentangle attribute factors and reduce spurious
correlations. Causal-Adapter achieves state-of-the-art performance on both
synthetic and real-world datasets, with up to 91\% MAE reduction on Pendulum
for accurate attribute control and 87\% FID reduction on ADNI for high-fidelity
MRI image generation. These results show that our approach enables robust,
generalizable counterfactual editing with faithful attribute modification and
strong identity preservation.

</details>


### [302] [TACO-Net: Topological Signatures Triumph in 3D Object Classification](https://arxiv.org/abs/2509.24802)
*Anirban Ghosh,Ayan Dutta*

Main category: cs.CV

TL;DR: 提出了一种结合拓扑数据分析与图像滤波技术的3D物体分类方法TACO-Net，通过体素化点云并提取拓扑特征，使用轻量级1D CNN进行分类，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 由于点云的无序性、不规则性和噪声，基于点云的3D物体分类仍面临挑战，现有方法在鲁棒性和准确性方面仍有提升空间。

Method: 将点云转换为体素化二值3D图像，利用拓扑数据分析和图像滤波技术提取区分性拓扑特征，并用轻量级1D CNN对特征进行训练和分类。

Result: 在ModelNet40和ModelNet10上分别达到99.05%和99.52%的准确率，在真实世界OmniObject3D数据集上表现出强鲁棒性，且在十种噪声输入下表现稳定。

Conclusion: TACO-Net是一种高效且鲁棒的3D物体分类框架，结合拓扑特征与轻量网络，在合成与真实数据上均实现了SOTA性能。

Abstract: 3D object classification is a crucial problem due to its significant
practical relevance in many fields, including computer vision, robotics, and
autonomous driving. Although deep learning methods applied to point clouds
sampled on CAD models of the objects and/or captured by LiDAR or RGBD cameras
have achieved remarkable success in recent years, achieving high classification
accuracy remains a challenging problem due to the unordered point clouds and
their irregularity and noise. To this end, we propose a novel state-of-the-art
(SOTA) 3D object classification technique that combines topological data
analysis with various image filtration techniques to classify objects when they
are represented using point clouds. We transform every point cloud into a
voxelized binary 3D image to extract distinguishing topological features. Next,
we train a lightweight one-dimensional Convolutional Neural Network (1D CNN)
using the extracted feature set from the training dataset. Our framework,
TACO-Net, sets a new state-of-the-art by achieving $99.05\%$ and $99.52\%$
accuracy on the widely used synthetic benchmarks ModelNet40 and ModelNet10, and
further demonstrates its robustness on the large-scale real-world OmniObject3D
dataset. When tested with ten different kinds of corrupted ModelNet40 inputs,
the proposed TACO-Net demonstrates strong resiliency overall.

</details>


### [303] [UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections](https://arxiv.org/abs/2509.24817)
*Zeyu Cai,Ziyang Li,Xiaoben Li,Boqian Li,Zeyu Wang,Zhenyu Zhang,Yuliang Xiu*

Main category: cs.CV

TL;DR: UP2You是一种无需调优的高保真3D穿衣人像重建方法，能直接从无约束的野外2D照片中高效生成高质量3D模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要干净、完整的输入图像（如全身照或多视角校准图像），难以处理姿态、视角、遮挡和裁剪变化大的真实场景照片，限制了其在现实场景中的应用。

Method: 提出数据校正范式，通过单次前向传播将无约束输入转换为干净的正交多视角图像；引入姿态相关特征聚合模块（PCFA）以更好保留身份信息，并使用基于perceiver的多参考形状预测器，无需预定义身体模板。

Result: 在4D-Dress、PuzzleIOI和野外数据上表现优越，几何精度（Chamfer下降15%，P2S下降18%）和纹理保真度（PSNR提升21%，LPIPS改善46%）均优于先前方法，处理速度为每人物1.5分钟。

Conclusion: UP2You实现了高效、无需训练、支持任意姿态控制和多服装虚拟试穿的3D重建，适用于真实场景中随意拍摄的人像，推动了该未充分探索任务的发展。

Abstract: We present UP2You, the first tuning-free solution for reconstructing
high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D
photos. Unlike previous approaches that require "clean" inputs (e.g., full-body
images with minimal occlusions, or well-calibrated cross-view captures), UP2You
directly processes raw, unstructured photographs, which may vary significantly
in pose, viewpoint, cropping, and occlusion. Instead of compressing data into
tokens for slow online text-to-3D optimization, we introduce a data rectifier
paradigm that efficiently converts unconstrained inputs into clean, orthogonal
multi-view images in a single forward pass within seconds, simplifying the 3D
reconstruction. Central to UP2You is a pose-correlated feature aggregation
module (PCFA), that selectively fuses information from multiple reference
images w.r.t. target poses, enabling better identity preservation and nearly
constant memory footprint, with more observations. We also introduce a
perceiver-based multi-reference shape predictor, removing the need for
pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and
in-the-wild captures demonstrate that UP2You consistently surpasses previous
methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and
texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5
minutes per person), and versatile (supports arbitrary pose control, and
training-free multi-garment 3D virtual try-on), making it practical for
real-world scenarios where humans are casually captured. Both models and code
will be released to facilitate future research on this underexplored task.
Project Page: https://zcai0612.github.io/UP2You

</details>


### [304] [Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models](https://arxiv.org/abs/2509.24837)
*Youngeun Kim,Youjia Zhang,Huiling Liu,Aecheon Jung,Sunwoo Lee,Sungeun Hong*

Main category: cs.CV

TL;DR: 提出一种无需训练的视觉语言模型token剪枝方法，通过零阶梯度扰动估计token敏感性，在保持准确性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力或多样性的token剪枝方法存在不稳定性或关键信息丢失风险，需更可靠的方法识别重要视觉token。

Method: 在投影层使用零阶梯度扰动估计token敏感性，选择高敏感且互补的视觉token，避免冗余和信息丢失。

Result: 在多个VLM和基准上验证，最高可剪枝94.4% token，实现2.30倍端到端加速，同时保持模型准确性。

Conclusion: \ours是一种高效、稳定且无需训练的token剪枝框架，优于现有注意力和多样性方法，显著降低VLM推理成本。

Abstract: Large Vision-Language Models (VLMs) enable strong multimodal reasoning but
incur heavy inference costs from redundant visual tokens. Token pruning
alleviates this issue, yet existing approaches face limitations.
Attention-based methods rely on raw attention scores, which are often unstable
across layers and heads and can lead to redundant selections. Diversity-based
methods improve robustness by selecting tokens far apart in feature space but
risk dropping regions needed for accurate prediction. We propose \ours, a
training-free framework built on a simple intuition: tokens with higher
sensitivity are more likely to influence the model's output, and they should
also capture complementary visual cues rather than overlapping information. To
achieve this, we estimate token sensitivity using zeroth-order perturbations at
the projection layer, a shallow and computationally light component of the
model. This approach measures how small random perturbations affect the
projection outputs, allowing us to approximate each token's influence through
lightweight forward passes without backpropagation. Extensive experiments
across multiple VLMs and benchmarks show that \ours consistently outperforms
prior methods, pruning up to 94.4\% of tokens while maintaining accuracy and
significantly improving efficiency, achieving up to 2.30x faster end-to-end
inference over the baseline.

</details>


### [305] [PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement](https://arxiv.org/abs/2509.24850)
*Bo Zhao,Dan Guo,Junzhe Cao,Yong Xu,Tao Tan,Yue Sun,Bochao Zou,Jie Zhang,Zitong Yu*

Main category: cs.CV

TL;DR: 提出基于物理原理的rPPG新范式，通过Navier-Stokes方程导出脉搏信号的二阶动态系统模型，设计轻量级PHASE-Net网络，在运动和光照变化下实现高效、准确的非接触生理监测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在头动和光照变化下鲁棒性差，缺乏理论支持，限制了rPPG的准确性和可解释性。

Method: 从血流动力学Navier-Stokes方程出发，推导出脉搏信号服从二阶动态系统，其离散解对应因果卷积，据此设计PHASE-Net：包含零FLOPs轴向交换模块、自适应空间滤波器和门控TCN。

Result: PHASE-Net在多个实验中达到SOTA性能，具有高效率和强鲁棒性，尤其在头动和光照变化场景下表现优异。

Conclusion: 该工作为rPPG提供了理论基础，PHASE-Net兼具理论合理性与部署实用性，是高效、可解释的非接触生理监测方案。

Abstract: Remote photoplethysmography (rPPG) measurement enables non-contact
physiological monitoring but suffers from accuracy degradation under head
motion and illumination changes. Existing deep learning methods are mostly
heuristic and lack theoretical grounding, which limits robustness and
interpretability. In this work, we propose a physics-informed rPPG paradigm
derived from the Navier-Stokes equations of hemodynamics, showing that the
pulse signal follows a second-order dynamical system whose discrete solution
naturally leads to a causal convolution. This provides a theoretical
justification for using a Temporal Convolutional Network (TCN). Based on this
principle, we design PHASE-Net, a lightweight model with three key components:
(1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial
channels to mix distant facial regions and enhance cross-region feature
interaction without breaking temporal order; (2) Adaptive Spatial Filter, which
learns a soft spatial mask per frame to highlight signal-rich areas and
suppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models
long-range temporal dynamics for accurate pulse recovery. Extensive experiments
demonstrate that PHASE-Net achieves state-of-the-art performance with strong
efficiency, offering a theoretically grounded and deployment-ready rPPG
solution.

</details>


### [306] [ELPG-DTFS: Prior-Guided Adaptive Time-Frequency Graph Neural Network for EEG Depression Diagnosis](https://arxiv.org/abs/2509.24860)
*Jingru Qiu,Jiale Liang,Xuanhan Fan,Mingda Zhang,Zhenli He*

Main category: cs.CV

TL;DR: 提出了一种基于先验知识引导的自适应时频图神经网络ELPG-DTFS，用于基于脑电图（EEG）的重度抑郁症（MDD）筛查，在MODMA数据集上达到97.63%准确率，优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型将EEG频谱视为静态图像，固定通道间图结构，并忽略神经科学先验知识，导致诊断准确性与可解释性受限。

Method: 提出ELPG-DTFS模型，包含三项创新：(1) 带跨频段互信息的通道-频段注意力机制；(2) 可学习的邻接矩阵以建模动态功能连接；(3) 残差知识图路径注入神经科学先验。

Result: 在128通道MODMA数据集（53名受试者）上，ELPG-DTFS达到97.63%准确率和97.33% F1分数，优于2025年最先进的ACM-GNN；消融实验显示任一模块移除均使其F1下降最多达4.35%，验证各模块互补性。

Conclusion: ELPG-DTFS提供了一个鲁棒且可解释的框架，有望推动下一代基于EEG的MDD诊断技术发展。

Abstract: Timely and objective screening of major depressive disorder (MDD) is vital,
yet diagnosis still relies on subjective scales. Electroencephalography (EEG)
provides a low-cost biomarker, but existing deep models treat spectra as static
images, fix inter-channel graphs, and ignore prior knowledge, limiting accuracy
and interpretability. We propose ELPG-DTFS, a prior-guided adaptive
time-frequency graph neural network that introduces: (1) channel-band attention
with cross-band mutual information, (2) a learnable adjacency matrix for
dynamic functional links, and (3) a residual knowledge-graph pathway injecting
neuroscience priors. On the 128-channel MODMA dataset (53 subjects), ELPG-DTFS
achieves 97.63% accuracy and 97.33% F1, surpassing the 2025 state-of-the-art
ACM-GNN. Ablation shows that removing any module lowers F1 by up to 4.35,
confirming their complementary value. ELPG-DTFS thus offers a robust and
interpretable framework for next-generation EEG-based MDD diagnostics.

</details>


### [307] [Vision At Night: Exploring Biologically Inspired Preprocessing For Improved Robustness Via Color And Contrast Transformations](https://arxiv.org/abs/2509.24863)
*Lorena Stracke,Lia Nimmermann,Shashank Agnihotri,Margret Keuper,Volker Blanz*

Main category: cs.CV

TL;DR: 提出一种受生物视觉系统启发的预处理方法，通过DoG滤波增强局部对比度，提升语义分割在恶劣天气下的鲁棒性，且不改变模型结构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉系统中对比度增强和颜色拮抗机制的启发，探索能够提高语义分割在不利条件下鲁棒性的输入预处理方法。

Method: 对RGB、灰度和对立颜色通道应用高斯差（Difference-of-Gaussians, DoG）滤波进行生物启发式输入预处理。

Result: 在Cityscapes、ACDC和Dark Zurich数据集上的实验表明，该方法在保持正常条件下性能的同时，显著提升了夜间、雾天和雪天等恶劣条件下的鲁棒性。

Conclusion: 该预处理方法具有模型无关性和轻量化特点，有望集成到成像管道中，为安全关键场景中的下游视觉模型提供任务就绪且鲁棒的输入。

Abstract: Inspired by the human visual system's mechanisms for contrast enhancement and
color-opponency, we explore biologically motivated input preprocessing for
robust semantic segmentation. By applying Difference-of-Gaussians (DoG)
filtering to RGB, grayscale, and opponent-color channels, we enhance local
contrast without modifying model architecture or training. Evaluations on
Cityscapes, ACDC, and Dark Zurich show that such preprocessing maintains
in-distribution performance while improving robustness to adverse conditions
like night, fog, and snow. As this processing is model-agnostic and
lightweight, it holds potential for integration into imaging pipelines,
enabling imaging systems to deliver task-ready, robust inputs for downstream
vision models in safety-critical environments.

</details>


### [308] [StreamForest: Efficient Online Video Understanding with Persistent Event Memory](https://arxiv.org/abs/2509.24871)
*Xiangyu Zeng,Kefan Qiu,Qingyu Zhang,Xinhao Li,Jing Wang,Jiaxin Li,Ziang Yan,Kun Tian,Meng Tian,Xinhai Zhao,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为StreamForest的新架构，用于流式视频理解，通过持久事件记忆森林和细粒度时空窗口提升长期记忆与实时感知能力，并构建了专用指令微调数据集OnlineIT和自动驾驶场景下的新基准ODV-Bench，实验证明该方法在多种基准上达到最先进性能且具有强鲁棒性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在流式视频理解中受限于历史视觉特征的存储压力和实时时空推理能力不足，难以满足实际应用需求。

Method: 提出StreamForest架构，包含持久事件记忆森林（组织视频帧为事件级树结构，基于时间距离、内容相似性和合并频率的惩罚函数进行自适应管理）和细粒度时空窗口（捕捉短期视觉线索以增强当前场景感知），并构建了专用于流式任务的指令微调数据集OnlineIT。

Result: 在StreamingBench、OVBench和OVO-Bench上分别取得77.3%、60.5%和55.6%的准确率，即使在极端视觉token压缩（仅1024 tokens）下仍保持默认设置96.8%的平均精度。

Conclusion: StreamForest在流式视频理解任务中表现出色，具备良好的鲁棒性、效率和泛化能力，适用于资源受限的实际应用场景。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable
progress in video understanding. However, their effectiveness in real-time
streaming scenarios remains limited due to storage constraints of historical
visual features and insufficient real-time spatiotemporal reasoning. To address
these challenges, we propose StreamForest, a novel architecture specifically
designed for streaming video understanding. Central to StreamForest is the
Persistent Event Memory Forest, a memory mechanism that adaptively organizes
video frames into multiple event-level tree structures. This process is guided
by penalty functions based on temporal distance, content similarity, and merge
frequency, enabling efficient long-term memory retention under limited
computational resources. To enhance real-time perception, we introduce a
Fine-grained Spatiotemporal Window, which captures detailed short-term visual
cues to improve current scene perception. Additionally, we present OnlineIT, an
instruction-tuning dataset tailored for streaming video tasks. OnlineIT
significantly boosts MLLM performance in both real-time perception and future
prediction. To evaluate generalization in practical applications, we introduce
ODV-Bench, a new benchmark focused on real-time streaming video understanding
in autonomous driving scenarios. Experimental results demonstrate that
StreamForest achieves the state-of-the-art performance, with accuracies of
77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In
particular, even under extreme visual token compression (limited to 1024
tokens), the model retains 96.8% of its average accuracy in eight benchmarks
relative to the default setting. These results underscore the robustness,
efficiency, and generalizability of StreamForest for streaming video
understanding.

</details>


### [309] [Environment-Aware Satellite Image Generation with Diffusion Models](https://arxiv.org/abs/2509.24875)
*Nikos Kostagiolas,Pantelis Georgiades,Yannis Panagakis,Mihalis A. Nicolaou*

Main category: cs.CV

TL;DR: 提出一种基于环境上下文的新型扩散模型，通过文本、元数据和视觉数据的多模态条件生成卫星图像，显著提升生成质量与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在遥感领域应用中受限于环境上下文利用不足、对缺失或损坏数据处理能力弱以及难以准确反映用户意图。

Method: 提出一种新型扩散模型，结合文本、元数据和视觉数据作为控制信号，并引入元数据融合策略以建模属性嵌入交互，应对部分缺失或损坏的观测。

Result: 在单幅图像和时序生成任务中，定性和定量结果均优于先前方法，包括更高的生成保真度、准确性、质量（6项指标）以及对缺失元数据的鲁棒性和更强的控制响应能力。

Conclusion: 实验证明，结合环境上下文可有效提升遥感图像生成基础模型的性能，所提模型在下游任务中具有广泛应用潜力；同时贡献了首个公开的三模态遥感数据集。

Abstract: Diffusion-based foundation models have recently garnered much attention in
the field of generative modeling due to their ability to generate images of
high quality and fidelity. Although not straightforward, their recent
application to the field of remote sensing signaled the first successful trials
towards harnessing the large volume of publicly available datasets containing
multimodal information. Despite their success, existing methods face
considerable limitations: they rely on limited environmental context, struggle
with missing or corrupted data, and often fail to reliably reflect user
intentions in generated outputs. In this work, we propose a novel diffusion
model conditioned on environmental context, that is able to generate satellite
images by conditioning from any combination of three different control signals:
a) text, b) metadata, and c) visual data. In contrast to previous works, the
proposed method is i) to our knowledge, the first of its kind to condition
satellite image generation on dynamic environmental conditions as part of its
control signals, and ii) incorporating a metadata fusion strategy that models
attribute embedding interactions to account for partially corrupt and/or
missing observations. Our method outperforms previous methods both
qualitatively (robustness to missing metadata, higher responsiveness to control
inputs) and quantitatively (higher fidelity, accuracy, and quality of
generations measured using 6 different metrics) in the trials of single-image
and temporal generation. The reported results support our hypothesis that
conditioning on environmental context can improve the performance of foundation
models for satellite imagery, and render our model a promising candidate for
usage in downstream tasks. The collected 3-modal dataset is to our knowledge,
the first publicly-available dataset to combine data from these three different
mediums.

</details>


### [310] [Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs](https://arxiv.org/abs/2509.24880)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 本文通过合并多个数据集构建了一个16类车辆识别数据集，并采用SMOTE过采样和欠采样方法创建了六种平衡版本，比较了轻量级集成模型与深度CNN的性能，发现CNN表现更优，但最稀有类别（Barge）仍难以识别，表明需结合更多小样本数据收集和代价敏感学习等策略。


<details>
  <summary>Details</summary>
Motivation: 公共数据集中严重的类别不平衡问题抑制了稀有车辆类别的识别性能，影响智能交通与物流系统的准确性，因此需要系统性地评估不同重平衡策略和模型在车辆类型识别中的效果。

Method: 整合Kaggle、ImageNet和网络爬取数据构建约47,000张图像的16类车辆数据集，采用SMOTE过采样和针对性欠采样生成六种平衡变体；对比Random Forest、AdaBoost及基于MobileNet-V2特征的软投票集成模型与使用强数据增强和标签平滑的可配置ResNet风格CNN的性能。

Result: 最佳集成模型（SMOTE-组合）测试准确率为74.8%，CNN在完整测试集上达到79.19%，在未见推理批次上达81.25%，显示深度模型优势；但最稀有类别Barge仍表现极差，揭示仅靠重平衡的局限性。

Conclusion: 尽管重平衡技术和深度CNN提升了整体车辆识别性能，但对极端稀有类别的识别仍具挑战，未来应优先收集更多少数类样本，采用代价敏感目标（如focal loss），并探索结合可解释性与强表征能力的混合模型（如集成-CNN）管道。

Abstract: Accurate vehicle type recognition underpins intelligent transportation and
logistics, but severe class imbalance in public datasets suppresses performance
on rare categories. We curate a 16-class corpus (~47k images) by merging
Kaggle, ImageNet, and web-crawled data, and create six balanced variants via
SMOTE oversampling and targeted undersampling. Lightweight ensembles, such as
Random Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2
features are benchmarked against a configurable ResNet-style CNN trained with
strong augmentation and label smoothing. The best ensemble (SMOTE-combined)
attains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set
and 81.25% on an unseen inference batch, confirming the advantage of deep
models. Nonetheless, the most under-represented class (Barge) remains a failure
mode, highlighting the limits of rebalancing alone. Results suggest
prioritizing additional minority-class collection and cost-sensitive objectives
(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine
interpretability with representational power.

</details>


### [311] [VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines](https://arxiv.org/abs/2509.24891)
*Mostafa Mohaimen Akand Faisal,Rabeya Amin Jhuma*

Main category: cs.CV

TL;DR: 本文提出了一种名为VagueGAN的攻击方法，通过在生成模型（如GAN和扩散模型）的潜在空间中引入隐蔽扰动，实现对生成图像的可控篡改，且可能提升输出视觉质量，揭示了现有像素级防御的盲点。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型（如GAN和扩散模型）在面对隐蔽输入扰动时的安全漏洞，探索针对生成式管道的新型攻击方式，尤其是潜在空间中毒的可能性。

Method: 提出VagueGAN攻击框架，结合模块化扰动网络PoisonerNet与生成器-判别器结构，生成隐蔽触发器；使用代理指标评估攻击效果，并通过感知和频域分析评估隐蔽性；进一步在基于ControlNet的扩散模型中测试迁移性。

Result: 实验证明该方法可在不显眼的情况下实现对生成图像的定向控制，部分中毒输出的视觉质量甚至高于干净样本；潜在空间扰动可保持或增强美学质量，且攻击具有跨模型的可迁移性。

Conclusion: 生成模型的潜在空间中毒构成新型安全威胁，因其既能保持甚至提升输出质量，又可绕过传统像素级防御机制，需引起重视并开发新的防护策略。

Abstract: Generative models such as GANs and diffusion models are widely used to
synthesize photorealistic images and to support downstream creative and editing
tasks. While adversarial attacks on discriminative models are well studied,
attacks targeting generative pipelines where small, stealthy perturbations in
inputs lead to controlled changes in outputs are less explored. This study
introduces VagueGAN, an attack pipeline combining a modular perturbation
network PoisonerNet with a Generator Discriminator pair to craft stealthy
triggers that cause targeted changes in generated images. Attack efficacy is
evaluated using a custom proxy metric, while stealth is analyzed through
perceptual and frequency domain measures. The transferability of the method to
a modern diffusion based pipeline is further examined through ControlNet guided
editing. Interestingly, the experiments show that poisoned outputs can display
higher visual quality compared to clean counterparts, challenging the
assumption that poisoning necessarily reduces fidelity. Unlike conventional
pixel level perturbations, latent space poisoning in GANs and diffusion
pipelines can retain or even enhance output aesthetics, exposing a blind spot
in pixel level defenses. Moreover, carefully optimized perturbations can
produce consistent, stealthy effects on generator outputs while remaining
visually inconspicuous, raising concerns for the integrity of image generation
pipelines.

</details>


### [312] [DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping](https://arxiv.org/abs/2509.24893)
*Yu Ma,Guoliang Wei,Yue Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为DWGS的新框架，用于在稀疏视角下提升3D高斯点阵（3DGS）的新型视图合成效果，通过引入混合损失深度估计、双向虚拟视图合成和遮挡感知重建模块，在标准数据集上实现了最先进的性能，同时保持实时推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于多视角约束有限，稀疏视角下的新视图合成常面临过拟合、几何失真和场景恢复不完整的问题；而现有3DGS方法在稀疏输入下存在漂浮伪影和结构不一致的缺陷，因此需要更鲁棒的解决方案。

Method: 提出DWGS框架，包含三个核心模块：1）结合重投影、点传播和光滑性约束的混合损失深度估计模块以增强多视角一致性；2）双向扭曲虚拟视图合成方法生成虚拟训练视图以加强几何与光度约束；3）利用深度差掩码和基于学习的修复模型进行遮挡区域重建。

Result: 在LLFF、Blender和DTU等标准基准上的实验表明，DWGS在PSNR上最高达到21.13 dB，LPIPS指标为0.189，显著优于现有方法，并保持实时渲染性能。

Conclusion: DWGS通过融合结构先验、虚拟视图约束与遮挡感知修复，有效提升了稀疏视角下的新视图合成质量，解决了3DGS在稀疏输入下的关键缺陷，实现了高保真且实时的渲染效果。

Abstract: Novel View Synthesis (NVS) from sparse views remains a core challenge in 3D
reconstruction, typically suffering from overfitting, geometric distortion, and
incomplete scene recovery due to limited multi-view constraints. Although 3D
Gaussian Splatting (3DGS) enables real-time, high-fidelity rendering, it
suffers from floating artifacts and structural inconsistencies under
sparse-input settings. To address these issues, we propose DWGS, a novel
unified framework that enhances 3DGS for sparse-view synthesis by integrating
robust structural cues, virtual view constraints, and occluded region
completion. Our approach introduces three principal contributions: a
Hybrid-Loss Depth Estimation module that leverages dense matching priors with
reprojection, point propagation, and smoothness constraints to enforce
multi-view consistency; a Bidirectional Warping Virtual View Synthesis method
generates virtual training views to impose stronger geometric and photometric
constraints; and an Occlusion-Aware Reconstruction component that utilizes
depth-difference mask and a learning-based inpainting model to recover obscured
regions. Extensive experiments on standard benchmarks (LLFF, Blender, and DTU)
show that DWGS achieves a new state-of-the-art, achieving up to 21.13 dB PSNR
and 0.189 LPIPS, while retaining real-time inference capabilities.

</details>


### [313] [DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation](https://arxiv.org/abs/2509.24896)
*Xi Chen,Hongxun Yao,Zhaopan Xu,Kui Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为DAM的新框架，通过整合视觉-语言模型的多模态监督与人工标注，形成双重监督信号，显著提升了源域自由主动域适应的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效融合视觉-语言模型与数据监督，限制了伪标签质量和特征对齐效果。

Method: 提出DAM框架，利用ViL模型生成稳定的初始目标，并采用双向知识蒸馏机制，在迭代适应过程中实现目标模型与双重监督信号之间的相互知识交换。

Result: 在多个SFADA基准和主动学习策略上，DAM consistently超越现有方法，达到新的state-of-the-art水平。

Conclusion: DAM通过有效融合多模态监督与人工标注，显著提升了源域自由主动域适应的性能，验证了双重监督信号与双向蒸馏机制的有效性。

Abstract: Source-free active domain adaptation (SFADA) enhances knowledge transfer from
a source model to an unlabeled target domain using limited manual labels
selected via active learning. While recent domain adaptation studies have
introduced Vision-and-Language (ViL) models to improve pseudo-label quality or
feature alignment, they often treat ViL-based and data supervision as separate
sources, lacking effective fusion. To overcome this limitation, we propose Dual
Active learning with Multimodal (DAM) foundation model, a novel framework that
integrates multimodal supervision from a ViL model to complement sparse human
annotations, thereby forming a dual supervisory signal. DAM initializes stable
ViL-guided targets and employs a bidirectional distillation mechanism to foster
mutual knowledge exchange between the target model and the dual supervisions
during iterative adaptation. Extensive experiments demonstrate that DAM
consistently outperforms existing methods and sets a new state-of-the-art
across multiple SFADA benchmarks and active learning strategies.

</details>


### [314] [Accurate Cobb Angle Estimation via SVD-Based Curve Detection and Vertebral Wedging Quantification](https://arxiv.org/abs/2509.24898)
*Chang Shi,Nan Meng,Yipeng Zhuang,Moxin Zhao,Jason Pui Yin Cheung,Hua Huang,Xiuyuan Chen,Cong Nie,Wenting Zhong,Guiqiang Jiang,Yuxin Wei,Jacob Hong Man Yu,Si Chen,Xiaowen Ou,Teng Zhang*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的脊柱侧弯评估新框架，结合HRNet与Swin-Transformer，并引入生物力学约束和椎体楔形指数（VWI），在真实临床数据上实现高精度、强泛化能力的自动AIS评估。


<details>
  <summary>Details</summary>
Motivation: 传统Cobb角测量存在观察者变异大、依赖预设曲线模式的问题，现有自动化方法难以应对临床复杂性，因此需要更符合解剖实际且灵活的评估方法。

Method: 采用HRNet与Swin-Transformer混合骨干网络，同时预测每个椎体的上下终板角度及中点坐标，利用奇异值分解（SVD）从椎体形态直接分析角度，引入生物力学约束提升特征提取能力，并提出椎体楔形指数（VWI）。

Result: 在630例全脊柱正位X光片上达到83.45%的诊断准确率和2.55°的平均绝对误差，模型在分布外数据上表现出优异泛化能力；纵向分析显示VWI与侧弯进展显著相关，而Cobb角无此相关性。

Conclusion: 该框架能更准确、稳定地评估AIS严重程度，VWI作为新指标具有良好的预后价值，支持早期检测、个体化治疗和病情监测。

Abstract: Adolescent idiopathic scoliosis (AIS) is a common spinal deformity affecting
approximately 2.2% of boys and 4.8% of girls worldwide. The Cobb angle serves
as the gold standard for AIS severity assessment, yet traditional manual
measurements suffer from significant observer variability, compromising
diagnostic accuracy. Despite prior automation attempts, existing methods use
simplified spinal models and predetermined curve patterns that fail to address
clinical complexity. We present a novel deep learning framework for AIS
assessment that simultaneously predicts both superior and inferior endplate
angles with corresponding midpoint coordinates for each vertebra, preserving
the anatomical reality of vertebral wedging in progressive AIS. Our approach
combines an HRNet backbone with Swin-Transformer modules and biomechanically
informed constraints for enhanced feature extraction. We employ Singular Value
Decomposition (SVD) to analyze angle predictions directly from vertebral
morphology, enabling flexible detection of diverse scoliosis patterns without
predefined curve assumptions. Using 630 full-spine anteroposterior radiographs
from patients aged 10-18 years with rigorous dual-rater annotation, our method
achieved 83.45% diagnostic accuracy and 2.55{\deg} mean absolute error. The
framework demonstrates exceptional generalization capability on
out-of-distribution cases. Additionally, we introduce the Vertebral Wedging
Index (VWI), a novel metric quantifying vertebral deformation. Longitudinal
analysis revealed VWI's significant prognostic correlation with curve
progression while traditional Cobb angles showed no correlation, providing
robust support for early AIS detection, personalized treatment planning, and
progression monitoring.

</details>


### [315] [Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer](https://arxiv.org/abs/2509.24899)
*Mohsen Ghafoorian,Denis Korzhenkov,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 提出了一种名为“注意力手术”（Attention Surgery）的高效框架，用于在预训练的视频扩散模型中线性化或混合注意力机制，显著降低计算成本的同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: Transformer-based视频扩散模型（VDMs）因自注意力的二次复杂度而在长序列和高分辨率下计算昂贵，现有线性注意力方法难以在不重新训练的情况下匹配softmax注意力的表达能力。

Method: 提出一种混合注意力机制，结合softmax和线性注意力 token，并设计轻量级蒸馏与微调流程；引入基于代价感知的块率策略，在不同层间平衡效率与表达力。

Result: 在Wan2.1 1.3B模型上应用后，注意力计算开销最多减少40% FLOPs，在VBench和VBench-2.0基准上保持竞争力的生成质量，首次实现高效的亚二次复杂度视频扩散模型。

Conclusion: Attention Surgery为预训练VDM提供了一种无需从头训练即可提升效率的有效注意力优化方案，推动了高质量视频生成的实用化。

Abstract: Transformer-based video diffusion models (VDMs) deliver state-of-the-art
video generation quality but are constrained by the quadratic cost of
self-attention, making long sequences and high resolutions computationally
expensive. While linear attention offers sub-quadratic complexity, prior
attempts fail to match the expressiveness of softmax attention without costly
retraining. We introduce \textit{Attention Surgery}, an efficient framework for
\textit{linearizing} or \textit{hybridizing} attention in pretrained VDMs
without training from scratch. Inspired by recent advances in language models,
our method combines a novel hybrid attention mechanism-mixing softmax and
linear tokens-with a lightweight distillation and fine-tuning pipeline
requiring only a few GPU-days. Additionally, we incorporate a cost-aware
block-rate strategy to balance expressiveness and efficiency across layers.
Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery
achieves the first competitive sub-quadratic attention video diffusion models,
reducing attention cost by up to 40\% in terms of FLOPs, while maintaining
generation quality as measured on the standard VBench and VBench-2.0
benchmarks.

</details>


### [316] [OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing](https://arxiv.org/abs/2509.24900)
*Zhihong Chen,Xuehai Bai,Yang Shi,Chaoyou Fu,Huanyu Zhang,Haotian Wang,Xiaoyan Sun,Zhang Zhang,Liang Wang,Yuanxing Zhang,Pengfei Wan,Yi-Fan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为OpenGPT-4o-Image的大规模多模态数据集，通过分层任务分类与自动化生成方法，覆盖11个领域和51个子任务，显著提升了图像生成与编辑模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成与编辑数据集缺乏系统性和复杂场景，限制了统一多模态模型在真实应用场景中的表现，因此需要构建更全面、结构化的高质量数据集。

Method: 提出一种结合分层任务分类与自动化数据生成的新方法，利用结构化资源池和GPT-4o构建包含80k指令-图像对的数据集，涵盖基础能力（如文本渲染、风格控制）和高挑战性任务（如化学科学图像、复杂多操作编辑）。

Result: 在多个基准测试中，基于该数据集微调的模型表现出显著性能提升，图像编辑任务最高提升18%（UniWorld-V1在ImgEdit-Bench上），生成任务提升13%（Harmon在GenEval上）。

Conclusion: 系统性的数据构建是推动多模态AI能力进步的关键，OpenGPT-4o-Image为未来研究提供了高质量、多样化且结构化的数据支持。

Abstract: The performance of unified multimodal models for image generation and editing
is fundamentally constrained by the quality and comprehensiveness of their
training data. While existing datasets have covered basic tasks like style
transfer and simple object manipulation, they often lack the systematic
structure and challenging scenarios required for real-world applications. To
address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset
constructed using a novel methodology that combines hierarchical task taxonomy
with automated data generation. Our taxonomy not only includes fundamental
capabilities such as text rendering and style control but also introduces
highly practical yet challenging categories like scientific imagery for
chemistry illustrations and complex instruction editing requiring simultaneous
execution of multiple operations. Through an automated pipeline leveraging
structured resource pools and GPT-4o, we generate 80k high-quality
instruction-image pairs with controlled diversity, covering 11 major domains
and 51 subtasks. Extensive experiments show that fine-tuning leading models on
our dataset achieves significant performance gains across multiple benchmarks,
with improvements of up to 18\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)
and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that
systematic data construction is key to advancing multimodal AI capabilities.

</details>


### [317] [Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale](https://arxiv.org/abs/2509.24910)
*Songze Li,Zun Wang,Gengze Zhou,Jialu Li,Xiangyu Zeng,Limin Wang,Yu Qiao,Qi Wu,Mohit Bansal,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SID的自改进演示方法，用于目标导向的语言导航任务，通过迭代生成具有更强探索策略的轨迹来提升智能体的探索能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖最短路径轨迹，缺乏有效的探索先验，难以在未知环境中进行充分探索。

Method: SID首先在环境采样的最短路径数据上训练初始智能体，然后利用该智能体生成新的探索轨迹，将其作为演示用于训练更优的智能体，并迭代优化。

Result: SID显著提升了智能体的探索能力和泛化性，在REVERIE和SOON等任务上达到SOTA性能，尤其在SOON的未见验证集上实现了50.9%的成功率，超越先前最优方法13.9%。

Conclusion: SID通过自改进的演示机制有效增强了语言导航智能体的探索能力，具备良好的可扩展性和跨任务迁移能力。

Abstract: Goal-oriented language-guided navigation requires robust exploration
capabilities for agents to navigate to specified goals in unknown environments
without step-by-step instructions. Existing methods tend to exclusively utilize
shortest-path trajectories, lacking effective exploration priors for training
navigation agents. To address the above challenges, we present SID, a
goal-oriented language-guided navigation learning approach with Self-Improving
Demonstrations. Specifically, SID learns an initial agent on the shortest-path
data sampled from environments and then leverages this agent to generate novel
exploration trajectories. The novel rollouts provide demonstrations with
stronger exploration strategies to train a better agent, which in turn produces
higher-quality agent demonstrations for the next round of training. We show
that this iterative self-improving pipeline readily scales to new environments,
and the resulting demonstrations can be transferred across a variety of
language-guided navigation tasks, elevating the performance ceiling in diverse
goal-oriented navigation tasks. Extensive experiments demonstrate that SID
significantly boosts the exploration capabilities and generalization of
navigation agents. The resulting agent achieves new state-of-the-art
performance on goal-oriented language-guided navigation tasks, including
REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation
splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.

</details>


### [318] [Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis](https://arxiv.org/abs/2509.24913)
*Tian Xia,Matthew Sinclair,Andreas Schuh,Fabio De Sousa Ribeiro,Raghav Mehta,Rajat Rasal,Esther Puyol-Antón,Samuel Gerber,Kersten Petersen,Michiel Schaap,Ben Glocker*

Main category: cs.CV

TL;DR: 提出了一种名为Seg-CFT的新方法，用于生成结构特定的反事实图像，能够在保持局部一致性的同时有效干预标量变量，适用于胸部X光片等医学图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有反事实图像生成方法在结构特定干预时存在不足，依赖外部分类器或回归器可能导致全局不良影响，且先前方法需要繁琐的手动分割标注。

Method: 提出Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT)，利用分割器指导反事实微调，无需像素级标签图，仅干预结构特定的标量变量。

Result: 在生成逼真的胸部X光片方面表现出色，并在模拟冠状动脉疾病建模中取得初步成果。

Conclusion: Seg-CFT能够在不引入全局副作用的情况下实现局部结构的精确干预，是一种简洁且有效的反事实图像生成方法。

Abstract: Counterfactual image generation is a powerful tool for augmenting training
data, de-biasing datasets, and modeling disease. Current approaches rely on
external classifiers or regressors to increase the effectiveness of
subject-level interventions (e.g., changing the patient's age). For
structure-specific interventions (e.g., changing the area of the left lung in a
chest radiograph), we show that this is insufficient, and can result in
undesirable global effects across the image domain. Previous work used
pixel-level label maps as guidance, requiring a user to provide hypothetical
segmentations which are tedious and difficult to obtain. We propose
Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the
simplicity of intervening on scalar-valued, structure-specific variables while
producing locally coherent and effective counterfactuals. We demonstrate the
capability of generating realistic chest radiographs, and we show promising
results for modeling coronary artery disease. Code:
https://github.com/biomedia-mira/seg-cft.

</details>


### [319] [Scalable GANs with Transformers](https://arxiv.org/abs/2509.24935)
*Sangeek Hyun,MinKyu Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出了一种基于纯Transformer架构和潜在空间训练的生成对抗网络（GAT），通过引入轻量级中间监督和宽度感知学习率调整，解决了GAN在扩展过程中出现的早期层利用不足和优化不稳定问题，在ImageNet-256上实现了2.96的FID性能，且仅需40个周期。


<details>
  <summary>Details</summary>
Motivation: 探索生成对抗网络（GANs）的可扩展性，借鉴其他生成模型的成功设计（如潜在空间训练和纯Transformer结构），以提升训练效率和生成性能。

Method: 采用变分自编码器的紧凑潜在空间进行GAN训练，并使用纯Transformer作为生成器和判别器；引入轻量级中间监督缓解生成器早期层利用不足问题，采用宽度感知学习率调整策略提升优化稳定性。

Result: GAT模型在多种容量下均能稳定训练；GAT-XL/2在ImageNet-256上的单步类条件生成达到2.96的FID，为当前最优，且训练周期仅为先前强基线的1/6（40个epoch）。

Conclusion: 通过合理的架构设计与优化策略，GAN同样可以实现良好的可扩展性，纯Transformer结构与潜在空间训练相结合是构建高效、高性能GAN的有效路径。

Abstract: Scalability has driven recent advances in generative modeling, yet its
principles remain underexplored for adversarial learning. We investigate the
scalability of Generative Adversarial Networks (GANs) through two design
choices that have proven to be effective in other types of generative models:
training in a compact Variational Autoencoder latent space and adopting purely
transformer-based generators and discriminators. Training in latent space
enables efficient computation while preserving perceptual fidelity, and this
efficiency pairs naturally with plain transformers, whose performance scales
with computational budget. Building on these choices, we analyze failure modes
that emerge when naively scaling GANs. Specifically, we find issues as
underutilization of early layers in the generator and optimization instability
as the network scales. Accordingly, we provide simple and scale-friendly
solutions as lightweight intermediate supervision and width-aware learning-rate
adjustment. Our experiments show that GAT, a purely transformer-based and
latent-space GANs, can be easily trained reliably across a wide range of
capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art
single-step, class-conditional generation performance (FID of 2.96) on
ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.

</details>


### [320] [Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents](https://arxiv.org/abs/2509.24943)
*Jiahua Li,Kun Wei,Zhe Xu,Zibo Su,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出CogniGPT框架，通过多粒度感知与验证增强反思的交互机制，实现高效可靠的长视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法在长视频理解中难以兼顾完整性和效率，且易产生幻觉，需更可靠的信息提取机制。

Method: 设计双代理交互框架：多粒度感知代理（MGPA）模拟人类注意力捕获任务相关线索，验证增强反思代理（VERA）验证关键线索并优化感知策略。

Result: 在EgoSchema、Video-MME、NExT-QA和MovieChat数据集上实验表明，CogniGPT在准确性和效率方面均优于现有无训练方法；在EgoSchema上仅用11.2帧即达到与Gemini 1.5-Pro相当的性能。

Conclusion: CogniGPT通过模拟人类渐进式视觉认知的交互式推理机制，有效提升了长视频理解的准确性与效率，具有较强的实用潜力。

Abstract: Long videos, characterized by temporal complexity and sparse task-relevant
information, pose significant reasoning challenges for AI systems. Although
various Large Language Model (LLM)-based approaches have advanced long video
understanding, they still struggle to achieve both completeness and efficiency
in capturing task-critical information. Inspired by human progressive visual
cognition, we propose CogniGPT, a framework that leverages an interactive loop
between Multi-Granular Perception Agent (MGPA) and Verification-Enhanced
Reflection Agent (VERA) for efficient and reliable long video understanding.
Specifically, MGPA mimics human visual divergent and focused attention to
capture task-related information, while VERA verifies perceived key clues to
mitigate hallucination and optimize subsequent perception strategies. Through
this interactive process, CogniGPT explores a minimal set of informative and
reliable task-related clues. Extensive experiments on EgoSchema, Video-MME,
NExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both
accuracy and efficiency. Notably, on EgoSchema, it surpasses existing
training-free methods using only 11.2 frames and achieves performance
comparable to Gemini 1.5-Pro.

</details>


### [321] [Evaluating Temperature Scaling Calibration Effectiveness for CNNs under Varying Noise Levels in Brain Tumour Detection](https://arxiv.org/abs/2509.24951)
*Ankur Chanda,Kushan Choudhury,Shubhrodeep Roy,Shubhajit Biswas,Somenath Kuiry*

Main category: cs.CV

TL;DR: 本研究评估了温度缩放（TS）在提高卷积神经网络（CNN）用于脑肿瘤分类的可靠性方面的有效性，通过引入五种图像噪声模拟真实世界的不确定性，并证明TS能显著降低预期校准误差（ECE）和负对数似然（NLL），同时不损害分类精度。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的精确置信度估计在医学影像等高风险领域至关重要，过度自信的错误分类可能导致严重后果。因此，需要提升模型在不确定环境下的预测可靠性。

Method: 开发了一个自定义CNN并在合并的脑MRI数据集上进行训练，采用温度缩放（TS）作为事后校准技术，并引入高斯、泊松、椒盐、斑点和均匀五种噪声来模拟现实中的不确定性；使用准确率、精确率、召回率、F1分数、NLL和ECE指标评估模型在校准前后的性能。

Result: TS在校准后显著降低了所有噪声条件下的ECE和NLL，且未牺牲分类准确性。

Conclusion: 温度缩放在各种噪声条件下均有效提升了模型的置信度校准效果，是一种计算高效、适用于医疗AI系统的方法，有助于增强在噪声或不确定环境中的决策可靠性。

Abstract: Precise confidence estimation in deep learning is vital for high-stakes
fields like medical imaging, where overconfident misclassifications can have
serious consequences. This work evaluates the effectiveness of Temperature
Scaling (TS), a post-hoc calibration technique, in improving the reliability of
convolutional neural networks (CNNs) for brain tumor classification. We develop
a custom CNN and train it on a merged brain MRI dataset. To simulate real-world
uncertainty, five types of image noise are introduced: Gaussian, Poisson, Salt
& Pepper, Speckle, and Uniform. Model performance is evaluated using precision,
recall, F1-score, accuracy, negative log-likelihood (NLL), and expected
calibration error (ECE), both before and after calibration. Results demonstrate
that TS significantly reduces ECE and NLL under all noise conditions without
degrading classification accuracy. This underscores TS as an effective and
computationally efficient approach to enhance decision confidence of medical AI
systems, hence making model outputs more reliable in noisy or uncertain
settings.

</details>


### [322] [Social 3D Scene Graphs: Modeling Human Actions and Relations for Interactive Service Robots](https://arxiv.org/abs/2509.24966)
*Ermanno Bartoli,Dennis Rotondi,Buwei He,Patric Jensfelt,Kai O. Arras,Iolanda Leite*

Main category: cs.CV

TL;DR: 本文提出了Social 3D Scene Graphs，一种增强的3D场景图表示方法，用于捕捉环境中的人类、属性、活动及关系，支持开放词汇框架下的局部和远程交互建模，并引入了一个包含丰富人类-场景关系标注的新基准，实验证明该方法在人类活动预测和人-环境关系推理方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图方法大多忽略场景中的人类及其与环境的关系，且仅基于单帧图像捕捉短距离关系，难以支持机器人实现社会合规和情境感知行为。

Method: 提出Social 3D Scene Graphs，扩展传统3D场景图以包含人类属性、活动及局部与远程关系，采用开放词汇框架，并构建合成环境数据集进行评估。

Result: 实验表明，该方法在人类活动预测和人-环境关系推理任务中性能提升，验证了其在社交场景理解中的有效性。

Conclusion: Social 3D Scene Graphs为实现 socially intelligent 的机器人提供了有效的语义表示基础。

Abstract: Understanding how people interact with their surroundings and each other is
essential for enabling robots to act in socially compliant and context-aware
ways. While 3D Scene Graphs have emerged as a powerful semantic representation
for scene understanding, existing approaches largely ignore humans in the
scene, also due to the lack of annotated human-environment relationships.
Moreover, existing methods typically capture only open-vocabulary relations
from single image frames, which limits their ability to model long-range
interactions beyond the observed content. We introduce Social 3D Scene Graphs,
an augmented 3D Scene Graph representation that captures humans, their
attributes, activities and relationships in the environment, both local and
remote, using an open-vocabulary framework. Furthermore, we introduce a new
benchmark consisting of synthetic environments with comprehensive human-scene
relationship annotations and diverse types of queries for evaluating social
scene understanding in 3D. The experiments demonstrate that our representation
improves human activity prediction and reasoning about human-environment
relations, paving the way toward socially intelligent robots.

</details>


### [323] [Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning](https://arxiv.org/abs/2509.24968)
*Donghwa Kang,Junho Kim,Dongwoo Kang*

Main category: cs.CV

TL;DR: 提出了一种基于跨模态融合注意力（CMFA）和自监督多事件表示学习（SSMER）的新型框架，用于基于事件的面部关键点对齐，在真实和合成事件数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB面部关键点对齐方法在事件数据上表现不佳，纯事件数据训练受限于空间信息不足，且缺乏标注完整的事件数据集。

Method: 提出CMFA模块融合RGB数据以引导模型从事件数据中提取鲁棒特征，同时引入SSMER进行自监督学习，利用未标注事件数据提升特征学习效果。

Result: 在真实事件数据集E-SIE和合成事件版本WFLW-V上实验表明，该方法在多个评估指标上 consistently 超过最先进方法。

Conclusion: 所提出的CMFA和SSMER联合框架有效解决了事件数据中面部关键点对齐的挑战，显著提升了性能。

Abstract: Event cameras offer unique advantages for facial keypoint alignment under
challenging conditions, such as low light and rapid motion, due to their high
temporal resolution and robustness to varying illumination. However, existing
RGB facial keypoint alignment methods do not perform well on event data, and
training solely on event data often leads to suboptimal performance because of
its limited spatial information. Moreover, the lack of comprehensive labeled
event datasets further hinders progress in this area. To address these issues,
we propose a novel framework based on cross-modal fusion attention (CMFA) and
self-supervised multi-event representation learning (SSMER) for event-based
facial keypoint alignment. Our framework employs CMFA to integrate
corresponding RGB data, guiding the model to extract robust facial features
from event input images. In parallel, SSMER enables effective feature learning
from unlabeled event data, overcoming spatial limitations. Extensive
experiments on our real-event E-SIE dataset and a synthetic-event version of
the public WFLW-V benchmark show that our approach consistently surpasses
state-of-the-art methods across multiple evaluation metrics.

</details>


### [324] [On-the-Fly Data Augmentation for Brain Tumor Segmentation](https://arxiv.org/abs/2509.24973)
*Ishika Jain,Siri Willems,Steven Latre,Tom De Schepper*

Main category: cs.CV

TL;DR: 提出一种基于预训练生成对抗网络（GliGANs）的在线数据增强策略，用于在训练过程中动态插入合成肿瘤，提升模型在不同治疗阶段胶质瘤分割中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 缺乏多样化且高质量的标注数据限制了跨治疗阶段胶质瘤分割模型的训练，现有数据增强方法因存储和计算成本高而受限。

Method: 采用on-the-fly数据增强策略，利用预训练的GliGANs在训练时动态生成并插入合成肿瘤到MRI图像中，构建三种nnU-Net模型及其集成：无增强基线、常规on-the-fly增强和定制化on-the-fly增强模型。

Result: 在BraTS 2025在线验证平台上，模型集成取得了ET: 0.79, NETC: 0.749, RC: 0.872, SNFH: 0.825, TC: 0.79, WT: 0.88的病灶级Dice分数，并在BraTS Lighthouse Challenge 2025 Task 1中排名第一。

Conclusion: 所提出的on-the-fly增强策略有效提升了模型对不同治疗阶段胶质瘤的分割性能，同时避免了大规模增强数据的存储开销，具有良好的临床应用潜力。

Abstract: Robust segmentation across both pre-treatment and post-treatment glioma scans
can be helpful for consistent tumor monitoring and treatment planning. BraTS
2025 Task 1 addresses this by challenging models to generalize across varying
tumor appearances throughout the treatment timeline. However, training such
generalized models requires access to diverse, high-quality annotated data,
which is often limited. While data augmentation can alleviate this, storing
large volumes of augmented 3D data is computationally expensive. To address
these challenges, we propose an on-the-fly augmentation strategy that
dynamically inserts synthetic tumors using pretrained generative adversarial
networks (GliGANs) during training. We evaluate three nnU-Net-based models and
their ensembles: (1) a baseline without external augmentation, (2) a regular
on-the-fly augmented model, and (3) a model with customized on-the-fly
augmentation. Built upon the nnU-Net framework, our pipeline leverages
pretrained GliGAN weights and tumor insertion methods from prior
challenge-winning solutions. An ensemble of the three models achieves
lesion-wise Dice scores of 0.79 (ET), 0.749 (NETC), 0.872 (RC), 0.825 (SNFH),
0.79 (TC), and 0.88 (WT) on the online BraTS 2025 validation platform. This
work ranked first in the BraTS Lighthouse Challenge 2025 Task 1- Adult Glioma
Segmentation.

</details>


### [325] [Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel](https://arxiv.org/abs/2509.24979)
*Haotian Dong,Wenjing Wang,Chen Li,Di Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Wan-Alpha的新框架，用于生成包含透明度信息的RGBA视频，通过联合学习RGB和alpha通道，在视觉质量、运动真实感和透明度渲染方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGBA视频生成方法常忽视视觉质量，限制了其实际应用，因此需要一种能同时提升透明度和视觉效果的新型生成框架。

Method: 设计了一个有效的变分自编码器（VAE），将alpha通道编码到RGB潜在空间，并构建了一个高质量、多样化的RGBA视频数据集以支持扩散Transformer的训练。

Result: Wan-Alpha在生成半透明物体、发光效果和精细细节（如发丝）方面表现出色，显著提升了视觉质量和运动 realism。

Conclusion: Wan-Alpha在RGBA视频生成任务中实现了先进的性能，具备广泛的应用潜力，模型已公开发布。

Abstract: RGBA video generation, which includes an alpha channel to represent
transparency, is gaining increasing attention across a wide range of
applications. However, existing methods often neglect visual quality, limiting
their practical usability. In this paper, we propose \textit{Wan-Alpha}, a new
framework that generates transparent videos by learning both RGB and alpha
channels jointly. We design an effective variational autoencoder (VAE) that
encodes the alpha channel into the RGB latent space. Then, to support the
training of our diffusion transformer, we construct a high-quality and diverse
RGBA video dataset. Compared with state-of-the-art methods, our model
demonstrates superior performance in visual quality, motion realism, and
transparency rendering. Notably, our model can generate a wide variety of
semi-transparent objects, glowing effects, and fine-grained details such as
hair strands. The released model is available on our website:
\href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.

</details>


### [326] [SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation](https://arxiv.org/abs/2509.24980)
*Shuang Liang,Jing He,Chuanmeizhi Wang,Lejun Liao,Guo Zhang,Yingcong Chen,Yuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了SDPose，一种基于Stable Diffusion的微调框架，用于人体姿态估计，通过在图像潜在空间中直接预测关键点热图并结合轻量级卷积头和辅助RGB重建分支，在跨域基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练扩散模型在密集预测任务中展现出强大的跨域泛化能力，但其在结构化输出（如人体姿态估计）中的潜力尚未被充分探索。

Method: SDPose直接在Stable Diffusion U-Net的图像潜在空间中预测关键点热图，使用轻量卷积姿态头，并引入辅助RGB重建分支以增强分布外鲁棒性。

Result: 在仅使用Sapiens五分之一训练时间的情况下，SDPose在COCO验证集上达到与其相当的性能，并在HumanArt和COCO-OOD跨域基准上取得新纪录，同时可作为零样本姿态标注器用于可控生成任务。

Conclusion: SDPose有效利用了预训练扩散先验，在保持生成先验完整性的同时实现了高性能的人体姿态估计，具备良好的跨域泛化和下游应用能力。

Abstract: Pre-trained diffusion models provide rich multi-scale latent features and are
emerging as powerful vision backbones. While recent works such as
Marigold~\citep{ke2024repurposing} and Lotus~\citep{he2024lotus} adapt
diffusion priors for dense prediction with strong cross-domain generalization,
their potential for structured outputs (e.g., human pose estimation) remains
underexplored. In this paper, we propose \textbf{SDPose}, a fine-tuning
framework built upon Stable Diffusion to fully exploit pre-trained diffusion
priors for human pose estimation. First, rather than modifying cross-attention
modules or introducing learnable embeddings, we directly predict keypoint
heatmaps in the SD U-Net's image latent space to preserve the original
generative priors. Second, we map these latent features into keypoint heatmaps
through a lightweight convolutional pose head, which avoids disrupting the
pre-trained backbone. Finally, to prevent overfitting and enhance
out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction
branch that preserves domain-transferable generative semantics. To evaluate
robustness under domain shift, we further construct \textbf{COCO-OOD}, a
style-transferred variant of COCO with preserved annotations. With just
one-fifth of the training schedule used by Sapiens on COCO, SDPose attains
parity with Sapiens-1B/2B on the COCO validation set and establishes a new
state of the art on the cross-domain benchmarks HumanArt and COCO-OOD.
Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream
controllable generation tasks, including ControlNet-based image synthesis and
video generation, where it delivers qualitatively superior pose guidance.

</details>


### [327] [PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion](https://arxiv.org/abs/2509.24997)
*Yuyang Yin,HaoXiang Guo,Fangfu Liu,Mengyu Wang,Hanwen Liang,Eric Li,Yikai Wang,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出PanoWorld-X，一种用于高保真、可控制全景视频生成的新型框架，通过球面感知扩散Transformer架构实现多样化相机轨迹的连续全景场景合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于狭窄视场或相机可控性不足，难以生成连续、完整的360度全景视频，限制了用户或智能体的自由探索能力。

Method: 构建大规模全景视频-探索路径数据集，并设计球面感知扩散Transformer，将等距柱状投影特征重映射到球面以建模潜在空间中的几何邻接关系。

Result: 实验表明，PanoWorld-X在运动范围、控制精度和视觉质量方面均优于现有方法，显著提升时空连续性和视觉保真度。

Conclusion: PanoWorld-X有效解决了全景视频生成中的视场与控制问题，具备广泛的实际应用潜力。

Abstract: Generating a complete and explorable 360-degree visual world enables a wide
range of downstream applications. While prior works have advanced the field,
they remain constrained by either narrow field-of-view limitations, which
hinder the synthesis of continuous and holistic scenes, or insufficient camera
controllability that restricts free exploration by users or autonomous agents.
To address this, we propose PanoWorld-X, a novel framework for high-fidelity
and controllable panoramic video generation with diverse camera trajectories.
Specifically, we first construct a large-scale dataset of panoramic
video-exploration route pairs by simulating camera trajectories in virtual 3D
environments via Unreal Engine. As the spherical geometry of panoramic data
misaligns with the inductive priors from conventional video diffusion, we then
introduce a Sphere-Aware Diffusion Transformer architecture that reprojects
equirectangular features onto the spherical surface to model geometric
adjacency in latent space, significantly enhancing visual fidelity and
spatiotemporal continuity. Extensive experiments demonstrate that our
PanoWorld-X achieves superior performance in various aspects, including motion
range, control precision, and visual quality, underscoring its potential for
real-world applications.

</details>


### [328] [LVT: Large-Scale Scene Reconstruction via Local View Transformers](https://arxiv.org/abs/2509.25001)
*Tooba Imtiaz,Lucy Chai,Kathryn Heal,Xuan Luo,Jungyeon Park,Jennifer Dy,John Flynn*

Main category: cs.CV

TL;DR: 提出了一种名为Local View Transformer (LVT) 的架构，用于大规模场景重建和新视角合成，避免了标准Transformer的二次复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度限制了其在大场景中的应用，而邻近视图对局部场景结构提供更强的信息。

Method: 通过仅处理每个视图周围局部邻域内的信息，并利用基于查询视图与邻近视图之间相对几何变换的新位置编码来实现注意力机制。

Result: 能够在一个前向传递中重建任意大小、高分辨率的3D场景，并输出包含颜色和不透明度视图依赖性的3D高斯点阵场景表示。

Conclusion: Local View Transformer有效解决了大规模场景下Transformer计算复杂度高的问题，实现了高效且高质量的新视角合成。

Abstract: Large transformer models are proving to be a powerful tool for 3D vision and
novel view synthesis. However, the standard Transformer's well-known quadratic
complexity makes it difficult to scale these methods to large scenes. To
address this challenge, we propose the Local View Transformer (LVT), a
large-scale scene reconstruction and novel view synthesis architecture that
circumvents the need for the quadratic attention operation. Motivated by the
insight that spatially nearby views provide more useful signal about the local
scene composition than distant views, our model processes all information in a
local neighborhood around each view. To attend to tokens in nearby views, we
leverage a novel positional encoding that conditions on the relative geometric
transformation between the query and nearby views. We decode the output of our
model into a 3D Gaussian Splat scene representation that includes both color
and opacity view-dependence. Taken together, the Local View Transformer enables
reconstruction of arbitrarily large, high-resolution scenes in a single forward
pass. See our project page for results and interactive demos
https://toobaimt.github.io/lvt/.

</details>


### [329] [CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation](https://arxiv.org/abs/2509.25016)
*Max Curie,Paulo da Costa*

Main category: cs.CV

TL;DR: CLASP是一种无需训练、无需标注数据的轻量级无监督图像分割框架，通过自监督ViT提取特征、谱聚类和DenseCRF优化边界，在COCO Stuff和ADE20K上表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计一种无需人工调参、无需训练且易于复现的无监督图像分割方法，适用于大规模未标注数据场景，如数字广告和内容审核。

Method: 使用自监督ViT（DINO）提取图像块特征，构建相似性矩阵并进行谱聚类；通过特征间隙与轮廓系数搜索自动确定分割数量，并用全连接DenseCRF sharpen边界。

Result: 在COCO Stuff和ADE20K数据集上达到与最新无监督方法相当的mIoU和像素准确率，验证了其有效性。

Conclusion: CLASP是一种简单、高效、可复现的无监督图像分割基准方法，特别适用于实际工业场景中的大规模未标注图像处理。

Abstract: We introduce CLASP (Clustering via Adaptive Spectral Processing), a
lightweight framework for unsupervised image segmentation that operates without
any labeled data or finetuning. CLASP first extracts per patch features using a
self supervised ViT encoder (DINO); then, it builds an affinity matrix and
applies spectral clustering. To avoid manual tuning, we select the segment
count automatically with a eigengap silhouette search, and we sharpen the
boundaries with a fully connected DenseCRF. Despite its simplicity and training
free nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff
and ADE20K, matching recent unsupervised baselines. The zero training design
makes CLASP a strong, easily reproducible baseline for large unannotated
corpora especially common in digital advertising and marketing workflows such
as brand safety screening, creative asset curation, and social media content
moderation

</details>


### [330] [GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning](https://arxiv.org/abs/2509.25026)
*Mustansar Fiaz,Hiyam Debary,Paolo Fraccaro,Danda Paudel,Luc Van Gool,Fahad Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出了一种新的后训练框架，通过引入任务感知奖励来增强基于推理的强化学习模型在地球观测任务中的适应性，显著提升了遥感图像的性能。


<details>
  <summary>Details</summary>
Motivation: 地球观测任务具有独特挑战，需要任务感知的推理能力，而现有强化学习模型在此领域的潜力尚未充分挖掘。

Method: 设计了一个新颖的后训练框架，结合任务感知奖励，以提升基于推理的强化学习模型在多种地球观测任务中的适应性和鲁棒性。

Result: 在多个地球观测基准上实验表明，该方法相比现有的通用和专用视觉语言模型均取得了持续的性能提升。

Conclusion: 所提框架有效增强了模型在遥感图像上的推理能力、优化稳定性和鲁棒性，推动了强化学习在地球观测领域的应用。

Abstract: Recent advances in reinforcement learning (RL) have delivered strong
reasoning capabilities in natural image domains, yet their potential for Earth
Observation (EO) remains largely unexplored. EO tasks introduce unique
challenges, spanning referred object detection, image or region captioning,
change detection, grounding, and temporal analysis, that demand task aware
reasoning. We propose a novel post training framework that incorporates task
aware rewards to enable effective adaptation of reasoning based RL models to
diverse EO tasks. This training strategy enhances reasoning capabilities for
remote sensing images, stabilizes optimization, and improves robustness.
Extensive experiments across multiple EO benchmarks show consistent performance
gains over state of the art generic and specialized vision language models.
Code and models will be released publicly at
https://mustansarfiaz.github.io/GeoVLM-R1/ .

</details>


### [331] [STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation](https://arxiv.org/abs/2509.25027)
*Xiaoxiao Ma,Haibo Qiu,Guohui Zhang,Zhixiong Zeng,Siqi Yang,Lin Ma,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为STAGE的稳定且可泛化的框架，用于改进自回归图像生成中的强化学习训练过程，通过优势/KL重加权和熵奖励解决了梯度冲突和策略熵不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法在应用于自回归图像模型时存在训练不稳定、图像质量下降和泛化能力差的问题，亟需提高训练稳定性与生成效果。

Method: 提出了STAGE框架，包括两种关键技术：1）基于相似性的优势/KL重加权以缓解令牌间的梯度冲突；2）引入基于参考模型的熵奖励来稳定策略学习过程。

Result: 实验表明，STAGE在多个基准上相比基线GRPO方法显著提升了图像视觉质量、训练稳定性和跨任务泛化能力。

Conclusion: STAGE有效缓解了自回归图像生成中强化学习训练的不稳定性问题，增强了模型对预训练分布的保持能力和抗奖励欺骗性，具有良好的通用性和迁移性能。

Abstract: Reinforcement learning has recently been explored to improve text-to-image
generation, yet applying existing GRPO algorithms to autoregressive (AR) image
models remains challenging. The instability of the training process easily
disrupts the pretrained model capability during long runs, resulting in
marginal gains, degraded image quality, and poor generalization. In this work,
we revisit GRPO for AR image generation and identify two key issues:
contradictory gradients from unnecessary tokens and unstable policy entropy
dynamics. To address these, we introduce STAGE, a stable and generalizable
framework that leverages two targeted solutions: 1) Advantage/KL reweighting.
Similarity-aware reweighting to alleviate conflicting updates; and 2) Entropy
reward. An entropy-based reward corresponding to reference model to stabilize
learning. With the help of alleviating conflicts between tokens and an entropy
reward for stabilizing training, we reduce disruption of the pretrained
distribution and mitigate reward hacking, which in turn improves generalization
and transfer better to other benchmarks. Experiments across multiple benchmarks
show that STAGE consistently improves visual quality, stability, and cross-task
generalization compared to baseline GRPO.

</details>


### [332] [VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning](https://arxiv.org/abs/2509.25033)
*Wenhao Li,Qiangchang Wang,Xianjing Meng,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: 提出了一种结合视觉与文本的少样本学习框架VT-FSL，利用大语言模型生成基于支持图像的精确类描述，并通过几何感知对齐实现多模态融合，在多种FSL场景中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入语义信息时容易产生与视觉证据矛盾的幻觉语义，导致噪声干扰和修正成本高，缺乏对实际实例的 grounding。

Method: 提出VT-FSL框架，包含跨模态迭代提示（CIP）和跨模态几何对齐（CGA）。CIP利用大语言模型结合类别名称和支持图像迭代生成精确的类描述，并合成语义一致的图像；CGA通过最小化三者张成的平行六面体核体积，实现文本、支持图像和合成图像的联合对齐。

Result: 在十个标准、跨域和细粒度的少样本学习基准上均取得当前最优性能。

Conclusion: VT-FSL通过LLM驱动的精确语义生成与几何感知的多模态对齐，有效提升了少样本学习的性能，为视觉-语言协同学习提供了新思路。

Abstract: Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples. Recent studies enhance support features by
incorporating additional semantic information or designing complex semantic
fusion modules. However, they still suffer from hallucinating semantics that
contradict the visual evidence due to the lack of grounding in actual
instances, resulting in noisy guidance and costly corrections. To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment. It mainly consists of
Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment
(CGA). Specifically, the CIP conditions an LLM on both class names and support
images to generate precise class descriptions iteratively in a single
structured reasoning pass. These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images. The descriptions and synthetic images act
respectively as complementary textual and visual prompts, providing high-level
class semantics and low-level intra-class diversity to compensate for limited
support data. Furthermore, the CGA jointly aligns the fused textual, support,
and synthetic visual representations by minimizing the kernelized volume of the
3-dimensional parallelotope they span. It captures global and nonlinear
relationships among all representations, enabling structured and consistent
multimodal integration. The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios. Code is available
at https://github.com/peacelwh/VT-FSL.

</details>


### [333] [Fast Real-Time Pipeline for Robust Arm Gesture Recognition](https://arxiv.org/abs/2509.25042)
*Milán Zsolt Bagladi,László Gulyás,Gergő Szalay*

Main category: cs.CV

TL;DR: 本文提出了一种基于OpenPose关键点估计、关键点归一化和循环神经网络分类器的实时动态手臂手势识别流程。


<details>
  <summary>Details</summary>
Motivation: 为了实现对动态手臂手势的准确、实时识别，并提高系统在不同摄像机角度下的鲁棒性。

Method: 采用OpenPose进行关键点检测，使用1x1归一化方法和两种特征表示（基于坐标和基于角度），并通过人工旋转数据增强来提升模型对视角变化的鲁棒性，最后使用RNN进行分类。

Result: 在自建交通控制手势数据集上的实验表明，该方法在不同视角和速度下均具有高识别精度，并可计算手臂信号的速度。

Conclusion: 所提出的流程在实际应用中具有良好的实时性和鲁棒性，适用于复杂视角下的动态手势识别任务。

Abstract: This paper presents a real-time pipeline for dynamic arm gesture recognition
based on OpenPose keypoint estimation, keypoint normalization, and a recurrent
neural network classifier. The 1 x 1 normalization scheme and two feature
representations (coordinate- and angle-based) are presented for the pipeline.
In addition, an efficient method to improve robustness against camera angle
variations is also introduced by using artificially rotated training data.
Experiments on a custom traffic-control gesture dataset demonstrate high
accuracy across varying viewing angles and speeds. Finally, an approach to
calculate the speed of the arm signal (if necessary) is also presented.

</details>


### [334] [A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration](https://arxiv.org/abs/2509.25044)
*Rohit Jena,Vedant Zope,Pratik Chaudhari,James C. Gee*

Main category: cs.CV

TL;DR: 本文提出了FFDP，一种支持大规模图像配准的IO感知非GEMM融合核与分布式框架，显著提升了性能与内存效率。


<details>
  <summary>Details</summary>
Motivation: 图像配准在生物医学中至关重要，但现有算法未能跟上图像采集技术的发展速度。

Method: 提出FFDP框架，结合IO感知的非GEMM融合核与卷积感知的张量分片策略，优化模型并行中的非GEMM瓶颈。

Result: 在8块A6000 GPU上，仅用约一分钟完成100微米人脑MRI数据的多模态配准，问题规模超过临床标准数据570倍；相比现有方法加速6-7倍，峰值内存降低20-59%，单GPU可处理64倍更大的问题。

Conclusion: FFDP显著提升了大规模图像配准的可扩展性、速度与内存效率，填补了图像获取与处理能力之间的差距。

Abstract: In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels
supplemented with a distributed framework for image registration at
unprecedented scales. Image registration is an inverse problem fundamental to
biomedical and life sciences, but algorithms have not scaled in tandem with
image acquisition capabilities. Our framework complements existing model
parallelism techniques proposed for large-scale transformer training by
optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding.
We demonstrate unprecedented capabilities by performing multimodal registration
of a 100 micron ex-vivo human brain MRI volume at native resolution - an
inverse problem more than 570x larger than a standard clinical datum in about a
minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art
optimization and deep learning registration pipelines by upto 6 - 7x while
reducing peak memory consumption by 20 - 59%. Comparative analysis on a 250
micron dataset shows that FFDP can fit upto 64x larger problems than existing
SOTA on a single GPU, and highlights both the performance and efficiency gains
of FFDP compared to SOTA image registration methods.

</details>


### [335] [GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction](https://arxiv.org/abs/2509.25075)
*Huaizhi Qu,Xiao Wang,Gengwei Zhang,Jie Peng,Tianlong Chen*

Main category: cs.CV

TL;DR: GEM是一种基于3D高斯点阵的新型冷冻电镜（cryo-EM）三维重构框架，能够在实空间中高效运行，兼顾高精度与低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶方法因反复变换损失精度，而基于神经辐射场的实空间方法虽精度高但计算和内存开销大，因此需要一种兼具效率与精度的新方法。

Method: 采用3D高斯点阵（3DGS）表示蛋白质结构，每个高斯仅用11个参数；设计了针对3D高斯的新型梯度计算方法，降低内存和训练成本。

Result: 在标准benchmark上，相比现有最先进方法，训练速度提升最多48%，内存使用减少12%，局部分辨率最高提升38.8%。

Conclusion: GEM在速度、效率和高分辨率准确性之间取得了良好平衡，为cryo-EM重构提供了一种实用且可扩展的新范式。

Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for
high-resolution structural biology, yet the massive scale of datasets (often
exceeding 100k particle images) renders 3D reconstruction both computationally
expensive and memory intensive. Traditional Fourier-space methods are efficient
but lose fidelity due to repeated transforms, while recent real-space
approaches based on neural radiance fields (NeRFs) improve accuracy but incur
cubic memory and computation overhead. Therefore, we introduce GEM, a novel
cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that
operates directly in real-space while maintaining high efficiency. Instead of
modeling the entire density volume, GEM represents proteins with compact 3D
Gaussians, each parameterized by only 11 values. To further improve the
training efficiency, we designed a novel gradient computation to 3D Gaussians
that contribute to each voxel. This design substantially reduced both memory
footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to
48% faster training and 12% lower memory usage compared to state-of-the-art
methods, while improving local resolution by as much as 38.8%. These results
establish GEM as a practical and scalable paradigm for cryo-EM reconstruction,
unifying speed, efficiency, and high-resolution accuracy. Our code is available
at https://github.com/UNITES-Lab/GEM.

</details>


### [336] [BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation](https://arxiv.org/abs/2509.25077)
*Dingning Liu,Haoyu Guo,Jingyi Zhou,Tong He*

Main category: cs.CV

TL;DR: 提出BRIDGE框架，通过强化学习优化的深度到图像生成方法，合成大规模高质量数据集，并采用混合监督策略训练单目深度估计模型，显著提升性能和细节捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 传统单目深度估计方法受限于数据稀缺和质量差，难以实现鲁棒性，因此需要一种能生成大规模、几何精确且多样化的训练数据的方法。

Method: 提出BRIDGE，一个基于强化学习优化的深度到图像（D2I）生成框架，从多种源深度图合成超过2000万张逼真且几何准确的RGB图像，并配对真实深度图；采用结合教师模型伪标签与真实深度的混合监督策略进行训练。

Result: 在多个基准上定量和定性均优于现有最先进方法，尤其在复杂场景细节还原方面表现突出，实现了更大规模和更广域的数据覆盖。

Conclusion: BRIDGE通过创新的数据生成与训练范式，显著提升了单目深度估计的泛化性和鲁棒性，推动了该领域的发展。

Abstract: Monocular Depth Estimation (MDE) is a foundational task for computer vision.
Traditional methods are limited by data scarcity and quality, hindering their
robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps. Then we train our depth estimation
model on this dataset, employing a hybrid supervision strategy that integrates
teacher pseudo-labels with ground truth depth for comprehensive and robust
training. This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features. Code and models are available at
https://dingning-liu.github.io/bridge.github.io/.

</details>


### [337] [MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification](https://arxiv.org/abs/2509.25082)
*Xiaoyi Huang,Junwei Wu,Kejia Zhang,Carl Yang,Zhiming Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于幅度自适应的扩散模型净化框架MANI-Pure，通过分析对抗扰动在频域中的分布特性，针对性地在高频低幅区域抑制扰动，同时保留语义关键的低频信息，在CIFAR-10和ImageNet-1K上取得了优于现有方法的鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗净化方法通常采用均匀噪声注入，会破坏语义结构并削弱鲁棒性；而作者发现对抗扰动主要集中在高频区域且幅度分布不均，因此提出应根据频域幅度特征进行自适应净化。

Method: 提出MANI-Pure框架，利用输入的幅度谱指导净化过程，自适应地施加异质化、频率定向的噪声，重点抑制脆弱的高频低幅对抗扰动，同时保护重要的低频语义内容。

Result: 在CIFAR-10和ImageNet-1K上的实验表明，MANI-Pure将干净样本准确率差距控制在0.59以内，鲁棒准确率提升2.15，并在RobustBench排行榜上达到第一的鲁棒准确率，超越先前最优方法。

Conclusion: MANI-Pure通过幅度自适应的频域净化策略，有效平衡了语义保持与对抗扰动抑制，显著提升了扩散模型在对抗防御中的性能。

Abstract: Adversarial purification with diffusion models has emerged as a promising
defense strategy, but existing methods typically rely on uniform noise
injection, which indiscriminately perturbs all frequencies, corrupting semantic
structures and undermining robustness. Our empirical study reveals that
adversarial perturbations are not uniformly distributed: they are predominantly
concentrated in high-frequency regions, with heterogeneous magnitude intensity
patterns that vary across frequencies and attack types. Motivated by this
observation, we introduce MANI-Pure, a magnitude-adaptive purification
framework that leverages the magnitude spectrum of inputs to guide the
purification process. Instead of injecting homogeneous noise, MANI-Pure
adaptively applies heterogeneous, frequency-targeted noise, effectively
suppressing adversarial perturbations in fragile high-frequency, low-magnitude
bands while preserving semantically critical low-frequency content. Extensive
experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of
MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original
classifier, while boosting robust accuracy by 2.15, and achieves the top-1
robust accuracy on the RobustBench leaderboard, surpassing the previous
state-of-the-art method.

</details>


### [338] [Triangle Splatting+: Differentiable Rendering with Opaque Triangles](https://arxiv.org/abs/2509.25122)
*Jan Held,Renaud Vandeghen,Sanghyun Son,Daniel Rebain,Matheus Gadelha,Yi Zhou,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 本文提出了Triangle Splatting+，一种直接在可微渲染框架中优化三角形的方法，用于3D场景重建和新视角合成。相比3D高斯点阵，该方法生成的半连接网格可直接用于标准图形引擎，无需后处理，并在视觉质量和训练效率上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点阵方法虽实现快速渲染，但与VR和实时图形应用中广泛使用的基于网格的管线不兼容，现有转换方法需后处理且损害视觉质量。因此需要一种原生支持网格生成、无需后处理的高效可微渲染方法。

Method: 提出Triangle Splatting+，将三角形作为基本图元进行可微点阵化；设计三角形参数化方式以通过共享顶点建立连接性，并采用特定训练策略确保三角形不透明性，从而直接输出可用于标准图形引擎的三角网格。

Result: 在Mip-NeRF360和Tanks & Temples数据集上实现了基于网格的新视角合成SOTA性能，视觉保真度优于先前点阵方法，训练更高效，生成的半连接网格支持物理仿真和交互式漫游等下游应用。

Conclusion: Triangle Splatting+实现了高质量、高效的端到端可微三角形点阵化，生成的网格无需后处理即可集成到标准图形管线中，在保持实时性的同时提升了视觉质量和应用兼容性。

Abstract: Reconstructing 3D scenes and synthesizing novel views has seen rapid progress
in recent years. Neural Radiance Fields demonstrated that continuous volumetric
radiance fields can achieve high-quality image synthesis, but their long
training and rendering times limit practicality. 3D Gaussian Splatting (3DGS)
addressed these issues by representing scenes with millions of Gaussians,
enabling real-time rendering and fast optimization. However, Gaussian
primitives are not natively compatible with the mesh-based pipelines used in VR
headsets, and real-time graphics applications. Existing solutions attempt to
convert Gaussians into meshes through post-processing or two-stage pipelines,
which increases complexity and degrades visual quality. In this work, we
introduce Triangle Splatting+, which directly optimizes triangles, the
fundamental primitive of computer graphics, within a differentiable splatting
framework. We formulate triangle parametrization to enable connectivity through
shared vertices, and we design a training strategy that enforces opaque
triangles. The final output is immediately usable in standard graphics engines
without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples
datasets show that Triangle Splatting+achieves state-of-the-art performance in
mesh-based novel view synthesis. Our method surpasses prior splatting
approaches in visual fidelity while remaining efficient and fast to training.
Moreover, the resulting semi-connected meshes support downstream applications
such as physics-based simulation or interactive walkthroughs. The project page
is https://trianglesplatting2.github.io/trianglesplatting2/.

</details>


### [339] [Score Distillation of Flow Matching Models](https://arxiv.org/abs/2509.25127)
*Mingyuan Zhou,Yi Gu,Huangjie Zheng,Liangchen Song,Guande He,Yizhe Zhang,Wenze Hu,Yinfei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于贝叶斯规则和条件期望的简单推导，统一了高斯扩散模型与流匹配方法，并将得分蒸馏（SiD）直接应用于预训练的文本到图像流匹配模型（如SANA、SD3系列、FLUX.1-dev），在无需教师模型微调或架构修改的情况下实现了有效的单步或少步生成，验证了得分蒸馏在流匹配模型上的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量高，但采样速度慢；蒸馏方法可加速生成，而流匹配虽理论等价于扩散，但其是否能直接应用得分蒸馏尚不明确，本文旨在验证这一迁移的可行性。

Method: 基于贝叶斯规则和条件期望，提出一种不依赖ODE/SDE的统一视角，并将Score identity Distillation (SiD) 直接迁移到多种基于DiT架构的预训练流匹配模型上，进行数据自由与数据辅助实验。

Result: 实验证明SiD在多个主流流匹配模型上无需额外微调或结构改动即可有效工作，显著提升了生成效率，且适用于不同设置（如不同模型规模和数据条件）。

Conclusion: 得分蒸馏技术可直接适用于文本到图像的流匹配模型，无需复杂调整，为扩散与流匹配模型的加速提供了统一框架，解决了此前对稳定性和有效性的担忧。

Abstract: Diffusion models achieve high-quality image generation but are limited by
slow iterative sampling. Distillation methods alleviate this by enabling one-
or few-step generation. Flow matching, originally introduced as a distinct
framework, has since been shown to be theoretically equivalent to diffusion
under Gaussian assumptions, raising the question of whether distillation
techniques such as score distillation transfer directly. We provide a simple
derivation -- based on Bayes' rule and conditional expectations -- that unifies
Gaussian diffusion and flow matching without relying on ODE/SDE formulations.
Building on this view, we extend Score identity Distillation (SiD) to
pretrained text-to-image flow-matching models, including SANA, SD3-Medium,
SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show
that, with only modest flow-matching- and DiT-specific adjustments, SiD works
out of the box across these models, in both data-free and data-aided settings,
without requiring teacher finetuning or architectural changes. This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators. We will make the PyTorch implementation publicly
available.

</details>


### [340] [VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning](https://arxiv.org/abs/2509.25151)
*Zhaozhi Wang,Tong Zhang,Mingyue Guo,Yaowei Wang,Qixiang Ye*

Main category: cs.CV

TL;DR: 本文提出了一种名为VideoAnchor的即插即用模块，通过利用子空间亲和性来增强跨帧视觉线索，从而改善多模态大语言模型在视觉-空间推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言对齐方面取得了显著进展，但在视觉-空间推理方面仍存在不足，主要原因是注意力机制中视觉标记被语言标记所掩盖。

Method: 基于稀疏子空间聚类中的自表达特性与Transformer中注意力机制之间的联系，提出了VideoAnchor模块，该模块利用子空间亲和性在不重新训练的情况下强化跨帧的视觉线索。

Result: 在多个基准测试和骨干网络上的实验表明，VideoAnchor在VSI-Bench和Video-MME（空间相关任务）上分别带来了3.2%和4.6%的性能提升，并且定性分析显示其具有更一致的子空间划分和更强的视觉定位能力。

Conclusion: VideoAnchor有效解决了MLLMs在视觉-空间推理中的注意力偏差问题，提升了模型对共享视觉结构的关注，具备良好的通用性和应用前景。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive progress in
vision-language alignment, yet they remain limited in visual-spatial reasoning.
We first identify that this limitation arises from the attention mechanism:
visual tokens are overshadowed by language tokens, preventing the model from
consistently recognizing the same visual cues across frames. To address this
challenge, we draw a novel connection between the self-expressiveness property
in sparse subspace clustering and the attention mechanism in Transformers.
Building on this insight, we propose VideoAnchor, a plug-and-play module that
leverages subspace affinities to reinforce visual cues across frames without
retraining, effectively anchoring attention to shared visual structures.
Extensive experiments across benchmarks and backbone models show consistent
performance gains -- $e.g.$, 3.2% and 4.6% improvements on VSI-Bench and
Video-MME (spatial-related tasks) with InternVL2-8B and Qwen2.5VL-72B -- while
qualitative analyses demonstrate more coherent subspace partitions and stronger
visual grounding. Our codes will be made public available at
https://github.com/feufhd/VideoAnchor.

</details>


### [341] [Rolling Forcing: Autoregressive Long Video Diffusion in Real Time](https://arxiv.org/abs/2509.25161)
*Kunhao Liu,Wenbo Hu,Jiale Xu,Ying Shan,Shijian Lu*

Main category: cs.CV

TL;DR: 提出了一种名为Rolling Forcing的流式视频生成新方法，通过联合去噪、注意力锚机制和高效训练算法，显著减少长时视频流中的误差累积，实现高质量、低延迟的实时生成。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频生成方法在长时间生成中存在严重误差累积问题，导致视频质量下降，需设计能抑制误差传播的新技术。

Method: 1) 设计联合去噪方案，同时处理多个含渐进噪声的帧；2) 引入注意力sink机制，保留初始帧的关键状态作为全局上下文锚点；3) 提出非重叠窗口上的少步蒸馏训练算法，缓解基于自生成历史的暴露偏差。

Result: 实验表明，该方法可在单个GPU上实现实时多分钟视频流生成，显著降低误差累积，提升长期时间一致性与生成效率。

Conclusion: Rolling Forcing通过三项创新设计有效解决了流式视频生成中的误差累积问题，为交互式世界模型和神经游戏引擎提供了更可靠的长视频生成方案。

Abstract: Streaming video generation, as one fundamental component in interactive world
models and neural game engines, aims to generate high-quality, low-latency, and
temporally coherent long video streams. However, most existing work suffers
from severe error accumulation that often significantly degrades the generated
stream videos over long horizons. We design Rolling Forcing, a novel video
generation technique that enables streaming long videos with minimal error
accumulation. Rolling Forcing comes with three novel designs. First, instead of
iteratively sampling individual frames, which accelerates error propagation, we
design a joint denoising scheme that simultaneously denoises multiple frames
with progressively increasing noise levels. This design relaxes the strict
causality across adjacent frames, effectively suppressing error growth. Second,
we introduce the attention sink mechanism into the long-horizon stream video
generation task, which allows the model to keep key value states of initial
frames as a global context anchor and thereby enhances long-term global
consistency. Third, we design an efficient training algorithm that enables
few-step distillation over largely extended denoising windows. This algorithm
operates on non-overlapping windows and mitigates exposure bias conditioned on
self-generated histories. Extensive experiments show that Rolling Forcing
enables real-time streaming generation of multi-minute videos on a single GPU,
with substantially reduced error accumulation.

</details>


### [342] [Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models](https://arxiv.org/abs/2509.25162)
*Bowei Chen,Sai Bi,Hao Tan,He Zhang,Tianyuan Zhang,Zhengqi Li,Yuanjun Xiong,Jianming Zhang,Kai Zhang*

Main category: cs.CV

TL;DR: 提出一种三阶段对齐策略，将预训练视觉编码器用作扩散模型的图像分词器，利用基础模型的语义结构，在ImageNet和LAION数据集上显著加速扩散模型收敛并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统VAE分词器侧重低层次细节，缺乏语义信息，难以有效支持扩散模型生成高质量图像。因此，需要一种能保留丰富语义的图像分词方法。

Method: 提出三阶段对齐策略：首先冻结编码器，训练适配器和解码器构建语义潜在空间；然后联合优化所有组件，并引入语义保持损失；最后微调解码器以提升重建质量。

Result: 在ImageNet 256×256上，仅64个epoch即达到gFID 1.90；在LAION上，使用该分词器的2B参数文本到图像模型优于FLUX VAE。

Conclusion: 该方法简单且可扩展，为连续分词器设计提供了语义基础的新范式。

Abstract: In this work, we propose aligning pretrained visual encoders to serve as
tokenizers for latent diffusion models in image generation. Unlike training a
variational autoencoder (VAE) from scratch, which primarily emphasizes
low-level details, our approach leverages the rich semantic structure of
foundation encoders. We introduce a three-stage alignment strategy: (1) freeze
the encoder and train an adapter and a decoder to establish a semantic latent
space; (2) jointly optimize all components with an additional semantic
preservation loss, enabling the encoder to capture perceptual details while
retaining high-level semantics; and (3) refine the decoder for improved
reconstruction quality. This alignment yields semantically rich image
tokenizers that benefit diffusion models. On ImageNet 256$\times$256, our
tokenizer accelerates the convergence of diffusion models, reaching a gFID of
1.90 within just 64 epochs, and improves generation both with and without
classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model
trained with our tokenizer consistently outperforms FLUX VAE under the same
training steps. Overall, our method is simple, scalable, and establishes a
semantically grounded paradigm for continuous tokenizer design.

</details>


### [343] [YOLO26: Key Architectural Enhancements and Performance Benchmarking for Real-Time Object Detection](https://arxiv.org/abs/2509.25164)
*Ranjan Sapkota,Rahul Harsha Cheppally,Ajay Sharda,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文介绍了Ultralytics于2025年9月发布的YOLO26，重点阐述其在实时边缘目标检测中的架构创新与性能表现。


<details>
  <summary>Details</summary>
Motivation: 提升边缘设备上目标检测的效率与精度，推动YOLO系列在低功耗设备上的实际部署能力。

Method: 引入无NMS端到端推理、移除DFL、采用ProgLoss和STAL策略优化小目标检测，并使用受大模型训练启发的MuSGD优化器。

Result: 在NVIDIA Orin Jetson等边缘设备上的基准测试显示，YOLO26相比YOLOv8、YOLO11、YOLOv12和YOLOv13在效率、精度和部署灵活性方面均表现出优越性。

Conclusion: YOLO26是YOLO系列发展的重要里程碑，显著提升了边缘计算场景下的性能与实用性。

Abstract: This study presents Key Architectural Enhancements and Performance
Benchmarking of Ultralytics YOLO26 for real-time edge object detection,
providing a comprehensive overview of the design principles of YOLO26,
technological advances, and deployment readiness. YOLO26, released in September
2025 by Ultralytics, represents the newest and most cutting-edge member of the
You Only Look Once (YOLO) family, engineered to push the boundaries of
efficiency and accuracy on edge and low-power devices. This paper highlights
architectural innovations in YOLO26, including end-to-end NMS-free inference,
removal of Distribution Focal Loss (DFL) for streamlined exports, introduction
of ProgLoss and Small-Target-Aware Label Assignment (STAL) for improved
stability and small-object detection, and the adoption of the MuSGD optimizer
inspired by large language model training. In addition, we report performance
benchmarks for YOLO26 across edge devices, specifically NVIDIA Orin Jetson
platforms, and compare results against YOLOv8 and YOLO11 (previous Ultralytics
releases) as well as YOLOv12 and YOLOv13, which bridged the lineage between
YOLO11 and YOLO26. Our comparative analysis highlights superior efficiency of
YOLO26, accuracy, and deployment versatility, establishing it as a pivotal
milestone in the YOLO evolution.

</details>


### [344] [Personalized Vision via Visual In-Context Learning](https://arxiv.org/abs/2509.25172)
*Yuxin Jiang,Yuchao Gu,Yiren Song,Ivor Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了PICO，一种基于扩散变换器的视觉上下文学习框架，通过单个标注示例实现个性化视觉任务的零样本迁移，无需微调。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法依赖昂贵的微调或合成数据流水线，缺乏灵活性且局限于固定任务格式；而视觉上下文学习虽具潜力，但先前方法泛化能力差，难以应对开放性个性化需求。

Method: 提出PICO框架，采用四格结构将扩散变换器重构为视觉上下文学习器，并构建具有高任务多样性的VisRel小型调优数据集，结合注意力引导的种子评分机制提升推理可靠性。

Result: 实验表明PICO在性能上超越微调和合成数据基线方法，能灵活适应用户自定义新任务，并在识别与生成任务中均表现出良好泛化能力。

Conclusion: PICO为个性化视觉任务提供了一种高效、灵活且无需训练的解决方案，证明了任务多样性对上下文学习泛化的重要性。

Abstract: Modern vision models, trained on large-scale annotated datasets, excel at
predefined tasks but struggle with personalized vision -- tasks defined at test
time by users with customized objects or novel objectives. Existing
personalization approaches rely on costly fine-tuning or synthetic data
pipelines, which are inflexible and restricted to fixed task formats. Visual
in-context learning (ICL) offers a promising alternative, yet prior methods
confine to narrow, in-domain tasks and fail to generalize to open-ended
personalization. We introduce Personalized In-Context Operator (PICO), a simple
four-panel framework that repurposes diffusion transformers as visual
in-context learners. Given a single annotated exemplar, PICO infers the
underlying transformation and applies it to new inputs without retraining. To
enable this, we construct VisRel, a compact yet diverse tuning dataset, showing
that task diversity, rather than scale, drives robust generalization. We
further propose an attention-guided seed scorer that improves reliability via
efficient inference scaling. Extensive experiments demonstrate that PICO (i)
surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to
novel user-defined tasks, and (iii) generalizes across both recognition and
generation.

</details>


### [345] [Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding](https://arxiv.org/abs/2509.25177)
*Bingkui Tong,Jiaer Xia,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为Layer Contrastive Decoding (LayerCD)的简单方法，通过对比视觉编码器浅层和深层特征生成的输出分布来有效减少多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）常出现幻觉问题，即生成的语言输出与输入图像内容不一致，源于浅层视觉特征包含偏见且信息不足。

Method: 利用视觉编码器不同层次（浅层与深层）的视觉特征生成输出分布，并通过对比这些分布来过滤幻觉。

Result: 在两个幻觉基准上的实验表明，LayerCD显著优于当前最先进方法。

Conclusion: LayerCD能有效缓解MLLMs中的幻觉问题，提升生成内容的准确性。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive perception and
reasoning capabilities, yet they often suffer from hallucinations -- generating
outputs that are linguistically coherent but inconsistent with the context of
the input image, including inaccuracies in objects, attributes, and relations.
To address this challenge, we propose a simple approach called Layer
Contrastive Decoding (LayerCD). Our design is motivated by the observation that
shallow visual features are much more likely than deep visual features to cause
an MLLM to hallucinate as they only capture biased, low-level information that
is insufficient for high-level reasoning. Therefore, LayerCD aims to filter out
hallucinations by contrasting the output distributions generated from visual
features of different levels, specifically those from the shallow and deep
layers of the vision encoder, respectively. We conduct extensive experiments on
two hallucination benchmarks and show that LayerCD significantly outperforms
current state-of-the-art. The code for LayerCD is available at
https://github.com/maifoundations/LayerCD .

</details>


### [346] [GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs](https://arxiv.org/abs/2509.25178)
*Aryan Yazdan Parast,Parsa Hosseini,Hesam Asadollahzadeh,Arshia Soltani Moakhar,Basim Azam,Soheil Feizi,Naveed Akhtar*

Main category: cs.CV

TL;DR: 本文提出了一种名为GHOST的方法，通过优化隐写令牌来自动生成引发多模态大语言模型（MLLM）幻觉的图像，以主动发现模型的幻觉漏洞。该方法无需人工监督，在图像嵌入空间中进行优化并利用扩散模型生成视觉自然的图像，成功诱导多种MLLM产生幻觉，揭示了可迁移的脆弱性，并可用于微调来缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM幻觉研究依赖静态基准测试，难以发现模型特有或未预见的幻觉问题，因此需要一种能主动探测和暴露模型脆弱性的动态方法。

Method: GHOST通过在图像嵌入空间中优化误导性特征，使模型误判不存在的目标对象，并结合条件扩散模型生成外观自然的图像，整个过程无需人工干预且不依赖先验知识。

Result: 在多个MLLM上验证，GHOST实现了超过28%的幻觉成功率，显著高于以往约1%的数据驱动方法；生成的图像质量高且不含目标对象；还发现了跨模型的可迁移幻觉问题（如对Qwen2.5-VL优化的图像可在GPT-4o上达到66.5%的幻觉率）；并通过微调证明其具备纠正能力。

Conclusion: GHOST是一种有效的诊断与纠正工具，可用于提升多模态系统的可靠性，不仅能够暴露MLLM的潜在幻觉漏洞，还可用于训练更鲁棒的模型。

Abstract: Object hallucination in Multimodal Large Language Models (MLLMs) is a
persistent failure mode that causes the model to perceive objects absent in the
image. This weakness of MLLMs is currently studied using static benchmarks with
fixed visual scenarios, which preempts the possibility of uncovering
model-specific or unanticipated hallucination vulnerabilities. We introduce
GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method
designed to stress-test MLLMs by actively generating images that induce
hallucination. GHOST is fully automatic and requires no human supervision or
prior knowledge. It operates by optimizing in the image embedding space to
mislead the model while keeping the target object absent, and then guiding a
diffusion model conditioned on the embedding to generate natural-looking
images. The resulting images remain visually natural and close to the original
input, yet introduce subtle misleading cues that cause the model to
hallucinate. We evaluate our method across a range of models, including
reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success
rate exceeding 28%, compared to around 1% in prior data-driven discovery
methods. We confirm that the generated images are both high-quality and
object-free through quantitative metrics and human evaluation. Also, GHOST
uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce
hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on
our images mitigates hallucination, positioning GHOST as both a diagnostic and
corrective tool for building more reliable multimodal systems.

</details>


### [347] [DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space](https://arxiv.org/abs/2509.25180)
*Wenkun He,Yuchao Gu,Junyu Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Haocheng Xi,Muyang Li,Ligeng Zhu,Jincheng Yu,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 本文提出DC-Gen，一种通过深度压缩潜在空间来加速文本到图像扩散模型的通用框架，在保持生成质量的同时显著提升高分辨率（如4K）图像生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在高分辨率生成时效率低下，且未充分解决潜在空间中的冗余问题，亟需一种高效、兼容性强的加速方法。

Method: DC-Gen采用高效的后训练流程，首先通过轻量级嵌入对齐训练弥合基础模型与深度压缩潜在空间之间的表示差距，再结合少量LoRA微调恢复生成质量。

Result: 在SANA和FLUX.1-Krea上验证了DC-Gen的有效性，生成质量相当，但速度大幅提升；DC-Gen-FLUX在NVIDIA H100上将4K图像生成延迟降低53倍，结合NVFP4 SVDQuant在单块NVIDIA 5090上仅需3.5秒生成4K图像，总延迟降低138倍。

Conclusion: DC-Gen通过压缩潜在空间和轻量对齐策略，实现了高分辨率文本到图像生成的显著加速，同时保持了原有模型的生成质量，具有良好的实用性和扩展性。

Abstract: Existing text-to-image diffusion models excel at generating high-quality
images, but face significant efficiency challenges when scaled to high
resolutions, like 4K image generation. While previous research accelerates
diffusion models in various aspects, it seldom handles the inherent redundancy
within the latent space. To bridge this gap, this paper introduces DC-Gen, a
general framework that accelerates text-to-image diffusion models by leveraging
a deeply compressed latent space. Rather than a costly training-from-scratch
approach, DC-Gen uses an efficient post-training pipeline to preserve the
quality of the base model. A key challenge in this paradigm is the
representation gap between the base model's latent space and a deeply
compressed latent space, which can lead to instability during direct
fine-tuning. To overcome this, DC-Gen first bridges the representation gap with
a lightweight embedding alignment training. Once the latent embeddings are
aligned, only a small amount of LoRA fine-tuning is needed to unlock the base
model's inherent generation quality. We verify DC-Gen's effectiveness on SANA
and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve
quality comparable to their base models but with a significant speedup.
Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on
the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a
4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total
latency reduction of 138x compared to the base FLUX.1-Krea model. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [348] [DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder](https://arxiv.org/abs/2509.25182)
*Junyu Chen,Wenkun He,Yuchao Gu,Yuyang Zhao,Jincheng Yu,Junsong Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Muyang Li,Haocheng Xi,Ligeng Zhu,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-VideoGen 是一种用于高效视频生成的后训练加速框架，可应用于任何预训练视频扩散模型，通过轻量级微调将其适配到深度压缩的潜在空间。


<details>
  <summary>Details</summary>
Motivation: 提高视频生成模型的推理效率，降低计算资源消耗，同时保持生成质量，并支持高分辨率长视频生成。

Method: 提出深度压缩视频自编码器（具有新型块因果时间设计）实现32x/64x空间和4x时间压缩，并设计AE-Adapt-V适应策略，将预训练模型快速稳定地迁移到新潜在空间。

Result: 在NVIDIA H100上仅用10个GPU天即可完成对Wan-2.1-14B模型的适配；加速后的模型推理延迟最高降低14.8倍，可在单个GPU上生成2160x3840分辨率视频。

Conclusion: DC-VideoGen能高效加速现有视频生成模型，在显著降低计算成本的同时保持生成质量，具备良好的实用性和扩展性。

Abstract: We introduce DC-VideoGen, a post-training acceleration framework for
efficient video generation. DC-VideoGen can be applied to any pre-trained video
diffusion model, improving efficiency by adapting it to a deep compression
latent space with lightweight fine-tuning. The framework builds on two key
innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal
temporal design that achieves 32x/64x spatial and 4x temporal compression while
preserving reconstruction quality and generalization to longer videos; and (ii)
AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer
of pre-trained models into the new latent space. Adapting the pre-trained
Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100
GPU. The accelerated models achieve up to 14.8x lower inference latency than
their base counterparts without compromising quality, and further enable
2160x3840 video generation on a single GPU. Code:
https://github.com/dc-ai-projects/DC-VideoGen.

</details>


### [349] [PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos](https://arxiv.org/abs/2509.25183)
*Ting-Hsuan Liao,Haowen Liu,Yiran Xu,Songwei Ge,Gengshan Yang,Jia-Bin Huang*

Main category: cs.CV

TL;DR: PAD3R是一种从随意拍摄的单目视频中重建可变形3D物体的方法，具有处理长序列、大变形和相机运动的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理包含大幅变形、大范围相机运动和有限视角覆盖的长视频序列，因此需要一种更鲁棒的可变形3D重建方法。

Method: 提出PAD3R，通过预训练的图像到3D模型监督，训练一个个性化的物体中心姿态估计器，并结合长期2D点跟踪来优化可变形3D高斯表示。利用生成先验和可微渲染实现类别无关的高保真重建。

Result: 在多种挑战性场景下实现了高质量的可变形3D重建，表现出良好的鲁棒性和泛化能力。

Conclusion: PAD3R能够有效应对复杂真实场景下的可变形物体3D重建，为动态场景理解和3D内容创作提供了可行方案。

Abstract: We present PAD3R, a method for reconstructing deformable 3D objects from
casually captured, unposed monocular videos. Unlike existing approaches, PAD3R
handles long video sequences featuring substantial object deformation,
large-scale camera movement, and limited view coverage that typically challenge
conventional systems. At its core, our approach trains a personalized,
object-centric pose estimator, supervised by a pre-trained image-to-3D model.
This guides the optimization of deformable 3D Gaussian representation. The
optimization is further regularized by long-term 2D point tracking over the
entire input video. By combining generative priors and differentiable
rendering, PAD3R reconstructs high-fidelity, articulated 3D representations of
objects in a category-agnostic way. Extensive qualitative and quantitative
results show that PAD3R is robust and generalizes well across challenging
scenarios, highlighting its potential for dynamic scene understanding and 3D
content creation.

</details>


### [350] [PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images](https://arxiv.org/abs/2509.25185)
*Shuoshuo Zhang,Zijian Li,Yizhen Zhang,Jingjing Fu,Lei Song,Jiang Bian,Jun Zhang,Yujiu Yang,Rui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PixelCraft的多智能体系统，用于提升多模态大模型在结构化图像（如图表和几何图形）上的视觉推理能力，通过高保真图像处理和灵活的三阶段推理流程显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于线索的方法受限于低质量的图像处理和线性的、僵化的推理模式，难以应对复杂结构化图像任务中的感知误差累积问题。

Method: 构建了一个包含调度器、规划器、推理器、批评者和多个视觉工具智能体的多智能体系统；通过高质量语料库训练一个具备像素级定位能力的接地模型，并结合传统计算机视觉算法实现高保真处理；引入动态的三阶段工作流（工具选择、智能体讨论、自我批评）和图像记忆机制以支持灵活推理。

Result: 在具有挑战性的图表和几何推理基准上进行了广泛实验，结果表明PixelCraft显著提升了先进多模态大模型的视觉推理性能。

Conclusion: PixelCraft通过高保真图像处理和灵活的多智能体协作推理机制，为结构化图像理解设立了新标准。

Abstract: Structured images (e.g., charts and geometric diagrams) remain challenging
for multimodal large language models (MLLMs), as perceptual slips can cascade
into erroneous conclusions. Intermediate visual cues can steer reasoning;
however, existing cue-based methods are constrained with low-fidelity image
processing and linear, rigid reasoning patterns, limiting their effectiveness
on complex structured-image tasks. In this paper, we propose PixelCraft, a
novel multi-agent system for high-fidelity image processing and flexible visual
reasoning on structured images. The system comprises a dispatcher, a planner, a
reasoner, critics, and a set of visual tool agents. To achieve high-fidelity
processing, we construct a high-quality corpus and fine-tune an MLLM into a
grounding model, whose pixel-level localizations are integrated with
traditional computer vision (CV) algorithms in tool agents. Building on this
foundation, PixelCraft facilitates flexible visual reasoning through a dynamic
three-stage workflow of tool selection, agent discussion, and self-criticism.
Moreover, unlike prior linear reasoning patterns that simply append historical
images, PixelCraft maintains an image memory to allow the planner to adaptively
revisit earlier visual steps, explore alternative reasoning branches, and
dynamically adjust the reasoning trajectory during discussion. Extensive
experiments on challenging chart and geometry benchmarks demonstrate that
PixelCraft significantly improves visual reasoning performance for advanced
MLLMs, setting a new standard for structured image reasoning. Our code will be
available at https://github.com/microsoft/PixelCraft.

</details>


### [351] [FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation](https://arxiv.org/abs/2509.25187)
*Yunyang Ge,Xinhua Cheng,Chengshu Zhao,Xianyi He,Shenghai Yuan,Bin Lin,Bin Zhu,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种名为FlashI2V的图像到视频生成方法，通过潜在空间移位和傅里叶引导机制有效缓解了条件图像泄露问题，提升了模型在域外数据上的泛化能力和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法因直接拼接条件图像与噪声潜变量导致条件图像泄露，引发运动缓慢、色彩不一致等问题，并在域外数据上表现不佳。本文旨在解决这一泄露问题以提升模型泛化性。

Method: 提出FlashI2V，包含两个核心组件：(1) 潜在移位——通过从噪声潜变量中减去条件图像信息，隐式引入条件；(2) 傅里叶引导——利用傅里叶变换提取高频幅度特征，加速收敛并调节生成细节层次。

Result: 实验表明，FlashI2V有效克服了条件图像泄露，在多种I2V范式中实现了最佳的域外数据性能和泛化能力。仅用1.3B参数，在Vbench-I2V上动态评分达53.01，优于CogVideoX1.5-5B-I2V和Wan2.1-I2V-14B-480P。

Conclusion: FlashI2V通过隐式条件建模和频域引导策略，显著改善了I2V生成中的条件泄露问题，为高效、高保真的视频生成提供了新思路。

Abstract: In Image-to-Video (I2V) generation, a video is created using an input image
as the first-frame condition. Existing I2V methods concatenate the full
information of the conditional image with noisy latents to achieve high
fidelity. However, the denoisers in these methods tend to shortcut the
conditional image, which is known as conditional image leakage, leading to
performance degradation issues such as slow motion and color inconsistency. In
this work, we further clarify that conditional image leakage leads to
overfitting to in-domain data and decreases the performance in out-of-domain
scenarios. Moreover, we introduce Fourier-Guided Latent Shifting I2V, named
FlashI2V, to prevent conditional image leakage. Concretely, FlashI2V consists
of: (1) Latent Shifting. We modify the source and target distributions of flow
matching by subtracting the conditional image information from the noisy
latents, thereby incorporating the condition implicitly. (2) Fourier Guidance.
We use high-frequency magnitude features obtained by the Fourier Transform to
accelerate convergence and enable the adjustment of detail levels in the
generated video. Experimental results show that our method effectively
overcomes conditional image leakage and achieves the best generalization and
performance on out-of-domain data among various I2V paradigms. With only 1.3B
parameters, FlashI2V achieves a dynamic degree score of 53.01 on Vbench-I2V,
surpassing CogVideoX1.5-5B-I2V and Wan2.1-I2V-14B-480P. Github page:
https://pku-yuangroup.github.io/FlashI2V/

</details>


### [352] [Visual Jigsaw Post-Training Improves MLLMs](https://arxiv.org/abs/2509.25190)
*Penghao Wu,Yushan Zhang,Haiwen Diao,Bo Li,Lewei Lu,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了Visual Jigsaw，一种通用的自监督后训练框架，通过将视觉输入分割并打乱顺序，要求模型以自然语言恢复其正确排列，从而增强多模态大语言模型的视觉理解能力。该方法无需额外标注或视觉生成组件，适用于图像、视频和3D数据，显著提升了细粒度感知、时序推理和3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型后训练方法主要以文本为中心，忽视了对视觉信号的深层理解；缺乏真正以视觉为中心且无需人工标注的自监督训练范式。

Method: 提出Visual Jigsaw框架，将视觉输入划分为若干块并随机打乱，构建一个排序任务，模型需输出正确的排列顺序作为自然语言响应，并通过可验证奖励的强化学习（RLVR）进行优化，整个过程无需人工标注或额外的视觉生成模块。

Result: 在图像、视频和3D数据三种模态上均实现了显著性能提升，特别是在细粒度视觉感知、时间序列推理和3D空间理解方面优于现有方法。

Conclusion: Visual Jigsaw证明了自监督、以视觉为中心的后训练策略在提升MLLM视觉理解能力方面的有效性，为未来设计更多视觉主导的预训练任务提供了新方向。

Abstract: Reinforcement learning based post-training has recently emerged as a powerful
paradigm for enhancing the alignment and reasoning capabilities of multimodal
large language models (MLLMs). While vision-centric post-training is crucial
for enhancing MLLMs' intrinsic understanding of visual signals, current
post-training paradigms are predominantly text-centric, where dense visual
inputs are only leveraged to extract sparse cues for text-based reasoning.
There exist a few approaches in this direction, however, they often still rely
on text as an intermediate mediator or introduce additional visual generative
designs. In this work, we introduce Visual Jigsaw, a generic self-supervised
post-training framework designed to strengthen visual understanding in MLLMs.
Visual Jigsaw is formulated as a general ordering task: visual inputs are
partitioned, shuffled, and the model must reconstruct the visual information by
producing the correct permutation in natural language. This naturally aligns
with reinforcement learning from verifiable rewards (RLVR), requires no
additional visual generative components, and derives its supervisory signal
automatically without any annotations. We instantiate Visual Jigsaw across
three visual modalities, including images, videos, and 3D data. Extensive
experiments demonstrate substantial improvements in fine-grained perception,
temporal reasoning, and 3D spatial understanding. Our findings highlight the
potential of self-supervised vision-centric tasks in post-training MLLMs and
aim to inspire further research on vision-centric pretext designs. Project
Page: https://penghao-wu.github.io/visual_jigsaw/

</details>


### [353] [VGGT-X: When VGGT Meets Dense Novel View Synthesis](https://arxiv.org/abs/2509.25191)
*Yang Liu,Chuanchen Luo,Zimo Tang,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文研究了将3D基础模型（3DFMs）应用于密集新视角合成（NVS）的问题，提出了VGGT-X方法以解决扩展3DFMs到密集视图时面临的显存负担和输出质量下降的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的NVS方法依赖于SfM获取精确的3D属性，但该过程在低纹理或低重叠场景中缓慢且脆弱；而当前3DFMs多限于稀疏视图设置，难以直接扩展到密集视图应用。

Method: 提出VGGT-X，包括内存高效的VGGT实现、自适应全局对齐优化输出，并结合鲁棒的3DGS训练策略，支持上千张图像的处理。

Result: 实验表明，VGGT-X显著缩小了与COLMAP初始化方法之间的保真度差距，在无需COLMAP的密集NVS和位姿估计中达到最先进性能。

Conclusion: VGGT-X有效解决了3DFMs在密集NVS中的可扩展性和初始化敏感性问题，为未来3D基础模型的发展提供了实践路径与分析依据。

Abstract: We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel
View Synthesis (NVS). Despite significant progress in Novel View Synthesis
powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D
attributes (e.g., camera poses and point clouds) acquired from
Structure-from-Motion (SfM), which is often slow and fragile in low-texture or
low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over
the traditional pipeline and great potential for online NVS. But most of the
validation and conclusions are confined to sparse-view settings. Our study
reveals that naively scaling 3DFMs to dense views encounters two fundamental
barriers: dramatically increasing VRAM burden and imperfect outputs that
degrade initialization-sensitive 3D training. To address these barriers, we
introduce VGGT-X, incorporating a memory-efficient VGGT implementation that
scales to 1,000+ images, an adaptive global alignment for VGGT output
enhancement, and robust 3DGS training practices. Extensive experiments show
that these measures substantially close the fidelity gap with
COLMAP-initialized pipelines, achieving state-of-the-art results in dense
COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of
remaining gaps with COLMAP-initialized rendering, providing insights for the
future development of 3D foundation models and dense NVS. Our project page is
available at https://dekuliutesla.github.io/vggt-x.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [354] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: 提出一种基于不确定性衡量内容审核模型公平性的无监督方法，利用共形预测技术分析模型对弱势群体标注消息的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的内容审核分类器存在种族和社会偏见，且缺乏有效的公平性评估方法。

Method: 采用共形预测技术计算模型在分类来自脆弱群体标注消息时的不确定性，并以此作为偏见分析的代理指标，对比11个模型在女性和非白人标注者数据上的表现与F1分数等性能指标的差异。

Result: 发现一些预训练模型虽能高精度预测少数群体标签，但预测置信度较低；通过模型置信度可识别哪些标注群体在预训练模型中表示更充分。

Conclusion: 测量模型置信度有助于揭示内容审核模型中的社会偏见，指导模型去偏过程，提升其在实际应用中的公平性。

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [355] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 本文提出了AccessEval，一个用于评估大语言模型在不同残疾背景下表现的基准，发现模型对残障相关查询的回应存在负面情绪、刻板印象和事实错误等偏见问题。


<details>
  <summary>Details</summary>
Motivation: 为了系统研究大语言模型在现实查询中对不同残障群体的处理差异，揭示潜在的偏见和不平等现象。

Method: 构建了包含6个现实领域和9种残障类型的配对中立与残障感知查询的数据集，评估21个闭源和开源大模型在情感、社会认知和事实准确性方面的表现。

Result: 残障感知查询的回应相比中立查询更具负面情绪、更多刻板印象和更高的事实错误率，其中听觉、言语和行动残障受影响最显著。

Conclusion: 大语言模型中存在根深蒂固的残障偏见，可能在实际决策场景中对残障用户造成真实伤害，需加强日常应用中的偏见缓解措施。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [356] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: 提出RAR²框架，通过联合学习改进推理增强检索和检索增强推理，在生物医学问答任务中优于传统RAG方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理复杂医学问题时难以捕捉深层知识需求，且缺乏对推理过程的显式建模，限制了其性能。

Method: 提出RAR²框架，构建思维过程以揭示隐含知识需求，并指导检索与回答生成；使用混合偏好数据对和直接偏好优化（DPO）进行训练，并设计两种测试时扩展策略。

Result: 在多个生物医学问答数据集上实验表明，RAR²均优于有无微调的RAG基线方法。

Conclusion: RAR²通过联合优化推理与检索，有效提升复杂医学问题的回答准确性和相关性，展现出在临床场景中的应用潜力。

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [357] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: TRUEBench是一个面向大语言模型（LLM）生产力助手的新型基准测试，旨在更真实地评估其在多语言、多轮对话和显/隐性约束下的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在多语言支持、用户请求中的隐性约束捕捉以及多轮对话复杂性方面存在不足，难以准确评估LLM在现实场景中的表现。

Method: 构建包含12种语言、跨语言指令、复杂多轮对话场景及累积约束的测试集，并利用LLM验证器优化约束定义，采用严格评估标准衡量模型表现。

Result: 实验表明TRUEBench比现有基准更具挑战性，例如OpenAI o1模型整体通过率仅为69.07%。

Conclusion: TRUEBench为LLM生产力助手提供了更可靠、真实的评估框架，有效揭示了当前模型的能力与局限。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [358] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: 提出了一种轻量级的动态注意力融合（DAF）框架，用于多模态情感分析，结合冻结的文本嵌入和声学特征，通过自适应注意力机制在每句话上动态加权不同模态，在无需微调编码器的情况下显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统的情感分析仅依赖文本，忽略了语调和韵律等非语言线索，难以准确捕捉真实情感意图，因此需要一种能有效融合多模态信息的方法。

Method: 提出动态注意力融合（DAF）框架，使用预训练语言模型的冻结文本嵌入和语音编码器提取的声学特征，通过自适应注意力机制对每个话语中的模态进行动态加权，不进行编码器微调。

Result: 在大型多模态基准上，DAF模型在F1分数上表现优于静态融合和单模态基线，显著降低预测误差，消融实验验证了动态加权策略对复杂情感输入建模的重要性。

Conclusion: DAF通过有效整合语言和非语言信息，为情感预测提供了更鲁棒的基础，对情感计算应用（如情绪识别、心理健康评估和人机交互）具有广泛影响。

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [359] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: 本文提出了一种在掩码扩散语言模型中近似并行采样多个令牌的方法，通过在大型扩散语言模型之上添加一个轻量级单层“采样器”，以更高效地从联合分布中生成文本。


<details>
  <summary>Details</summary>
Motivation: 在掩码扩散语言模型中，并行解码多个令牌会偏离真实的联合分布，导致生成质量下降。本文旨在在保持生成质量的同时提高解码效率。

Method: 设计一个轻量的单层采样器，附加于预训练的扩散语言模型之上，通过多次仅运行该采样器的前向传播，在每次完整模型去噪步骤后解码多个令牌，从而近似联合分布采样。

Result: 在Dream-7B-Base和Dream-7B-Instruct模型上验证了方法的有效性；当每步解码4个令牌时，相对于真实联合分布的MAUVE得分为0.87，显著优于基线的0.31。

Conclusion: 所提出的近似联合采样方法能够在保持生成质量的同时显著提升扩散语言模型的解码效率，适用于语言建模、数学与编程任务。

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [360] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: 本文提出了Painless Activation Steering (PAS)，一种完全自动化的激活 steering 方法，无需人工构造提示或标注特征，可在多种任务和模型上快速提升语言模型的行为控制能力，尤其在偏见、道德和对齐等行为任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型后训练方法（如基于权重的微调或基于提示的引导）存在耗时、昂贵或不可控的问题，而当前的激活引导技术又依赖人工设计的提示对或繁琐的特征标注，限制了其易用性。因此，需要一种更便捷、自动化且高效的替代方案。

Method: 提出Painless Activation Steering (PAS)，利用带标签的数据集自动生成激活向量，实现无需人工干预的激活引导；并引入其内省变体iPAS以增强因果引导效果。该方法可与上下文学习(ICL)和监督微调(SFT)结合使用。

Result: 在三个开源大模型和18项任务上的实验表明，PAS能稳定提升行为类任务的表现（如iPAS在偏见、道德和对齐任务上分别提升10.1%、5.2%和34.8%），但对智力导向任务无效；同时PAS可与ICL和SFT叠加增益。

Conclusion: PAS是一种实用、自动化且轻量的语言模型后训练方法，有效扩展了激活引导的适用范围，明确了其在行为调节任务中的优势与局限，为模型可控性提供了新工具。

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [361] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了MIRAGE基准，用于研究多跳推理中的歧义问题，并设计了CLARION多智能体框架以提升在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理包含歧义的多跳问答时表现不佳，难以正确探索推理路径并给出完整答案。

Method: 构建了一个包含1,142个高质量样本的MIRAGE基准，涵盖句法、一般和语义歧义，并提出CLARION多智能体框架来逐步澄清歧义并进行推理。

Result: 实验表明当前最先进的模型在MIRAGE上表现较差，而CLARION框架显著优于现有方法。

Conclusion: 解决多跳推理中的歧义是一个重要且具有挑战性的问题，CLARION为构建更自适应和鲁棒的推理系统提供了新方向。

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [362] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: 本文提出了ML2B，首个用于评估多语言机器学习代码生成的基准，包含30个Kaggle竞赛任务翻译成13种自然语言，并揭示了非英语任务中15-45%的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习代码生成基准主要局限于英语，忽视了机器学习研究与实践的全球性和多语言需求。

Method: 构建了ML2B基准，包含30个Kaggle竞赛翻译为13种语言，涵盖表格、文本和图像数据类型，并采用AIDE自动化框架进行端到端评估。

Result: 实验结果显示模型在非英语任务上性能下降15-45%，暴露出多语言表示学习在代码生成中的关键挑战。

Conclusion: ML2B填补了多语言ML代码生成评测的空白，揭示了当前模型在非英语语境下的局限性，推动未来多语言代码生成研究。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [363] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

TL;DR: 本文介绍了首个包含多种阿拉伯语方言的伪造语音数据集，并通过多模型评估和人类评分等方式验证了FishSpeech在阿拉伯语语音合成中的优越性，但也指出单一TTS模型可能限制数据集的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语的语音伪造检测研究相对不足，尤其是缺乏多方言的合成语音数据集，现有工作主要集中于英语，因此需要构建针对阿拉伯语的高质量、多模型支持的伪造语音数据集以推动该领域发展。

Method: 提出了一套综合评估流程，包括使用基于嵌入的现代方法、传统MFCC特征结合机器学习分类器以及RawNet2架构进行分类训练；同时计算人工评分的平均意见得分（MOS）和通过ASR模型获得的词错误率（WER），以评估不同TTS模型生成语音的真实性和挑战性。

Result: 实验结果表明，FishSpeech在Casablanca语料库上的阿拉伯语语音克隆任务中表现最佳，生成的语音更逼真且更具挑战性；然而依赖单一TTS模型构建数据集可能影响其泛化能力。

Conclusion: FishSpeech是目前阿拉伯语合成语音中最先进的模型之一，但为了提升检测系统的鲁棒性，未来应整合多个TTS模型来构建更具代表性的多方言阿拉伯语伪造语音数据集。

Abstract: With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [364] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 本文提出了一种名为EditGRPO的混合策略强化学习算法，用于优化放射学报告生成，通过临床动机奖励提升多模态大语言模型在胸片报告生成中的性能和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调方法在临床有效性上缺乏明确对齐，且强化学习存在探索困境和采样效率问题。

Method: 提出EditGRPO算法，结合on-policy探索与off-policy指导，在训练过程中注入句子级详细修正，以临床相关的奖励信号优化生成过程。

Result: 在四个主要胸部X光报告数据集上，相比SFT和GRPO基线，EditGRPO在CheXbert、GREEN、Radgraph和RATEScore指标上平均提升3.4%，并在未见数据集上实现5.9%的平均增益。

Conclusion: EditGRPO有效提升了医学报告生成的准确性和泛化能力，验证了基于临床奖励的强化学习在该领域的潜力。

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [365] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: 提出了一种新的强化学习方法CRL，通过引入批判机制提升大模型的推理和批判能力，训练出的Critique-Coder在代码生成和逻辑推理任务上均优于传统RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在生成响应时缺乏显式的批判与反思机制，而近期研究表明教授模型批判能力有益，因此本文旨在结合批判与强化学习以提升模型推理性能。

Method: 提出Critique Reinforcement Learning（CRL），让模型对（问题, 解答）对生成批判，奖励信号基于批判结论是否与真实标签一致；构建Critique-Coder，使用20% CRL数据与80%标准RL数据混合训练。

Result: Critique-Coder在多个基准上优于纯RL模型，8B版本在LiveCodeBench(v5)上超过60%，优于DeepCoder-14B和GPT-o1，并在BBEH逻辑推理任务中表现出更强的泛化能力。

Conclusion: CRL能有效增强大模型的批判与推理能力，且该能力可跨任务迁移，是标准RL在语言模型推理训练中的有力补充。

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [366] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

TL;DR: 提出了一种多轮响应选择模型RSM-DCK，通过检测上下文和知识库中的相关部分来提升对话系统中回复选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的对话系统在匹配回复候选时使用了全部上下文和知识内容，但其中许多信息因话题转移而无用，影响匹配效果。因此需要一种能识别并利用关键信息的方法。

Method: 提出RSM-DCK模型，首先以最近上下文作为查询，在词级和语句级语义上预选相关的上下文和知识片段；然后让回复候选分别与选定的部分交互；最后利用上下文与回复的融合表示进一步后选相关知识进行匹配。

Result: 在两个基准数据集上的实验表明，该模型优于现有方法，并能有效识别出与回复选择相关的上下文和知识内容。

Conclusion: RSM-DCK通过多层次的相关性检测机制，显著提升了知识增强型对话系统中回复选择的性能。

Abstract: Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [367] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新型的针对大语言模型代理的间接提示注入攻击方法ChatInject，通过模仿原生聊天模板并利用多轮对话中的说服策略，显著提高了攻击成功率，并揭示了现有防御机制对此类攻击的不足。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的代理在外部环境中部署增多，其面临新的安全威胁，尤其是间接提示注入攻击。现有研究主要关注纯文本注入，而忽略了LLM对结构化聊天模板的依赖及其在多轮对话中被上下文操控的潜在风险。因此，本文旨在探索这一被忽视的漏洞。

Method: 提出ChatInject攻击方法，将恶意载荷格式化为类似原生聊天模板的形式，以利用模型遵循指令的特性；进一步设计基于说服策略的多轮变体，通过多轮对话逐步引导代理执行可疑操作。在多个前沿LLM上进行实验，评估攻击成功率、跨模型迁移性和对现有防御手段的规避能力。

Result: 实验结果显示：(1) ChatInject相比传统方法显著提升攻击成功率，在AgentDojo上从5.18%提升至32.05%，在InjecAgent上从15.13%提升至45.90%，多轮变体在InjecAgent上平均达52.33%；(2) 基于聊天模板的载荷具有强跨模型迁移性，即使对闭源模型也有效；(3) 现有基于提示的防御措施对此类攻击基本无效，尤其难以应对多轮变体。

Conclusion: 当前LLM代理系统在面对结构化提示注入和多轮上下文操纵时存在严重安全漏洞，亟需设计更鲁棒的防御机制来应对此类高级攻击。

Abstract: The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [368] [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
*Jasin Cekinmez,Omid Ghahroodi,Saad Fowad Chandle,Dhiman Gupta,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 本文提出了ADAM框架，用于评估和改进多模态大语言模型在传记推理中的表现，包含大规模多语言多模态数据集AdamDB、基于布鲁姆分类的认知分层评测集AdamBench，以及针对传记场景的检索增强生成系统AdamRAG，有效减少幻觉并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传记推理是事实性知识的重要但被忽视的方面，现有模型在处理人物生平信息时易产生幻觉，尤其对知名度较低的人物表现不佳，因此需要系统性的评估框架与增强方法。

Method: 构建了覆盖400多万人物的多语言多模态数据集AdamDB，并设计基于布鲁姆认知分类的六层推理评测集AdamBench；提出专为传记任务优化的检索增强生成系统AdamRAG，结合文本与面部图像输入进行实验验证。

Result: AdamRAG显著提升了开源模型的表现，在闭源模型上也有小幅增益，尤其在低阶推理任务上效果最明显；人物知名度显著影响准确性，面部图像输入带来的提升较小且不稳定。

Conclusion: ADAM是首个融合认知、文化与多模态维度的传记评估框架，为构建多语言、准确且抗幻觉的多模态大语言模型提供了基准与路径。

Abstract: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating
and improving multimodal large language models (MLLMs) in biographical
reasoning. To the best of our knowledge, this is the first work to
systematically examine LLM capabilities in biography, a critical yet
underexplored dimension of factual knowledge. At its core, AdamDB is a
multilingual and multimodal dataset covering over 4 million individuals across
geography, time, and profession, while AdamBench provides cognitively
structured evaluations based on Bloom's taxonomy, spanning six reasoning levels
in both English and native languages. To address hallucinations, particularly
for lesser-known individuals, we propose AdamRAG, a retrieval-augmented
generation system tailored to biographical contexts. Experiments show that
AdamRAG substantially improves open-source models and modestly benefits
closed-source ones, with the largest gains on lower-order reasoning. Popularity
strongly mediates accuracy, and multimodal input via face images offers
smaller, less consistent improvements than retrieval. ADAM establishes the
first benchmark and framework for cognitively, culturally, and multimodally
grounded biographical evaluation, advancing the development of multilingual,
accurate, and hallucination-resistant MLLMs.

</details>


### [369] [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
*Jiaqian Li,Yanshu Li,Ligong Han,Ruixiang Tang,Wenya Wang*

Main category: cs.CL

TL;DR: 提出了一种新的隐式上下文学习方法In-Context Routing (ICR)，在注意力logits层面建模可泛化的上下文学习模式，无需任务特定训练即可实现跨领域和模型的优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有隐式上下文学习方法依赖于残差流中的偏移向量，难以充分利用上下文学习的结构机制且泛化能力有限。

Method: 在注意力logits层面提取可复用的结构方向，并设计可学习的输入条件路由模块来调节注意力logits，实现一次训练、多次复用的框架。

Result: 在12个真实世界数据集和多个大语言模型上验证，ICR持续优于需任务特定检索或训练的先前方法，并在跨领域任务中表现出强健的泛化能力。

Conclusion: ICR通过在注意力logits层面建模通用上下文学习结构，提升了隐式上下文学习的性能与泛化性，推动了其实际应用价值。

Abstract: Implicit in-context learning (ICL) has newly emerged as a promising paradigm
that simulates ICL behaviors in the representation space of Large Language
Models (LLMs), aiming to attain few-shot performance at zero-shot cost.
However, existing approaches largely rely on injecting shift vectors into
residual flows, which are typically constructed from labeled demonstrations or
task-specific alignment. Such designs fall short of utilizing the structural
mechanisms underlying ICL and suffer from limited generalizability. To address
this, we propose In-Context Routing (ICR), a novel implicit ICL method that
internalizes generalizable ICL patterns at the attention logits level. It
extracts reusable structural directions that emerge during ICL and employs a
learnable input-conditioned router to modulate attention logits accordingly,
enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world
datasets spanning diverse domains and multiple LLMs. The results show that ICR
consistently outperforms prior implicit ICL methods that require task-specific
retrieval or training, while demonstrating robust generalization to
out-of-domain tasks where existing methods struggle. These findings position
ICR to push the boundary of ICL's practical value.

</details>


### [370] [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
*R. Alexander Knipper,Charles S. Knipper,Kaiqi Zhang,Valerie Sims,Clint Bowers,Santu Karmaker*

Main category: cs.CL

TL;DR: 该论文大规模评估了45个大语言模型（LLMs）在8种经典认知偏差上的表现，发现LLMs在17.8%-57.3%的案例中表现出与人类相似的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于现实决策场景，探究其是否具备类似人类的认知偏差对提升模型可靠性至关重要。

Method: 提出一个基于多项选择任务的评估框架，联合心理学家构建了包含220个决策情境的数据集，并通过可控提示变体生成超过280万条模型响应进行分析。

Result: LLMs在锚定、可得性、确认、框架、解释、过度归因、前景理论和代表性偏差上均表现出显著偏差倾向；模型规模增大（>32B参数）在39.5%的情况下降低偏差，更高细节的提示最多减少14.9%的偏差，但过度归因偏差反而增加8.8%。

Conclusion: 大语言模型存在系统性认知偏差，提示设计和模型规模可部分缓解但不能消除这些偏差，需进一步关注其决策可靠性。

Abstract: As Large Language Models (LLMs) are increasingly embedded in real-world
decision-making processes, it becomes crucial to examine the extent to which
they exhibit cognitive biases. Extensively studied in the field of psychology,
cognitive biases appear as systematic distortions commonly observed in human
judgments. This paper presents a large-scale evaluation of eight
well-established cognitive biases across 45 LLMs, analyzing over 2.8 million
LLM responses generated through controlled prompt variations. To achieve this,
we introduce a novel evaluation framework based on multiple-choice tasks,
hand-curate a dataset of 220 decision scenarios targeting fundamental cognitive
biases in collaboration with psychologists, and propose a scalable approach for
generating diverse prompts from human-authored scenario templates. Our analysis
shows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances
across a range of judgment and decision-making contexts targeting anchoring,
availability, confirmation, framing, interpretation, overattribution, prospect
theory, and representativeness biases. We find that both model size and prompt
specificity play a significant role on bias susceptibility as follows: larger
size (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt
detail reduces most biases by up to 14.9%, except in one case
(Overattribution), which is exacerbated by up to 8.8%.

</details>


### [371] [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
*Yibo Yan,Guangwei Xu,Xin Zou,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了DocPruner，首个用于视觉文档检索（VDR）的自适应patch级嵌入剪枝框架，通过利用文档内部patch注意力分布动态剔除冗余嵌入，在保持检索性能的同时显著降低50-60%的存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多向量范式的VDR方法虽然效果好，但每个文档需存储数百个向量，导致存储开销过大，难以大规模部署。

Method: 提出DocPruner框架，利用文档内部的patch注意力分布来自适应地识别并剔除冗余的patch级嵌入，实现高效的存储压缩。

Result: 在十余个代表性数据集上实验表明，DocPruner可将主流多向量VDR模型的存储减少50-60%，同时对检索性能影响极小。

Conclusion: DocPruner为构建高效、可扩展的大规模视觉文档检索系统提供了一种鲁棒、灵活且有效的解决方案。

Abstract: Visual Document Retrieval (VDR), the task of retrieving visually-rich
document pages using queries that combine visual and textual cues, is crucial
for numerous real-world applications. Recent state-of-the-art methods leverage
Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing
each document as patch-level embeddings to capture fine-grained details. While
highly effective, this approach introduces a critical challenge: prohibitive
storage overhead, as storing hundreds of vectors per page makes large-scale
deployment costly and impractical. To address this, we introduce DocPruner, the
first framework to employ adaptive patch-level embedding pruning for VDR to
effectively reduce the storage overhead. DocPruner leverages the intra-document
patch attention distribution to dynamically identify and discard redundant
embeddings for each document. This adaptive mechanism enables a significant
50-60% reduction in storage for leading multi-vector VDR models with negligible
degradation in document retrieval performance. Extensive experiments across
more than ten representative datasets validate that DocPruner offers a robust,
flexible, and effective solution for building storage-efficient, large-scale
VDR systems.

</details>


### [372] [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
*Passant Elchafei,Mayar Osama,Mohamed Rageh,Mervat Abuelkheir*

Main category: cs.CL

TL;DR: 提出了一种基于图神经网络和词典的混合方法，用于阿拉伯语文档级可读性预测，结合GNN和Transformer模型并通过后期融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 在BAREC 2025共享任务的约束赛道中，提升阿拉伯语文档级可读性预测的准确性，探索图结构与上下文表示的结合优势。

Method: 将文档建模为句子级图，节点表示句子和词元，边表示词汇共现和类别关系；使用SAMER词典特征和阿拉伯语Transformer嵌入，并通过GNN和Transformer双分支结构进行 late fusion，最后用max pooling聚合句子级输出以得到文档级预测。

Result: 实验表明，该混合方法在多个可读性指标上优于单独的GNN或Transformer分支，在文档级预测中表现更优，但GNN在句子级可读性预测上仍更具优势。

Conclusion: 融合GNN与Transformer的双分支架构能有效提升阿拉伯语文档级可读性预测性能，而GNN更适合精细的句子级预测任务。

Abstract: We present a graph-based approach enriched with lexicons to predict
document-level readability in Arabic, developed as part of the Constrained
Track of the BAREC Shared Task 2025. Our system models each document as a
sentence-level graph, where nodes represent sentences and lemmas, and edges
capture linguistic relationships such as lexical co-occurrence and class
membership. Sentence nodes are enriched with features from the SAMER lexicon as
well as contextual embeddings from the Arabic transformer model. The graph
neural network (GNN) and transformer sentence encoder are trained as two
independent branches, and their predictions are combined via late fusion at
inference. For document-level prediction, sentence-level outputs are aggregated
using max pooling to reflect the most difficult sentence. Experimental results
show that this hybrid method outperforms standalone GNN or transformer branches
across multiple readability metrics. Overall, the findings highlight that
fusion offers advantages at the document level, but the GNN-only approach
remains stronger for precise prediction of sentence-level readability.

</details>


### [373] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: AceSearcher是一种基于合作自对弈的搜索增强大语言模型框架，通过让单一LLM在查询分解和答案求解两个角色间切换，结合监督微调与强化微调，显著提升复杂推理任务的性能，且在参数量远小于现有模型的情况下实现更优效果。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强的大语言模型在复杂多跳推理任务中表现不佳，主要受限于检索效率和推理能力，缺乏有效的训练机制来协同优化分解与求解过程。

Method: 提出AceSearcher框架，采用单一LLM交替扮演分解者和求解者角色，通过监督微调处理多样化的搜索、推理和分解任务，并结合以最终答案准确率为优化目标的强化微调，无需中间步骤标注。

Result: 在10个数据集上的三个高推理需求任务中，AceSearcher平均精确匹配率提升7.6%；在文档级金融推理任务中，其32B版本性能媲美DeepSeek-V3，但参数量不足其5%；即使1.5B和8B版本也常优于参数多达9倍的现有模型。

Conclusion: AceSearcher通过协同训练机制有效提升了搜索增强LLM在复杂推理任务中的性能与效率，展现出卓越的参数利用效率和可扩展性。

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [374] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

TL;DR: HEART是一种利用情感驱动提示进行迭代自我修正的新框架，通过情绪反馈提升语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展策略主要关注逻辑或结构优化，未能利用情感反馈的引导潜力。受情绪调节认知表现的心理学研究启发，本文探索情感在模型推理中的作用。

Method: 提出HEART框架，使用基于保罗·艾克曼六种基本情绪的情感化短语为模型提供错误反馈，并在迭代中系统调整情绪语气，引导模型摆脱错误推理路径，探索更优解。

Result: 在OlympiadBench、Humanity's Last Exam和SimpleQA等基准上，HEART配合oracle验证器显著提升了推理深度和准确性；但在无验证器场景下效果不稳定，存在应用瓶颈。

Conclusion: 情感反馈可有效增强语言模型的推理能力，未来机器推理的突破可能不仅在于逻辑优化，还在于理解和利用模型的“心灵”（HEART）。

Abstract: Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [375] [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
*Khanh Trinh Pham,Thu Huong Nguyen,Jun Jo,Quoc Viet Hung Nguyen,Thanh Tam Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了MultiSpider 2.0，一个将Spider 2.0扩展到八种语言的多语言Text-to-SQL基准测试，揭示了当前大模型在多语言环境下的性能显著下降，并提出了基于协作的语言代理基线方法以提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL基准主要局限于英语，缺乏对多语言支持的评估，限制了多语言场景下的发展和实际企业应用。

Method: 构建MultiSpider 2.0数据集，涵盖八种语言，在保持原有结构复杂性的同时引入语言和方言多样性；提出一种协作驱动的语言代理基线方法，通过迭代优化SQL查询提升性能。

Result: 当前最先进的大模型（如DeepSeek-R1和OpenAI o1）在MultiSpider 2.0上的执行准确率仅为4%，远低于在MultiSpider 1.0上的60%；所提基线方法将准确率提升至15%。

Conclusion: 存在显著的多语言性能差距，需开发更具跨语言鲁棒性的方法，以推动Text-to-SQL技术在真实企业环境中的部署。

Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.

</details>


### [376] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

TL;DR: 本文提出了一种专注于心智理论（ToM）的对话代理ToMAgent（ToMA），通过将ToM与对话前瞻结合，提升大语言模型在对话中的目标达成能力和社会智能表现。


<details>
  <summary>Details</summary>
Motivation: 当前聊天机器人和基于大语言模型的社会代理通常缺乏对他人心理状态的理解（即心智理论，ToM），限制了其社会智能的发展。因此，研究旨在探索如何有效整合ToM以提升对话代理的社交能力和目标达成效率。

Method: 首先通过提示让模型在对话轮次间生成心理状态，验证其有效性；然后提出ToMAgent（ToMA），通过将心智理论（ToM）与对话前瞻相结合进行训练，使生成的心理状态更有利于实现长期对话目标。在Sotopia社交评估基准上进行实验验证。

Result: 实验表明，ToMA在多种基线方法中表现出更优的对话性能，展现出更强的战略性和目标导向推理能力，能够进行长视野适应，并更好地维护与对话伙伴的关系。

Conclusion: 将心智理论与对话规划相结合可有效提升大语言模型代理的社会智能水平，ToMA为构建更具社交能力的智能代理提供了可行路径。

Abstract: Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


### [377] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了DataMind，一种可扩展的数据合成与代理训练方法，旨在构建通用的开源数据解析代理，解决了现有方法在数据资源、训练策略和多轮代码生成稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的数据分析代理依赖专有模型和提示工程，而开源模型难以应对真实场景中的多样化数据格式和复杂的多步推理任务，因此需要一种更强大且开放的数据分析代理训练框架。

Method: DataMind采用细粒度任务分类和递归的由易到难任务组合机制来提升合成查询的多样性与难度；结合知识增强的轨迹采样与模型/规则过滤；动态调整SFT与RL联合训练目标；并设计了内存高效且稳定的基于代码的多轮 rollout 框架。基于此构建了高质量的数据集DataMind-12K。

Result: 在多个数据分析基准上，DataMind-14B以71.16%的平均得分达到SOTA，超越DeepSeek-V3.1和GPT-5；DataMind-7B以68.10%的成绩成为最佳开源模型。

Conclusion: DataMind为构建高性能开源数据解析代理提供了有效方案，其发布的数据集与模型将促进社区在智能代理训练方面的研究。

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [378] [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
*Henrique Godoy*

Main category: cs.CL

TL;DR: Extract-0是一个70亿参数的语言模型，专为文档信息抽取优化，在性能上超越了参数规模大得多的模型，如GPT-4系列。


<details>
  <summary>Details</summary>
Motivation: 为了在文档信息提取任务中实现高效且高性能的模型，减少对大规模通用模型的依赖及其带来的高计算资源消耗。

Method: 采用合成数据生成、基于LoRA的监督微调以及基于组相对策略优化（GRPO）的强化学习，结合语义相似性奖励函数进行训练。

Result: 在1000个多样化文档提取任务的基准上取得0.573的平均奖励，超过GPT-4.1（0.457）、o3（0.464）和GPT-4.1-2025（0.459），仅微调0.53%的参数（4040万/76.6亿）。

Conclusion: 任务特定优化可显著提升效率与性能，使小规模模型超越大型通用模型，同时大幅降低计算资源需求。

Abstract: This paper presents Extract-0, a 7-billion parameter language model
specifically optimized for document information extraction that achieves
performance exceeding models with parameter counts several orders of magnitude
larger. Through a novel combination of synthetic data generation, supervised
fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via
Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of
0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming
GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology
employs a memory-preserving synthetic data generation pipeline that produces
280,128 training examples from diverse document sources, followed by
parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M
out of 7.66B parameters). The reinforcement learning phase introduces a novel
semantic similarity-based reward function that handles the inherent ambiguity
in information extraction tasks. This research demonstrates that task-specific
optimization can yield models that surpass general-purpose systems while
requiring substantially fewer computational resource.

</details>


### [379] [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
*Feng Wang,Yuqing Li,Han Xiao*

Main category: cs.CL

TL;DR: jina-reranker-v3是一种具有0.6B参数的多语言文档重排序模型，通过在查询和文档之间引入因果自注意力机制实现“最后但不迟”的交互，在保持模型紧凑的同时在BEIR基准上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在减少模型大小的同时提升多语言文档重排序的性能，克服传统late interaction模型（如ColBERT）缺乏充分交互的局限。

Method: 在同一个上下文窗口内对查询和文档进行联合编码，采用因果自注意力机制实现深度交互，并从每个文档的最后一个token提取上下文化嵌入进行排序。

Result: 在BEIR数据集上实现了61.94的nDCG@10得分，性能达到SOTA，且模型大小仅为生成式列表重排序器的十分之一。

Conclusion: jina-reranker-v3通过新颖的交互机制在效率和效果之间取得了良好平衡，是一种高效、紧凑且高性能的多语言重排序模型。

Abstract: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.

</details>


### [380] [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
*Kelli Henry,Steven Xu,Kaitlin Blotske,Moriah Cargile,Erin F. Barreto,Brian Murray,Susan Smith,Seth R. Bauer,Yanjun Gao,Tianming Liu,Andrea Sikora*

Main category: cs.CL

TL;DR: GPT-4o在药物名称匹配、药物相互作用识别和药物医嘱生成方面表现不佳，显示出在医疗用药推荐中存在显著缺陷，需通过临床注释数据进行领域特定训练。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-4o在药物治疗推荐中的准确性与一致性，填补大语言模型在临床用药决策支持中性能评估的空白。

Method: 通过三项基准测试：药物剂型匹配、药物相互作用识别（使用内部知识和网络搜索增强）、药物医嘱语句生成；采用余弦相似度、Levenshtein相似度、ROUGE-F1及临床专家人工评估来衡量准确性。

Result: GPT-4o在剂型匹配中仅49%药物完全正确，常遗漏或虚构剂型；药物相互作用识别在有交互时表现较好（69.2%），但无交互时搜索反而降低准确性（40%）；药物医嘱语句中34.2%存在错误。

Conclusion: GPT-4o在当前状态下不适用于独立药物推荐，需结合领域特定训练和更全面的评估框架以提升其临床可用性。

Abstract: Background: Large language models (LLMs) can be useful in diagnosing medical
conditions, but few studies have evaluated their consistency in recommending
appropriate medication regimens. The purpose of this evaluation was to test
GPT-4o on three medication benchmarking tests including mapping a drug name to
its correct formulation, identifying drug-drug interactions using both its
internal knowledge and using a web search, and preparing a medication order
sentence after being given the medication name. Methods: Using GTP-4o three
experiments were completed. Accuracy was quantified by computing cosine
similarity on TF-IDF vectors, normalized Levenshtein similarity, and
ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual
evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation
matching, with frequent omissions of available drug formulations (mean 1.23 per
medication) and hallucinations of formulations that do not exist (mean 1.14 per
medication). Only 49% of tested medications were correctly matched to all
available formulations. Accuracy was decreased for medications with more
formulations (p<0.0001). GPT-4o was also inconsistent at identifying
drug-drug-interactions, although it had better performance with the
search-augmented assessment compared to its internal knowledge (54.7% vs.
69.2%, p=0.013). However, allowing a web-search worsened performance when there
was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,
GPT-4o performed moderately with preparing a medication order sentence, with
only 65.8% of medication order sentences containing no medication or
abbreviation errors. Conclusions: Model performance was overall poor for all
tests. This highlights the need for domain-specific training through
clinician-annotated datasets and a comprehensive evaluation framework for
benchmarking performance.

</details>


### [381] [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
*Yuan Liang,Jiaxian Li,Yuqing Wang,Piaohong Wang,Motong Tian,Pai Liu,Shuofei Qiao,Runnan Fang,He Zhu,Ge Zhang,Minghao Liu,Yuchen Eleanor Jiang,Ningyu Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了首个评估深度研究代理（DRA）个性化能力的基准——Personalized Deep Research Bench，并配套提出PQR评估框架，用于衡量个性化对齐、内容质量和事实可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有DRA评估多依赖于封闭式基准，缺乏针对开放性任务和个性化场景的评估体系，限制了个性化AI研究助手的发展。

Method: 构建包含10个领域50项研究任务与25个真实用户画像的基准数据集，形成250个现实用户-任务查询对；提出PQR评估框架，综合评估个性化对齐（P）、内容质量（Q）和事实可靠性（R）。

Result: 实验揭示了当前系统在处理个性化深度研究任务中的能力与局限，验证了新基准和评估框架的有效性。

Conclusion: 该工作为开发和评估下一代真正个性化的AI研究助手提供了严谨的基础。

Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations
and generate comprehensive reports, demonstrating strong real-world potential.
However, existing evaluations mostly rely on close-ended benchmarks, while
open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introduce Personalized Deep
Research Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user
profiles that combine structured persona attributes with dynamic real-world
contexts, yielding 250 realistic user-task queries. To assess system
performance, we propose the PQR Evaluation Framework, which jointly measures
(P) Personalization Alignment, (Q) Content Quality, and (R) Factual
Reliability. Our experiments on a range of systems highlight current
capabilities and limitations in handling personalized deep research. This work
establishes a rigorous foundation for developing and evaluating the next
generation of truly personalized AI research assistants.

</details>


### [382] [LLMs Behind the Scenes: Enabling Narrative Scene Illustration](https://arxiv.org/abs/2509.22940)
*Melissa Roemmele,John Joon Young Chung,Taewook Kim,Yuqian Sun,Alex Calderwood,Max Kreminski*

Main category: cs.CL

TL;DR: 本文探讨了利用大语言模型（LLM）作为接口，结合文本到图像模型自动生成故事场景插图的流程，并构建了一个用于跨模态叙事转换研究的公开数据集SceneIllustrations。


<details>
  <summary>Details</summary>
Motivation: 受文本到图像生成模型进展的启发，探索如何通过生成式AI将纯文本故事自动转化为视觉插图，以增强叙事表现力。

Method: 采用基于LLM的提示接口调用文本到图像模型生成插图，并在大型故事语料库上应用不同变体的生成流程，通过人工标注进行插图质量评估。

Result: 成功构建了SceneIllustrations数据集，实验表明LLM能有效提取并表达故事中隐含的场景知识，显著提升插图生成与评估效果。

Conclusion: LLM在跨模态叙事转换中具有关键作用，既能促进高质量插图生成，也为自动化评估提供了新路径。

Abstract: Generative AI has established the opportunity to readily transform content
from one medium to another. This capability is especially powerful for
storytelling, where visual illustrations can illuminate a story originally
expressed in text. In this paper, we focus on the task of narrative scene
illustration, which involves automatically generating an image depicting a
scene in a story. Motivated by recent progress on text-to-image models, we
consider a pipeline that uses LLMs as an interface for prompting text-to-image
models to generate scene illustrations given raw story text. We apply
variations of this pipeline to a prominent story corpus in order to synthesize
illustrations for scenes in these stories. We conduct a human annotation task
to obtain pairwise quality judgments for these illustrations. The outcome of
this process is the SceneIllustrations dataset, which we release as a new
resource for future work on cross-modal narrative transformation. Through our
analysis of this dataset and experiments modeling illustration quality, we
demonstrate that LLMs can effectively verbalize scene knowledge implicitly
evoked by story text. Moreover, this capability is impactful for generating and
evaluating illustrations.

</details>


### [383] [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
*Mohammed Sabry,Anya Belz*

Main category: cs.CL

TL;DR: 本文研究了在等计算量条件下，引入特定合成数据（如前向/后向复制）是否能提升模型的上下文学习（ICL）能力。结果表明，尽管Bi-Induct方法可加速小模型中归纳头的形成，但并未持续提升ICL性能；自然文本训练的大模型反而表现更优。


<details>
  <summary>Details</summary>
Motivation: 探究显式激活归纳电路是否真正有助于提升上下文学习能力，还是自然文本已足够，尤其是在固定计算资源的情况下。

Method: 提出Bi-Induct轻量课程学习方法，在预训练中引入前向复制、后向复制或混合模式的合成数据，对比不同数据分布下模型在ICL任务、电路激活情况和语言建模性能上的表现。

Result: Bi-Induct可加速小模型中归纳头的出现，但未带来一致的ICL提升；1B参数的纯自然文本模型在函数式ICL任务上表现最佳；更大模型即使无显式诱导也能自发发展出更强的归纳头；移除关键归纳头对自然模型影响更大，说明其电路更具中心性。

Conclusion: 仅诱导归纳电路激活不足以提升ICL，关键是这些电路需成为功能上必要的负载结构。研究强调应设计机制感知的预训练策略，促进形成真正起支撑作用的结构，而非仅仅存在的结构。

Abstract: Does explicitly exercising the induction circuit during pretraining improve
in-context learning (ICL), or is natural text sufficient when compute is held
constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate
induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight
curriculum that injects forward-copy (Induction), backward-copy (Anti), or a
balanced mix into the pretraining stream. We train models from 0.13B to 1B
parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)
head-level telemetry, and (iii) held-out language modeling perplexity. Our
findings challenge the assumption that early induction circuit activation
directly improves ICL. While Bi-Induct accelerates induction-head emergence at
small scales, this does not consistently yield stronger generalization. On
standard LM benchmarks, Bi-Induct matches natural-only training; on
function-style ICL probes, the 1B natural-only performs best. Stress tests
(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these
trends. Telemetry shows larger natural-only models develop broader, earlier
induction heads without explicit induction patterns. Anti-induction data fails
to elicit meaningful activation. Perplexity penalties from synthetic data
shrink with scale, suggesting larger models can absorb non-natural patterns
with minimal cost. Crucially, ablating the top 2% of induction heads degrades
ICL more than random ablations, especially for natural-only models, indicating
more centralized, load-bearing circuits. Bi-Induct variants exhibit more
redundant induction activity, implying different circuit utilization. Overall,
inducing activation is not sufficient: ICL gains depend on these circuits
becoming functionally necessary. These results underscore mechanism-aware
pretraining diagnostics and data mixtures that foster load-bearing, not merely
present, structure.

</details>


### [384] [Emergent morpho-phonological representations in self-supervised speech models](https://arxiv.org/abs/2509.22973)
*Jon Gauthier,Canaan Breiss,Matthew Leonard,Edward F. Chang*

Main category: cs.CL

TL;DR: 该研究探讨了用于语音识别的自监督语音模型如何表示英语名词和动词的规则屈折变化，发现其表征具有全局线性几何结构，反映的是词汇间的规律性分布关系，而非直接对应音位或形态单位。


<details>
  <summary>Details</summary>
Motivation: 理解自监督语音模型在识别口语词汇时所使用的语言表征类型。

Method: 分析在单词识别任务上优化的S3M变体模型对英语常见名词和动词屈折现象的表征方式。

Result: 模型表征呈现出全局线性几何结构，能够关联基本词形与其规则屈折形式，这种结构反映的是词汇间的规律性分布关系，而非直接对应音系或形态单元。

Conclusion: 自监督模型可能通过分布关系的几何结构支持口语词识别，挑战了传统认为必须依赖独立音系和形态表征的观点。

Abstract: Self-supervised speech models can be trained to efficiently recognize spoken
words in naturalistic, noisy environments. However, we do not understand the
types of linguistic representations these models use to accomplish this task.
To address this question, we study how S3M variants optimized for word
recognition represent phonological and morphological phenomena in frequent
English noun and verb inflections. We find that their representations exhibit a
global linear geometry which can be used to link English nouns and verbs to
their regular inflected forms.
  This geometric structure does not directly track phonological or
morphological units. Instead, it tracks the regular distributional
relationships linking many word pairs in the English lexicon -- often, but not
always, due to morphological inflection. These findings point to candidate
representational strategies that may support human spoken word recognition,
challenging the presumed necessity of distinct linguistic representations of
phonology and morphology.

</details>


### [385] [Same Content, Different Representations: A Controlled Study for Table QA](https://arxiv.org/abs/2509.22983)
*Yue Zhang,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 本文提出了首个系统研究表格表示形式对模型性能影响的控制实验，通过保持内容不变而改变结构，引入了一个新的诊断性基准来评估不同Table QA方法在真实场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Table QA基准通常局限于固定的数据格式，未能系统地探讨数据表示形式本身如何影响模型性能，尤其是在同时涉及结构化和半结构化表格的真实场景中。

Method: 作者设计了一个文本化（verbalization）流程，生成内容相同但结构不同的成对结构化与半结构化表格，并构建了一个包含多种维度（如表格大小、连接需求、查询复杂度和模式质量）的诊断性基准，用于比较SQL-based方法、大语言模型（LLMs）和混合方法的表现。

Result: 实验表明：基于SQL的方法在结构化数据上表现优异但面对半结构化数据时性能下降；LLMs具有灵活性但精度较低；混合方法在噪声模式下表现更优，尤其在大表和复杂查询下效果更明显。没有单一方法在所有条件下都表现最佳。

Conclusion: 表格的表示形式显著影响Table QA系统的性能，应根据数据特征选择合适的方法；研究强调了表示形式的核心作用，并为面向多样化真实数据的鲁棒混合方法设计提供了实践指导。

Abstract: Table Question Answering (Table QA) in real-world settings must operate over
both structured databases and semi-structured tables containing textual fields.
However, existing benchmarks are tied to fixed data formats and have not
systematically examined how representation itself affects model performance. We
present the first controlled study that isolates the role of table
representation by holding content constant while varying structure. Using a
verbalization pipeline, we generate paired structured and semi-structured
tables, enabling direct comparisons across modeling paradigms. To support
detailed analysis, we introduce a diagnostic benchmark with splits along table
size, join requirements, query complexity, and schema quality. Our experiments
reveal consistent trade-offs: SQL-based methods achieve high accuracy on
structured inputs but degrade on semi-structured data, LLMs exhibit flexibility
but reduced precision, and hybrid approaches strike a balance, particularly
under noisy schemas. These effects intensify with larger tables and more
complex queries. Ultimately, no single method excels across all conditions, and
we highlight the central role of representation in shaping Table QA
performance. Our findings provide actionable insights for model selection and
design, paving the way for more robust hybrid approaches suited for diverse
real-world data formats.

</details>


### [386] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本文介绍了使用大语言模型生成的英语和捷克语文本语料库，旨在与人类撰写的文本进行语言学对比分析。


<details>
  <summary>Details</summary>
Motivation: 创建一个可用于比较人类书写文本与大语言模型生成文本的语言学资源，并确保其涵盖多种体裁、主题、作者和文本类型，同时与现有人类语料库具有可比性。

Method: 基于OpenAI、Anthropic、Alphabet、Meta和DeepSeek等公司的大语言模型（从GPT-3到GPT-4.5）生成文本，复制了BE21和Koditex两个参考语料库，并按照通用依存关系标准进行分词、词形还原及形态句法标注。

Result: 生成的语料库包含平均每个模型864k英文标记（共2700万）和768k捷克文标记（共2150万），已公开发布并可通过捷克国家语料库搜索界面访问。

Conclusion: 该资源为分析LLM生成文本与人类文本之间的语言差异提供了高质量、多体裁、跨语言的基础数据。

Abstract: This article presents two corpora of English and Czech texts generated with
large language models (LLMs). The motivation is to create a resource for
comparing human-written texts with LLM-generated text linguistically. Emphasis
was placed on ensuring these resources are multi-genre and rich in terms of
topics, authors, and text types, while maintaining comparability with existing
human-created corpora. These generated corpora replicate reference human
corpora: BE21 by Paul Baker, which is a modern version of the original Brown
Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in
Czech. The new corpora were generated using models from OpenAI, Anthropic,
Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and
are tagged according to the Universal Dependencies standard (i.e., they are
tokenized, lemmatized, and morphologically and syntactically annotated). The
subcorpus size varies according to the model used (the English part contains on
average 864k tokens per model, 27M tokens altogether, the Czech partcontains on
average 768k tokens per model, 21.5M tokens altogether). The corpora are freely
available for download under the CC BY 4.0 license (the annotated data are
under CC BY-NC-SA 4.0 licence) and are also accessible through the search
interface of the Czech National Corpus.

</details>


### [387] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: 本文提出ReMemR1，一种具有回调增强记忆的内存增强代理，结合多级奖励强化学习（RLMLR），以解决大模型在长上下文问答中信息丢失和监督稀疏的问题，显著提升了长文档问答性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文问答时面临关键证据分散、信息 overwritten 和学习信号稀疏等问题，现有单向读取记忆方法存在不可逆的信息损失。

Method: 提出ReMemR1，支持从完整记忆历史中选择性检索，并实现非线性推理与早期证据回溯；引入多级奖励强化学习（RLMLR），结合最终答案奖励与密集的步骤级信号，以指导有效记忆使用。

Result: 在长文档问答任务上，ReMemR1显著优于现有的基于记忆的方法，验证了其在缓解信息退化、增强监督和多跳记忆利用方面的有效性。

Conclusion: ReMemR1通过可回调的记忆机制和多级强化学习，为长上下文推理代理提供了一个高效且可扩展的解决方案。

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [388] [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
*Binwei Yao,Chao Shang,Wanyu Du,Jianfeng He,Ruixue Lian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CL

TL;DR: 本文提出了多智能体辩论系统（MADS）中智能体谄媚行为的首个可操作框架，定义了其形式化概念，开发了评估指标，并揭示了其对信息交换和辩论结果的负面影响，进而提出平衡合作与分歧的设计原则。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多智能体辩论系统中表现出的过度顺从（谄媚）倾向可能导致过早达成共识，削弱辩论系统的创新性和准确性，但目前对智能体间谄媚行为的影响尚缺乏理解。

Method: 提出针对MADS场景的谄媚行为形式化定义，设计新的评估指标，并在去中心化与中心化辩论框架中系统研究不同角色（辩者与裁判）的谄媚程度对辩论结果的影响。

Result: 发现谄媚是导致多智能体辩论提前崩溃、准确率低于单智能体基线的核心失败模式，且存在由辩者和裁判分别驱动的不同失败机制。

Conclusion: 应通过有针对性的设计原则来调控智能体间的谄媚行为，以在合作与建设性分歧之间取得平衡，提升多智能体辩论系统的有效性。

Abstract: Large language models (LLMs) often display sycophancy, a tendency toward
excessive agreeability. This behavior poses significant challenges for
multi-agent debating systems (MADS) that rely on productive disagreement to
refine arguments and foster innovative thinking. LLMs' inherent sycophancy can
collapse debates into premature consensus, potentially undermining the benefits
of multi-agent debate. While prior studies focus on user--LLM sycophancy, the
impact of inter-agent sycophancy in debate remains poorly understood. To
address this gap, we introduce the first operational framework that (1)
proposes a formal definition of sycophancy specific to MADS settings, (2)
develops new metrics to evaluate the agent sycophancy level and its impact on
information exchange in MADS, and (3) systematically investigates how varying
levels of sycophancy across agent roles (debaters and judges) affects outcomes
in both decentralized and centralized debate frameworks. Our findings reveal
that sycophancy is a core failure mode that amplifies disagreement collapse
before reaching a correct conclusion in multi-agent debates, yields lower
accuracy than single-agent baselines, and arises from distinct debater-driven
and judge-driven failure modes. Building on these findings, we propose
actionable design principles for MADS, effectively balancing productive
disagreement with cooperation in agent interactions.

</details>


### [389] [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
*Chunyang Jiang,Yonggang Zhang,Yiyang Cai,Chi-Min Chan,Yulong Liu,Mingming Chen,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 提出一种无需自我评估的轻量级自改进方法，通过语义投票（基于语义相似性）替代传统硬匹配，提升大语言模型在不可验证任务上的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 降低获取监督数据的成本，解决现有自评估方法在不可验证任务中计算开销高和存在内在偏见的问题。

Method: 引入语义投票机制，使用轻量级句子嵌入模型衡量响应间的语义相似性，实现软匹配，避免依赖大语言模型进行自我评估。

Result: 实验表明该方法在多种模型架构和任务上均优于自评估方法，显著提高计算效率并取得更好整体性能。

Conclusion: 语义投票是一种高效、低开销的自改进方法，适用于不可验证任务，有效克服了自评估的局限性。

Abstract: The rising cost of acquiring supervised data has driven significant interest
in self-improvement for large language models (LLMs). Straightforward
unsupervised signals like majority voting have proven effective in generating
pseudo-labels for verifiable tasks, while their applicability to unverifiable
tasks (e.g., translation) is limited by the open-ended character of responses.
As a result, self-evaluation mechanisms (e.g., self-judging and entropy
minimization) are predominantly used to derive pseudo-labels. However,
self-evaluation relying on LLMs typically incurs high computational overhead
and introduces overconfidence issues due to intrinsic biases. To address these
challenges, we propose a novel self-evaluation-free approach for unverifiable
tasks, designed for lightweight yet effective self-improvement. Inspired by
majority voting commonly employed in verifiable tasks, we propose semantic
voting as a novel mechanism that relaxes the principle of hard matching (i.e.,
exact matching) toward soft matching (i.e., semantic similarity). Soft matching
is achieved by leveraging a lightweight sentence embedding model to quantify
semantic similarity, thereby mitigating excessive computational burden and
intrinsic bias-associated limitations of self-evaluation. Comprehensive
experiments demonstrate that our method achieves substantial gains in
computational efficiency and overall better performance than self-evaluation
methods across diverse model architectures and tasks.

</details>


### [390] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 本文提出了EviPath，一种基于证据锚定的推理路径合成范式，用于增强检索生成（RAG）代理的开发，通过合成包含环境交互的推理轨迹数据，显著提升小规模语言模型在问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG代理缺乏过程级监督，强化学习面临奖励稀疏问题，且现有数据合成方法无法建模环境交互，限制了代理的推理与工具使用能力。

Method: EviPath包括三部分：(i) 基于溯因推理的子任务规划，分解问题并规划最优解路径；(ii) 基于证据支持的子问题回答，构建代理环境生成推理过程和答案；(iii) 对话式微调，将完整的代理-环境交互轨迹转化为对话格式用于监督微调。

Result: 在多个问答基准上的实验表明，使用EviPath合成数据训练的8B参数模型在开放域问答中比现有最先进方法绝对EM指标高出14.7%，性能显著且稳定提升。

Conclusion: EviPath通过合成高质量、包含环境交互的推理路径数据，有效提升了语言模型作为RAG代理的复杂推理和工具使用能力，为小规模模型的智能代理训练提供了新路径。

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [391] [The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models](https://arxiv.org/abs/2509.23088)
*Esteban Garces Arias,Julian Rodemann,Christian Heumann*

Main category: cs.CL

TL;DR: 提出一种基于可信集的几何框架，用于量化和分解大语言模型在创意文本生成中的不确定性，并通过与人类创作变异对比进行校准。


<details>
  <summary>Details</summary>
Motivation: 理解和量化大语言模型在创意任务中的不确定性具有挑战性，尤其是在存在多种合理输出的情况下。需要一种可解释且结构化的不确定性分析方法。

Method: 采用可信集（概率分布的凸包）作为几何表示，对神经文本生成中的不确定性进行建模；在500个写作提示、10种人类续写的基础上，评估四种语言模型和五种解码策略，共生成10万篇故事。

Result: 最佳模型与人类创作的校准得分为0.434（Gemma-2B，温度0.7）；总认知不确定性中，39.4%到72.0%由解码策略选择引起；模型规模与校准质量相关性弱，基础模型与指令微调模型在校准上无显著差异。

Conclusion: 所提出的几何框架能有效揭示模型在创意生成中与人类变异之间的差距，为提升人机创意对齐提供了可操作的见解。

Abstract: Understanding uncertainty in large language models remains a fundamental
challenge, particularly in creative tasks where multiple valid outputs exist.
We present a geometric framework using credal sets - convex hulls of
probability distributions - to quantify and decompose uncertainty in neural
text generation, calibrated against human creative variation. Analyzing 500
creative writing prompts from the WritingPrompts dataset with 10 unique human
continuations each, we evaluate four language models across five decoding
strategies, generating 100,000 stories. Our credal set analysis reveals
substantial gaps in capturing human creative variation, with the best
model-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We
decompose total uncertainty into epistemic and aleatoric components, finding
that the choice of decoding strategy contributes 39.4% to 72.0% of total
epistemic uncertainty. Model scale shows weak correlation with calibration
quality and no significant difference exists between base and instruction-tuned
models in calibration quality. Our geometric framework provides actionable
insights for improving generation systems for human-AI creative alignment. We
release our complete experimental framework.

</details>


### [392] [d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)
*Yuchu Jiang,Yue Cai,Xiangzhong Luo,Jiale Fu,Jiarui Wang,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: 提出了一种无需训练的近似KV缓存框架d²Cache，用于加速扩散式大语言模型（dLLM）的推理，通过两阶段细粒度选择策略提升推理效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）由于依赖双向注意力，无法直接使用标准KV缓存，导致推理效率低下。

Method: 设计了Dual aDaptive Cache（d²Cache），采用两阶段细粒度选择策略，在每一步解码中识别关键token并自适应更新其KV状态，其余token的KV状态被缓存复用，支持准从左到右生成，减少序列末尾token的过早过度自信。

Result: 在LLaDA和Dream两个代表性dLLM上的实验表明，d²Cache显著提升了推理速度，并一致改善了生成质量。

Conclusion: d²Cache是一种高效、无需训练的KV缓存方法，有效解决了dLLM推理效率低的问题，同时提升生成性能。

Abstract: Diffusion-based large language models (dLLMs), despite their promising
performance, still suffer from inferior inference efficiency. This is because
dLLMs rely on bidirectional attention and cannot directly benefit from the
standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle
this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a
training-free approximate KV cache framework for accelerating dLLM inference.
d$^2$Cache features a two-stage fine-grained selection strategy to identify
tokens and adaptively update their KV states at each decoding step, while
caching the KV states of the remaining tokens for reuse. Furthermore,
d$^2$Cache naturally offers a more reliable decoding alternative, which can
enable quasi left-to-right generation and mitigate premature overconfidence in
tokens at the end of the sequence. Extensive experimental results on two
representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not
only achieves substantial inference speedups, but also yields consistent
improvements in generation quality. The code is available at
https://github.com/Kamichanw/d2Cache.

</details>


### [393] [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
*Wen Tao,Jing Tang,Alvin Chan,Bryan Hooi,Baolong Bi,Nanyun Peng,Yuansheng Liu,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出SmiSelf框架，通过将无效SMILES转换为SELFIES实现100%有效的分子生成，解决了大语言模型在少样本下生成有效分子的难题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在少样本设置下使用SMILES表示生成分子时常产生无效结构，限制了其在药物发现等领域的应用。

Method: 提出SmiSelf框架，利用语法规则将无效SMILES转换为SELFIES表示，借助SELFIES的语法完备性纠正无效分子结构。

Result: 实验表明SmiSelf能保证生成分子100%有效，保持分子特性，并在其他指标上维持或提升性能。

Conclusion: SmiSelf有效解决了LLMs在分子生成中的有效性问题，拓展了其在生物医学领域的应用，且兼容所有基于SMILES的生成模型。

Abstract: Molecule generation is key to drug discovery and materials science, enabling
the design of novel compounds with specific properties. Large language models
(LLMs) can learn to perform a wide range of tasks from just a few examples.
However, generating valid molecules using representations like SMILES is
challenging for LLMs in few-shot settings. In this work, we explore how LLMs
can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a
representation where every string corresponds to a valid molecule, for valid
molecule generation but find that LLMs perform worse with SELFIES than with
SMILES. We then examine LLMs' ability to correct invalid SMILES and find their
capacity limited. Finally, we introduce SmiSelf, a cross-chemical language
framework for invalid SMILES correction. SmiSelf converts invalid SMILES to
SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the
invalid SMILES. Experiments show that SmiSelf ensures 100% validity while
preserving molecular characteristics and maintaining or even enhancing
performance on other metrics. SmiSelf helps expand LLMs' practical applications
in biomedicine and is compatible with all SMILES-based generative models. Code
is available at https://github.com/wentao228/SmiSelf.

</details>


### [394] [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
*Jeonghoon Shim,Woojung Song,Cheyon Jin,Seungwon KooK,Yohan Jo*

Main category: cs.CL

TL;DR: A non-collaborative user simulation method for tool agents.


<details>
  <summary>Details</summary>
Motivation: Existing user simulators are overly cooperative and do not reflect real-world user behaviors, making it difficult to train and evaluate tool agents under challenging conditions.

Method: Proposes a novel user simulator architecture that models four types of non-collaborative behaviors: requesting unavailable services, digressing into tangential conversations, expressing impatience, and providing incomplete utterances.

Result: Experiments on MultiWOZ and τ-bench show significant performance degradation in state-of-the-art tool agents when facing non-collaborative users, with increased hallucinations and dialogue breakdowns.

Conclusion: The proposed framework provides a more realistic and extensible simulation environment to develop and diagnose tool agents under real-world, non-collaborative user interactions.

Abstract: Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon
Shim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:
25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY
4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue
Simulation TL;DR: A non-collaborative user simulation method for tool agent.
Abstract: Tool agents interact with users through multi-turn dialogues to
accomplish various tasks. Recent studies have adopted user simulation methods
to develop these agents in multi-turn settings. However, existing user
simulators tend to be agent-friendly, exhibiting only cooperative behaviors,
which fails to train and test agents against non-collaborative users in the
real world. To address this, we propose a novel user simulator architecture
that simulates four categories of non-collaborative behaviors: requesting
unavailable services, digressing into tangential conversations, expressing
impatience, and providing incomplete utterances. Our user simulator can
simulate challenging and natural non-collaborative behaviors while reliably
delivering all intents and information necessary to accomplish the task. Our
experiments on MultiWOZ and $\tau$-bench reveal significant performance
degradation in state-of-the-art tool agents when encountering non-collaborative
users. We provide detailed analyses of agents' weaknesses under each
non-collaborative condition, such as escalated hallucinations and dialogue
breakdowns. Ultimately, we contribute an easily extensible user simulation
framework to help the research community develop tool agents and preemptively
diagnose them under challenging real-world conditions within their own
services.

</details>


### [395] [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
*Song Jin,Juntian Zhang,Yong Liu,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 提出TagPR框架，通过标记思维方法提升大语言模型的个性化推理能力，在公开和自建数据集上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用推理方面表现出色，但在个性化推理（如分析用户历史、推断偏好）方面存在不足，需要增强其对用户特定逻辑的理解与响应能力。

Method: 设计一种基于标签化思维的训练框架TagPR：首先构建数据驱动流水线生成并语义标注推理链，形成结构化可解释数据集；然后采用结合监督微调和多阶段强化学习的协同训练策略，其中强化学习使用基于标签约束和新型个性化奖励模型（PRMU）的复合奖励信号。

Result: 在LaMP基准和自建数据集上实验表明，相比基础模型，TagPR平均性能提升32.65%，达到当前最优水平。

Conclusion: 结构化的、可解释的推理是实现大语言模型真实个性化能力的有效路径，TagPR为提升模型个性化提供了新方向。

Abstract: Recent advancements have endowed Large Language Models (LLMs) with impressive
general reasoning capabilities, yet they often struggle with personalization
reasoning - the crucial ability to analyze user history, infer unique
preferences, and generate tailored responses. To address this limitation, we
introduce TagPR, a novel training framework that significantly enhances an
LLM's intrinsic capacity for personalization reasoning through a tagging the
thought approach. Our method first develops a data-driven pipeline to
automatically generate and semantically label reasoning chains, creating a
structured dataset that fosters interpretable reasoning. We then propose a
synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on
this tagged data to establish foundational reasoning patterns, followed by a
multi-stage reinforcement learning (RL) process. This RL phase is guided by a
unique composite reward signal, which integrates tag-based constraints and a
novel Personalization Reward Model with User Embeddings (PRMU) to achieve
fine-grained alignment with user-specific logic. Extensive experiments on the
public LaMP benchmark and a self-constructed dataset demonstrate that our
approach achieves state-of-the-art results, delivering an average improvement
of 32.65% over the base model across all tasks. Our work validates that
structured, interpretable reasoning is a highly effective pathway to unlocking
genuine personalization capabilities in LLMs.

</details>


### [396] [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
*Zichao Yu,Ming Li,Wenyi Zhang,Weiguo Gao*

Main category: cs.CL

TL;DR: TReASURe是一种针对掩码扩散语言模型的树搜索测试时对齐方法，通过UnmaskBranch和ResubstituteScore策略解决并行去掩码相关性强和奖励评估方差高的问题，在低NFE条件下表现尤为出色。


<details>
  <summary>Details</summary>
Motivation: 将树搜索应用于掩码扩散语言模型时面临分支高度相关和奖励估计方差大的挑战，影响探索和剪枝稳定性，因此需要更高效的对齐方法。

Method: 提出TReASURe，包括基于首次命中去掩码的UnmaskBranch策略以单次模型调用实现多样化分支，以及使用确定性重代入的ResubstituteScore规则进行低方差评分。

Result: 理论分析表明该方法在NFE效率、评分误差界和宽树性能上均有提升；实验显示其在困惑度、语言可接受性、情感与毒性控制等任务上优于现有方法，尤其在低计算预算下效果显著。

Conclusion: TReASURe有效提升了掩码扩散语言模型在树搜索中的探索效率与评分稳定性，是当前最先进的测试时对齐方法之一。

Abstract: Tree search has recently emerged as a powerful framework for aligning
generative models with task-specific rewards at test time. Applying tree search
to Masked Diffusion Language Models, however, introduces two key challenges:
(i) parallel unmasking yields highly correlated branches, limiting exploration,
and (ii) reward evaluation via sampled completions produces high-variance
estimates, making pruning unstable. We propose TReASURe, a tree-search
test-time alignment method that addresses these issues. It introduces (i)
UnmaskBranch, a branching strategy based on first-hitting unmasking that
diversifies both token content and reveal order with a single model call per
parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic
resubstitution to score partially masked sequences with low-variance proxy
completions. Theoretically, we quantify branching efficiency gains in NFEs
(number of function evaluations), show that the scoring rule approximates the
true reward with error bounded by predictive uncertainty, and prove
improvements with larger tree widths. Empirically, TReASURe achieves
state-of-the-art results on perplexity, linguistic acceptability, and control
of sentiment and toxicity, outperforming prior methods under matched compute
budgets, with especially strong gains in low-NFE regimes.

</details>


### [397] [Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs](https://arxiv.org/abs/2509.23166)
*Chenxing Wei,Hong Wang,Ying He,Fei Yu,Yao Shu*

Main category: cs.CL

TL;DR: 提出了一种新的多轮交互测试时策略自适应范式T2PAM，并引入轻量级算法ROSA，利用用户反馈实时调整模型策略，实现高效对话内自我修正，在多个基准上显著提升了任务效果与效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常在静态单轮数据上训练，难以适应多轮交互中的实时用户反馈，导致复杂任务中性能下降。

Method: 提出T2PAM范式，利用用户反馈作为奖励信号估计潜在最优策略，并通过ROSA算法在单步更新中调整模型少量参数，使其趋近该策略，避免复杂的迭代优化。

Result: 理论分析证明ROSA策略随交互次数增加收敛于用户偏好，实验表明其在多个挑战性基准上显著提升任务完成的效果与效率。

Conclusion: ROSA实现了高效的多轮对话自适应，为大模型在动态交互环境中的持续优化提供了可行方案。

Abstract: Large Language Models (LLMs) employ multi-turn interaction as a fundamental
paradigm for completing complex tasks. However, their performance often
degrades in extended interactions, as they are typically trained on static,
single-turn data, which hinders their ability to adapt to real-time user
feedback. To address this limitation, we first propose a new paradigm:
Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes
user feedback from the ongoing interaction as a reward signal to estimate a
latent optimal policy aligned with user preferences, then updates a small
subset of parameters to steer the model toward this policy, ultimately enabling
efficient in-conversation self-correction. We then introduce Optimum-Referenced
One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.
ROSA guides the model parameters toward a theoretical optimal policy in a
single, efficient update step, avoiding costly iterative gradient-based
optimization and minimizing computational overhead. We provide a rigorous
theoretical analysis guaranteeing that the policy of ROSA converges to the
preference of user as the number of interactions increases. Extensive
experiments on challenging benchmark demonstrate that ROSA achieves significant
improvements in both task effectiveness and efficiency.

</details>


### [398] [Pretraining LLM with Latent Thoughts in Continuous Space](https://arxiv.org/abs/2509.23184)
*Boyi Zeng,He Li,Shixiang Song,Yixuan Wang,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出一种新的预训练方法“潜思考语言模型”，通过在每个token生成前引入潜在的中间思考步骤，提升模型性能，在相同推理成本下优于参数翻倍的标准模型。


<details>
  <summary>Details</summary>
Motivation: 受思维链（CoT）在测试时通过增加推理步数提升性能的启发，探索在预训练阶段是否也能通过增加计算步骤来提升单个token的生成质量。

Method: 在预训练中让语言模型先生成一个中间的潜在思考（即当前位置的最后隐藏状态），然后利用该状态预测下一个实际token，从而在连续空间中细化预测。

Result: 实验表明，生成一个额外潜在思考的1.4B模型在相同数据上显著优于标准2.8B模型；增加每token的潜在思考数量能持续提升性能。

Conclusion: 通过引入潜思考机制，在不增加推理成本的前提下，可显著提升语言模型的表达和下游任务能力，验证了在预训练中扩展计算步骤的有效性。

Abstract: The remarkable success of Chain-of-Thought (CoT), which enhances performance
by scaling generation steps at test-time, inspires us to ask: can we leverage a
similar scaling of computational steps during pretraining to improve the
generation of each individual token? To address this, we propose a novel
pre-training methodology: Pretraining Language Models with Latent Thoughts. Our
approach pretrains a language model (LM) to first generate an intermediate
latent thought-the last hidden state of the current position-which is then used
as input to predict the actual subsequent token. This additional computational
step enables the LM to refine its prediction within unconstrained continuous
space. Our experiments demonstrate that, at an identical inference cost, a LM
that generates one additional latent thought per token outperforms a standard
model with double the parameters. For instance, ours-1.4B (Pythia Arch),
pretrained on 300B tokens from the Pile, significantly surpasses the vanilla
Pythia-2.8B trained on the same data on both language modeling and a range of
general downstream tasks. Furthermore, increasing the number of latent thoughts
generated before each actual token-forming a chain analogous to
CoT-consistently improves the model's performance.

</details>


### [399] [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
*Guancheng Wan,Leixin Sun,Longxu Dou,Zitong Shi,Fang Wu,Eric Hanchen Jiang,Wenke Huang,Guibin Zhang,Hejia Geng,Xiangru Tang,Zhenfei Yin,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型多智能体系统中指令冲突导致的层级合规性问题的全栈式解决方案，包括诊断、定位和对齐三个阶段。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多智能体系统在面对指令冲突时存在系统性失效问题，且宏观指标难以揭示微观违规行为，缺乏可操作的改进指导。

Method: 提出三阶段框架：1）诊断阶段采用上下文感知的角色遵从评分（CRAS）；2）定位阶段通过注意力漂移分析确定关键层；3）对齐阶段使用仅在关键层安装LoRA的SAIL方法，优化基于注意力贡献加权的偏好目标。

Result: 在多个基准和多智能体框架上验证了该方法的有效性，显著提升了指令层级合规性（如AutoGen在MedQA上提升5.60%），且无需全模型微调。

Conclusion: SAIL方法能有效解决多智能体系统中的指令冲突问题，在不进行全模型微调的情况下显著提高系统可靠性。

Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly
advanced collaborative reasoning, tool use, and role-specialized coordination
in complex tasks. However, reliability-critical deployment remains hindered by
a systemic failure mode: hierarchical compliance under instruction conflicts
(system-user, peer-peer), where agents misprioritize system-level rules in the
presence of competing demands. Moreover, widely used macro-level metrics (e.g.,
pass@k) obscure these micro-level violations and offer little actionable
guidance for remedy. In this work, we present a full-stack, three-stage
framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a
query-wise, context-aware scoring metric that decomposes role adherence into
four measurable dimensions; (2) Localize - attention drift analysis revealing
that instruction conflicts are resolved by attention heads that are largely
concentrated in middle layers; (3) Align - Surgical Alignment of Instruction
Layers (SAIL), which installs LoRA only on the localized focal layers and
optimizes a token-weighted DPO-style preference objective that credits tokens
by their focal attentional contribution. Across standard benchmarks and MAS
frameworks, our surgical approach improves instruction hierarchy compliance
(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

</details>


### [400] [Estimating the strength and timing of syntactic structure building in naturalistic reading](https://arxiv.org/abs/2509.23195)
*Nan Wang,Jiaxuan Li*

Main category: cs.CL

TL;DR: 该研究通过EEG和眼动追踪数据，揭示了句子加工中短语结构构建先于词性检测，并主导词汇影响，支持预测性的‘树状支架’理解模型。


<details>
  <summary>Details</summary>
Motivation: 探讨句法处理中的时间顺序问题，特别是短语结构构建与词性检测的先后关系。

Method: 使用ZuCo语料库中的共注册EEG和眼动数据，分析注视转移、贝叶斯网络建模和固定相关电位。

Result: 发现读者在句法中心词间优先移动视线；结构深度是偏离线性阅读的最强驱动因素；句法意外性在词出现前和早期整合阶段影响神经活动。

Conclusion: 短语结构构建可先于词性检测并主导词汇影响，支持预测性‘树状支架’模型。

Abstract: A central question in psycholinguistics is the timing of syntax in sentence
processing. Much of the existing evidence comes from violation paradigms, which
conflate two separable processes - syntactic category detection and phrase
structure construction - and implicitly assume that phrase structure follows
category detection. In this study, we use co-registered EEG and eye-tracking
data from the ZuCo corpus to disentangle these processes and test their
temporal order under naturalistic reading conditions. Analyses of gaze
transitions showed that readers preferentially moved between syntactic heads,
suggesting that phrase structures, rather than serial word order, organize
scanpaths. Bayesian network modeling further revealed that structural depth was
the strongest driver of deviations from linear reading, outweighing lexical
familiarity and surprisal. Finally, fixation-related potentials demonstrated
that syntactic surprisal influences neural activity before word onset (-184 to
-10 ms) and during early integration (48 to 300 ms). These findings extend
current models of syntactic timing by showing that phrase structure
construction can precede category detection and dominate lexical influences,
supporting a predictive "tree-scaffolding" account of comprehension.

</details>


### [401] [From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs](https://arxiv.org/abs/2509.23196)
*Haonan Wang,Weida Liang,Zihang Fu,Nie Zheng,Yifan Zhang,Yao Tong,Tongyao Zhu,Hao Jiang,Chuang Li,Jiaying Wu,Kenji Kawaguchi*

Main category: cs.CL

TL;DR: 本文提出了一种名为Insight-to-Solve（I2S）的新方法，以解决推理大模型在使用少样本思维链时性能下降的问题。通过将示例转化为可重用的洞察，I2S显著提升了多种模型在多个基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 近期基于验证器强化学习训练的推理大模型在使用少样本思维链时表现不如直接回答，存在性能下降的悖论。本文旨在分析该现象并提出有效利用上下文示例的方法。

Method: 通过高质量推理轨迹分析问题根源，提出I2S框架：将示例转化为显式的、可复用的洞察，并生成针对目标问题的推理路径；进一步引入自我 refinement 步骤形成I2S+。

Result: 在多个开源和闭源模型上实验表明，I2S和I2S+ consistently 优于直接回答和测试时扩展基线。GPT-4.1在AIME'25上提升14.0%，o1-mini在AIME和GPQA上分别提升2.7%和1.7%。

Conclusion: 通过洞察提炼与求解框架，可以有效克服语义误导和策略迁移失败问题，充分发挥上下文演示的潜力。

Abstract: Recent reasoning LLMs (RLMs), especially those trained with verifier-based
reinforcement learning, often perform worse with few-shot CoT than with direct
answering. We revisit this paradox using high-quality reasoning traces from
DeepSeek-R1 as demonstrations and find that adding more exemplars consistently
degrades accuracy, even when demonstrations are optimal. A detailed analysis
reveals two mechanisms behind this decline: (i) semantic misguidance, where
high textual similarity leads the model to treat the target as the same as the
exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer
failure, where the model struggles to extract useful reasoning strategies and
apply them to target questions. Guided by these, we introduce Insight-to-Solve
(I2S), a sequential test-time procedure that turns demonstrations into
explicit, reusable insights and derives a target-specific reasoning trace;
optionally, the reasoning is self-refined for coherence and correctness (I2S+).
Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently
outperform both direct answering and test-time scaling baselines across open-
and closed-source models. Even for GPT models, our method helps: on AIME'25,
GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on
GPQA, indicating that in-context demonstrations can be harnessed effectively
via insight-refine-solve framework.

</details>


### [402] [Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts](https://arxiv.org/abs/2509.23197)
*Aditya Narayan Sankaran,Reza Farahbakhsh,Noel Crespi*

Main category: cs.CL

TL;DR: 本研究分析了2017至2025年登上Billboard榜单的K-pop歌曲，发现英语使用和韩英混用（code-switching）在全球化成功中起关键作用，尤其在进入美国市场的Hot 100榜单时更为显著。


<details>
  <summary>Details</summary>
Motivation: 探讨K-pop在全球走红背后的语言策略，特别是韩英双语切换如何反映美学选择与全球市场战略。

Method: 收集2017–2025年登上Billboard Hot 100和Global 200榜单的K-pop歌曲数据集，分析其中英语与韩语歌词比例、代码转换频率，并利用多语言嵌入和人工特征进行性别分类预测。

Result: 英语在国际榜单K-pop歌曲中占主导地位，男女艺人代码转换程度高，女性独唱艺人更持续使用英语；统计上无显著性别差异，但英语使用更多有助于进入Hot 100榜单；基于歌词特征的性别预测F1得分达0.76。

Conclusion: K-pop歌词中的语言选择深受全球市场压力影响，英语使用不仅是传播策略，也反映了表演者身份特征和不同榜单的受众偏好。

Abstract: Code switching, particularly between Korean and English, has become a
defining feature of modern K-pop, reflecting both aesthetic choices and global
market strategies. This paper is a primary investigation into the linguistic
strategies employed in K-pop songs that achieve global chart success, with a
focus on the role of code-switching and English lyric usage. A dataset of K-pop
songs that appeared on the Billboard Hot 100 and Global 200 charts from 2017 to
2025, spanning 14 groups and 8 solo artists, was compiled. Using this dataset,
the proportion of English and Korean lyrics, the frequency of code-switching,
and other stylistic features were analysed. It was found that English dominates
the linguistic landscape of globally charting K-pop songs, with both male and
female performers exhibiting high degrees of code-switching and English usage.
Statistical tests indicated no significant gender-based differences, although
female solo artists tend to favour English more consistently. A classification
task was also performed to predict performer gender from lyrics, achieving
macro F1 scores up to 0.76 using multilingual embeddings and handcrafted
features. Finally, differences between songs charting on the Hot 100 versus the
Global 200 were examined, suggesting that, while there is no significant gender
difference in English, higher English usage may be more critical for success in
the US-focused Hot 100. The findings highlight how linguistic choices in K-pop
lyrics are shaped by global market pressures and reveal stylistic patterns that
reflect performer identity and chart context.

</details>


### [403] [Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2](https://arxiv.org/abs/2509.23204)
*Stefan Arnold,René Gröbner*

Main category: cs.CL

TL;DR: 研究了Gemma-2模型在生成介词短语时对工具性状语和属性修饰语的偏好，并通过调整注意力头的值向量来控制其生成倾向。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型在生成介词短语时如何决定补足语的功能角色（工具性状语或属性修饰语），并探索其内部机制。

Method: 构建包含上下文可支持两种解释的with引导介词短语的提示集，通过将激活投影到词汇空间来识别偏好工具性补语的注意力头，并通过缩放特定注意力头的值向量来干预生成结果。

Result: 发现Gemma-2倾向于生成工具性补语（比例为3:4）；通过调节单个注意力头的值向量，可将工具性补语的比例降至33%，属性修饰语提升至36%。

Conclusion: 特定注意力头在决定介词补语功能角色中起关键作用，且可通过向量缩放进行有效调控。

Abstract: Language Models, when generating prepositional phrases, must often decide for
whether their complements functions as an instrumental adjunct (describing the
verb adverbially) or an attributive modifier (enriching the noun adjectivally),
yet the internal mechanisms that resolve this split decision remain poorly
understood. In this study, we conduct a targeted investigation into Gemma-2 to
uncover and control the generation of prepositional complements. We assemble a
prompt suite containing with-headed prepositional phrases whose contexts
equally accommodate either an instrumental or attributive continuation,
revealing a strong preference for an instrumental reading at a ratio of 3:4. To
pinpoint individual attention heads that favor instrumental over attributive
complements, we project activations into the vocabulary space. By scaling the
value vector of a single attention head, we can shift the distribution of
functional roles of complements, attenuating instruments to 33% while elevating
attributes to 36%.

</details>


### [404] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: 本文提出了PARL-MT框架，通过引入进度感知（progress awareness）来提升大语言模型在多轮函数调用中的表现，结合自动生成数据的PAG流程和基于进度感知的强化学习算法PAG-RL，在两个公开基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮对话方法要么将训练简化为单轮任务，忽略整体规划，要么使用端到端强化学习但存在冗余且缺乏对进度感知的显式建模，因此需要一种能显式融合进度感知的训练框架。

Method: 提出PARL-MT框架，包括Progress Awareness Generation（PAG）管道用于自动构建包含对话摘要与未来任务规划的数据集，以及Progress Awareness-Guided Reinforcement Learning（PAG-RL）算法，在强化学习中引入进度感知以减少上下文冗余并提升局部动作与全局目标的一致性。

Result: 在两个公开的多轮函数调用基准上，PARL-MT显著优于现有方法，验证了其在提升任务连贯性和执行效率方面的有效性。

Conclusion: 显式地建模进度感知对于实现鲁棒高效的多轮函数调用至关重要，PARL-MT为大语言模型在复杂、长周期任务中的应用提供了有效解决方案。

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [405] [A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks](https://arxiv.org/abs/2509.23208)
*Haorui Yu,Ramon Ruiz-Dolz,Qiufeng Yi*

Main category: cs.CL

TL;DR: 本研究开发了一个用于评估主流视觉语言模型（VLMs）在生成中国传统画评方面能力的量化框架，通过提取人类专家评论中的多维评价特征，并定义多种评论者人设进行引导式提示实验，系统评估了Llama、Qwen、Gemini等模型的表现，揭示了其在艺术评论任务中的优势与不足。


<details>
  <summary>Details</summary>
Motivation: 评估当前主流视觉语言模型在复杂语义理解与内容生成任务中对传统中国画进行批评的能力，填补艺术与人工智能交叉领域中自动评论生成的评估空白。

Method: 构建了一个基于人类专家评论的多维度评价特征提取框架，利用零样本分类模型提取评价立场、关注特征和评论质量等维度，定义并量化代表性评论人设，采用人设引导提示法对多个VLM进行测试与评估。

Result: 成功建立了可量化的中国画评论评估框架，发现现有VLM在不同评论视角下表现存在差异，部分模型能生成具有一定深度的艺术评论，但在风格把握、文化语境理解和细粒度艺术特征分析上仍有不足。

Conclusion: 当前VLM在传统中国画评论生成任务中展现出一定潜力，但其在文化深层理解和精细化艺术表达方面仍需改进，该研究为VLM在艺术领域的应用提供了可量化的评估路径与改进方向。

Abstract: This study aims to test and evaluate the capabilities and characteristics of
current mainstream Visual Language Models (VLMs) in generating critiques for
traditional Chinese painting. To achieve this, we first developed a
quantitative framework for Chinese painting critique. This framework was
constructed by extracting multi-dimensional evaluative features covering
evaluative stance, feature focus, and commentary quality from human expert
critiques using a zero-shot classification model. Based on these features,
several representative critic personas were defined and quantified. This
framework was then employed to evaluate selected VLMs such as Llama, Qwen, or
Gemini. The experimental design involved persona-guided prompting to assess the
VLM's ability to generate critiques from diverse perspectives. Our findings
reveal the current performance levels, strengths, and areas for improvement of
VLMs in the domain of art critique, offering insights into their potential and
limitations in complex semantic understanding and content generation tasks. The
code used for our experiments can be publicly accessed at:
https://github.com/yha9806/VULCA-EMNLP2025.

</details>


### [406] [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
*Sina J. Semnani,Jirayu Burapacheep,Arpandeep Khatua,Thanawan Atchariyachanvanit,Zheng Wang,Monica S. Lam*

Main category: cs.CL

TL;DR: 本文提出了CLAIRE，一个结合大语言模型推理与检索的代理系统，用于检测维基百科中的事实不一致性，并通过用户研究和构建WIKICOLLIDE基准验证其有效性，发现至少3.3%的英文维基百科事实存在矛盾。


<details>
  <summary>Details</summary>
Motivation: 维基百科是训练大语言模型和检索增强生成系统的重要资源，但其事实准确性至关重要。然而，目前缺乏对维基百科中不一致性的系统性检测方法，因此需要一种可扩展的方法来识别和纠正这些错误。

Method: 提出CLAIRE系统，采用基于大语言模型的推理与检索机制，在整个语料库层面自动发现潜在的事实不一致，并提供上下文证据辅助人工审核；同时结合人工标注构建WIKICOLLIDE基准，评估自动化系统的性能。

Result: 在用户研究中，87.5%的资深编辑表示使用CLAIRE后更有信心，且在相同时间内发现了64.7%更多的不一致性；随机抽样分析显示至少3.3%的英文维基百科事实相互矛盾，这些不一致性还传播到了7.3%的FEVEROUS和4.0%的AmbigQA样本中；最佳全自动系统的AUROC仅为75.1%，表明仍有改进空间。

Conclusion: 事实矛盾是维基百科中可测量的问题，基于大语言模型的系统如CLAIRE能有效辅助编辑大规模提升知识一致性，具有实际应用价值。

Abstract: Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.

</details>


### [407] [Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin](https://arxiv.org/abs/2509.23259)
*Soumick Sarker,Abhijit Kumar Rai*

Main category: cs.CL

TL;DR: 提出Fin-ExBERT，一个轻量级、模块化的框架，用于从金融对话中提取用户意图相关句子，结合LoRA增强的领域适配BERT和动态阈值策略，在有限标注数据下实现高效准确的信息抽取。


<details>
  <summary>Details</summary>
Motivation: 金融对话文本非正式结构、领域术语多、意图密度不一，传统句子级信息抽取方法难以有效处理。

Method: 基于领域适配的BERT模型，引入LoRA进行低秩适应以支持小样本微调；采用两阶段渐进解冻训练策略和差异化学习率；通过概率曲率（拐点检测）实现动态阈值判定。

Result: 在真实金融对话转录文本上表现出高精确率和F1分数，输出可解释，支持批处理、可视化与校准导出，适用于下游审计与问答任务。

Conclusion: Fin-ExBERT是一种高效、可部署的金融对话信息抽取解决方案，能够在少量标注数据条件下实现精准意图相关句提取。

Abstract: Financial dialogue transcripts pose a unique challenge for sentence-level
information extraction due to their informal structure, domain-specific
vocabulary, and variable intent density. We introduce Fin-ExBERT, a lightweight
and modular framework for extracting user intent-relevant sentences from
annotated financial service calls. Our approach builds on a domain-adapted BERT
(Bidirectional Encoder Representations from Transformers) backbone enhanced
with LoRA (Low-Rank Adaptation) adapters, enabling efficient fine-tuning using
limited labeled data. We propose a two-stage training strategy with progressive
unfreezing: initially training a classifier head while freezing the backbone,
followed by gradual fine-tuning of the entire model with differential learning
rates. To ensure robust extraction under uncertainty, we adopt a dynamic
thresholding strategy based on probability curvature (elbow detection),
avoiding fixed cutoff heuristics. Empirical results show strong precision and
F1 performance on real-world transcripts, with interpretable output suitable
for downstream auditing and question-answering workflows. The full framework
supports batched evaluation, visualization, and calibrated export, offering a
deployable solution for financial dialogue mining.

</details>


### [408] [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
*Wonje Jeung,Sangyeon Yoon,Yoonjun Cho,Dongjae Jeon,Sangwoo Shin,Hyesoo Hong,Albert No*

Main category: cs.CL

TL;DR: A2D是一种针对扩散大语言模型（dLLMs）的任意顺序、任意步骤防御方法，通过在token级别进行对齐，使模型在检测到有害内容时立即输出[EOS]拒绝信号，有效抵御各类预填充攻击，并实现安全生成的实时监控与快速终止。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的任意顺序生成特性扩大了安全攻击面，使得有害内容可能出现在任意位置，且现有基于模板的预填充攻击（如DIJA）可绕过响应级拒绝机制，因此需要更细粒度和鲁棒的防御方法。

Method: 提出A2D方法，在token级别通过随机掩码的方式对dLLMs进行安全对齐训练，使其在任何解码顺序和任意步骤的预填充攻击下，一旦生成有害内容即刻输出[EOS]信号，从而实现细粒度的安全控制。

Result: 在多个安全基准测试中，A2D将DIJA攻击成功率从超过80%降至接近零（LLaDA-8B-Instruct为1.3%，Dream-v0-Instruct-7B为0.0%），并支持基于阈值的早期拒绝，实现最高达19.3倍的更快安全终止。

Conclusion: A2D通过token级别的安全对齐，为扩散大语言模型提供了强大且高效的防御机制，能够应对任意顺序生成和任意步骤预填充攻击，同时支持实时安全监控与快速响应。

Abstract: Diffusion large language models (dLLMs) enable any-order generation, but this
flexibility enlarges the attack surface: harmful spans may appear at arbitrary
positions, and template-based prefilling attacks such as DIJA bypass
response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a
token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal
whenever harmful content arises. By aligning safety directly at the token-level
under randomized masking, A2D achieves robustness to both any-decoding-order
and any-step prefilling attacks under various conditions. It also enables
real-time monitoring: dLLMs may begin a response but automatically terminate if
unsafe continuation emerges. On safety benchmarks, A2D consistently prevents
the generation of harmful outputs, slashing DIJA success rates from over 80% to
near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and
thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x
faster safe termination.

</details>


### [409] [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文提出了Policy Reasoning Traces (PRT)，一种专门生成的推理链，用于提升大语言模型在政策合规性评估中的表现。实验表明，PRT在推理和训练阶段均显著提升了开源和商业模型的性能，并在HIPAA和GDPR政策上达到了新的SOTA水平。此外，PRT还能提高模型准确引用政策条款的能力，并通过利用原始思维链影响合规决策。


<details>
  <summary>Details</summary>
Motivation: 现有的政策合规性评估依赖专家手动进行，成本高昂且难以获取高质量的推理过程记录。因此，需要一种自动化方法来模拟专家的系统性推理，以提升大语言模型在此类任务上的表现。

Method: 提出Policy Reasoning Traces (PRT)，即生成专门的推理链作为桥梁，将政策条款与输入案例的分析过程连接起来。PRT可用于推理时提示和模型微调，从而增强模型对政策合规判断的能力。

Result: 在HIPAA和GDPR政策数据集上的实验显示，使用PRT显著提升了多种开源和商业大模型的准确性，达到新的SOTA水平；同时提高了模型引用政策条款的准确率，并有效利用原始思维链影响合规决策。

Conclusion: PRT是一种有效的方法，能够显著提升大语言模型在政策合规性评估任务中的性能，兼具高准确性和可解释性，具有实际应用价值。

Abstract: Policy compliance assessment is a fundamental task of evaluating whether an
input case strictly complies with a set of human-defined rules, more generally
known as policies. In practice, human experts follow a systematic, step-by-step
process to identify violations with respect to specific stipulations outlined
in the policy. However, such documentation of gold-standard, expert-level
reasoning processes is costly to acquire. In this paper, we introduce Policy
Reasoning Traces (PRT), a form of specialized generated reasoning chains that
serve as a reasoning bridge to improve an LLM's policy compliance assessment
capabilities. Our empirical evaluations demonstrate that the use of PRTs for
both inference-time and training-time scenarios significantly enhances the
performance of open-weight and commercial models, setting a new
state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also
highlight how PRTs can improve an LLM's ability to accurately cite policy
clauses, as well as influence compliance decisions through their high
utilization from the raw chains of thought.

</details>


### [410] [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
*Peng Yu,Zeyuan Zhao,Shao Zhang,Luoyi Fu,Xinbing Wang,Ying Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为SIE的结构化上下文环境框架，通过从大规模结构化数据自动构建推理环境，实现了可扩展性、可泛化推理和可验证性，显著提升了大语言模型在领域内及跨领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的数学和编程环境因依赖专家标注而难以扩展，基于游戏的环境所学技能又难以泛化，因此需要一个兼具可扩展性、可泛化性和可验证性的理想推理环境。

Method: 提出SIE框架，利用大规模结构化数据自动生成推理环境，借助数据中的显式模式和推理链实现规则基础的可验证性，并支持信息受限的部分环境下的探索学习。

Result: 实验表明，SIE不仅在领域内结构化推理任务上表现优异，还能将学到的组合推理能力有效迁移到跨领域的数学和逻辑推理任务中；即使在信息不全的环境下，LLM也能通过探索推断缺失信息，提升鲁棒性和泛化性能。

Conclusion: SIE框架为大语言模型提供了一个高效、可扩展且可泛化的强化学习推理环境，推动了模型在复杂推理任务上的广泛应用潜力。

Abstract: Large language models (LLMs) have achieved significant advancements in
reasoning capabilities through reinforcement learning (RL) via environmental
exploration. As the intrinsic properties of the environment determine the
abilities that LLMs can learn, the environment plays a important role in the RL
finetuning process. An ideal LLM reasoning environment should possess three
core characteristics: scalability, generalizable reasoning, and verifiability.
However, existing mathematical and coding environments are difficult to scale
due to heavy reliance on expert annotation, while the skills learned in
game-based environments are too specialized to generalize. To bridge this gap,
we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment
(SIE) framework. SIE achieves scalability by automatically constructing
reasoning environments from large-scale structured data, where the rich
compositional patterns naturally support generalizable reasoning. Moreover, the
explicit schemas and reasoning chains in structured data provide a foundation
for rule-based verifiability. Experimental results show that SIE framework not
only achieves substantial improvements in in-domain structured reasoning, but
also enables the learned compositional reasoning skills to generalize
effectively to out-of-domain mathematical and logical reasoning tasks. We
further explored learning in information-limited partial SIEs and found that
LLMs can infer the missing information through exploring the environment,
leading to robust reasoning improvements and generalization performance.

</details>


### [411] [C-Evolve: Consensus-based Evolution for Prompt Groups](https://arxiv.org/abs/2509.23331)
*Tiancheng Li,Yuhang Wang,Zhiyang Chen,Zijun Wang,Liyuan Ma,Guo-jun Qi*

Main category: cs.CL

TL;DR: 本文提出了Consensus-Evolve（C-Evolve），一种通过多数投票聚合多个提示输出以提升AI系统性能的进化算法，在多种任务上实现了最先进的表现。


<details>
  <summary>Details</summary>
Motivation: 探索通过聚合多个提示的结果达成共识，是否能进一步提升基于闭源模型的AI系统能力边界。

Method: 采用基于岛屿的进化算法保持种群多样性，并引入投票得分作为个体提示在群体中的适应度评分，优化群体整体性能。

Result: 在HotpotQA和MATH等多个任务上达到最先进水平，例如在Qwen3-8B上HotpotQA准确率达70.67%，IFBench达43.88%；GPT-4.1-mini在IFBench和MATH上分别达到47.96%和95.33%。

Conclusion: C-Evolve通过群体共识机制有效提升了提示进化算法的性能，展现出强大的泛化能力和应用潜力。

Abstract: Prompt evolution algorithms offer a powerful paradigm for enhancing AI
systems based on closed-source models, while few work explores whether
aggregating results from multiple prompts to reach a consensus can further
advance the system capability boundary. In this paper, we introduce
Consensus-Evolve (C-Evolve), an evolutionary algorithm that discovers a group
of prompts whose aggregated outputs after majority voting achieve optimal
performance. More specifically, C-Evolve employs an island-based evolutionary
algorithm to maintain population diversity, and prompts from distinct islands
are selected to form groups to aggregate their outputs. The key difference from
single individual evolution is a voting score, which evaluates each individual
prompt's contribution within groups. We take this as the fitness score for
evolution instead of individual performance. Consequently, C-Evolve is more
likely to produce and maintain prompts with higher potential to form a
high-performing group and eliminate low-performing ones, gradually improving
the group performance after reaching consensus. Our method achieves
state-of-the-art performance across a wide range of tasks, including both
open-ended tasks like HotpotQA and closed-ended tasks like MATH. On Qwen3-8B,
C-Evolve achieves 70.67% on HotpotQA and 43.88% on IFBench, which are 4.95% and
2.73% higher than GEPA, respectively. For GPT-4.1-mini, the accuracy on IFBench
is further improved to 47.96% and reaches 95.33% in the MATH benchmark. These
results demonstrate the C-Evolve's competitive performance.

</details>


### [412] [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
*Han Yan,Zheyuan Liu,Meng Jiang*

Main category: cs.CL

TL;DR: PRISM是一个统一的机器遗忘框架，通过在表示空间和参数空间中实施双空间平滑性，提升模型在遗忘效果、效用保持和隐私保护之间的平衡，并增强对重学习和越狱攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的机器遗忘方法常面临灾难性遗忘和指标不平衡问题，且易受微小扰动引发的重学习和越狱攻击威胁。

Method: PRISM包含两个平滑优化阶段：(i) 在表示空间中使用鲁棒训练的探针防御越狱攻击；(ii) 在参数空间中解耦保留-遗忘梯度冲突，减少不平衡并平滑参数空间以抵御重学习攻击。

Result: 在WMDP和MUSE数据集上的实验表明，PRISM在多种攻击下优于现有最先进基线方法，并在关键指标间实现更好平衡。

Conclusion: PRISM通过双空间平滑优化，有效提升了机器遗忘过程中的鲁棒性和多目标平衡性，为安全、隐私保护的模型更新提供了新思路。

Abstract: With the rapid advancement of large language models, Machine Unlearning has
emerged to address growing concerns around user privacy, copyright
infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning
methods often suffer from catastrophic forgetting and metric imbalance, for
example by over-optimizing one objective (e.g., unlearning effectiveness,
utility preservation, or privacy protection) at the expense of others. In
addition, small perturbations in the representation or parameter space can be
exploited by relearn and jailbreak attacks. To address these challenges, we
propose PRISM, a unified framework that enforces dual-space smoothness in
representation and parameter spaces to improve robustness and balance
unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a
representation space stage that employs a robustly trained probe to defend
against jailbreak attacks, and (ii) a parameter-space stage that decouples
retain-forget gradient conflicts, reduces imbalance, and smooths the parameter
space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,
across conversational-dialogue and continuous-text settings, show that PRISM
outperforms SOTA baselines under multiple attacks while achieving a better
balance among key metrics.

</details>


### [413] [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
*Xinchun Su,Chunxu Luo,Yixuan Li,Weidong Yang,Lipeng Ma*

Main category: cs.CL

TL;DR: 提出了一种名为MedCritical的两阶段框架，利用大模型指导小模型通过自我迭代和直接偏好优化（DPO）提升医学复杂推理能力，在低成本下实现了优于现有7B级小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在医学复杂推理任务中表现不佳，而依赖大模型进行知识蒸馏的方法成本高、效率低，亟需一种更高效且低成本的训练方法。

Method: 第一阶段从大模型提取长链思维模板来引导小模型生成复杂推理；第二阶段通过小模型与自身微调版本的协作进行直接偏好优化（DPO），实现自我迭代纠错。

Result: MedCritical 7B在CMExam基准上比Taiyi和Huatuo-o1-7B分别高出3.04%和10.12%，达到7B级小模型的新SOTA性能。

Conclusion: MedCritical通过自我对弈和DPO策略，有效提升了小模型在医学复杂推理任务中的表现，同时显著降低了对大模型依赖带来的成本和效率问题。

Abstract: In the field of medicine, complex reasoning tasks such as clinical diagnosis,
treatment planning, and medical knowledge integration pose significant
challenges, where small language models often underperform compared to large
language models like GPT-4 and Deepseek. Recent knowledge distillation-based
methods aim to address these issues through teacher-guided error correction,
but this LLM as judge approach remains challenging in terms of cost, time, and
efficiency. To circumvent this issue, we propose a novel two-stage framework,
MedCritical, which uses a small language model fine-tuned by a large teacher
model to play against itself. In the first stage, we extract high-level and
detailed long-chain thought templates from the teacher model to guide the
student model to generate more complex reasoning thoughts. In the second stage,
we introduce direct preference optimization (DPO) through model self-iteration
collaboration to enhance the reasoning ability of the student model by playing
against the correction trajectory of the fine-tuned model during training. This
model self-learning DPO approach teaches the student model to use its own
error-driven insights to consolidate its skills and knowledge to solve complex
problems, and achieves comparable results to traditional knowledge distillation
methods using teacher models at a lower cost. Notably, our MedCritical 7B model
outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\%
respectively on the CMExam benchmark, achieving new SOTA performance among
7B-class small models.

</details>


### [414] [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
*Junming Yang,Ning Xu,Biao Liu,Shiqi Qiao,Xin Geng*

Main category: cs.CL

TL;DR: 提出MetaAPO框架，通过元学习动态平衡在线和离线偏好数据，提升大模型对齐效果并降低42%标注成本。


<details>
  <summary>Details</summary>
Motivation: 解决离线偏好数据与模型策略之间的分布不匹配问题，现有方法难以适应模型的动态学习状态。

Method: 设计轻量级元学习器作为对齐差距估计器，动态耦合数据生成与模型训练，为在线采样分配样本级元权重。

Result: 在AlpacaEval 2、Arena-Hard和MT-Bench上均优于现有方法，并减少42%的在线标注成本。

Conclusion: MetaAPO能有效桥接离线与在线数据的分布差距，实现更高效、自适应的偏好优化。

Abstract: Preference optimization is crucial for aligning large language models (LLMs)
with human values and intentions. A significant challenge in this process is
the distribution mismatch between pre-collected offline preference data and the
evolving model policy. Existing methods attempt to reduce this gap using static
heuristics or decoupled online sampling strategies, but they often fail to
adapt to the model's dynamic learning state. To bridge this gap, we propose
Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework
that dynamically couples data generation with model training. MetaAPO employs a
lightweight meta-learner, as an "alignment gap estimator", to evaluate the
potential benefits of on-policy sampling in relation to offline data. This
guides targeted online generation and assigns sample-wise meta-weights to the
optimization objective, dynamically balancing the quality and distribution of
online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench
demonstrate that MetaAPO consistently outperforms existing preference
optimization approaches across various settings, while reducing 42% in online
annotation costs.

</details>


### [415] [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
*Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho*

Main category: cs.CL

TL;DR: 提出了一种无需训练和检索的推理框架CCD，通过结合任务特定的放射学专家模型中的结构化临床信号，减少多模态大语言模型在放射学报告生成中的医学幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在放射学中存在生成缺乏临床支持描述（即医学幻觉）的问题，尤其对临床文本部分过度敏感，影响输出的准确性和图像依据性。

Method: 提出临床对比解码（CCD），采用双阶段对比机制，在生成过程中细化token级logits，整合来自专家模型的结构化临床信号，无需训练或检索，不修改基础MLLM。

Result: 在三个数据集和多个模型上实验表明，CCD显著提升放射学报告生成性能；在MIMIC-CXR数据集上，应用于最先进模型时RadGraph-F1最高提升17%。

Conclusion: CCD是一种轻量、通用的解决方案，有效缓解医学幻觉问题，成功连接专家模型与MLLMs，提升临床保真度。

Abstract: Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.

</details>


### [416] [Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT](https://arxiv.org/abs/2509.23381)
*Wonhyuk Lee,Youngchol Kim,Yunjin Park,Junhyung Moon,Dongyoung Jeong,Wanjin Park*

Main category: cs.CL

TL;DR: 本文提出了Guard Vector方法，通过预训练语言模型与防护模型的参数差异生成安全任务向量，并将其组合到目标模型中形成目标防护模型（TGM），在无需额外训练或标签的情况下提升安全性、支持多语言并具备跨模型可移植性。


<details>
  <summary>Details</summary>
Motivation: 为了提升语言模型的安全性，同时实现多语言扩展和模型可移植性，减少对额外训练数据和计算资源的依赖。

Method: 提出Guard Vector，即防护模型与同架构预训练模型的参数差；将该向量叠加至目标模型得到TGM；采用基于前缀的监督微调和单token输出分类器进行流式适配。

Result: TGM在标准安全测试集上优于现有防护模型，支持中日韩语言扩展，无需额外训练或标签；在Llama和Gemma上验证了可移植性；前缀SFT保持流式场景下的分类性能，单token输出降低延迟、提高吞吐。

Conclusion: Guard Vector提供了一种高效、可扩展、低资源消耗的模型安全增强方案，推动了流式场景下负责任AI的实践。

Abstract: We introduce Guard Vector, a safety task vector computed as the parameter
difference between a guardrail model (Guard Model) and a same-architecture
pretrained language model. Composing this vector with a target language model
yields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware
approach that combines prefix-based training and evaluation with a classifier
that produces a single-token output. With this composition alone, TGM improves
classification quality over established Guard Models across standard safety
suites and enables language extensibility to Chinese, Japanese, and Korean,
requiring neither additional training nor target language labels. It also
demonstrates model portability across two widely used public guardrail
backbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM
preserves classification quality under streaming by aligning the behavior
between prefix inputs and full-text inputs. The single-token output design
increases throughput and reduces latency. Together, these components reduce
data and compute requirements while promoting streaming-aware evaluation
practices, thereby contributing to a more responsible AI ecosystem.

</details>


### [417] [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
*Sebastian Bordt,Martin Pawelczyk*

Main category: cs.CL

TL;DR: 提出在单次训练中同时进行多个预训练实验的方法，以低成本研究大语言模型的特性，验证了其在数据污染、中毒、记忆化等方面的有效性，并探索了知识获取、数学推理和水印等新方向。


<details>
  <summary>Details</summary>
Motivation: 由于预训练计算成本高昂，限制了对大语言模型学习机制的系统研究，因此需要一种更高效的实验方法。

Method: 在单次训练过程中并行执行多个预训练实验，使用一个15亿参数模型在2100亿token上进行训练，动态调整数据以测试知识获取等能力，并检测实验间的相互影响。

Result: 成功复现了多个已有研究结果，开展了多项新探究，发现各实验对模型整体训练影响极小，且实验间交互作用可忽略。

Conclusion: 单次训练中并行开展多个预训练实验是可行且高效的，能在有限算力下实现对大模型的严谨科学实验。

Abstract: Recent work has demonstrated that controlled pretraining experiments are a
powerful tool for understanding learning, reasoning, and memorization in large
language models (LLMs). However, the computational cost of pretraining presents
a significant constraint. To overcome this constraint, we propose to conduct
multiple pretraining experiments simultaneously during a single training run.
We demonstrate the feasibility of this approach by conducting ten experiments
during the training of a 1.5B parameter model on 210B tokens. Although we only
train a single model, we can replicate the results from multiple previous works
on data contamination, poisoning, and memorization. We also conduct novel
investigations into knowledge acquisition, mathematical reasoning, and
watermarking. For example, we dynamically update the training data until the
model acquires a particular piece of knowledge. Remarkably, the influence of
the ten experiments on the model's training dynamics and overall performance is
minimal. However, interactions between different experiments may act as a
potential confounder in our approach. We propose to test for interactions with
continual pretraining experiments, finding them to be negligible in our setup.
Overall, our findings suggest that performing multiple pretraining experiments
in a single training run can enable rigorous scientific experimentation with
large models on a compute budget.

</details>


### [418] [No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization](https://arxiv.org/abs/2509.23387)
*Wenhang Shi,Yiren Chen,Shuqing Bian,Xinyi Zhang,Kai Tang,Pengfei Hu,Zhe Zhao,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: GRACE是一种高效的自动提示优化框架，通过门控精炼和自适应压缩策略克服了现有方法易陷入局部最优、效率低的问题，在11项任务上显著提升了性能，且仅需以往方法25%的提示生成预算。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示优化方法在生成有效提示时稳定性差、效率低，且容易陷入局部最优，难以充分发挥大语言模型的潜力。

Method: 提出GRACE框架，结合门控精炼（引入反馈调节门和更新拒绝门）和自适应压缩（在优化停滞时提取提示核心概念并重构优化路径）两种策略，通过有控制的信息损失实现高效稳定的提示优化。

Result: 在BBH、领域特定和通用NLP共11项任务上，GRACE相比最先进方法平均相对性能提升分别为4.7%、4.4%和2.7%，且仅使用25%的提示生成预算即达到更优效果。

Conclusion: GRACE通过协同的精炼与压缩机制，显著提高了提示优化的效率和性能，具备低计算开销和强实用性的优势，为自动提示优化提供了有效解决方案。

Abstract: Prompt engineering is crucial for leveraging the full potential of large
language models (LLMs). While automatic prompt optimization offers a scalable
alternative to costly manual design, generating effective prompts remains
challenging. Existing methods often struggle to stably generate improved
prompts, leading to low efficiency, and overlook that prompt optimization
easily gets trapped in local optima. Addressing this, we propose GRACE, a
framework that integrates two synergistic strategies: Gated Refinement and
Adaptive Compression, achieving Efficient prompt optimization. The gated
refinement strategy introduces a feedback regulation gate and an update
rejection gate, which refine update signals to produce stable and effective
prompt improvements. When optimization stagnates, the adaptive compression
strategy distills the prompt's core concepts, restructuring the optimization
trace and opening new paths. By strategically introducing information loss
through refinement and compression, GRACE delivers substantial gains in
performance and efficiency. In extensive experiments on 11 tasks across three
practical domains, including BIG-Bench Hard (BBH), domain-specific, and general
NLP tasks, GRACE achieves significant average relative performance improvements
of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further
analysis shows that GRACE achieves these gains using only 25% of the prompt
generation budget required by prior methods, highlighting its high optimization
efficiency and low computational overhead. Our code is available at
https://github.com/Eric8932/GRACE.

</details>


### [419] [Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation](https://arxiv.org/abs/2509.23395)
*Sherrie Shen,Weixuan Wang,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文提出了机器翻译中的副文本显化任务，旨在通过脚注和尾注来更好地传达文化特定术语的含义。研究基于经典中文短篇小说集《聊斋》的四个英译本构建了包含560个专家对齐副文本的数据集，并评估了大语言模型在该任务上的表现。实验表明，尽管生成的副文本能提升读者理解，但仍不及专业译者所写的有效。研究还发现专业译者在使用副文本时差异显著，说明文化传播本质上是开放而非规范性的。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译在处理文化特定术语时面临挑战，尤其是缺乏对专业译者常用的副文本（如脚注、尾注）的有效利用。现有方法局限于文内显化，忽视了副文本在意义传递中的作用。因此，需要引入副文本显化机制以提升翻译的文化忠实度与可理解性。

Method: 基于Genette（1987）的副文本理论，正式定义了机器翻译中的副文本显化任务；构建了一个包含560个专家对齐副文本的数据集，源自《聊斋》的四个英文译本；采用内在提示和代理检索两种方法，在有无推理轨迹的情况下评估大语言模型在副文本选择与内容生成上的表现。

Result: 实验证明副文本显化任务对当前模型具有挑战性；人类评估显示LLM生成的副文本能够提升目标读者的理解，但效果仍明显低于专业译者所撰写的副文本；统计分析揭示即使专业译者之间在副文本使用上也存在广泛差异。

Conclusion: 副文本显化为超越语言等效性的机器翻译提供了新路径，有助于更忠实地传递文化嵌入的意义；研究支持将副文本纳入MT系统，未来可拓展至单语解释与个性化适配场景。

Abstract: The faithful transfer of contextually-embedded meaning continues to challenge
contemporary machine translation (MT), particularly in the rendering of
culture-bound terms--expressions or concepts rooted in specific languages or
cultures, resisting direct linguistic transfer. Existing computational
approaches to explicitating these terms have focused exclusively on in-text
solutions, overlooking paratextual apparatus in the footnotes and endnotes
employed by professional translators. In this paper, we formalize Genette's
(1987) theory of paratexts from literary and translation studies to introduce
the task of paratextual explicitation for MT. We construct a dataset of 560
expert-aligned paratexts from four English translations of the classical
Chinese short story collection Liaozhai and evaluate LLMs with and without
reasoning traces on choice and content of explicitation. Experiments across
intrinsic prompting and agentic retrieval methods establish the difficulty of
this task, with human evaluation showing that LLM-generated paratexts improve
audience comprehension, though remain considerably less effective than
translator-authored ones. Beyond model performance, statistical analysis
reveals that even professional translators vary widely in their use of
paratexts, suggesting that cultural mediation is inherently open-ended rather
than prescriptive. Our findings demonstrate the potential of paratextual
explicitation in advancing MT beyond linguistic equivalence, with promising
extensions to monolingual explanation and personalized adaptation.

</details>


### [420] [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
*Haowei Hua,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 本研究探讨了人类评分者与大语言模型（LLM）在自动评分中的评分理由，分析其一致性与差异，利用大规模测试的作文数据，评估GPT-4o、Gemini等模型的评分准确性和理由相似性，并通过聚类分析揭示推理模式。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在自动评分中的应用增多，理解其评分理由与人类评分者的异同成为提升评分系统透明性与可靠性的关键。

Method: 采用二次加权Kappa和归一化互信息评估评分准确性，使用余弦相似度比较评分理由的相似性，并基于理由嵌入进行主成分分析和聚类分析。

Result: 发现LLM评分与人类评分具有一定一致性，但在理由表达和推理模式上存在差异，聚类分析揭示了不同评分者群体的思维模式差异。

Conclusion: LLM在自动评分中展现出良好的潜力，但其推理过程与人类仍有区别，未来需进一步优化以提升可解释性与一致性。

Abstract: Advances in automated scoring are closely aligned with advances in
machine-learning and natural-language-processing techniques. With recent
progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,
and other generative-AI chatbots for automated scoring has been explored. Given
their strong reasoning capabilities, LLMs can also produce rationales to
support the scores they assign. Thus, evaluating the rationales provided by
both human and LLM raters can help improve the understanding of the reasoning
that each type of rater applies when assigning a score. This study investigates
the rationales of human and LLM raters to identify potential causes of scoring
inconsistency. Using essays from a large-scale test, the scoring accuracy of
GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa
and normalized mutual information. Cosine similarity is used to evaluate the
similarity of the rationales provided. In addition, clustering patterns in
rationales are explored using principal component analysis based on the
embeddings of the rationales. The findings of this study provide insights into
the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve
the understanding of the rationales behind both human scoring and LLM-based
automated scoring.

</details>


### [421] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码策略Retrieval-Constrained Decoding (RCD)，通过限制语言模型输出的表面形式来更准确地评估其蕴含的事实知识，发现现有方法因过于严格的评估标准低估了模型的知识量。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然包含大量事实知识，但因其生成的答案表面形式多样而常被误判为错误，导致对其参数化知识的低估。作者希望改进评估方式以更真实反映模型的知识水平。

Method: 提出Retrieval-Constrained Decoding (RCD) 解码策略，限制模型输出为唯一的表面形式，并构建包含19,137个常识问题的数据集YAGO-QA进行评估。

Result: 在开源语言模型（135M到70B参数）上的实验表明，RCD显著提升了F1分数。例如，Llama-3.1-70B的F1从32.3%提升至46.0%，而Llama-3.1-8B使用RCD后达到33.0%，超过大模型在标准解码下的表现。

Conclusion: 标准解码方式低估了语言模型的参数知识，RCD能更准确地衡量模型的真实知识水平，且小模型在适当解码策略下可超越大模型的表现。

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [422] [Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models](https://arxiv.org/abs/2509.23441)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为Cognition-of-Thought (CooT)的新型解码时框架，通过显式的认知自监控循环来增强大语言模型的安全性和对齐性，使对齐过程动态、可审计且无需重新训练即可更新策略。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法通常将安全性隐式地嵌入模型权重中，导致控制静态且难以修改，而CooT旨在实现动态、显式且可干预的对齐机制。

Method: CooT结合了一个文本生成器和一个认知感知器，后者基于分层原则持续监控生成序列，在检测到违规时回滚并结合通用社会先验和上下文警告进行重生成。

Result: 在多个基准和模型家族上的实验表明，CooT显著提升了安全性和社会推理性能。

Conclusion: CooT将对齐从固定的模型属性转变为推理过程中可灵活调整的动态过程，为安全可控生成提供了新范式。

Abstract: Large language models (LLMs) excel at complex reasoning but can still exhibit
harmful behaviors. Current alignment strategies typically embed safety into
model weights, making these controls implicit, static, and difficult to modify.
This paper introduces Cognition-of-Thought (CooT), a novel decoding-time
framework that equips LLMs with an explicit cognitive self-monitoring loop.
CooT couples a standard text Generator with a cognitive Perceiver that
continuously monitors the unfolding sequence. The Perceiver uses a structured,
precedence-based hierarchy of principles (e.g., safety over obedience) to
detect potential misalignments as they arise. When violations are flagged, CooT
intervenes by rolling back the generation to the point of error and
regenerating under injected guidance that combines universal social priors with
context-specific warnings. CooT thus transforms alignment from a fixed property
into an explicit, dynamic, and auditable process active during inference,
allowing for flexible policy updates without retraining the model. Extensive
experiments across multiple benchmarks and model families confirm that CooT
consistently improves safety and social reasoning performance.

</details>


### [423] [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
*Sydney Peters,Nan Zhang,Hong Jiao,Ming Li,Tianyi Zhou,Robert Lissitz*

Main category: cs.CL

TL;DR: 本文综述了37项基于文本的自动化题目难度预测研究，比较了传统机器学习与基于Transformer的语言模型在大规模测评中的表现，结果显示语言模型无需手工特征即可有效捕捉语言特征，预测精度高，具有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统题目难度标定方法依赖于试测和经典测量理论或项目反应理论，耗时且成本高，亟需更高效、低成本的自动化预测方法。

Method: 系统综述了截至2025年5月发表的37篇相关研究，提取并分析每项研究的数据集、难度指标、学科领域、题型、样本量、数据划分、输入特征、模型类型、评估标准及性能结果。

Result: 经典机器学习模型因可解释性强仍具价值，但先进的语言模型（尤其是基于Transformer的小规模和大规模架构）无需人工特征工程即可捕捉句法和语义模式；整体上，文本方法可实现RMSE低至0.165、皮尔逊相关系数高达0.87、准确率达0.806。

Conclusion: 基于文本的自动化题目难度预测方法具有较高准确性与应用潜力，未来研究可进一步优化模型可解释性、跨学科泛化能力，并推动其在实际测评中的集成应用。

Abstract: Item difficulty plays a crucial role in test performance, interpretability of
scores, and equity for all test-takers, especially in large-scale assessments.
Traditional approaches to item difficulty modeling rely on field testing and
classical test theory (CTT)-based item analysis or item response theory (IRT)
calibration, which can be time-consuming and costly. To overcome these
challenges, text-based approaches leveraging machine learning and language
models, have emerged as promising alternatives. This paper reviews and
synthesizes 37 articles on automated item difficulty prediction in large-scale
assessment settings published through May 2025. For each study, we delineate
the dataset, difficulty parameter, subject domain, item type, number of items,
training and test data split, input, features, model, evaluation criteria, and
model performance outcomes. Results showed that although classic machine
learning models remain relevant due to their interpretability, state-of-the-art
language models, using both small and large transformer-based architectures,
can capture syntactic and semantic patterns without the need for manual feature
engineering. Uniquely, model performance outcomes were summarized to serve as a
benchmark for future research and overall, text-based methods have the
potential to predict item difficulty with root mean square error (RMSE) as low
as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.
The review concludes by discussing implications for practice and outlining
future research directions for automated item difficulty modeling.

</details>


### [424] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: 本研究探讨了在零样本和少样本学习场景中，基于角色的提示设计对大语言模型（如GPT-3.5、GPT-4o、Llama2-7b和Llama2-13b）性能的影响，发现在情感分析、文本分类、问答和数学推理任务中，合理的角色配置可显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管提示工程已被广泛研究，但提示中角色设计的影响仍缺乏探索。本文旨在填补这一空白，系统评估不同角色配置对大语言模型在多种任务中表现的影响。

Method: 在零样本和少样本设置下，对GPT系列和Llama2系列模型采用不同角色提示结构，评估其在多个标准数据集上的表现，涵盖情感分析、文本分类、问答和数学推理任务。

Result: 实验结果表明，合理设计的角色提示能有效提升大语言模型的预测性能，尤其是在零样本场景下效果更为显著。

Conclusion: 角色配置是提示工程中的一个重要因素，基于角色的提示设计有助于释放大语言模型在多样化任务中的潜力。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [425] [AraS2P: Arabic Speech-to-Phonemes System](https://arxiv.org/abs/2509.23504)
*Bassam Matar,Mohamed Fayed,Ayman Khalafallah*

Main category: cs.CL

TL;DR: 本文提出了AraS2P系统，采用两阶段训练策略基于Wav2Vec2-BERT实现阿拉伯语语音到音素转换，在Iqra'Eval 2025共享任务中排名第一。


<details>
  <summary>Details</summary>
Motivation: 为了提升阿拉伯语语音中音素级发音错误检测的性能，探索预训练模型在特定任务上的适应性改进方法。

Method: 采用两阶段训练：第一阶段在大规模阿拉伯语音-音素数据集上进行任务自适应持续预训练（数据由MSA Phonetiser生成）；第二阶段在官方共享任务数据上微调，并引入XTTS-v2合成的多样化诵读样本进行数据增强，包括不同经文片段、说话人嵌入和文本扰动。

Result: 该系统在官方排行榜上排名第一，验证了音素感知预训练与针对性数据增强结合的有效性。

Conclusion: 音素感知的预训练策略结合精细化数据增强能显著提升语音到音素系统的性能，尤其适用于发音错误检测任务。

Abstract: This paper describes AraS2P, our speech-to-phonemes system submitted to the
Iqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training
strategy. In the first stage, task-adaptive continue pretraining was performed
on large-scale Arabic speech-phonemes datasets, which were generated by
converting the Arabic text using the MSA Phonetiser. In the second stage, the
model was fine-tuned on the official shared task data, with additional
augmentation from XTTS-v2-synthesized recitations featuring varied Ayat
segments, speaker embeddings, and textual perturbations to simulate possible
human errors. The system ranked first on the official leaderboard,
demonstrating that phoneme-aware pretraining combined with targeted
augmentation yields strong performance in phoneme-level mispronunciation
detection.

</details>


### [426] [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
*Dania Refai,Alaa Dalaq,Doaa Dalaq,Irfan Ahmad*

Main category: cs.CL

TL;DR: 本文提出了一种用于阿拉伯语情感分析的主动学习框架，利用大语言模型（LLM）辅助标注，显著降低了标注成本，同时在多个数据集上达到了与人工标注相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语情感分析因缺乏大规模高质量标注数据而发展受限，且主动学习和大语言模型在该语言中的应用研究较少。

Method: 设计了一个基于主动学习的框架，比较了LSTM、GRU和RNN等多种深度学习模型，并在Hunger Station、AJGT和MASAC三个基准数据集上评估了五种大语言模型（GPT-4o、Claude 3 Sonnet、Gemini 2.5 Pro、DeepSeek Chat、LLaMA 3 70B Instruct）作为标注者的有效性。

Result: LLM辅助的主动学习在多个数据集上表现优异：在Hunger Station上，使用GPT-4o标注的450个样本，LSTM达到93%准确率；在MASAC上，DeepSeek Chat用650个样本达到82%准确率，与人工标注相当。

Conclusion: LLM辅助的主动学习能有效减少阿拉伯语情感分析的标注成本，同时保持高性能，具有实际应用潜力。

Abstract: Natural language processing (NLP), particularly sentiment analysis, plays a
vital role in areas like marketing, customer service, and social media
monitoring by providing insights into user opinions and emotions. However,
progress in Arabic sentiment analysis remains limited due to the lack of large,
high-quality labeled datasets. While active learning has proven effective in
reducing annotation efforts in other languages, few studies have explored it in
Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for
assisting annotation and comparing their performance to human labeling is still
largely unexplored in the Arabic context. In this paper, we propose an active
learning framework for Arabic sentiment analysis designed to reduce annotation
costs while maintaining high performance. We evaluate multiple deep learning
architectures: Specifically, long short-term memory (LSTM), gated recurrent
units (GRU), and recurrent neural networks (RNN), across three benchmark
datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard
Arabic and dialectal variations. Additionally, two annotation strategies are
compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as
annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3
70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for
Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our
results show that LLM-assisted active learning achieves competitive or superior
performance compared to human labeling. For example, on the Hunger Station
dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples
using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat
reached 82% accuracy with 650 labeled samples, matching the accuracy obtained
through human labeling.

</details>


### [427] [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
*Janvijay Singh,Austin Xu,Yilun Zhou,Yefan Zhou,Dilek Hakkani-Tur,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文研究了在数学领域中，针对生成模型响应的微调评判模型在未来适应性、向后兼容性和问题泛化能力方面的表现，发现当前模型在面对未见问题时性能下降，且未来适应性具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的微调评判模型在实际部署中忽略了未来模型或过去模型响应的变化以及对新问题的泛化能力，需要系统评估其“保质期”问题。

Method: 在统一框架下，通过改变训练和测试分布，结合三种基于SFT和DPO的微调算法及三种不同基础模型进行实验。

Result: 未来适应性具有挑战性，向后兼容性相对容易，DPO训练模型表现更优；持续学习能更好平衡新旧响应分布变化；在未见问题上所有模型均出现性能下降。

Conclusion: 当前微调评判模型在面对不断演进的生成模型时存在局限性，需考虑持续学习和更强的泛化能力以提升实用性和部署寿命。

Abstract: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model
responses and reward modeling for model alignment and finetuning. Recently,
finetuning judges with judge-specific data has emerged as an often preferred
choice over directly prompting frontier models as judges, as the former
achieves better performance with smaller model sizes while being more robust to
common biases. However, the standard evaluation ignores several practical
concerns of finetuned judges regarding their real world deployment. In this
paper, we identify and formalize three aspects that affect the shelf life of
these judges: future proofing and backward compatibility -- how well judges
finetuned on responses by today's generator models perform on responses by
future models or past models, as well as question generalization -- how well
judges generalize to unseen questions at test time. We study these three
aspects in the math domain under a unified framework with varying train and
test distributions, three SFT- and DPO-based finetuning algorithms and three
different base models. Experiments suggest that future-proofing is challenging
for most models, while backward compatibility is relatively easy, with
DPO-trained models consistently improving performance. We further find that
continual learning provides a more balanced adaptation to shifts between older
and newer response distributions than training solely on stronger or weaker
responses. Moreover, all models observe certain degrees of performance
degradation when moving from questions seen during training to unseen ones,
showing that current judges do not fully generalize to unseen questions. These
findings provide insights into practical considerations for developing and
deploying judge models in the face of ever-changing generators.

</details>


### [428] [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
*Vardis Georgilas,Themos Stafylakis*

Main category: cs.CL

TL;DR: 本文提出了一种针对希腊语医学语音转录的领域特定系统，结合自动语音识别与文本校正模型，提升医疗文档自动化水平。


<details>
  <summary>Details</summary>
Motivation: 为了减轻医疗专业人员的手动文档负担并提高工作效率，需要开发适用于希腊语医学领域的语音转录系统。

Method: 结合自动语音识别技术和文本校正模型，通过对现有语言和语音技术进行领域特定的微调，以应对希腊语中复杂的医学术语和语言变异。

Result: 该系统在希腊语医学语音转录任务中实现了更准确、连贯的输出，有效处理了领域术语和语言不一致性问题。

Conclusion: 所提出的系统有助于推动希腊医疗领域实用语言技术的发展，具有实际应用价值。

Abstract: Medical dictation systems are essential tools in modern healthcare, enabling
accurate and efficient conversion of speech into written medical documentation.
The main objective of this paper is to create a domain-specific system for
Greek medical speech transcriptions. The ultimate goal is to assist healthcare
professionals by reducing the overload of manual documentation and improving
workflow efficiency. Towards this goal, we develop a system that combines
automatic speech recognition techniques with text correction model, allowing
better handling of domain-specific terminology and linguistic variations in
Greek. Our approach leverages both acoustic and textual modeling to create more
realistic and reliable transcriptions. We focused on adapting existing language
and speech technologies to the Greek medical context, addressing challenges
such as complex medical terminology and linguistic inconsistencies. Through
domain-specific fine-tuning, our system achieves more accurate and coherent
transcriptions, contributing to the development of practical language
technologies for the Greek healthcare sector.

</details>


### [429] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 本文提出了一种面向模型的推理选择蒸馏方法（MoRSD），通过筛选高质量的推理链来提升小语言模型的推理能力，并引入“推理难度”指标评估学生模型在给定推理下的表现，在减少使用推理数量的同时实现了平均4.6%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有思维链蒸馏方法过于关注数据量而忽视推理质量，可能导致错误或噪声信息传递给学生模型。

Method: 提出MoRSD框架，结合推理准确性、多样性和难度进行高质量推理选择；设计推理难度（RD）指标评估学生模型生成正确答案的能力。

Result: 在三个任务的七个数据集上比基线平均提升4.6%，且使用的推理数量更少。

Conclusion: 少量高质量的推理链比大量低质量推理更能有效提升小模型的推理能力，MoRSD为高效的思维链蒸馏提供了可行方案。

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [430] [Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks](https://arxiv.org/abs/2509.23579)
*Kevin Frank,Anmol Gulati,Elias Lumer,Sindy Campagna,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出了Jackal，一个大规模的自然语言到JQL查询的基准数据集，包含10万对自然语言请求与验证过的JQL查询，并在真实Jira实例上执行。研究评估了23个大模型在该数据集子集Jackal-5K上的表现，发现最佳模型（Gemini 2.5 Pro）平均执行准确率仅为60.3%，揭示了当前大模型在企业级JQL生成任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开的、基于真实场景和执行结果的自然语言到JQL查询的基准测试，难以评估模型在企业环境中生成正确可执行JQL的能力。

Method: 构建名为Jackal的大规模文本到JQL数据集，包含10万对自然语言请求与经执行验证的JQL查询，涵盖四种用户请求类型，并发布配套评分工具和Jira实例快照；在Jackal-5K子集上评估23个大语言模型的执行准确率、精确匹配率等指标。

Result: 在Jackal-5K上，Gemini 2.5 Pro表现最佳，平均执行准确率为60.3%，但在不同请求类型中差异显著：长自然语言（86.0%）、短自然语言（35.7%）、语义相似（22.7%）、语义完全一致（99.3%）。

Conclusion: 当前最先进的大语言模型在生成可执行JQL查询方面仍存在明显不足，尤其在处理简短或语义相近请求时表现较差，Jackal为未来研究提供了新的基于执行的挑战和评估标准。

Abstract: Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter
issues from Jira. Yet, to our knowledge, there is no open, real-world,
execution-based benchmark for mapping natural language queries to JQL. We
introduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000
natural language (NL) requests paired with validated JQL queries and
execution-based results on a live Jira instance with over 200,000 issues. To
reflect real-world usage, each JQL query is associated with four types of user
requests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)
Semantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,
together with an execution-based scoring toolkit, and a static snapshot of the
evaluated Jira instance for reproducibility. We report text-to-JQL results on
23 Large Language Models (LLMs) spanning parameter sizes, open and closed
source models, across execution accuracy, exact match, and canonical exact
match. In this paper, we report results on Jackal-5K, a 5,000-pair subset of
Jackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only
60.3% execution accuracy averaged equally across four user request types.
Performance varies significantly across user request types: (i) Long NL
(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)
Semantically Exact (99.3%). By benchmarking LLMs on their ability to produce
correct and executable JQL queries, Jackal exposes the limitations of current
state-of-the-art LLMs and sets a new, execution-based challenge for future
research in Jira enterprise data.

</details>


### [431] [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
*JinXin Li,Gang Tu,JunJie Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于隐藏层时间信号频域分析的幻觉检测方法HSAD，通过将大语言模型的推理过程建模为随时间展开的认知旅程，并利用快速傅里叶变换提取频谱特征，有效识别生成内容中的幻觉，克服了现有方法在知识覆盖和推理偏差检测上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法受限于知识覆盖面或难以捕捉推理过程中的偏差，因此需要一种能够动态分析模型推理过程并有效识别幻觉的新方法。

Method: 将LLM的推理过程视为认知旅程，利用隐藏层的时间信号进行建模，应用快速傅里叶变换（FFT）将其映射到频域以构建频谱特征，并基于这些特征设计幻觉检测算法。

Result: 实验表明，频谱特征能有效捕获推理过程中的异常，HSAD方法在幻觉检测上具有更高的准确性和鲁棒性。

Conclusion: HSAD通过结合推理过程建模与频域特征提取，显著提升了幻觉检测性能，为大语言模型在关键场景中的可靠部署提供了新思路。

Abstract: Although Large Language Models have demonstrated powerful capabilities in a
wide range of tasks such as language understanding and code generation, the
frequent occurrence of hallucinations during the generation process has become
a significant impediment to their deployment in critical application scenarios.
Current mainstream hallucination detection methods rely on factual consistency
verification or static hidden layer features. The former is constrained by the
scope of knowledge coverage, while the latter struggles to capture reasoning
biases during the inference process. To address these issues, and inspired by
signal analysis methods in cognitive neuroscience, this paper proposes a
hallucination detection method based on the frequency-domain analysis of hidden
layer temporal signals, named HSAD (\textbf{H}idden \textbf{S}ignal
\textbf{A}nalysis-based \textbf{D}etection). First, by treating the LLM's
reasoning process as a cognitive journey that unfolds over time, we propose
modeling and simulating the human process of signal perception and
discrimination in a deception-detection scenario through hidden layer temporal
signals. Next, The Fast Fourier Transform is applied to map these temporal
signals into the frequency domain to construct spectral features, which are
used to capture anomalies that arise during the reasoning process; analysis
experiments on these spectral features have proven the effectiveness of this
approach. Finally, a hallucination detection algorithm is designed based on
these spectral features to identify hallucinations in the generated content. By
effectively combining the modeling of the reasoning process with
frequency-domain feature extraction, the HSAD method overcomes the limitations
of existing approaches in terms of knowledge coverage and the detection of
reasoning biases, demonstrating higher detection accuracy and robustness.

</details>


### [432] [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Zenan Xu,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的方法Timber，通过细微调整权重增量来增强Instruct模型的探索能力，同时保持其利用能力。


<details>
  <summary>Details</summary>
Motivation: 后训练被认为只是表面性的，尽管提升了模型的利用能力，但限制了探索能力。本文旨在解决这一权衡问题。

Method: 通过从权重层面分析有效秩（eRank）的变化，提出Timber方法，部分回滚Instruct模型向Base模型的转变，进行有针对性的权重微调。

Result: 在Llama和Qwen系列模型上的实验证明，Timber能持续提升Instruct模型性能，尤其在Pass@k指标上表现更优。

Conclusion: Timber为后训练阶段提供了新的权重层面见解，并提出了无需训练即可优化Instruct模型的实用策略。

Abstract: Post-training, which elicits a pretrained Base model into the corresponding
Instruct model, is widely considered to be superficial. In this work, we first
reinforce this hypothesis by providing novel quantitative evidence from the
weight level that the effective rank (eRank) remains negligibly changed.
However, this superficiality also suffers a critical trade-off, improving the
exploitation capabilities at the cost of limiting its exploration. To tackle
this issue, we propose Timber, a simple yet effective training-free method that
enhances the exploration capability of the Instruct model while preserving its
exploitation. The key insight is to partially revert Instruct towards the
paired Base model by subtle yet targeted refinement of the weight deltas.
Extensive experiments on Llama and Qwen series demonstrate that Timber
consistently improves vanilla Instruct models, particularly on Pass@k
performance. Our findings offer new insights into the post-training stage at
the weight level and practical strategies to refine the Instruct model without
training.

</details>


### [433] [Fast Thinking for Large Language Models](https://arxiv.org/abs/2509.23633)
*Haoyu Zheng,Zhuonan Wang,Yuqian Yuan,Tianwei Lin,Wenqiao Zhang,Zheqi Lv,Juncheng Li,Siliang Tang,Yueting Zhuang,Hongyang He*

Main category: cs.CL

TL;DR: 提出了一种名为Latent Codebooks for Fast Thinking的框架，通过训练时使用简洁的思维链草图学习离散策略先验，在推理时利用少量连续思考向量进行策略级引导，无需生成显式推理标记，并结合GainRouter机制自适应切换快速与慢速推理模式，显著降低推理成本的同时保持竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于显式逐步生成标记的推理方法效率低下，依赖大规模监督微调或强化学习，且长推理轨迹导致延迟和令牌消耗增加。希望实现更高效、可控的推理过程。

Method: 引入Latent Codebooks框架，在训练中使用简洁的CoT草图学习离散策略先验；在推理时通过单次传递从codebook蒸馏出的少量连续思考向量进行条件生成；并设计GainRouter轻量路由机制，自适应选择快速或慢速推理路径。

Result: 在多个推理基准测试上，该方法在显著降低推理成本（如减少token生成）的同时，达到了具有竞争力或更优的准确率，并能有效抑制过度思考。

Conclusion: 该方法为大型语言模型提供了高效且可控的推理路径，平衡了推理质量与计算开销。

Abstract: Reasoning-oriented Large Language Models (LLMs) often rely on generating
explicit tokens step by step, and their effectiveness typically hinges on
large-scale supervised fine-tuning or reinforcement learning. While
Chain-of-Thought (CoT) techniques substantially enhance performance on complex
reasoning tasks, they remain inefficient, requiring long reasoning traces that
increase latency and token usage. In this work, we introduce Latent Codebooks
for Fast Thinking, a framework that uses concise CoT sketches only during
training to learn a codebook of discrete strategy priors. At inference, the
model conditions on a handful of continuous thinking vectors distilled from the
codebook in a single pass, enabling strategy-level guidance without producing
explicit reasoning tokens. To complement this design, we propose GainRouter, a
lightweight routing mechanism that adaptively switches between fast codebook
guided inference and slow explicit reasoning, thereby suppressing overthinking
and reducing unnecessary token generation. Experiments across multiple
reasoning benchmarks show that our approach achieves competitive or superior
accuracy while substantially lowering inference cost, offering a practical path
toward efficient and controllable reasoning in large language models.

</details>


### [434] [Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models](https://arxiv.org/abs/2509.23653)
*Zemin Huang,Yuhang Wang,Zhiyang Chen,Guo-Jun Qi*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于掩码的扩散语言模型RemeDi，通过引入“再掩码”机制，结合置信度评分和强化学习，实现了更灵活的文本生成与修正，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散语言模型在生成文本后难以修正错误，缺乏识别输入中潜在错误的能力，限制了文本生成的灵活性和质量。

Method: 提出RemeDi模型，联合预测每个token的分布及其置信度分数，利用置信度决定哪些token需要被重新掩码，并在后续步骤中基于更丰富的上下文重新采样；采用监督微调和强化学习相结合的方式训练模型。

Result: 实验表明，RemeDi在多个数据集上达到了开源扩散语言模型中的最先进水平，显著优于现有方法。

Conclusion: RemeDi通过引入再掩码机制和置信度驱动的动态修正，有效提升了扩散语言模型的文本优化能力，为生成式文本建模提供了新思路。

Abstract: Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect
tokens: once a token is generated, it typically remains fixed. The key
challenge is to identify potential errors in the inputs. In this paper, we
propose \emph{\underline{Rem}asking-\underline{e}nabled \underline{Di}ffusion
Language Model (RemeDi}, a mask-based DLM that introduces \emph{remasking} as
another fundamental mechanism, enabling more flexible text refinement in
diffusion-based text generation. To achieve this, RemeDi jointly predicts token
distributions and per-token confidence scores at each step. The confidence
scores determine which tokens to be unmasked after the current step, allowing
the model to identify tokens with low quality and remask them. These remasked
tokens can be resampled with richer context in subsequent steps. We design a
remask-aware pipeline to train this ability, including supervised fine-tuning
which teaches the model to detect and remask incorrect tokens in addition to
predict mask tokens, and reinforcement learning which optimizes full generation
trajectories toward higher rewards. Experiments show that RemeDi achieves the
state-of-the-art results among open-source DLMs on multiple datasets.

</details>


### [435] [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
*Shulin Huang,Yiran Ding,Junshu Pan,Yue Zhang*

Main category: cs.CL

TL;DR: 本研究首次系统探讨了强化学习（RL）与监督微调（SFT）在多语言复杂推理中的跨语言泛化能力，发现RL不仅准确率更高，且在非英语数据上训练时表现出更强的跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升大模型复杂推理能力方面表现优异，但其在跨语言泛化方面的潜力尚未被探索，而SFT在此方面的局限性亟需对比研究。

Method: 以Qwen2.5-3B-Base为基座模型，在数学、常识和科学推理等多个多语言基准上，比较RL与SFT在不同语言数据上的训练效果，并进行机制分析。

Result: 1. RL相比SFT具有更高的准确率和更强的跨语言泛化能力；2. 在非英语数据上进行RL训练比英语数据效果更好，这一现象在SFT中未观察到；3. 机制分析表明RL能赋予模型更鲁棒的推理策略。

Conclusion: 强化学习在多语言复杂推理任务中优于监督微调，尤其在非英语数据上训练时展现出更优的整体性能和泛化能力，为构建公平高效的多语言推理系统提供了重要指导。

Abstract: Enhancing the complex reasoning capabilities of Large Language Models (LLMs)
attracts widespread attention. While reinforcement learning (RL) has shown
superior performance for improving complex reasoning, its impact on
cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains
unexplored. We present the first systematic investigation into cross-lingual
reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation
model, we conduct experiments on diverse multilingual reasoning benchmarks,
including math reasoning, commonsense reasoning, and scientific reasoning. Our
investigation yields two significant findings: (1) Tuning with RL not only
achieves higher accuracy but also demonstrates substantially stronger
cross-lingual generalization capabilities compared to SFT. (2) RL training on
non-English data yields better overall performance and generalization than
training on English data, which is not observed with SFT. Furthermore, through
comprehensive mechanistic analyses, we explore the underlying factors of RL's
superiority and generalization across languages. Our results provide compelling
evidence that RL enables the model with more robust reasoning strategies,
offering crucial guidance for more equitable and effective multilingual
reasoning.

</details>


### [436] [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
*Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 提出一种批量对齐策略，通过在训练批次中使用语义等价的多语言数据来微调大语言模型，显著提升非英语语言的准确性（最高达23.9%），同时保持英语性能和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源与中低资源语言之间存在显著性能差距，导致全球企业应用中的客户体验和操作可靠性下降。

Method: 采用批处理级别的对齐策略，在每个训练批次中利用语义等价的多语言数据进行微调，使模型在不同语言间输出对齐。

Result: 非英语语言的准确率提升高达23.9%，且未牺牲英语性能、模型推理或检索质量。

Conclusion: 该方法简单、可扩展，并能无缝集成到现有LLM训练与部署流程中，有助于实现更稳健和公平的多语言AI解决方案。

Abstract: Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.

</details>


### [437] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: 本文提出了TF-Bench，一个基于System F类型推断的代码语义推理基准，并通过去除非语义相关的自然语言构建了纯语义版本TF-Bench_pure，揭示了当前大模型在真正程序语义推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准缺乏形式化的程序中心演绎框架，无法判断模型是真正理解程序语义还是仅依赖表面的语言与代码关联。

Method: 设计基于System F类型推断的TF-Bench基准，采用验证过的变换去除自然语言干扰，构建纯语义的TF-Bench_pure，并提出两个新指标评估模型鲁棒性和测试时推理能力。

Result: 最先进的LLM（Claude-3.7-sonnet）在TF-Bench_pure上仅达到55.85%的准确率，显示出当前模型在语义推理上的显著不足。

Conclusion: 当前大语言模型在程序语义推理方面存在根本性局限，需更严格的评估框架和更强的推理能力提升方法。

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [438] [VIVA+: Human-Centered Situational Decision-Making](https://arxiv.org/abs/2509.23698)
*Zhe Hu,Yixiao Ren,Guanzhong Liu,Jing Li,Yu Yin*

Main category: cs.CL

TL;DR: 本文提出了VIVA+，一个基于认知科学的基准，用于评估多模态大语言模型在人类中心情境中的推理与决策能力，涵盖情境理解、行动合理性和反思性推理三个方面，并通过实验揭示了现有模型的局限性及改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在复杂人类环境中表现出潜力，但其在细微且类人的推理与决策能力方面缺乏有效评估手段，因此需要一个更系统、认知基础更强的评测基准。

Method: 构建包含1,317个真实场景和6,373个选择题的VIVA+基准，聚焦三大核心能力：基础情境理解、上下文驱动的行为解释和反思性推理；并在最新商业与开源模型上进行评估，探索针对性训练与多步推理策略。

Result: 实验揭示了不同模型在各项能力上的表现模式，表明当前模型仍存在明显不足；引入针对性训练和多步推理可带来持续性能提升。

Conclusion: VIVA+为评估MLLM的社交化决策提供了系统框架，揭示了当前模型的局限性，并为未来提升模型的情境感知与社会适应能力提供了可行路径。

Abstract: Multimodal Large Language Models (MLLMs) show promising results for embodied
agents in operating meaningfully in complex, human-centered environments. Yet,
evaluating their capacity for nuanced, human-like reasoning and decision-making
remains challenging. In this work, we introduce VIVA+, a cognitively grounded
benchmark for evaluating the reasoning and decision-making of MLLMs in
human-centered situations. VIVA+ consists of 1,317 real-world situations paired
with 6,373 multiple-choice questions, targeting three core abilities for
decision-making: (1) Foundational Situation Comprehension, (2) Context-Driven
Action Justification, and (3) Reflective Reasoning. Together, these dimensions
provide a systematic framework for assessing a model's ability to perceive,
reason, and act in socially meaningful ways. We evaluate the latest commercial
and open-source models on VIVA+, where we reveal distinct performance patterns
and highlight significant challenges. We further explore targeted training and
multi-step reasoning strategies, which yield consistent performance
improvements. Finally, our in-depth analysis highlights current model
limitations and provides actionable insights for advancing MLLMs toward more
robust, context-aware, and socially adept decision-making in real-world
settings.

</details>


### [439] [Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion](https://arxiv.org/abs/2509.23714)
*Zhiqiang Liu,Yichi Zhang,Mengshu Sun,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的多模态知识图谱补全方法M-Hyper，结合融合和独立模态表示，利用双四元数建模模态间交互，在性能、鲁棒性和计算效率方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多模态知识图谱补全时，要么因固定融合策略丢失模态特有信息，要么难以捕捉模态间的上下文依赖语义交互。

Method: 提出M-Hyper模型，引入细粒度实体表示分解（FERF）和鲁棒关系感知模态融合（R2MF）模块，并利用双四元数的四个正交基表示多个独立与融合模态，通过哈密顿积建模模态间交互。

Result: 实验表明M-Hyper在多个指标上达到最先进水平，具有良好的鲁棒性和计算效率。

Conclusion: M-Hyper有效实现了融合与独立模态表示的共存与协作，提升了多模态知识图谱补全的效果。

Abstract: Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts
in multi-modal knowledge graphs (MMKGs) by leveraging both structural
relationships and diverse modality information of entities. Existing MMKGC
methods follow two multi-modal paradigms: fusion-based and ensemble-based.
Fusion-based methods employ fixed fusion strategies, which inevitably leads to
the loss of modality-specific information and a lack of flexibility to adapt to
varying modality relevance across contexts. In contrast, ensemble-based methods
retain modality independence through dedicated sub-models but struggle to
capture the nuanced, context-dependent semantic interplay between modalities.
To overcome these dual limitations, we propose a novel MMKGC method M-Hyper,
which achieves the coexistence and collaboration of fused and independent
modality representations. Our method integrates the strengths of both
paradigms, enabling effective cross-modal interactions while maintaining
modality-specific information. Inspired by ``quaternion'' algebra, we utilize
its four orthogonal bases to represent multiple independent modalities and
employ the Hamilton product to efficiently model pair-wise interactions among
them. Specifically, we introduce a Fine-grained Entity Representation
Factorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)
module to obtain robust representations for three independent modalities and
one fused modality. The resulting four modality representations are then mapped
to the four orthogonal bases of a biquaternion (a hypercomplex extension of
quaternion) for comprehensive modality interaction. Extensive experiments
indicate its state-of-the-art performance, robustness, and computational
efficiency.

</details>


### [440] [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
*Eduard Barbu,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本文评估了大语言模型在罗马尼亚驾驶法规问答中的表现，发布了一个包含1208个问题的数据集，并比较了纯文本与多模态系统的性能，发现经过领域微调的小规模模型具有竞争力，且文本描述优于直接视觉输入，同时揭示了LLM作为评判者时存在自偏好偏差。


<details>
  <summary>Details</summary>
Motivation: 确保驾驶员掌握交通规则对道路安全至关重要，但针对资源较少语言（如罗马尼亚语）的可解释问答研究不足，需评估大语言模型在此类法律领域的适用性与解释能力。

Method: 构建了一个包含1208个问题（387个多模态）的罗马尼亚驾驶法规数据集，比较了纯文本和多模态SOTA系统的表现，对Llama 3.1-8B-Instruct和RoLlama 3.1-8B-Instruct进行领域微调，并使用LLM-as-a-Judge评估生成解释的质量。

Result: 最先进的模型表现良好，但经过领域微调的8B模型表现同样具有竞争力；使用图像的文本描述比直接视觉输入效果更好；LLM作为评判者时表现出自我偏好偏差。

Conclusion: 该研究表明，在资源较少的语言中，经过适当微调的小规模模型可在法律问答任务中达到理想效果，且文本化处理多模态信息更有效，但需注意LLM在评估自身输出时的偏见问题。

Abstract: Ensuring that both new and experienced drivers master current traffic rules
is critical to road safety. This paper evaluates Large Language Models (LLMs)
on Romanian driving-law QA with explanation generation. We release a
1{,}208-question dataset (387 multimodal) and compare text-only and multimodal
SOTA systems, then measure the impact of domain-specific fine-tuning for Llama
3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but
fine-tuned 8B models are competitive. Textual descriptions of images outperform
direct visual input. Finally, an LLM-as-a-Judge assesses explanation quality,
revealing self-preference bias. The study informs explainable QA for
less-resourced languages.

</details>


### [441] [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
*Yucheng Wang,Yifan Hou,Aydin Javadov,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出了一种基于逻辑的评估框架，用于分析多模态大语言模型中的跨模态推理，识别出任务组合瓶颈和融合瓶颈两个核心问题，并指出模态集成而非感知能力是当前多模态推理的主要障碍。


<details>
  <summary>Details</summary>
Motivation: 现有研究对多模态是否提升推理能力存在矛盾结论，缺乏系统性评估框架和内部机制分析，因此需要一种可控方法来揭示不同模态交互如何影响推理性能。

Method: 构建一个将多模态推理分为六种交互模式的逻辑驱动评估框架，通过实证分析模态间事实分布与逻辑组合方式的影响，并结合注意力模式、模态可分离性等内部特征进行诊断。

Result: 发现额外模态仅在提供独立且充分的推理路径时提升性能；识别出三种系统性退化模式：弱模态拖累整体、模态冲突导致偏好偏倚、跨模态信号整合失败；揭示了任务组合瓶颈和融合瓶颈两类核心失败机制。

Conclusion: 多模态推理的主要瓶颈在于信息集成过程而非单模态感知能力，建议未来研究关注组合感知训练和早期融合控制策略。

Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by
integrating diverse inputs such as text, vision, and audio. Yet cross-modal
reasoning remains underexplored, with conflicting reports on whether added
modalities help or harm performance. These inconsistencies stem from a lack of
controlled evaluation frameworks and analysis of models' internals to isolate
when and why modality interactions support or undermine reasoning. We address
this gap through a logic-grounded evaluation framework that categorizes
multimodal reasoning into six interaction patterns, varying how facts are
distributed across modalities and logically combined. Empirically, additional
modalities enhance reasoning only when they provide independent and sufficient
reasoning paths, while redundant or chained entailment support often hurts
performance. Moreover, reasoning degrades in three systematic ways: weaker
modalities drag down overall performance, conflicts bias preference toward
certain modalities, and joint signals from different modalities fail to be
integrated effectively. Therefore, we identify two core failures:
task-composition bottleneck, where recognition and reasoning cannot be jointly
executed in one pass, and fusion bottleneck, where early integration introduces
bias. For further investigation, we find that attention patterns fail to encode
fact usefulness, but a simple two-step prompting (recognize then reason)
restores performance, confirming the task-composition bottleneck. Moreover,
modality identity remains recoverable in early layers, and softening attention
in early fusion improves reasoning, highlighting biased fusion as another
failure mode. Overall, our findings show that integration, not perception, is
the main barrier to multimodal reasoning, suggesting composition-aware training
and early fusion control as promising directions.

</details>


### [442] [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
*Chao Wang,Rui-Chen Zheng,Yang Ai,Zhen-Hua Ling*

Main category: cs.CL

TL;DR: 本文研究了语音集成到大语言模型中导致文本能力下降的问题，提出通过参数重要性估计分析该问题，并采用分层学习率调度和低秩适应（LoRA）来缓解，实验表明这些方法能更好保持文本能力并提升口语问答性能。


<details>
  <summary>Details</summary>
Motivation: 语音增强的大语言模型在获得语音能力的同时常削弱其原有的文本理解能力，限制了模型对预训练文本知识的充分利用，因此需要探究并解决这一退化问题。

Method: 通过分析编码器-适配器范式，提出基于参数重要性估计的分析框架，揭示语音微调导致文本关键参数分布的层间偏移，并探索分层学习率调度和LoRA等策略以保留原始参数结构。

Result: 实验表明，与全量微调相比，分层学习率调度和LoRA能更有效地保持模型的文本能力，同时在下游口语问答任务中表现更优。

Conclusion: 文本知识在LLM中具有特定的结构属性，保护其参数重要性分布是维持多模态模型综合性能的关键，所提出的两种策略为此提供了有效解决方案。

Abstract: The integration of speech into Large Language Models (LLMs) has substantially
expanded their capabilities, but often at the cost of weakening their core
textual competence. This degradation limits the ability of speech-enabled LLMs
to fully exploit their pre-trained text-based knowledge. In this work, we
analyze the underlying mechanisms of this issue through a focused study of the
widely used encoder-adaptor paradigm. We propose an analytical framework based
on parameter importance estimation, which reveals that fine-tuning for speech
introduces a textual importance distribution shift: the layer-wise allocation
of parameters critical to textual reasoning is disrupted. Building on this
insight, we investigate two mitigation strategies: layer-wise learning rate
scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original
parameter distribution. Experimental results show that both approaches better
maintain textual competence than full fine-tuning, while also improving
downstream spoken question answering performance. Furthermore, our analysis
offers a principled explanation for the effectiveness of the proposed
mitigation strategies, linking their benefits to the structural properties of
textual knowledge in LLMs.

</details>


### [443] [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
*Junliang Li,Yucheng Wang,Yan Chen,Yu Ran,Ruiqing Zhang,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出了一种新的知识级一致性强化学习框架（KLCF），通过双事实对齐机制提升大语言模型在长文本生成中的事实性和一致性，无需外部知识，具有高效和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人类反馈的强化学习方法在处理长文本生成时，忽视了模型内部的知识边界，导致幻觉问题严重，影响模型可靠性。

Method: 利用预训练知识边界构建事实清单，指导在线强化学习以提高事实覆盖率和召回率；同时训练基于基础模型内部知识的自评估模块，增强生成过程中的事实精确度。

Result: 实验结果表明，KLCF在多个长文本基准上显著提升了事实性指标，并有效缓解了模型幻觉问题。

Conclusion: KLCF是一种高效、轻量且无需外部知识的框架，能够显著提升大语言模型在长文本生成中的事实性和可靠性。

Abstract: Hallucination and factuality deficits remain key obstacles to the reliability
of large language models (LLMs) in long-form generation. Existing reinforcement
learning from human feedback (RLHF) frameworks primarily rely on preference
rewards, yet they often overlook the model's internal knowledge boundaries,
exacerbating the so-called "hallucination tax". To address this challenge, we
propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a
novel framework that focuses on the knowledge consistency between the policy
model's expressed knowledge and the base model's parametric knowledge, and
introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall
and precision. Specifically, KLCF leverages pretrained knowledge boundaries to
construct fact checklist, guiding online reinforcement learning to improve
factual coverage and recall; simultaneously, it trains a self-assessment module
based on the base model's internal knowledge to enhance factual precision
during generation. Unlike prior methods that rely on external retrieval or
heavy verification, our reward design is fully external-knowledge-free and
lightweight, making KLCF efficient and easily scalable to large-scale training.
Experimental results demonstrate that KLCF substantially improves factuality
metrics across multiple long-form benchmarks and effectively alleviates model
hallucinations.

</details>


### [444] [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
*Zehong Wang,Junlin Wu,ZHaoxuan Tan,Bolian Li,Xianrui Zhong,Zheli Liu,Qingkai Zeng*

Main category: cs.CL

TL;DR: 提出了一种结合局部个性化记忆和全局集体记忆的本地-全局记忆框架（LoGo），并通过调解模块解决两者之间的冲突，有效缓解了大语言模型个性化中的冷启动和偏置问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型个性化方法在用户历史数据不足（冷启动）或数据偏差（偏置）时效果受限，且缺乏对用户间共性知识的建模能力。

Method: 设计了一个本地-全局记忆框架（LoGo），其中本地记忆捕捉个体行为，全局记忆建模群体共享兴趣，并引入一个调解模块来协调两种记忆信号之间的冲突。

Result: 在多个基准上的实验表明，LoGo能显著提升个性化质量，尤其改善冷启动用户的推荐效果并减少偏差预测。

Conclusion: 通过融合集体知识的全局记忆可有效增强大语言模型的个性化能力，解决了冷启动和偏置问题，凸显了跨用户知识建模的重要性。

Abstract: Large language model (LLM) personalization aims to tailor model behavior to
individual users based on their historical interactions. However, its
effectiveness is often hindered by two key challenges: the \textit{cold-start
problem}, where users with limited history provide insufficient context for
accurate personalization, and the \textit{biasing problem}, where users with
abundant but skewed history cause the model to overfit to narrow preferences.
We identify both issues as symptoms of a common underlying limitation, i.e.,
the inability to model collective knowledge across users. To address this, we
propose a local-global memory framework (LoGo) that combines the personalized
local memory with a collective global memory that captures shared interests
across the population. To reconcile discrepancies between these two memory
sources, we introduce a mediator module designed to resolve conflicts between
local and global signals. Extensive experiments on multiple benchmarks
demonstrate that LoGo consistently improves personalization quality by both
warming up cold-start users and mitigating biased predictions. These results
highlight the importance of incorporating collective knowledge to enhance LLM
personalization.

</details>


### [445] [Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions](https://arxiv.org/abs/2509.23782)
*Yoonah Park,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种名为KAPPA的无参数干预方法，通过在特定子空间中对隐藏状态进行投影调整，使大语言模型的知识表征与预测表征对齐，从而缩小其在多项选择题中的知识-预测差距，并在多个基准上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备正确知识，但在多项选择题上常表现不佳，存在知识与预测之间的不一致问题，本文旨在探究这一现象的机制并加以缓解。

Method: 通过探针分析发现残差流中存在编码知识和预测的两个基底构成的子空间，错误预测源于这两个基底间的不对齐；提出KAPPA方法，通过基于投影的调整将预测坐标向知识坐标对齐。

Result: 在Big-Bench-Hard和ARC-Challenge的二选一任务上，KAPPA显著提升了模型准确率，且效果优于基线方法；该子空间具有一定跨任务泛化能力，并可扩展至自由形式问答任务。

Conclusion: KAPPA提供了一种几何视角来理解大模型的知识-预测差距，是一种有效、无需训练的干预手段，有助于更好地释放模型的潜在知识。

Abstract: Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)
despite demonstrating correct knowledge in other contexts, such as free-form
generation. To investigate the mechanism underlying this knowledge-prediction
gap on MCQs and alleviate it, we conduct a probing analysis and find that
residual streams in certain layers contain a subspace spanned by two important
bases: a \emph{knowledge basis} that encodes the probability of the
ground-truth answer for a given MCQ and a \emph{prediction basis} that encodes
the probability of the answer choice predicted by the model. We observe that
incorrect predictions arise from a misalignment of the model's hidden states
along these two bases. Hence, we introduce \textbf{KAPPA} (Knowledge-Aligned
Prediction through Projection-based Adjustment), a parameter-free intervention
that transforms the hidden states to align the prediction coordinate with the
knowledge coordinate within this subspace. Experiments on binary-choice
reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA
substantially improves accuracy and consistently outperforms baselines. While
optimal subspaces differ across tasks, subspaces generalize to some extent, as
supported by cross-dataset experiments. Moreover, KAPPA extends its
effectiveness to free-form questions beyond MCQs. Our work provides a new
geometric understanding of the knowledge-prediction gap and offers a practical
method for better aligning model behavior with its latent knowledge.

</details>


### [446] [Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering](https://arxiv.org/abs/2509.23793)
*Muhammad Abu Ahmad,Mohamad Ballout,Raia Abu Ahmad,Elia Bruni*

Main category: cs.CL

TL;DR: 本文提出了一种用于伊斯兰知识理解和推理的混合检索增强生成（RAG）系统，结合稀疏与密集检索及交叉编码器重排序，显著提升了大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升大语言模型在伊斯兰知识理解与推理任务中的准确性和语义匹配能力，解决现有方法在复杂知识问答中的局限性。

Method: 采用三阶段混合RAG框架：首先使用BM25进行稀疏检索，然后利用密集嵌入模型进行语义匹配，最后通过交叉编码器进行重排序以提高检索精度。

Result: 在两个子任务中使用Fanar和Mistral模型进行评估，结果显示所提方法最高可带来25%的准确率提升；其中Fanar模型表现最佳，在子任务1中达到45%准确率，子任务2中达到80%准确率。

Conclusion: 混合RAG框架能有效增强大语言模型在伊斯兰知识理解任务中的表现，尤其是在结合BM25、密集检索与交叉编码器重排序时效果显著。

Abstract: This paper presents our submission to the QIAS 2025 shared task on Islamic
knowledge understanding and reasoning. We developed a hybrid
retrieval-augmented generation (RAG) system that combines sparse and dense
retrieval methods with cross-encoder reranking to improve large language model
(LLM) performance. Our three-stage pipeline incorporates BM25 for initial
retrieval, a dense embedding retrieval model for semantic matching, and
cross-encoder reranking for precise content retrieval. We evaluate our approach
on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the
proposed RAG pipeline enhances performance across both, with accuracy
improvements up to 25%, depending on the task and model configuration. Our best
configuration is achieved with Fanar, yielding accuracy scores of 45% in
Subtask 1 and 80% in Subtask 2.

</details>


### [447] [Open-DeBias: Toward Mitigating Open-Set Bias in Language Models](https://arxiv.org/abs/2509.23805)
*Arti Rani,Shweta Singh,Nihar Ranjan Sahoo,Gaurav Kumar Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种新的开放集偏见检测与去偏方法Open-DeBias，并构建了OpenBiasBench基准用于评估文本问答中的社会偏见，该方法在少量数据下显著提升准确率并具备跨语言零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法多局限于预定义类别，难以应对新型或上下文相关的新兴偏见，因此需要能够处理开放集中已知和未知偏见的解决方案。

Method: 提出Open-DeBias方法，利用适配器模块进行高效参数与数据的去偏训练，并构建OpenBiasBench基准以涵盖广泛已知与未见偏见类别。

Result: 相比SOTA的BMBI方法，在BBQ数据集上模糊子集准确率提升近48%，明确子集提升6%；在仅使用少量训练数据的情况下，对韩语BBQ实现84%的零样本迁移准确率，并在StereoSet、CrowS-Pairs等多个NLP任务中验证其有效性。

Conclusion: Open-DeBias是一种高效、通用且具有强大多语言泛化能力的去偏方法，适用于开放域中的社会偏见缓解，为实现更公平可信的大型语言模型提供了有效路径。

Abstract: Large Language Models (LLMs) have achieved remarkable success on question
answering (QA) tasks, yet they often encode harmful biases that compromise
fairness and trustworthiness. Most existing bias mitigation approaches are
restricted to predefined categories, limiting their ability to address novel or
context-specific emergent biases. To bridge this gap, we tackle the novel
problem of open-set bias detection and mitigation in text-based QA. We
introduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases
across a wide range of categories and subgroups, encompassing both known and
previously unseen biases. Additionally, we propose Open-DeBias, a novel,
data-efficient, and parameter-efficient debiasing method that leverages adapter
modules to mitigate existing social and stereotypical biases while generalizing
to unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias
improves QA accuracy on BBQ dataset by nearly $48\%$ on ambiguous subsets and
$6\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction
of the training data. Remarkably, the same adapters, in a zero-shot transfer to
Korean BBQ, achieve $84\%$ accuracy, demonstrating robust language-agnostic
generalization. Through extensive evaluation, we also validate the
effectiveness of Open-DeBias across a broad range of NLP tasks, including
StereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,
and suitability for general-purpose, open-domain bias mitigation. The project
page is available at: https://sites.google.com/view/open-debias25

</details>


### [448] [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863)
*Ziyi Yang,Weizhou Shen,Ruijun Chen,Chenliang Li,Fanqi Wan,Ming Yan,Xiaojun Quan,Fei Huang*

Main category: cs.CL

TL;DR: 本文提出了SPELL，一种无需标签的多角色自对弈强化学习框架，用于提升大语言模型的长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可靠的人工标注和可编程验证的奖励信号，大语言模型在长上下文推理方面的进展相对滞后。

Method: SPELL框架将提问者、回答者和验证者三个角色集成于单一模型中，通过自我对弈生成问题、回答并评估语义等价性以提供奖励信号；同时引入自动课程学习和自适应奖励函数来稳定训练过程。

Result: 在六个长上下文基准上的实验表明，SPELL在多种大语言模型上均显著提升性能，优于同等规模的标注数据微调模型，在Qwen3-30B-A3B-Thinking上平均pass@8提升7.6点。

Conclusion: SPELL实现了可扩展、无标签的长上下文推理优化，展现出向更强模型扩展的潜力。

Abstract: Progress in long-context reasoning for large language models (LLMs) has
lagged behind other recent advances. This gap arises not only from the
intrinsic difficulty of processing long texts, but also from the scarcity of
reliable human annotations and programmatically verifiable reward signals. In
this paper, we propose SPELL, a multi-role self-play reinforcement learning
framework that enables scalable, label-free optimization for long-context
reasoning. SPELL integrates three cyclical roles-questioner, responder, and
verifier-within a single model to enable continual self-improvement. The
questioner generates questions from raw documents paired with reference
answers; the responder learns to solve these questions based on the documents;
and the verifier evaluates semantic equivalence between the responder's output
and the questioner's reference answer, producing reward signals to guide
continual training. To stabilize training, we introduce an automated curriculum
that gradually increases document length and a reward function that adapts
question difficulty to the model's evolving capabilities. Extensive experiments
on six long-context benchmarks show that SPELL consistently improves
performance across diverse LLMs and outperforms equally sized models fine-tuned
on large-scale annotated data. Notably, SPELL achieves an average 7.6-point
gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising
its performance ceiling and showing promise for scaling to even more capable
models.

</details>


### [449] [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)
*Shaobo Wang,Jiaming Wang,Jiajun Zhang,Cong Wang,Yue Min,Zichen Wen,Fei Huang,Huiqiang Jiang,Junyang Lin,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了Q-Tuning，一种基于错误-不确定性平面的统一数据修剪框架，通过联合优化样本级和令牌级修剪，在仅使用12.5%训练数据的情况下，在SmolLM2-1.7B上平均性能超越全数据微调基线38%，显著提升大模型微调的数据效率。


<details>
  <summary>Details</summary>
Motivation: 随着监督微调（SFT）计算量增加，数据效率成为关键问题；现有方法孤立地进行样本或令牌级修剪，导致高价值样本中冗余信息与重要信号被同时保留或丢弃，无法协同优化两个维度。

Method: 提出错误-不确定性（EU）平面用于分析样本与令牌层面的数据效用，并在此基础上设计Q-Tuning框架：第一阶段进行样本筛选，保留包含丰富误解或校准信号的样本；第二阶段对误解样本采用上下文感知的非对称令牌修剪策略，而完整保留校准样本。

Result: 在五个不同基准上达到当前最优性能，在SmolLM2-1.7B模型上仅用12.5%数据即实现比全数据SFT平均高出38%的性能提升，且首次证明动态剪枝可稳定优于全数据训练。

Conclusion: Q-Tuning为资源受限下的大语言模型监督微调提供了一种高效、可扩展的数据利用方案，是首个能持续超越全数据训练的动态剪枝方法。

Abstract: As supervised fine-tuning (SFT) evolves from a lightweight post-training step
into a compute-intensive phase rivaling mid-training in scale, data efficiency
has become critical for aligning large language models (LLMs) under tight
budgets. Existing data pruning methods suffer from a fragmented design: they
operate either at the sample level or the token level in isolation, failing to
jointly optimize both dimensions. This disconnect leads to significant
inefficiencies--high-value samples may still contain redundant tokens, while
token-level pruning often discards crucial instructional or corrective signals
embedded in individual examples. To address this bottleneck, we introduce the
Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes
the heterogeneous utility of training data across samples and tokens. Guided by
this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework
that strategically coordinates sample pruning and token pruning. Q-Tuning
employs a two-stage strategy: first, it performs sample-level triage to retain
examples rich in informative misconceptions or calibration signals; second, it
applies an asymmetric token-pruning policy, using a context-aware scoring
mechanism to trim less salient tokens exclusively from misconception samples
while preserving calibration samples in their entirety. Our method sets a new
state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,
Q-Tuning achieves a +38\% average improvement over the full-data SFT baseline
using only 12.5\% of the original training data. As the first dynamic pruning
approach to consistently outperform full-data training, Q-Tuning provides a
practical and scalable blueprint for maximizing data utilization in
budget-constrained LLM SFT.

</details>


### [450] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一种针对掩码扩散语言模型（MDLMs）的新型解码策略EOSER和ASS，以及强化学习算法CJ-GRPO，解决了传统方法在训练与推理之间不一致的问题，在数学推理和规划任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型技术直接迁移到MDLMs存在训练与推理不一致的问题，且缺乏专为MDLM设计的高效解码和强化学习方法。

Method: 提出了EOS Early Rejection（EOSER）和Ascending Step-Size（ASS）解码调度机制，并设计了Consistency Trajectory Group Relative Policy Optimization（CJ-GRPO）算法，确保rollout轨迹与优化轨迹的一致性。

Result: 在LLaDA-8B-Instruct模型上实验表明，所提方法在更少解码步数下实现了具有竞争力的性能，尤其在数学和规划类推理任务中表现优异。

Conclusion: EOSER、ASS与CJ-GRPO有效提升了MDLM的推理效率与训练稳定性，为MDLM的优化提供了新的方向。

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [451] [Assessing Large Language Models in Updating Their Forecasts with New Information](https://arxiv.org/abs/2509.23936)
*Zhangdie Yuan,Zifeng Ding,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出了EVOLVECAST框架，用于评估大语言模型在新信息出现时是否能适当调整其预测和置信度，并发现模型的更新往往不一致或过于保守，且与人类预测者相比仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 先前的研究将未来事件预测视为静态任务，忽略了随着新证据出现预测和置信度应如何动态调整的问题，因此需要一个能够评估模型动态更新能力的框架。

Method: 提出EVOLVECAST框架，通过引入训练截止日期后发布的新信息，测试大语言模型对未来事件预测的调整情况，并以人类预测者为参照，比较模型在预测变化和置信度校准方面的表现。

Result: 实验发现大语言模型虽能响应新信息，但更新常不一致或过于保守；基于言语化或logits的置信度估计均未显著优于对方，且两者都远不及人类水平。

Conclusion: 当前大语言模型在动态信念更新方面存在局限，表现出保守偏差，亟需更鲁棒的方法来提升其随新证据调整预测的能力。

Abstract: Prior work has largely treated future event prediction as a static task,
failing to consider how forecasts and the confidence in them should evolve as
new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework
for evaluating whether large language models appropriately revise their
predictions in response to new information. In particular, EVOLVECAST assesses
whether LLMs adjust their forecasts when presented with information released
after their training cutoff. We use human forecasters as a comparative
reference to analyze prediction shifts and confidence calibration under updated
contexts. While LLMs demonstrate some responsiveness to new information, their
updates are often inconsistent or overly conservative. We further find that
neither verbalized nor logits-based confidence estimates consistently
outperform the other, and both remain far from the human reference standard.
Across settings, models tend to express conservative bias, underscoring the
need for more robust approaches to belief updating.

</details>


### [452] [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
*Guojian Li,Chengyou Wang,Hongfei Xue,Shuiyuan Wang,Dehui Gao,Zihan Zhang,Yuke Lin,Wenjie Li,Longshuai Xiao,Zhonghua Fu,Lei Xie*

Main category: cs.CL

TL;DR: 本文提出了一种名为Easy Turn的开源、模块化双模态（声学与语言）对话回合状态检测模型，可识别完整、不完整、反馈和等待四种状态，并发布了包含1145小时语音的Easy Turn训练集，在自建测试集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有全双工交互中的回合检测模型大多未开源，或参数量大、仅支持单模态；微调大模型需大量标注数据，而开源的全双工数据稀缺。因此需要一个高效、开源、多模态且数据充足的解决方案。

Method: 设计了一个模块化的双模态模型Easy Turn，融合声学与语言信息，预测四种对话回合状态；同时构建了大规模开源数据集Easy Turn trainset用于训练和评估。

Result: 在自建的Easy Turn测试集上，模型性能优于现有的开源方案如TEN Turn Detection和Smart Turn V2，实现了最先进的回合检测准确率。

Conclusion: Easy Turn提供了一个高效、开源、多模态的回合检测解决方案，配合大规模训练数据，推动全双工对话系统的开放研究与应用。

Abstract: Full-duplex interaction is crucial for natural human-machine communication,
yet remains challenging as it requires robust turn-taking detection to decide
when the system should speak, listen, or remain silent. Existing solutions
either rely on dedicated turn-taking models, most of which are not
open-sourced. The few available ones are limited by their large parameter size
or by supporting only a single modality, such as acoustic or linguistic.
Alternatively, some approaches finetune LLM backbones to enable full-duplex
capability, but this requires large amounts of full-duplex data, which remain
scarce in open-source form. To address these issues, we propose Easy Turn, an
open-source, modular turn-taking detection model that integrates acoustic and
linguistic bimodal information to predict four dialogue turn states: complete,
incomplete, backchannel, and wait, accompanied by the release of Easy Turn
trainset, a 1,145-hour speech dataset designed for training turn-taking
detection models. Compared to existing open-source models like TEN Turn
Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking
detection accuracy on our open-source Easy Turn testset. The data and model
will be made publicly available on GitHub.

</details>


### [453] [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
*Claudio Fantinuoli*

Main category: cs.CL

TL;DR: 本文提出了一种基于视觉的机器传译方法（VGI），通过结合语音和视觉信息来提升翻译中的消歧能力，实验表明视觉信息在词汇消歧方面有显著改善。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态机器传译系统仅依赖语言信号，难以处理需要视觉或情境信息才能消歧的翻译任务，限制了实际性能。

Method: 提出Vision-Grounded Interpreting (VGI) 方法，构建一个融合视觉-语言模型的原型系统，利用网络摄像头输入的视觉信息对翻译过程进行上下文引导，并设计了一个针对三类歧义的手工诊断语料库进行评估。

Result: 视觉 grounding 显著提升了词汇消歧效果，在性别消解上表现出有限且不稳定的优势，但对句法歧义无明显改善。

Conclusion: 多模态融合是提升机器传译质量的必要方向，尤其是在依赖上下文消歧的场景中，视觉信息具有重要价值。

Abstract: Machine Interpreting systems are currently implemented as unimodal, real-time
speech-to-speech architectures, processing translation exclusively on the basis
of the linguistic signal. Such reliance on a single modality, however,
constrains performance in contexts where disambiguation and adequacy depend on
additional cues, such as visual, situational, or pragmatic information. This
paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed
to address the limitations of unimodal machine interpreting. We present a
prototype system that integrates a vision-language model to process both speech
and visual input from a webcam, with the aim of priming the translation process
through contextual visual information. To evaluate the effectiveness of this
approach, we constructed a hand-crafted diagnostic corpus targeting three types
of ambiguity. In our evaluation, visual grounding substantially improves
lexical disambiguation, yields modest and less stable gains for gender
resolution, and shows no benefit for syntactic ambiguities. We argue that
embracing multimodality represents a necessary step forward for advancing
translation quality in machine interpreting.

</details>


### [454] [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)
*Ken Deng,Zizheng Zhan,Wen Xiang,Wenqiang Zhu,Tianhao Peng,Xinping Lei,Weihao Li,Jingxuan Xu,Kun Wu,Yifan Yao,Haoyang Huang,Huaixi Tang,Kepeng Lei,Zhiyi Lai,Songwei Yu,Zongxian Feng,Zuchen Gao,Weihao Xie,Chenchen Zhang,Yanan Wu,Yuanxing Zhang,Lecheng Huang,Yuqun Zhang,Jie Liu,Zhaoxiang Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.CL

TL;DR: 本文提出了HiPO框架，通过混合策略优化实现大语言模型的自适应推理控制，能够在保持或提升准确率的同时显著减少推理所需的token数量。


<details>
  <summary>Details</summary>
Motivation: 为了提高大语言模型在复杂任务上的效率，避免始终生成冗长的推理链导致的资源浪费和高成本。

Method: 提出HiPO框架，结合混合数据管道（提供Think-on和Think-off配对响应）与混合强化学习奖励机制，使模型能自主决定何时进行详细推理，何时直接回答。

Result: 在数学和编程基准测试中，HiPO显著减少了推理token长度，同时保持或提高了准确性。

Conclusion: HiPO为高效、自适应的推理提供了一种原则性方法，有助于推理型大语言模型在资源受限的实际场景中的部署。

Abstract: Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)
reasoning to improve accuracy on complex tasks. However, always generating
lengthy reasoning traces is inefficient, leading to excessive token usage and
higher inference costs. This paper introduces the Hybrid Policy Optimization
(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to
selectively decide when to engage in detailed reasoning (Think-on) and when to
respond directly (Think-off). Specifically, HiPO combines a hybrid data
pipelineproviding paired Think-on and Think-off responseswith a hybrid
reinforcement learning reward system that balances accuracy and efficiency
while avoiding over-reliance on detailed reasoning. Experiments across
mathematics and coding benchmarks demonstrate that HiPO can substantially
reduce token length while maintaining or improving accuracy. Finally, we hope
HiPO a can be a principled approach for efficient adaptive reasoning, advancing
the deployment of reasoning-oriented LLMs in real-world, resource-sensitive
settings.

</details>


### [455] [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
*Haonan Wang,Junfeng Sun,Xingdi Yuan,Ruoyao Wang,Ziang Xiao*

Main category: cs.CL

TL;DR: 本文提出了ByteSized32Refactored，一个重构的、模块化的文本游戏生成框架，通过构建GameBasic.py基础库将代码量减少一半，并提升可扩展性，但实验表明其分层结构对大语言模型生成质量带来新挑战。


<details>
  <summary>Details</summary>
Motivation: 为了提升大语言模型在交互式世界建模中的能力，尤其是文本游戏生成任务中的可扩展性和代码复用性，需要对现有ByteSized32实现进行模块化重构。

Method: 重构原始ByteSized32语料库，设计并实现GameBasic.py基础库，抽象出7个基类以统一管理32个文本游戏的公共逻辑，显著减少代码冗余并增强模块化和可扩展性。

Result: 代码总量从20k行减少到10k行；基于GPT-4o的实验显示，在四个评估维度中两个有所提升，两个下降，表明重构后的分层结构对LLM生成带来新挑战。

Conclusion: 所提出的模块化与中心化代码结构不仅提升了文本游戏系统的可扩展性，也为LLM适应环境规范提供了更高效的框架，同时揭示了复杂代码结构对生成模型的新影响。

Abstract: Simulating interactive world models remains a core challenge in Large
Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a
refactored, modular, and extensible implementation of the original ByteSized32
corpus to explore the task of text game generation. We further optimize the
code structure of each text game and create the GameBasic.py foundation
library, which centralizes common logic across all 32 games by abstracting 7
base classes (GameObject, etc.) into reusable modules, thereby reducing from
20k to 10k total lines of Python code compared to the original Bytesized32. Our
refactored implementation enables extendability - with our centralized design,
ByteSized32Refactored can be more efficiently extended to include text games of
new scenarios and specifications by reusing the shared logic and
functionalities. Extensive experiments with GPT-4o demonstrate a mix of
performance - with Bytesized32Refactored, the generated text games for unseen
scenarios showcase quality improvements on two of the four evaluation
dimensions while decreases on the other two, indicating that the hierarchical
structure of the refactored code presents new challenges for LLMs. Overall, we
highlight that our extensible code structure, centered on the foundation
library and the modular optimization, not only facilitates LLM adaptation to
environment specifications but also establishes a scalable environment that
supports future extensions.

</details>


### [456] [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的偏好对齐方法PaLRS，通过残差流中的偏好信号生成轻量级引导向量，在推理时实现模型行为调控。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法需要大量标注数据和昂贵的参数优化，且产生特定任务模型，限制了灵活性和效率。

Method: 利用LLM残差流中编码的偏好信号，从少量偏好对中提取可插拔的引导向量，用于推理时的行为调节。

Result: 在多个中小规模开源LLM上验证，PaLRS在数学推理和代码生成任务上持续提升性能，保持原有通用能力，并显著优于DPO方法且节省大量时间。

Conclusion: PaLRS提供了一种高效、灵活、无需训练的偏好对齐新范式，仅需极少数据即可实现即插即用的模型行为调控。

Abstract: Preference alignment is a critical step in making Large Language Models
(LLMs) useful and aligned with (human) preferences. Existing approaches such as
Reinforcement Learning from Human Feedback or Direct Preference Optimization
typically require curated data and expensive optimization over billions of
parameters, and eventually lead to persistent task-specific models. In this
work, we introduce Preference alignment of Large Language Models via Residual
Steering (PaLRS), a training-free method that exploits preference signals
encoded in the residual streams of LLMs. From as few as one hundred preference
pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be
applied at inference time to push models toward preferred behaviors. We
evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that
PaLRS-aligned models achieve consistent gains on mathematical reasoning and
code generation benchmarks while preserving baseline general-purpose
performance. Moreover, when compared to DPO-aligned models, they perform better
with huge time savings. Our findings highlight that PaLRS offers an effective,
much more efficient and flexible alternative to standard preference
optimization pipelines, offering a training-free, plug-and-play mechanism for
alignment with minimal data.

</details>


### [457] [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
*Dhaathri Vijay,Anandaswarup Vadapalli*

Main category: cs.CL

TL;DR: 该研究通过机器翻译案例，比较了全规模、蒸馏和量化模型在翻译质量与效率之间的权衡，发现模型压缩技术能显著降低计算成本和环境影响，同时保持较高的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，其计算和环境成本引发关注，因此需要探讨在保证性能的同时提升效率和可持续性的方法。

Method: 使用Flores+基准测试和人工评估法，在法语、印地语和卡纳达语的对话翻译任务中，对比全规模、蒸馏和量化模型的性能，并分析每次评估运行的碳排放量。

Result: 全尺寸3.3B fp32模型BLEU分数最高，但碳足迹最大（每次运行约0.007-0.008 kg CO2）；蒸馏模型推理速度快达4.5倍，BLEU得分略有下降；INT4量化模型在人工评估中仍保持高准确性和流畅性。低资源场景下权衡更明显。

Conclusion: 模型压缩策略可在几乎不牺牲翻译质量的前提下大幅降低计算开销和环境影响，建议将效率与可持续性纳入NLP发展的核心评估维度。

Abstract: The rapid expansion of large language models (LLMs) has heightened concerns
about their computational and environmental costs. This study investigates the
trade-offs between translation quality and efficiency by comparing full-scale,
distilled, and quantized models using machine translation as a case study. We
evaluated performance on the Flores+ benchmark and through human judgments of
conversational translations in French, Hindi, and Kannada. Our analysis of
carbon emissions per evaluation run revealed that the full 3.3B fp32 model,
while achieving the highest BLEU scores, incurred the largest environmental
footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an
inference of up to 4.5x faster than the full 3.3B model, with only minimal
reductions in BLEU scores. Human evaluations also showed that even aggressive
quantization (INT4) preserved high levels of accuracy and fluency, with
differences between models generally minor. These findings demonstrate that
model compression strategies can substantially reduce computational demands and
environmental impact while maintaining competitive translation quality, though
trade-offs are more pronounced in low-resource settings. We argue for
evaluation frameworks that integrate efficiency and sustainability alongside
objective metrics as central dimensions of progress in NLP.

</details>


### [458] [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: 提出了一种名为“Policy as Prompt”的新框架，利用大语言模型将非结构化设计文档自动转化为可验证的实时保护机制，通过上下文理解和最小权限原则来解释和执行自然语言策略。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理在工业中的广泛应用，确保其安全变得至关重要。现有的策略执行方法难以应对非结构化设计文档和动态运行环境，因此需要一种能够自动化、可验证且可审计的安全保障框架。

Method: 系统首先解析技术文档以构建可验证的策略树，然后将其编译为轻量级的基于提示的分类器，在运行时对AI代理行为进行审计。采用大语言模型实现对自然语言策略的理解与执行，结合上下文感知和最小权限原则。

Result: 在多种应用场景中验证了该方法的有效性，展示了从策略到实践的可扩展、可审计的管道，显著缩小了策略制定与实际执行之间的差距。

Conclusion: 该框架实现了AI行为的可验证安全保障，提升了AI系统的安全性与合规性，为未来可监管AI的发展提供了可行路径。

Abstract: As autonomous AI agents are increasingly deployed in industry, it is
essential to safeguard them. We introduce a novel framework that automates the
translation of unstructured design documents into verifiable, real-time
guardrails. We introduce "Policy as Prompt," a new approach that uses Large
Language Models (LLMs) to interpret and enforce natural language policies by
applying contextual understanding and the principle of least privilege. Our
system first ingests technical artifacts to construct a verifiable policy tree,
which is then compiled into lightweight, prompt-based classifiers that audit
agent behavior at runtime. We validate our approach across diverse
applications, demonstrating a scalable and auditable pipeline that bridges the
critical policy-to-practice gap, paving the way for verifiably safer and more
regulatable AI.

</details>


### [459] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: 本文提出了MCPMark，一个包含127个高质量任务的基准测试，用于更全面、真实地评估大型语言模型（LLM）在MCP（模型上下文协议）中的表现。与现有侧重读取或交互深度有限的任务不同，MCPMark强调多样化的创建、读取、更新和删除（CRUD）操作，并通过程序化脚本自动验证结果。实验表明，即使是最先进的模型如gpt-5-medium，其pass@1得分也仅为52.56%，其他主流模型表现更差，显示出当前LLM在复杂交互任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的MCP基准测试范围狭窄，主要集中在读取密集型或交互深度有限的任务上，无法反映现实世界工作流的复杂性和真实性。因此，需要一个更具挑战性和代表性的基准来准确评估LLM在实际应用场景下的能力。

Method: 提出MCPMark基准，包含由领域专家和AI代理共同构建的127个高质量任务。每个任务具有预设初始状态和可编程验证脚本，涵盖丰富的CRUD操作。采用最小化智能体框架，在工具调用循环中对前沿LLM进行评估。

Result: 实验结果显示，最佳模型gpt-5-medium的pass@1为52.56%，pass^4为33.86%；其他强模型如claude-sonnet-4和o3均低于30% pass@1和15% pass^4。平均每个任务需16.2轮执行和17.4次工具调用，显著高于以往基准，体现出更高的复杂度和压力测试能力。

Conclusion: MCPMark提供了一个更真实、更具挑战性的评估环境，揭示了当前先进LLM在复杂、多步骤交互任务中的性能瓶颈，推动未来研究朝向更强健、更可靠的智能代理发展。

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [460] [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
*Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang*

Main category: cs.CL

TL;DR: 本文提出了Next Sequence Prediction (NSP) 框架和Sequential Diffusion Language Model (SDLM)，统一了next-token与next-block预测，支持动态生成长度，兼容KV缓存，并可高效微调自回归语言模型，在少量训练样本下即超越强基线且显著提升推理吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型受限于固定长度解码且不兼容KV缓存，而块扩散虽有所改进但仍需昂贵训练并强制固定块大小，因此需要一种更灵活、高效且兼容现有架构的生成范式。

Method: 提出Next Sequence Prediction (NSP) 框架，统一next-token与next-block预测，允许每步自适应决定生成长度；基于NSP构建Sequential Diffusion Language Model (SDLM)，在固定大小掩码块内进行扩散推理，但根据模型置信度动态解码连续子序列，保持对KV缓存的兼容性，并可低成本微调预训练自回归模型。

Result: 实验表明，SDLM仅用3.5M训练样本即可匹敌或超越强自回归基线，推理吞吐比Qwen-2.5高2.1倍，且SDLM-32B展现出更强的效率增益与可扩展性。

Conclusion: NSP与SDLM提供了一种灵活、高效且可扩展的扩散语言建模新范式，兼顾生成质量、推理效率与对现有架构的兼容性，展现出在大模型中的应用潜力。

Abstract: Diffusion language models (DLMs) have strong theoretical efficiency but are
limited by fixed-length decoding and incompatibility with key-value (KV)
caches. Block diffusion mitigates these issues, yet still enforces a fixed
block size and requires expensive training. We introduce Next Sequence
Prediction (NSP), which unifies next-token and next-block prediction, enabling
the model to adaptively determine the generation length at each step. When the
length is fixed to 1, NSP reduces to standard next-token prediction. Building
on NSP, we propose Sequential Diffusion Language Model (SDLM), which can
retrofit pre-trained autoregressive language models (ALMs) at minimal cost.
Specifically, SDLM performs diffusion inference within fixed-size mask blocks,
but dynamically decodes consecutive subsequences based on model confidence,
thereby preserving KV-cache compatibility and improving robustness to varying
uncertainty and semantics across the sequence. Experiments show that SDLM
matches or surpasses strong autoregressive baselines using only 3.5M training
samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the
SDLM-32B model delivers even more pronounced efficiency gains, demonstrating
the strong scalability potential of our modeling paradigm. Project page and
codes: https://github.com/OpenGVLab/SDLM

</details>


### [461] [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)
*Zeqing Wang,Gongfan Fang,Xinyin Ma,Xingyi Yang,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SparseD的新型稀疏注意力方法，用于解决扩散语言模型（DLMs）在长上下文应用中的高推理延迟问题。SparseD通过预计算头特定的稀疏模式并在所有去噪步骤中重用，同时在早期使用全注意力、后期切换到稀疏注意力，实现了无损加速。


<details>
  <summary>Details</summary>
Motivation: 现有的开源DLMs由于注意力机制在上下文长度上的二次复杂度导致推理延迟较高，而传统的稀疏注意力方法不适用于DLMs特有的注意力行为。

Method: 提出SparseD，利用DLMs中注意力模式在不同头之间变化但在每个头内跨去噪步高度相似的特点，预先计算并重用头特定的稀疏模式，并在早期去噪步使用全注意力，后期切换为稀疏注意力。

Result: 实验结果表明，在64k上下文长度和1024个去噪步下，SparseD相比FlashAttention最高可实现1.50倍的速度提升，且保持生成质量无损。

Conclusion: SparseD是一种高效且实用的DLM部署方案，特别适用于长上下文场景，能够在不牺牲生成质量的前提下显著降低推理成本。

Abstract: While diffusion language models (DLMs) offer a promising alternative to
autoregressive models (ARs), existing open-source DLMs suffer from high
inference latency. This bottleneck is mainly due to the attention's quadratic
complexity with respect to context length in computing all query-key pairs.
Intuitively, to reduce this complexity, a natural strategy is to restrict
attention to sparse patterns that retain only the most relevant connections.
Such approaches are well-established in ARs, where attention follows fixed and
clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity
behaviors: (1) attention patterns vary across heads, (2) attention patterns in
each head remain highly similar across denoising steps, and (3) early denoising
steps are critical for generation. These findings render sparse attention
methods designed for ARs largely incompatible with DLMs, as they fail to
capture head-specific structures and risk degrading generation when applied in
early denoising steps. To address these challenges, we propose SparseD, a novel
sparse attention method for DLMs. Leveraging the observations, SparseD only
requires pre-computing head-specific sparse patterns one time, and reuses them
across all steps. This prevents recomputing sparse patterns at each denoising
step. Meanwhile, SparseD uses full attention in the early steps, then switches
to sparse attention later to maintain generation quality. Together, these
establish SparseD as a practical and efficient solution for deploying DLMs in
long-context applications. Experimental results demonstrate that SparseD
achieves lossless acceleration, delivering up to $1.50\times$ speedup over
FlashAttention at a 64k context length with 1,024 denoising steps.

</details>


### [462] [ResFormer: All-Time Reservoir Memory for Long Sequence Classification](https://arxiv.org/abs/2509.24074)
*Hongbo Liu,Jia Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ResFormer的新神经网络架构，通过结合储层计算和Transformer来高效处理长文本序列，在多个情感分析和对话理解数据集上显著优于现有模型，并具有更低的内存消耗。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优越，但其二次时间与内存复杂度限制了输入长度，难以高效处理长上下文。因此需要一种能有效建模不同上下文长度且计算效率更高的方法。

Method: ResFormer采用级联架构：使用具有非线性读出的储层计算网络以线性时间捕捉长期上下文依赖；同时用固定长度输入的传统Transformer建模句子内的短期依赖。

Result: 实验表明，ResFormer在EmoryNLP数据集上准确率最高提升22.3%，并在MultiWOZ、MELD和IEMOCAP数据集上持续取得增益，同时内存消耗更低。

Conclusion: ResFormer在保持高性能的同时显著提升了对长上下文的建模能力和计算效率，是序列分类任务中处理变长上下文的一种有效且高效的解决方案。

Abstract: Sequence classification is essential in NLP for understanding and
categorizing language patterns in tasks like sentiment analysis, intent
detection, and topic classification. Transformer-based models, despite
achieving state-of-the-art performance, have inherent limitations due to
quadratic time and memory complexity, restricting their input length. Although
extensive efforts have aimed at reducing computational demands, processing
extensive contexts remains challenging.
  To overcome these limitations, we propose ResFormer, a novel neural network
architecture designed to model varying context lengths efficiently through a
cascaded methodology. ResFormer integrates an reservoir computing network
featuring a nonlinear readout to effectively capture long-term contextual
dependencies in linear time. Concurrently, short-term dependencies within
sentences are modeled using a conventional Transformer architecture with
fixed-length inputs.
  Experiments demonstrate that ResFormer significantly outperforms baseline
models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of
up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD,
and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption,
underscoring its effectiveness and efficiency in modeling extensive contextual
information.

</details>


### [463] [Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets](https://arxiv.org/abs/2509.24080)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 本文提出了一种基于多语言预训练模型的集成方法，用于提升无标注数据情况下外语情感分析的性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注数据，传统情感分析方法在外语处理上表现不佳，因此需要一种能有效处理多语言情感分析的方法。

Method: 采用由bert-base-multilingual-uncased-sentiment和XLM-R组成的预训练模型集成方法，并结合大语言模型对多语言数据集进行情感分析。

Result: 实验结果表明，所提出方法的情感分析准确率超过86%。

Conclusion: 该集成方法在多语言情感分析任务中表现出色，适用于缺乏标注资源的外语场景。

Abstract: Sentiment analysis is a very important natural language processing activity
in which one identifies the polarity of a text, whether it conveys positive,
negative, or neutral sentiment. Along with the growth of social media and the
Internet, the significance of sentiment analysis has grown across numerous
industries such as marketing, politics, and customer service. Sentiment
analysis is flawed, however, when applied to foreign languages, particularly
when there is no labelled data to train models upon. In this study, we present
a transformer ensemble model and a large language model (LLM) that employs
sentiment analysis of other languages. We used multi languages dataset.
Sentiment was then assessed for sentences using an ensemble of pre-trained
sentiment analysis models: bert-base-multilingual-uncased-sentiment, and XLM-R.
Our experimental results indicated that sentiment analysis performance was more
than 86% using the proposed method.

</details>


### [464] [Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了大规模约束生成（LSCG）问题，并通过Words Checker任务评估LLM在处理大量细粒度约束时的表现，提出FoCusNet模型以提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索现有大语言模型在面对大量细粒度、通用约束时的生成控制能力不足的问题。

Method: 构建Words Checker任务，分析模型特性与引导技术对性能的影响，并提出FoCusNet模型用于约束筛选。

Result: 随着约束数量增加，现有方法性能显著下降，而FoCusNet带来了8-13%的准确率提升。

Conclusion: FoCusNet能有效帮助LLM聚焦关键约束，在大规模约束生成任务中显著提升性能。

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [465] [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
*Kaiyu He,Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 本文提出了GEAR，一种用于评估大语言模型在溯因推理（abductive reasoning）中生成新知识能力的通用、自动化、无标签评估框架，并通过多模型实验和基于动量的课程学习策略验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于指令遵循与演绎推理，但大语言模型是否能发现新知识仍不清楚。传统评估方法依赖人工标注或固定答案，难以衡量模型生成新颖且合理假设的能力，因此需要一种可扩展、透明且无需标签的评估方式。

Method: 提出GEAR框架，从一致性、可泛化性和多样性三个指标自动评估假设集；在四个溯因推理基准上对九个大语言模型进行细粒度分析；设计基于学习速度的动量式课程学习方法，动态调整训练数据以优化GEAR目标。

Result: GEAR能够有效区分不同模型在无监督情况下的溯因能力差异；所提课程学习策略在无金标签监督下提升了所有GEAR指标，并且增益可迁移到传统基准任务上。

Conclusion: GEAR为评估和促进大语言模型进行溯因推理提供了可扩展、可靠且开放式的框架，证明了无需人工标注也能推动模型生成更丰富、更可靠的假设。

Abstract: Since the advent of large language models (LLMs), research has focused on
instruction following and deductive reasoning. A central question remains: can
these models discover new knowledge, and how can we evaluate this ability? We
address this by studying abductive reasoning-the generation of plausible
hypotheses to explain observations-and introduce GEAR (General Evaluation for
Abductive Reasoning), a general-purpose, fully automated, transparent, and
label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:
consistency (each hypothesis explains the observations), generalizability
(consistent hypotheses make meaningful predictions on unseen inputs), and
diversity (the set covers distinct predictions and patterns). Built this way,
GEAR is scalable (no human gold answers), reliable (deterministic scoring
aligned with classical abduction), and open-ended (scores improve only when
models produce new plausible hypotheses, unlike static benchmarks that saturate
once accuracy is high). Using GEAR, we conduct a fine-grained study of nine
LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000
candidate hypotheses and revealing model differences obscured by gold-answer or
purely human evaluations. We further propose a momentum-based curriculum that
adjusts GEAR-derived training data by learning velocity: it starts with what
the model learns quickly and shifts toward harder objectives such as generating
diverse hypotheses once the model is confident on foundational objectives.
Without gold-label supervision, this strategy improves all GEAR objectives and
these gains transfer to established abductive reasoning benchmarks. Taken
together, GEAR provides a principled framework that evaluates abduction and
supplies label-free, scalable training signals that help LLMs produce more
diverse and reliable hypotheses.

</details>


### [466] [BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models](https://arxiv.org/abs/2509.24101)
*Zsolt T. Kardkovács,Lynda Djennane,Anna Field,Boualem Benatallah,Yacine Gaci,Fabio Casati,Walid Gaaloul*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的新型偏见测试框架BTC-SAM，用于生成高质量、多样化的情感分析模型偏见测试用例。


<details>
  <summary>Details</summary>
Motivation: 情感分析模型中存在有害的社会偏见，传统构建测试句子的方法成本高且难以覆盖多种偏见。

Method: 利用大语言模型进行可控的测试句子生成，仅需最小化指定即可产生自然、多样且覆盖广泛的测试用例。

Result: 实验表明，该方法相比基础提示方法能提供更高的语言多样性和测试覆盖率，甚至对未见过的偏见也有效。

Conclusion: BTC-SAM框架能有效降低偏见测试成本，并提升情感分析模型偏见检测的全面性和实用性。

Abstract: Sentiment Analysis (SA) models harbor inherent social biases that can be
harmful in real-world applications. These biases are identified by examining
the output of SA models for sentences that only vary in the identity groups of
the subjects. Constructing natural, linguistically rich, relevant, and diverse
sets of sentences that provide sufficient coverage over the domain is
expensive, especially when addressing a wide range of biases: it requires
domain experts and/or crowd-sourcing. In this paper, we present a novel bias
testing framework, BTC-SAM, which generates high-quality test cases for bias
testing in SA models with minimal specification using Large Language Models
(LLMs) for the controllable generation of test sentences. Our experiments show
that relying on LLMs can provide high linguistic variation and diversity in the
test sentences, thereby offering better test coverage compared to base
prompting methods even for previously unseen biases.

</details>


### [467] [Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics](https://arxiv.org/abs/2509.24102)
*Guangliang Liu,Xi Chen,Bocheng Chen,Xitong Zhang,Kristen Johnson*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在道德推理中实现泛化的挑战，并提出基于道德基础理论的语用推断方法，以提升其道德推理的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于LLM擅长捕捉分布语义，而道德推理涉及语用层面，导致其在道德推理任务中难以泛化，因此需要桥接这一语义与语用之间的差距。

Method: 提出基于道德基础理论的语用推断方法，利用每一步的上下文信息，将道德基础与道德推理目标联系起来，从而引导LLM进行更有效的推理。

Result: 实验结果表明，所提出的方法显著提升了LLM在道德推理任务中的泛化能力。

Conclusion: 该方法为基于道德基础理论的LLM道德推理研究提供了可行路径和未来方向。

Abstract: Moral reasoning has emerged as a promising research direction for Large
Language Models (LLMs), yet achieving generalization remains a central
challenge. From a linguistic standpoint, this difficulty arises because LLMs
are adept at capturing distributional semantics, which fundamentally differs
from the morals which operate at the pragmatic level. This paper investigates
how LLMs can achieve generalized moral reasoning despite their reliance on
distributional semantics. We propose pragmatic inference methods grounded in
moral foundations theory, which leverage contextual information at each step to
bridge the pragmatic gap and guide LLMs in connecting moral foundations with
moral reasoning objectives. Experimental results demonstrate that our approach
significantly enhances LLMs' generalization in moral reasoning, providing a
foundation for future research grounded in moral foundations theory.

</details>


### [468] [Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems](https://arxiv.org/abs/2509.24116)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: GLoW是一种基于双尺度世界模型的LLM智能体方法，通过全局发现前沿和局部多路径优势反思机制，在硬探索任务中显著提升探索效率，在Jericho文本游戏基准上达到SOTA性能，且环境交互需求减少100-800倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在需要通过探索学习新知识的“硬探索”任务中表现受限，缺乏有效的探索引导机制。

Method: 提出GLoW框架，结合双尺度世界模型：全局尺度维护高价值发现的轨迹前沿，局部尺度通过多路径优势反射机制从试错中学习，利用基于优势的进步信号指导探索。

Result: 在Jericho文本游戏基准上，GLoW在LLM-based方法中实现了新的SOTA性能，与先进的基于RL的方法性能相当，但仅需其1/100到1/800的环境交互次数。

Conclusion: GLoW有效提升了LLM智能体在硬探索任务中的性能，通过高效探索机制显著减少了对环境交互的依赖，展示了其在复杂探索任务中的潜力。

Abstract: LLM-based agents have seen promising advances, yet they are still limited in
"hard-exploration" tasks requiring learning new knowledge through exploration.
We present GLoW, a novel approach leveraging dual-scale world models,
maintaining a trajectory frontier of high-value discoveries at the global
scale, while learning from local trial-and-error in exploration through a
Multi-path Advantage Reflection mechanism which infers advantage-based progress
signals to guide exploration. To evaluate our framework for hard-exploration,
we tackle the Jericho benchmark suite of text-based games, where GLoW achieves
a new state-of-theart performance for LLM-based approaches. Compared to
state-of-the-art RLbased methods, our approach achieves comparable performance
while requiring 100-800x fewer environment interactions.

</details>


### [469] [EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos](https://arxiv.org/abs/2509.24120)
*Sourjyadip Ray,Shubham Sharma,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种利用多模态大语言模型（MLLMs）自动回答在线讲座中学生问题的新方法，并构建了一个包含5252个问答对的EduVidQA数据集。通过实证研究和基准测试，评估了现有MLLM在该任务上的表现，揭示了其挑战性，并为自然语言处理在教育领域的应用提供了新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台改变教育模式，在线学习中的互动性至关重要。然而，目前缺乏有效的自动化问答系统来响应学生在观看在线课程时提出的问题，尤其是在涉及多模态内容（如视频）的情况下。因此，需要探索基于多模态大语言模型的解决方案以提升学习体验。

Method: 作者构建了一个名为EduVidQA的数据集，包含来自296个计算机科学视频的5252个问答对（包括合成与真实数据），覆盖多种主题和难度级别。他们通过实证研究分析学生对回答质量的偏好，并对6种最先进的多模态大语言模型进行了基准测试，使用文本和定性指标评估模型性能，同时研究合成数据在微调中的有效性。

Result: 实验表明该任务具有较高挑战性，现有MLLM表现有限；合成数据在一定程度上有助于模型微调；结合文本与定性评估能更全面地反映模型性能；学生偏好分析为未来系统设计提供了重要参考。

Conclusion: 本研究为在线教育中的多模态问答任务建立了首个基准，验证了MLLM在此场景下的潜力与局限，强调了综合评估的重要性，并为自然语言处理在教育中的应用开辟了新方向。

Abstract: As digital platforms redefine educational paradigms, ensuring interactivity
remains vital for effective learning. This paper explores using Multimodal
Large Language Models (MLLMs) to automatically respond to student questions
from online lectures - a novel question answering task of real world
significance. We introduce the EduVidQA Dataset with 5252 question-answer pairs
(both synthetic and real-world) from 296 computer science videos covering
diverse topics and difficulty levels. To understand the needs of the dataset
and task evaluation, we empirically study the qualitative preferences of
students, which we provide as an important contribution to this line of work.
Our benchmarking experiments consist of 6 state-of-the-art MLLMs, through which
we study the effectiveness of our synthetic data for finetuning, as well as
showing the challenging nature of the task. We evaluate the models using both
text-based and qualitative metrics, thus showing a nuanced perspective of the
models' performance, which is paramount to future work. This work not only sets
a benchmark for this important problem, but also opens exciting avenues for
future research in the field of Natural Language Processing for Education.

</details>


### [470] [Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE](https://arxiv.org/abs/2509.24130)
*Guancheng Wan,Lucheng Fu,Haoxin Liu,Yiqiao Jin,Hui Yi Leong,Eric Hanchen Jiang,Hejia Geng,Jinhe Bi,Yunpu Ma,Xiangru Tang,B. Aditya Prakash,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了文本尖锐性（textual sharpness）的概念，用以描述提示词在语义邻域内因微小改写导致性能大幅波动的问题，并提出TARE和ATARE两种无需梯度的提示优化框架，在保证准确性的同时提升提示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法大多只关注准确率，忽视了提示在语义等价改写下的稳定性（即鲁棒性），导致自动提示搜索在实践中脆弱且不可靠。

Method: 提出文本尖锐性的形式化定义及基于语义邻域的鲁棒性准则；设计TARE框架，通过内层基于采样的对抗搜索生成难性 paraphrase 和外层鲁棒选择机制联合优化；进一步提出ATARE，引入各向异性权重和动态邻域半径以平衡探索与保真。

Result: 在多种任务上验证了TARE和ATARE的有效性，生成的提示在保持高准确率的同时显著提升了对 paraphrase 的鲁棒性，优于仅优化准确率的方法，且计算开销可控。

Conclusion: 通过最小化文本尖锐性差距，所提方法实现了更稳定、可靠的提示优化，推动了黑盒条件下自动化提示搜索的实用性进展。

Abstract: The performance of Large Language Models (LLMs) hinges on carefully
engineered prompts. However, prevailing prompt optimization methods, ranging
from heuristic edits and reinforcement learning to evolutionary search,
primarily target point-wise accuracy. They seldom enforce paraphrase invariance
or searching stability, and therefore cannot remedy this brittleness in
practice. Automated prompt search remains brittle: small, semantically
preserving paraphrases often cause large performance swings. We identify this
brittleness as the textual sharpness of the prompt landscape. In this work, we
provide the first formal treatment of textual sharpness in the discrete,
semantic space of prompts, together with an operational robustness criterion
over a semantic neighborhood; the design is black-box or API-only, requiring no
gradients to update the model's parameters. Then we introduce TARE (Textual
Sharpness-Aware Evolving), a derivative-free framework that alternates between
an inner, sampling-based adversarial search that stresses a prompt with hard
paraphrases and an outer, robust selection that prefers candidates whose
neighborhoods remain strong. We further propose ATARE, which learns anisotropic
weights to shape the semantic neighborhood and adapts its radius over time to
balance exploration and fidelity. Diverse tasks evaluate our methods, whose
design for minimizing textual sharpness gap leads to prompts that preserve
accuracy under paraphrasing, outperforming accuracy-only prompt search while
remaining computationally practical.

</details>


### [471] [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
*Yida Chen,Yuning Mao,Xianjun Yang,Suyu Ge,Shengjie Bi,Lijuan Liu,Saghar Hosseini,Liang Tan,Yixin Nie,Shaoliang Nie*

Main category: cs.CL

TL;DR: 本文提出了LLM-proposed Open Taxonomy (LOT) 方法，用于分析和比较大型推理模型（LRMs）的推理过程差异。LOT利用生成式语言模型对推理轨迹进行分类，并构建人类可读的自然语言分类体系，揭示不同模型在数学、科学和编程任务中的系统性思维差异。实验表明，LOT能以80-100%的准确率区分不同规模、基础模型或目标领域的LRMs，并通过案例研究显示，使小模型的推理风格向大模型对齐可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有对大型推理模型的比较主要集中在宏观统计指标上，缺乏对模型推理过程差异的深入理解。本文旨在填补这一空白，探究不同LRMs是否以及如何以不同的方式进行推理。

Method: 提出LOT方法：使用生成式语言模型分析和比较来自不同LRMs的推理轨迹，提取其特征并形成可解释的自然语言分类体系；通过建模这些特征在不同模型输出中的经验分布，预测推理轨迹的来源模型，并在迭代中构建完整的分类体系。

Result: LOT在12个开源LRMs上的实验中成功识别出系统性的推理差异，在区分不同模型的推理轨迹时达到80-100%的准确率；同时生成了描述模型思维方式差异的可读分类体系；案例研究表明，将较小Qwen3模型的推理风格与最大Qwen3对齐，可在GPQA任务上提升3.3-5.7%的准确率。

Conclusion: LOT提供了一种可解释的方法来系统比较大型推理模型的思维过程，不仅能够准确区分不同模型的推理模式，还能通过自然语言描述其差异，并为提升小模型性能提供了新思路——通过推理风格对齐来增强表现。

Abstract: Current comparisons of large reasoning models (LRMs) focus on macro-level
statistics such as task accuracy or reasoning length. Whether different LRMs
reason differently remains an open question. To address this gap, we introduce
the LLM-proposed Open Taxonomy (LOT), a classification method that uses a
generative language model to compare reasoning traces from two LRMs and
articulate their distinctive features in words. LOT then models how these
features predict the source LRM of a reasoning trace based on their empirical
distributions across LRM outputs. Iterating this process over a dataset of
reasoning traces yields a human-readable taxonomy that characterizes how models
think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in
math, science, and coding. LOT identifies systematic differences in their
thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from
LRMs that differ in scale, base model family, or objective domain. Beyond
classification, LOT's natural-language taxonomy provides qualitative
explanations of how LRMs think differently. Finally, in a case study, we link
the reasoning differences to performance: aligning the reasoning style of
smaller Qwen3 models with that of the largest Qwen3 during test time improves
their accuracy on GPQA by 3.3-5.7%.

</details>


### [472] [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
*Haolin Yang,Hakaze Cho,Naoya Inoue*

Main category: cs.CL

TL;DR: 提出基于任务子空间对数归因（TSLA）的新框架，识别在上下文学习中负责任务识别和任务学习的注意力头，并揭示其机制。


<details>
  <summary>Details</summary>
Motivation: 调和大语言模型中上下文学习（ICL）的两种主流视角：注意力头的组件级分析与ICL的整体分解（任务识别与任务学习）。

Method: 提出任务子空间对数归因（TSLA）框架，结合相关性分析、消融实验、输入扰动和引导实验，分析注意力头在任务识别和任务学习中的作用。

Result: 识别出专门负责任务识别和任务学习的注意力头，发现前者将隐藏状态对齐到任务子空间，后者在子空间内旋转状态以进行预测，并统一了归纳头和任务向量等先前发现。

Conclusion: TSLA框架为大语言模型如何在多种任务中执行上下文学习提供了统一且可解释的机制解释。

Abstract: We investigate the mechanistic underpinnings of in-context learning (ICL) in
large language models by reconciling two dominant perspectives: the
component-level analysis of attention heads and the holistic decomposition of
ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel
framework based on Task Subspace Logit Attribution (TSLA) to identify attention
heads specialized in TR and TL, and demonstrate their distinct yet
complementary roles. Through correlation analysis, ablation studies, and input
perturbations, we show that the identified TR and TL heads independently and
effectively capture the TR and TL components of ICL. Using steering experiments
with geometric analysis of hidden states, we reveal that TR heads promote task
recognition by aligning hidden states with the task subspace, while TL heads
rotate hidden states within the subspace toward the correct label to facilitate
prediction. We further show how previous findings on ICL mechanisms, including
induction heads and task vectors, can be reconciled with our
attention-head-level analysis of the TR-TL decomposition. Our framework thus
provides a unified and interpretable account of how large language models
execute ICL across diverse tasks and settings.

</details>


### [473] [Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight](https://arxiv.org/abs/2509.24169)
*Haolin Yang,Hakaze Cho,Kaize Ding,Naoya Inoue*

Main category: cs.CL

TL;DR: 提出并训练Learned Task Vectors (LTVs)，揭示其在上下文学习中的机制，发现其通过注意力头OV电路影响预测，并表现出线性传播特性。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量提取方法繁琐且不透明，缺乏对任务向量如何影响模型计算的机制解释。

Method: 直接训练Learned Task Vectors (LTVs)，并通过系统分析研究其在Transformer中的作用机制，包括低层的注意力头OV电路和高层的线性传播特性。

Result: LTVs在准确性上优于传统提取的任务向量，具有更好的灵活性；发现关键注意力头主导预测，且任务向量在传播过程中主要表现为旋转和缩放。

Conclusion: LTVs不仅为获取有效任务向量提供了实用方法，也为理解上下文学习的机制提供了原则性视角。

Abstract: Large Language Models (LLMs) can perform new tasks from in-context
demonstrations, a phenomenon known as in-context learning (ICL). Recent work
suggests that these demonstrations are compressed into task vectors (TVs),
compact task representations that LLMs exploit for predictions. However, prior
studies typically extract TVs from model outputs or hidden states using
cumbersome and opaque methods, and they rarely elucidate the mechanisms by
which TVs influence computation. In this work, we address both limitations.
First, we propose directly training Learned Task Vectors (LTVs), which surpass
extracted TVs in accuracy and exhibit superior flexibility-acting effectively
at arbitrary layers, positions, and even with ICL prompts. Second, through
systematic analysis, we investigate the mechanistic role of TVs, showing that
at the low level they steer predictions primarily through attention-head OV
circuits, with a small subset of "key heads" most decisive. At a higher level,
we find that despite Transformer nonlinearities, TV propagation is largely
linear: early TVs are rotated toward task-relevant subspaces to improve logits
of relevant labels, while later TVs are predominantly scaled in magnitude.
Taken together, LTVs not only provide a practical approach for obtaining
effective TVs but also offer a principled lens into the mechanistic foundations
of ICL.

</details>


### [474] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: 提出RAG-GUI，一种轻量级视觉语言模型，利用网络教程在推理时提升GUI代理性能，具有良好的泛化和即插即用能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理因训练数据稀缺和任务复杂性（涉及长尾知识）而在实际应用中受限。

Method: 通过监督微调（SFT）进行 warm-start，并采用自指导拒绝采样微调（RSF）进一步优化；在推理时引入网络教程作为外部知识源。

Result: 在三个不同任务上评估，RAG-GUI 持续优于基线模型，在两种模型规模上超越其他推理方法2.6%至13.3%。

Conclusion: RAG-GUI 是一种模型无关的通用插件，能有效增强基于VLM的代理在真实场景中的表现，具备强泛化性和实用的即插即用特性。

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [475] [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
*Zhimeng Luo,Lixin Wu,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文提出了MedIRT，一种基于项目反应理论（IRT）的严格评估框架，用于更准确地评估大型语言模型（LLMs）在医学高风险应用中的表现。通过收集80个不同LLMs在1100道USMLE对齐问题上的新响应数据，MedIRT同时估计模型能力、题目难度和区分度，揭示了传统准确率指标无法捕捉的“尖峰”能力特征，并识别出特定领域的优势模型。此外，该框架可用于审计测试题质量，为LLM在医疗领域的安全可靠部署提供支持。


<details>
  <summary>Details</summary>
Motivation: 传统的准确性指标在评估LLM于高风险医学应用场景时存在不足，无法反映问题特征和提供主题特定洞察，亟需一种更可靠、精细的评估方法。

Method: 基于项目反应理论（IRT），采用单维双参数逻辑模型，对每个医学主题分别建模；前瞻性收集80个LLMs在1100道平衡的USMLE对齐问题上的回答数据，联合估计模型能力、题目难度与区分度。

Result: MedIRT生成比单纯准确率更稳定和细致的性能排名；发现LLMs存在‘尖峰’能力分布，整体排名可能误导；GPT-5在11个领域中8个领先，但Claude-3-opus在社会科学与沟通领域表现最佳；成功识别出有缺陷的测试题目。

Conclusion: MedIRT提供了一种心理测量学上严谨的多维度评估方法，能够更真实地刻画LLM在医学知识上的能力结构，支持更安全、可信的临床AI部署决策。

Abstract: As Large Language Models (LLMs) are increasingly proposed for high-stakes
medical applications, there has emerged a critical need for reliable and
accurate evaluation methodologies. Traditional accuracy metrics fail
inadequately as they neither capture question characteristics nor offer
topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a
rigorous evaluation framework grounded in Item Response Theory (IRT), the gold
standard in high-stakes educational testing. Unlike previous research relying
on archival data, we prospectively gathered fresh responses from 80 diverse
LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one
unidimensional two-parameter logistic IRT model per topic, we estimate LLM's
latent model ability jointly with question difficulty and discrimination,
yielding more stable and nuanced performance rankings than accuracy alone.
Notably, we identify distinctive ``spiky'' ability profiles, where overall
rankings can be misleading due to highly specialized model abilities. While
\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was
outperformed in Social Science and Communication by \texttt{Claude-3-opus},
demonstrating that even an overall 23rd-ranked model can hold the top spot for
specific competencies. Furthermore, we demonstrate IRT's utility in auditing
benchmarks by identifying flawed questions. We synthesize these findings into a
practical decision-support framework that integrates our multi-factor
competency profiles with operational metrics. This work establishes a robust,
psychometrically grounded methodology essential for the safe, effective, and
trustworthy deployment of LLMs in healthcare.

</details>


### [476] [PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution](https://arxiv.org/abs/2509.24189)
*Luyang Zhang,Siyuan Peng,Jialu Wang,Shichao Zhu,Beibei Li,Zhongcun Wang,Guangmou Pan,Yan Li,Song Yang*

Main category: cs.CL

TL;DR: 提出了一种名为Preference Evolution Tracking (PET)的框架，通过推断用户在偏好聚类格上的动态概率分布，实现透明且可解释的用户偏好建模，在多个数据集上显著提升了排序性能，尤其在长尾内容推荐中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的端到端生成方法在用户偏好预测中存在个性化不足、决策过程不透明和流行度偏差等问题，难以实现可解释和公平的推荐。

Method: 将用户偏好演化建模为在稳定且可解释的偏好聚类格上的动态概率分布，利用logit-probing和生成式分类技术推断用户的偏好分布，从而实现透明的偏好学习。

Result: 在Yelp和MovieLens等公开基准上，PET相比直接生成基线在NDCG指标上最高提升40%；在真实短视频平台大规模数据集上，对长尾内容的推荐效果优于当前SOTA生产模型7倍。

Conclusion: PET将用户画像从直接生成偏好列表转变为透明的概率分布映射，为更可解释、公平和多样化的个性化系统提供了新路径。

Abstract: Understanding how user preference evolves over time is a fundamental
challenge central to modern digital ecosystems, for which Large Language Models
(LLMs) are an increasingly prominent and popular approach due to their ability
to comprehend the rich semantic context within behavioral data. A common
practice is to use LLMs to predict a user's next action by directly generating
a ranked list of preferred items. Although effective for short-term prediction,
the end-to-end generation paradigm inherently limits personalization. Its
opaque decision-making process obscures holistic user profiling and exacerbates
popularity bias. To address these limitations, we propose Preference Evolution
Tracking (PET), a framework that reframes the task as inferring a dynamic
probability distribution over a stable and interpretable lattice of preference
clusters. By applying logit-probing and generative classification techniques,
PET infers a user's preference as a probability distribution, enabling
transparent preference learning. On public benchmarks (Yelp, MovieLens), PET
improves ranking quality by up to 40% in NDCG over direct generation baselines.
On a large-scale, real-world dataset from a short-video platform, it excels at
ranking long-tail contents, significantly outperforming a SOTA production model
by 7 times in the NDCG score. Ultimately, PET transforms the user profile model
from direct preference list generation to a transparent distributional
preference mapping, paving the way for more explainable, fair, and diverse
personalization systems.

</details>


### [477] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: 该论文提出利用语言学置信度（LC），即通过模型使用模糊表达（如“可能”、“也许”）来表达不确定性，作为大语言模型（LLM）中可靠置信度估计的轻量且以人为中心的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法存在实际障碍：logits常不可见、多采样计算成本高、数值化不确定性表达不符合自然交流方式。因此需要一种更实用、贴近人类沟通的替代方案。

Method: 1) 构建首个大规模、多样化的带有人工标注置信度分数的模糊表达数据集；2) 提出一个轻量级映射器将模糊词转换为置信度分数；3) 在现代LLM和问答基准上首次系统研究LC表现；4) 引入微调框架以提升LC可靠性。

Result: 发现大多数LLM在表达可靠LC方面表现不佳，但通过精心设计的提示可实现良好的校准性和区分性，微调后进一步提升性能。

Conclusion: 语言学置信度是一种可扩展、高效且与人类对齐的LLM不确定性估计方法，值得深入探索。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [478] [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
*Gaurav Srivastava,Aafiya Hussain,Zhenyu Bi,Swastik Roy,Priya Pitre,Meng Lu,Morteza Ziyadi,Xuan Wang*

Main category: cs.CL

TL;DR: 本文提出了BeyondBench，一个通过算法生成数学问题的评估框架，以避免传统基准测试因训练数据污染而导致的评估偏差。该框架包含44个任务共117种变体，覆盖从基础算术到NP完全问题的不同难度级别，确保每个问题都是新鲜且无污染的。在101个语言模型上的实验表明，随着问题复杂度增加，模型表现显著下降，尤其是在高难度任务中，即使大型模型准确率也较低，且缺乏工具使用时性能进一步恶化。


<details>
  <summary>Details</summary>
Motivation: 随着互联网数据被广泛用于训练语言模型，传统静态基准测试面临训练数据污染的风险，导致难以判断模型是真正推理还是仅仅记忆答案。因此，需要一种不受数据污染影响的公平评估方法。

Method: 提出BeyondBench框架，采用算法实时生成具有数学基础的问题，问题空间组合超过10^15种唯一实例，并通过数学证明进行确定性验证。框架分为三个难度等级：简单套件（基础算术与统计）、中等套件（序列模式与推理）和困难套件（NP完全与约束满足问题），并对101个语言模型进行评估。

Result: 在101个语言模型上的评估显示，模型在复杂问题上表现显著下降。例如，在困难套件中，Gemini-2.5-pro、Llama-3.3-70B和Qwen2.5-72B的平均准确率分别为56.38%、26.91%和33.60%。此外，不使用工具时，GPT-5系列模型在困难任务上的准确率大幅下降，降幅达16.81%至47.59%。

Conclusion: BeyondBench提供了一种抗数据污染的语言模型评估方式，揭示了当前语言模型在面对复杂算法问题时的推理能力局限，尤其在无工具辅助情况下表现更差，强调了对真实推理能力评估的重要性。

Abstract: Evaluating language models fairly is becoming harder as static benchmarks
available on the internet risk contamination by training data. This makes it
unclear whether models are truly reasoning or just recalling answers. In this
paper, we introduce BeyondBench, an evaluation framework that avoids this
problem by using algorithmic problem generation. Unlike traditional benchmarks
that risk contamination from internet-scale training data, BeyondBench creates
mathematically grounded problems on the fly, ensuring each test remains fresh
and uncontaminated. Our framework covers 44 algorithmic tasks with a total of
117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)
for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)
for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68
variations) tackling NP-complete and constraint satisfaction problems. Each
task generates problems from a combinatorial space larger than 10^15 unique
instances, with solutions verified deterministically by mathematical proofs. We
evaluated 101 language models, including 85 open-source and 16 closed-source
models, spanning sizes from 0.5B to 141B parameters and multiple quantization
schemes. Our results show consistent reasoning deficiencies across model
families, with performance degrading sharply as problem complexity increases
from polynomial to exponential. In our Hard Suite evaluations, models such as
Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of
56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance
drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano
showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our
leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/

</details>


### [479] [ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG](https://arxiv.org/abs/2509.24212)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: ScenarioBench是一个基于策略、追踪感知的基准，用于评估合规场景下的Text-to-SQL和检索增强生成系统，强调决策依据的可验证性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL和RAG基准缺乏对决策依据的严格溯源和可审计性，难以评估系统在合规场景中的实际表现。

Method: 设计包含黄金标准决策、最小见证轨迹、管辖条款集和规范SQL的YAML场景，要求系统使用政策条款ID进行输出解释，并通过多维度指标进行端到端评分。

Result: 提供了决策准确率、轨迹质量、检索效果、SQL正确性、政策覆盖率、延迟和解释幻觉率等综合评估结果，并提出SDI和SDI-R指标来衡量场景难度和检索预算下的性能。

Conclusion: ScenarioBench通过严格的 grounding 和 no-peek 规则，将评估重点从单纯输出正确性转向解释质量和时间约束下的实际表现，提升了合规场景下系统评估的可信度和实用性。

Abstract: ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating
Text-to-SQL and retrieval-augmented generation in compliance contexts. Each
YAML scenario includes a no-peek gold-standard package with the expected
decision, a minimal witness trace, the governing clause set, and the canonical
SQL, enabling end-to-end scoring of both what a system decides and why. Systems
must justify outputs using clause IDs from the same policy canon, making
explanations falsifiable and audit-ready. The evaluator reports decision
accuracy, trace quality (completeness, correctness, order), retrieval
effectiveness, SQL correctness via result-set equivalence, policy coverage,
latency, and an explanation-hallucination rate. A normalized Scenario
Difficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while
accounting for retrieval difficulty and time. Compared with prior Text-to-SQL
or KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level
evidence under strict grounding and no-peek rules, shifting gains toward
justification quality under explicit time budgets.

</details>


### [480] [MoVa: Towards Generalizable Classification of Human Morals and Values](https://arxiv.org/abs/2509.24216)
*Ziyu Chen,Junfei Sun,Chenxi Li,Tuan Dung Nguyen,Jing Yao,Xiaoyuan Yi,Xing Xie,Chenhao Tan,Lexing Xie*

Main category: cs.CL

TL;DR: 本文提出了MoVa，一个用于人类道德和价值观分类的资源套件，包括16个标注数据集、基于四种理论框架的基准测试结果、一种轻量级LLM提示策略以及一个新的心理调查评估应用。


<details>
  <summary>Details</summary>
Motivation: 研究人员在分析语言中的人类道德和价值观时，面临理论框架和数据多样性的挑战，缺乏通用且易于使用的工具。

Method: 构建了一个包含多框架标注数据集的资源套件MoVa，提出一种名为all@once的轻量级LLM提示策略，通过同时评分所有相关概念实现多标签分类，并进行跨领域和框架的性能评估。

Result: 所提出的all@once提示策略在多个领域和框架上优于微调模型，MoVa资源套件支持对人类和机器交流的细粒度解读，并有助于机器行为对齐研究。

Conclusion: MoVa提供了一套系统化、可推广的工具，有效支持跨理论框架的道德与价值观分类，具有在心理学和人工智能领域广泛应用的潜力。

Abstract: Identifying human morals and values embedded in language is essential to
empirical studies of communication. However, researchers often face substantial
difficulty navigating the diversity of theoretical frameworks and data
available for their analysis. Here, we contribute MoVa, a well-documented suite
of resources for generalizable classification of human morals and values,
consisting of (1) 16 labeled datasets and benchmarking results from four
theoretically-grounded frameworks; (2) a lightweight LLM prompting strategy
that outperforms fine-tuned models across multiple domains and frameworks; and
(3) a new application that helps evaluate psychological surveys. In practice,
we specifically recommend a classification strategy, all@once, that scores all
related concepts simultaneously, resembling the well-known multi-label
classifier chain. The data and methods in MoVa can facilitate many fine-grained
interpretations of human and machine communication, with potential implications
for the alignment of machine behavior.

</details>


### [481] [Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents](https://arxiv.org/abs/2509.24229)
*Kangxu Wang,Ze Chen,Chengcheng Wei,Jiewen Zheng,Jiarong He,Max Gao*

Main category: cs.CL

TL;DR: 本文介绍了opdainlp团队在CPDC 2025挑战赛GPU赛道中的解决方案，使用Qwen3-14B结合LoRA微调与模型融合，在三项任务中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 在资源和推理时间受限的情况下，构建符合角色设定、游戏世界观并支持函数调用的游戏中对话AI。

Method: 采用基于竞赛数据合成的数据，使用Qwen3-14B模型结合LoRA微调和模型融合，推理时集成多个LoRA适配器（分别处理工具调用、带/不带工具结果的回复生成），并通过vLLM实现MultiLoRA推理。

Result: 在GPU赛道中，Task 1和Task 3获得第一名，Task 2获得第二名。

Conclusion: 该方法在效果与效率之间取得了良好平衡，验证了多LoRA适配器融合策略在复杂对话任务中的有效性。

Abstract: This paper presents the opdainlp team's solution for the GPU track of the
CPDC 2025 challenge. The challenge consists of three tasks, aiming to build an
in-game conversational AI that adheres to character personas, aligns with the
game's worldview, and supports function calling. Considering both effectiveness
and resource/time constraints during inference, we synthesized data for some of
the tasks based on the datasets provided by the competition organizers. We
employed Qwen3-14B with LoRA fine-tuning and model fusion, and utilized a base
model integrated with multiple LoRA adapters during inference. Specifically, in
the competition, we used three distinct LoRA adapters to handle tool calling,
response generation with tool call results, and response generation without
tool call results, respectively. MultiLoRA inference was implemented using
vLLM. Our solution achieved the first place in Task 1 and Task 3, and the
second place in Task 2 of the GPU track.

</details>


### [482] [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
*Xiaohe Bo,Rui Li,Zexu Sun,Quanyu Dai,Zeyu Zhang,Zihang Tian,Xu Chen,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文提出了MetaTuner，一种联合集成提示优化与微调的新型大语言模型训练框架，通过共享编码层和监督正则化损失实现提示与参数的协同优化。


<details>
  <summary>Details</summary>
Motivation: 提示优化与微调通常被独立研究，其协同潜力未被充分挖掘，本文旨在探索二者结合的可能性以提升大模型性能。

Method: 设计两个神经网络分别生成提示和模型参数，并共享底层编码层以促进知识迁移；引入监督正则化损失来统一优化离散的提示空间与连续的参数空间。

Result: 在多个基准任务上的实验表明，该方法显著优于单独使用提示优化或微调的基线方法。

Conclusion: MetaTuner有效融合了提示优化与微调的优势，验证了二者协同训练的潜力，为大模型训练提供了新思路。

Abstract: Prompt optimization and fine-tuning are two major approaches to improve the
performance of Large Language Models (LLMs). They enhance the capabilities of
LLMs from complementary perspectives: the former through explicit natural
language, and the latter through implicit parameter updates. However, prior
work has typically studied them in isolation, leaving their synergistic
potential largely underexplored. To bridge this gap, in this paper, we
introduce MetaTuner, a novel framework that jointly integrates prompt
optimization and fine-tuning for LLM training. Specifically, we introduce two
neural networks to generate prompts and parameters, respectively, while
allowing them to share a common bottom encoding layer to enable knowledge
sharing. By the guidance of the final supervised signals, our framework is
optimized to discover the optimal combinations between the prompts and
parameters. Given that prompt learning involves discrete optimization while
fine-tuning operates in a continuous parameter space, we design a supervised
regularization loss to train our framework effectively. Extensive experiments
across diverse benchmarks show that our method consistently outperforms the
baselines.

</details>


### [483] [MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2509.24253)
*Yuelyu Ji*

Main category: cs.CL

TL;DR: 提出MRAG-Suite，一个用于多模态检索增强生成（Visual RAG）的诊断评估平台，整合多个基准并引入难度与歧义感知评估方法，揭示了系统在困难和模糊查询下的性能下降及幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法未能系统考虑查询难度和歧义性对Visual RAG系统的影响。

Method: 构建MRAG-Suite平台，集成多个多模态基准，并设计基于难度和歧义感知的过滤策略，以及MM-RAGChecker诊断工具。

Result: 实验显示在困难和模糊查询下准确率显著下降，且存在普遍幻觉；MM-RAGChecker能有效诊断这些问题。

Conclusion: MRAG-Suite能更全面地评估Visual RAG系统，有助于未来改进其鲁棒性和可靠性。

Abstract: Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances
question answering by integrating visual and textual evidence. Yet, current
evaluations fail to systematically account for query difficulty and ambiguity.
We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse
multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce
difficulty-based and ambiguity-aware filtering strategies, alongside
MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate
substantial accuracy reductions under difficult and ambiguous queries,
highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses
these issues, guiding future improvements in Visual RAG systems.

</details>


### [484] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了SimuHome，一个基于Matter协议的时间加速智能家居模拟环境，用于训练和评估具备多步推理、工具调用和状态验证能力的大型语言模型代理。同时提供了包含600个任务的基准测试，实验表明现有模型在隐含意图推断和时序调度等任务上表现不佳，凸显了状态验证和时间协调机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代理在真实智能家居环境中面临缺乏高保真模拟平台和挑战性基准测试的问题，难以有效处理用户潜在意图、时间依赖性和设备约束等复杂需求。

Method: 构建了一个基于Matter协议的智能家居模拟器SimuHome，支持API调用和环境变量变化，并设计了一个包含12类用户查询、共600个任务的评估基准，在统一的ReAct框架下对11种代理模型进行评测。

Result: 实验结果显示，即使表现最好的GPT-4.1模型在该基准上的成功率也仅为54%，模型在隐含意图推断、状态验证尤其是时序调度方面存在显著困难。

Conclusion: 开发可靠的智能家居代理需要能够通过工具验证当前状态并协调时间相关操作的新方法，SimuHome为这一方向的研究提供了高保真模拟环境和具有挑战性的评估基准。

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [485] [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
*Yu-Che Tsai,Kuan-Yu Chen,Yuan-Chi Li,Yuan-Hao Chen,Ching-Yu Tsai,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出GIRCSE框架，利用生成式迭代优化对比句子嵌入，超越现有LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的嵌入方法多采用编码器-only范式，忽视了LLM的生成能力，难以捕捉深层语义。

Method: 通过自回归生成软token序列，在对比学习目标下进行迭代优化，提出迭代对比优化（ICR）目标来逐步提升表示质量。

Result: 在MTEB基准和指令跟随任务上优于强基线，且表现出测试时生成更多token可持续提升性能的涌现特性。

Conclusion: 生成式迭代优化是一种新的、有效的表示学习范式。

Abstract: Existing large language model (LLM)-based embeddings typically adopt an
encoder-only paradigm, treating LLMs as static feature extractors and
overlooking their core generative strengths. We introduce GIRCSE (Generative
Iterative Refinement for Contrastive Sentence Embeddings), a novel framework
that leverages autoregressive generation to iteratively refine semantic
representations. By producing sequences of soft tokens optimized under
contrastive objective, GIRCSE captures latent concepts and implicit semantics
that encoder-only methods often miss. To guide this process, we propose an
Iterative Contrastive Refinement (ICR) objective that encourages each
refinement step to yield better representations. Extensive experiments show
that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB
benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an
emergent test-time scaling property: generating more tokens at inference
steadily improves embedding quality. Our results establish generative iterative
refinement as a new paradigm for representation learning.

</details>


### [486] [LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research](https://arxiv.org/abs/2509.24294)
*Xinyu Pi,Qisen Yang,Chuong Nguyen*

Main category: cs.CL

TL;DR: LOGOS是一个端到端的自动化框架，利用大语言模型和语义聚类等技术，将定性研究中的扎根理论流程完全自动化，显著提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统扎根理论依赖专家手动编码，难以扩展；现有计算工具未能实现真正自动化，限制了定性研究的效率和普及。

Method: 提出LOGOS框架，结合LLM驱动的编码、语义聚类、图推理和迭代优化机制，实现从原始文本到结构化层级理论的全自动转化，并设计五维评估指标和训练-测试分割协议进行公平评估。

Result: 在五个不同数据集上，LOGOS持续优于强基线方法，在一个复杂数据集上与专家构建的模式达到88.2%的一致性。

Conclusion: LOGOS为定性研究提供了可扩展且不失理论深度的自动化路径，有望推动扎根理论的民主化和广泛应用。

Abstract: Grounded theory offers deep insights from qualitative data, but its reliance
on expert-intensive manual coding presents a major scalability bottleneck.
Current computational tools stop short of true automation, keeping researchers
firmly in the loop. We introduce LOGOS, a novel, end-to-end framework that
fully automates the grounded theory workflow, transforming raw text into a
structured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic
clustering, graph reasoning, and a novel iterative refinement process to build
highly reusable codebooks. To ensure fair comparison, we also introduce a
principled 5-dimensional metric and a train-test split protocol for
standardized, unbiased evaluation. Across five diverse corpora, LOGOS
consistently outperforms strong baselines and achieves a remarkable $88.2\%$
alignment with an expert-developed schema on a complex dataset. LOGOS
demonstrates a powerful new path to democratize and scale qualitative research
without sacrificing theoretical nuance.

</details>


### [487] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 本文研究了扩散大语言模型（dLLMs）在迭代生成过程中面临的新颖安全漏洞，提出了一种无需训练的防御框架DiffuGuard，通过随机退火重掩码和块级审计修复机制有效降低 jailbreak 攻击成功率。


<details>
  <summary>Details</summary>
Motivation: dLLMs因其独特的生成机制面临与传统自回归模型不同的安全威胁，现有防御方法难以应对，亟需针对其生成动态设计新型防护机制。

Method: 从单步内（intra-step）和跨步骤（inter-step）两个维度分析dLLM的脆弱性，提出Denoising-path Dependence现象，并设计DiffuGuard框架，包含Stochastic Annealing Remasking和Block-level Audit and Repair两个阶段进行防御。

Result: 在四个dLLM上实验表明，DiffuGuard将六种典型jailbreak攻击的平均攻击成功率从47.9%降至14.7%，同时保持了模型生成性能和效率。

Conclusion: dLLMs虽存在新的安全隐患，但具备内在安全潜力，DiffuGuard通过解耦生成过程中的风险控制，为dLLM提供了高效、通用且无需训练的防御方案。

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [488] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种将纯文本问答对（TQA）转化为高质量多模态问答对（MMQA）的框架，并构建了用于评估生成与理解模型的基准。通过引入Q-Mirror代理系统，实现了MMQA的闭环迭代优化，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 手动创建高质量多模态科学推理基准成本高、难以扩展，因此需要自动化方法将现有的文本问答对转化为多模态形式，以推动大模型在科学推理方面的发展。

Method: 提出了TQA-to-MMQA转换框架，建立了多维度的MMQA质量评估标准，构建了两个大规模基准测试集，并开发了Q-Mirror代理系统，将生成与评估结合形成闭环迭代机制。

Result: 实验表明现有最先进模型在MMQA生成上仍有明显不足；顶级理解模型在质量评估上与人类判断高度一致；Q-Mirror系统将平均得分从78.90提升至85.22，通过率从72%提高到95%。

Conclusion: 该研究为大规模构建高质量科学多模态基准提供了可行路径，验证了生成与评估联合优化的有效性，推动了自动化基准构建的发展。

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [489] [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
*Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型中内在表达和提示表达的价值机制，发现两者既有共享成分也有独特成分，导致不同的可引导性和响应多样性。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在价值对齐和角色引导中的内在与提示价值表达机制的异同。

Method: 通过残差流中的价值向量和MLP中的价值神经元分析内在与提示价值机制。

Result: 发现两种机制部分共享关键成分，但各自独有的成分导致提示表达更易引导，而内在表达更具词汇多样性。

Conclusion: 内在和提示价值机制具有部分重叠但功能不同的结构基础，影响模型的行为特性。

Abstract: Large language models (LLMs) can express different values in two distinct
ways: (1) intrinsic expression, reflecting the model's inherent values learned
during training, and (2) prompted expression, elicited by explicit prompts.
Given their widespread use in value alignment and persona steering, it is
paramount to clearly understand their underlying mechanisms, particularly
whether they mostly overlap (as one might expect) or rely on substantially
different mechanisms, but this remains largely understudied. We analyze this at
the mechanistic level using two approaches: (1) value vectors, feature
directions representing value mechanisms extracted from the residual stream,
and (2) value neurons, MLP neurons that contribute to value expressions. We
demonstrate that intrinsic and prompted value mechanisms partly share common
components that are crucial for inducing value expression, but also possess
unique elements that manifest in different ways. As a result, these mechanisms
lead to different degrees of value steerability (prompted > intrinsic) and
response diversity (intrinsic > prompted). In particular, components unique to
the intrinsic mechanism seem to promote lexical diversity in responses, whereas
those specific to the prompted mechanism primarily strengthen instruction
following, taking effect even in distant tasks like jailbreaking.

</details>


### [490] [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](https://arxiv.org/abs/2509.24322)
*Yuntao Shou,Tao Meng,Wei Ai,Keqin Li*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLMs）和多模态大语言模型（MLLMs）在情感识别与推理中的应用，涵盖了模型架构、数据集和性能基准，并指出了关键挑战和未来研究方向。据作者所知，这是首次全面调研MLLMs与多模态情感识别与推理交叉领域的论文。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在情感识别与推理方面取得了进展，但该领域仍缺乏系统性综述，亟需整合最新发展以指导后续研究。

Method: 对现有LLMs和MLLMs在情感识别与推理中的应用进行综合调查，包括模型架构、数据集和性能基准的梳理与分析。

Result: 总结了当前方法的发展现状，提出了主要挑战和未来研究方向，并提供了GitHub资源库作为研究参考。

Conclusion: 该综述为多模态情感识别与推理领域提供了权威参考和实用洞见，有助于推动该领域进一步发展。

Abstract: In recent years, large language models (LLMs) have driven major advances in
language understanding, marking a significant step toward artificial general
intelligence (AGI). With increasing demands for higher-level semantics and
cross-modal fusion, multimodal large language models (MLLMs) have emerged,
integrating diverse information sources (e.g., text, vision, and audio) to
enhance modeling and reasoning in complex scenarios. In AI for Science,
multimodal emotion recognition and reasoning has become a rapidly growing
frontier. While LLMs and MLLMs have achieved notable progress in this area, the
field still lacks a systematic review that consolidates recent developments. To
address this gap, this paper provides a comprehensive survey of LLMs and MLLMs
for emotion recognition and reasoning, covering model architectures, datasets,
and performance benchmarks. We further highlight key challenges and outline
future research directions, aiming to offer researchers both an authoritative
reference and practical insights for advancing this domain. To the best of our
knowledge, this paper is the first attempt to comprehensively survey the
intersection of MLLMs with multimodal emotion recognition and reasoning. The
summary of existing methods mentioned is in our Github:
\href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.

</details>


### [491] [Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding](https://arxiv.org/abs/2509.24328)
*Sungkyun Kim,Jaemin Kim,Dogyung Yoon,Jiho Shin,Junyeol Lee,Jiwon Seo*

Main category: cs.CL

TL;DR: 提出了一种名为Speculative Verification (SV) 的方法，通过动态预测推测准确率并调整验证长度，提升大语言模型解码的吞吐量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测性解码（SD）在推测准确率低时会产生大量被拒绝的令牌，导致开销增加，尤其在大批量情况下效果受限。

Method: 引入一个小型辅助模型（伴生模型）来估计草稿模型与目标模型分布之间的对齐程度，并基于信息增益动态调整验证长度，从而优化验证决策。

Result: 在多种LLM和任务上实验表明，SV在各种批量大小下均优于标准解码和SD，大批量场景下平均加速1.4倍，最高可达2倍。

Conclusion: SV能有效减少无效计算，提升解码效率，无需修改原有模型，兼容现有SD变体，具有良好的可扩展性和实用价值。

Abstract: LLMs have low GPU efficiency and high latency due to autoregressive decoding.
Speculative decoding (SD) mitigates this using a small draft model to
speculatively generate multiple tokens, which are then verified in parallel by
a target model. However, when speculation accuracy is low, the overhead from
rejected tokens can offset the benefits, limiting SD's effectiveness,
especially at large batch sizes. To address this, we propose Speculative
Verification (SV), an efficient augmentation to SD that dynamically predicts
speculation accuracy and adapts the verification length to maximize throughput.
SV introduces a companion model - a small auxiliary model similar in size to
the draft model - to estimate the alignment between draft and target model
distributions. By maximizing the information gain from quantifying this
alignment, SV refines verification decisions, reducing wasted computation on
rejected tokens and improving decoding efficiency. Moreover, SV requires no
modifications to the draft or target models and is compatible with existing SD
variants. We extensively evaluated SV on publicly available LLMs across three
NLP tasks using nine combinations of draft, companion, and target models,
including 13B-72B target models and three types of variations: base (no
finetuning), instruction-tuned, and task fine-tuned. Across all experiments and
batch sizes (4-80), SV consistently outperforms both SD and standard decoding
with the target model. It improves SD performance by up to 2$\times$, with an
average speedup of 1.4 $\times$ in large-batch settings (batch sizes 32-80).
These results demonstrate SV's robustness, scalability, and practical utility
for efficient LLM inference.

</details>


### [492] [AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment](https://arxiv.org/abs/2509.24338)
*Mengyu Bu,Shaolei Zhang,Zhongjun He,Hua Wu,Yang Feng*

Main category: cs.CL

TL;DR: 提出AlignX，一种两阶段表示级框架，用于提升预训练多语言大模型的多语言性能，通过语义对齐和指令微调增强跨语言生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非主导语言上的表现和跨语言对齐效果较差，大规模微调难以实现精确对齐和有效知识迁移。

Method: 第一阶段进行多语言语义对齐与语言特征融合以对齐表示；第二阶段通过多语言指令微调激发模型多语言能力。

Result: 在多个预训练大模型上验证了AlignX能显著提升多语言通用和跨语言生成能力，使多语言表示更接近，改善跨语言对齐。

Conclusion: AlignX有效缩小了多语言性能差距，为提升非主导语言表现提供了高效解决方案。

Abstract: Multilingual large language models (LLMs) possess impressive multilingual
understanding and generation capabilities. However, their performance and
cross-lingual alignment often lag for non-dominant languages. A common solution
is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but
such approaches often lead to imprecise alignment and suboptimal knowledge
transfer, struggling with limited improvements across languages. In this paper,
we propose AlignX to bridge the multilingual performance gap, which is a
two-stage representation-level framework for enhancing multilingual performance
of pre-trained LLMs. In the first stage, we align multilingual representations
with multilingual semantic alignment and language feature integration. In the
second stage, we stimulate the multilingual capability of LLMs via multilingual
instruction fine-tuning. Experimental results on several pre-trained LLMs
demonstrate that our approach enhances LLMs' multilingual general and
cross-lingual generation capability. Further analysis indicates that AlignX
brings the multilingual representations closer and improves the cross-lingual
alignment.

</details>


### [493] [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
*Matthew Theodore Roque,Dan John Velasco*

Main category: cs.CL

TL;DR: 研究在数据受限情况下，通过文本简化和复杂度排序的课程学习对语言模型预训练的影响，发现添加简化文本能提升表示质量，小模型受益于由易到难的顺序，大模型则更适合交错排序。


<details>
  <summary>Details</summary>
Motivation: 在数据受限的预训练场景中，训练数据顺序和同一文本不同版本的使用影响尚不明确，本文旨在探索文本复杂度排序和数据增强（简化）对表示质量的作用。

Method: 基于人类编写段落与其LLM简化版本对齐的平行语料库，设计四种数据调度策略：重复暴露、低到高复杂度、高到低复杂度、交错混合，并从微调样本效率和零样本性能（语言知识、实体追踪、世界知识、常识推理）评估模型表现。

Result: 加入简化文本相比重复使用原始数据能提升微调和零样本性能；小模型在低到高复杂度排序下表现更好，大模型则在交错排序下更优。

Conclusion: 在数据受限的预训练中，引入简化文本并根据模型规模选择合适的数据顺序可有效提升语言模型的表示能力。

Abstract: Most studies on language model pretraining focus on large datasets, leaving
open questions about optimization in data-constrained settings. In such
settings, the effects of training data order and of including alternative
versions of the same text remain underexplored. We address this by studying
curriculum learning in pretraining, focusing on text-complexity ordering and
data augmentation via simplification. We ask: (1) Does simplifying texts
enhance representation quality more than reusing the original data? and (2)
Does ordering data by text complexity yield better representations? To answer,
we build on a pair of parallel corpora where human-written paragraphs are
aligned with LLM-simplified variants, and test four data schedules: repeated
exposure, low-to-high complexity, high-to-low, and interleaved. We analyze
models' representation quality from a sample efficiency perspective via
fine-tuning, as well as its zero-shot performance on linguistic knowledge,
entity tracking, world knowledge, and commonsense reasoning. Our findings show
that adding simplified data improves fine-tuning and zero-shot performance over
a repeated-exposure baseline: smaller models benefit from low-to-high
complexity, while larger models perform better with interleaved ordering.

</details>


### [494] [Reinforcement Mid-Training](https://arxiv.org/abs/2509.24375)
*Yijun Tian,Shaoyu Chen,Zhichao Xu,Yawei Wang,Jinhe Bi,Peng Han,Wei Wang*

Main category: cs.CL

TL;DR: 提出强化中间训练（RMT）框架，解决大模型训练中的推理效率、令牌熵分布不平衡和信息利用不足问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型训练流程缺乏中间强化阶段，导致推理效率低、令牌信息利用不充分，限制了整体性能提升。

Method: 提出RMT框架，包含动态令牌预算机制、基于课程的自适应采样方法和结合强化学习与下一令牌预测的双重训练策略。

Result: 实验显示RMT相比最先进方法在语言建模中以仅21%推理长度获得高达+64.91%性能提升，并使后续后训练在数学领域提升达+18.76%。

Conclusion: 强化中间训练是有效且必要的新阶段，RMT框架能显著提升训练效率与模型性能，具有广泛应用前景。

Abstract: The development of state-of-the-art large language models is commonly
understood as a two-stage process involving pre-training and post-training. We
point out the need for an additional intermediate stage called reinforcement
mid-training with potential for strong performance gains. In this paper, we
formally define the problem and identify three key challenges: (1) inefficient
training due to excessive reasoning steps, (2) disregard of the imbalanced
token entropy distribution, and (3) underutilization of token information. To
address these challenges, we propose RMT, a framework for efficient, adaptive,
and unified reinforcement mid-training with various innovative components. In
particular, we first introduce a dynamic token budget mechanism that constrains
unnecessary reasoning steps and mitigates model overthinking. Next, we design a
curriculum-based adaptive sampling method that fosters a progressive learning
trajectory from easy to hard tokens. Finally, we present a dual training
strategy that combines reinforcement learning with next-token prediction,
ensuring targeted learning on key tokens and full exploitation of all token
information. Extensive experiments demonstrate the superiority of RMT over
state-of-the-art methods, achieving up to +64.91% performance improvement with
only 21% of the reasoning length in language modeling. We also show that
checkpoints obtained after reinforcement mid-training can benefit the
subsequent post-training, yielding up to +18.76% improvement in the
mathematical domain.

</details>


### [495] [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
*Langqi Yang,Tianhang Zheng,Kedong Xiu,Yixuan Chen,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了HarmMetric Eval，一个用于评估有害性度量和判断器的综合基准，发现传统指标METEOR和ROUGE-1在评估模型输出有害性方面优于基于大语言模型的判断器。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱攻击评估缺乏系统性基准来衡量不同有害性度量和判断器的有效性，影响了越狱成功率等风险评估的可信度。

Method: 构建了一个包含代表性有害提示及多样化响应的高质量数据集，并设计灵活评分机制，支持多种度量与判断器的细粒度评估。

Result: 实验表明，METEOR和ROUGE-1等传统指标在有害性评估中表现优于当前流行的基于大语言模型的判断器。

Conclusion: HarmMetric Eval为评估有害性提供了可靠基准，挑战了大语言模型在该任务上必然优越的普遍认知。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe deployment, yet jailbreak attacks can subvert this alignment to
elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak
attacks has emerged, accompanied by diverse metrics and judges to assess the
harmfulness of the LLM outputs. However, the absence of a systematic benchmark
to assess the quality and effectiveness of these metrics and judges undermines
the credibility of the reported jailbreak effectiveness and other risks. To
address this gap, we introduce HarmMetric Eval, a comprehensive benchmark
designed to support both overall and fine-grained evaluation of harmfulness
metrics and judges. Our benchmark includes a high-quality dataset of
representative harmful prompts paired with diverse harmful and non-harmful
model responses, alongside a flexible scoring mechanism compatible with various
metrics and judges. With HarmMetric Eval, our extensive experiments uncover a
surprising result: two conventional metrics--METEOR and ROUGE-1--outperform
LLM-based judges in evaluating the harmfulness of model responses, challenging
prevailing beliefs about LLMs' superiority in this domain. Our dataset is
publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,
and the code is available at
https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

</details>


### [496] [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
*Fengqi Zhu,Zebin You,Yipeng Xing,Zenan Huang,Lin Liu,Yihong Zhuang,Guoshan Lu,Kangyu Wang,Xudong Wang,Lanning Wei,Hongrui Guo,Jiaqi Hu,Wentao Ye,Tieyuan Chen,Chenchen Li,Chengfu Tang,Haibo Feng,Jun Hu,Jun Zhou,Xiaolu Zhang,Zhenzhong Lan,Junbo Zhao,Da Zheng,Chongxuan Li,Jianguo Li,Ji-Rong Wen*

Main category: cs.CL

TL;DR: LLaDA-MoE是一种基于Mixture-of-Experts架构的大型语言扩散模型，训练于约20T token，仅激活1.4B参数即实现优于更大规模扩散模型的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过稀疏化MoE架构在保持高效推理的同时提升扩散语言模型的性能，探索其在减少计算开销方面的潜力。

Method: 从头训练一个具有7B总参数、仅激活1.4B参数的MoE架构扩散语言模型，并在多个基准上进行评估，同时发布指令调优版本。

Result: LLaDA-MoE在多个基准上超越了LLaDA、LLaDA 1.5和Dream等现有扩散语言模型；其指令调优版本LLaDA-MoE-7B-A1B-Instruct在知识理解、代码生成、数学推理等方面表现接近Qwen2.5-3B-Instruct。

Conclusion: 将稀疏MoE架构引入掩码扩散语言模型的训练是有效的，能够在低激活参数下实现高性能，为未来扩散语言模型的研究提供了新方向。

Abstract: We introduce LLaDA-MoE, a large language diffusion model with the
Mixture-of-Experts (MoE) architecture, trained from scratch on approximately
20T tokens. LLaDA-MoE achieves competitive performance with significantly
reduced computational overhead by maintaining a 7B-parameter capacity while
activating only 1.4B parameters during inference. Our empirical evaluation
reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion
language models with larger parameters, surpassing previous diffusion language
models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The
instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities
comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,
mathematical reasoning, agent and alignment tasks, despite using fewer active
parameters. Our results show that integrating a sparse MoE architecture into
the training objective of masked diffusion language models still brings out
MoE's strengths under efficient inference with few active parameters, and opens
ample room for further exploration of diffusion language models. LLaDA-MoE
models are available at Huggingface.

</details>


### [497] [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
*Pengfei Wang,Baolin Sun,Xuemei Dong,Yaxun Dai,Hongwei Yuan,Mengdie Chu,Yingqi Gao,Xiang Qi,Peng Zhang,Ying Yan*

Main category: cs.CL

TL;DR: 本文提出了Agentar-Scale-SQL，一种通过协调测试时扩展策略提升Text-to-SQL性能的新框架，在BIRD基准上达到81.67%执行准确率，位居榜首。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在复杂基准上仍远落后于人类专家，且当前测试时扩展方法缺乏协调策略并忽视模型内部推理过程。

Method: 提出Agentar-Scale-SQL框架，结合三种扩展视角：基于强化学习增强的内部推理（内部扩展）、迭代优化（顺序扩展）和多样化生成与锦标赛选择（并行扩展）。

Result: 在BIRD基准测试中取得81.67%的执行准确率，排名第一，显著提升性能。

Conclusion: Agentar-Scale-SQL通过协同的测试时扩展策略有效提升了Text-to-SQL模型的表现，为实现类人水平性能提供了可行路径。

Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind
human experts on challenging benchmarks like BIRD. Current approaches that
explore test-time scaling lack an orchestrated strategy and neglect the model's
internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,
a novel framework leveraging scalable computation to improve performance.
Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that
synergistically combines three distinct perspectives: i) Internal Scaling via
RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative
Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament
Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy
adaptation to new databases and more powerful language models. Extensive
experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD
benchmark, reaching 81.67\% execution accuracy on the test set and ranking
first on the official leaderboard, demonstrating an effective path toward
human-level performance.

</details>


### [498] [CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://arxiv.org/abs/2509.24422)
*Haosi Mo,Xinyu Ma,Xuebo Liu,Derek F. Wong,Yu Li,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的评估大语言模型能力的框架CDT，结合认知理论全面衡量模型在多个维度上的表现，并在数据集评估和数据选择中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常关注孤立的能力，缺乏一个整体框架来评估大语言模型的综合能力。

Method: 提出了Cognition-Domain-Task (CDT) 框架，引入Cattell-Horn-Carroll认知理论扩展模型能力定义，并应用于数据集能力评估和数据选择。

Result: 实验表明CDT的能力指标与下游任务性能高度相关，在通用和特定基准上分别达到44.3和45.4分，比基线提高1.6和2.2点。

Conclusion: CDT框架有效且实用，能够支持模型能力的全面评估、数据分析与构建。

Abstract: Recent advances in Large Language Models (LLMs) have significantly enhanced
their capabilities, highlighting the need for comprehensive evaluation
frameworks that extend beyond task-specific benchmarks. However, existing
benchmarks often focus on isolated abilities, lacking a holistic framework for
assessing LLM capabilities. To address this gap, we propose the
Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's
capabilities across three dimensions. We expand the scope of model capability
definitions at the cognitive level by incorporating the Cattell-Horn-Carroll
cognitive theory, refining the categorization of model capabilities. We apply
CDT in two directions: dataset capability evaluation and data selection.
Experiments show that our capability metrics correlate well with downstream
performance and can support effective dataset analysis and construction. The
experiments on data selection also show significant improvements in both
general and specific benchmarks, achieving scores of 44.3 and 45.4, with an
increase of 1.6 and 2.2 points over the baselines, respectively. These results
validate the effectiveness and practicality of CDT. Source code and models are
available at https://github.com/Alessa-mo/CDT.

</details>


### [499] [Alternatives To Next Token Prediction In Text Generation -- A Survey](https://arxiv.org/abs/2509.24435)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 本文综述了下一代语言模型中替代“下一个词预测”（NTP）范式的新兴方法，提出了五类替代方案，并构建了一个系统分类体系，以推动克服现有大模型在规划、错误累积和计算效率方面局限性的研究。


<details>
  <summary>Details</summary>
Motivation: 尽管下一个词预测（NTP）推动了大语言模型的成功，但其固有的缺陷如缺乏长期规划、错误累积和计算低效亟需解决，促使研究者探索NTP之外的新范式。

Method: 通过文献综述与系统归纳，将替代NTP的方法分为五类：多词预测、先计划后生成、潜在推理、连续生成方法和非Transformer架构，并建立统一的分类体系。

Result: 提出并详细描述了五个NTP替代范式的分类框架，总结了各类方法的代表性工作与技术路径，揭示了未来语言模型向非自回归、并行化和结构化生成发展的趋势。

Conclusion: 替代NTP的多种新范式展现出巨大潜力，该分类体系为突破当前语言模型局限、发展新一代高效、可控的自然语言处理模型提供了清晰的研究方向。

Abstract: The paradigm of Next Token Prediction (NTP) has driven the unprecedented
success of Large Language Models (LLMs), but is also the source of their most
persistent weaknesses such as poor long-term planning, error accumulation, and
computational inefficiency. Acknowledging the growing interest in exploring
alternatives to NTP, the survey describes the emerging ecosystem of
alternatives to NTP. We categorise these approaches into five main families:
(1) Multi-Token Prediction, which targets a block of future tokens instead of a
single one; (2) Plan-then-Generate, where a global, high-level plan is created
upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the
autoregressive process itself into a continuous latent space; (4) Continuous
Generation Approaches, which replace sequential generation with iterative,
parallel refinement through diffusion, flow matching, or energy-based methods;
and (5) Non-Transformer Architectures, which sidestep NTP through their
inherent model structure. By synthesizing insights across these methods, this
survey offers a taxonomy to guide research into models that address the known
limitations of token-level generation to develop new transformative models for
natural language processing.

</details>


### [500] [Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset](https://arxiv.org/abs/2509.24468)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文提出了一种名为SOBACO的日语基准，用于统一评估大语言模型中的社会偏见和文化常识，并发现去偏方法会显著降低模型在文化常识任务上的表现，最高达75%的准确率下降。


<details>
  <summary>Details</summary>
Motivation: 社会偏见缓解方法可能损害大语言模型的能力，而现有评估多集中于一般语言理解任务，缺乏对与社会规范密切相关的文化常识影响的研究。

Method: 构建了一个名为SOBACO的日本语基准，统一评估社会偏见和文化常识，并在多个大语言模型上测试不同去偏方法的影响。

Result: 实验结果显示，现有的去偏方法会导致模型在文化常识任务上的性能显著下降，最高准确率下降达75%。

Conclusion: 研究强调在设计去偏方法时需权衡其对文化常识的影响，以提升大语言模型的公平性与实用性。

Abstract: Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.

</details>


### [501] [A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems](https://arxiv.org/abs/2509.24478)
*Lasse Borgholt,Jakob Havtorn,Christian Igel,Lars Maaløe,Zheng-Hua Tan*

Main category: cs.CL

TL;DR: 提出了一种结合动态规划与束搜索评分的新型对齐算法，以实现更精确的语音识别错误分析。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别评估指标（如词错误率）容易掩盖对罕见词、命名实体和领域特定词汇的错误，需要更细粒度的错误分析方法。

Method: 提出一种结合动态规划与束搜索评分的对齐算法，并通过PyPI发布。

Result: 相比传统文本对齐方法，该算法能更准确地对齐单个错误，支持可靠的细粒度错误分析。

Conclusion: 该算法提升了参考文本与模型转录文本之间的对齐精度，有助于揭示语音识别系统在关键词汇上的真实表现。

Abstract: Modern neural networks have greatly improved performance across speech
recognition benchmarks. However, gains are often driven by frequent words with
limited semantic weight, which can obscure meaningful differences in word error
rate, the primary evaluation metric. Errors in rare terms, named entities, and
domain-specific vocabulary are more consequential, but remain hidden by
aggregate metrics. This highlights the need for finer-grained error analysis,
which depends on accurate alignment between reference and model transcripts.
However, conventional alignment methods are not designed for such precision. We
propose a novel alignment algorithm that couples dynamic programming with beam
search scoring. Compared to traditional text alignment methods, our approach
provides more accurate alignment of individual errors, enabling reliable error
analysis. The algorithm is made available via PyPI.

</details>


### [502] [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
*Wenjie Fu,Huandong Wang,Junyao Gao,Guoan Wan,Tao Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Self-Sanitize的新型LLM安全缓解框架，受认知心理学启发，通过自我监控和自我修复机制在生成过程中实时检测并修正有害内容，尤其关注隐私泄露问题，在多个LLM和场景中验证了其高效性和低开销。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种应用中取得成功，但其可能生成有害或侵犯隐私的内容，现有防御方法多依赖事后过滤，存在高延迟且不支持流式生成，因此需要一种更高效、实时的内生式安全机制。

Method: 提出Self-Sanitize框架，包含两个模块：1）Self-Monitor模块通过表示工程在token级别持续监控LLM内部的高层意图；2）Self-Repair模块在发现有害内容时进行原位修正，无需额外对话轮次，实现流式生成中的实时自检与修复。

Result: 在四个大语言模型和三种隐私泄露场景下进行了广泛实验，结果显示Self-Sanitize在几乎不增加延迟和资源消耗的情况下，显著优于现有方法，有效抑制了隐私泄露等有害内容生成，同时保持了模型原有功能性能。

Conclusion: Self-Sanitize为大语言模型提供了一种实用、高效且低侵入性的安全解决方案，特别适用于对实时性和隐私保护要求较高的应用场景，推动了安全LLM的部署实践。

Abstract: As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize

</details>


### [503] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: 本文提出GRPO-MA方法，通过每个思维过程生成多个答案来解决GRPO在思维链训练中存在的梯度耦合、稀疏奖励和优势估计不稳定等问题，理论和实验均表明该方法能有效降低梯度方差、提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在训练大模型思维链推理时面临梯度耦合、稀疏奖励信号和优势估计不稳定三个关键挑战，限制了其训练效果和稳定性。

Method: 提出GRPO-MA方法，通过从每个思维过程中生成多个答案，解耦思维与答案的梯度，提升奖励信号密度，并稳定优势估计；理论上证明了每思维多答案可降低思维优势的方差。

Result: 理论分析显示GRPO-MA可减少梯度方差，实验证明其在数学、代码和多模态任务上显著优于GRPO，梯度分析显示梯度尖峰减少，消融研究显示增加每思维答案数能持续提升性能。

Conclusion: GRPO-MA是一种简单且理论支持的有效改进方法，能够显著提升基于强化学习的思维链训练的稳定性、效率和性能。

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [504] [Knowledge Editing with Subspace-Aware Key-Value Mappings](https://arxiv.org/abs/2509.24502)
*Haewon Park,Sangwoo Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种名为SUIT的知识编辑方法，通过仅修改与编辑相关的关键特征子空间来减少对语言模型的扰动，同时保持高编辑有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑方法在没有约束的情况下修改MLP层的键值向量，导致模型受到显著扰动，因此需要一种更精确的方法来减少副作用。

Method: 采用定位后编辑的方法，识别并仅修改与编辑相关的关键特征子空间，限制对MLP层的修改范围。

Result: 在LLaMA-3-8B、GPT-J-6B和Qwen2.5-7B模型上的实验表明，SUIT在知识保持方面显著优于强基线方法，同时维持了高编辑效果。

Conclusion: SUIT能够有效识别编辑的关键子空间，在减少模型扰动的同时实现高效的知识编辑。

Abstract: Knowledge editing aims to efficiently correct factual errors in Language
Models (LMs). The popular locate-then-edit approach modifies an MLP layer by
finding an optimal mapping between its input vector (key) and output vector
(value) that leads to the expression of the edited knowledge. However, existing
methods without any constraints on the key and value vectors cause significant
perturbations to the edited model. To address this, we propose Subspace
Knowledge Edit (SUIT), a method that identifies and modifies only the subspace
of critical features relevant to the edit. Our empirical results on LLaMA-3-8B,
GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge
preservation over strong baselines while maintaining high edit efficacy. This
effectiveness confirms that SUIT successfully identifies the critical subspace
for the edit. Further analyses provide additional validation for our approach.
The source code and data will be released to the public upon publication of the
paper.

</details>


### [505] [Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings](https://arxiv.org/abs/2509.24506)
*Hamna,Gayatri Bhat,Sourabrata Mukherjee,Faisal Lalani,Evan Hadfield,Divya Siddarth,Kalika Bali,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 提出了一种名为Samiksha的社区驱动评估方法，通过与民间社会组织和社区成员合作，实现对大语言模型在真实生活场景中的可扩展、文化敏感的自动化评估，特别应用于印度的医疗领域。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型评估通常依赖通用或领域特定基准，缺乏对终端用户实际需求和文化背景的考虑，尤其在医疗等关键领域需要更贴近现实的评估方式。

Method: 与民间社会组织和社区成员共同构建社区驱动的评估流程，利用社区反馈指导评估内容、基准构建方式及输出评分，实现可扩展且文化敏感的自动化评估。

Result: 在印度医疗领域的应用表明，当前多语言大语言模型在处理复杂的社区健康查询方面存在一定能力，同时该方法为情境化、包容性的模型评估提供了可行路径。

Conclusion: Samiksha提供了一种更具现实相关性和文化敏感性的大语言模型评估框架，有助于推动更加公平和包容的人工智能发展。

Abstract: Large Language Models (LLMs) are typically evaluated through general or
domain-specific benchmarks testing capabilities that often lack grounding in
the lived realities of end users. Critical domains such as healthcare require
evaluations that extend beyond artificial or simulated tasks to reflect the
everyday needs, cultural practices, and nuanced contexts of communities. We
propose Samiksha, a community-driven evaluation pipeline co-created with
civil-society organizations (CSOs) and community members. Our approach enables
scalable, automated benchmarking through a culturally aware, community-driven
pipeline in which community feedback informs what to evaluate, how the
benchmark is built, and how outputs are scored. We demonstrate this approach in
the health domain in India. Our analysis highlights how current multilingual
LLMs address nuanced community health queries, while also offering a scalable
pathway for contextually grounded and inclusive LLM evaluation.

</details>


### [506] [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.CL

TL;DR: AdaThink-Med 是一种面向医疗大语言模型的端到端自适应推理框架，通过不确定性引导的长度校准机制，动态调整推理链长度，在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在推理时对简单和复杂问题均采用长链思维，导致推理成本高，缺乏根据问题难度自适应调整思维长度的端到端方法。

Method: 提出 AdaThink-Med 框架：生成多个候选输出，评估其正确性和不确定性，通过不确定性引导的长度校准模块估计问题难度；对简单问题惩罚长推理路径，对复杂错误问题鼓励扩展推理链。

Result: 在六个公开医疗问答基准上，AdaThink-Med 平均实现最高 6.4 倍的推理长度缩减，性能仅轻微下降，并自发形成“非思考”和“思考”两种推理模式。

Conclusion: AdaThink-Med 能有效提升医疗大模型的自适应推理能力，在显著降低计算开销的同时保持高性能，为实际应用中的效率优化提供了可行方案。

Abstract: Recent advances in inference time scaling with extended long chain-of thought
have significantly improved the reasoning capabilities of both general and
medical large language models (LLMs). However, these models tend to engage in
lengthy reasoning processes regardless of the difficulty of the input question,
leading to increased inference costs in real-world applications. Therefore,
enabling adaptive thinking where models think less for simpler questions and
think more for complex ones is critical for the effective use of medical LLMs
in practice. Despite its importance, there is a lack of end-to-end approaches
designed to enhance the adaptive thinking capabilities of medical LLMs while
providing a comprehensive examination of the trade-off between performance and
computational cost. To bridge this gap, we propose AdaThink-Med, the first
end-to-end framework designed to enhance adaptive thinking ability in medical
reasoning models with uncertainty-guided length calibration. AdaThink-Med first
generates multiple candidate outputs for each question, evaluates the
correctness and uncertainty of each candidate, and then estimates problem
difficulty via an uncertainty-guided length calibration module. For outputs
with low difficulty and correct answers, the framework penalizes longer
reasoning paths; whereas for those with high difficulty and incorrect answers,
it encourages extending the chain of thought to explore alternative solutions.
On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length
reduction on average while retaining performance with only minimal degradation.
Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct
reasoning modes, which we characterize as "non-thinking" and "thinking",
demonstrating the model's ability to suppress redundant reasoning processes
dynamically.

</details>


### [507] [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
*Melika Honarmand,Ayati Sharma,Badr AlKhamissi,Johannes Mehrer,Martin Schrimpf*

Main category: cs.CL

TL;DR: 本研究利用大规模视觉-语言模型（VLMs）模拟阅读障碍，通过识别并干扰模型中类似视觉词形区的单元，成功复现了阅读障碍的关键特征，特别是语音处理缺陷，而拼写处理能力保持相对完整。


<details>
  <summary>Details</summary>
Motivation: 传统行为与神经影像方法难以验证阅读障碍机制的因果假设，因此需要一种可操控的计算模型来探究其根本机制。

Method: 使用认知神经科学中的刺激材料，在视觉-语言模型中识别对视觉词形敏感的单元，并进行针对性消融实验，对比随机单元消融的影响。

Result: 针对视觉词形选择性单元的消融导致模型在阅读任务上出现特异性损伤，但整体视觉和语言理解能力未受影响，且表现出与人类阅读障碍相似的语音缺陷。

Conclusion: 该建模方法成功复制了阅读障碍的核心特征，为研究阅读障碍提供了一个有效的计算框架。

Abstract: Dyslexia, a neurodevelopmental disorder characterized by persistent reading
difficulties, is often linked to reduced activity of the visual word form area
in the ventral occipito-temporal cortex. Traditional approaches to studying
dyslexia, such as behavioral and neuroimaging methods, have provided valuable
insights but remain limited in their ability to test causal hypotheses about
the underlying mechanisms of reading impairments. In this study, we use
large-scale vision-language models (VLMs) to simulate dyslexia by functionally
identifying and perturbing artificial analogues of word processing. Using
stimuli from cognitive neuroscience, we identify visual-word-form-selective
units within VLMs and demonstrate that targeted ablation of these units, unlike
ablation of random units, leads to selective impairments in reading tasks while
general visual and language comprehension abilities remain intact. In
particular, the resulting model matches dyslexic humans' phonological deficits
without a significant change in orthographic processing. Taken together, our
modeling results replicate key characteristics of dyslexia and establish a
computational framework for investigating reading disorders.

</details>


### [508] [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
*Gio Paik,Yongbeom Kim,Soungmin Lee,Sangmin Ahn,Chanwoo Kim*

Main category: cs.CL

TL;DR: 本文提出了HiKE，首个面向韩英混合语码转换的公开基准，旨在推动多语言语音识别（ASR）模型在语码转换场景下的评估与研究。


<details>
  <summary>Details</summary>
Motivation: 语码转换在日常对话中常见，但在多语言ASR中仍缺乏充分研究，尤其是韩英混合语码转换缺少公开可用的评估框架。

Method: 构建了一个高质量、涵盖多主题的韩英语码转换数据集，并提出分层标注体系（词、短语、句子级）及精确的借词标签，用于系统评估ASR模型在不同语码转换层级的表现。

Result: 实验表明，当前多语言ASR模型在未经微调时难以处理语码转换，但通过使用语码转换数据进行微调可显著提升性能。

Conclusion: HiKE为韩英语码转换提供了标准化评估平台，有助于推动多语言ASR在真实混合语言场景中的发展。

Abstract: Despite advances in multilingual automatic speech recognition (ASR),
code-switching (CS), the mixing of languages within an utterance common in
daily speech, remains a severely underexplored challenge. In this paper, we
introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the
first globally accessible evaluation framework for Korean-English CS, aiming to
provide a means for the precise evaluation of multilingual ASR models and to
foster research in the field. The proposed framework not only consists of
high-quality, natural CS data across various topics, but also provides
meticulous loanword labels and a hierarchical CS-level labeling scheme (word,
phrase, and sentence) that together enable a systematic evaluation of a model's
ability to handle each distinct level of code-switching. Through evaluations of
diverse multilingual ASR models and fine-tuning experiments, this paper
demonstrates that while most multilingual ASR models initially struggle with
CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE
will be available at https://github.com/ThetaOne-AI/HiKE.

</details>


### [509] [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
*Bojan Batalo,Erica K. Shimomoto,Neil Millar*

Main category: cs.CL

TL;DR: 本文首次将“夸大宣传语言”（hype）的自动检测作为自然语言处理任务提出，定义了hype为作者用来美化或夸大其研究的夸张或主观性语言，并制定了标注规范，应用于NIH基金申请文本的标注。实验表明，该规范有助于人类可靠标注，且机器学习模型在该数据集上表现良好，但任务具有语言复杂性，可能需要领域知识和时间敏感性。


<details>
  <summary>Details</summary>
Motivation: 科学文献中夸大宣传语言（hype）日益增多，可能影响对证据的客观评估、阻碍科研发展并削弱公众对科学的信任，因此亟需自动检测方法来识别此类语言。

Method: 提出hype的定义与形式化标注指南，对NIH基金申请语料库进行人工标注；使用传统文本分类器和语言模型进行实验，并与人类标注基线对比性能。

Result: 形式化的标注指南有助于人类一致地识别潜在的hype形容词；基于标注数据训练的机器学习模型取得初步良好效果；发现该任务具有语言复杂性，可能依赖领域知识和事实的时间背景。

Conclusion: 本文首次将hype检测视为NLP任务，验证了构建标注数据和训练模型的可行性，强调未来研究需结合领域知识与时间感知以提升检测效果。

Abstract: In science, promotional language ('hype') is increasing and can undermine
objective evaluation of evidence, impede research development, and erode trust
in science. In this paper, we introduce the task of automatic detection of
hype, which we define as hyperbolic or subjective language that authors use to
glamorize, promote, embellish, or exaggerate aspects of their research. We
propose formalized guidelines for identifying hype language and apply them to
annotate a portion of the National Institutes of Health (NIH) grant application
corpus. We then evaluate traditional text classifiers and language models on
this task, comparing their performance with a human baseline. Our experiments
show that formalizing annotation guidelines can help humans reliably annotate
candidate hype adjectives and that using our annotated dataset to train machine
learning models yields promising results. Our findings highlight the linguistic
complexity of the task, and the potential need for domain knowledge and
temporal awareness of the facts. While some linguistic works address hype
detection, to the best of our knowledge, we are the first to approach it as a
natural language processing task.

</details>


### [510] [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
*Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了InfLLM-V2，一种可切换密集-稀疏注意力的框架，用于解决长序列处理中Transformer模型的计算与内存瓶颈。该方法无需额外参数即可实现从短序列预训练到长序列微调的平滑过渡，并在保持高性能的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer中的自注意力机制在处理长序列时面临严重的计算和内存瓶颈，现有稀疏注意力方法引入过多参数并破坏预训练-微调流程，导致收敛慢且难以加速。

Method: 提出InfLLM-V2，一种可训练的稀疏注意力框架，通过无参数架构修改复用密集注意力参数，在短序列上使用密集注意力、长序列上平滑切换至稀疏注意力，并提供高效实现以降低计算开销。

Result: 实验表明，InfLLM-V2比密集注意力快4倍，分别保留了98.1%和99.7%的性能；基于该框架训练并开源了MiniCPM4.1模型。

Conclusion: InfLLM-V2实现了高效、兼容且可扩展的长序列处理，支持快速推理与模型复用，为大语言模型的长上下文理解与复杂推理提供了实用解决方案。

Abstract: Long-sequence processing is a critical capability for modern large language
models. However, the self-attention mechanism in the standard Transformer
architecture faces severe computational and memory bottlenecks when processing
long sequences. While trainable sparse attention methods offer a promising
solution, existing approaches such as NSA introduce excessive extra parameters
and disrupt the conventional \textit{pretrain-on-short, finetune-on-long}
workflow, resulting in slow convergence and difficulty in acceleration. To
overcome these limitations, we introduce dense-sparse switchable attention
framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that
seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2
reuses dense attention parameters through parameter-free architecture
modification, maintaining consistency between short and long sequence
processing. Additionally, InfLLM-V2 ensures computational efficiency across all
sequence lengths, by using dense attention for short inputs and smoothly
transitioning to sparse attention for long sequences. To achieve practical
acceleration, we further introduce an efficient implementation of InfLLM-V2
that significantly reduces the computational overhead. Our experiments on
long-context understanding and chain-of-thought reasoning demonstrate that
InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and
99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we
have trained and open-sourced MiniCPM4.1
(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,
providing a reproducible implementation for the research community.

</details>


### [511] [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
*Qingjie Zhang,Haoting Qian,Zhicong Huang,Cheng Hong,Minlie Huang,Ke Xu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 本文提出了一个可解释的框架unPact，用于分析大语言模型中的知识遗忘机制，发现现有遗忘方法要么不足以彻底删除知识，要么因过度惩罚导致灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 由于难以追踪大语言模型中复杂结构的知识，现有的遗忘方法缺乏可解释性分析，因此需要一种能够揭示遗忘机制的方法。

Method: 提出unPact框架，通过提示词归因和贡献追踪量化每个提示词对输出的影响，并在六种主流遗忘方法、三种大语言模型和三个基准上进行前后对比分析。

Result: 发现遗忘方法通常通过破坏关键词关注来实现表面遗忘；但多数知识仍可通过强调关键词恢复；灾难性遗忘源于对所有令牌的无差别惩罚。

Conclusion: 现有遗忘方法面临两难困境：要么知识未真正删除，要么造成过度性能下降，尚无可靠解决方案。

Abstract: Unlearning seeks to remove specific knowledge from large language models
(LLMs), but its effectiveness remains contested. On one side, "forgotten"
knowledge can often be recovered through interventions such as light
fine-tuning; on the other side, unlearning may induce catastrophic forgetting
that degrades general capabilities. Despite active exploration of unlearning
methods, interpretability analyses of the mechanism are scarce due to the
difficulty of tracing knowledge in LLMs' complex architectures. We address this
gap by proposing unPact, an interpretable framework for unlearning via prompt
attribution and contribution tracking. Typically, it quantifies each prompt
token's influence on outputs, enabling pre- and post-unlearning comparisons to
reveal what changes. Across six mainstream unlearning methods, three LLMs, and
three benchmarks, we find that: (1) Unlearning appears to be effective by
disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly
erased and can be recovered by simply emphasizing these keywords in prompts,
without modifying the model's weights; (3) Catastrophic forgetting arises from
indiscriminate penalization of all tokens. Taken together, our results suggest
an unlearning dilemma: existing methods tend either to be insufficient -
knowledge remains recoverable by keyword emphasis, or overly destructive -
general performance collapses due to catastrophic forgetting, still leaving a
gap to reliable unlearning.

</details>


### [512] [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
*Leander Girrbach,Chi-Ping Su,Tankred Saanum,Richard Socher,Eric Schulz,Zeynep Akata*

Main category: cs.CL

TL;DR: 研究了LLM作为评判者在无参考情况下的评分可靠性，提出了基于潜在模型信号的“潜在评判者”方法，相比传统提示方法更稳定且更具区分性。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM评判者在自由文本评分中因采样不稳定和校准差导致的评分压缩和频繁平局问题。

Method: 提出三种基于内部模型信号的潜在评判方法：概率加权评分、验证式‘是’概率、基于评分位置激活值训练的线性探针，并在多类基准上进行评估。

Result: 潜在方法在配对准确率和列表排序上表现一致更优；概率加权评分在单评分相关性上最强，线性探针在输出logits校准不佳时仍能提取有用信号。

Conclusion: 潜在信息可提供确定性且更具区分性的无参考评估信号，有望改进Best-of-N、多教师蒸馏和路由等选择与训练方法。

Abstract: How reliable are single-response LLM-as-a-judge ratings without references,
and can we obtain fine-grained, deterministic scores in this setting? We study
the common practice of asking a judge model to assign Likert-scale scores to
free-text responses and show two systematic issues: scores are unstable under
sampling and poorly calibrated, leading to compression near the top of the
scale and frequent ties. We then propose and evaluate Latent Judges, which
derive scalar ratings from internal model signals: (i) probability-weighted
scores over integer ratings, (ii) verifier-style probabilities of "yes", and
(iii) linear probes trained on model activations at the rating position. Across
a broad suite of pairwise and single-rating benchmarks, latent methods match or
surpass standard prompting, with consistent gains on pairwise accuracy and
listwise ranking relevant to Best-of-N selection. Probability-weighted scores
achieve the strongest single-rating correlations, while probes recover useful
signals when output logits are miscalibrated. These results indicate that
latent information provides deterministic and more discriminative signals for
reference-free evaluation, and can improve selection and training approaches
like Best-of-$N$, multi-teacher distillation, and routing.

</details>


### [513] [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
*Guibin Zhang,Muxin Fu,Shuicheng Yan*

Main category: cs.CL

TL;DR: MemGen是一种动态生成式记忆框架，通过记忆触发器和记忆编织器实现LLM代理在推理过程中动态调用和增强潜在记忆，显著优于现有外部记忆系统，并自发演化出类人记忆功能。


<details>
  <summary>Details</summary>
Motivation: 现有参数化和基于检索的记忆范式无法捕捉推理与记忆的动态交织过程，缺乏类人认知能力，限制了LLM代理的自我进化能力。

Method: 提出MemGen框架，包含记忆触发器（监测推理状态以决定是否调用记忆）和记忆编织器（将当前状态作为刺激生成潜在token序列作为机器原生记忆），实现记忆与推理的深度融合。

Result: 在八个基准上实验表明，MemGen相比ExpeL和AWM最高提升38.22%，超过GRPO达13.44%，具备强跨域泛化能力，并自发演化出规划记忆、程序记忆和工作记忆等类人记忆功能。

Conclusion: MemGen实现了记忆与认知的紧密循环，推动LLM代理向更自然的机器认知形态演进，为构建具自主学习能力的智能体提供了新路径。

Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to
the human brain, progressively refine themselves through environment
interactions. Existing paradigms remain constrained: parametric memory forcibly
adjusts model parameters, and retrieval-based memory externalizes experience
into structured databases, yet neither captures the fluid interweaving of
reasoning and memory that underlies human cognition. To address this gap, we
propose MemGen, a dynamic generative memory framework that equips agents with a
human-esque cognitive faculty. It consists of a \textit{memory trigger}, which
monitors the agent's reasoning state to decide explicit memory invocation, and
a \textit{memory weaver}, which takes the agent's current state as stimulus to
construct a latent token sequence as machine-native memory to enrich its
reasoning. In this way, MemGen enables agents to recall and augment latent
memory throughout reasoning, producing a tightly interwoven cycle of memory and
cognition. Extensive experiments across eight benchmarks show that MemGen
surpasses leading external memory systems such as ExpeL and AWM by up to
$38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain
generalization ability. More importantly, we find that without explicit
supervision, MemGen spontaneously evolves distinct human-like memory faculties,
including planning memory, procedural memory, and working memory, suggesting an
emergent trajectory toward more naturalistic forms of machine cognition.

</details>


### [514] [Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution](https://arxiv.org/abs/2509.24726)
*Shaobo Wang,Zhengbo Jiao,Zifan Zhang,Yilang Peng,Xu Ze,Boyu Yang,Wei Wang,Hu Wei,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出Socratic-Zero框架，通过教师、解题者和生成器三代理协同进化，实现从少量种子问题自主生成高质量训练数据，显著提升大模型在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法受限于数据质量不稳定且无法动态适应模型能力演化，难以提供有效的训练信号。

Method: 构建一个包含教师、解题者和生成器的闭环系统：解题者通过偏好反馈学习成功与失败的推理路径；教师根据解题者弱点设计更具挑战性的问题；生成器提炼教师的出题策略以实现可扩展的课程生成。

Result: 仅用100个种子问题，Socratic-Solver-8B在七个数学推理基准上平均提升+20.2个百分点；Socratic-Generator-32B生成的数据使学生模型性能超越包括GPT-5、Claude-4等在内的多个SOTA商业大模型。

Conclusion: Socratic-Zero实现了无需人工标注、无需预设任务的自进化课程生成，为大模型推理能力训练提供了高效可扩展的新范式。

Abstract: Recent breakthroughs in large language models (LLMs) on reasoning tasks rely
heavily on massive, high-quality datasets-typically human-annotated and thus
difficult to scale. While data synthesis or distillation offers a promising
alternative, existing methods struggle with inconsistent data quality and an
inability to dynamically adapt to the evolving capabilities of the model,
leading to suboptimal training signals. To address these limitations, we
introduce Socratic-Zero, a fully autonomous framework that generates
high-quality training data from minimal seed examples through the co-evolution
of three agents: the Teacher, the Solver, and the Generator. The Solver
continuously refines its reasoning by learning from preference feedback on both
successful and failed trajectories; the Teacher adaptively crafts increasingly
challenging questions based on the Solver's weaknesses; and the Generator
distills the Teacher's question-design strategy to enable scalable,
high-fidelity curriculum generation. This closed-loop system produces a
self-improving curriculum-requiring no pre-existing tasks or labels.
Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B
achieves an average gain of +20.2 percentage points over prior data synthesis
methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,
Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3
and GLM4 series models. Even more surprisingly, synthetic data from
Socratic-Generator-32B enables student LLMs to achieve superior performance
compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,
including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,
and Claude-4.1-Opus.

</details>


### [515] [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
*Yixuan Wang,Huang He,Siqi Bao,Hua Wu,Haifeng Wang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的稀疏注意力算法ProxyAttn，通过压缩注意力头维度和动态预算估计，实现高效且精细的块重要性评估，在保持性能的同时显著加速大语言模型的长文本处理。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次复杂度限制了大语言模型在长文本任务中的效率，现有基于块稀疏注意力的方法因粗粒度的重要性估计在高稀疏率下性能下降明显。

Method: 利用注意力头之间的相似性，通过池化代表性头的得分来近似所有头的得分，并结合块感知的动态预算分配方法，实现更细粒度的块重要性评估。

Result: 在多种主流模型和基准测试上验证了注意力头间的相似性，ProxyAttn实现了最高10.3倍的注意力加速和2.4倍的预填充加速，且无显著性能损失。

Conclusion: ProxyAttn是一种高效、无需训练的稀疏注意力方法，通过细粒度重要性估计在长文本场景下显著提升了LLM的推理效率。

Abstract: The quadratic complexity of attention mechanisms limits the efficiency of
Large Language Models (LLMs) on long-text tasks. Recently, methods that
dynamically estimate block importance have enabled efficient block sparse
attention, leading to significant acceleration in long-text pre-filling of
LLMs. However, their coarse-grained estimation inevitably leads to performance
degradation at high sparsity rates. In this work, we propose ProxyAttn, a
training-free sparse attention algorithm that achieves more precise block
estimation by compressing the dimension of attention heads. Based on our
observation of the similarity among multiple attention heads, we use the scores
of pooled representative heads to approximate the scores for all heads. To
account for the varying sparsity among heads, we also propose a block-aware
dynamic budget estimation method. By combining the scores from representative
proxy heads with multi-head dynamic budgets, we achieve a more fine-grained
block importance evaluation at low computational cost. Experiments on a variety
of mainstream models and extensive benchmarks confirm the underlying similarity
among attention heads. Leveraging a fine-grained estimation, the proposed
method achieves substantial gains in performance and efficiency compared to
existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention
acceleration and 2.4x prefilling acceleration without significant performance
loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

</details>


### [516] [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
*Guibin Zhang,Fanci Meng,Guancheng Wan,Zherui Li,Kun Wang,Zhenfei Yin,Lei Bai,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出了一种名为LatentEvolve的自演化潜在测试时扩展框架，通过模拟人类大脑的快速回忆和慢速巩固机制，显著提升了大语言模型在推理阶段的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展方法相互独立，缺乏渐进式学习能力，目标是让大语言模型学会如何更有效地进行测试时计算扩展。

Method: 受互补学习系统理论启发，设计了包含白天扩展（快速检索历史潜在表示）和夜间扩展（整合过去潜在优化）的双阶段演化框架，在无需监督的情况下实现LLM的快速与慢速演化。

Result: 在八个基准和五个模型主干上实验表明，LatentEvolve比现有最先进方法（如LatentSeek和TTRL）最高提升13.33%，并展现出卓越的跨领域和跨主干泛化能力。

Conclusion: LatentEvolve实现了类人认知动态的测试时扩展，为大语言模型的持续自我优化提供了新路径。

Abstract: Test-time Scaling (TTS) has been demonstrated to significantly enhance the
reasoning capabilities of Large Language Models (LLMs) during the inference
phase without altering model parameters. However, existing TTS methods are
largely independent, implying that LLMs have not yet evolved to progressively
learn how to scale more effectively. With the objective of evolving LLMs to
learn ``how to scale test-time computation,'' we propose LatentEvolve, a
self-evolving latent TTS framework inspired by the complementary learning
system (CLS) theory. Analogous to the human brain's dual system of a
fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve
comprises two evolutionary components: \textit{daytime scaling}, which rapidly
retrieves historical latent representations to better guide current LLM
reasoning; and \textit{nighttime scaling}, which integrates past latent
optimizations in a manner akin to the human brain's consolidation of
experiences during sleep. The alternation of daytime and nighttime processes
facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive
dynamics in a fully unsupervised manner. Extensive experiments across eight
benchmarks and five model backbones demonstrate that our LatentEvolve surpasses
state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\%$ and
exhibits exceptional cross-domain and cross-backbone generalization.

</details>


### [517] [SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models](https://arxiv.org/abs/2509.24781)
*Jun Rao,Yunjie Liao,Xuebo Liu,Zepeng Lin,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SeaPO的策略性错误放大方法，通过引入LLM中常见的三类错误来增强负样本的错误程度，从而提升大语言模型在偏好优化中的性能。实验表明该方法在多个能力维度和不同模型规模下均显著提升了模型表现，尤其在真实性方面提高了5-10个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的对齐方法因正负样本质量趋于相似而导致优化困难，难以有效区分优劣响应。为解决这一问题，需设计一种能够系统性增强负样本缺陷的方法，以明确偏好信号。

Method: 提出SeaPO（Strategic Error Amplification for Preference Optimization）方法，利用LLMs中常见的三类错误，在生成负样本时有目的地注入特定错误模式，使负样本明显劣于正样本，并通过基于偏好的训练减少这些错误的发生。

Result: 在五个能力维度和不同规模模型（1.5B至14B）上的评估显示，该方法显著提升整体性能，尤其在真实性方面提高5-10个百分点；不同错误类型的影响各异：常见错误类型的注入能提升相关任务表现，混合错误类型则带来更广泛的整体改进。

Conclusion: SeaPO通过战略性放大错误增强了偏好学习的训练信号，有效提升了大语言模型的性能，特别是在真实性方面效果显著，且适用于不同规模的模型。

Abstract: Existing alignment methods for preference optimization of large language
models (LLMs) aim to enhance model performance by utilizing pairs of positive
and negative samples. However, due to the limited capacity of models in scoring
or generating responses, the quality of positive and negative samples may
become similar during training, which complicates optimization for preference
learning. To address this issue, we introduce SeaPO, a Strategic Error
Amplification method that leverages three error types commonly occurring in
LLMs to introduce specific error patterns into the model Preference
Optimization. This strategy ensures that negative samples are more erroneous
than positive samples and preference-based training is employed to mitigate the
occurrence of these errors, thereby enhancing model performance. Evaluations
across five capability dimensions and different model scales (1.5B to 14B)
demonstrate that the generated data significantly improved overall model
performance, particularly in terms of truthfulness, with improvements of 5-10
percentage points observed. Further analysis reveals that task performance
varies depending on the error types introduced. Injecting the most common error
types improves performance in related tasks, while a mix of error types leads
to a broader performance enhancement: most tasks show stable improvements,
while a few tasks exhibit significant gains.

</details>


### [518] [Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions](https://arxiv.org/abs/2509.24792)
*Luisa Geiger,Mareike Hartmann,Michael Sullivan,Alexander Koller*

Main category: cs.CL

TL;DR: 提出一种基于树的自动评估指标，用于评估大模型生成的分步组装指令，尤其在缝纫指令领域表现出比传统指标更好的相关性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU和BERT相似度）无法准确反映生成指令中的时空构造合理性，因此需要一种更能体现步骤逻辑和空间顺序的评估方法。

Method: 设计一种基于树结构的评估指标，通过建模步骤间的时空关系来评估生成的装配指令，并在缝纫指令数据上进行验证。

Result: 该指标与人工标注的错误数量及人类质量评分具有更高相关性，且在对抗文本相似性干扰的反事实样例中表现更稳健。

Conclusion: 所提出的树基评估指标在评估大模型生成的装配指令（尤其是缝纫指令）方面优于传统文本相似性指标，能更准确地反映生成内容的时空合理性。

Abstract: In this paper, we propose a novel, automatic tree-based evaluation metric for
LLM-generated step-by-step assembly instructions, that more accurately reflects
spatiotemporal aspects of construction than traditional metrics such as BLEU
and BERT similarity scores. We apply our proposed metric to the domain of
sewing instructions, and show that our metric better correlates with
manually-annotated error counts as well as human quality ratings, demonstrating
our metric's superiority for evaluating the spatiotemporal soundness of sewing
instructions. Further experiments show that our metric is more robust than
traditional approaches against artificially-constructed counterfactual examples
that are specifically constructed to confound metrics that rely on textual
similarity.

</details>


### [519] [KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning](https://arxiv.org/abs/2509.24816)
*Xilin Dang,Kexin Chen,Xiaorui Su,Ayush Noori,Iñaki Arango,Lucas Vittor,Xinyi Long,Yuyang Du,Marinka Zitnik,Pheng Ann Heng*

Main category: cs.CL

TL;DR: 本文提出了一种名为KnowGuard的新型“先调查后 abstain”范式，通过系统性地探索医学知识图谱来改进大语言模型在临床决策中的 abstention 能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在面对不完整患者信息时难以有效 abstain，容易给出过度自信的错误回答，缺乏结合外部医学证据识别知识边界的机制。

Method: KnowGuard 包含两个阶段：1）证据发现阶段，通过图扩展和直接检索系统探索医学知识空间；2）证据评估阶段，基于多种因素对证据进行排序，并根据患者上下文和对话历史调整探索策略。

Result: 在开放式多轮临床基准测试中，KnowGuard 在诊断准确率上提升了3.93%，平均减少7.27轮不必要的交互，优于现有最先进的 abstention 方法。

Conclusion: KnowGuard 通过引入系统化的知识图谱探索机制，显著提升了大语言模型在医疗场景下的安全性和决策效率，为实现可靠临床辅助提供了新思路。

Abstract: In clinical practice, physicians refrain from making decisions when patient
information is insufficient. This behavior, known as abstention, is a critical
safety mechanism preventing potentially harmful misdiagnoses. Recent
investigations have reported the application of large language models (LLMs) in
medical scenarios. However, existing LLMs struggle with the abstentions,
frequently providing overconfident responses despite incomplete information.
This limitation stems from conventional abstention methods relying solely on
model self-assessments, which lack systematic strategies to identify knowledge
boundaries with external medical evidences. To address this, we propose
\textbf{KnowGuard}, a novel \textit{investigate-before-abstain} paradigm that
integrates systematic knowledge graph exploration for clinical decision-making.
Our approach consists of two key stages operating on a shared contextualized
evidence pool: 1) an evidence discovery stage that systematically explores the
medical knowledge space through graph expansion and direct retrieval, and 2) an
evidence evaluation stage that ranks evidence using multiple factors to adapt
exploration based on patient context and conversation history. This two-stage
approach enables systematic knowledge graph exploration, allowing models to
trace structured reasoning paths and recognize insufficient medical evidence.
We evaluate our abstention approach using open-ended multi-round clinical
benchmarks that mimic realistic diagnostic scenarios, assessing abstention
quality through accuracy-efficiency trade-offs beyond existing closed-form
evaluations. Experimental evidences clearly demonstrate that KnowGuard
outperforms state-of-the-art abstention approaches, improving diagnostic
accuracy by 3.93\% while reducing unnecessary interaction by 7.27 turns on
average.

</details>


### [520] [DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework](https://arxiv.org/abs/2509.24821)
*Rui Jia,Yuang Wei,Ruijia Li,Yuang-Hao Jiang,Xinyu Xie,Yaomin Shen,Min Zhang,Bo Jiang*

Main category: cs.CL

TL;DR: 本文提出了DiaCDM模型，首次将认知诊断应用于师生对话场景，通过IRE框架和图编码方法提升诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型难以处理非结构化、动态的对话数据，且难以从长对话中准确提取诊断语义。

Method: 基于教育学中的IRE（引发-回应-评价）框架构建对话诊断结构，并提出一种融合教师问题与知识点的图编码方法，以更精确捕捉关键信息。

Result: 在三个真实对话数据集上的实验表明，DiaCDM显著提高了诊断准确性，并增强了结果的可解释性。

Conclusion: DiaCDM是首个用于对话场景的认知诊断模型，为教师评估学生认知状态提供了有效工具。

Abstract: While cognitive diagnosis (CD) effectively assesses students' knowledge
mastery from structured test data, applying it to real-world teacher-student
dialogues presents two fundamental challenges. Traditional CD models lack a
suitable framework for handling dynamic, unstructured dialogues, and it's
difficult to accurately extract diagnostic semantics from lengthy dialogues. To
overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted
the initiation-response-evaluation (IRE) framework from educational theory to
design a diagnostic framework tailored for dialogue. We also developed a unique
graph-based encoding method that integrates teacher questions with relevant
knowledge components to capture key information more precisely. To our
knowledge, this is the first exploration of cognitive diagnosis in a dialogue
setting. Experiments on three real-world dialogue datasets confirm that DiaCDM
not only significantly improves diagnostic accuracy but also enhances the
results' interpretability, providing teachers with a powerful tool for
assessing students' cognitive states. The code is available at
https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.

</details>


### [521] [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
*Xinye Zhao,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 提出了一种名为SemShareKV的框架，通过在语义相似但词汇不同的提示之间共享和压缩键值缓存，加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法主要依赖于单个提示内部或跨提示的精确词元匹配，难以应对语义相似但词汇不同的场景，如多文档摘要和对话系统。

Method: 采用基于局部敏感哈希（LSH）的模糊词元匹配，并结合旋转位置编码（RoPE）来保留位置信息，从而在不同提示间实现语义级别的KV缓存复用。

Result: 实验显示，在5k tokens输入下，最高实现6.25倍加速和42%的GPU内存降低，且质量损失可忽略。

Conclusion: SemShareKV通过语义感知的缓存共享，显著提升了大语言模型推理效率，具有广泛的应用潜力。

Abstract: As large language models (LLMs) continue to scale, the memory footprint of
key-value (KV) caches during inference has become a significant bottleneck.
Existing approaches primarily focus on compressing KV caches within a single
prompt or reusing shared prefixes or frequently ocurred text segments across
prompts. However, such strategies are limited in scenarios where prompts are
semantically similar but lexically different, which frequently occurs in tasks
such as multi-document summarization and conversational agents. We propose
\textit{SemShareKV}, a KV cache sharing and compression framework that
accelerates LLM inference by reusing KVCache in semantically similar prompts.
Instead of relying on exact token matches, SemShareKV applies fuzzy token
matching using locality-sensitive hashing (LSH) on token embeddings and
incorporates Rotary Position Embedding (RoPE) to better preserve positional
information. By selectively reusing relevant key-value pairs from a reference
prompt's cache, SemShareKV reduces redundant computation while maintaining
output quality. Experiments on diverse summarization datasets show up to
6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with
negligible quality degradation. These results highlight the potential of
semantic-aware cache sharing for efficient LLM inference.

</details>


### [522] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: 提出了一种层次化错误纠正（HEC）框架，通过系统性错误分析提升大模型在专业领域的表现，实验显示在多个领域和模型上均有显著准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域表现不佳，例如在医疗编码任务中准确率仅为45.9%，亟需针对性的改进方法。

Method: 分析四个专业领域中的错误模式，发现错误具有层次结构（知识层、推理层、复杂度层），并据此设计三阶段的层次化错误纠正框架。

Result: 在医疗转录、法律分类、政治偏见检测和法律推理任务中，跨五个大模型架构平均提升了11.2个百分点（p < 0.001），但在高基线任务（>75%准确率）中效果有限甚至可能干扰原有推理。

Conclusion: 系统性错误分析可有效指导专业领域AI性能提升，HEC框架对中等基线任务尤为有效，但需注意其在高基线任务中的应用边界。

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [523] [Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs](https://arxiv.org/abs/2509.24857)
*Adrian Arnaiz-Rodriguez,Miguel Baidal,Erik Derner,Jenn Layton Annable,Mark Ball,Mark Ince,Elvira Perez Vallejos,Nuria Oliver*

Main category: cs.CL

TL;DR: 本研究提出了一种临床指导的心理健康危机分类体系，构建了评估数据集和专家评审协议，系统评测了三种先进大语言模型在识别和回应心理危机方面的能力，发现尽管模型在明确危机情境下表现稳定，但仍存在生成有害回应、难以处理模糊信号、缺乏情境适配等问题，尤其是开源模型风险更高，强调需加强安全机制与上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于心理健康支持等高风险场景，其在识别和应对急性心理危机方面的安全性尚不明确，且缺乏统一的危机分类体系、标注数据集和基于临床实践的评估方法，亟需系统性研究以保障用户安全。

Method: 提出一个包含六类临床指导的心理健康危机统一分类体系，构建多样化的评估数据集，并设计由专家制定的回应适当性评估协议，对三种最先进的大语言模型进行系统性基准测试，评估其危机分类能力和生成回应的安全性与适当性。

Result: 实验结果显示，大语言模型在处理明确危机表述时具有一致性和可靠性，但仍有不可忽视比例的回应被评定为不当或有害，其中开源模型的表现劣于商业模型；同时发现模型在应对间接或模糊风险信号时存在系统性缺陷，常依赖刻板、不自然的默认回复，且频繁与用户实际情境脱节。

Conclusion: 当前大语言模型在心理健康支持应用中仍存在显著安全隐患，亟需增强危机检测能力、上下文理解与个性化干预策略；本研究提出的分类体系、数据集和评估框架为后续研究和负责任的AI创新奠定了基础，有助于降低风险并保护脆弱用户群体。

Abstract: The widespread use of chatbots powered by large language models (LLMs) such
as ChatGPT and Llama has fundamentally reshaped how people seek information and
advice across domains. Increasingly, these chatbots are being used in
high-stakes contexts, including emotional support and mental health concerns.
While LLMs can offer scalable support, their ability to safely detect and
respond to acute mental health crises remains poorly understood. Progress is
hampered by the absence of unified crisis taxonomies, robust annotated
benchmarks, and empirical evaluations grounded in clinical best practices. In
this work, we address these gaps by introducing a unified taxonomy of six
clinically-informed mental health crisis categories, curating a diverse
evaluation dataset, and establishing an expert-designed protocol for assessing
response appropriateness. We systematically benchmark three state-of-the-art
LLMs for their ability to classify crisis types and generate safe, appropriate
responses. The results reveal that while LLMs are highly consistent and
generally reliable in addressing explicit crisis disclosures, significant risks
remain. A non-negligible proportion of responses are rated as inappropriate or
harmful, with responses generated by an open-weight model exhibiting higher
failure rates than those generated by the commercial ones. We also identify
systemic weaknesses in handling indirect or ambiguous risk signals, a reliance
on formulaic and inauthentic default replies, and frequent misalignment with
user context. These findings underscore the urgent need for enhanced
safeguards, improved crisis detection, and context-aware interventions in LLM
deployments. Our taxonomy, datasets, and evaluation framework lay the
groundwork for ongoing research and responsible innovation in AI-driven mental
health support, helping to minimize harm and better protect vulnerable users.

</details>


### [524] [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
*Matteo Fuoli,Weihang Huang,Jeannette Littlemore,Sarah Turner,Ellen Wilding*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）在自动化识别全文隐喻中的潜力，比较了检索增强生成、提示工程和微调三种方法，发现最先进的闭源LLM通过微调可达到中位F1分数0.79，且多数与人类标注的差异具有系统性，表明LLM可部分自动化隐喻识别并用于改进识别协议和理论。


<details>
  <summary>Details</summary>
Motivation: 由于隐喻具有语境敏感性，大规模分析受限于手动标注的需求，因此需要探索自动化方法以提高效率和可扩展性。

Method: 比较了三种基于大型语言模型的方法：检索增强生成（RAG）、提示工程（包括零样本、少样本和思维链策略）和微调，并评估其在隐喻识别任务上的表现。

Result: 最先进的闭源LLM在微调下取得中位F1得分为0.79；与人类标注相比，LLM的差异多为系统性，反映出隐喻理论中的已知模糊地带和概念挑战。

Conclusion: LLM可被用于部分自动化隐喻识别，并可作为发展和优化隐喻识别协议及其理论基础的测试平台。

Abstract: Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.

</details>


### [525] [Expanding Computation Spaces of LLMs at Inference Time](https://arxiv.org/abs/2509.24884)
*Yoonna Jang,Kisu Yang,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本研究探讨了在推理阶段插入填充标记是否能为语言模型提供额外的计算空间，实验表明在较小模型中这种方法显著提升了性能，尤其是在开放域问答和数学任务上。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在推理时利用人工插入的填充标记作为额外计算空间的可能性，以提升其问题解决能力。

Method: 通过识别有效的标记类型、数量和插入位置，研究模型在训练的哪个阶段开始利用扩展的计算空间，并通过注意力图分析这些空间内的动态。

Result: 实验显示，在最终'Answer:'标记前插入填充标记最有效，小模型提升最多达12.372个百分点，注意力图显示扩展空间延续了原有的注意力机制并参与有意义的计算。

Conclusion: 填充标记提供的扩展空间可作为额外计算容量，而非冗余输入，尤其对较小的语言模型更具益处。

Abstract: Chain-of-thought (CoT) rationale enables language models to use additional
task-related text for problem-solving, benefiting not only from detailed
reasoning steps but also from the expanded computational space of longer
inputs. Prior work has trained filler or special tokens to serve as additional
computation spaces. In this study, we investigate whether language models can
leverage artificially inserted sequences of filler tokens solely at inference.
We first identify effective token types, numbers, and insertion locations, then
examine at what stage of training models begin to exploit the expanded
computation space, and finally analyze dynamics within these spaces via
attention maps. Experiments on models ranging from 1.7B to 32B across
open-domain QA and math tasks show that appropriate token types and counts
vary, but placing filler tokens directly before the final 'Answer:' token is
most effective. Smaller models benefit most, up to 12.372 percentage points in
SmolLM2-1.7B-Instruct, indicating that these spaces act as additional
computational capacity rather than redundant input. Attention maps reveal that
expanded spaces often continue the original attention mechanism and sometimes
focus on questions or answer options, suggesting meaningful computation for
problem-solving.

</details>


### [526] [BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications](https://arxiv.org/abs/2509.24908)
*Andrés Fernández García,Javier de la Rosa,Julio Gonzalo,Roser Morante,Enrique Amigó,Alejandro Benito-Santos,Jorge Carrillo-de-Albornoz,Víctor Fresno,Adrian Ghajari,Guillermo Marco,Laura Plaza,Eva Sánchez Salido*

Main category: cs.CL

TL;DR: 本文提出了一个名为BOE-XSUM的新数据集，包含3,648个西班牙语法律文档的简洁摘要，用于训练和评估大语言模型在法律文本摘要任务中的表现。实验结果表明，经过微调的模型显著优于零样本通用模型。


<details>
  <summary>Details</summary>
Motivation: 由于信息过载，对长文档进行简洁摘要是必要的，但目前缺乏针对西班牙语尤其是法律领域文档的高质量摘要资源。

Method: 构建了一个名为BOE-XSUM的数据集，包含来自西班牙官方公报的文档及其人工撰写的简明摘要，并使用该数据集对中等规模的大语言模型进行微调，与零样本通用生成模型进行比较评估。

Result: 微调后的模型显著优于零样本模型，其中表现最佳的BERTIN GPT-J 6B比最强的零样本模型DeepSeek-R1准确率提高了24%（41.6% vs. 33.5%）。

Conclusion: 针对特定领域（如西班牙语法律文本）构建高质量摘要数据集并进行模型微调，能显著提升摘要生成性能，验证了领域适应的重要性。

Abstract: The ability to summarize long documents succinctly is increasingly important
in daily life due to information overload, yet there is a notable lack of such
summaries for Spanish documents in general, and in the legal domain in
particular. In this work, we present BOE-XSUM, a curated dataset comprising
3,648 concise, plain-language summaries of documents sourced from Spain's
``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each
entry in the dataset includes a short summary, the original text, and its
document type label. We evaluate the performance of medium-sized large language
models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose
generative models in a zero-shot setting. Results show that fine-tuned models
significantly outperform their non-specialized counterparts. Notably, the
best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\%
performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of
41.6\% vs.\ 33.5\%).

</details>


### [527] [How Well Do LLMs Imitate Human Writing Style?](https://arxiv.org/abs/2509.24930)
*Rebira Jemama,Rajesh Kumar*

Main category: cs.CL

TL;DR: 提出一种无需训练的作者风格验证与模仿分析框架，结合TF-IDF字符n-gram与Transformer嵌入，通过经验距离分布分类文本对，在学术论文和跨领域任务中准确率分别达97.5%和94.5%，显著降低训练时间和内存消耗。实验表明提示策略比模型规模更影响风格保真度，其中少样本和补全式提示显著提升模仿效果，但LLM输出的困惑度低于人类文本，说明高风格保真不等于人类级不可预测性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在多大程度上能真实复现特定人类作者的独特写作风格，并建立一种无需训练、无需阈值调优的高效作者风格分析方法。

Method: 融合TF-IDF字符n-gram与Transformer嵌入，利用经验距离分布对文本对进行分类，实现无需监督训练和阈值调整的作者风格验证与模仿分析。

Result: 该方法在学术论文数据集上达到97.5%的准确率，跨领域评估中达94.5%，训练时间减少91.8%，内存使用降低59%；少样本提示比零样本提示风格匹配准确率最高提升23.5倍，补全式提示与原作者风格一致性达99.9%；但LLM生成文本的平均困惑度（15.2）显著低于人类文本（29.5）。

Conclusion: 风格保真度与统计可检测性是可分离的：尽管LLM可通过合适提示策略高度模仿作者风格，但其生成结果仍缺乏人类文本的不可预测性，为未来作者身份建模、检测与条件化生成研究提供了可复现的基础。

Abstract: Large language models (LLMs) can generate fluent text, but their ability to
replicate the distinctive style of a specific human author remains unclear. We
present a fast, training-free framework for authorship verification and style
imitation analysis. The method integrates TF-IDF character n-grams with
transformer embeddings and classifies text pairs through empirical distance
distributions, eliminating the need for supervised training or threshold
tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in
cross-domain evaluation, while reducing training time by 91.8\% and memory
usage by 59\% relative to parameter-based baselines. Using this framework, we
evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across
four prompting strategies - zero-shot, one-shot, few-shot, and text completion.
Results show that the prompting strategy has a more substantial influence on
style fidelity than model size: few-shot prompting yields up to 23.5x higher
style-matching accuracy than zero-shot, and completion prompting reaches 99.9\%
agreement with the original author's style. Crucially, high-fidelity imitation
does not imply human-like unpredictability - human essays average a perplexity
of 29.5, whereas matched LLM outputs average only 15.2. These findings
demonstrate that stylistic fidelity and statistical detectability are
separable, establishing a reproducible basis for future work in authorship
modeling, detection, and identity-conditioned generation.

</details>


### [528] [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
*Changsheng Zhao,Ernie Chang,Zechun Liu,Chia-Jung Chang,Wei Wen,Chen Lai,Rick Cao,Yuandong Tian,Raghuraman Krishnamoorthi,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 本研究挑战了大语言模型推理能力必须依赖超大规模数据训练的假设，提出通过精心筛选和重采样高质量开源数据（仅需约2T tokens），在较小数据量下也能涌现出强大的推理能力。基于此方法训练的MobileLLM-R1系列模型在多个推理基准上超越此前使用全开源数据训练的模型，并媲美更大规模训练的Qwen3-0.6B。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为模型的推理能力不仅需要大参数量，还依赖于超大规模训练数据（>10T tokens）。尽管小模型具备推理能力已被验证，但对大数据必要性的质疑仍不足。本文旨在探究高质量、精选数据是否能在更小数据规模下激发推理能力。

Method: 设计评估指标，筛选并重采样有益的开源数据集；使用约2T高质量tokens构建4.2T token的预训练数据；采用标准后训练流程；开发出MobileLLM-R1系列子十亿参数模型。

Result: MobileLLM-R1-950M在AIME测试中得分15.5，远超OLMo-2-1.48B（0.6）和SmolLM-2-1.7B（0.3）；尽管预训练token仅为Qwen3的11.7%（4.2T vs 36T），但在多项推理任务上达到或超过Qwen3-0.6B的表现。

Conclusion: 推理能力的涌现不必然依赖于海量数据训练，高质量、精选的数据组合可在显著减少数据量的情况下实现强大推理性能。该研究为高效、可复现的小模型推理训练提供了可行路径与完整开源资源。

Abstract: The paradigm shift in large language models (LLMs) from instinctive responses
to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)
reasoning capabilities only emerge in sufficiently large models, and (2) such
capabilities require training on massive datasets. While the first assumption
has already been challenged by recent sub-billion-parameter reasoning models
such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely
unquestioned. In this work, we revisit the necessity of scaling to extremely
large corpora (>10T tokens) for reasoning emergence. By carefully curating and
resampling open-source datasets that we identify as beneficial under our
designed metrics, we demonstrate that strong reasoning abilities can emerge
with far less data. Specifically, we show that only ~2T tokens of high-quality
data are sufficient, and pre-training with 4.2T tokens on the dataset resampled
from these ~2T tokens, followed by a established post-training procedure,
enables the development of MobileLLM-R1, a series of sub-billion-parameter
reasoning models that substantially outperform prior models trained on fully
open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of
15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.
Remarkably, despite being trained on only 11.7% of the tokens compared to
Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches
or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate
further research in this direction, we have released the complete training
recipe, data sources, data mixing ratio, and model checkpoints, together with
the key insights obtained throughout this study.

</details>


### [529] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: MAQuE是一个用于评估医疗多轮问诊的综合性基准，包含3000个模拟患者代理和多维度评估框架，揭示了当前AI医生在应对真实患者行为时的不足与权衡。


<details>
  <summary>Details</summary>
Motivation: 现有AI医生虽具备诊断能力，但在同理心、沟通等关键素质上仍不足，缺乏全面评估其多轮问诊能力的基准。

Method: 提出MAQuE基准，包含3000个具有多样性语言、认知、情绪和被动披露倾向的模拟患者，并设计涵盖任务成功率、提问能力、对话能力、效率及患者体验的多维评估框架。

Result: 实验表明现有大模型在各项指标上仍有显著提升空间，对真实患者行为变化敏感，影响诊断准确性，且不同评估维度间存在性能权衡。

Conclusion: MAQuE为评估AI医生的临床问诊能力提供了新标准，凸显了在实际医疗场景中平衡性能与实用性的挑战。

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [530] [SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](https://arxiv.org/abs/2509.24961)
*Kaihong Li,Huichi Zhou,Bin Ma,Fangjun Huang*

Main category: cs.CL

TL;DR: 提出了一种结合项目侧语义信息的两阶段检测框架SemanticShield，利用大语言模型检测推荐系统中的洗牌攻击，具有良好的效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要关注用户行为，忽视了项目侧语义特征（如标题、描述）在识别恶意意图中的潜力，导致对洗牌攻击的检测不充分。

Method: 构建一个两阶段检测框架：第一阶段使用低成本行为标准预筛可疑用户；第二阶段利用大语言模型进行语义一致性审计，并通过强化微调优化轻量级LLM，提升检测性能。

Result: 在六种代表性攻击策略上验证了SemanticShield的有效性，且在未见过的攻击方法上表现出强泛化能力。

Conclusion: 融合项目侧语义信息能有效增强对洗牌攻击的防御，SemanticShield为推荐系统提供了一种高效且可推广的检测方案。

Abstract: Recommender systems (RS) are widely used in e-commerce for personalized
suggestions, yet their openness makes them susceptible to shilling attacks,
where adversaries inject fake behaviors to manipulate recommendations. Most
existing defenses emphasize user-side behaviors while overlooking item-side
features such as titles and descriptions that can expose malicious intent. To
address this gap, we propose a two-stage detection framework that integrates
item-side semantics via large language models (LLMs). The first stage
pre-screens suspicious users using low-cost behavioral criteria, and the second
stage employs LLM-based auditing to evaluate semantic consistency. Furthermore,
we enhance the auditing model through reinforcement fine-tuning on a
lightweight LLM with carefully designed reward functions, yielding a
specialized detector called SemanticShield. Experiments on six representative
attack strategies demonstrate the effectiveness of SemanticShield against
shilling attacks, and further evaluation on previously unseen attack methods
shows its strong generalization capability. Code is available at
https://github.com/FrankenstLee/SemanticShield.

</details>


### [531] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出了一种广义正确性模型（GCM），通过系统编码历史正确性信息来提升大语言模型的置信度估计，发现该能力具有跨模型和数据集的可迁移性，而非依赖于模型自我内省。


<details>
  <summary>Details</summary>
Motivation: 准确且校准的置信度估计对高风险或用户导向的应用至关重要，但当前大语言模型在自我信心校准方面仍面临挑战，尤其在假设模型能自我判断答案正确性的前提下存在局限。

Method: 提出广义正确性模型（GCM），利用目标模型的历史预测数据进行训练，并探索将历史信息作为上下文示例注入或通过后处理校准的方法；在多个模型族和数据集上评估其跨模型泛化能力。

Result: 实验表明，LLM对其自身输出正确性的预测并不优于其他无关模型；GCM能在不同模型和数据集间有效预测答案正确性，答案表述方式是正确性的重要预测因子；引入历史上下文示例和后处理校准均可提升置信度估计性能。

Conclusion: 可靠的LLM置信度估计是一种可通过编码历史正确性信息习得的通用、模型无关技能，而非依赖特定模型自我认知的能力，为构建更可信的生成系统提供了新路径。

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [532] [Circuit Distillation](https://arxiv.org/abs/2509.25002)
*Somin Wadhwa,Silvio Amir,Byron C. Wallace*

Main category: cs.CL

TL;DR: 提出了一种称为“电路蒸馏”的新方法，通过匹配教师模型和学生模型中功能对应的内部组件来对齐其内部表示，从而更有效地传递算法能力。


<details>
  <summary>Details</summary>
Motivation: 传统的模型蒸馏主要关注行为模仿，将教师模型的内部计算视为黑箱。本文旨在探索一种替代方法，即蒸馏教师模型中底层的计算机制，以实现更有针对性和可解释性的能力迁移。

Method: 提出电路蒸馏方法，识别教师与学生模型中功能对应的电路组件，并引入损失函数来对齐这些组件所诱导的内部表示，仅调整学生模型的一小部分参数。

Result: 在实体追踪和心智理论（ToM）任务上使用Llama3系列模型进行评估，结果显示电路蒸馏优于标准蒸馏方法，能够成功转移算法能力。

Conclusion: 电路蒸馏验证了机制迁移的可行性，有望通过可解释和可控的内部机制，高效地蒸馏教师模型的特定能力。

Abstract: Model distillation typically focuses on behavioral mimicry, where a student
model is trained to replicate a teacher's output while treating its internal
computations as a black box. In this work we propose an alternative approach:
Distilling the underlying computational mechanisms implemented by a teacher
model. Specifically, we propose circuit distillation, which introduces an
objective to align internal representations between analogous circuit
components in teacher and student models. We propose a method to match
``functionally correspondent'' circuit components and introduce a loss
reflecting similarities between the representations that these induce. We
evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks
using models from the Llama3 family. Our results demonstrate that circuit
distillation outperforms standard distillation, successfully transferring
algorithmic capabilities by adjusting only a small, targeted subset of student
model parameters. This work establishes the feasibility of transferring
mechanisms, which may in turn allow for efficient distillation of targeted
teacher capabilities via interpretable and controllable internal student
mechanisms.

</details>


### [533] [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
*Haoyang Zheng,Xinyang Liu,Cindy Xiangrui Kong,Nan Jiang,Zheyuan Hu,Weijian Luo,Wei Deng,Guang Lin*

Main category: cs.CL

TL;DR: 本文提出了DiDi-Instruct，一种基于预训练离散扩散语言模型的高效文本生成方法，实现了比GPT-2和标准dLLM更快且性能更优的生成效果，理论上有KL散度最小化的支持，并通过多种技术提升训练稳定性和推理性能。


<details>
  <summary>Details</summary>
Motivation: 追求在AI时代实现快速语言文本生成，克服现有加速生成模型在性能和效率上的局限。

Method: 提出DiDi-Instruct方法，基于预训练的离散扩散语言模型，结合积分KL散度最小化框架，并引入分组奖励归一化、中间状态匹配和奖励引导祖先采样器（RGAS）等技术。

Result: 在OpenWebText上，DiDi-Instruct在8到128次NFE下样本困惑度从62.2降至18.4，达到64倍加速，仅损失约1%熵，额外训练时间减少20倍，且在蛋白质序列生成等任务中表现出鲁棒性。

Conclusion: DiDi-Instruct是一种高效且有效的蒸馏方法，能实现极快的语言生成，具有良好的扩展性和应用潜力，代码和模型将公开。

Abstract: Fast generation of language texts is the holy grail that people pursue in the
AI era. In this work, we introduced Discrete Diffusion Divergence Instruct
(DiDi-Instruct), a training-based method that leads to fast language generation
models by initializing from a pre-trained (masked) discrete diffusion language
model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM
counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical
part of the paper, we build the foundation of DiDi-Instruct in a framework of
integral KL-divergence minimization, with practical training algorithms. We
also introduce techniques like grouped reward normalization, intermediate-state
matching, and the reward-guided ancestral sampler (RGAS) that significantly
improve the training stability, the model coverage, and the inference
performances. On OpenWebText, DiDi-Instruct outperforms all accelerated
language generation models as well as the GPT-2 baseline and the standard
dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128
NFEs). These performance gains are accomplished with a negligible entropy loss
of about 1% and 20x less additional training wall-clock time. We further
validate the robustness and effectiveness of DiDi-Instruct through extensive
ablation studies, model scaling, and the generation of discrete protein
sequences. In conclusion, DiDi-Instruct is an efficient yet effective
distillation method, enabling language generation in the blink of an eye. We
will release both code and models at github.com/haoyangzheng-ai/didi-instruct.

</details>


### [534] [GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2509.25037)
*Adamu Lawan,Haruna Yunusa*

Main category: cs.CL

TL;DR: 提出了一种新的门控多模态架构GateMABSA，用于解决多模态方面情感分析中的噪声视觉信号过滤和跨模态对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有MABSA模型难以有效过滤噪声视觉信号，并在不同模态间准确对齐方面与观点内容。

Method: 引入三种专用的mLSTM：Syn-mLSTM整合句法结构，Sem-mLSTM强调方面-语义相关性，Fuse-mLSTM实现选择性多模态融合。

Result: 在两个基准Twitter数据集上的实验表明，GateMABSA优于多个基线模型。

Conclusion: GateMABSA能更有效地处理多模态情感分析中的关键挑战，提升性能。

Abstract: Aspect-based Sentiment Analysis (ABSA) has recently advanced into the
multimodal domain, where user-generated content often combines text and images.
However, existing multimodal ABSA (MABSA) models struggle to filter noisy
visual signals, and effectively align aspects with opinion-bearing content
across modalities. To address these challenges, we propose GateMABSA, a novel
gated multimodal architecture that integrates syntactic, semantic, and
fusion-aware mLSTM. Specifically, GateMABSA introduces three specialized
mLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize
aspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal
fusion. Extensive experiments on two benchmark Twitter datasets demonstrate
that GateMABSA outperforms several baselines.

</details>


### [535] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: 本文提出了Hyperdimensional Probe，一种结合符号表示和神经探测的新范式，用于从大语言模型的向量空间中解码信息，克服了现有方法的局限性，并在多种任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型可解释性方法（如直接logit归因和稀疏自编码器）受限于输出词汇或特征命名不明确，难以充分理解模型内部表示。

Method: 提出Hyperdimensional Probe，利用向量符号架构（VSA）将模型残差流投影到可解释的概念空间，结合了稀疏自编码器和传统探针的优点。

Result: 在句法模式识别、键值关联、抽象推理和问答任务中，该方法能稳定提取跨不同模型、嵌入维度和输入领域的有意义概念，并有助于识别模型失败。

Conclusion: Hyperdimensional Probe提升了大语言模型向量空间中的信息解码能力，能够提取更丰富、可解释且结构化的特征，推动了神经表征的可解释性研究。

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [536] [Confidence-Guided Error Correction for Disordered Speech Recognition](https://arxiv.org/abs/2509.25048)
*Abner Hernandez,Tomás Arias Vergara,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.CL

TL;DR: 提出一种基于置信度的提示方法，利用大语言模型对紊乱语音的ASR结果进行错误校正，通过在训练中引入词级不确定性估计，显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 紊乱语音的自动语音识别（ASR）存在较高错误率，传统后处理方法易出现过度校正问题，需提升大语言模型（LLM）在跨说话人和数据集上的鲁棒性与泛化能力。

Method: 提出置信度知情提示（confidence-informed prompting）方法，将词级置信度信息嵌入LLM训练过程，指导模型关注ASR中低置信度区域，并对LLaMA 3.1模型进行微调，与仅使用文本的微调和事后置信度过滤方法进行对比。

Result: 在Speech Accessibility Project数据集上相对WER降低10%，在TORGO数据集上降低47%，显著优于朴素LLM校正和其他基线方法。

Conclusion: 置信度感知的微调策略能有效提升LLM在紊乱语音ASR后处理中的性能，减少过度校正，增强模型泛化能力，为辅助语音技术提供了实用解决方案。

Abstract: We investigate the use of large language models (LLMs) as post-processing
modules for automatic speech recognition (ASR), focusing on their ability to
perform error correction for disordered speech. In particular, we propose
confidence-informed prompting, where word-level uncertainty estimates are
embedded directly into LLM training to improve robustness and generalization
across speakers and datasets. This approach directs the model to uncertain ASR
regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare
our approach to both transcript-only fine-tuning and post hoc confidence-based
filtering. Evaluations show that our method achieves a 10% relative WER
reduction compared to naive LLM correction on the Speech Accessibility Project
spontaneous speech and a 47% reduction on TORGO, demonstrating the
effectiveness of confidence-aware fine-tuning for impaired speech.

</details>


### [537] [An empirical study on the limitation of Transformers in program trace generation](https://arxiv.org/abs/2509.25073)
*Simeng Sun*

Main category: cs.CL

TL;DR: 研究了Transformer在程序跟踪生成（PTG）任务上的表现，尽管模型在分布内准确率较高，但在泛化到不同因素时存在系统性失败，某些设计能显著改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在需要逐步推理的算法任务中的表现，特别是外部化推理过程的程序跟踪生成任务。

Method: 使用小型Transformer模型，尝试多种改进方法，包括替代的位置编码、softmax替换、混合模型和短卷积。

Result: 模型在训练数据分布内表现出高准确率，但在程序长度、跟踪步数等泛化方面出现系统性失败，部分架构改进显著提升了泛化性能。

Conclusion: PTG任务揭示了Transformer在长序列逐步推理中的局限性，某些模型设计有助于提升其泛化能力。

Abstract: We study Transformers on the task \emph{program trace generation} (PTG),
where models produce step-by-step execution traces for synthetic programs.
Unlike existing algorithmic problems, PTG externalizes reasoning through long
traces where each step is trivial. We train small Transformers with diverse
modifications, including alternative position encodings, softmax replacements,
hybrid model, and short convolutions. While these models achieve strong
in-distribution accuracy, they exhibit systematic failures when generalizing to
various factors (e.g., program length, trace steps), though some designs
significantly improve generalization.

</details>


### [538] [Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs](https://arxiv.org/abs/2509.25086)
*Akio Hayakawa,Stefan Bott,Horacio Saggion*

Main category: cs.CL

TL;DR: 提出了一种用于词汇简化（LS）的高效且安全的小型语言模型框架，通过知识蒸馏和上下文学习进行实验，发现输出概率可有效检测有害简化，并提出过滤策略在保留有益简化的同时抑制有害输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在隐私敏感和资源受限环境中应用词汇简化存在挑战，且需确保对弱势用户群体输出的安全性和正确性。

Method: 构建基于小型语言模型的本地化LS框架，采用知识蒸馏（使用合成数据）和上下文学习作为基线方法，并利用模型输出概率设计过滤策略以识别和抑制有害简化。

Result: 在五种语言上的实验表明，知识蒸馏提升自动指标但增加有害简化；模型输出概率可有效检测此类问题；所提过滤策略能显著减少有害输出同时保留大部分有益简化。

Conclusion: 建立了小型语言模型在高效、安全词汇简化任务上的基准，揭示了性能、效率与安全性之间的权衡，为现实世界中安全部署LS系统提供了可行路径。

Abstract: Despite their strong performance, large language models (LLMs) face
challenges in real-world application of lexical simplification (LS),
particularly in privacy-sensitive and resource-constrained environments.
Moreover, since vulnerable user groups (e.g., people with disabilities) are one
of the key target groups of this technology, it is crucial to ensure the safety
and correctness of the output of LS systems. To address these issues, we
propose an efficient framework for LS systems that utilizes small LLMs
deployable in local environments. Within this framework, we explore knowledge
distillation with synthesized data and in-context learning as baselines. Our
experiments in five languages evaluate model outputs both automatically and
manually. Our manual analysis reveals that while knowledge distillation boosts
automatic metric scores, it also introduces a safety trade-off by increasing
harmful simplifications. Importantly, we find that the model's output
probability is a useful signal for detecting harmful simplifications.
Leveraging this, we propose a filtering strategy that suppresses harmful
simplifications while largely preserving beneficial ones. This work establishes
a benchmark for efficient and safe LS with small LLMs. It highlights the key
trade-offs between performance, efficiency, and safety, and demonstrates a
promising approach for safe real-world deployment.

</details>


### [539] [Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?](https://arxiv.org/abs/2509.25107)
*Kai Sun,Yin Huang,Srishti Mehra,Mohammad Kachuee,Xilun Chen,Renjie Tao,Zhaojiang Lin,Andrea Jessee,Nirav Shah,Alex Betty,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文研究了在大型语言模型（LLM）时代，从网页内容中提取知识三元组对问答系统的价值，发现尽管LLM在问答任务上表现良好，但知识提取仍具挑战性，结合三元组增强和多任务学习可提升其性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在基于网页的问答系统中的广泛应用，知识提取是否仍有价值成为问题，本文旨在探究三元组提取在此新范式下的作用。

Method: 通过扩展现有基准数据集并添加知识提取标注，评估不同规模的商用和开源大模型在知识提取与问答任务上的表现。

Result: 实验表明，当前大模型在大规模知识提取任务上仍面临挑战，但在引入提取的三元组进行增强或多任务学习后，问答性能得到提升。

Conclusion: 知识三元组提取在基于网页的问答系统中依然具有重要价值，结合知识提取可有效提升不同规模大模型的性能，尤其在资源受限场景下更具潜力。

Abstract: The advent of Large Language Models (LLMs) has significantly advanced
web-based Question Answering (QA) systems over semi-structured content, raising
questions about the continued utility of knowledge extraction for question
answering. This paper investigates the value of triple extraction in this new
paradigm by extending an existing benchmark with knowledge extraction
annotations and evaluating commercial and open-source LLMs of varying sizes.
Our results show that web-scale knowledge extraction remains a challenging task
for LLMs. Despite achieving high QA accuracy, LLMs can still benefit from
knowledge extraction, through augmentation with extracted triples and
multi-task learning. These findings provide insights into the evolving role of
knowledge triple extraction in web-based QA and highlight strategies for
maximizing LLM effectiveness across different model sizes and resource
settings.

</details>


### [540] [Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2509.25138)
*Ivan Vykopal,Antonia Karamolegkou,Jaroslav Kopčan,Qiwei Peng,Tomáš Javůrek,Michal Gregor,Marián Šimko*

Main category: cs.CL

TL;DR: 本文研究了多语言大模型在事实核查中的语言偏见和检索偏见，通过20种语言的评估揭示了模型在不同语言和检索内容上的性能差异，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在高资源语言（如英语）上表现良好，但在低资源语言上存在性能差距，且信息检索系统可能存在检索偏见，影响事实核查的公平性。

Method: 使用AMC-16K数据集和全多语言提示策略，评估六种开源多语言大模型在20种语言上的表现，并分析多语言嵌入模型和检索声明频率以探究检索偏见。

Result: 发现了显著的语言和检索偏见：模型在高资源语言上表现更好，流行声明被过度检索，而罕见声明则被忽视；模型家族、规模和提示策略影响性能。

Conclusion: 多语言大模型在跨语言事实核查中存在持续偏见，需改进提示策略和检索机制以提升多语言公平性。

Abstract: Multilingual Large Language Models (LLMs) offer powerful capabilities for
cross-lingual fact-checking. However, these models often exhibit language bias,
performing disproportionately better on high-resource languages such as English
than on low-resource counterparts. We also present and inspect a novel concept
- retrieval bias, when information retrieval systems tend to favor certain
information over others, leaving the retrieval process skewed. In this paper,
we study language and retrieval bias in the context of Previously Fact-Checked
Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20
languages using a fully multilingual prompting strategy, leveraging the AMC-16K
dataset. By translating task prompts into each language, we uncover disparities
in monolingual and cross-lingual performance and identify key trends based on
model family, size, and prompting strategy. Our findings highlight persistent
bias in LLM behavior and offer recommendations for improving equity in
multilingual fact-checking. To investigate retrieval bias, we employed
multilingual embedding models and look into the frequency of retrieved claims.
Our analysis reveals that certain claims are retrieved disproportionately
across different posts, leading to inflated retrieval performance for popular
claims while under-representing less common ones.

</details>


### [541] [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
*Yen-Ju Lu,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 提出Paired by the Teacher (PbT)方法，通过教师-学生框架从非配对数据生成高质量输入-输出对，无需人工标注，在多个NLG任务上接近有监督性能。


<details>
  <summary>Details</summary>
Motivation: 在低资源自然语言生成场景中，常只有输入或输出的原始数据，缺乏成对标注数据，导致小模型训练困难或依赖大模型生成的低效合成数据。

Method: 采用两阶段教师-学生框架：教师LLM将非配对样本压缩为中间表示（IR），学生模型学习从IR重建输入，从而与原始输出形成高质量合成配对数据。

Result: 在五个基准和SwitchBoard上的非配对设置中，仅用PbT数据训练的8B学生模型优于使用70B教师生成数据的模型，ROUGE-L仅比人工标注低1.2，填补82%的oracle差距，且注释成本仅为直接合成的三分之一；人类评估显示其生成的摘要更简洁、忠实且风格一致。

Conclusion: PbT能有效利用非配对数据生成领域内高质量训练样本，显著降低对人工标注和大规模合成数据的依赖，推动低资源NLG的发展。

Abstract: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline
that synthesizes accurate input-output pairs without human labels or parallel
data. In many low-resource natural language generation (NLG) scenarios,
practitioners may have only raw outputs, like highlights, recaps, or questions,
or only raw inputs, such as articles, dialogues, or paragraphs, but seldom
both. This mismatch forces small models to learn from very few examples or rely
on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses
this by asking a teacher LLM to compress each unpaired example into a concise
intermediate representation (IR), and training a student to reconstruct inputs
from IRs. This enables outputs to be paired with student-generated inputs,
yielding high-quality synthetic data. We evaluate PbT on five
benchmarks-document summarization (XSum, CNNDM), dialogue summarization
(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired
setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained
only on PbT data outperforms models trained on 70 B teacher-generated corpora
and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated
pairs and closing 82% of the oracle gap at one-third the annotation cost of
direct synthesis. Human evaluation on SwitchBoard further confirms that only
PbT produces concise, faithful summaries aligned with the target style,
highlighting its advantage of generating in-domain sources that avoid the
mismatch, limiting direct synthesis.

</details>


### [542] [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
*NVIDIA,Felix Abecassis,Anjulie Agrusa,Dong Ahn,Jonah Alben,Stefania Alborghetti,Michael Andersch,Sivakumar Arayandi,Alexis Bjorlin,Aaron Blakeman,Evan Briones,Ian Buck,Bryan Catanzaro,Jinhang Choi,Mike Chrzanowski,Eric Chung,Victor Cui,Steve Dai,Bita Darvish Rouhani,Carlo del Mundo,Deena Donia,Burc Eryilmaz,Henry Estela,Abhinav Goel,Oleg Goncharov,Yugi Guvvala,Robert Hesse,Russell Hewett,Herbert Hum,Ujval Kapasi,Brucek Khailany,Mikail Khona,Nick Knight,Alex Kondratenko,Ronny Krashinsky,Ben Lanir,Simon Layton,Michael Lightstone,Daniel Lo,Paulius Micikevicius,Asit Mishra,Tim Moon,Deepak Narayanan,Chao Ni,Abhijit Paithankar,Satish Pasumarthi,Ankit Patel,Mostofa Patwary,Ashwin Poojary,Gargi Prasad,Sweta Priyadarshi,Yigong Qin,Xiaowei Ren,Oleg Rybakov,Charbel Sakr,Sanjeev Satheesh,Stas Sergienko,Pasha Shamis,Kirthi Shankar,Nishant Sharma,Mohammad Shoeybi,Michael Siu,Misha Smelyanskiy,Darko Stosic,Dusan Stosic,Bor-Yiing Su,Frank Sun,Nima Tajbakhsh,Shelby Thomas,Przemek Tredak,Evgeny Tsykunov,Gandhi Vaithilingam,Aditya Vavre,Rangharajan Venkatesan,Roger Waleffe,Qiyu Wan,Hexin Wang,Mengdi Wang,Lizzie Wei,Hao Wu,Evan Wu,Keith Wyss,Ning Xu,Jinze Xue,Charlene Yang,Yujia Zhai,Ruoxi Zhang,Jingyang Zhu,Zhongbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于NVFP4格式的4位精度大模型训练方法，结合随机Hadamard变换、二维量化方案、随机舍入和选择性高精度层，实现了稳定且准确的LLM训练，并在120亿参数模型上完成了迄今最长的4位精度训练验证，结果媲美FP8基线。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型预训练效率，降低计算资源消耗，推动更强大模型的发展，同时探索低于8位精度（如4位）训练的可行性与挑战。

Method: 采用NVFP4格式，引入随机Hadamard变换抑制块级异常值，设计前后向一致的二维量化方案，使用随机舍入实现无偏梯度估计，并保留部分关键层为高精度。

Result: 成功训练了120亿参数模型达10万亿token，是目前公开最长的4位精度训练；模型在训练损失和下游任务准确率上与FP8基线相当。

Conclusion: NVFP4结合所提方法可实现稳定高效的极低精度大模型训练，显著提升计算资源利用率，为下一代大模型的高效训练提供了可行路径。

Abstract: Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.

</details>


### [543] [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
*Haolei Xu,Xinyu Mei,Yuchen Yan,Rui Zhou,Wenqi Zhang,Weiming Lu,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: EasySteer是一个基于vLLM的高效、可扩展的大语言模型（LLM）控制框架，通过模块化设计和深度集成实现推理时的隐藏状态操控，在过思考缓解、幻觉减少等任务中显著提升速度与功能，推动LLM控制从研究走向实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有LLM控制框架存在计算效率低、可扩展性差和功能受限等问题，阻碍了研究进展与实际应用，因此需要一个高性能且易于扩展的统一框架。

Method: 构建基于vLLM的模块化框架EasySteer，支持分析型与学习型控制方法的插件式接口，提供细粒度参数控制、预计算的八个领域控制向量，并集成交互式演示系统。

Result: 相比现有框架实现5.5-11.4倍的速度提升，在过思考缓解、幻觉减少等多个应用中表现出优异性能，显著提高推理效率与控制效果。

Conclusion: EasySteer将LLM控制从研究技术转变为可部署的生产能力，为可控语言模型提供了关键基础设施。

Abstract: Large language model (LLM) steering has emerged as a promising paradigm for
controlling model behavior at inference time through targeted manipulation of
hidden states, offering a lightweight alternative to expensive retraining.
However, existing steering frameworks suffer from critical limitations:
computational inefficiency, limited extensibility, and restricted functionality
that hinder both research progress and practical deployment. We present
EasySteer, a unified framework for high-performance, extensible LLM steering
built on vLLM. Our system features modular architecture with pluggable
interfaces for both analysis-based and learning-based methods, fine-grained
parameter control, pre-computed steering vectors for eight application domains,
and an interactive demonstration system. Through deep integration with vLLM's
optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over
existing frameworks. Extensive experiments demonstrate its effectiveness in
overthinking mitigation, hallucination reduction, and other key applications.
EasySteer transforms steering from research technique to production-ready
capability, establishing critical infrastructure for deployable, controllable
language models.

</details>


### [544] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: NAIPv2 是一种去偏且高效的论文质量评估框架，通过领域-年度内的成对学习和评审倾向信号（RTS）提升预测一致性与性能，在大规模数据集 NAIDv2 上实现了最先进的结果，并展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的论文质量评估方法推理成本高，而直接回归方法存在评分尺度不一致的问题，因此需要一种高效且一致的评估框架。

Method: 提出 NAIPv2 框架，采用领域-年度分组内的成对学习来减少评审偏差，引入评审倾向信号（RTS）融合评审分数与置信度，并构建包含 24,276 篇 ICLR 提交论文的大规模数据集 NAIDv2 用于训练与评估。

Result: NAIPv2 在评估中达到 78.2% AUC 和 0.432 Spearman 相关性，推理效率为线性时间；在未见的 NeurIPS 数据上表现出良好泛化，预测分数随录用决策等级上升而一致增加。

Conclusion: NAIPv2 是一种去偏、可扩展且高效的自动化论文质量评估框架，为未来科学智能系统的发展提供了重要基础。

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [545] [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
*Yanchen Jiang,Zhe Feng,Aranyak Mehta*

Main category: cs.CL

TL;DR: 提出了一种名为Truthful Text Summarization (TTS)的激励对齐框架，通过分解声明、评估来源立场、评分和过滤不可靠来源来提升文本合成的事实准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多源文本合成中缺乏对准确性的有效激励，易受对抗内容影响，需要一种无需真实标签即可提升事实可靠性的方法。

Method: 将初步合成分解为原子声明，收集各来源对每个声明的立场，采用改进的多任务同伴预测机制评分并奖励信息性一致，过滤不可靠来源后重新生成摘要。

Result: 实验证明TTS能显著提升事实准确性和鲁棒性，同时保持语言流畅性，并有效抑制操纵行为。

Conclusion: TTS通过激励对齐机制使真实报告成为最优策略，实现了在无监督环境下对合成内容的事实性增强。

Abstract: Large language models (LLMs) are increasingly used in modern search and
answer systems to synthesize multiple, sometimes conflicting, texts into a
single response, yet current pipelines offer weak incentives for sources to be
accurate and are vulnerable to adversarial content. We introduce Truthful Text
Summarization (TTS), an incentive-aligned framework that improves factual
robustness without ground-truth labels. TTS (i) decomposes a draft synthesis
into atomic claims, (ii) elicits each source's stance on every claim, (iii)
scores sources with an adapted multi-task peer-prediction mechanism that
rewards informative agreement, and (iv) filters unreliable sources before
re-summarizing. We establish formal guarantees that align a source's incentives
with informative honesty, making truthful reporting the utility-maximizing
strategy. Experiments show that TTS improves factual accuracy and robustness
while preserving fluency, aligning exposure with informative corroboration and
disincentivizing manipulation.

</details>


### [546] [Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2509.25188)
*Wenrui Bao,Zhiben Chen,Dan Xu,Yuzhang Shang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Learn2PD的框架，通过训练一个轻量级的自适应过滤模型来实现大语言模型中的并行解码，从而显著提升推理速度而不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有的并行解码策略依赖于固定的、与输入无关的启发式方法，无法适应不同输入的特点，导致在多样化的自然语言处理任务中存在次优的速度-质量权衡问题。

Method: 提出Learning to Parallel Decode (Learn2PD)框架，训练一个轻量级过滤模型，动态预测每个token位置当前预测是否已正确；采用后训练方式学习该模型，并引入End-of-Text Prediction（EoTP）机制以避免对填充token进行冗余解码。

Result: 在LLaDA基准测试上，该方法实现了最高22.58倍的加速，在结合KV-Cache时可达57.51倍加速，且无性能下降。

Conclusion: Learn2PD提供了一种灵活、高效的并行解码方案，能够在保持输出质量的同时大幅提升大语言模型的推理吞吐量。

Abstract: Autoregressive decoding in large language models (LLMs) requires
$\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting
inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token
generation through iterative denoising. However, current parallel decoding
strategies rely on fixed, input-agnostic heuristics (e.g., confidence
thresholds), which fail to adapt to input-specific characteristics, resulting
in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,
we explore a more flexible and dynamic approach to parallel decoding. We
propose Learning to Parallel Decode (Learn2PD), a framework that trains a
lightweight and adaptive filter model to predict, for each token position,
whether the current prediction matches the final output. This learned filter
approximates an oracle parallel decoding strategy that unmasks tokens only when
correctly predicted. Importantly, the filter model is learned in a
post-training manner, requiring only a small amount of computation to optimize
it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction
(EoTP) to detect decoding completion at the end of sequence, avoiding redundant
decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that
our method achieves up to 22.58$\times$ speedup without any performance drop,
and up to 57.51$\times$ when combined with KV-Cache.

</details>


### [547] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 本文提出了InfoAgent，一种基于创新数据合成管道和自托管网络搜索工具的深度研究代理，通过两阶段训练方法显著提升了复杂查询任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了应对现有研究中依赖商业搜索工具带来的不透明性和局限性，并提升大型语言模型在复杂、难以查找的信息检索任务中的能力。

Method: 构建实体树并采用子树采样与实体模糊化生成高难度问题；开发自托管搜索基础设施；采用冷启动监督微调和强化学习两阶段后训练策略优化Qwen3-14B模型。

Result: InfoAgent在BrowseComp、BrowseComp-ZH和Xbench-DS三个基准上分别达到15.3%、29.2%和40.4%的准确率，优于WebSailor-72B和DeepDive-32B等开源深度研究代理。

Conclusion: 所提出的数据合成 pipeline 和自托管搜索工具有效提升了语言模型代理在复杂信息检索任务中的性能，推动了开放、透明的智能代理研究发展。

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


### [548] [Exploring Large Language Models for Translating Romanian Computational Problems into English](https://arxiv.org/abs/2501.05601)
*Adrian Marius Dumitran,Adrian-Catalin Badea,Stefan-Gabriel Muscalu,Angela-Liliana Dumitran,Stefan-Cosmin Dascalescu,Radu-Sebastian Amarie*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLM）在翻译数学和计算机科学任务时，从罗马尼亚语译为英语的表现，并发现通过结构化提示可提升其性能。


<details>
  <summary>Details</summary>
Motivation: 由于自动翻译在编程竞赛、教育材料生成及防止人为翻译错误等方面具有重要意义，因此需要确保小众语言翻译的准确性。

Method: 评估多种LLM（如OpenRoLLM、Llama 3.1 8B、Llama 3.2 3B和GPT-4o）在不同翻译方法下的准确性和稳定性，结合重复实验、句法与语义分析，并引入人类专家对比评估。

Result: 研究表明，配合良好设计的提示和适当监督，LLM在翻译IOI风格任务时表现稳定甚至优于原始表现；同时扩展了OJI数据集的英译版本，并验证了其可用于后续训练与评估。

Conclusion: 在人类监督下，LLM可成为多语言问题解决中可靠且可行的翻译工具，具备实际应用潜力。

Abstract: Recent studies have suggested that large language models (LLMs) underperform
on mathematical and computer science tasks when these problems are translated
from Romanian into English, compared to their original Romanian format.
Accurate translation is critical for applications ranging from automatic
translations in programming competitions to the creation of high-quality
educational materials, as well as minimizing errors or fraud in human
translations. This study shows that robust large language models (LLMs) can
maintain or even enhance their performance in translating less common languages
when given well-structured prompts. Our findings suggest that LLMs, with
appropriate supervision, can be reliably used for the automatic translation of
IOI (International Olympiad in Informatics)-style tasks. We evaluate several
translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,
Llama 3.2 3B and GPT-4o, assessing their translation accuracy and performance
stability through repeated runs. Additionally, we augment the OJI (Romanian
County-Level Informatics Olympiad) Romanian dataset with accurate English
translations, enhancing its utility for future LLM training and evaluation.
Through detailed syntactic and semantic analyses, we confirm that with human
oversight, LLMs can serve as a viable solution for multilingual
problem-solving. We also compare the translation quality of LLMs against human
translators, as evaluated by a certified expert, underscoring the potential of
LLMs in realworld scenarios.

</details>


### [549] [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/abs/2506.05991)
*Alexandru-Gabriel Ganea,Antonia-Adelina Popovici,Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: 本研究基于罗马尼亚版《谁想成为百万富翁》节目视频构建了一个富含文化信息的多语言数据集，用于评估大语言模型在不同文化背景下的表现，发现模型对国际性问题的表现显著优于罗马尼亚本土文化问题。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在不同语言和文化背景下的性能差异，尤其是对特定文化相关问题的理解能力不足的问题。

Method: 结合光学字符识别（OCR）、自动文本提取与人工验证，构建包含问题领域、文化相关性和难度等元数据的多语言问答数据集，并在该数据集上评测多种先进大语言模型，同时进行机器翻译和跨语言对比实验。

Result: 模型在国际性问题上的准确率显著高于罗马尼亚特有文化问题（80-95% vs 50-75%），且跨语言翻译和法语对比实验进一步揭示了文化背景对模型性能的影响。

Conclusion: 文化背景和训练数据来源显著影响大语言模型的表现，构建具备文化感知能力的多语言NLP系统需更多本地化、文化相关数据支持。

Abstract: Large Language Models (LLMs) demonstrate varying performance across languages
and cultural contexts. This study introduces a novel, culturally-rich,
multilingual dataset derived from video recordings of the Romanian game show
"Who Wants to Be a Millionaire?" (Vrei s\u{a} fii Milionar?). We employed an
innovative process combining optical character recognition (OCR), automated
text extraction, and manual verification to collect question-answer pairs,
enriching them with metadata including question domain (e.g., biology,
history), cultural relevance (Romanian-specific vs. international), and
difficulty. Benchmarking state-of-the-art LLMs, including Romanian-adapted
models, on this dataset revealed significant performance disparities: models
consistently achieve higher accuracy (80-95%) on international questions
compared to Romanian-specific cultural questions (50-75%). We further
investigate these differences through experiments involving machine translation
of Romanian questions into English and cross-lingual tests using a comparable
dataset in French. Our findings underscore the impact of cultural context and
data source on LLM performance and offer practical insights for building
robust, culturally-aware multilingual NLP systems, especially in educational
domains. The dataset is publicly available at Hugging Face.

</details>


### [550] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为VocabTrim的训练-free技术，通过重构drafter模型的语言建模头（LM head），仅保留目标模型词汇表中最常采样的有限词元子集，从而减少推测解码（speculative decoding）在内存受限环境下的推理开销，提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于drafter的推测解码方法通常共享目标模型与drafter模型的词汇表甚至LM头，但在目标模型词汇表很大时，会导致drafter在采样过程中产生不必要的推理开销，尤其是在内存受限的设备上影响效率。

Method: 提出VocabTrim方法，重构drafter的LM头，将其词汇限制为目标模型最频繁采样的词元子集，从而降低drafter的计算和内存负担，减少 drafting 阶段的延迟。

Result: 在Spec-Bench上对Llama-3系列模型的实验表明，该方法显著提升了内存受限场景下的生成速度，例如在Llama-3.2-3B-Instruct上实现了16%的内存受限加速提升（MBSU），尽管接受率略有下降。

Conclusion: VocabTrim是一种简单有效的训练-free优化方法，能够在轻微牺牲接受率的前提下，显著降低drafter的推理开销，提高内存受限环境下的整体生成效率，适用于边缘设备等资源受限场景。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [551] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: 本文提出了GRILE，首个基于罗马尼亚语高风险考试的开源基准，用于评估多语言和特定语言大模型在选择正确答案和生成语言准确解释方面的能力，揭示了低资源语言教育NLP中的可信挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理中取得了革命性进展，但其在低资源语言教学中的价值尚不明确，因此需要一个专门的基准来评估模型在这类语言中的表现。

Method: 构建了一个包含1,151道多项选择题的GRILE基准，题目来源于罗马尼亚的国家评估、高考和大学入学考试，并评估了七个最先进的多语言和罗马尼亚语专用大模型在答题准确性和生成语言学上准确解释方面的能力。

Result: Gemini 2.5 Pro达到83%的准确率，大多数开源模型低于65%，且48%的解释存在事实或教学错误；错误分析显示模型在形态学和最新正字法规范应用上存在系统性弱点。

Conclusion: 研究揭示了低资源语言环境下可信教育NLP的开放性挑战，确立了GRILE作为可控解释生成与评估的新测试平台，并公开了数据、代码和演示以促进未来研究。

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [552] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 提出了一种名为Q2E的查询到事件分解方法，用于零样本多语言文本到视频检索，通过利用大语言模型和视觉语言模型中的隐含知识来提升复杂现实事件相关视频的识别与检索性能。


<details>
  <summary>Details</summary>
Motivation: 改进复杂现实事件相关视频的识别与检索，解决人类查询过于简化的问题。

Method: 提出Q2E方法，通过大语言模型和视觉语言模型自动提取事件的潜在参数化知识，并对查询进行分解；结合基于熵的融合评分实现零样本多模态融合，支持文本、视觉和语音输入。

Result: 在两个不同数据集上评估显示，Q2E优于多个最先进的基线方法，且融合音频信息显著提升了文本到视频检索效果。

Conclusion: Q2E能有效利用多模态模型中的隐含知识，提升零样本多语言文本到视频检索性能，具有跨数据集、领域、模型的适应性。

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [553] [DiffTex: Differentiable Texturing for Architectural Proxy Models](https://arxiv.org/abs/2509.23336)
*Weidan Xiong,Yongli Wu,Bochuan Zeng,Jianwei Guo,Dani Lischinski,Daniel Cohen-Or,Hui Huang*

Main category: cs.GR

TL;DR: 提出一种基于无序注册照片的建筑代理模型纹理映射自动化方法，通过可微分渲染优化混合参数，生成在光照和视角上一致且无缝的高质量纹理。


<details>
  <summary>Details</summary>
Motivation: 代理模型因几何简化导致细节丢失，需从原始密集重建中保留丰富的纹理信息，但处理无序RGB照片仍具挑战。

Method: 建立UV图上纹素与输入图像像素间的对应关系，每个纹素颜色由相关像素值加权混合得到，并利用可微分渲染优化混合参数以保证光影、视角一致性及纹理连贯性。

Result: 实验结果表明该方法在不同建筑模型和拍摄条件下均能有效生成高视觉保真度和结构细节保持的高质量纹理。

Conclusion: 所提方法能稳健地为建筑代理模型生成逼真的纹素级纹理，有效补偿几何简化带来的细节损失。

Abstract: Simplified proxy models are commonly used to represent architectural
structures, reducing storage requirements and enabling real-time rendering.
However, the geometric simplifications inherent in proxies result in a loss of
fine color and geometric details, making it essential for textures to
compensate for the loss. Preserving the rich texture information from the
original dense architectural reconstructions remains a daunting task,
particularly when working with unordered RGB photographs. We propose an
automated method for generating realistic texture maps for architectural proxy
models at the texel level from an unordered collection of registered
photographs. Our approach establishes correspondences between texels on a UV
map and pixels in the input images, with each texel's color computed as a
weighted blend of associated pixel values. Using differentiable rendering, we
optimize blending parameters to ensure photometric and perspective consistency,
while maintaining seamless texture coherence. Experimental results demonstrate
the effectiveness and robustness of our method across diverse architectural
models and varying photographic conditions, enabling the creation of
high-quality textures that preserve visual fidelity and structural detail.

</details>


### [554] [Modeling and Exploiting the Time Course of Chromatic Adaptation for Display Power Optimizations in Virtual Reality](https://arxiv.org/abs/2509.23489)
*Ethan Chen,Sushant Kondguli,Carl Marshall,Yuhao Zhu*

Main category: cs.GR

TL;DR: 提出一种无需眼动追踪的VR中OLED显示节能方法，通过利用色适应的时间特性，在保证视觉感知质量的同时显著降低功耗。


<details>
  <summary>Details</summary>
Motivation: 在虚拟现实（VR）中减少OLED显示器的功耗，同时最小化对用户感知的影响，提升能效与用户体验。

Method: 提出一种新的心理物理范式，建模人眼适应状态随场景光照变化的过程，并据此计算最优的光照变化轨迹，在给定感知损失预算下控制光照变化速率和程度以降低功耗。

Result: 相比先前瞬时改变光照的方法，该技术显著提升了感知质量；结合已有的亮度调暗技术，可在无统计学感知质量损失的情况下降低31%的显示功耗。

Conclusion: 该方法有效平衡了节能与视觉体验，为VR显示系统提供了一种高效、感知透明的节能方案。

Abstract: We introduce a gaze-tracking--free method to reduce OLED display power
consumption in VR with minimal perceptual impact. This technique exploits the
time course of chromatic adaptation, the human visual system's ability to
maintain stable color perception under changing illumination. To that end, we
propose a novel psychophysical paradigm that models how human adaptation state
changes with the scene illuminant. We exploit this model to compute an optimal
illuminant shift trajectory, controlling the rate and extent of illumination
change, to reduce display power under a given perceptual loss budget. Our
technique significantly improves the perceptual quality over prior work that
applies illumination shifts instantaneously. Our technique can also be combined
with prior work on luminance dimming to reduce display power by 31% with no
statistical loss of perceptual quality.

</details>


### [555] [Automated design of compound lenses with discrete-continuous optimization](https://arxiv.org/abs/2509.23572)
*Arjun Teh,Delio Vicini,Bernd Bickel,Ioannis Gkioulekas,Matthew O'Toole*

Main category: cs.GR

TL;DR: 提出一种联合优化复合透镜连续和离散参数的方法，通过结合梯度优化与定制的马尔可夫链蒙特卡洛采样算法，自动提升透镜在锐度和速度方面的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅能优化给定拓扑结构下的连续参数，且需大量专家干预来调整拓扑结构，缺乏对离散参数（如透镜数量和类型）的自动优化能力。

Method: 结合梯度优化与定制的马尔可夫链蒙特卡洛采样算法，引入跨维度变异和近轴投影操作，实现对连续和离散参数的联合优化。

Result: 在多种透镜设计任务中验证了该方法的有效性，能够在扩大的设计空间中探索更优的复合透镜结构，优于以往方法，并提升了自动化透镜设计中的速度-锐度权衡性能。

Conclusion: 所提方法实现了复合透镜设计的全自动联合优化，显著增强了设计灵活性和性能，推动了自动化光学设计的发展。

Abstract: We introduce a method that automatically and jointly updates both continuous
and discrete parameters of a compound lens design, to improve its performance
in terms of sharpness, speed, or both. Previous methods for compound lens
design use gradient-based optimization to update continuous parameters (e.g.,
curvature of individual lens elements) of a given lens topology, requiring
extensive expert intervention to realize topology changes. By contrast, our
method can additionally optimize discrete parameters such as number and type
(e.g., singlet or doublet) of lens elements. Our method achieves this
capability by combining gradient-based optimization with a tailored Markov
chain Monte Carlo sampling algorithm, using transdimensional mutation and
paraxial projection operations for efficient global exploration. We show
experimentally on a variety of lens design tasks that our method effectively
explores an expanded design space of compound lenses, producing better designs
than previous methods and pushing the envelope of speed-sharpness tradeoffs
achievable by automated lens design.

</details>


### [556] [ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single Image and Controllable Texture Editing](https://arxiv.org/abs/2509.23607)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 本文提出了一种名为ZeroScene的新系统，能够在单张图像输入下实现零样本的3D场景重建与纹理编辑，通过利用大规模视觉模型的先验知识，结合2D分割、深度估计和联合优化策略，生成几何准确、布局一致且细节丰富的3D场景，并支持多视角一致的纹理编辑与PBR材质估计。


<details>
  <summary>Details</summary>
Motivation: 现有的单图3D场景重建方法难以在复杂环境中同时保证个体资产质量与整体场景连贯性，而纹理编辑技术常无法兼顾局部连续性和多视角一致性。

Method: ZeroScene从输入图像中提取对象级的2D分割与深度信息以推断场景空间关系；通过联合优化点云的3D与2D投影损失来更新物体位姿，实现精确对齐；在纹理编辑中引入扩散模型约束和掩码引导的渐进生成策略，并结合PBR材质估计提升真实感。

Result: 实验结果表明，该框架能准确重建物体几何与外观，忠实还原场景布局，生成高度细节化的纹理，并在文本驱动的纹理编辑中保持多视角一致性。

Conclusion: ZeroScene实现了高质量、零样本的单图到3D场景重建与纹理编辑，在几何精度、场景连贯性和纹理真实性方面均表现出优越性能，为3D内容生成提供了有效解决方案。

Abstract: In the field of 3D content generation, single image scene reconstruction
methods still struggle to simultaneously ensure the quality of individual
assets and the coherence of the overall scene in complex environments, while
texture editing techniques often fail to maintain both local continuity and
multi-view consistency. In this paper, we propose a novel system ZeroScene,
which leverages the prior knowledge of large vision models to accomplish both
single image-to-3D scene reconstruction and texture editing in a zero-shot
manner. ZeroScene extracts object-level 2D segmentation and depth information
from input images to infer spatial relationships within the scene. It then
jointly optimizes 3D and 2D projection losses of the point cloud to update
object poses for precise scene alignment, ultimately constructing a coherent
and complete 3D scene that encompasses both foreground and background.
Moreover, ZeroScene supports texture editing of objects in the scene. By
imposing constraints on the diffusion model and introducing a mask-guided
progressive image generation strategy, we effectively maintain texture
consistency across multiple viewpoints and further enhance the realism of
rendered results through Physically Based Rendering (PBR) material estimation.
Experimental results demonstrate that our framework not only ensures the
geometric and appearance accuracy of generated assets, but also faithfully
reconstructs scene layouts and produces highly detailed textures that closely
align with text prompts.

</details>


### [557] [DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph](https://arxiv.org/abs/2509.23703)
*Zhenyu Shu,Jian Yao,Shiqing Xin*

Main category: cs.GR

TL;DR: 本文提出了一种名为DFG-PCN的点云补全框架，通过自适应分配节点度数和几何感知图融合模块，有效提升了不完整点云的重建质量，尤其在细节复杂区域表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用固定的局部区域划分策略（如k近邻），无法应对形状不同区域几何复杂度分布不均的问题，导致表示效率低和重建效果不佳，尤其是在细节丰富或结构不连续的区域。

Method: 提出Degree-Flexible Point Graph Completion Network (DFG-PCN)，采用结合特征变化和曲率的细节感知度量来自适应分配节点度数，并引入基于曼哈顿距离的边聚合与细节引导的局部-全局特征融合的几何感知图集成模块。

Result: 在多个基准数据集上的大量实验表明，该方法 consistently 优于当前最先进的点云补全方法。

Conclusion: DFG-PCN通过自适应图结构和增强的特征融合机制，显著提升了点云补全的精度和鲁棒性，尤其适用于处理几何复杂度不均的不完整点云。

Abstract: Point cloud completion is a vital task focused on reconstructing complete
point clouds and addressing the incompleteness caused by occlusion and limited
sensor resolution. Traditional methods relying on fixed local region
partitioning, such as k-nearest neighbors, which fail to account for the highly
uneven distribution of geometric complexity across different regions of a
shape. This limitation leads to inefficient representation and suboptimal
reconstruction, especially in areas with fine-grained details or structural
discontinuities. This paper proposes a point cloud completion framework called
Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns
node degrees using a detail-aware metric that combines feature variation and
curvature, focusing on structurally important regions. We further introduce a
geometry-aware graph integration module that uses Manhattan distance for edge
aggregation and detail-guided fusion of local and global features to enhance
representation. Extensive experiments on multiple benchmark datasets
demonstrate that our method consistently outperforms state-of-the-art
approaches.

</details>


### [558] [StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency Diffusion Transformer](https://arxiv.org/abs/2509.23709)
*Zhenyu Shu,Jiajun Shen,Zhongui Chen,Xiaoguang Han,Shiqing Xin*

Main category: cs.GR

TL;DR: 本文首次提出通过形状结构（包括部件存在性和部件邻接关系）来控制点云生成，构建了StructureGraph表示，并提出了StrucADT模型，实现了高质量、多样化的可控3D点云生成，在ShapeNet数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云生成方法难以满足用户对生成形状的特定控制需求，缺乏对形状结构的有效控制，限制了其广泛应用。

Method: 提出StructureGraph表示，标注点云部件间的邻接关系；设计StrucADT模型，包含StructureGraphNet提取结构感知特征、cCNF Prior学习受邻接关系控制的潜在特征分布，以及基于这些特征的Diffusion Transformer生成点云。

Result: 在ShapeNet数据集上实现了高质量、多样化且结构一致的点云生成，支持基于用户指定结构的可控生成，性能优于现有方法。

Conclusion: 该方法首次实现了基于形状结构的可控点云生成，显著提升了生成的可控性与质量，推动了3D点云生成技术的实际应用。

Abstract: In the field of 3D point cloud generation, numerous 3D generative models have
demonstrated the ability to generate diverse and realistic 3D shapes. However,
the majority of these approaches struggle to generate controllable 3D point
cloud shapes that meet user-specific requirements, hindering the large-scale
application of 3D point cloud generation. To address the challenge of lacking
control in 3D point cloud generation, we are the first to propose controlling
the generation of point clouds by shape structures that comprise part
existences and part adjacency relationships. We manually annotate the adjacency
relationships between the segmented parts of point cloud shapes, thereby
constructing a StructureGraph representation. Based on this StructureGraph
representation, we introduce StrucADT, a novel structure-controllable point
cloud generation model, which consists of StructureGraphNet module to extract
structure-aware latent features, cCNF Prior module to learn the distribution of
the latent features controlled by the part adjacency, and Diffusion Transformer
module conditioned on the latent features and part adjacency to generate
structure-consistent point cloud shapes. Experimental results demonstrate that
our structure-controllable 3D point cloud generation method produces
high-quality and diverse point cloud shapes, enabling the generation of
controllable point clouds based on user-specified shape structures and
achieving state-of-the-art performance in controllable point cloud generation
on the ShapeNet dataset.

</details>


### [559] [Diff-3DCap: Shape Captioning with Diffusion Models](https://arxiv.org/abs/2509.23718)
*Zhenyu Shu,Jiawei Wen,Shiyang Li,Shiqing Xin,Ligang Liu*

Main category: cs.GR

TL;DR: 本文提出了一种名为Diff-3DCap的3D形状描述方法，利用投影视图和连续扩散模型生成高质量的文本描述，无需依赖昂贵的体素表示或额外分类器。


<details>
  <summary>Details</summary>
Motivation: 传统3D形状描述方法依赖成本高昂的体素表示或物体检测技术，且效果不理想，因此需要一种更高效、准确的新方法。

Method: 采用多视角投影表示3D对象，结合预训练视觉-语言模型提取视觉嵌入，并在扩散模型中将其作为引导信号；通过前向加噪和反向重建过程生成文本描述。

Result: 实验结果表明，Diff-3DCap性能与当前最先进的方法相当，在3D形状描述任务中具有竞争力。

Conclusion: Diff-3DCap通过结合连续扩散模型和视觉语言嵌入，提供了一种有效且无需额外分类器的3D形状描述新框架。

Abstract: The task of 3D shape captioning occupies a significant place within the
domain of computer graphics and has garnered considerable interest in recent
years. Traditional approaches to this challenge frequently depend on the
utilization of costly voxel representations or object detection techniques, yet
often fail to deliver satisfactory outcomes. To address the above challenges,
in this paper, we introduce Diff-3DCap, which employs a sequence of projected
views to represent a 3D object and a continuous diffusion model to facilitate
the captioning process. More precisely, our approach utilizes the continuous
diffusion model to perturb the embedded captions during the forward phase by
introducing Gaussian noise and then predicts the reconstructed annotation
during the reverse phase. Embedded within the diffusion framework is a
commitment to leveraging a visual embedding obtained from a pre-trained
visual-language model, which naturally allows the embedding to serve as a
guiding signal, eliminating the need for an additional classifier. Extensive
results of our experiments indicate that Diff-3DCap can achieve performance
comparable to that of the current state-of-the-art methods.

</details>


### [560] [ReLumix: Extending Image Relighting to Video via Video Diffusion Models](https://arxiv.org/abs/2509.23769)
*Lezhong Wang,Shutong Jin,Ruiqi Cui,Anders Bjorholm Dahl,Jeppe Revall Frisvad,Siavash Bigdeli*

Main category: cs.GR

TL;DR: 本文提出了ReLumix，一种将图像重光照技术无缝应用于视频的两阶段视频重光照框架。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照方法灵活性不足，受限于特定的重光照模型，难以满足实际后期制作中对光照控制的需求。

Method: 将视频重光照分解为两个阶段：首先由艺术家使用任意图像重光照技术对参考帧进行重光照；然后利用微调后的稳定视频扩散模型（SVD）将光照效果传播到整个视频序列，并引入门控交叉注意力机制和时间自举策略以保证时序一致性。

Result: ReLumix在合成数据上训练但能良好泛化到真实视频，在视觉保真度方面表现出显著提升，且支持多种图像重光照方法。

Conclusion: ReLumix通过解耦重光照算法与时间合成过程，提供了一种可扩展、灵活且高效的视频动态光照控制解决方案。

Abstract: Controlling illumination during video post-production is a crucial yet
elusive goal in computational photography. Existing methods often lack
flexibility, restricting users to certain relighting models. This paper
introduces ReLumix, a novel framework that decouples the relighting algorithm
from temporal synthesis, thereby enabling any image relighting technique to be
seamlessly applied to video. Our approach reformulates video relighting into a
simple yet effective two-stage process: (1) an artist relights a single
reference frame using any preferred image-based technique (e.g., Diffusion
Models, physics-based renderers); and (2) a fine-tuned stable video diffusion
(SVD) model seamlessly propagates this target illumination throughout the
sequence. To ensure temporal coherence and prevent artifacts, we introduce a
gated cross-attention mechanism for smooth feature blending and a temporal
bootstrapping strategy that harnesses SVD's powerful motion priors. Although
trained on synthetic data, ReLumix shows competitive generalization to
real-world videos. The method demonstrates significant improvements in visual
fidelity, offering a scalable and versatile solution for dynamic lighting
control.

</details>


### [561] [SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where](https://arxiv.org/abs/2509.23852)
*Yiheng Huang,Junran Peng,Silei Shen,Jingwei Yang,ZeJi Wei,ChenCheng Bai,Yonghao He,Wei Sui,Muyi Sun,Yan Liu,Xu-Cheng Yin,Man Zhang,Zhaoxiang Zhang,Chuanchen Luo*

Main category: cs.GR

TL;DR: 提出了一种结合音频、语言和空间数据的全栈式对话手势生成方法，并通过高精度数据采集和专用评估指标实现了上下文感知的交互手势生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖描述性语言或音频生成非交互式手势，缺乏对交互时机和空间意图的刻画，限制了在机器人、游戏和动画中的应用。

Method: 建立了一种同步捕捉人体运动和空间意图的高精度数据采集方法，开发了基于音频、语言和空间信息驱动的手势生成模型，并设计了用于评估交互时机和空间准确性的专用指标。

Result: 成功在人形机器人上部署该方案，实现了丰富且具有上下文感知能力的物理交互动作。

Conclusion: 该全栈解决方案有效提升了对话中手势生成的交互性和空间准确性，增强了其在实际场景中的适用性。

Abstract: The accompanying actions and gestures in dialogue are often closely linked to
interactions with the environment, such as looking toward the interlocutor or
using gestures to point to the described target at appropriate moments. Speech
and semantics guide the production of gestures by determining their timing
(WHEN) and style (HOW), while the spatial locations of interactive objects
dictate their directional execution (WHERE). Existing approaches either rely
solely on descriptive language to generate motions or utilize audio to produce
non-interactive gestures, thereby lacking the characterization of interactive
timing and spatial intent. This significantly limits the applicability of
conversational gesture generation, whether in robotics or in the fields of game
and animation production. To address this gap, we present a full-stack
solution. We first established a unique data collection method to
simultaneously capture high-precision human motion and spatial intent. We then
developed a generation model driven by audio, language, and spatial data,
alongside dedicated metrics for evaluating interaction timing and spatial
accuracy. Finally, we deployed the solution on a humanoid robot, enabling rich,
context-aware physical interactions.

</details>


### [562] [Neural Visibility of Point Sets](https://arxiv.org/abs/2509.24150)
*Jun-Hao Wang,Yi-Yang Tian,Baoquan Chen,Peng-Shuai Wang*

Main category: cs.GR

TL;DR: 本文提出了一种基于深度学习的点云可见性判定新方法，将问题建模为二分类任务，采用3D U-Net提取视图无关特征，并结合MLP预测点的可见性，显著优于传统HPR方法，在准确性和效率上均有大幅提升。


<details>
  <summary>Details</summary>
Motivation: 由于点云稀疏且缺乏显式连接关系，从给定视角确定点的可见性具有挑战性；传统方法在计算效率、抗噪能力和处理凹面区域或低密度点云方面存在局限。

Method: 提出一种端到端训练的神经网络方法，核心为3D U-Net提取点级特征，结合共享MLP利用特征和视角方向预测可见性，使用渲染3D模型生成的真实可见性标签进行训练。

Result: 在ShapeNet、ABC Dataset和真实数据集上实验表明，该方法在可见性精度上显著优于HPR，最大可达126倍的速度提升，且对噪声和不同密度鲁棒，能良好泛化到未见形状。

Conclusion: 所提方法高效、鲁棒且通用，可广泛应用于点云可视化、表面重建、法线估计、阴影渲染和视点优化等任务。

Abstract: Point clouds are widely used representations of 3D data, but determining the
visibility of points from a given viewpoint remains a challenging problem due
to their sparse nature and lack of explicit connectivity. Traditional methods,
such as Hidden Point Removal (HPR), face limitations in computational
efficiency, robustness to noise, and handling concave regions or low-density
point clouds. In this paper, we propose a novel approach to visibility
determination in point clouds by formulating it as a binary classification
task. The core of our network consists of a 3D U-Net that extracts
view-independent point-wise features and a shared multi-layer perceptron (MLP)
that predicts point visibility using the extracted features and view direction
as inputs. The network is trained end-to-end with ground-truth visibility
labels generated from rendered 3D models. Our method significantly outperforms
HPR in both accuracy and computational efficiency, achieving up to 126 times
speedup on large point clouds. Additionally, our network demonstrates
robustness to noise and varying point cloud densities and generalizes well to
unseen shapes. We validate the effectiveness of our approach through extensive
experiments on the ShapeNet, ABC Dataset and real-world datasets, showing
substantial improvements in visibility accuracy. We also demonstrate the
versatility of our method in various applications, including point cloud
visualization, surface reconstruction, normal estimation, shadow rendering, and
viewpoint optimization. Our code and models are available at
https://github.com/octree-nn/neural-visibility.

</details>


### [563] [NeuralPVS: Learned Estimation of Potentially Visible Sets](https://arxiv.org/abs/2509.24677)
*Xiangyu Wang,Thomas Köhler,Jun Lin Qiu,Shohei Mori,Markus Steinberger,Dieter Schmalstieg*

Main category: cs.GR

TL;DR: NeuralPVS是首个用于可见性计算的深度学习方法，能够在大场景中高效地实现实时从区域可见性判断，运行速度约为100 Hz，漏检几何体少于1%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实时可见性判定在大规模或动态变化环境中一直是计算机图形学中的难题，现有技术计算成本高且多用于静态场景预计算，缺乏对动态和复杂场景的高效支持。

Method: 提出NeuralPVS，使用在体素化场景表示上运行的神经网络进行可见性计算；结合稀疏卷积与3D体积保持交错策略实现数据压缩，并引入一种新的排斥可见性损失函数以引导网络收敛。

Result: NeuralPVS在处理速度（约100 Hz）和精度（漏检<1%）方面均优于现有方法，展现出更强的鲁棒性和对未见场景的泛化能力。

Conclusion: NeuralPVS通过深度学习实现了高效、准确的实时可见性计算，为大规模动态场景的渲染提供了有前景的解决方案。

Abstract: Real-time visibility determination in expansive or dynamically changing
environments has long posed a significant challenge in computer graphics.
Existing techniques are computationally expensive and often applied as a
precomputation step on a static scene. We present NeuralPVS, the first
deep-learning approach for visibility computation that efficiently determines
from-region visibility in a large scene, running at approximately 100 Hz
processing with less than $1\%$ missing geometry. This approach is possible by
using a neural network operating on a voxelized representation of the scene.
The network's performance is achieved by combining sparse convolution with a 3D
volume-preserving interleaving for data compression. Moreover, we introduce a
novel repulsive visibility loss that can effectively guide the network to
converge to the correct data distribution. This loss provides enhanced
robustness and generalization to unseen scenes. Our results demonstrate that
NeuralPVS outperforms existing methods in terms of both accuracy and
efficiency, making it a promising solution for real-time visibility
computation.

</details>


### [564] [Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes](https://arxiv.org/abs/2509.24986)
*Yuhan Wang,Weikai Chen,Zeyu Hu,Runze Zhang,Yingda Yin,Ruoyu Wu,Keyang Luo,Shengju Qian,Yiyan Ma,Hongyi Li,Yuan Gao,Yuhuan Zhou,Hao Luo,Wan Wang,Xiaobin Shen,Zhaowei Li,Kuixin Zhu,Chuanlang Hong,Yueyue Wang,Lijie Feng,Xin Wang,Chen Change Loy*

Main category: cs.GR

TL;DR: 本文提出了一种名为Light-SQ的新型超二次曲面优化框架，用于用户生成内容中的3D形状抽象，强调结构感知、低重叠、部件对齐和基元紧凑性。


<details>
  <summary>Details</summary>
Motivation: 在用户生成内容（UGC）场景中，非专业用户依赖图像到3D的生成模型创建3D资产，需要一种高效且可编辑的形状抽象方法。

Method: 引入SDF雕刻、基于结构感知体分解的块再生填充策略以及自适应残差剪枝，并支持多尺度拟合以保留几何细节。

Result: 实验表明，Light-SQ能高效实现高保真、可编辑的复杂几何形状抽象。

Conclusion: Light-SQ推动了3D UGC创作的可行性，为基于基元的形状抽象提供了有效解决方案。

Abstract: In user-generated-content (UGC) applications, non-expert users often rely on
image-to-3D generative models to create 3D assets. In this context,
primitive-based shape abstraction offers a promising solution for UGC scenarios
by compressing high-resolution meshes into compact, editable representations.
Towards this end, effective shape abstraction must therefore be
structure-aware, characterized by low overlap between primitives, part-aware
alignment, and primitive compactness. We present Light-SQ, a novel
superquadric-based optimization framework that explicitly emphasizes
structure-awareness from three aspects. (a) We introduce SDF carving to
iteratively udpate the target signed distance field, discouraging overlap
between primitives. (b) We propose a block-regrow-fill strategy guided by
structure-aware volumetric decomposition, enabling structural partitioning to
drive primitive placement. (c) We implement adaptive residual pruning based on
SDF update history to surpress over-segmentation and ensure compact results. In
addition, Light-SQ supports multiscale fitting, enabling localized refinement
to preserve fine geometric details. To evaluate our method, we introduce
3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both
reconstruction quality and primitive-level editability. Extensive experiments
demonstrate that Light-SQ enables efficient, high-fidelity, and editable shape
abstraction with superquadrics for complex generated geometry, advancing the
feasibility of 3D UGC creation.

</details>


### [565] [CharGen: Fast and Fluent Portrait Modification](https://arxiv.org/abs/2509.25058)
*Jan-Niklas Dihlmann,Arnela Killguss,Hendrik P. A. Lensch*

Main category: cs.GR

TL;DR: 本文提出了CharGen，一个面向角色图像编辑的系统，结合属性特定的概念滑块和StreamDiffusion采样流程，实现快速、精确且身份一致的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型中进行角色图像的交互式编辑面临细粒度控制、生成速度和视觉保真度之间的权衡挑战，需要更高效且高质量的编辑方法。

Method: 结合属性特定的概念滑块（Concept Sliders）与StreamDiffusion采样流程，并引入轻量级修复步骤（Repair Step）以恢复加速采样中丢失的细节纹理。

Result: 在消融实验和与InstructPix2Pix、Google Gemini的对比中，CharGen实现了2到4倍更快的编辑速度，同时保持精确控制和身份一致性。

Conclusion: CharGen在保证视觉质量和结构一致性的前提下，显著提升了角色图像编辑的效率和可控性，适用于需要快速迭代的交互式应用场景。

Abstract: Interactive editing of character images with diffusion models remains
challenging due to the inherent trade-off between fine-grained control,
generation speed, and visual fidelity. We introduce CharGen, a
character-focused editor that combines attribute-specific Concept Sliders,
trained to isolate and manipulate attributes such as facial feature size,
expression, and decoration with the StreamDiffusion sampling pipeline for more
interactive performance. To counteract the loss of detail that often
accompanies accelerated sampling, we propose a lightweight Repair Step that
reinstates fine textures without compromising structural consistency.
Throughout extensive ablation studies and in comparison to open-source
InstructPix2Pix and closed-source Google Gemini, and a comprehensive user
study, CharGen achieves two-to-four-fold faster edit turnaround with precise
editing control and identity-consistent results. Project page:
https://chargen.jdihlmann.com/

</details>


### [566] [Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives](https://arxiv.org/abs/2509.25094)
*AmirHossein Zamani,Bruno Roy,Arianna Rampini*

Main category: cs.GR

TL;DR: 本文提出了一种语义与可视性感知的无监督可微分框架，用于自动化3D网格的UV映射，提升了纹理生成质量并减少可见接缝。


<details>
  <summary>Details</summary>
Motivation: 现有自动UV映射方法常忽略语义一致性和可视性，且依赖人工操作，导致3D资产创建效率低下。

Method: 结合几何保持的UV学习，引入语义感知（通过无监督分割与逐部分参数化）和可视性感知（利用环境光遮蔽引导接缝位置）的目标函数。

Result: 实验表明，该方法在定性和定量评估中均优于现有方法，生成的UV图集更利于纹理生成，显著减少视觉上的接缝伪影。

Conclusion: 所提方法有效实现了高质量、感知优化的自动UV映射，推动了3D内容生成的自动化。

Abstract: Recent 3D generative models produce high-quality textures for 3D mesh
objects. However, they commonly rely on the heavy assumption that input 3D
meshes are accompanied by manual mesh parameterization (UV mapping), a manual
task that requires both technical precision and artistic judgment. Industry
surveys show that this process often accounts for a significant share of asset
creation, creating a major bottleneck for 3D content creators. Moreover,
existing automatic methods often ignore two perceptually important criteria:
(1) semantic awareness (UV charts should align semantically similar 3D parts
across shapes) and (2) visibility awareness (cutting seams should lie in
regions unlikely to be seen). To overcome these shortcomings and to automate
the mesh parameterization process, we present an unsupervised differentiable
framework that augments standard geometry-preserving UV learning with semantic-
and visibility-aware objectives. For semantic-awareness, our pipeline (i)
segments the mesh into semantic 3D parts, (ii) applies an unsupervised learned
per-part UV-parameterization backbone, and (iii) aggregates per-part charts
into a unified UV atlas. For visibility-awareness, we use ambient occlusion
(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted
seam objective to steer cutting seams toward occluded regions. By conducting
qualitative and quantitative evaluations against state-of-the-art methods, we
show that the proposed method produces UV atlases that better support texture
generation and reduce perceptible seam artifacts compared to recent baselines.
Our implementation code is publicly available at:
https://github.com/AHHHZ975/Semantic-Visibility-UV-Param.

</details>


### [567] [LayerD: Decomposing Raster Graphic Designs into Layers](https://arxiv.org/abs/2509.25134)
*Tomoyuki Suzuki,Kang-Jun Liu,Naoto Inoue,Kota Yamaguchi*

Main category: cs.GR

TL;DR: 提出LayerD方法，将光栅图形设计分解为可重新编辑的图层。


<details>
  <summary>Details</summary>
Motivation: 设计师在图层表示中进行图形设计和编辑，但一旦合成为光栅图像，图层编辑就无法进行。因此需要一种方法将合成后的图像还原为可编辑的图层。

Method: 通过迭代提取未被遮挡的前景图层来实现分解，并采用基于图层通常具有均匀外观的假设的简单而有效的优化方法。

Result: 实验表明，LayerD能够实现高质量的分解，优于基线方法，并可与最新的图像生成器和图层编辑技术结合使用。

Conclusion: LayerD有效解决了光栅图像图层分解问题，支持可重编辑的创意工作流。

Abstract: Designers craft and edit graphic designs in a layer representation, but
layer-based editing becomes impossible once composited into a raster image. In
this work, we propose LayerD, a method to decompose raster graphic designs into
layers for re-editable creative workflow. LayerD addresses the decomposition
task by iteratively extracting unoccluded foreground layers. We propose a
simple yet effective refinement approach taking advantage of the assumption
that layers often exhibit uniform appearance in graphic designs. As
decomposition is ill-posed and the ground-truth layer structure may not be
reliable, we develop a quality metric that addresses the difficulty. In
experiments, we show that LayerD successfully achieves high-quality
decomposition and outperforms baselines. We also demonstrate the use of LayerD
with state-of-the-art image generators and layer-based editing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [568] [How good are LLMs at Retrieving Documents in a Specific Domain?](https://arxiv.org/abs/2509.22658)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型（LLM）和检索增强生成（RAG）的环境科学领域数据检索方法，通过构建领域特定评估数据集，验证了其在多意图查询理解下的检索精度优于传统Elasticsearch系统。


<details>
  <summary>Details</summary>
Motivation: 传统基于索引的搜索引擎缺乏语义理解能力，难以捕捉用户多意图查询的真实需求，尤其在环境与地球科学领域的研究基础设施中表现不佳。

Method: 提出一种自动化构建领域特定评估数据集的方法，并引入基于大语言模型的检索增强生成（RAG）技术，以支持自然语言查询下的高效信息检索。

Result: 定量与定性分析表明，相比基于Elasticsearch的系统，LLM驱动的检索系统在处理多意图查询时具有更高的检索精度和语义理解能力。

Conclusion: 结合RAG与LLM的方法能有效提升环境科学领域知识库的检索质量，特别是在理解复杂、多意图自然语言查询方面优于传统关键词检索系统。

Abstract: Classical search engines using indexing methods in data infrastructures
primarily allow keyword-based queries to retrieve content. While these
indexing-based methods are highly scalable and efficient, due to a lack of an
appropriate evaluation dataset and a limited understanding of semantics, they
often fail to capture the user's intent and generate incomplete responses
during evaluation. This problem also extends to domain-specific search systems
that utilize a Knowledge Base (KB) to access data from various research
infrastructures. Research infrastructures (RIs) from the environmental and
earth science domain, which encompass the study of ecosystems, biodiversity,
oceanography, and climate change, generate, share, and reuse large volumes of
data. While there are attempts to provide a centralized search service using
Elasticsearch as a knowledge base, they also face similar challenges in
understanding queries with multiple intents. To address these challenges, we
proposed an automated method to curate a domain-specific evaluation dataset to
analyze the capability of a search system. Furthermore, we incorporate the
Retrieval of Augmented Generation (RAG), powered by Large Language Models
(LLMs), for high-quality retrieval of environmental domain data using natural
language queries. Our quantitative and qualitative analysis of the evaluation
dataset shows that LLM-based systems for information retrieval return results
with higher precision when understanding queries with multiple intents,
compared to Elasticsearch-based systems.

</details>


### [569] [Federated Consistency- and Complementarity-aware Consensus-enhanced Recommendation](https://arxiv.org/abs/2509.22659)
*Yunqi Mi,Boyang Yan,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 提出了一种名为Fed3CR的联邦推荐系统方法，通过自适应增强共识和一致性与互补性感知优化策略，提升个性化推荐中的共识利用效率和解耦质量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模客户端中交互数据稀疏性和高均匀性导致的共识退化和解耦不足问题，以改善联邦推荐系统中的统计异质性挑战和个人化性能。

Method: 提出自适应共识增强（ACE）策略来学习全局与客户端特定项目嵌入的关系，并设计一致性与互补性感知优化（C2O）策略以获得更有效且互补的表示。Fed3CR为即插即用型方法，可与其他联邦推荐方法结合使用。

Result: 在四个真实世界数据集上的实验表明，Fed3CR在推荐性能上优于现有方法，显著提升了共识的有效性和模型的个性化能力。

Conclusion: Fed3CR能有效提升联邦推荐系统中共识的利用效率与表示解耦质量，具有良好的通用性和应用潜力。

Abstract: Personalized federated recommendation system (FedRec) has gained significant
attention for its ability to preserve privacy in delivering tailored
recommendations. To alleviate the statistical heterogeneity challenges among
clients and improve personalization, decoupling item embeddings into the server
and client-specific views has become a promising way. Among them, the global
item embedding table serves as a consensus representation that integrates and
reflects the collective patterns across all clients. However, the inherent
sparsity and high uniformity of interaction data from massive-scale clients
results in degraded consensus and insufficient decoupling, reducing consensus's
utility. To this end, we propose a \textbf{Fed}erated \textbf{C}onsistency- and
\textbf{C}omplementarity-aware \textbf{C}onsensus-enhanced
\textbf{R}ecommendation (Fed3CR) method for personalized FedRec. To improve the
efficiency of the utilization of consensus, we propose an \textbf{A}daptive
\textbf{C}onsensus \textbf{E}nhancement (ACE) strategy to learn the
relationship between global and client-specific item embeddings. It enables the
client to adaptively enhance specific information in the consensus,
transforming it into a form that best suits itself. To improve the quality of
decoupling, we propose a \textbf{C}onsistency- and
\textbf{C}omplementarity-aware \textbf{O}ptimization (C2O) strategy, which
helps to learn more effective and complementary representations. Notably, our
proposed Fed3CR is a plug-and-play method, which can be integrated with other
FedRec methods to improve its performance. Extensive experiments on four
real-world datasets represent the superior performance of Fed3CR.

</details>


### [570] [Fairness for niche users and providers: algorithmic choice and profile portability](https://arxiv.org/abs/2509.22660)
*Elizabeth McKinnie,Anas Buhayh,Clement Canel,Robin Burke*

Main category: cs.IR

TL;DR: 本文通过模拟研究算法多元化和用户配置文件可移植性对推荐系统中消费者和提供者公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注算法干预以提升推荐系统的公平性，而较少探讨推荐生态系统本身的结构性变化。本文旨在探索算法与平台解耦、用户可选择算法的结构变革对公平性的影响。

Method: 采用模拟方法，研究不同用户配置文件可移植性政策如何影响消费者和提供者的公平性结果。

Result: 模拟结果显示，用户对推荐算法的选择权（算法多元化）有利于小众消费者和小众内容提供者，且不同配置文件处理政策会影响各方的公平性结果。

Conclusion: 推动算法多元化和配置文件可移植性等结构性改革，有助于提升推荐系统中多方利益相关者的公平性，特别是对边缘化群体有益。

Abstract: Ensuring fair outcomes for multiple stakeholders in recommender systems has
been studied mostly in terms of algorithmic interventions: building new models
with better fairness properties, or using reranking to improve outcomes from an
existing algorithm. What has rarely been studied is structural changes in the
recommendation ecosystem itself. Our work explores the fairness impact of
algorithmic pluralism, the idea that the recommendation algorithm is decoupled
from the platform through which users access content, enabling user choice in
algorithms. Prior work using a simulation approach has shown that niche
consumers and (especially) niche providers benefit from algorithmic choice. In
this paper, we use simulation to explore the question of profile portability,
to understand how different policies regarding the handling of user profiles
interact with fairness outcomes for consumers and providers.

</details>


### [571] [Next Point-of-interest (POI) Recommendation Model Based on Multi-modal Spatio-temporal Context Feature Embedding](https://arxiv.org/abs/2509.22661)
*Lingyu Zhang,Guobin Wu,Yan Wang,Pengfei Xu,Jian Liang,Xuan Song,Yunhai Wang*

Main category: cs.IR

TL;DR: 本文提出了一种基于多模态时空上下文特征嵌入的POI推荐模型，结合用户长期偏好与关键时空上下文信息，通过加权融合动态调整长短时特征权重，提升了下一POI预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统POI预测模型主要依赖短期序列信息，忽略了用户的长期偏好和重要的时空上下文特征，导致预测精度受限。

Method: 设计了一个包含时空特征处理、多模态嵌入和自注意力聚合的模型，提取长期偏好和关键时空上下文特征，并采用加权融合方法动态整合长短时特征，最后通过注意力机制进行位置匹配与概率计算。

Result: 在多个交通数据集上实验表明，该模型相比现有SOTA方法具有更高的预测准确率。

Conclusion: 融合多类型特征（尤其是长期偏好与时空上下文）能有效提升下一POI推荐性能，所提出的模型在实际应用中具有优越性。

Abstract: The next Point-of-interest (POI) recommendation is mainly based on sequential
traffic information to predict the user's next boarding point location. This is
a highly regarded and widely applied research task in the field of intelligent
transportation, and there have been many research results to date. Traditional
POI prediction models primarily rely on short-term traffic sequence
information, often neglecting both long-term and short-term preference data, as
well as crucial spatiotemporal context features in user behavior. To address
this issue, this paper introduces user long-term preference information and key
spatiotemporal context information, and proposes a POI recommendation model
based on multimodal spatiotemporal context feature embedding. The model
extracts long-term preference features and key spatiotemporal context features
from traffic data through modules such as spatiotemporal feature processing,
multimodal embedding, and self-attention aggregation. It then uses a weighted
fusion method to dynamically adjust the weights of long-term and short-term
features based on users' historical behavior patterns and the current context.
Finally, the fused features are matched using attention, and the probability of
each location candidate becoming the next location is calculated. This paper
conducts experimental verification on multiple transportation datasets, and the
results show that the POI prediction model combining multiple types of features
has higher prediction accuracy than existing SOTA models and methods.

</details>


### [572] [MTRec: Learning to Align with User Preferences via Mental Reward Models](https://arxiv.org/abs/2509.22807)
*Mengchen Zhao,Yifan Gao,Yaqing Hou,Xiangyang Li,Pengjie Gu,Zhenhua Dong,Ruiming Tang,Yi Cai*

Main category: cs.IR

TL;DR: 提出MTRec框架，通过心理奖励模型和分布逆向强化学习来揭示用户内在满意度，从而提升推荐系统对真实用户偏好的匹配度。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈（如点击）不一定反映用户真实偏好，可能导致推荐系统被误导。

Method: 引入心理奖励模型量化用户满意度，并采用分布逆向强化学习进行训练，用以指导推荐模型。

Result: MTRec在多种推荐模型上均带来显著提升，并在工业短视​​频平台部署后使用户平均观看时长增加7%。

Conclusion: MTRec能有效对齐用户真实偏好，提升推荐性能，具有实际应用价值。

Abstract: Recommendation models are predominantly trained using implicit user feedback,
since explicit feedback is often costly to obtain. However, implicit feedback,
such as clicks, does not always reflect users' real preferences. For example, a
user might click on a news article because of its attractive headline, but end
up feeling uncomfortable after reading the content. In the absence of explicit
feedback, such erroneous implicit signals may severely mislead recommender
systems. In this paper, we propose MTRec, a novel sequential recommendation
framework designed to align with real user preferences by uncovering their
internal satisfaction on recommended items. Specifically, we introduce a mental
reward model to quantify user satisfaction and propose a distributional inverse
reinforcement learning approach to learn it. The learned mental reward model is
then used to guide recommendation models to better align with users' real
preferences. Our experiments show that MTRec brings significant improvements to
a variety of recommendation models. We also deploy MTRec on an industrial short
video platform and observe a 7 percent increase in average user viewing time.

</details>


### [573] [WARBERT: A Hierarchical BERT-based Model for Web API Recommendation](https://arxiv.org/abs/2509.23175)
*Zishuo Xu,Yuhong Gu,Dezhong Yao*

Main category: cs.IR

TL;DR: 本文提出了一种基于分层BERT模型的Web API推荐方法WARBERT，通过融合双组件特征和注意力比较机制，解决了现有方法在语义歧义、细粒度对比不足和检索效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 由于Web 2.0和微服务架构的发展，Web API数量激增，导致对高效API推荐的需求上升。然而，现有方法在语义匹配、细粒度比较和检索效率方面存在挑战，因此需要更精确高效的推荐模型。

Method: 提出WARBERT模型，包含两个部分：WARBERT(R)用于候选API初筛，引入辅助任务（mashup类别判断）提升效果；WARBERT(M)用于精细化匹配，计算API与mashup的相似性。通过结合两者预测结果进行最终推荐，并采用双组件特征融合与注意力机制提取语义表示。

Result: 在ProgrammableWeb数据集上的实验表明，WARBERT优于大多数现有方法，在MTFM模型基础上最高提升了11.7%，显著提高了推荐的准确性和效率。

Conclusion: WARBERT通过结合推荐与匹配机制，并引入辅助任务和语义融合策略，有效提升了Web API推荐的性能，兼具高精度与高效率，适用于大规模API环境下的应用。

Abstract: With the emergence of Web 2.0 and microservices architecture, the number of
Web APIs has increased dramatically, further intensifying the demand for
efficient Web API recommendation. Existing solutions typically fall into two
categories: recommendation-type methods, which treat each API as a label for
classification, and match-type methods, which focus on matching mashups through
API retrieval. However, three critical challenges persist: 1) the semantic
ambiguities in comparing API and mashup descriptions, 2) the lack of detailed
comparisons between the individual API and the mashup in recommendation-type
methods, and 3) time inefficiencies for API retrieval in match-type methods. To
address these challenges, we propose WARBERT, a hierarchical BERT-based model
for Web API recommendation. WARBERT leverages dual-component feature fusion and
attention comparison to extract precise semantic representations of API and
mashup descriptions. WARBERT consists of two main components: WARBERT(R) for
Recommendation and WARBERT(M) for Matching. Specifically, WAR-BERT(R) serves as
an initial filter, narrowing down the candidate APIs, while WARBERT(M) refines
the matching process by calculating the similarity between candidate APIs and
mashup. The final likelihood of a mashup being matched with an API is
determined by combining the predictions from WARBERT(R) and WARBERT(M).
Additionally, WARBERT(R) incorporates an auxiliary task of mashup category
judgment, which enhances its effectiveness in candidate selection. Experimental
results on the ProgrammableWeb dataset demonstrate that WARBERT outperforms
most existing solutions and achieves improvements of up to 11.7% compared to
the model MTFM (Multi-Task Fusion Model), delivering significant enhancements
in accuracy and effiency.

</details>


### [574] [From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation](https://arxiv.org/abs/2509.23649)
*KaiWen Wei,Kejun He,Xiaomian Kang,Jie Zhang,Yuming Yang,Jiang Zhong,He Bai,Junnan Zhu*

Main category: cs.IR

TL;DR: 提出了一种新的生成式推荐训练框架Masked History Learning (MHL)，通过引入历史项目重建任务和熵引导的掩码策略，提升模型对用户行为历史的理解能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐系统依赖于纯粹的自回归训练，仅关注预测下一个项目而忽略了用户交互历史中的内在结构，导致无法捕捉用户的深层意图。

Method: 提出Masked History Learning (MHL)框架，结合自回归目标与掩码历史项目重建任务；采用熵引导的掩码策略选择最具信息量的历史项目进行重建，并设计课程学习调度器逐步从历史重建过渡到未来预测。

Result: 在三个公开数据集上的实验表明，该方法显著优于当前最先进的生成式推荐模型，验证了深入理解用户历史行为对提升推荐性能的重要性。

Conclusion: 通过增强对用户历史行为的理解，而非仅仅预测下一个项目，MHL能够更准确地建模用户意图，从而提升生成式推荐系统的性能。

Abstract: Generative recommendation, which directly generates item identifiers, has
emerged as a promising paradigm for recommendation systems. However, its
potential is fundamentally constrained by the reliance on purely autoregressive
training. This approach focuses solely on predicting the next item while
ignoring the rich internal structure of a user's interaction history, thus
failing to grasp the underlying intent. To address this limitation, we propose
Masked History Learning (MHL), a novel training framework that shifts the
objective from simple next-step prediction to deep comprehension of history.
MHL augments the standard autoregressive objective with an auxiliary task of
reconstructing masked historical items, compelling the model to understand
``why'' an item path is formed from the user's past behaviors, rather than just
``what'' item comes next. We introduce two key contributions to enhance this
framework: (1) an entropy-guided masking policy that intelligently targets the
most informative historical items for reconstruction, and (2) a curriculum
learning scheduler that progressively transitions from history reconstruction
to future prediction. Experiments on three public datasets show that our method
significantly outperforms state-of-the-art generative models, highlighting that
a comprehensive understanding of the past is crucial for accurately predicting
a user's future path. The code will be released to the public.

</details>


### [575] [Constructing Opera Seria in the Iberian Courts: Metastasian Repertoire for Spain and Portugal](https://arxiv.org/abs/2509.23771)
*Ana Llorens,Alvaro Torrente*

Main category: cs.IR

TL;DR: 本文通过统计分析15部为马德里和里斯本宫廷剧院创作的正歌剧咏叹调，并与2404首选自126个梅塔斯塔西奥剧本版本的咏叹调数据进行比较，探讨了佩雷斯、加卢皮、约梅利、孔福尔托和科雷利五位作曲家在为伊比利亚宫廷创作时风格的变化，揭示了当地音乐传统、性别刻板印象和个人创作风格对音乐特征的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨18世纪伊比利亚半岛在欧洲正歌剧潮流中的独特地位，分析国际作曲家为该地区宫廷创作时的风格调整及其背后的社会与音乐动因。

Method: 采用定量统计方法，分析15部专为伊比利亚宫廷创作的歌剧咏叹调在调性、节拍、速度和声乐处理方面的特征，并与包含2404首咏叹调的大规模语料库进行对比。

Result: 发现为伊比利亚宫廷创作的作品在音乐风格上存在特定倾向，表明其音乐实践不仅融入欧洲主流，也受到本地习俗、性别角色预期和作曲家个人风格的影响。

Conclusion: 18世纪伊比利亚宫廷的正歌剧创作体现了国际化与本土化的交织，其独特音乐特征是外部潮流与本地文化因素共同作用的结果。

Abstract: The exceptional reception of Pietro Metastasio's works during the eighteenth
century, all over Europe and in the Iberian Peninsula in particular, is well
documented. Due to that unparalleled success, it is possible to ascertain Spain
and Portugal's participation in international, contemporary tastes and artistic
webs, applicable to both composers and performers. However, this
internationalisation needs to be nuanced, as some characteristics of the
repertoire specifically written for the Peninsula indicate that their court
audiences may have had expectations, both social and strictly musical,
different from those of the public in opera theatres elsewhere in the
continent. In this light, this article investigates in what ways the style of
five composers in the international scene - Perez, Galuppi, Jommelli, Conforto,
and Corselli - varied when commissioned to write opera seria for the Iberian
courts. The statistical analysis of fifteen settings especially written for the
court theatres in Madrid and Lisbon, in comparison to the average data
extracted from a corpus of 2,404 arias from 126 versions of a select number of
Metastasian librettos, allows us to evaluate some particular usages regarding
key, metre, tempo, and treatment of the vocal part. In this manner, through
quantitative analysis, this article places eighteenth-century Iberian music
production and consumption in the context of European opera seria, while
ultimately suggesting that its unique musical characteristics were also partly
dependent on local musical customs, gender stereotypes, and personal
idiosyncrasies alike.

</details>


### [576] [Semantic Representation of Processes with Ontology Design Patterns](https://arxiv.org/abs/2509.23776)
*Ebrahim Norouzi,Sven Hertling,Jörg Waitelonis,Harald Sack*

Main category: cs.IR

TL;DR: 本研究调查了与科学工作流和工程过程建模相关的本体，识别其中隐含的设计模式，并提出一种从现有本体中自动提取设计模式的基线方法，相关资源公开共享。


<details>
  <summary>Details</summary>
Motivation: 材料科学中实验和计算的可重复性依赖于结构化和语义一致的过程模型，但现有本体复杂且难以复用，设计模式往往缺乏明确发布和文档记录。

Method: 调研相关科学工作流与工程过程建模本体，识别其内部隐含的设计模式，提出一种自动提取设计模式的基线方法，并基于人工整理的基准模式进行评估。

Result: 成功识别出多个隐含设计模式，验证了自动提取方法的有效性，并将提取结果与工作流公开在GitHub上供社区使用。

Conclusion: 该研究促进了本体设计模式的可重用性和透明度，为材料科学中的过程建模提供了可扩展、易访问的支持框架。

Abstract: The representation of workflows and processes is essential in materials
science engineering, where experimental and computational reproducibility
depend on structured and semantically coherent process models. Although
numerous ontologies have been developed for process modeling, they are often
complex and challenging to reuse. Ontology Design Patterns (ODPs) offer modular
and reusable modeling solutions to recurring problems; however, these patterns
are frequently neither explicitly published nor documented in a manner
accessible to domain experts. This study surveys ontologies relevant to
scientific workflows and engineering process modeling and identifies implicit
design patterns embedded within their structures. We evaluate the capacity of
these ontologies to fulfill key requirements for process representation in
materials science. Furthermore, we propose a baseline method for the automatic
extraction of design patterns from existing ontologies and assess the approach
against curated ground truth patterns. All resources associated with this work,
including the extracted patterns and the extraction workflow, are made openly
available in a public GitHub repository.

</details>


### [577] [GSID: Generative Semantic Indexing for E-Commerce Product Understanding](https://arxiv.org/abs/2509.23860)
*Haiyang Yang,Qinye Xie,Qingheng Zhang,Liyu Chen,Huike Zou,Chengbao Lian,Shuguang Han,Fei Huang,Jufeng Chen,Bo Zheng*

Main category: cs.IR

TL;DR: 提出了一种数据驱动的方法GSID，用于生成商品的结构化表示，解决了传统手动分类在长尾商品和买家偏好匹配上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于人工定义类别和属性的商品信息组织方式难以覆盖长尾商品且与买家偏好不一致，限制了电商平台效率。

Method: GSID包含两个关键步骤：首先在非结构化商品元数据上进行预训练以学习领域内语义嵌入，然后生成更有效的语义编码用于下游任务。

Result: 实验验证了GSID的有效性，并已在真实电商平台上部署，在商品理解和下游任务中取得了良好效果。

Conclusion: GSID通过数据驱动的方式生成商品语义索引，显著提升了对长尾商品的表达能力和下游应用性能。

Abstract: Structured representation of product information is a major bottleneck for
the efficiency of e-commerce platforms, especially in second-hand ecommerce
platforms. Currently, most product information are organized based on manually
curated product categories and attributes, which often fail to adequately cover
long-tail products and do not align well with buyer preference. To address
these problems, we propose \textbf{G}enerative \textbf{S}emantic
\textbf{I}n\textbf{D}exings (GSID), a data-driven approach to generate product
structured representations. GSID consists of two key components: (1)
Pre-training on unstructured product metadata to learn in-domain semantic
embeddings, and (2) Generating more effective semantic codes tailored for
downstream product-centric applications. Extensive experiments are conducted to
validate the effectiveness of GSID, and it has been successfully deployed on
the real-world e-commerce platform, achieving promising results on product
understanding and other downstream tasks.

</details>


### [578] [Investigating Multi-layer Representations for Dense Passage Retrieval](https://arxiv.org/abs/2509.23861)
*Zhongbin Xie,Thomas Lukasiewicz*

Main category: cs.IR

TL;DR: 本文提出了一种利用预训练语言模型中多个编码层表示来构建文档表示的多层表示（MLR）方法，通过实验验证了其在单向量检索和其他先进训练技术下的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于预训练语言模型的不同层包含不同类型的语言知识，并且在微调过程中表现不同，因此仅使用最后一层隐藏层的向量不足以充分表示文档。

Method: 提出利用多个编码器层的表示来构建文档表示（MLR），研究不同层表示对多向量检索性能的影响，并采用池化策略将多向量模型简化为单向量模型以提高检索效率。

Result: 实验表明，MLR在单向量检索设置下优于双编码器、ME-BERT和ColBERT，并能有效结合检索导向的预训练和难负样本挖掘等先进技术。

Conclusion: 使用多层表示可以更充分地利用预训练语言模型中的信息，提升密集检索模型的性能和效率。

Abstract: Dense retrieval models usually adopt vectors from the last hidden layer of
the document encoder to represent a document, which is in contrast to the fact
that representations in different layers of a pre-trained language model
usually contain different kinds of linguistic knowledge, and behave differently
during fine-tuning. Therefore, we propose to investigate utilizing
representations from multiple encoder layers to make up the representation of a
document, which we denote Multi-layer Representations (MLR). We first
investigate how representations in different layers affect MLR's performance
under the multi-vector retrieval setting, and then propose to leverage pooling
strategies to reduce multi-vector models to single-vector ones to improve
retrieval efficiency. Experiments demonstrate the effectiveness of MLR over
dual encoder, ME-BERT and ColBERT in the single-vector retrieval setting, as
well as demonstrate that it works well with other advanced training techniques
such as retrieval-oriented pre-training and hard negative mining.

</details>


### [579] [Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification](https://arxiv.org/abs/2509.23874)
*Huike Zou,Haiyang Yang,Yindu Su,Liyu Chen,Chengbao Lian,Qingheng Zhang,Shuguang Han,Jufeng Chen*

Main category: cs.IR

TL;DR: 提出了一种结合检索、生成和分类的多值产品检索增强生成模型（MVP-RAG），用于解决商品属性值识别中的级联错误、OOD问题和泛化能力不足，并在真实工业场景中成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有商品属性值识别方法存在级联错误、无法处理分布外属性值及泛化能力差等问题，限制了电商场景下的应用效果。

Method: 将PAVI定义为检索-生成任务，利用商品标题描述作为查询，以商品和属性值为语料库，采用多级检索策略先检索同类商品和候选属性值，再通过大语言模型生成标准化属性值。

Result: 实验表明MVP-RAG优于现有最先进基线方法，并已在实际工业环境中成功部署。

Conclusion: MVP-RAG通过融合检索与生成有效提升了商品属性值识别的准确性与鲁棒性，尤其在处理OOD值和实际应用方面表现出显著优势。

Abstract: Identifying attribute values from product profiles is a key task for
improving product search, recommendation, and business analytics on e-commerce
platforms, which we called Product Attribute Value Identification (PAVI) .
However, existing PAVI methods face critical challenges, such as cascading
errors, inability to handle out-of-distribution (OOD) attribute values, and
lack of generalization capability. To address these limitations, we introduce
Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the
strengths of retrieval, generation, and classification paradigms. MVP-RAG
defines PAVI as a retrieval-generation task, where the product title
description serves as the query, and products and attribute values act as the
corpus. It first retrieves similar products of the same category and candidate
attribute values, and then generates the standardized attribute values. The key
advantages of this work are: (1) the proposal of a multi-level retrieval
scheme, with products and attribute values as distinct hierarchical levels in
PAVI domain (2) attribute value generation of large language model to
significantly alleviate the OOD problem and (3) its successful deployment in a
real-world industrial environment. Extensive experimental results demonstrate
that MVP-RAG performs better than the state-of-the-art baselines.

</details>


### [580] [Multi-Item-Query Attention for Stable Sequential Recommendation](https://arxiv.org/abs/2509.24424)
*Mingshi Xu,Haoren Zhu,Wilfred Siu Hung Ng*

Main category: cs.IR

TL;DR: 提出多查询注意力机制MIQ-Attn以提升序列推荐系统的稳定性与准确性，通过构建多个多样化查询向量缓解噪声影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于单查询的掩码注意力模型对用户交互数据中的噪声敏感，导致预测不可靠。

Method: 设计Multi-Item-Query注意力机制（MIQ-Attn），从用户交互中构建多个多样化的查询向量，并可作为现有序列推荐模型的即插即用组件。

Result: 实验表明，MIQ-Attn在多个基准数据集上显著提升了推荐性能。

Conclusion: MIQ-Attn有效增强了模型的稳定性和预测准确性，适用于噪声环境下的序列推荐任务。

Abstract: The inherent instability and noise in user interaction data challenge
sequential recommendation systems. Prevailing masked attention models, relying
on a single query from the most recent item, are sensitive to this noise,
reducing prediction reliability. We propose the Multi-Item-Query attention
mechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn
constructs multiple diverse query vectors from user interactions, effectively
mitigating noise and improving consistency. It is designed for easy adoption as
a drop-in replacement for existing single-query attention. Experiments show
MIQ-Attn significantly improves performance on benchmark datasets.

</details>


### [581] [UniDex: Rethinking Search Inverted Indexing with Unified Semantic Modeling](https://arxiv.org/abs/2509.24632)
*Zan Li,Jiahui Chen,Yuan Chai,Xiaoze Jiang,Xiaohua Qi,Zhiheng Qin,Runbin Zhou,Shun Zuo,Guangchao Hao,Kefeng Wang,Jingshan Lv,Yupeng Huang,Xiao Liang,Han Li*

Main category: cs.IR

TL;DR: UniDex是一种基于模型的新型倒排索引方法，通过统一语义建模提升检索效果，取代传统的术语匹配，已在快手短视频搜索系统中成功部署。


<details>
  <summary>Details</summary>
Motivation: 传统倒排索引依赖精确术语匹配，局限于表面词汇重叠，泛化能力和检索效果受限。

Method: 提出UniDex，包含UniTouch（将查询和文档映射为语义ID）和UniRank（语义匹配排序）两个组件，采用统一语义建模的简化架构。

Result: 在大规模工业数据集和真实在线流量测试中，UniDex显著提升了检索能力，并在快手搜索系统中验证了其实际有效性。

Conclusion: UniDex实现了从基于术语到基于模型的倒排索引范式转变，提高了语义泛化能力并降低了维护开销。

Abstract: Inverted indexing has traditionally been a cornerstone of modern search
systems, leveraging exact term matches to determine relevance between queries
and documents. However, this term-based approach often emphasizes surface-level
token overlap, limiting the system's generalization capabilities and retrieval
effectiveness. To address these challenges, we propose UniDex, a novel
model-based method that employs unified semantic modeling to revolutionize
inverted indexing. UniDex replaces complex manual designs with a streamlined
architecture, enhancing semantic generalization while reducing maintenance
overhead. Our approach involves two key components: UniTouch, which maps
queries and documents into semantic IDs for improved retrieval, and UniRank,
which employs semantic matching to rank results effectively. Through
large-scale industrial datasets and real-world online traffic assessments, we
demonstrate that UniDex significantly improves retrieval capabilities, marking
a paradigm shift from term-based to model-based indexing. Our deployment within
Kuaishou's short-video search systems further validates UniDex's practical
effectiveness, serving hundreds of millions of active users efficiently.

</details>


### [582] [Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval](https://arxiv.org/abs/2509.24869)
*Junwei Lan,Jianlyu Chen,Zheng Liu,Chaofan Li,Siqi Bao,Defu Lian*

Main category: cs.IR

TL;DR: 本文提出了一种名为Retro*的新型文档检索方法，通过基于评分标准的相关性评估机制和强化学习算法，在任务与文档关系复杂的情况下实现细粒度、可解释且高效的相关性判断，显著提升了推理密集型检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索技术在处理任务与文档之间间接或隐含关联时，难以进行细粒度推理以准确评估相关性，限制了LLM代理和RAG系统的效果。

Method: 提出Retro*方法，引入基于评分标准（rubric-based）的相关性打分机制，并结合多条推理路径的分数集成实现测试时扩展；设计一种新的强化学习算法，利用复合奖励优化模型的推理能力。

Result: 实验表明，Retro*在BRIGHT基准上优于现有的文档检索方法，取得了最先进的性能。

Conclusion: Retro*通过显式的推理机制和可扩展的评分策略，有效解决了复杂相关性判断下的文档检索难题，具备良好的适用性、可扩展性和效率。

Abstract: With the growing popularity of LLM agents and RAG, it has become increasingly
important to retrieve documents that are essential for solving a task, even
when their connection to the task is indirect or implicit. Addressing this
problem requires fine-grained reasoning to accurately assess the relevance
between the task and each candidate document. This capability, however, poses a
significant challenge for existing IR techniques. Despite recent progress in
reasoning-enhanced IR, existing approaches still face significant challenges in
applicability, scalability, and efficiency. In this work, we propose Retro*, a
novel approach for reasoning-intensive document retrieval. Our method
introduces a rubric-based relevance scoring mechanism, enabling the model to
reason about the relationship between a task and a document based on explicitly
defined criteria, whereby producing a fine-grained, interpretable relevance
score. Retro* also supports test-time scaling by combining multiple reasoning
trajectories via score integration, which produces more reliable relevance
estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel
reinforcement learning algorithm tailored for its relevance scoring mechanism,
which employs two composite rewards to fully exploit the trajectories of each
training sample. Our experiments show that Retro* outperforms existing document
retrieval methods with notable advantages, leading to state-of-the-art
performance on the BRIGHT benchmark.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [583] [Localizing Adversarial Attacks To Produces More Imperceptible Noise](https://arxiv.org/abs/2509.22710)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.LG

TL;DR: 本研究系统评估了局部对抗性攻击在FGSM、PGD和C&W等主流方法中的有效性，发现局部攻击在降低像素扰动、提升图像质量（PSNR、SSIM）方面优于全局攻击，但计算成本更高且攻击成功率略有下降。


<details>
  <summary>Details</summary>
Motivation: 局部对抗噪声的潜力尚未充分探索，而传统对抗攻击多关注全局扰动，因此需要系统评估局部攻击的性能与权衡。

Method: 通过引入二值掩码限制噪声作用区域，对FGSM、PGD和C&W等方法进行局部化改造，并比较其在攻击成功率、扰动程度、图像质量和计算效率方面的表现。

Result: 局部攻击显著降低了平均像素扰动，提升了PSNR和SSIM，表明其更具隐蔽性；但计算开销增加，攻击成功率略有下降；其中PGD和C&W等迭代方法比FGSM更适应局部约束。

Conclusion: 局部对抗攻击在隐蔽性方面优于全局攻击，但需权衡计算成本与攻击成功率，迭代方法更适合局部攻击场景，该研究为攻击策略优化和防御机制设计提供了重要参考。

Abstract: Adversarial attacks in machine learning traditionally focus on global
perturbations to input data, yet the potential of localized adversarial noise
remains underexplored. This study systematically evaluates localized
adversarial attacks across widely-used methods, including FGSM, PGD, and C&W,
to quantify their effectiveness, imperceptibility, and computational
efficiency. By introducing a binary mask to constrain noise to specific
regions, localized attacks achieve significantly lower mean pixel
perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved
Structural Similarity Index (SSIM) compared to global attacks. However, these
benefits come at the cost of increased computational effort and a modest
reduction in Attack Success Rate (ASR). Our results highlight that iterative
methods, such as PGD and C&W, are more robust to localization constraints than
single-step methods like FGSM, maintaining higher ASR and imperceptibility
metrics. This work provides a comprehensive analysis of localized adversarial
attacks, offering practical insights for advancing attack strategies and
designing robust defensive systems.

</details>


### [584] [In-Context Learning can Perform Continual Learning Like Humans](https://arxiv.org/abs/2509.22764)
*Liuwang Kang,Fan Wang,Shaoshan Liu,Hung-Chyun Chou,Chuan Lin,Ning Ding*

Main category: cs.LG

TL;DR: 本文研究了大语言模型在多任务持续学习中的记忆保持特性，提出了上下文内持续学习（ICCL）框架，并发现特定大模型在分布式练习下表现出类似人类的记忆“间隔效应”，且线性注意力模型展现出更接近人类的记忆模式。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在多任务顺序到达时是否能实现长期保持和跨任务知识积累，受人类记忆研究启发，填补上下文学习在持续学习场景中的研究空白。

Method: 通过任务调度和提示重排，将上下文学习扩展为上下文内持续学习（ICCL），并在马尔可夫链基准上进行实验，提出人类记忆相似性度量来评估模型与人类记忆动态的一致性。

Result: 实验表明某些大语言模型存在记忆‘甜点’区间，线性注意力模型如MAMBA和RWKV表现出更接近人类的记忆模式，尽管其整体保持性能低于Transformer模型。

Conclusion: ICCL是一种认知上合理且实际有效的纯推理持续学习范式，可缓解灾难性遗忘，解决传统持续学习中的稳定性-可塑性困境。

Abstract: Large language models (LLMs) can adapt to new tasks via in-context learning
(ICL) without parameter updates, making them powerful learning engines for fast
adaptation. While extensive research has examined ICL as a few-shot learner,
whether it can achieve long-term retention and cross-task knowledge
accumulation when multitasks arrive sequentially remains underexplored.
Motivated by human memory studies, we investigate the retention characteristics
of ICL in multitask settings and extend it to in-context continual learning
(ICCL), where continual learning ability emerges through task scheduling and
prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,
for specific large-language models, ICCL benefits from distributed practice
(DP) in a manner analogous to humans, consistently revealing a spacing "sweet
spot" for retention. Beyond retention performance, we propose a human-retention
similarity metric to quantify how closely a continual-learning (CL) method
aligns with human retention dynamics. Using this metric, we show that
linear-attention models such as MAMBA and RWKV exhibit particularly human-like
retention patterns, despite their retention performance lagging behind that of
Transformer-based LLMs. Overall, our results establish ICCL as both cognitively
plausible and practically effective, providing an inference-only CL paradigm
that mitigates catastrophic forgetting and addresses the stability-plasticity
dilemma in conventional CL methods.

</details>


### [585] [Communication-Efficient and Interoperable Distributed Learning](https://arxiv.org/abs/2509.22823)
*Mounssif Krouka,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出了一种通信高效的分布式学习框架，支持模型异构性和推理时的模块化组合，在保持隐私的同时实现了跨不同架构的协同学习。


<details>
  <summary>Details</summary>
Motivation: 解决异构模型架构之间协同学习的互操作性和隐私保护问题。

Method: 所有客户端采用统一的融合层输出维度，将模型分为个性化的基础模块和通用的模块化模块，仅共享融合层输出，保留模型参数和架构的私密性。

Result: 实验结果表明，该框架相比联邦学习（FL）和联邦分割学习（FSL）基线方法具有更高的通信效率，并在异构架构下保持稳定的训练性能。

Conclusion: 所提出的框架有效支持了异构模型间的高效、隐私保护的协同学习。

Abstract: Collaborative learning across heterogeneous model architectures presents
significant challenges in ensuring interoperability and preserving privacy. We
propose a communication-efficient distributed learning framework that supports
model heterogeneity and enables modular composition during inference. To
facilitate interoperability, all clients adopt a common fusion-layer output
dimension, which permits each model to be partitioned into a personalized base
block and a generalized modular block. Clients share their fusion-layer
outputs, keeping model parameters and architectures private. Experimental
results demonstrate that the framework achieves superior communication
efficiency compared to federated learning (FL) and federated split learning
(FSL) baselines, while ensuring stable training performance across
heterogeneous architectures.

</details>


### [586] [On the Capacity of Self-Attention](https://arxiv.org/abs/2509.22840)
*Micah Adler*

Main category: cs.LG

TL;DR: 本文提出了关系图识别（RGR）框架，用于形式化分析自注意力机制在给定资源预算下能可靠恢复的关系数量，并推导出一个容量扩展定律：$D_K = \Theta(m' \log m' / d_{\text{model}})$ 是恢复 $m'$ 个关系的必要且充分条件。该定律为多头注意力机制提供了基于容量的新解释，表明将固定键维度 $D_K$ 分配给多个小头可缓解压缩引起的关系干扰，从而提升可恢复关系数。实验验证了理论预测的性能阈值和多头优势。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制虽被广泛认为能学习token间的关系，但其在给定资源预算下能可靠恢复多少种不同关系尚缺乏形式化理解。现有研究缺乏对注意力容量的量化分析，尤其是多头机制的设计原则缺乏理论依据。因此，需要一个形式化的框架来定义并分析自注意力的关系容量及其与模型结构（如多头）的关系。

Method: 引入关系图识别（RGR）任务，将键-查询通道建模为包含 $m$ 个节点和 $m'$ 条有向边的图，目标是在给定上下文时恢复每个节点的邻居。使用总键维度 $D_K = h\,d_k$ 作为资源度量，在此框架下通过信息论方法推导容量下限，并通过显式构造证明其充分性，从而得到容量缩放律。同时设计控制变量的单层实验验证理论结果。

Result: 1) 推导并验证了自注意力容量的缩放律：$D_K = \Theta(m' \log m' / d_{\text{model}})$ 是恢复 $m'$ 个关系的必要且充分条件；2) 提出了基于容量的多头注意力新解释：当存在特征压缩（$m > d_{\text{model}}$）时，单一大头会因子空间重叠产生干扰，而将 $D_K$ 分配给多个小头可有效缓解干扰；3) 实验显示性能存在与理论预测一致的锐利阈值，并证实多头结构在相同 $D_K$ 下优于单头。

Conclusion: 本文为自注意力机制提供了首个形式化的关系容量分析框架（RGR），并得出明确的容量缩放定律。该结果为多头注意力的设计提供了新的理论依据——即使每个元素只关注单一目标，分散的小头仍可通过减少压缩导致的表示干扰来提升容量。这为模型架构中键-查询资源的分配提供了原则性指导。

Abstract: While self-attention is known to learn relations among tokens, we lack a
formal understanding of its capacity: how many distinct relations can a single
layer reliably recover for a given budget?
  To formalize this, we introduce Relational Graph Recognition (RGR), where the
key-query channel represents a graph on $m$ items with $m'$ directed edges,
and, given a context of items, must recover the neighbors of each item. We
measure resources by the total key dimension $D_K = h\,d_k$. Within this
framework, we analytically derive a capacity scaling law and validate it
empirically. We show that $D_K = \Theta(m' \log m' / d_{\text{model}})$ is both
necessary (information-theoretic lower bound) and sufficient (explicit
construction) in a broad class of graphs to recover $m'$ relations. This
scaling law directly leads to a new, capacity-based rationale for multi-head
attention that applies even when each item only attends to a single target.
When embeddings are uncompressed ($m = d_{\text{model}}$) and the graph is a
permutation, a single head suffices. However, compression ($m >
d_{\text{model}}$) forces relations into overlapping subspaces, creating
interference that a single large head cannot disentangle. Our analysis shows
that allocating a fixed $D_K$ across many small heads mitigates this
interference, increasing the number of recoverable relations. Controlled
single-layer experiments mirror the theory, revealing a sharp performance
threshold that matches the predicted capacity scaling and confirms the benefit
of distributing $D_K$ across multiple heads.
  Altogether, these results provide a concrete scaling law for self-attention
capacity and a principled design rule for allocating key-query budget across
heads.

</details>


### [587] [Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data](https://arxiv.org/abs/2509.22850)
*Roie Kazoom,Yuval Ratzabi,Etamar Rothstein,Ofer Hadar*

Main category: cs.LG

TL;DR: 提出一种针对表格数据的新型黑盒、基于决策的对抗攻击方法，结合无梯度方向估计和迭代边界搜索，高效攻击离散和连续特征空间，在少量查询下实现超过90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 结构化数据中的对抗鲁棒性相比视觉和语言领域研究较少，亟需探索针对表格数据的有效对抗攻击方法以评估模型安全性。

Method: 采用无梯度的方向估计与迭代边界搜索相结合的方法，设计了一种适用于表格数据的黑盒、决策-based对抗攻击，能在有限的 oracle 访问下有效操作离散和连续特征。

Result: 在多种模型（从传统机器学习分类器到基于大语言模型的流水线）上验证了该方法的有效性，几乎攻破整个测试集，成功率稳定超过90%，且每次攻击所需查询次数很少。

Conclusion: 表格数据模型在对抗扰动面前极为脆弱，暴露出实际决策系统中潜在的安全风险，迫切需要构建更强的防御机制。

Abstract: Adversarial robustness in structured data remains an underexplored frontier
compared to vision and language domains. In this work, we introduce a novel
black-box, decision-based adversarial attack tailored for tabular data. Our
approach combines gradient-free direction estimation with an iterative boundary
search, enabling efficient navigation of discrete and continuous feature spaces
under minimal oracle access. Extensive experiments demonstrate that our method
successfully compromises nearly the entire test set across diverse models,
ranging from classical machine learning classifiers to large language model
(LLM)-based pipelines. Remarkably, the attack achieves success rates
consistently above 90%, while requiring only a small number of queries per
instance. These results highlight the critical vulnerability of tabular models
to adversarial perturbations, underscoring the urgent need for stronger
defenses in real-world decision-making systems.

</details>


### [588] [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
*Yaswanth Chittepu,Prasann Singhal,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 提出DPO-PoP方法，利用偏好之上的偏好（preference-over-preference）来推断自适应边距，提升奖励模型的判别与生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在奖励模型学习中使用的固定或简单边距无法准确反映不同偏好的强度差异，且依赖难以可靠获取的精确偏好评分，因此需要更灵活、鲁棒的边距建模方式。

Method: 引入偏好之上的偏好作为监督信号，通过该序数信号在每个数据点上推断自适应边距，并将其融入Direct Preference Optimization（DPO），提出DPO-PoP方法。同时设计两种采样策略以平衡判别与生成性能。

Result: 在UltraFeedback数据集上，DPO-PoP优于标准DPO、带固定边距的DPO以及使用真实边距的DPO；实验还揭示了判别性能与生成质量之间的权衡关系。

Conclusion: 建模偏好强度有助于提升模型泛化能力和对齐效果，通过偏好之上的偏好实现自适应边距是一种有效且可行的方法，且可通过采样策略调节判别与生成性能的平衡。

Abstract: Margin-based optimization is fundamental to improving generalization and
robustness in classification tasks. In the context of reward model learning
from preferences within Reinforcement Learning from Human Feedback (RLHF),
existing methods typically rely on no margins, fixed margins, or margins that
are simplistic functions of preference ratings. However, such formulations
often fail to account for the varying strengths of different preferences, for
example some preferences are associated with larger margins between responses,
or they rely on noisy margin information derived from ratings. We argue that
modeling the strength of preferences can lead to better generalization and more
faithful alignment. Furthermore, many existing methods that use adaptive
margins assume access to accurate preference scores, which can be difficult for
humans to provide reliably. We propose an approach that leverages preferences
over preferences, that is annotations indicating which of two preferences
reflects a stronger distinction. We use this ordinal signal to infer adaptive
margins on a per-datapoint basis. We introduce an extension to Direct
Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from
preference-over-preference supervision, enabling improved discriminative and
generative performance. Empirically, our method outperforms vanilla DPO, DPO
with fixed margins, and DPO with ground-truth margins on the UltraFeedback
dataset. Additionally, we show that there is a tradeoff between discriminative
and generative performance: improving test classification accuracy,
particularly by correctly labeling weaker preferences at the expense of
stronger ones, can lead to a decline in generative quality. To navigate this
tradeoff, we propose two sampling strategies to gather
preference-over-preference labels: one favoring discriminative performance and
one favoring generative performance.

</details>


### [589] [Observation-Free Attacks on Online Learning to Rank](https://arxiv.org/abs/2509.22855)
*Sameep Chattopadhyay,Nikhil Karamchandani,Sharayu Mohair*

Main category: cs.LG

TL;DR: 本文提出了一种针对在线学习排序（OLTR）算法的新型攻击框架，能够以极少的操作次数（O(log T)）有效提升目标项目在推荐列表中的排名，同时导致学习算法产生线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 尽管OLTR算法被广泛应用，但其面对协同对抗攻击的脆弱性尚未被充分研究。本文旨在填补这一空白，揭示主流OLTR算法的安全隐患。

Method: 提出了一个新型攻击框架，设计了两种攻击策略CascadeOFA（针对CascadeUCB1）和PBMOFA（针对PBM-UCB），并通过理论分析证明其实现高效攻击所需的操纵次数仅为O(log T)。

Result: 理论结果表明，所提攻击策略可在T - o(T)轮中使目标项目持续出现在前K推荐列表中，并导致线性遗憾；实验部分在真实数据上验证了攻击的有效性。

Conclusion: 该研究表明主流OLTR算法易受低代价对抗攻击，提示在实际部署中需考虑更强的安全防御机制。

Abstract: Online learning to rank (OLTR) plays a critical role in information retrieval
and machine learning systems, with a wide range of applications in search
engines and content recommenders. However, despite their extensive adoption,
the susceptibility of OLTR algorithms to coordinated adversarial attacks
remains poorly understood. In this work, we present a novel framework for
attacking some of the widely used OLTR algorithms. Our framework is designed to
promote a set of target items so that they appear in the list of top-K
recommendations for T - o(T) rounds, while simultaneously inducing linear
regret in the learning algorithm. We propose two novel attack strategies:
CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical
guarantees showing that both strategies require only O(log T) manipulations to
succeed. Additionally, we supplement our theoretical analysis with empirical
results on real-world data.

</details>


### [590] [Neighborhood Sampling Does Not Learn the Same Graph Neural Network](https://arxiv.org/abs/2509.22868)
*Zehao Niu,Mihai Anitescu,Jie Chen*

Main category: cs.LG

TL;DR: 本文通过神经正切核（NTK）工具对图神经网络中的邻域采样方法进行理论分析，研究了不同采样方法在有限样本下的后验高斯过程差异及其收敛性，并指出后验协方差的不可比较性解释了为何没有一种采样方法始终占优。


<details>
  <summary>Details</summary>
Motivation: 邻域采样在大规模图神经网络训练中广泛应用，但其系统行为尚不明确，缺乏理论理解。

Method: 采用神经正切核（NTK）方法，将图神经网络的训练动态与其无限宽对应的高斯过程（GP）进行类比，分析多种邻域采样方法的后验GP特性。

Result: 在样本有限时，不同采样方法对应的后验分布不同；随着样本量增加则趋于相同；后验协方差下界表明均方预测误差无法一致比较，导致无单一方法占优。

Conclusion: 邻域采样方法的选择会影响训练动态和预测性能，但由于后验协方差的不可比较性，不存在绝对最优的采样策略，需根据具体场景权衡。

Abstract: Neighborhood sampling is an important ingredient in the training of
large-scale graph neural networks. It suppresses the exponential growth of the
neighborhood size across network layers and maintains feasible memory
consumption and time costs. While it becomes a standard implementation in
practice, its systemic behaviors are less understood. We conduct a theoretical
analysis by using the tool of neural tangent kernels, which characterize the
(analogous) training dynamics of neural networks based on their infinitely wide
counterparts -- Gaussian processes (GPs). We study several established
neighborhood sampling approaches and the corresponding posterior GP. With
limited samples, the posteriors are all different, although they converge to
the same one as the sample size increases. Moreover, the posterior covariance,
which lower-bounds the mean squared prediction error, is uncomparable, aligning
with observations that no sampling approach dominates.

</details>


### [591] [FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents](https://arxiv.org/abs/2509.23803)
*Pramit Saha,Joshua Strong,Divyanshu Mishra,Cheng Ouyang,J. Alison Noble*

Main category: cs.LG

TL;DR: 本文提出了一种基于智能体的联邦学习框架及评测基准FedAgentBench，旨在解决医疗场景中联邦学习部署的复杂操作挑战，如客户端选择、数据异构性协调和算法适配等，通过201个数据集和多种LLM评估了智能体在自动化联邦学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中联邦学习在医疗领域的应用面临诸多操作性挑战，包括客户端选择、协调、数据预处理、数据标准化和算法选择，现有研究忽视了这些实际问题，因此需要构建能自主协作的智能体系统以减少人为干预。

Method: 设计了一个涵盖联邦学习全流程的智能体驱动框架，并构建了包含40种算法和201个医疗数据集的评测基准FedAgentBench，评估了14个开源和10个专有大语言模型在6种医疗模态下的自动化能力。

Result: 实验表明GPT-4.1和DeepSeek V3等强模型能在部分阶段实现自动化，但在涉及隐式目标的复杂、相互依赖任务上仍存在挑战。

Conclusion: 智能体驱动的联邦学习具有潜力，但需进一步提升模型在复杂、多步骤协作任务中的推理与自主决策能力。

Abstract: Federated learning (FL) allows collaborative model training across healthcare
sites without sharing sensitive patient data. However, real-world FL deployment
is often hindered by complex operational challenges that demand substantial
human efforts. This includes: (a) selecting appropriate clients (hospitals),
(b) coordinating between the central server and clients, (c) client-level data
pre-processing, (d) harmonizing non-standardized data and labels across
clients, and (e) selecting FL algorithms based on user instructions and
cross-client data characteristics. However, the existing FL works overlook
these practical orchestration challenges. These operational bottlenecks
motivate the need for autonomous, agent-driven FL systems, where intelligent
agents at each hospital client and the central server agent collaboratively
manage FL setup and model training with minimal human intervention. To this
end, we first introduce an agent-driven FL framework that captures key phases
of real-world FL workflows from client selection to training completion and a
benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to
autonomously coordinate healthcare FL. Our framework incorporates 40 FL
algorithms, each tailored to address diverse task-specific requirements and
cross-client characteristics. Furthermore, we introduce a diverse set of
complex tasks across 201 carefully curated datasets, simulating 6
modality-specific real-world healthcare environments, viz., Dermatoscopy,
Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic
performance of 14 open-source and 10 proprietary LLMs spanning small, medium,
and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3
can automate various stages of the FL pipeline, our results reveal that more
complex, interdependent tasks based on implicit goals remain challenging for
even the strongest models.

</details>


### [592] [From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants](https://arxiv.org/abs/2509.22881)
*Karim Khamaisi,Nicolas Keller,Stefan Krummenacher,Valentin Huber,Bernhard Fässler,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 本文对基于声音的异常检测方法进行了比较分析，旨在改善水电站的预测性维护。研究中使用了来自奥地利Rodundwerk II抽水蓄能电站的两个真实世界数据集，并评估了LSTM自动编码器、K-Means和单类SVM三种机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对水电站的数据集，现有的声学异常检测研究大多依赖于通用工业或合成数据集。本文旨在填补这一空白，通过实际案例提高水电站的预测性维护能力。

Method: 在高度嘈杂条件下解决声学前处理的关键挑战后，提取时域和频域特征，然后对比测试三种机器学习模型：LSTM自动编码器、K-Means和单类SVM。

Result: 单类SVM在准确率（ROC AUC 0.966-0.998）和训练时间之间取得了最佳平衡，而LSTM自动编码器虽然检测效果强（ROC AUC 0.889-0.997），但计算成本较高。

Conclusion: 单类SVM是适用于水电站声学异常检测的高效模型，能够在保证高精度的同时显著降低计算开销，适合实际部署。

Abstract: In the context of industrial factories and energy producers, unplanned
outages are highly costly and difficult to service. However, existing
acoustic-anomaly detection studies largely rely on generic industrial or
synthetic datasets, with few focused on hydropower plants due to limited
access. This paper presents a comparative analysis of acoustic-based anomaly
detection methods, as a way to improve predictive maintenance in hydropower
plants. We address key challenges in the acoustic preprocessing under highly
noisy conditions before extracting time- and frequency-domain features. Then,
we benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which
are tested on two real-world datasets from the Rodundwerk II pumped-storage
plant in Austria, one with induced anomalies and one with real-world
conditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC
0.966-0.998) and minimal training time, while the LSTM autoencoder delivered
strong detection (ROC AUC 0.889-0.997) at the expense of higher computational
cost.

</details>


### [593] [GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning](https://arxiv.org/abs/2509.24031)
*Umang Garg,Bowen Zhang,Anantanjit Subrahmanya,Chandrakanth Gudavalli,BS Manjunath*

Main category: cs.LG

TL;DR: 本文提出了GPSMasked Trajectory Transformer (GPS-MTM)，一种用于大规模移动数据的基础模型，通过将移动性分解为状态（兴趣点类别）和动作（个体转移）两种模态，并利用双向Transformer进行自监督掩码建模，有效捕捉人类移动中的正常模式，在多种下游任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹建模方法通常将轨迹扁平化为坐标流，忽略了移动行为的语义结构。为此，作者希望构建一个能够捕捉人类移动规律的基础模型，以支持复杂的轨迹分析任务。

Method: 提出GPS-MTM模型，将轨迹分解为状态和动作两种模态，采用双向Transformer架构，通过跨模态掩码重建实现自监督学习，从而捕捉移动数据中的语义相关性。

Result: 在Numosim-LA、Urban Anomalies和Geolife等多个基准数据集上，GPS-MTM在轨迹补全、下一站点预测等任务中均优于现有方法，尤其在需要上下文推理的逆向和正向动力学任务中表现突出。

Conclusion: GPS-MTM是一种强大的轨迹分析基础模型，有望推动移动数据作为大规模表征学习的一类重要模态的发展。

Abstract: Foundation models have driven remarkable progress in text, vision, and video
understanding, and are now poised to unlock similar breakthroughs in trajectory
modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a
foundation model for large-scale mobility data that captures patterns of
normalcy in human movement. Unlike prior approaches that flatten trajectories
into coordinate streams, GPS-MTM decomposes mobility into two complementary
modalities: states (point-of-interest categories) and actions (agent
transitions). Leveraging a bi-directional Transformer with a self-supervised
masked modeling objective, the model reconstructs missing segments across
modalities, enabling it to learn rich semantic correlations without manual
labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and
Geolife, GPS-MTM consistently outperforms on downstream tasks such as
trajectory infilling and next-stop prediction. Its advantages are most
pronounced in dynamic tasks (inverse and forward dynamics), where contextual
reasoning is critical. These results establish GPS-MTM as a robust foundation
model for trajectory analytics, positioning mobility data as a first-class
modality for large-scale representation learning. Code is released for further
reference.

</details>


### [594] [FedCF: Fair Federated Conformal Prediction](https://arxiv.org/abs/2509.22907)
*Anutam Srinivasan,Aditya T. Vadlamani,Amin Meghrazi,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 本文将Conformal Fairness (CF) 框架扩展到联邦学习环境中，提出了一种在保护隐私的同时审计模型公平性的方法，并通过多领域数据集实验验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 标准的Conformal Prediction不考虑数据中的敏感属性，缺乏对不同子群体的公平性保障，因此需要在联邦学习中引入公平性并进行审计。

Method: 扩展Conformal Fairness (CF) 框架至联邦学习设置，利用交换性假设分析不同人口统计组之间的公平性差距。

Result: 在多个跨领域数据集上进行了实验，结果表明所提框架能有效实现条件覆盖率并评估联邦模型中的公平性差距。

Conclusion: 该工作成功地将CF框架应用于联邦学习，实现了对不同子群体的公平性审计，为隐私保护与模型公平性提供了可行方案。

Abstract: Conformal Prediction (CP) is a widely used technique for quantifying
uncertainty in machine learning models. In its standard form, CP offers
probabilistic guarantees on the coverage of the true label, but it is agnostic
to sensitive attributes in the dataset. Several recent works have sought to
incorporate fairness into CP by ensuring conditional coverage guarantees across
different subgroups. One such method is Conformal Fairness (CF). In this work,
we extend the CF framework to the Federated Learning setting and discuss how we
can audit a federated model for fairness by analyzing the fairness-related gaps
for different demographic groups. We empirically validate our framework by
conducting experiments on several datasets spanning multiple domains, fully
leveraging the exchangeability assumption.

</details>


### [595] [Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders](https://arxiv.org/abs/2509.22913)
*Jake S. Rhodes,Adam G. Rustad,Marshall S. Nielsen,Morgan Chase McClellan,Dallan Gardner,Dawson Hedges*

Main category: cs.LG

TL;DR: 提出一种基于几何正则化双自动编码器的引导表示学习框架，用于增强流形对齐并支持跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 传统流形对齐方法无法进行样本外扩展，限制了其在实际场景中的应用。

Method: 采用预训练对齐模型和多任务学习框架，通过几何正则化双自动编码器强制结构化的跨模态映射，保持嵌入的几何一致性。

Result: 在多个流形对齐方法上验证了该方法在嵌入一致性、信息保留和跨域迁移方面的改进，并在阿尔茨海默病诊断中展示了其提升单域预测精度的能力。

Conclusion: 所提框架有效增强了流形对齐的泛化能力和表示鲁棒性，适用于多模态数据融合与跨域学习任务。

Abstract: Manifold alignment (MA) involves a set of techniques for learning shared
representations across domains, yet many traditional MA methods are incapable
of performing out-of-sample extension, limiting their real-world applicability.
We propose a guided representation learning framework leveraging a
geometry-regularized twin autoencoder (AE) architecture to enhance MA while
enabling generalization to unseen data. Our method enforces structured
cross-modal mappings to maintain geometric fidelity in learned embeddings. By
incorporating a pre-trained alignment model and a multitask learning
formulation, we improve cross-domain generalization and representation
robustness while maintaining alignment fidelity. We evaluate our approach using
several MA methods, showing improvements in embedding consistency, information
preservation, and cross-domain transfer. Additionally, we apply our framework
to Alzheimer's disease diagnosis, demonstrating its ability to integrate
multi-modal patient data and enhance predictive accuracy in cases limited to a
single domain by leveraging insights from the multi-modal problem.

</details>


### [596] [Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective](https://arxiv.org/abs/2509.22921)
*Matthieu Zimmer,Xiaotong Ji,Tu Nguyen,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 提出了一种基于约束强化学习的大语言模型蒸馏新方法，通过在保持与教师模型差异约束的同时最大化任务特定奖励，实现了更优的推理性能和约束满足率。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法通常依赖于启发式奖励加权，缺乏对教师模型偏离程度的明确约束，限制了在资源受限场景下的应用。

Method: 将蒸馏建模为约束强化学习问题，采用改进的奖励函数，在不依赖状态增强或部署时访问教师模型的前提下，保证约束满足的理论性质。

Result: 在数学推理任务上实验表明，该方法相比软拉格朗日松弛基线，在约束满足率和推理能力方面表现更优，同时保持有竞争力的任务性能。

Conclusion: 所提框架为资源受限环境下的奖励感知蒸馏提供了一个理论严谨且实践高效的新方案。

Abstract: We introduce a novel approach to large language model (LLM) distillation by
formulating it as a constrained reinforcement learning problem. While recent
work has begun exploring the integration of task-specific rewards into
distillation processes, existing methods typically rely on ad-hoc reward
weighting. We propose a principled optimization framework that maximizes
task-specific rewards while constraining the divergence from the teacher model
to remain below a specified threshold. Our approach adapts constrained state
augmented reinforcement learning to the distillation setting, introducing a
modified reward function that maintains theoretical guarantees of constraint
satisfaction without requiring state augmentation or teacher model access
during deployment and without the computational overhead of the dual Lagrangian
methods. Through extensive experiments on mathematical reasoning tasks, we
demonstrate that our method achieves better constraint satisfaction rates and
better reasoning compared to the soft Lagrangian relaxation baselines while
maintaining competitive task performance. Our framework provides a
theoretically grounded and practically efficient solution for reward-aware
distillation in resource-constrained settings.

</details>


### [597] [MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints](https://arxiv.org/abs/2509.22931)
*Shreyas Gokhale*

Main category: cs.LG

TL;DR: MonoCon是一种通过函数约束学习鲁棒、解耦且紧凑表示的简单框架，在图像分类和句子相似性任务中表现出色，同时几乎不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度度量学习方法在学习高质量、鲁棒、高效和解耦表示方面的局限，引入新的函数约束方法以提升表示质量。

Method: 提出MonoCon框架，使用一个小型单调MLP头连接预训练编码器，并通过对比损失和单调性约束实现编码器与MLP头的协同适应。

Result: 在CIFAR-100上获得近9倍更紧凑、1.5倍更鲁棒的表示，保持99%的5-NN分类准确率；在SNLI任务上实现3.4倍更紧凑、1.4倍更鲁棒的表示，STSb评分仅有轻微下降。

Conclusion: MonoCon作为一种通用、领域无关的框架，通过函数约束统一解决了边缘计算到云规模检索等多种场景中的关键挑战。

Abstract: Learning high-quality, robust, efficient, and disentangled representations is
a central challenge in artificial intelligence (AI). Deep metric learning
frameworks tackle this challenge primarily using architectural and optimization
constraints. Here, we introduce a third approach that instead relies on
$\textit{functional}$ constraints. Specifically, we present MonoCon, a simple
framework that uses a small monotonic multi-layer perceptron (MLP) head
attached to any pre-trained encoder. Due to co-adaptation between encoder and
head guided by contrastive loss and monotonicity constraints, MonoCon learns
robust, disentangled, and highly compact embeddings at a practically negligible
performance cost. On the CIFAR-100 image classification task, MonoCon yields
representations that are nearly 9x more compact and 1.5x more robust than the
fine-tuned encoder baseline, while retaining 99\% of the baseline's 5-NN
classification accuracy. We also report a 3.4x more compact and 1.4x more
robust representation on an SNLI sentence similarity task for a marginal
reduction in the STSb score, establishing MonoCon as a general domain-agnostic
framework. Crucially, these robust, ultra-compact representations learned via
functional constraints offer a unified solution to critical challenges in
disparate contexts ranging from edge computing to cloud-scale retrieval.

</details>


### [598] [Compute-Optimal Quantization-Aware Training](https://arxiv.org/abs/2509.22935)
*Aleksandr Dremov,David Grangier,Angelos Katharopoulos,Awni Hannun*

Main category: cs.LG

TL;DR: 本文研究了量化感知训练（QAT）与全精度训练阶段的计算资源分配问题，提出了一种预测最优QAT比例和模型性能的损失缩放定律，并引入一种新的QAT与学习率衰减融合方法，提升了量化模型的训练效率与质量。


<details>
  <summary>Details</summary>
Motivation: 尽管分阶段进行全精度训练和QAT已被证明有效，但不同计算预算下两阶段的最优资源分配尚不明确，尤其在不同模型规模和量化位宽下缺乏系统性指导。

Method: 通过在多种计算预算、量化位宽和模型规模（8600万到22亿参数）下进行大规模实验，分析QAT时长对最终性能的影响，提出基于tokens-per-parameter-byte的统计量来预测最优QAT比例，并建立损失缩放定律；同时提出QAT与学习率衰减融合的新策略。

Result: 发现最优QAT占比随总计算量增加而上升，且可通过tokens-per-parameter-byte准确预测；所提出的缩放律能有效预测不同设置下的最优QAT比例和最终性能；新提出的融合方法减少了冗余更新，显著节省计算开销。

Conclusion: 该研究为量化感知训练的资源分配提供了可量化的指导原则，使在相同计算预算下训练更高精度的量化模型成为可能，具有实际应用价值。

Abstract: Quantization-aware training (QAT) is a leading technique for improving the
accuracy of quantized neural networks. Previous work has shown that decomposing
training into a full-precision (FP) phase followed by a QAT phase yields
superior accuracy compared to QAT alone. However, the optimal allocation of
compute between the FP and QAT phases remains unclear. We conduct extensive
experiments with various compute budgets, QAT bit widths, and model sizes from
86.0M to 2.2B to investigate how different QAT durations impact final
performance. We demonstrate that, contrary to previous findings, the
loss-optimal ratio of QAT to FP training increases with the total amount of
compute. Moreover, the optimal fraction can be accurately predicted for a wide
range of model sizes and quantization widths using the
tokens-per-parameter-byte statistic. From experimental data, we derive a loss
scaling law that predicts both optimal QAT ratios and final model performance
across different QAT/FP compute allocation strategies and QAT bit widths. We
use the scaling law to make further predictions, which we verify
experimentally, including which QAT bit width is optimal under a given memory
constraint and how QAT accuracy with different bit widths compares to
full-precision model accuracy. Additionally, we propose a novel cooldown and
QAT fusion approach that performs learning rate decay jointly with
quantization-aware training, eliminating redundant full-precision model updates
and achieving significant compute savings. These findings provide practical
insights into efficient QAT planning and enable the training of higher-quality
quantized models with the same compute budget.

</details>


### [599] [Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases](https://arxiv.org/abs/2509.23471)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Drift-Adapter是一种轻量级可学习转换层，用于在不重新构建ANN索引的情况下桥接不同版本的嵌入模型，显著降低向量数据库升级的成本和中断。


<details>
  <summary>Details</summary>
Motivation: 升级生产环境中的嵌入模型通常需要重新编码整个语料库并重建近似最近邻（ANN）索引，带来高昂的计算成本和操作中断。因此，需要一种能减少重计算、支持平滑模型升级的方法。

Method: 提出Drift-Adapter，包含三种参数化形式：正交Procrustes、低秩仿射变换和残差MLP，通过在少量新旧嵌入对上训练，将新模型的查询映射到旧的嵌入空间，从而复用现有ANN索引。

Result: 在MTEB文本语料库和CLIP图像模型升级（100万项目）实验中，Drift-Adapter恢复了95%-99%的检索召回率（Recall@10, MRR），查询延迟增加不到10微秒，重计算成本降低100倍以上。

Conclusion: Drift-Adapter是一种实用且高效的方法，能够在几乎无操作中断的情况下实现嵌入模型的敏捷部署，适用于大规模系统升级。

Abstract: Upgrading embedding models in production vector databases typically requires
re-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor
(ANN) index, leading to significant operational disruption and computational
cost. This paper presents Drift-Adapter, a lightweight, learnable
transformation layer designed to bridge embedding spaces between model
versions. By mapping new queries into the legacy embedding space, Drift-Adapter
enables the continued use of the existing ANN index, effectively deferring full
re-computation. We systematically evaluate three adapter parameterizations:
Orthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on
a small sample of paired old and new embeddings. Experiments on MTEB text
corpora and a CLIP image model upgrade (1M items) show that Drift-Adapter
recovers 95-99% of the retrieval recall (Recall@10, MRR) of a full
re-embedding, adding less than 10 microseconds of query latency. Compared to
operational strategies like full re-indexing or dual-index serving,
Drift-Adapter reduces recompute costs by over 100 times and facilitates
upgrades with near-zero operational interruption. We analyze robustness to
varied model drift, training data size, scalability to billion-item systems,
and the impact of design choices like diagonal scaling, demonstrating
Drift-Adapter's viability as a pragmatic solution for agile model deployment.

</details>


### [600] [Understanding SOAP from the Perspective of Gradient Whitening](https://arxiv.org/abs/2509.22938)
*Yanqing Lu,Letao Wang,Jinbo Liu*

Main category: cs.LG

TL;DR: SOAP、Shampoo和Adam优化算法在梯度白化视角下被分析，理论表明SOAP与Shampoo在理想条件下等价，实验显示三者最终性能无显著差异。


<details>
  <summary>Details</summary>
Motivation: 理解SOAP相较于Adam和Shampoo的优势来源，并从梯度白化的角度统一解释这些自适应优化方法的机制。

Method: 通过将优化器的预处理器视为对白化矩阵的近似，从二阶曲率信息的角度分析Adam、Shampoo和SOAP；并在Kronecker积假设下建立SOAP与Shampoo的理想版本之间的理论等价性。

Result: 理论推导出理想SOAP与Shampoo的等价性；在nanoGPT语言建模和图像着色任务上的实验表明，SOAP与Shampoo收敛速度相似，且在最终损失上并无显著优势。

Conclusion: SOAP并未真正超越Shampoo或Adam，其性能提升可能源于实现细节而非本质改进，三者在理论上具有等价性。

Abstract: Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) has recently
emerged as a promising optimization algorithm for neural network training,
achieving superior training efficiency over both Adam and Shampoo in language
modeling tasks. In this work, we analyze Adam, Shampoo, and SOAP from the
perspective of gradient whitening, interpreting their preconditioners as
approximations to the whitening matrix, which captures second-order curvature
information. We further establish a theoretical equivalence between idealized
versions of SOAP and Shampoo under the Kronecker product assumption. To
empirically evaluate these insights, we reproduce the language modeling
experiments using nanoGPT and grayscale image colorization. Our results show
that SOAP exhibits similar convergence rate as Shampoo, and no significant
advantage over both Adam and Shampoo in the final loss achieved, which aligns
with their equivalence in theory.

</details>


### [601] [GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data](https://arxiv.org/abs/2509.23742)
*Yewang Chen,Junfeng Li,Shuyin Xia,Qinghong Lai,Xinbo Gao,Guoyin Wang,Dongdong Cheng,Yi Liu,Yi Wang*

Main category: cs.LG

TL;DR: 提出了一种可扩展的骨架聚类算法GBSK，利用粒球技术捕捉数据底层结构，通过多采样和多粒度粒球构建统计‘骨架’，显著降低计算开销并保持高聚类精度，并提出了参数更简化的自适应版本AGBSK。


<details>
  <summary>Details</summary>
Motivation: 为了有效处理大规模数据集的聚类任务，克服传统方法在计算效率和可扩展性方面的局限。

Method: 采用粒球技术对数据集进行多采样并构建多粒度粒球，逐步提取能够近似原始数据结构与分布的统计‘骨架’，在此基础上进行高效聚类；同时设计了参数自适应的AGBSK版本以提升实用性。

Result: 在标准硬件上实验证明，GBSK在包含高达1亿样本、256维的大规模数据集上表现出高效率和强聚类性能，显著优于现有方法。

Conclusion: GBSK是一种高效、可扩展且准确的大规模聚类算法，结合粒球抽象与骨架构建策略，为大规模数据聚类提供了可行解决方案，具备实际应用价值。

Abstract: To effectively handle clustering task for large-scale datasets, we propose a
novel scalable skeleton clustering algorithm, namely GBSK, which leverages the
granular-ball technique to capture the underlying structure of data. By
multi-sampling the dataset and constructing multi-grained granular-balls, GBSK
progressively uncovers a statistical "skeleton" -- a spatial abstraction that
approximates the essential structure and distribution of the original data.
This strategy enables GBSK to dramatically reduce computational overhead while
maintaining high clustering accuracy. In addition, we introduce an adaptive
version, AGBSK, with simplified parameter settings to enhance usability and
facilitate deployment in real-world scenarios. Extensive experiments conducted
on standard computing hardware demonstrate that GBSK achieves high efficiency
and strong clustering performance on large-scale datasets, including one with
up to 100 million instances across 256 dimensions. Our implementation and
experimental results are available at: https://github.com/XFastDataLab/GBSK/.

</details>


### [602] [SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights](https://arxiv.org/abs/2509.22944)
*Lorenz K. Müller,Philippe Bich,Jiawei Zhuang,Ahmet Çelik,Luca Benfenati,Lukas Cavigelli*

Main category: cs.LG

TL;DR: 本文提出了一种名为SINQ的后训练量化方法，通过引入第二轴缩放因子和快速Sinkhorn-Knopp风格算法来均衡矩阵的行列方差，从而减少低精度（≤4位）下大语言模型的困惑度退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有低精度量化方法在处理异常值时因共享缩放因子而导致精度下降，尤其在4位及以下表现明显，且校准自由的均匀量化方法更易受此影响。

Method: SINQ为现有后训练量化器增加行和列方向的额外缩放因子，采用类似Sinkhorn-Knopp的算法优化每层线性层的矩阵平衡性，以最小化矩阵不平衡作为新的量化代理目标。该方法无需跨层交互，可轻松适配新架构。

Result: 在Qwen3和DeepSeek-V2.5模型上验证，SINQ显著优于未校准的均匀量化基线，在WikiText2和C4数据集上的困惑度表现更优，并可与校准或非均匀量化结合进一步提升性能。

Conclusion: SINQ有效缓解了低比特量化中的异常值敏感问题，提供了一种通用、高效且易于集成的量化增强方案。

Abstract: Post-training quantization has emerged as the most widely used strategy for
deploying large language models at low precision. Still, current methods show
perplexity degradation at bit-widths less than or equal to 4, partly because
representing outliers causes precision issues in parameters that share the same
scales as these outliers. This problem is especially pronounced for
calibration-free, uniform quantization methods. We introduce SINQ to augment
existing post-training quantizers with an additional second-axis scale factor
and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize
per-row and per-column variances, thereby minimizing a novel per-matrix proxy
target for quantization: the matrix imbalance. Our method has no interactions
between layers and can be trivially applied to new architectures to quantize
any linear layers. We evaluate our method on the Qwen3 model family and
DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against
uncalibrated uniform quantization baselines and can be further enhanced by
combining it with calibration and non-uniform quantization levels. Code to
reproduce the results of this work and to easily quantize models using SINQ is
available at https://github.com/huawei-csl/SINQ.

</details>


### [603] [Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation](https://arxiv.org/abs/2509.22949)
*Hamidreza Moazzami,Asma Jamali,Nicholas Kevlahan,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 提出了一种基于傅里叶神经算子（FNO）的元学习框架，用于近似数据同化中的逆Hessian算子，显著提高了共轭梯度法的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 变分数据同化方法在涉及Hessian信息时计算成本高，尤其在病态条件下求解困难，需要更高效的优化初始化方法。

Method: 采用傅里叶神经算子（FNO）来学习并逼近一族数据同化问题中的逆Hessian算子，以此为共轭梯度法（CG）提供有效初始值。

Result: 在线性平流方程上的实验表明，相比标准CG方法，FNO-CG平均相对误差降低62%，迭代次数减少17%，且在病态情况下性能提升更明显。

Conclusion: FNO-CG框架能有效提升数据同化中优化过程的收敛速度和精度，尤其适用于条件恶劣的复杂DA问题。

Abstract: Data assimilation (DA) is crucial for enhancing solutions to partial
differential equations (PDEs), such as those in numerical weather prediction,
by optimizing initial conditions using observational data. Variational DA
methods are widely used in oceanic and atmospheric forecasting, but become
computationally expensive, especially when Hessian information is involved. To
address this challenge, we propose a meta-learning framework that employs the
Fourier Neural Operator (FNO) to approximate the inverse Hessian operator
across a family of DA problems, thereby providing an effective initialization
for the conjugate gradient (CG) method. Numerical experiments on a linear
advection equation demonstrate that the resulting FNO-CG approach reduces the
average relative error by $62\%$ and the number of iterations by $17\%$
compared to the standard CG. These improvements are most pronounced in
ill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG
for challenging DA problems.

</details>


### [604] [GDR-learners: Orthogonal Learning of Generative Models for Potential Outcomes](https://arxiv.org/abs/2509.22953)
*Valentyn Melnychuk,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出了一种新的生成式Neyman正交学习器（GDR-learners），用于从观测数据中估计潜在结果的条件分布，具有准oracle效率和双重鲁棒性，并基于多种深度生成模型实现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在估计潜在结果分布时缺乏Neyman正交性及相关优良理论性质（如准oracle效率和双重鲁棒性），限制了其在因果推断中的有效性与稳健性。

Method: 提出GDR-learners框架，具备Neyman正交性，可结合条件标准化流、生成对抗网络、变分自编码器和扩散模型等多种深度生成模型，以估计潜在结果的条件分布。

Result: 理论分析表明GDR-learners具有准oracle效率和率双重鲁棒性，实验结果显示其在（半）合成数据上显著优于现有方法。

Conclusion: GDR-learners是一类灵活且理论上更优的生成式学习器，在估计潜在结果条件分布方面表现卓越，为因果推断提供了更可靠的方法工具。

Abstract: Various deep generative models have been proposed to estimate potential
outcomes distributions from observational data. However, none of them have the
favorable theoretical property of general Neyman-orthogonality and, associated
with it, quasi-oracle efficiency and double robustness. In this paper, we
introduce a general suite of generative Neyman-orthogonal (doubly-robust)
learners that estimate the conditional distributions of potential outcomes. Our
proposed GDR-learners are flexible and can be instantiated with many
state-of-the-art deep generative models. In particular, we develop GDR-learners
based on (a) conditional normalizing flows (which we call GDR-CNFs), (b)
conditional generative adversarial networks (GDR-CGANs), (c) conditional
variational autoencoders (GDR-CVAEs), and (d) conditional diffusion models
(GDR-CDMs). Unlike the existing methods, our GDR-learners possess the
properties of quasi-oracle efficiency and rate double robustness, and are thus
asymptotically optimal. In a series of (semi-)synthetic experiments, we
demonstrate that our GDR-learners are very effective and outperform the
existing methods in estimating the conditional distributions of potential
outcomes.

</details>


### [605] [Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas](https://arxiv.org/abs/2509.22957)
*Luke Guerdan,Justin Whitehouse,Kimberly Truong,Kenneth Holstein,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文提出了一种双重稳健估计框架，利用具有特定社会人口特征的LLM模拟“ persona”评分，结合存在采样偏差的人类评分，以解决生成式AI系统评估中的外部有效性问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统在实际部署中面临评估结果难以从实验室推广到真实场景的问题，尤其是由于人类评分者和系统输出的样本分布与实际部署时不同，导致评估存在采样偏差。

Method: 提出双重稳健估计框架，结合LLM作为评判者生成的‘persona’评分与有偏的人类评分；使用预测模型或重加权模型来纠正偏差，并通过Persona Simulation Framework（PSF）进行验证。

Result: 理论分析和仿真表明，当预测模型或重加权模型质量足够高时，该方法能产生统计上有效的系统质量估计；PSF实验验证了框架在不同persona质量和采样偏差程度下的有效性。

Conclusion: 该工作为结合存在偏差的人类评分和不完美的persona评分提供了原则性方法，提升了生成式AI评估的外部有效性。

Abstract: As Generative AI (GenAI) systems see growing adoption, a key concern involves
the external validity of evaluations, or the extent to which they generalize
from lab-based to real-world deployment conditions. Threats to the external
validity of GenAI evaluations arise when the source sample of human raters and
system outputs used to obtain a system quality estimate differs from the target
distribution at deployment time. In this work, we propose a doubly-robust
estimation framework designed to address this evaluation sampling bias. Key to
our approach is the use of "persona" ratings produced by prompting an LLM
evaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific
sociodemographic characteristics. Our doubly-robust framework combines these
informative yet imperfect persona ratings with human ratings obtained under
evaluation sampling bias to produce statistically valid system quality
estimates. In particular, we show that our approach yields valid system quality
estimates when either (i) a model trained to predict human ratings using
persona ratings and source data observed under sampling bias, or (ii) a
reweighting model that corrects for sampling bias is of sufficient quality. We
validate our framework theoretically and via a novel Persona Simulation
Framework (PSF) designed to systematically manipulate persona quality and the
degree of evaluation sampling bias present in source data. Our work provides a
principled foundation for combining imperfect persona ratings with human
ratings observed under sampling bias to obtain valid system quality estimates.

</details>


### [606] [Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces](https://arxiv.org/abs/2509.22963)
*Haitong Ma,Ofir Nabati,Aviv Rosenberg,Bo Dai,Oran Lang,Idan Szpektor,Craig Boutilier,Na Li,Shie Mannor,Lior Shani,Guy Tenneholtz*

Main category: cs.LG

TL;DR: 本文提出了一种基于离散扩散模型的新型强化学习策略框架，通过策略镜像下降（PMD）定义正则化目标分布，将策略更新转化为分布匹配问题，从而在大规模组合动作空间中实现稳定高效的学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在面对大规模、组合性动作空间时难以扩展，现有方法在训练稳定性与样本效率方面存在不足。

Method: 提出一种解耦的训练框架，利用策略镜像下降（PMD）构建理想的目标策略分布，并训练离散扩散模型来拟合该分布，实现稳定的策略改进。

Result: 在DNA序列生成、宏动作强化学习和多智能体系统等多个复杂组合任务上实现了最先进的性能和更高的样本效率。

Conclusion: 该方法通过分布匹配视角有效提升了扩散策略在组合动作空间中的训练稳定性与性能，展现出广泛的应用潜力。

Abstract: Reinforcement learning (RL) struggles to scale to large, combinatorial action
spaces common in many real-world problems. This paper introduces a novel
framework for training discrete diffusion models as highly effective policies
in these complex settings. Our key innovation is an efficient online training
process that ensures stable and effective policy improvement. By leveraging
policy mirror descent (PMD) to define an ideal, regularized target policy
distribution, we frame the policy update as a distributional matching problem,
training the expressive diffusion model to replicate this stable target. This
decoupled approach stabilizes learning and significantly enhances training
performance. Our method achieves state-of-the-art results and superior sample
efficiency across a diverse set of challenging combinatorial benchmarks,
including DNA sequence generation, RL with macro-actions, and multi-agent
systems. Experiments demonstrate that our diffusion policies attain superior
performance compared to other baselines.

</details>


### [607] [Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic](https://arxiv.org/abs/2509.22964)
*Qinxun Bai,Yuxuan Han,Wei Xu,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的函数式评论家建模方法，解决了在“致命三元组”环境下无策略演员-评论家算法中的挑战，并在理论上证明了其收敛性，同时通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决无策略AC方法中由于“致命三元组”不稳定性和策略持续变化导致的评论家和演员学习困难的问题。

Method: 引入功能评论家建模的新概念，设计基于目标的功能评论家模型，并提出一种新的AC框架。

Result: 在理论上证明了该框架在线性函数设定下的可证明收敛性，并通过在DeepMind Control Benchmark任务上的初步实验展示了其有效性。

Conclusion: 所提出的框架是首个在致命三元组设置下可证明收敛的无策略目标型AC算法，为无策略强化学习提供了新思路。

Abstract: Off-policy reinforcement learning (RL) with function approximation offers an
effective way to improve sample efficiency by reusing past experience. Within
this setting, the actor-critic (AC) framework has achieved strong empirical
success. However, both the critic and actor learning is challenging for the
off-policy AC methods: first of all, in addition to the classic "deadly triad"
instability of off-policy evaluation, it also suffers from a "moving target"
problem, where the policy being evaluated changes continually; secondly, actor
learning becomes less efficient due to the difficulty of estimating the exact
off-policy policy gradient. The first challenge essentially reduces the problem
to repeatedly performing off-policy evaluation for changing policies. For the
second challenge, the off-policy policy gradient theorem requires a complex and
often impractical algorithm to estimate an additional emphasis critic, which is
typically neglected in practice, thereby reducing to the on-policy policy
gradient as an approximation. In this work, we introduce a novel concept of
functional critic modeling, which leads to a new AC framework that addresses
both challenges for actor-critic learning under the deadly triad setting. We
provide a theoretical analysis in the linear function setting, establishing the
provable convergence of our framework, which, to the best of our knowledge, is
the first convergent off-policy target-based AC algorithm. From a practical
perspective, we further propose a carefully designed neural network
architecture for the functional critic modeling and demonstrate its
effectiveness through preliminary experiments on widely used RL tasks from the
DeepMind Control Benchmark.

</details>


### [608] [Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders](https://arxiv.org/abs/2509.22969)
*Samuel V. Singh,Shirley Coyle,Mimi Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的功能自动编码器框架FAEclust，用于多维函数数据的聚类分析，具有强大的非线性建模能力和对相位变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类方法在处理多维函数数据时难以捕捉复杂的非线性依赖关系，且对函数间的相位变异敏感。

Method: 设计了一个基于通用逼近器的编码器-解码器结构，引入聚类损失和形状感知聚类目标，并采用创新的正则化策略提升稳定性。

Result: 证明了解码器的通用逼近性质，并通过大量实验验证了模型在聚类性能上的有效性与鲁棒性。

Conclusion: FAEclust能有效处理多维函数数据的复杂结构，生成适合聚类的潜在表示，且对相位变化具有强抵抗力。

Abstract: We introduce FAEclust, a novel functional autoencoder framework for cluster
analysis of multi-dimensional functional data, data that are random
realizations of vector-valued random functions. Our framework features a
universal-approximator encoder that captures complex nonlinear
interdependencies among component functions, and a universal-approximator
decoder capable of accurately reconstructing both Euclidean and manifold-valued
functional data. Stability and robustness are enhanced through innovative
regularization strategies applied to functional weights and biases.
Additionally, we incorporate a clustering loss into the network's training
objective, promoting the learning of latent representations that are conducive
to effective clustering. A key innovation is our shape-informed clustering
objective, ensuring that the clustering results are resistant to phase
variations in the functions. We establish the universal approximation property
of our non-linear decoder and validate the effectiveness of our model through
extensive experiments.

</details>


### [609] [OptiMind: Teaching LLMs to Think Like Optimization Experts](https://arxiv.org/abs/2509.22979)
*Zeyi Chen,Xinzhi Zhang,Humishka Zope,Hugo Barbalho,Konstantina Mellou,Marco Molinaro,Janardhan Kulkarni,Ishai Menache,Sirui Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合优化领域专业知识来提升大语言模型在混合整数线性规划问题中从自然语言生成数学规划模型准确性的方法，通过清洗训练数据和多轮推理策略显著提高了建模精度。


<details>
  <summary>Details</summary>
Motivation: 数学规划通常需要专业的运筹学知识，难以普及；现有自动化方法因训练数据稀缺且含噪声而准确性有限，因此需要引入领域知识以提升性能。

Method: 首先基于错误分类分析清洗训练数据，避免常见错误；然后设计多轮推理策略，利用特定类别的错误摘要和求解器反馈引导大语言模型进行迭代优化。

Result: 在多个基础大语言模型上的实验表明，结合清洗后的数据与领域知识驱动的提示和反馈机制，平均建模准确率提升了14个百分点。

Conclusion: 将领域知识系统地融入数据预处理和推理过程，能显著提升大语言模型在数学规划自动化中的表现，推动LLM辅助优化建模的发展。

Abstract: Mathematical programming -- the task of expressing operations and
decision-making problems in precise mathematical language -- is fundamental
across domains, yet remains a skill-intensive process requiring operations
research expertise. Recent advances in large language models for complex
reasoning have spurred interest in automating this task, translating natural
language into executable optimization models. Current approaches, however,
achieve limited accuracy, hindered by scarce and noisy training data without
leveraging domain knowledge. In this work, we systematically integrate
optimization expertise to improve formulation accuracy for mixed-integer linear
programming, a key family of mathematical programs. Our approach first cleans
training data through class-based error analysis to explicitly prevent common
mistakes within each optimization class. We then develop multi-turn inference
strategies that guide LLMs with class-specific error summaries and solver
feedback, enabling iterative refinement. Experiments across multiple base LLMs
demonstrate that combining cleaned data with domain-informed prompting and
feedback improves formulation accuracy by 14 percentage points on average,
enabling further progress toward robust LLM-assisted optimization formulation.

</details>


### [610] [MDP modeling for multi-stage stochastic programs](https://arxiv.org/abs/2509.22981)
*David P. Morton,Oscar Dowson,Bernardo K. Pagnoncelli*

Main category: cs.LG

TL;DR: 提出了一种结合马尔可夫决策过程特征的多阶段随机规划模型，扩展了策略图以包含决策依赖不确定性，并开发了新的随机对偶动态规划算法变体来求解。


<details>
  <summary>Details</summary>
Motivation: 为了增强多阶段随机规划在连续状态和动作空间中的建模能力，融合MDP特性并处理决策依赖不确定性和非凸性问题。

Method: 扩展策略图以包含决策依赖的转移概率和统计学习；设计新的随机对偶动态规划算法及其近似方法。

Result: 展示了所提方法在一系列复杂度递增示例中的表达能力，并提出了适用于非凸问题的近似求解方案。

Conclusion: 该建模框架具有较强的表达能力，所开发的算法能有效应对含决策依赖不确定性的复杂随机规划问题。

Abstract: We study a class of multi-stage stochastic programs, which incorporate
modeling features from Markov decision processes (MDPs). This class includes
structured MDPs with continuous state and action spaces. We extend policy
graphs to include decision-dependent uncertainty for one-step transition
probabilities as well as a limited form of statistical learning. We focus on
the expressiveness of our modeling approach, illustrating ideas with a series
of examples of increasing complexity. As a solution method, we develop new
variants of stochastic dual dynamic programming, including approximations to
handle non-convexities.

</details>


### [611] [T-TAMER: Provably Taming Trade-offs in ML Serving](https://arxiv.org/abs/2509.22992)
*Yuanyuan Yang,Ruimin Zhang,Jamie Morgenstern,Haifeng Xu*

Main category: cs.LG

TL;DR: 本文提出了一个名为T-Tamer的通用框架，用于在多模型服务中形式化多阶段决策过程，证明了“回溯”能力对于实现可证明性能保证的必要性和充分性，并通过实验验证了基于回溯的策略能有效优化准确率与延迟之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型日益庞大和复杂，高效推理面临诸多权衡（如精度、延迟、资源使用等），尤其是在级联或多模型场景下，现有方法多为启发式且缺乏理论保障，因此需要一个具有理论支持的通用框架来系统化地解决这些权衡问题。

Method: 将多模型服务中的早期退出决策建模为多阶段决策过程，引入“回溯”机制，并从理论上分析其对性能保证的影响，证明了回溯是实现最优权衡的充要条件，并设计可在多项式时间内达到最优权衡的回溯策略。

Result: 理论证明无回溯策略无法获得任何常数因子近似比，而基于回溯的策略可在多项式时间内实现最优权衡；在合成数据及视觉与NLP的早期退出任务上的实验表明，回溯策略 consistently 实现更优的精度-延迟权衡。

Conclusion: 回溯机制是实现多模型推理中可证明性能保证的关键，T-Tamer为级联模型的设计提供了连接启发式实践与理论保障的原则性基础。

Abstract: As machine learning models continue to grow in size and complexity, efficient
serving faces increasingly broad trade-offs spanning accuracy, latency,
resource usage, and other objectives. Multi-model serving further complicates
these trade-offs; for example, in cascaded models, each early-exit decision
balances latency reduction against potential accuracy loss. Despite the
pervasiveness and importance of such trade-offs, current strategies remain
largely heuristic and case-specific, limiting both their theoretical guarantees
and general applicability.
  We present a general framework, T-Tamer, which formalizes this setting as a
multi-stage decision process, where the objective is to determine both when to
exit and which model to consult. Our main result shows that recall (i.e., the
ability to revisit earlier models) is both necessary and sufficient for
achieving provable performance guarantees. In particular, we prove that
strategies without recall cannot obtain any constant-factor approximation to
the optimal trade-off, whereas recall-based strategies provably attain the
optimal trade-off in polynomial time.
  We validate our analysis through experiments on synthetic datasets and
early-exit workloads for vision and NLP benchmarks. The results show that
recall-based strategies consistently yield efficient accuracy-latency
trade-offs. We hope this work provides a principled foundation for bridging
heuristic practice with theoretical guarantees in the design of early-exit and
cascaded models.

</details>


### [612] [Analysis of Variational Autoencoders](https://arxiv.org/abs/2509.22994)
*Zachary Baker,Yuxiao Li*

Main category: cs.LG

TL;DR: 本文提出了变分稀疏自编码器（vSAE），通过引入高斯后验的随机采样和KL散度正则化来改进稀疏自编码器的特征组织与可解释性，但在Pythia-70M模型上的实验表明，尽管vSAE在特征独立性和消融指标上表现更好，但因KL项导致过多正则化压力，产生大量“死亡特征”，整体性能低于标准SAE。


<details>
  <summary>Details</summary>
Motivation: 探索将变分方法引入稀疏自编码器（SAE）是否能提升特征的组织性和可解释性，特别是通过概率化采样机制增强特征分离。

Method: 提出vSAE模型，用学习到的高斯后验的随机采样替代确定性的ReLU门控，并加入KL散度正则化以推动潜在空间向标准正态先验靠拢；在Pythia-70M的残差流激活上与TopK SAE进行对比评估。

Result: vSAE在核心评估指标上表现不如标准SAE，但特征更独立、鲁棒性更强；KL正则化导致‘活着的’特征比例显著下降，出现更多‘死亡特征’。

Conclusion: 直接将变分方法应用于SAE并不能有效改善特征组织或可解释性，过强的正则化压力可能损害模型性能，需谨慎设计变分结构。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising approach for
interpreting neural network representations by learning sparse,
human-interpretable features from dense activations. We investigate whether
incorporating variational methods into SAE architectures can improve feature
organization and interpretability. We introduce the variational Sparse
Autoencoder (vSAE), which replaces deterministic ReLU gating with stochastic
sampling from learned Gaussian posteriors and incorporates KL divergence
regularization toward a standard normal prior. Our hypothesis is that this
probabilistic sampling creates dispersive pressure, causing features to
organize more coherently in the latent space while avoiding overlap. We
evaluate a Topk vSAE against a standard TopK SAE on Pythia-70M transformer
residual steam activations using comprehensive benchmarks including SAE Bench,
individual feature interpretability analysis, and global latent space
visualization through t-SNE. The vSAE underperforms standard SAE across core
evaluation metrics, though excels at feature independence and ablation metrics.
The KL divergence term creates excessive regularization pressure that
substantially reduces the fraction of living features, leading to observed
performance degradation. While vSAE features demonstrate improved robustness,
they exhibit many more dead features than baseline. Our findings suggest that
naive application of variational methods to SAEs does not improve feature
organization or interpretability.

</details>


### [613] [Sample-efficient Multiclass Calibration under $\ell_{p}$ Error](https://arxiv.org/abs/2509.23000)
*Konstantina Bairaktari,Huy L. Nguyen*

Main category: cs.LG

TL;DR: 提出一种新的校准误差定义，可在多项式样本复杂度下校准多分类预测器，适用于插值范围内的所有点（除一端点外），并在另一端点改进了误差参数的依赖性。


<details>
  <summary>Details</summary>
Motivation: 多分类预测器的校准因输出空间指数增长而具有挑战性，现有方法在样本复杂度上存在局限。

Method: 提出一种介于两种已有校准误差概念之间的新定义，并结合自适应数据分析技术，实现低样本复杂度下的高效校准。

Result: 算法在插值范围内大多数点上仅需多项式样本数即可校准；在一端点达到接近最优的误差依赖，优于先前工作。

Conclusion: 该方法有效平衡了校准精度与样本复杂度，通过创新性地应用自适应数据分析，提升了多分类校准的可行性。

Abstract: Calibrating a multiclass predictor, that outputs a distribution over labels,
is particularly challenging due to the exponential number of possible
prediction values. In this work, we propose a new definition of calibration
error that interpolates between two established calibration error notions, one
with known exponential sample complexity and one with polynomial sample
complexity for calibrating a given predictor. Our algorithm can calibrate any
given predictor for the entire range of interpolation, except for one endpoint,
using only a polynomial number of samples. At the other endpoint, we achieve
nearly optimal dependence on the error parameter, improving upon previous work.
A key technical contribution is a novel application of adaptive data analysis
with high adaptivity but only logarithmic overhead in the sample complexity.

</details>


### [614] [Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery](https://arxiv.org/abs/2509.23003)
*Jiayin Liu,Yulong Yang,Vineet Bansal,Christine Allen-Blanchette*

Main category: cs.LG

TL;DR: 本文提出了Symplectic Phase Space GAN (SPS-GAN)，一种能够从多种测量类型中学习并生成多个物理系统的动力学模型，无需事先了解系统构型空间，并能推广到未见过的物理参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于经典力学的神经网络模型通常仅适用于单一固定参数系统，且需已知构型空间。本文旨在构建一个可泛化、无需先验知识并能发现系统结构的通用动力学模型。

Method: SPS-GAN采用条件GAN框架，嵌入哈密顿神经网络递归模块以保证物理合理性，并通过添加物理动机的稀疏性正则项优化目标函数，从而发现系统的配置空间结构。

Result: SPS-GAN能在轨迹预测、视频生成和对称性发现任务中表现良好，性能媲美针对单个系统设计的监督模型，且可推广至多个系统和未知物理参数。

Conclusion: SPS-GAN是一种无需先验配置空间知识、可跨系统泛化的物理驱动生成模型，有效结合了生成对抗网络与哈密顿动力学，实现了可解释且物理一致的时间序列生成。

Abstract: From metronomes to celestial bodies, mechanics underpins how the world
evolves in time and space. With consideration of this, a number of recent
neural network models leverage inductive biases from classical mechanics to
encourage model interpretability and ensure forecasted states are physical.
However, in general, these models are designed to capture the dynamics of a
single system with fixed physical parameters, from state-space measurements of
a known configuration space. In this paper we introduce Symplectic Phase Space
GAN (SPS-GAN) which can capture the dynamics of multiple systems, and
generalize to unseen physical parameters from. Moreover, SPS-GAN does not
require prior knowledge of the system configuration space. In fact, SPS-GAN can
discover the configuration space structure of the system from arbitrary
measurement types (e.g., state-space measurements, video frames). To achieve
physically plausible generation, we introduce a novel architecture which embeds
a Hamiltonian neural network recurrent module in a conditional GAN backbone. To
discover the structure of the configuration space, we optimize the conditional
time-series GAN objective with an additional physically motivated term to
encourages a sparse representation of the configuration space. We demonstrate
the utility of SPS-GAN for trajectory prediction, video generation and symmetry
discovery. Our approach captures multiple systems and achieves performance on
par with supervised models designed for single systems.

</details>


### [615] [MoE-PHDS: One MoE checkpoint for flexible runtime sparsity](https://arxiv.org/abs/2509.23012)
*Lauren. A Hannah,Soheil Zibakhsh,Kumari Nishu,Arnav Kundu,Mohammad Samragh Razlighi,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoE-PHDS的轻量级微调方法，使预训练的稀疏专家混合模型能够在推理时灵活调整全局稀疏度（如top-k），从而在一个模型上实现多点精度-延迟权衡，避免了为不同效率目标训练多个模型的高成本。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏MoE模型通常在固定稀疏度下训练，难以灵活应对多样化的延迟、效率和能耗需求，导致需维护多个模型，增加训练与部署成本。因此，需要一种可在运行时动态调整稀疏度的方法以提升灵活性并降低开销。

Method: 提出MoE-PHDS方法，通过在不同稀疏度级别上混合训练，并以高稀疏度短课程训练作为锚定，实现对预训练MoE模型的后设稀疏控制；无需修改模型结构，仅需一次轻量级SFT即可支持推理时任意调整k值。

Result: 在OLMoE、Qwen1.5-MoE等模型上的实验表明，PHDS在多个操作点上性能达到或超过专门训练的oracle模型，跨稀疏度一致性最高提升22%，并显著简化了MoE的部署流程。

Conclusion: MoE-PHDS使得全局稀疏度成为可调度的服务原语，实现了单个模型在运行时灵活调节精度与延迟的平衡，为高效、灵活的MoE部署提供了可行方案。

Abstract: Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed
sparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity
level determines an operating point on the accuracy/latency curve; currently,
meeting multiple efficiency targets means training and maintaining multiple
models. This practice complicates serving, increases training and maintenance
costs, and limits flexibility in meeting diverse latency, efficiency, and
energy requirements. We show that pretrained MoEs are more robust to runtime
sparsity shifts than commonly assumed, and introduce MoE-PHDS ({\bf P}ost {\bf
H}oc {\bf D}eclared {\bf S}parsity), a lightweight SFT method that turns a
single checkpoint into a global sparsity control surface. PHDS mixes training
across sparsity levels and anchors with a short curriculum at high sparsity,
requiring no architectural changes. The result is predictable accuracy/latency
tradeoffs from one model: practitioners can ``dial $k$'' at inference time
without swapping checkpoints, changing architecture, or relying on token-level
heuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary
models fit on multiple operating points show that PHDS matches or exceeds
well-specified oracle models, improves cross-sparsity agreement by up to 22\%
vs. well-specified oracle models, and enables simplified, flexible runtime MoE
deployment by making global sparsity a first-class serving primitive.

</details>


### [616] [On the Sheafification of Higher-Order Message Passing](https://arxiv.org/abs/2509.23020)
*Jacob Hume,Pietro Liò*

Main category: cs.LG

TL;DR: 本文探讨了高阶消息传递（HOMP）在拓扑深度学习中的应用，提出利用层论（sheaf theory）改进霍奇拉普拉斯算子的扩散机制，以增强局部与全局描述符之间的交互，并发展了适用于高阶情形的新型理论与方法。


<details>
  <summary>Details</summary>
Motivation: 现有的高阶消息传递方法基于霍奇拉普拉斯算子，其归纳偏置在高维情况下可能不明确甚至退化，因此需要更灵活、更具表达力的框架来建模复杂关系结构。

Method: 引入层论作为形式化工具，使用层拉普拉斯算子替代传统霍奇拉普拉斯算子，使其归纳偏置与数据感知的层上同调相关联，并扩展至高阶情形（k>0），同时提供从抽象到应用的完整介绍。

Result: 建立了适用于图学习中k=0情形的层扩散理论的新视角，揭示了其向k>0推广的局限性，并提出了针对高阶消息传递的新理论与实践方法。

Conclusion: 层论为高阶消息传递提供了自然且原则性的扩展路径，通过数据感知的上同调结构增强了模型表达能力，为拓扑深度学习中的高阶结构建模开辟了新方向。

Abstract: Recent work in Topological Deep Learning (TDL) seeks to generalize graph
learning's preeminent $message \ passing$ paradigm to more complex relational
structures: simplicial complexes, cell complexes, hypergraphs, and combinations
thereof. Many approaches to such ${higher\text{-}order \ message \ passing}$
(HOMP) admit formulation in terms of nonlinear diffusion with the Hodge
(combinatorial) Laplacian, a graded operator which carries an inductive bias
that dimension-$k$ data features correlate with dimension-$k$ topological
features encoded in the (singular) cohomology of the underlying domain. For
$k=0$ this recovers the graph Laplacian and its well-studied homophily bias. In
higher gradings, however, the Hodge Laplacian's bias is more opaque and
potentially even degenerate. In this essay, we position sheaf theory as a
natural and principled formalism for modifying the Hodge Laplacian's
diffusion-mediated interface between local and global descriptors toward more
expressive message passing. The sheaf Laplacian's inductive bias correlates
dimension-$k$ data features with dimension-$k$ $sheaf$ cohomology, a data-aware
generalization of singular cohomology. We will contextualize and novelly extend
prior theory on sheaf diffusion in graph learning ($k=0$) in such a light --
and explore how it fails to generalize to $k>0$ -- before developing novel
theory and practice for the higher-order setting. Our exposition is accompanied
by a self-contained introduction shepherding sheaves from the abstract to the
applied.

</details>


### [617] [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024)
*Melody Zixuan Li,Kumar Krishna Agrawal,Arna Ghosh,Komal Kumar Teru,Adam Santoro,Guillaume Lajoie,Blake A. Richards*

Main category: cs.LG

TL;DR: 本文提出了一种基于频谱的方法来研究大语言模型在预训练和后训练过程中表示几何结构的变化，发现了三个非单调的几何阶段：初始“预热”阶段、中间“熵寻求”阶段和后期“压缩寻求”阶段，并揭示了这些阶段与模型能力涌现之间的关系。


<details>
  <summary>Details</summary>
Motivation: 标准训练指标（如损失）无法解释大语言模型中复杂能力的出现，因此需要从表示几何结构的角度深入理解训练动态。

Method: 采用有效秩（RankMe）和特征谱衰减（α-ReQ）作为度量工具，分析OLMo和Pythia系列模型在训练过程中的表示空间几何变化。

Result: 发现了预训练过程中的三个阶段：快速表示坍缩的‘预热’阶段、表示流形扩展并伴随n-gram记忆峰值的‘熵寻求’阶段，以及各向异性压缩、保留主方向方差的‘压缩寻求’阶段；该阶段转换与下游任务性能提升密切相关。后训练方法如SFT和DPO推动熵寻求，而RLVR引发压缩寻求。

Conclusion: 表示几何的动态演变可归因于交叉熵优化与表示瓶颈（d << |V|）在词频分布偏斜下的基本相互作用，不同后训练方法对几何结构有显著差异的影响。

Abstract: Standard training metrics like loss fail to explain the emergence of complex
capabilities in large language models. We take a spectral approach to
investigate the geometry of learned representations across pretraining and
post-training, measuring effective rank (RankMe) and eigenspectrum decay
($\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a
consistent non-monotonic sequence of three geometric phases during
autoregressive pretraining. The initial "warmup" phase exhibits rapid
representational collapse. This is followed by an "entropy-seeking" phase,
where the manifold's dimensionality expands substantially, coinciding with peak
n-gram memorization. Subsequently, a "compression-seeking" phase imposes
anisotropic consolidation, selectively preserving variance along dominant
eigendirections while contracting others, a transition marked with significant
improvement in downstream task performance. We show these phases can emerge
from a fundamental interplay of cross-entropy optimization under skewed token
frequencies and representational bottlenecks ($d \ll |V|$). Post-training
further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to
integrate specific instructional or preferential data, improving
in-distribution performance while degrading out-of-distribution robustness.
Conversely, RLVR induces "compression-seeking", enhancing reward alignment but
reducing generation diversity.

</details>


### [618] [Understanding Catastrophic Interference On the Identifibility of Latent Representations](https://arxiv.org/abs/2509.23027)
*Yuke Li,Yujia Zheng,Tianyi Xiong,Zhenyi Wang,Heng Huang*

Main category: cs.LG

TL;DR: 本文从潜在表示学习的角度提出了一种新的理论框架，将灾难性干扰问题建模为识别问题，并通过识别PTA与ATA设置间的共享潜在变量来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是机器学习中的一个基本挑战，现有方法缺乏对这一现象的系统性建模和理论解释。

Method: 提出一种两阶段训练策略：首先使用最大似然估计学习PTA和ATA配置下的潜在表示，然后优化KL散度以识别共享潜在变量。

Result: 理论分析表明，通过识别共享潜在变量可有效减小PTA与ATA之间的距离，从而量化并缓解灾难性遗忘；在合成数据和基准数据集上均取得良好效果。

Conclusion: 识别并学习共享潜在表示是缓解灾难性干扰的有效途径，该方法兼具理论保证和实际性能提升。

Abstract: Catastrophic interference, also known as catastrophic forgetting, is a
fundamental challenge in machine learning, where a trained learning model
progressively loses performance on previously learned tasks when adapting to
new ones. In this paper, we aim to better understand and model the catastrophic
interference problem from a latent representation learning point of view, and
propose a novel theoretical framework that formulates catastrophic interference
as an identification problem. Our analysis demonstrates that the forgetting
phenomenon can be quantified by the distance between partial-task aware (PTA)
and all-task aware (ATA) setups. Building upon recent advances in
identifiability theory, we prove that this distance can be minimized through
identification of shared latent variables between these setups. When learning,
we propose our method \ourmeos with two-stage training strategy: First, we
employ maximum likelihood estimation to learn the latent representations from
both PTA and ATA configurations. Subsequently, we optimize the KL divergence to
identify and learn the shared latent variables. Through theoretical guarantee
and empirical validations, we establish that identifying and learning these
shared representations can effectively mitigate catastrophic interference in
machine learning systems. Our approach provides both theoretical guarantees and
practical performance improvements across both synthetic and benchmark
datasets.

</details>


### [619] [DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture Search for 6G Edge Intelligence](https://arxiv.org/abs/2509.23030)
*Yang Lv,Jin Cao,Ben Niu,Zhe Sun,Fengwei Wang,Fenghua Li,Hui Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合个性化差分隐私和自适应模型设计的新型联邦学习框架，用于6G网络下的边缘智能，实现了数据隐私保护与高效模型性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 6G网络追求普遍的人工智能，依赖边缘设备上的联邦学习，但面临数据敏感性和异构性带来的隐私泄露和模型适应性问题。

Method: 采用样本级表征进行知识共享，引入个性化差分隐私抵御重构攻击，并设计隐私感知的神经架构搜索（NAS）算法以生成本地定制化模型结构与超参数。

Result: 在CIFAR-10和CIFAR-100等基准数据集上，相比PerFedRLNAS方法准确率提升6.82%，模型大小减少至1/10，通信开销降低至1/20。

Conclusion: 该框架是首个面向表征型联邦学习且具备理论收敛保证的个性化差分隐私方案，在强隐私保护下显著提升模型性能与效率。

Abstract: The Sixth-Generation (6G) network envisions pervasive artificial intelligence
(AI) as a core goal, enabled by edge intelligence through on-device data
utilization. To realize this vision, federated learning (FL) has emerged as a
key paradigm for collaborative training across edge devices. However, the
sensitivity and heterogeneity of edge data pose key challenges to FL: parameter
sharing risks data reconstruction, and a unified global model struggles to
adapt to diverse local distributions. In this paper, we propose a novel
federated learning framework that integrates personalized differential privacy
(DP) and adaptive model design. To protect training data, we leverage
sample-level representations for knowledge sharing and apply a personalized DP
strategy to resist reconstruction attacks. To ensure distribution-aware
adaptation under privacy constraints, we develop a privacy-aware neural
architecture search (NAS) algorithm that generates locally customized
architectures and hyperparameters. To the best of our knowledge, this is the
first personalized DP solution tailored for representation-based FL with
theoretical convergence guarantees. Our scheme achieves strong privacy
guarantees for training data while significantly outperforming state-of-the-art
methods in model performance. Experiments on benchmark datasets such as
CIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\%
over the federated NAS method PerFedRLNAS, while reducing model size to 1/10
and communication cost to 1/20.

</details>


### [620] [GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models](https://arxiv.org/abs/2509.23037)
*Javad Forough,Mohammad Maheri,Hamed Haddadi*

Main category: cs.LG

TL;DR: 本文提出了GuardNet，一种用于检测和过滤大语言模型（LLM）越狱攻击提示的分层防御框架，通过构建结合序列、句法和注意力关系的图结构，并利用图神经网络在提示级和令牌级进行过滤，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易受到越狱攻击，这些攻击会绕过对齐机制并引发有害行为，威胁其在医疗、金融和法律等关键领域的安全性和可信度，因此需要有效的前置防御机制。

Method: GuardNet构建融合序列连接、句法依赖和注意力衍生词元关系的结构化图，使用两级图神经网络：提示级过滤器检测整体对抗性提示，词元级过滤器定位细粒度的对抗片段。

Result: 在三个数据集和多种攻击场景下，GuardNet显著优于现有防御方法：在LLM-Fuzzer上提示级F1分数从66.4%提升至99.8%，PLeak数据集上从67-79%提升至超过94%；词元级F1从48-75%提升至74-91%，IoU最大提升达28%。

Conclusion: GuardNet在保持可接受延迟的同时具有良好的跨域泛化能力，是一种实用且鲁棒的大语言模型越狱攻击防御方案。

Abstract: Large Language Models (LLMs) are increasingly susceptible to jailbreak
attacks, which are adversarial prompts that bypass alignment constraints and
induce unauthorized or harmful behaviors. These vulnerabilities undermine the
safety, reliability, and trustworthiness of LLM outputs, posing critical risks
in domains such as healthcare, finance, and legal compliance. In this paper, we
propose GuardNet, a hierarchical filtering framework that detects and filters
jailbreak prompts prior to inference. GuardNet constructs structured graphs
that combine sequential links, syntactic dependencies, and attention-derived
token relations to capture both linguistic structure and contextual patterns
indicative of jailbreak behavior. It then applies graph neural networks at two
levels: (i) a prompt-level filter that detects global adversarial prompts, and
(ii) a token-level filter that pinpoints fine-grained adversarial spans.
Extensive experiments across three datasets and multiple attack settings show
that GuardNet substantially outperforms prior defenses. It raises prompt-level
F$_1$ scores from 66.4\% to 99.8\% on LLM-Fuzzer, and from 67-79\% to over 94\%
on PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\% to
74-91\%, with IoU gains up to +28\%. Despite its structural complexity,
GuardNet maintains acceptable latency and generalizes well in cross-domain
evaluations, making it a practical and robust defense against jailbreak threats
in real-world LLM deployments.

</details>


### [621] [IsingFormer: Augmenting Parallel Tempering With Learned Proposals](https://arxiv.org/abs/2509.23043)
*Saleh Bunaiyan,Corentin Delacour,Shuvro Chowdhury,Kyle Lee,Kerem Y. Camsari*

Main category: cs.LG

TL;DR: IsingFormer是一种基于Transformer的模型，通过学习平衡样本生成全局自旋构型，作为并行回火中的全局移动提议，显著加速了蒙特卡洛采样和组合优化，尤其在临界区域和粗糙景观中表现出优越性能，并能泛化到未见实例，如未训练过的半素数分解问题。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在临界点附近和复杂能量景观中混合缓慢，仅依赖局部更新效率低，需要引入能够进行全局移动的高效提议机制以加速收敛。

Method: 提出IsingFormer，一个基于Transformer的模型，训练于平衡态样本以生成符合目标分布的完整自旋构型；将其生成的非相关样本作为Metropolis步骤中的全局提议，嵌入并行回火框架，补充传统的单自旋翻转。

Result: 在2D Ising模型中准确复现磁化强度和自由能曲线，泛化至未见温度（含临界区），单次提议即可大幅缩短平衡时间；在3D自旋玻璃中找到更低能量态；在Ising编码的整数分解任务中，对未见半素数的成功率超过训练分布。

Conclusion: IsingFormer展示了通过学习捕捉全局结构的神经提议机制，可系统性加速蒙特卡洛方法，在采样效率和组合优化性能上均有显著提升，并具备跨问题实例的泛化能力。

Abstract: Markov Chain Monte Carlo (MCMC) underlies both statistical physics and
combinatorial optimization, but mixes slowly near critical points and in rough
landscapes. Parallel Tempering (PT) improves mixing by swapping replicas across
temperatures, yet each replica still relies on slow local updates to change its
configuration. We introduce IsingFormer, a Transformer trained on equilibrium
samples that can generate entire spin configurations resembling those from the
target distribution. These uncorrelated samples are used as proposals for
global moves within a Metropolis step in PT, complementing the usual
single-spin flips. On 2D Ising models (sampling), IsingFormer reproduces
magnetization and free-energy curves and generalizes to unseen temperatures,
including the critical region. Injecting even a single proposal sharply reduces
equilibration time, replacing thousands of local updates. On 3D spin glasses
(optimization), PT enhanced with IsingFormer finds substantially lower-energy
states, demonstrating how global moves accelerate search in rugged landscapes.
Finally, applied to integer factorization encoded as Ising problems,
IsingFormer trained on a limited set of semiprimes transfers successfully to
unseen semiprimes, boosting success rates beyond the training distribution.
Since factorization is a canonical hard benchmark, this ability to generalize
across instances highlights the potential of learning proposals that move
beyond single problems to entire families of instances. The IsingFormer
demonstrates that Monte Carlo methods can be systematically accelerated by
neural proposals that capture global structure, yielding faster sampling and
stronger performance in combinatorial optimization.

</details>


### [622] [Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning](https://arxiv.org/abs/2509.23049)
*Zijian Wang,Xiaofei Zhang,Xin Zhang,Yukun Liu,Qiong Zhang*

Main category: cs.LG

TL;DR: 提出一种新的联邦学习范式，利用经验似然框架在保护数据隐私的同时，实现高效本地建模与新任务的最优客户端分配。


<details>
  <summary>Details</summary>
Motivation: 受医疗场景启发，希望中心服务器不仅能构建模型，还能将新患者引导至最适合其病情的医院，从而更充分地利用联邦学习系统中服务器的潜力。

Method: 引入基于经验似然的框架，同时学习各客户端上的有效局部模型，并为新查询匹配最合适的客户端。

Result: 在基准数据集上的实验结果表明，该方法在模型准确性和客户端引导精度方面均优于标准联邦学习方法。

Conclusion: 该工作开创了联邦学习的新方向，将数据异质性视为特征而非缺陷，构建更智能、资源高效的联邦系统。

Abstract: Federated learning (FL) is increasingly adopted in domains like healthcare,
where data privacy is paramount. A fundamental challenge in these systems is
statistical heterogeneity-the fact that data distributions vary significantly
across clients (e.g., different hospitals may treat distinct patient
demographics). While current FL algorithms focus on aggregating model updates
from these heterogeneous clients, the potential of the central server remains
under-explored. This paper is motivated by a healthcare scenario: could a
central server not only build a model but also guide a new patient to the
hospital best equipped for their specific condition? We generalize this idea to
propose a novel paradigm for FL systems where the server actively guides the
allocation of new tasks or queries to the most appropriate client in the
network. To enable this, we introduce an empirical likelihood-based framework
that simultaneously addresses two goals: (1) learning effective local models on
each client, and (2) finding the best matching client for a new query.
Empirical results demonstrate the framework's effectiveness on benchmark
datasets, showing improvements in both model accuracy and the precision of
client guidance compared to standard FL approaches. This work opens a new
direction for building more intelligent and resource-efficient federated
systems that leverage heterogeneity as a feature, not just a bug. Code is
available at https://github.com/zijianwang0510/FedDRM.git.

</details>


### [623] [Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding](https://arxiv.org/abs/2509.23050)
*Lin Long,Changdae Oh,Seongheon Park,Yixuan Li*

Main category: cs.LG

TL;DR: 本文提出通过链式嵌入分析大视觉语言模型中的语言先验现象，发现了一个关键的视觉集成点（VIP），并提出了总视觉集成（TVI）估计器来量化视觉信息对模型输出的影响，为理解语言先验提供了系统性工具。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖输入-输出探测来分析语言先验，无法揭示视觉信息如何在模型内部影响行为，因此需要一种能深入模型内部机制的系统性分析方法。

Method: 通过链式嵌入方法分析LVLMs中各层表示动态，识别视觉集成点（VIP），并提出TVI估计器，基于VIP后的表示距离量化视觉影响。

Result: 在9个主流LVLM和6个基准的54种组合上验证了VIP的普遍存在，TVI能可靠预测语言先验强度。

Conclusion: VIP和TVI为诊断和理解大视觉语言模型中的语言先验提供了有效且普适的分析工具。

Abstract: Large vision-language models (LVLMs) achieve strong performance on multimodal
tasks, yet they often default to their language prior (LP) -- memorized textual
patterns from pre-training while under-utilizing visual evidence. Prior
analyses of LP mostly rely on input-output probing, which fails to reveal the
internal mechanisms governing when and how vision influences model behavior. To
address this gap, we present the first systematic analysis of language prior
through the lens of chain-of-embedding, which examines the layer-wise
representation dynamics within LVLMs. Our analysis reveals a universal
phenomenon: each model exhibits a Visual Integration Point (VIP), a critical
layer at which visual information begins to meaningfully reshape hidden
representations and influence decoding. Building on this observation, we
introduce the Total Visual Integration (TVI) estimator, which aggregates
representation distance beyond the VIP to quantify how strongly visual query
influences response generation. Across 54 model-dataset combinations spanning 9
contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently
emerges, and that TVI reliably predicts the strength of language prior. This
offers a principled toolkit for diagnosing and understanding language prior in
LVLMs.

</details>


### [624] [Dynamics of Learning: Generative Schedules from Latent ODEs](https://arxiv.org/abs/2509.23052)
*Matt L. Sampson,Peter Melchior*

Main category: cs.LG

TL;DR: 提出一种基于动态系统建模的新型学习率调度器，利用超参数搜索中的训练过程学习潜在表示，预测最优长期验证性能的学习率调度，超越传统方法并实现多种模型的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度方法多依赖简单参数函数或短期训练信号，缺乏对神经网络训练过程的全面时序理解，难以优化长期性能。

Method: 将神经网络训练性能建模为动态系统，利用超参数搜索中的训练运行数据学习训练过程的潜在表示，并根据当前训练指标预测未来最优学习率调度。

Result: 在CNN、ResNet图像分类和Transformer的下一个词预测任务中均达到SOTA；生成的学习率调度不同于常见参数函数，模型收敛到更平坦的损失区域，具有更好泛化能力。

Conclusion: 该调度器能泛化至未见训练动态，具备计算高效、优化器无关、易集成于实验跟踪平台的优点，显著提升模型训练效果。

Abstract: The learning rate schedule is one of the most impactful aspects of neural
network optimization, yet most schedules either follow simple parametric
functions or react only to short-term training signals. None of them are
supported by a comprehensive temporal view of how well neural networks actually
train. We present a new learning rate scheduler that models the training
performance of neural networks as a dynamical system. It leverages training
runs from a hyperparameter search to learn a latent representation of the
training process. Given current training metrics, it predicts the future
learning rate schedule with the best long-term validation performance. Our
scheduler generalizes beyond previously observed training dynamics and creates
specialized schedules that deviate noticeably from common parametric functions.
It achieves SOTA results for image classification with CNN and ResNet models as
well as for next-token prediction with a transformer model. The trained models
are located in flatter regions of the loss landscape and thus provide better
generalization than those trained with other schedules. Our method is
computationally efficient, optimizer-agnostic, and can easily be layered on top
of ML experiment-tracking platforms. An implementation of our scheduler will be
made available after acceptance.

</details>


### [625] [Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting](https://arxiv.org/abs/2509.23074)
*Wanjin Feng,Yuan Yuan,Jingtao Ding,Yong Li*

Main category: cs.LG

TL;DR: 提出一种基于谱相干性的可预测性对齐诊断框架，用于更公平地评估时间序列预测模型。


<details>
  <summary>Details</summary>
Motivation: 标准评估指标将模型性能与数据内在不可预测性混淆，导致评估偏差。

Method: 引入谱相干性可预测性（SCP）和线性利用比（LUR），分别量化任务难度和模型对可预测信息的利用效率。

Result: 发现‘可预测性漂移’现象，并揭示复杂模型在低可预测性数据上表现更好，而线性模型在高可预测性任务中更有效。

Conclusion: 应转向基于可预测性感知的评估范式，以实现更公平的模型比较和深入理解模型行为。

Abstract: In the era of increasingly complex AI models for time series forecasting,
progress is often measured by marginal improvements on benchmark leaderboards.
However, this approach suffers from a fundamental flaw: standard evaluation
metrics conflate a model's performance with the data's intrinsic
unpredictability. To address this pressing challenge, we introduce a novel,
predictability-aligned diagnostic framework grounded in spectral coherence. Our
framework makes two primary contributions: the Spectral Coherence
Predictability (SCP), a computationally efficient ($O(N\log N)$) and
task-aligned score that quantifies the inherent difficulty of a given
forecasting instance, and the Linear Utilization Ratio (LUR), a
frequency-resolved diagnostic tool that precisely measures how effectively a
model exploits the linearly predictable information within the data. We
validate our framework's effectiveness and leverage it to reveal two core
insights. First, we provide the first systematic evidence of "predictability
drift", demonstrating that a task's forecasting difficulty varies sharply over
time. Second, our evaluation reveals a key architectural trade-off: complex
models are superior for low-predictability data, whereas linear models are
highly effective on more predictable tasks. We advocate for a paradigm shift,
moving beyond simplistic aggregate scores toward a more insightful,
predictability-aware evaluation that fosters fairer model comparisons and a
deeper understanding of model behavior.

</details>


### [626] [CLAD-Net: Continual Activity Recognition in Multi-Sensor Wearable Systems](https://arxiv.org/abs/2509.23077)
*Reza Rahimi Azghan,Gautham Krishna Gudur,Mohit Malu,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 本文提出CLAD-Net，一种结合自监督Transformer和监督CNN的持续学习框架，用于解决可穿戴传感器中因主体间分布变化和标签稀缺导致的灾难性遗忘问题，在PAMAP2数据集上表现出优异的准确性和低遗忘率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中可穿戴传感器数据存在主体间的分布偏移，且标注数据稀缺，传统深度学习模型在持续学习过程中易发生灾难性遗忘，难以兼顾新旧任务性能。

Method: 提出CLAD-Net框架，包含一个作为长期记忆的自监督Transformer，通过跨传感器交叉注意力捕捉无标签的全局活动模式；同时使用监督式CNN进行分类，并通过知识蒸馏在逐个主体微调时保留先前知识。

Result: 在PAMAP2数据集上，CLAD-Net达到91.36%的最终准确率，遗忘率仅为8.78%，优于经验回放和弹性权重固化等基线方法；在仅10-20%标签的半监督设置下仍表现良好。消融研究验证了各模块的有效性。

Conclusion: CLAD-Net有效缓解了可穿戴行为识别中的持续学习难题，能够在标签稀缺和分布变化的情况下持续更新模型而不显著遗忘历史知识，具备良好的实用潜力。

Abstract: The rise of deep learning has greatly advanced human behavior monitoring
using wearable sensors, particularly human activity recognition (HAR). While
deep models have been widely studied, most assume stationary data distributions
- an assumption often violated in real-world scenarios. For example, sensor
data from one subject may differ significantly from another, leading to
distribution shifts. In continual learning, this shift is framed as a sequence
of tasks, each corresponding to a new subject. Such settings suffer from
catastrophic forgetting, where prior knowledge deteriorates as new tasks are
learned. This challenge is compounded by the scarcity and inconsistency of
labeled data in human studies. To address these issues, we propose CLAD-Net
(Continual Learning with Attention and Distillation), a framework enabling
wearable-sensor models to be updated continuously without sacrificing
performance on past tasks. CLAD-Net integrates a self-supervised transformer,
acting as long-term memory, with a supervised Convolutional Neural Network
(CNN) trained via knowledge distillation for activity classification. The
transformer captures global activity patterns through cross-attention across
body-mounted sensors, learning generalizable representations without labels.
Meanwhile, the CNN leverages knowledge distillation to retain prior knowledge
during subject-wise fine-tuning. On PAMAP2, CLAD-Net achieves 91.36 percent
final accuracy with only 8.78 percent forgetting, surpassing memory-based and
regularization-based baselines such as Experience Replay and Elastic Weight
Consolidation. In semi-supervised settings with only 10-20 percent labeled
data, CLAD-Net still delivers strong performance, demonstrating robustness to
label scarcity. Ablation studies further validate each module's contribution.

</details>


### [627] [Signal Preserving Weight Initialization for Odd-Sigmoid Activations](https://arxiv.org/abs/2509.23085)
*Hyunwoo Lee,Hayoung Choi,Hyunju Kim*

Main category: cs.LG

TL;DR: 提出一种针对奇数sigmoid函数类的激活函数初始化方法，通过闭式选择噪声尺度避免前向传播中的方差坍缩或饱和，提升训练稳定性与数据效率。


<details>
  <summary>Details</summary>
Motivation: 激活函数与权重初始化相互依赖，传统初始化方法在某些非线性激活下易导致饱和、方差坍缩和学习率敏感，影响训练可靠性。

Method: 定义奇数sigmoid函数类，并为其中任意激活函数f提出定制化的初始化方法，该方法以闭式形式选择噪声尺度，确保前向激活在目标层之前保持良好分布。

Result: 实验表明，该方法无需归一化层即可稳定训练，具有较强的数据效率，并能使标准初始化方法（如Xavier、He、正交）难以收敛的激活函数成功学习。

Conclusion: 所提初始化方法有效解耦了激活函数与训练不稳定问题，增强了对多种非线性激活的适应性，提升了深度网络的可训练性。

Abstract: Activation functions critically influence trainability and expressivity, and
recent work has therefore explored a broad range of nonlinearities. However,
activations and weight initialization are interdependent: without an
appropriate initialization method, nonlinearities can cause saturation,
variance collapse, and increased learning rate sensitivity. We address this by
defining an odd sigmoid function class and, given any activation f in this
class, proposing an initialization method tailored to f. The method selects a
noise scale in closed form so that forward activations remain well dispersed up
to a target layer, thereby avoiding collapse to zero or saturation.
Empirically, the approach trains reliably without normalization layers,
exhibits strong data efficiency, and enables learning for activations under
which standard initialization methods (Xavier, He, Orthogonal) often do not
converge reliably.

</details>


### [628] [Unleashing Flow Policies with Distributional Critics](https://arxiv.org/abs/2509.23087)
*Deshu Chen,Yuchen Liu,Zhijian Zhou,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 本文提出了分布流式评论家（DFC），一种学习完整状态-动作回报分布的新评论家架构，通过流匹配建模回报分布，从而为基于流的策略提供更稳定和信息丰富的学习信号，在D4RL和OGBench基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的策略在离线和离线到在线强化学习中表现出色，但其批评者通常只能学习单一的标量预期回报估计，限制了其潜力。

Method: 提出分布流式评论家（DFC），利用流匹配技术将简单基础分布转换为复杂的回报目标分布，以建模连续且灵活的回报分布。

Result: 在D4RL和OGBench基准上的大量实验表明，该方法在需要多模态动作分布的任务中表现优异，并在离线及离线到在线微调场景中优于现有方法。

Conclusion: DFC通过提供丰富的分布贝尔曼目标，有效释放了表达性强的流策略的潜力，显著提升了强化学习性能。

Abstract: Flow-based policies have recently emerged as a powerful tool in offline and
offline-to-online reinforcement learning, capable of modeling the complex,
multimodal behaviors found in pre-collected datasets. However, the full
potential of these expressive actors is often bottlenecked by their critics,
which typically learn a single, scalar estimate of the expected return. To
address this limitation, we introduce the Distributional Flow Critic (DFC), a
novel critic architecture that learns the complete state-action return
distribution. Instead of regressing to a single value, DFC employs flow
matching to model the distribution of return as a continuous, flexible
transformation from a simple base distribution to the complex target
distribution of returns. By doing so, DFC provides the expressive flow-based
policy with a rich, distributional Bellman target, which offers a more stable
and informative learning signal. Extensive experiments across D4RL and OGBench
benchmarks demonstrate that our approach achieves strong performance,
especially on tasks requiring multimodal action distributions, and excels in
both offline and offline-to-online fine-tuning compared to existing methods.

</details>


### [629] [Demystifying Network Foundation Models](https://arxiv.org/abs/2509.23089)
*Sylee,Beltiukov,Satyandra Guthula,Wenbo Guo,Walter Willinger,Arpit Gupta*

Main category: cs.LG

TL;DR: 本文系统研究了网络基础模型（NFMs）中编码的潜在知识，重点关注隐藏表示分析而非仅下游任务性能。通过嵌入几何、度量对齐和因果敏感性三方面评估，发现现有NFM普遍存在各向异性、特征敏感性不一致、无法分离高层语义等问题，且改进这些缺陷可显著提升性能（F1分数最高提升0.35）。


<details>
  <summary>Details</summary>
Motivation: 现有NFM研究多关注下游任务表现，忽视其内部表示质量。本文旨在通过深入分析隐藏表示，揭示模型在表征网络数据时的根本缺陷。

Method: 提出三部分评估框架：嵌入几何分析（评估表示空间利用）、度量对齐评估（与领域专家特征对比）和因果敏感性测试（评估对协议扰动的鲁棒性），并在五个多样化的网络数据集上评估四种最先进的NFM。

Result: 发现所有被测NFM均存在显著的各向异性、特征敏感性模式不一致、难以分离高层上下文与负载依赖等问题；改进这些问题可在不改变架构的情况下使F1分数提升高达0.35。

Conclusion: 当前NFM在表示学习方面存在普遍且严重的局限性，通过优化表示质量可显著提升模型性能，未来NFM设计应更注重内部表示的有效性和鲁棒性。

Abstract: This work presents a systematic investigation into the latent knowledge
encoded within Network Foundation Models (NFMs) that focuses on hidden
representations analysis rather than pure downstream task performance.
Different from existing efforts, we analyze the models through a three-part
evaluation: Embedding Geometry Analysis to assess representation space
utilization, Metric Alignment Assessment to measure correspondence with
domain-expert features, and Causal Sensitivity Testing to evaluate robustness
to protocol perturbations. Using five diverse network datasets spanning
controlled and real-world environments, we evaluate four state-of-the-art NFMs,
revealing that they all exhibit significant anisotropy, inconsistent feature
sensitivity patterns, an inability to separate the high-level context, payload
dependency, and other properties. Our work identifies numerous limitations
across all models and demonstrates that addressing them can significantly
improve model performance (by up to +0.35 $F_1$ score without architectural
changes).

</details>


### [630] [Sensitivity Analysis for Diffusion Models](https://arxiv.org/abs/2509.23092)
*Christopher Scarvelis,Justin Solomon*

Main category: cs.LG

TL;DR: 提出了一种闭式方法，用于计算扩散模型中从数据分布到最优得分函数的映射的方向导数，能够预测训练集微小扰动对模型样本的影响，而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 希望在不进行昂贵重训练的情况下，预测扩散模型在训练集微小扰动下的变化，从而评估模型对数据扰动的敏感性。

Method: 通过黑箱访问预训练的得分模型及其输入导数，推导出该映射方向导数的闭式计算方法，并扩展至估计扩散模型样本对目标测度加性扰动的敏感性。

Result: 该方法运行时间与扩散采样和路径上对数似然计算相当，对数值和近似误差具有鲁棒性，且预测的敏感性与实际重训练和微调后的样本变化相关。

Conclusion: 所提方法能高效、稳定地估计扩散模型对训练数据扰动的敏感性，为模型调试和数据重要性分析提供了实用工具。

Abstract: Training a diffusion model approximates a map from a data distribution $\rho$
to the optimal score function $s_t$ for that distribution. Can we differentiate
this map? If we could, then we could predict how the score, and ultimately the
model's samples, would change under small perturbations to the training set
before committing to costly retraining. We give a closed-form procedure for
computing this map's directional derivatives, relying only on black-box access
to a pre-trained score model and its derivatives with respect to its inputs. We
extend this result to estimate the sensitivity of a diffusion model's samples
to additive perturbations of its target measure, with runtime comparable to
sampling from a diffusion model and computing log-likelihoods along the sample
path. Our method is robust to numerical and approximation error, and the
resulting sensitivities correlate with changes in an image diffusion model's
samples after retraining and fine-tuning.

</details>


### [631] [Causally-Enhanced Reinforcement Policy Optimization](https://arxiv.org/abs/2509.23095)
*Xiangqi Wang,Yue Huang,Yujun Zhou,Xiaonan Luo,Kehan Guo,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 本文提出了Causally-Enhanced Policy Optimization (CE-PO)，一种通过奖励塑形提升大语言模型推理因果一致性的方法，在保持准确率的同时显著增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在强化学习训练中容易依赖捷径策略，产生表面正确但推理不忠实的结果，尤其在因果扰动下表现下降，因此需要提升其推理过程的因果一致性。

Method: 提出CE-PO框架，利用Jacobian敏感性估计模型内部影响，通过反事实加固抑制干扰信号，并使用Minkowski组合器融合因果一致性得分与任务准确率反馈，实现可调的准确性-一致性权衡。该方法无需修改模型结构，可直接集成到PPO/GRPO中。

Result: 在4个数据集上的实验表明，CE-PO平均提升准确率5.49%（最高达9.58%），有效减少奖励黑客行为和不忠实的思维链，显著增强对因果-相关性反转和轻量级反事实编辑的鲁棒性。

Conclusion: CE-PO通过引入可微的因果一致性代理信号，在不牺牲准确率的前提下提升了模型推理路径的可靠性与鲁棒性，为强化学习中的奖励塑造提供了有效解决方案。

Abstract: Large language models (LLMs) trained with reinforcement objectives often
achieve superficially correct answers via shortcut strategies, pairing correct
outputs with spurious or unfaithful reasoning and degrading under small causal
perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a
drop-in reward-shaping framework that augments policy optimization with a
differentiable proxy for causal coherence along the generation pathway from
prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal
influence with Jacobian-based sensitivities, counterfactually hardens these
signals to suppress nuisance cues, and fuses the resulting coherence score with
task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single
tunable between accuracy and coherence trade-off. The unified reward integrates
with PPO/GRPO without architectural changes. Across reasoning benchmarks and
causal stress tests, CE-PO reduces reward hacking and unfaithful
chain-of-thought while improving robustness to correlation-causation flips and
light counterfactual edits, all at near-parity accuracy. Experimental results
across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on
average (up to 9.58%), while improving robustness to correlation-causation
flips and light counterfactual edits.

</details>


### [632] [Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks](https://arxiv.org/abs/2509.23101)
*M. Z. Haider,Tayyaba Noreen,M. Salman*

Main category: cs.LG

TL;DR: 提出一种集成图神经网络框架（GCN、GAT、GIN）用于区块链欺诈检测，在真实数据集Elliptic上实现了高召回率和低于1%的误报率，且架构支持未来量子计算融合。


<details>
  <summary>Details</summary>
Motivation: 区块链的伪匿名性为非法活动提供了机会，传统反洗钱技术难以应对复杂的欺诈模式，需要更强大、鲁棒的模型来检测欺诈交易。

Method: 构建一个集成学习框架，结合GCN、GAT和GIN三种图神经网络，采用软投票机制，并在Elliptic数据集上进行调优训练，同时设计了支持量子计算集成的模块化架构。

Result: 该集成模型在检测非法交易方面优于单个GNN模型和基线方法，具有高召回率且误报率低于1%，表现出良好的鲁棒性和可扩展性。

Conclusion: 集成图神经网络是实时加密货币监控的有效方案，兼具当前反洗钱应用价值和面向量子增强安全分析的长期适应性。

Abstract: Blockchain Business applications and cryptocurrencies such as enable secure,
decentralized value transfer, yet their pseudonymous nature creates
opportunities for illicit activity, challenging regulators and exchanges in
anti money laundering (AML) enforcement. Detecting fraudulent transactions in
blockchain networks requires models that can capture both structural and
temporal dependencies while remaining resilient to noise, imbalance, and
adversarial behavior. In this work, we propose an ensemble framework that
integrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT),
and Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection.
Using the real-world Elliptic dataset, our tuned soft voting ensemble achieves
high recall of illicit transactions while maintaining a false positive rate
below 1%, beating individual GNN models and baseline methods. The modular
architecture incorporates quantum-ready design hooks, allowing seamless future
integration of quantum feature mappings and hybrid quantum classical graph
neural networks. This ensures scalability, robustness, and long-term
adaptability as quantum computing technologies mature. Our findings highlight
ensemble GNNs as a practical and forward-looking solution for real-time
cryptocurrency monitoring, providing both immediate AML utility and a pathway
toward quantum-enhanced financial security analytics.

</details>


### [633] [Effective Quantization of Muon Optimizer States](https://arxiv.org/abs/2509.23106)
*Aman Gupta,Rafael Celente,Abhishek Shivanna,D. T. Braithwaite,Gregory Dexter,Shao Tang,Hiroto Udagawa,Daniel Silva,Rohan Ramanath,S. Sathiya Keerthi*

Main category: cs.LG

TL;DR: 本文提出了8位元的Muon优化器，采用区块量化技术，在保持线性和动态量化稳定性的同时，显著减少了内存占用，并在大规模语言模型预训练和微调中表现出优于AdamW及其8位变体的性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少Muon优化器在状态存储上的内存开销，同时保持其相较于AdamW的收敛速度和计算效率优势，研究如何将其与有效的量化方法结合。

Method: 引入基于区块的8位量化方案到Muon优化器中，支持线性与动态两种量化方式，并通过理论分析解释其在量化下的稳定性。

Result: 8位Muon相比全精度版本减少约74%内存占用，在1.6B模型预训练和Llama 3.2 3B模型微调任务中表现接近全精度Muon，且优于AdamW和8位AdamW。

Conclusion: 8位Muon在显著降低内存消耗的同时保持了良好的训练稳定性和模型性能，为大型语言模型的高效优化提供了一种可行方案。

Abstract: The Muon optimizer, based on matrix orthogonalization, has recently shown
faster convergence and up to 2x computational efficiency over AdamW in LLM
pretraining. Like AdamW, Muon is stateful, requiring storage of both model
weights and accumulated gradients. While 8-bit AdamW variants mitigate this
overhead using blockwise quantization, they are typically stable only under
dynamic quantization - which improves stability on linear quantization for
extreme values. In this paper, we introduce the 8-bit Muon optimizer using
blockwise quantization, supporting both linear and dynamic schemes. We
demonstrate that 8-bit Muon maintains stability under both, while delivering
$\sim$74\% reduction in memory footprint compared to full-precision Muon. In
extensive experiments, 8-bit Muon closely matches the performance of Muon while
outperforming AdamW and 8-bit AdamW in pre-training a 1.6B model on 4B FineWeb
tokens. It also shows competitive results when fine-tuning the Llama 3.2 3B
model on post-training data. We also provide a theoretical perspective to help
explain this robustness under quantization.

</details>


### [634] [RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.LG

TL;DR: RHYTHM是一个利用大语言模型进行人类移动预测的统一框架，通过分层时间标记化和冻结LLM主干来提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 人类移动预测因长距离依赖和多尺度周期行为而复杂，现有方法难以有效建模时空依赖关系。

Method: 提出RHYTHM框架，采用时间分词将轨迹划分为每日片段，并使用分层注意力捕捉日和周级依赖；通过冻结的大语言模型生成提示嵌入以增强表征，并将组合嵌入反馈至LLM主干。同时冻结预训练LLM主干以降低计算开销。

Result: 在三个真实数据集上评估显示，RHYTHM整体准确率提升2.4%，周末准确率提高5.0%，训练时间减少24.6%。

Conclusion: RHYTHM通过结合分层时间建模与冻结LLM的有效推理，在保持低计算成本的同时显著提升了人类移动预测性能。

Abstract: Predicting human mobility is inherently challenging due to complex long-range
dependencies and multi-scale periodic behaviors. To address this, we introduce
RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),
a unified framework that leverages large language models (LLMs) as
general-purpose spatio-temporal predictors and trajectory reasoners.
Methodologically, RHYTHM employs temporal tokenization to partition each
trajectory into daily segments and encode them as discrete tokens with
hierarchical attention that captures both daily and weekly dependencies,
thereby significantly reducing the sequence length while preserving cyclical
information. Additionally, we enrich token representations by adding
pre-computed prompt embeddings for trajectory segments and prediction targets
via a frozen LLM, and feeding these combined embeddings back into the LLM
backbone to capture complex interdependencies. Computationally, RHYTHM freezes
the pretrained LLM's backbone to reduce attention complexity and memory cost.
We evaluate our model against state-of-the-art methods using three real-world
datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a
5.0% increase on weekends, and a 24.6% reduction in training time. Code is
publicly available at https://github.com/he-h/rhythm.

</details>


### [635] [Impute-MACFM: Imputation based on Mask-Aware Flow Matching](https://arxiv.org/abs/2509.23126)
*Dengyi Liu,Honggang Wang,Hua Fang*

Main category: cs.LG

TL;DR: 提出Impute-MACFM，一种基于掩码感知的条件流匹配框架，用于表格数据中的缺失值填补，尤其适用于医疗领域的纵向数据，在多种缺失机制下实现了稳定、高效且高质量的填补效果。


<details>
  <summary>Details</summary>
Motivation: 现有填补方法要么假设过于严格，要么难以处理复杂的跨特征结构，而近期生成式方法存在不稳定和推理成本高的问题，因此需要一种更稳定、高效且适用于多种缺失机制（MCAR、MAR、MNAR）的填补方法。

Method: 提出Impute-MACFM，采用掩码感知的目标函数，仅在缺失项上构建轨迹，并通过非线性调度约束观测项的预测速度接近零；结合观测位置的稳定性惩罚、局部不变性的一致性正则化，以及针对数值特征的时间衰减噪声注入；推理阶段使用保持约束的常微分方程积分，并通过每步投影固定观测值，可选多轨迹聚合以增强鲁棒性。

Result: 在多个基准数据集上，Impute-MACFM在填补质量、稳健性和效率方面均优于现有方法，取得了当前最优的性能。

Conclusion: Impute-MACFM为表格型缺失数据（尤其是纵向数据）提供了一种有效解决方案，验证了流匹配方法在该领域中的潜力。

Abstract: Tabular data are central to many applications, especially longitudinal data
in healthcare, where missing values are common, undermining model fidelity and
reliability. Prior imputation methods either impose restrictive assumptions or
struggle with complex cross-feature structure, while recent generative
approaches suffer from instability and costly inference. We propose
Impute-MACFM, a mask-aware conditional flow matching framework for tabular
imputation that addresses missingness mechanisms, missing completely at random,
missing at random, and missing not at random. Its mask-aware objective builds
trajectories only on missing entries while constraining predicted velocity to
remain near zero on observed entries, using flexible nonlinear schedules.
Impute-MACFM combines: (i) stability penalties on observed positions, (ii)
consistency regularization enforcing local invariance, and (iii) time-decayed
noise injection for numeric features. Inference uses constraint-preserving
ordinary differential equation integration with per-step projection to fix
observed values, optionally aggregating multiple trajectories for robustness.
Across diverse benchmarks, Impute-MACFM achieves state-of-the-art results while
delivering more robust, efficient, and higher-quality imputation than competing
approaches, establishing flow matching as a promising direction for tabular
missing-data problems, including longitudinal data.

</details>


### [636] [C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning](https://arxiv.org/abs/2509.23129)
*Haotian Liu,Shuo Wang,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出一种名为C²GSPG的置信度校准组序列策略梯度方法，用于提升推理模型的性能并抑制过自信问题，在逻辑和数学推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）在推理模型中存在过自信问题，阻碍了自感知推理模型的发展。

Method: 提出组序列策略梯度（GSPG）框架，使用归一化的序列级概率定义模型置信度，并引入交叉熵正则化项将置信度校准到序列奖励；对非二值奖励采用非线性奖励归一化和自适应正则化裁剪。

Result: 在逻辑和数学推理任务上，C²GSPG在推理准确性和置信度校准方面均优于当前最先进的方法。

Conclusion: C²GSPG有效解决了推理模型中的过自信问题，实现了更好的性能与置信度匹配，推动了自感知推理模型的发展。

Abstract: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy
Optimization (GRPO) and its variants, play a central role in developing
reasoning models. However, these methods often suffer from a critical
overconfidence issue, which prevents them from achieving self-aware reasoning
models. In this study, we propose a simple yet effective confidence-calibration
group sequence policy gradient method, called C$^2$GSPG, which simultaneously
enhances reasoning performance while suppressing overconfidence. In principle,
we propose a Group Sequence Policy Gradient (GSPG) framework for learning
reasoning models, which eliminates the token-level bias commonly appearing in
GRPO and its variants. In this framework, we define the model confidence for
each reasoning problem using the normalized sequence-level probability, and
then apply a cross-entropy regularizer to calibrate the model confidence to the
sequence's reward. We demonstrate that the confidence calibration regularizer
and GSPG are collaborative for binary rewards, as their objectives always share
the same gradient direction. For non-binary rewards, we apply nonlinear reward
normalization and adaptive regularizer clipping, mitigating the potential
conflict between the two objectives. Applying C$^2$GSPG to post-train large
language models in logical and mathematical reasoning tasks, we show its
superiority over state-of-the-art methods in both reasoning accuracy and
confidence calibration. The code of C$^2$GSPG is available at
https://github.com/HaotianLiu123/CCGSPG.

</details>


### [637] [Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm](https://arxiv.org/abs/2509.23135)
*Yang Chen,Menglin Zou,Jiaqi Zhang,Yitan Zhang,Junyi Yang,Gael Gendron,Libo Zhang,Jiamou Liu,Michael J. Witbrock*

Main category: cs.LG

TL;DR: 本文提出了一种新的非对抗式逆强化学习框架TRRO，通过信任域奖励优化保证了专家行为似然的单调提升，并设计了PIRO算法，在多个基准任务上表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有逆强化学习方法在训练稳定性与理论保证之间存在权衡，作者希望构建一个兼具稳定性与理论保障的非对抗式IRL框架。

Method: 提出Trust Region Reward Optimization (TRRO)，基于Minorization-Maximization过程统一视角下最大化专家行为似然，并实例化为Proximal Inverse Reward Optimization (PIRO)算法。

Result: PIRO在MuJoCo、Gym-Robotics和真实动物行为建模任务上优于或媲美现有最先进方法，具有高样本效率和稳定训练过程。

Conclusion: TRRO为非对抗式逆强化学习提供了类似TRPO在正向强化学习中的稳定性理论保证，填补了该领域理论与实践之间的空白。

Abstract: Inverse Reinforcement Learning (IRL) learns a reward function to explain
expert demonstrations. Modern IRL methods often use the adversarial (minimax)
formulation that alternates between reward and policy optimization, which often
lead to unstable training. Recent non-adversarial IRL approaches improve
stability by jointly learning reward and policy via energy-based formulations
but lack formal guarantees. This work bridges this gap. We first present a
unified view showing canonical non-adversarial methods explicitly or implicitly
maximize the likelihood of expert behavior, which is equivalent to minimizing
the expected return gap. This insight leads to our main contribution: Trust
Region Reward Optimization (TRRO), a framework that guarantees monotonic
improvement in this likelihood via a Minorization-Maximization process. We
instantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical
and stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to
the stability guarantees of Trust Region Policy Optimization (TRPO) in forward
RL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward
recovery, policy imitation with high sample efficiency on MuJoCo and
Gym-Robotics benchmarks and a real-world animal behavior modeling task.

</details>


### [638] [Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations](https://arxiv.org/abs/2509.23139)
*Sipeng Chen,Yan Zhang,Shibo Li*

Main category: cs.LG

TL;DR: 本文提出了OptiINR，首个将隐式神经表示（INR）配置形式化为优化问题的统一框架，利用贝叶斯优化联合搜索激活函数与初始化参数，实现跨模态信号处理任务的性能最大化。


<details>
  <summary>Details</summary>
Motivation: 现有INR配置依赖启发式或网格搜索，缺乏系统性方法，导致性能不稳定且跨模态表现不一致。

Method: 提出OptiINR框架，结合贝叶斯优化，对离散的激活函数族（如SIREN、WIRE、FINER）及其连续初始化参数进行联合优化。

Result: OptiINR在多种信号处理任务中实现了更优且稳定的性能，显著优于手动调参和启发式方法。

Conclusion: OptiINR为INR配置提供了原则性解决方案，推动了INR设计的自动化与最优化。

Abstract: Implicit Neural Representations (INRs) have emerged as a transformative
paradigm in signal processing and computer vision, excelling in tasks from
image reconstruction to 3D shape modeling. Yet their effectiveness is
fundamentally limited by the absence of principled strategies for optimal
configuration - spanning activation selection, initialization scales,
layer-wise adaptation, and their intricate interdependencies. These choices
dictate performance, stability, and generalization, but current practice relies
on ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often
leading to inconsistent results across modalities. This work introduces
OptiINR, the first unified framework that formulates INR configuration as a
rigorous optimization problem. Leveraging Bayesian optimization, OptiINR
efficiently explores the joint space of discrete activation families - such as
sinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER) - and
their associated continuous initialization parameters. This systematic approach
replaces fragmented manual tuning with a coherent, data-driven optimization
process. By delivering globally optimal configurations, OptiINR establishes a
principled foundation for INR design, consistently maximizing performance
across diverse signal processing applications.

</details>


### [639] [TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts](https://arxiv.org/abs/2509.23145)
*Xiaowen Ma,Shuning Ge,Fan Yang,Xiangyu Li,Yun Chen,Mengting Ma,Wei Zhang,Zhipeng Liu*

Main category: cs.LG

TL;DR: 提出了一种基于专家混合机制的时间序列建模方法TMOE，通过局部过滤和全局共享专家机制改进Transformer的注意力机制，在多个真实世界长期预测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer在时间序列建模中无法动态适应历史时间戳相关性变化以及异常片段干扰的问题。

Method: 将键值对视为局部专家，引入局部滤波机制进行自适应专家选择，并保留共享全局专家以维持长期依赖捕获能力，替换现有时间序列Transformer中的注意力机制。

Result: 在七个真实世界的长期预测基准上，TimeExpert和TimeExpert-G均优于当前最先进方法。

Conclusion: TMOE能有效提升时间序列预测性能，兼顾局部动态适应与全局依赖建模，具有良好的通用性和实用性。

Abstract: Transformer-based architectures dominate time series modeling by enabling
global attention over all timestamps, yet their rigid 'one-size-fits-all'
context aggregation fails to address two critical challenges in real-world
data: (1) inherent lag effects, where the relevance of historical timestamps to
a query varies dynamically; (2) anomalous segments, which introduce noisy
signals that degrade forecasting accuracy. To resolve these problems, we
propose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism
that reimagines key-value (K-V) pairs as local experts (each specialized in a
distinct temporal context) and performs adaptive expert selection for each
query via localized filtering of irrelevant timestamps. Complementing this
local adaptation, a shared global expert preserves the Transformer's strength
in capturing long-range dependencies. We then replace the vanilla attention
mechanism in popular time-series Transformer frameworks (i.e., PatchTST and
Timer) with TMOE, without extra structural modifications, yielding our specific
version TimeExpert and general version TimeExpert-G. Extensive experiments on
seven real-world long-term forecasting benchmarks demonstrate that TimeExpert
and TimeExpert-G outperform state-of-the-art methods. Code is available at
https://github.com/xwmaxwma/TimeExpert.

</details>


### [640] [Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers](https://arxiv.org/abs/2509.23152)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: 提出Mirror-Critique框架，通过生成高质量的批评信号来训练验证器，提升大语言模型在推理任务中的准确性和诚实性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在选择正确答案时往往忽略少数但正确的回答，限制了推理性能的进一步提升。

Method: 利用小规模指令微调模型结合拒绝采样生成高质量批评数据，通过对比模型生成解与真实解来训练验证器，并在RLVR过程中冷启动LLM以增强验证能力。

Result: 实验表明，Mirror-Verifier显著优于多数投票法，在解的准确性上表现更优，并提高了模型识别自身能力边界并选择 abstain 的诚实性。

Conclusion: Mirror-Critique通过引入信息丰富的批评信号，有效提升了LLM推理中验证阶段的性能，为测试时扩展提供了新思路。

Abstract: Test-time scaling via solution sampling and aggregation has become a key
paradigm for improving the reasoning performance of Large Language Models
(LLMs). While reward model selection is commonly employed in this approach, it
often fails to identify minority-yet-correct answers, which limits its
effectiveness beyond that of simple majority voting. We argue that this
limitation stems from a lack of informative critique signals during verifier
training. To bridge this gap, we introduce Mirror-Critique, a framework that
trains a verifier with informative critiques. Our key insight is to leverage
the rich critique signal by contrasting model-generated solutions with
ground-truth solutions. We deploy a small instruction-tuned model to synthesize
high-quality critique data with rejection sampling that teaches the verifier
not only what is wrong, but also why. The synthetic data is used to cold-start
the LLMs in the RLVR process to further improve the verification ability. The
resulting Mirror-Verifier is deployed to evaluate candidate solutions by
generating multiple critiques per solution, aggregating them into a verify
score used for weighted voting or selective abstention. The experimental
results show that our Mirror-Verifier significantly outperforms majority voting
in terms of solution accuracy and also improves the solver's honesty to
recognize and abstain from answering beyond its capability boundaries.

</details>


### [641] [CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning](https://arxiv.org/abs/2509.23156)
*Prashant Govindarajan,Mathieu Reymond,Antoine Clavaud,Mariano Phielipp,Santiago Miret,Sarath Chandar*

Main category: cs.LG

TL;DR: 本文提出了CrystalGym，一个用于晶体材料发现的开源强化学习环境，旨在通过直接集成高成本的密度泛函理论（DFT）计算信号来优化材料设计过程。


<details>
  <summary>Details</summary>
Motivation: 由于DFT计算成本高，现有机器学习方法难以在材料设计中利用其直接反馈；因此需要一种支持在线强化学习以整合DFT信号的新框架。

Method: 开发了CrystalGym这一强化学习环境，并基于DFT计算的带隙、体积模量和密度等目标属性，对多种基于值和策略的强化学习算法进行了基准测试，同时研究了大语言模型结合强化学习进行微调的可行性。

Result: 实验表明不同算法在样本效率和收敛性方面表现各异，尽管尚无算法能完全解决所有任务，但为强化学习在耗时奖励信号下的应用提供了有效测试平台。

Conclusion: CrystalGym为强化学习与材料科学的交叉研究提供了实用工具，推动了面向现实应用的强化学习新挑战。

Abstract: In silico design and optimization of new materials primarily relies on
high-accuracy atomic simulators that perform density functional theory (DFT)
calculations. While recent works showcase the strong potential of machine
learning to accelerate the material design process, they mostly consist of
generative approaches that do not use direct DFT signals as feedback to improve
training and generation mainly due to DFT's high computational cost. To aid the
adoption of direct DFT signals in the materials design loop through online
reinforcement learning (RL), we propose CrystalGym, an open-source RL
environment for crystalline material discovery. Using CrystalGym, we benchmark
common value- and policy-based reinforcement learning algorithms for designing
various crystals conditioned on target properties. Concretely, we optimize for
challenging properties like the band gap, bulk modulus, and density, which are
directly calculated from DFT in the environment. While none of the algorithms
we benchmark solve all CrystalGym tasks, our extensive experiments and
ablations show different sample efficiencies and ease of convergence to
optimality for different algorithms and environment settings. Additionally, we
include a case study on the scope of fine-tuning large language models with
reinforcement learning for improving DFT-based rewards. Our goal is for
CrystalGym to serve as a test bed for reinforcement learning researchers and
material scientists to address these real-world design problems with practical
applications. We therefore introduce a novel class of challenges for
reinforcement learning methods dealing with time-consuming reward signals,
paving the way for future interdisciplinary research for machine learning
motivated by real-world applications.

</details>


### [642] [Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization](https://arxiv.org/abs/2509.23158)
*Yufei Shen,Ji Hwan Park,Minchao Huang,Jared F. Benge,Justin F. Rousseau,Rosemary A. Lester-Smith,Edison Thomaz*

Main category: cs.LG

TL;DR: 本研究利用LSTM模型结合被动智能手机感知数据，提出“日常行为感知增强”和“人口统计个性化”两种技术，提升跨个体认知障碍检测的泛化能力，在36名老年人中显著提高了模型性能（AUPRC从0.637升至0.766）。


<details>
  <summary>Details</summary>
Motivation: 传统临床评估频率低、敏感性不足，难以捕捉老年人细微的认知衰退，亟需更自然、连续的监测手段。

Method: 基于老年人为期一年的多模态感知数据，提取每日行为特征序列，构建LSTM模型；提出两种提升泛化性的技术：1）通过替换为行为相似日生成合成序列的日常感知增强；2）根据测试对象的人口统计特征对训练样本进行重加权。

Result: 在36名老年人的6个月数据上评估，两项技术联合使用使结合感知与人口统计特征的模型AUPRC从0.637提升至0.766。

Conclusion: 结合被动感知数据与个性化建模范式，有望实现老年人群认知障碍的可扩展、持续监测。

Abstract: Early detection of cognitive impairment is critical for timely diagnosis and
intervention, yet infrequent clinical assessments often lack the sensitivity
and temporal resolution to capture subtle cognitive declines in older adults.
Passive smartphone sensing has emerged as a promising approach for naturalistic
and continuous cognitive monitoring. Building on this potential, we implemented
a Long Short-Term Memory (LSTM) model to detect cognitive impairment from
sequences of daily behavioral features, derived from multimodal sensing data
collected in an ongoing one-year study of older adults. Our key contributions
are two techniques to enhance model generalizability across participants: (1)
routine-aware augmentation, which generates synthetic sequences by replacing
each day with behaviorally similar alternatives, and (2) demographic
personalization, which reweights training samples to emphasize those from
individuals demographically similar to the test participant. Evaluated on
6-month data from 36 older adults, these techniques jointly improved the Area
Under the Precision-Recall Curve (AUPRC) of the model trained on sensing and
demographic features from 0.637 to 0.766, highlighting the potential of
scalable monitoring of cognitive impairment in aging populations with passive
sensing.

</details>


### [643] [ProtoTS: Learning Hierarchical Prototypes for Explainable Time Series Forecasting](https://arxiv.org/abs/2509.23159)
*Ziheng Peng,Shijie Ren,Xinyue Gu,Linxiao Yang,Xiting Wang,Liang Sun*

Main category: cs.LG

TL;DR: 提出了一种名为ProtoTS的可解释时间序列预测框架，通过建模典型时间模式，在保持高精度的同时实现透明决策。


<details>
  <summary>Details</summary>
Motivation: 深度学习在时间序列预测中表现优异，但其决策过程缺乏可解释性，尤其是在高风险场景下难以建立信任；现有可解释模型通常只能提供局部、片面的解释，无法揭示异质输入变量如何共同影响整体预测趋势。

Method: ProtoTS基于去噪后保留丰富异质信息的表示来计算实例与原型的相似性；通过分层组织原型，用粗粒度原型捕捉全局时间模式，细粒度原型捕捉局部变化，从而实现多层次可解释性和专家引导。

Result: 在多个真实基准数据集（包括新发布的LOF数据集）上的实验表明，ProtoTS不仅在预测精度上优于现有方法，还能提供可由专家引导的解释，增强模型理解和决策支持。

Conclusion: ProtoTS实现了高精度与透明决策的结合，通过分层原型结构提供了多层级、可干预的可解释性，有助于提升复杂时间序列预测模型的可信度和实用性。

Abstract: While deep learning has achieved impressive performance in time series
forecasting, it becomes increasingly crucial to understand its decision-making
process for building trust in high-stakes scenarios. Existing interpretable
models often provide only local and partial explanations, lacking the
capability to reveal how heterogeneous and interacting input variables jointly
shape the overall temporal patterns in the forecast curve. We propose ProtoTS,
a novel interpretable forecasting framework that achieves both high accuracy
and transparent decision-making through modeling prototypical temporal
patterns. ProtoTS computes instance-prototype similarity based on a denoised
representation that preserves abundant heterogeneous information. The
prototypes are organized hierarchically to capture global temporal patterns
with coarse prototypes while capturing finer-grained local variations with
detailed prototypes, enabling expert steering and multi-level interpretability.
Experiments on multiple realistic benchmarks, including a newly released LOF
dataset, show that ProtoTS not only exceeds existing methods in forecast
accuracy but also delivers expert-steerable interpretations for better model
understanding and decision support.

</details>


### [644] [Dense associative memory on the Bures-Wasserstein space](https://arxiv.org/abs/2509.23162)
*Chandan Tankala,Krishnakumar Balasubramanian*

Main category: cs.LG

TL;DR: 本文将密集关联记忆（DAMs）从向量表示扩展到概率分布，特别是基于2-Wasserstein距离的高斯密度分布，提出了一种新的基于最优传输的检索机制，并证明了其具有指数级存储容量和鲁棒的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有DAM模型仅限于向量表示，无法处理分布型数据；本文旨在将关联记忆推广到概率分布空间，以更好地建模复杂数据结构。

Method: 引入Bures-Wasserstein类高斯密度作为存储单元，定义基于log-sum-exp的能量函数，并通过Gibbs加权聚合最优传输映射实现检索动态，固定点对应于自洽的Wasserstein质心。

Result: 理论证明模型具有指数级存储容量，在Wasserstein扰动下具备定量检索保证，并在合成与真实分布任务中验证了有效性。

Conclusion: 该工作将关联记忆从向量提升至分布层级，架起了经典DAM与现代生成模型之间的桥梁，为记忆增强学习中的分布存储与检索提供了新范式。

Abstract: Dense associative memories (DAMs) store and retrieve patterns via
energy-functional fixed points, but existing models are limited to vector
representations. We extend DAMs to probability distributions equipped with the
2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of
Gaussian densities. Our framework defines a log-sum-exp energy over stored
distributions and a retrieval dynamics aggregating optimal transport maps in a
Gibbs-weighted manner. Stationary points correspond to self-consistent
Wasserstein barycenters, generalizing classical DAM fixed points. We prove
exponential storage capacity, provide quantitative retrieval guarantees under
Wasserstein perturbations, and validate the model on synthetic and real-world
distributional tasks. This work elevates associative memory from vectors to
full distributions, bridging classical DAMs with modern generative modeling and
enabling distributional storage and retrieval in memory-augmented learning.

</details>


### [645] [F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning](https://arxiv.org/abs/2509.23173)
*Hangwei Zhang,Chun Kang,Yan Wang,Difan Zou*

Main category: cs.LG

TL;DR: 本文首次研究了科学机器学习中大型算子模型（LOMs）的参数高效微调（PEFT），发现LoRA在LOMs上表现不佳，而适配器调优更具优势。基于理论分析和PDE解的谱稀疏性，提出了频率自适应适配器（F-Adapter），在多个3D Navier-Stokes基准上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中的复杂物理系统建模尚未探索参数高效微调（PEFT）方法，尤其是在大型算子模型（LOMs）上的应用。现有PEFT方法如LoRA在视觉与语言任务中有效，但在LOMs上表现不佳，需针对性设计更优的微调策略。

Method: 提出频率自适应适配器（F-Adapter），根据频谱复杂度动态分配适配器容量：低频分量使用高维模块，高频分量使用低维模块，并在傅里叶域中集中参数于能量主导的低频模式。同时从理论上分析了LoRA与适配器在逼近误差上的差异。

Result: F-Adapter在多个3D Navier-Stokes基准任务上显著优于LoRA及其他主流PEFT方法，提升了泛化能力和频谱保真度，达到当前最优性能。理论分析表明，堆叠LoRA在傅里叶层中存在深度放大的逼近误差下界，而适配器具有通用逼近能力且误差随瓶颈宽度指数衰减。

Conclusion: 本工作首次将PEFT引入科学机器学习领域，验证了适配器调优相对于LoRA的优势，并提出F-Adapter作为该领域有效的PEFT范式，为LOMs的微调提供了新的理论与实践方向。

Abstract: Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for
complex downstream tasks has proven effective in vision and language
processing, yet this paradigm remains unexplored in scientific machine
learning, where the objective is to model complex physical systems. We conduct
the first systematic study of PEFT for pre-trained Large Operator Models (LOMs)
obtained by scaling variants of Fourier Neural Operator. First, we observe that
the widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance
on LOMs than Adapter tuning. Then, we further theoretically establish that
stacked LoRA incurs a depth-amplified lower bound on approximation error within
Fourier layers, whereas adapters retain universal approximation capacity and,
by concentrating parameters on energy-dominant low-frequency modes, attain
exponentially decaying error with bottleneck width in the Fourier domain.
Motivated by the robust empirical gains of adapters and by our theoretical
characterization of PDE solutions as spectrally sparse, we introduce
Frequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity
based on spectral complexity, assigning higher-dimension modules to
low-frequency components and lower-dimension modules to high-frequency
components. Our F-Adapters establish state-of-the-art (SOTA) results on
multiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both
generalization and spectral fidelity over LoRA and other PEFT techniques
commonly used in LLMs. To the best of our knowledge, this work is the first to
explore PEFT for scientific machine-learning and establishes F-Adapter as an
effective paradigm for this domain.

</details>


### [646] [ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse](https://arxiv.org/abs/2509.23183)
*Guohao Chen,Shuaicheng Niu,Deyu Chen,Jiahao Yang,Zitian Zhang,Mingkui Tan,Pengcheng Wu,Zhiqi Shen*

Main category: cs.LG

TL;DR: 本文提出了ZeroSiam，一种用于测试时熵最小化的高效非对称Siamese架构，通过非对称散度对齐防止模型崩溃，并在多种视觉适应和大语言模型推理任务中表现出稳定且优越的性能。


<details>
  <summary>Details</summary>
Motivation: 纯熵最小化容易导致模型陷入非泛化的捷径，例如预测结果坍缩为单一类别，从而限制了模型的真实学习能力。因此需要一种能防止崩溃并提升推理能力的方法。

Method: 提出ZeroSiam架构，采用非对称Siamese结构，结合可学习预测器和分类器前的停止梯度操作，实现非对称散度对齐，避免测试时熵最小化过程中的模型崩溃。

Result: 实验表明ZeroSiam能有效防止坍缩，稳定提升性能，适用于多种模型（包括易崩溃的小模型）和任务（如视觉适应与大语言模型推理），且开销极低。

Conclusion: ZeroSiam通过简单的架构设计解决了测试时熵最小化中的模型崩溃问题，增强了学习信号的鲁棒性，在多样场景下展现出广泛适用性和稳定性。

Abstract: Test-time entropy minimization helps adapt a model to novel environments and
incentivize its reasoning capability, unleashing the model's potential during
inference by allowing it to evolve and improve in real-time using its own
predictions, achieving promising performance. However, pure entropy
minimization can favor non-generalizable shortcuts, such as inflating the logit
norm and driving all predictions to a dominant class to reduce entropy, risking
collapsed solutions (e.g., constant one-hot outputs) that trivially minimize
the objective without meaningful learning. In this paper, we introduce
ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time
entropy minimization. ZeroSiam prevents collapse through asymmetric divergence
alignment, which is efficiently achieved by a learnable predictor and a
stop-gradient operator before the classifier. We provide empirical and
theoretical evidence that ZeroSiam not only prevents collapse solutions, but
also absorbs and regularizes biased learning signals, enhancing performance
even when no collapse occurs. Despite its simplicity, extensive results show
that ZeroSiam performs more stably over prior methods using negligible
overhead, demonstrating efficacy on both vision adaptation and large language
model reasoning tasks across challenging test scenarios and diverse models,
including tiny models that are particularly collapse-prone.

</details>


### [647] [CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy](https://arxiv.org/abs/2509.23190)
*Zhanhong Xie,Meifan Zhang,Lihua Yin*

Main category: cs.LG

TL;DR: 本文提出了一种名为CoSIFL的新型联邦学习框架，结合主动告警、局部差分隐私和基于Stackelberg博弈的激励机制，以提升系统安全性、隐私保护和参与积极性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临恶意客户端、推理攻击和激励不足等问题，现有方法难以兼顾安全、隐私与效率，因此需要一种集成化解决方案。

Method: CoSIFL采用主动告警机制和鲁棒聚合抵御拜占庭和推理攻击，引入Tullock竞赛启发的激励模块，并将服务器与客户端的交互建模为两阶段Stackelberg博弈，分析其均衡性及多维客户端属性的影响。

Result: 实验表明，CoSIFL在标准数据集上优于现有最先进方法，显著提升了模型鲁棒性并降低了服务器总成本。

Conclusion: CoSIFL通过整合安全防护、隐私保护与经济激励，在理论与实践中实现了更高效、安全和低成本的联邦学习。

Abstract: Federated learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data locality. However, it still faces
challenges from malicious or compromised clients, as well as difficulties in
incentivizing participants to contribute high-quality data under strict privacy
requirements. Motivated by these considerations, we propose CoSIFL, a novel
framework that integrates proactive alarming for robust security and local
differential privacy (LDP) for inference attacks, together with a
Stackelberg-based incentive scheme to encourage client participation and data
sharing. Specifically, CoSIFL uses an active alarming mechanism and robust
aggregation to defend against Byzantine and inference attacks, while a Tullock
contest-inspired incentive module rewards honest clients for both data
contributions and reliable alarm triggers. We formulate the interplay between
the server and clients as a two-stage game: in the first stage, the server
determines total rewards, selects participants, and fixes global iteration
settings, whereas in the second stage, each client decides its mini-batch size,
privacy noise scale, and alerting strategy. We prove that the server-client
game admits a unique equilibrium, and analyze how clients' multi-dimensional
attributes - such as non-IID degrees and privacy budgets - jointly affect
system efficiency. Experimental results on standard benchmarks demonstrate that
CoSIFL outperforms state-of-the-art solutions in improving model robustness and
reducing total server costs, highlighting the effectiveness of our integrated
design.

</details>


### [648] [Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization](https://arxiv.org/abs/2509.23202)
*Vage Egiazarian,Roberto L. Castro,Denis Kuznedelev,Andrei Panferov,Eldar Kurtic,Shubhra Pandit,Alexandre Marques,Mark Kurtz,Saleh Ashkboos,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 本文研究了MXFP4和NVFP4两种4位浮点格式在大语言模型推理中的实际性能，指出当前方法存在精度损失和优化困难的问题，并提出了一种专为FP4设计的量化算法MR-GPTQ，结合硬件优化实现了显著的速度提升且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 尽管MXFP4和NVFP4等4位浮点格式被寄望于提升大模型推理效率，但其实际效果尚未验证，现有量化方法在FP4上表现不佳，亟需针对性优化方案。

Method: 提出了Micro-Rotated-GPTQ（MR-GPTQ），采用块级Hadamard变换和格式特定优化，结合旋转融合权重和快速激活计算的高性能GPU内核，适配FP4特性。

Result: 在NVIDIA B200上实现最高3.6倍层间加速和2.2倍端到端加速，在RTX5090上达6倍层间和4倍端到端加速，同时精度接近或超越现有最先进方法，显著提升了MXFP4的表现。

Conclusion: FP4并非自动优于INT4，但通过专用方法如MR-GPTQ可充分发挥其潜力，开辟新的精度-性能权衡空间。

Abstract: The recent hardware-accelerated microscaling 4-bit floating-point formats
such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to
revolutionize large language model (LLM) inference. Yet, their practical
benefits remain unproven. We present the first comprehensive study of MXFP4 and
NVFP4 for post-training quantization, revealing gaps between their promise and
real-world performance. Our analysis shows that state-of-the-art methods
struggle with FP4, due to two key issues: (1) NVFP4's small group size provably
neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two
scale quantization severely degrades accuracy due to high induced error. To
bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the
classic GPTQ quantization algorithm that tailors the quantization process to
FP4's unique properties, by using block-wise Hadamard transforms and
format-specific optimizations. We support our proposal with a set of
high-performance GPU kernels that enable the MR-GPTQ format with negligible
overhead, by rotation fusion into the weights, and fast online computation of
the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and
2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on
RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches
or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the
point where it nears that of NVFP4. We conclude that, while FP4 is not an
automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock
a new frontier of accuracy-performance trade-offs.

</details>


### [649] [Towards Monotonic Improvement in In-Context Reinforcement Learning](https://arxiv.org/abs/2509.23209)
*Wenhao Zhang,Shao Zhang,Xihuai Wang,Yang Li,Ying Wen*

Main category: cs.LG

TL;DR: 本文提出了一种新的上下文强化学习方法CV-ICRL，通过引入上下文价值来缓解测试时性能下降的问题，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习方法在测试时无法持续提升性能，存在上下文模糊性问题，导致模型误判历史轨迹并选择低效动作。

Method: 提出上下文价值（Context Value）作为训练信号，表示在给定上下文中策略可达到的理想性能，并设计两种估计方法，在训练和测试阶段均保持性能非递减趋势。

Result: 在Dark Room和Minigrid平台上实验表明，CV-ICRL有效缓解了性能退化问题，提升了ICRL的整体适应能力。

Conclusion: 通过引入上下文价值，CV-ICRL解决了上下文模糊性问题，理论上保证了性能下界，实现了更稳定的持续改进。

Abstract: In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm
for developing agents that can rapidly adapt to new tasks by leveraging past
experiences as context, without updating their parameters. Recent approaches
train large sequence models on monotonic policy improvement data from online
RL, aiming to a continue improved testing time performance. However, our
experimental analysis reveals a critical flaw: these models cannot show a
continue improvement like the training data during testing time. Theoretically,
we identify this phenomenon as Contextual Ambiguity, where the model's own
stochastic actions can generate an interaction history that misleadingly
resembles that of a sub-optimal policy from the training data, initiating a
vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we
introduce Context Value into training phase and propose Context Value Informed
ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing
the ideal performance theoretically achievable by a policy given the current
context. As the context expands, Context Value could include more task-relevant
information, and therefore the ideal performance should be non-decreasing. We
prove that the Context Value tightens the lower bound on the performance gap
relative to an ideal, monotonically improving policy. We fruther propose two
methods for estimating Context Value at both training and testing time.
Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that
CV-ICRL effectively mitigates performance degradation and improves overall ICRL
abilities across various tasks and environments. The source code and data of
this paper are available at
https://github.com/Bluixe/towards_monotonic_improvement .

</details>


### [650] [One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences](https://arxiv.org/abs/2509.23213)
*Hugo Math,Robin Schön,Rainer Lienhart*

Main category: cs.LG

TL;DR: OSCAR是一种用于大规模事件序列因果发现的一次性自回归方法，利用预训练Transformer模型高效推断每序列的马尔可夫边界，显著提升计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在处理具有数千种稀疏事件类型的事件序列时难以扩展，限制了其在医疗、网络安全和车辆诊断等领域的实际应用。

Method: 提出OSCAR方法，采用两个预训练的Transformer作为密度估计器，通过一次性自回归方式推断每个序列的马尔可夫边界，避免昂贵的全局条件独立性检验，实现高效的并行因果发现。

Result: 在包含29,100个事件和474个标签的真实汽车数据集上，OSCAR能在几分钟内恢复出可解释的因果结构，而传统方法无法在此规模下运行。

Conclusion: OSCAR实现了可扩展、高效的因果发现，使大规模实际场景下的科学诊断成为可能。

Abstract: Understanding causality in event sequences with thousands of sparse event
types is critical in domains such as healthcare, cybersecurity, or vehicle
diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot
causal autoregressive method that infers per-sequence Markov Boundaries using
two pretrained Transformers as density estimators. This enables efficient,
parallel causal discovery without costly global CI testing. On a real-world
automotive dataset with 29,100 events and 474 labels, OSCAR recovers
interpretable causal structures in minutes, while classical methods fail to
scale, enabling practical scientific diagnostics at production scale.

</details>


### [651] [WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning](https://arxiv.org/abs/2509.23219)
*Xin Li,Mengbing Liu,Yiyang Zhu,Wenhe Zhang,Li Wei,Jiancheng An,Chau Yuen*

Main category: cs.LG

TL;DR: 本文提出WirelessMathLM，通过领域特定的强化学习和可验证奖励，使小型模型在无线通信数学问题上表现优异，显著提升性能并实现正向迁移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用数学推理上表现出色，但在专业领域的技术数学（如无线通信）中表现不佳，尤其是在需要精确处理信息论边界、优化约束和信号处理公式的问题上。

Method: 提出WirelessMathLM，利用可验证正确性的特性，在无需人工反馈的情况下，使用基于二值验证奖励的组相对策略优化（GRPO），直接从基础检查点训练0.5B-7B参数的模型，并构建包含4027个问题的WirelessMathBench-XL基准进行评估。

Result: 7B模型在WirelessMathBench-XL上达到39.5%准确率，接近GPT-4o的40.4%，且参数量远少于DeepSeek-R1；GRPO训练使各规模模型性能几乎翻倍（0.5B +11%，3B +103%，7B +81%），并在多个通用数学基准上实现平均+8.4分的零样本迁移提升。

Conclusion: 通过领域特定的强化学习与可验证奖励机制，小型语言模型可在专业数学任务上达到甚至超越超大规模模型的性能，并具备向通用数学任务的正向迁移能力。

Abstract: Large language models (LLMs) excel at general mathematical reasoning but fail
catastrophically on specialized technical mathematics. In wireless
communications, where problems require precise manipulation of
information-theoretic bounds, optimization constraints, and signal processing
formulations, even state-of-the-art models struggle to achieve competent
performance. We present WirelessMathLM, demonstrating that compact models
(0.5B-7B parameters) can match or exceed much larger models through
domain-specific reinforcement learning with verifiable rewards. Our key insight
is that wireless mathematics problems possess a unique property--verifiable
correctness--that enables effective reinforcement learning without human
feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027
problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with
binary verification rewards, we train models directly from base checkpoints
without supervised warm-start. Our 7B model achieves 39.5% accuracy on
WirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times
fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training
nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B
+81%), with positive transfer to general mathematics benchmarks--our models
gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and
AIME without any training on these tasks.

</details>


### [652] [SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts](https://arxiv.org/abs/2509.23232)
*Bingshuai Liu,Ante Wang,Zijun Min,Liang Yao,Haibo Zhang,Yang Liu,Anxiang Zeng,Jinsong Su*

Main category: cs.LG

TL;DR: 提出SPEC-RL框架，结合推测解码与强化学习 rollout 过程，重用历史轨迹段以减少冗余生成，在数学推理等任务上实现2-3倍加速，且不损害策略质量。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR训练中的rollout阶段计算成本高，且存在大量跨迭代的冗余生成，导致效率低下。

Method: 提出SPEC-RL，利用推测解码机制，将先前训练轮次的轨迹段作为推测前缀，通过草案生成与验证机制扩展，避免重复计算，同时保证策略一致性。

Result: 在GSM8K、MATH-500、OlympiadBench、MMLU-STEM等多个数学推理和泛化基准上，SPEC-RL实现了2-3倍的rollout加速，且策略性能保持不变。

Conclusion: SPEC-RL是一种通用、高效的rollout阶段优化方法，可无缝集成到PPO、GRPO、DAPO等主流算法中，为大规模推理模型的RLVR训练提供了实用的加速路径。

Abstract: Large Language Models (LLMs) increasingly rely on reinforcement learning with
verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.
However, the training process remains bottlenecked by the computationally
expensive rollout stage. Existing acceleration methods-such as parallelization,
objective- and data-driven modifications, and replay buffers-either incur
diminishing returns, introduce bias, or overlook redundancy across iterations.
We identify that rollouts from consecutive training epochs frequently share a
large portion of overlapping segments, wasting computation. To address this, we
propose SPEC-RL, a novel framework that integrates SPECulative decoding with
the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative
prefixes and extends them via a draft-and-verify mechanism, avoiding redundant
generation while ensuring policy consistency. Experiments on diverse math
reasoning and generalization benchmarks, including GSM8K, MATH-500,
OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout
time by 2-3x without compromising policy quality. As a purely rollout-stage
enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,
PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large
reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL

</details>


### [653] [More Data or Better Algorithms: Latent Diffusion Augmentation for Deep Imbalanced Regression](https://arxiv.org/abs/2509.23240)
*Shayan Alahyari*

Main category: cs.LG

TL;DR: 提出LatentDiff框架，利用条件扩散模型在潜在空间中生成高质量特征，有效解决深度不平衡回归中的数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 深度不平衡回归中缺乏适用于高维数据的数据级解决方案，现有方法主要依赖算法修改，难以有效提升对少数样本的预测性能。

Method: 提出LatentDiff框架，结合条件扩散模型与基于优先级的生成策略，在潜在表示空间中合成少数类样本的高质量特征，支持图像、文本等多种高维数据模态。

Result: 在三个深度不平衡回归基准上的实验表明，LatentDiff显著提升了对少数样本区域的预测性能，同时保持了整体准确性。

Conclusion: LatentDiff为深度不平衡回归提供了高效且通用的数据级解决方案，在多种数据模态上均表现出优越性能。

Abstract: In many real-world regression tasks, the data distribution is heavily skewed,
and models learn predominantly from abundant majority samples while failing to
predict minority labels accurately. While imbalanced classification has been
extensively studied, imbalanced regression remains relatively unexplored. Deep
imbalanced regression (DIR) represents cases where the input data are
high-dimensional and unstructured. Although several data-level approaches for
tabular imbalanced regression exist, deep imbalanced regression currently lacks
dedicated data-level solutions suitable for high-dimensional data and relies
primarily on algorithmic modifications. To fill this gap, we propose
LatentDiff, a novel framework that uses conditional diffusion models with
priority-based generation to synthesize high-quality features in the latent
representation space. LatentDiff is computationally efficient and applicable
across diverse data modalities, including images, text, and other
high-dimensional inputs. Experiments on three DIR benchmarks demonstrate
substantial improvements in minority regions while maintaining overall
accuracy.

</details>


### [654] [Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection](https://arxiv.org/abs/2509.23246)
*Manjiang Yu,Priyanka Singh,Xue Li,Yang Cao*

Main category: cs.LG

TL;DR: 提出了一种名为ATDP的自适应令牌加权差分隐私方法，通过在敏感token的梯度上集中噪声，显著减少训练时间并保护隐私，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易记忆敏感信息，现有差分隐私方法在训练效率和模型准确性上存在明显不足。

Method: 提出ATDP方法，在训练初期对敏感token梯度施加较大噪声以快速破坏记忆，并在后期进行轻量级后处理，仅对敏感参数注入目标噪声。该方法可集成到现有DP微调流程中，或直接用于非私有模型。

Result: ATDP相比现有DP-SGD方法，在实现相当或更好隐私保护的同时，将训练时间减少了约90%，且精度损失极小。结合初始脱敏微调阶段，形成高效的端到端DP流程。

Conclusion: ATDP是一种高效、灵活的差分隐私增强方法，能够在显著降低计算开销的同时，有效保护敏感信息，适用于各类大模型微调场景。

Abstract: Large language models (LLMs) frequently memorize sensitive or personal
information, raising significant privacy concerns. Existing variants of
differential privacy stochastic gradient descent (DPSGD) inject uniform noise
into every gradient step, significantly extending training time and reducing
model accuracy. We propose that concentrating noise primarily on gradients
associated with sensitive tokens can substantially decrease DP training time,
strengthen the protection of sensitive information, and simultaneously preserve
the model's performance on non-sensitive data. We operationalize this insight
through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of
vanilla DP-SGD that adaptively assigns different gradient weights to sensitive
and non-sensitive tokens. By employing a larger noise scale at the early stage
of training, ATDP rapidly disrupts memorization of sensitive content. As a
result, ATDP only requires a few additional epochs of lightweight
post-processing following standard fine-tuning, injecting targeted noise
primarily on parameters corresponding to sensitive tokens, thus minimally
affecting the model's general capabilities. ATDP can be seamlessly integrated
into any existing DP-based fine-tuning pipeline or directly applied to
non-private models as a fast privacy-enhancing measure. Additionally, combined
with an initial redacted fine-tuning phase, ATDP forms a streamlined DP
pipeline that achieves comparable canary protection to state-of-the-art DP-SGD
methods, significantly reduces the computational overhead of DP fine-tuning,
shortening training time by approximately 90 percent, while achieving
comparable or superior privacy protection and minimal accuracy degradation.

</details>


### [655] [Deep Learning for Subspace Regression](https://arxiv.org/abs/2509.23249)
*Vladimir Fanaskov,Vladislav Trifonov,Alexander Rudikov,Ekaterina Muravleva,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出将子空间插值问题松弛为回归问题，使用神经网络预测高维参数下的子空间，并通过预测更大维度的子空间来提升模型平滑性和精度。


<details>
  <summary>Details</summary>
Motivation: 传统插值方法在高维参数空间中不可行或不可靠，需寻找更有效的子空间逼近方法。

Method: 将子空间插值转化为回归问题，设计适用于子空间数据的损失函数，采用神经网络拟合高维目标函数，并引入冗余——预测比所需更大的子空间以简化学习。

Result: 理论表明该策略降低了椭圆特征问题中映射的复杂性并使映射更平滑；实验结果显示预测更大子空间显著提高精度。

Conclusion: 所提出的子空间回归方法在多种任务（如参数化特征问题、偏微分方程求解等）中具有实用价值，尤其适用于高维参数下的降阶建模。

Abstract: It is often possible to perform reduced order modelling by specifying linear
subspace which accurately captures the dynamics of the system. This approach
becomes especially appealing when linear subspace explicitly depends on
parameters of the problem. A practical way to apply such a scheme is to compute
subspaces for a selected set of parameters in the computationally demanding
offline stage and in the online stage approximate subspace for unknown
parameters by interpolation. For realistic problems the space of parameters is
high dimensional, which renders classical interpolation strategies infeasible
or unreliable. We propose to relax the interpolation problem to regression,
introduce several loss functions suitable for subspace data, and use a neural
network as an approximation to high-dimensional target function. To further
simplify a learning problem we introduce redundancy: in place of predicting
subspace of a given dimension we predict larger subspace. We show theoretically
that this strategy decreases the complexity of the mapping for elliptic
eigenproblems with constant coefficients and makes the mapping smoother for
general smooth function on the Grassmann manifold. Empirical results also show
that accuracy significantly improves when larger-than-needed subspaces are
predicted. With the set of numerical illustrations we demonstrate that subspace
regression can be useful for a range of tasks including parametric
eigenproblems, deflation techniques, relaxation methods, optimal control and
solution of parametric partial differential equations.

</details>


### [656] [NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning](https://arxiv.org/abs/2509.23252)
*Raviteja Anantha,Soheil Hor,Teodor Nicola Antoniu,Layne C. Price*

Main category: cs.LG

TL;DR: NanoFlux是一种新型对抗性框架，通过生成少于200个样本的针对性训练数据，显著提升大语言模型在数学、科学和医学推理任务上的表现，同时降低3-14倍计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要大量标注数据和计算资源，效率低下。作者旨在探索一种更高效的数据生成方式，以极小但高质量的数据集提升LLM的复杂推理能力。

Method: 提出NanoFlux框架，采用攻击者-防御者竞争机制，在工具增强的裁判监督下生成多步推理问题及解释性标注；结合基于嵌入的新颖性过滤和多跳推理技术，自动化合成高价值训练数据。

Result: 在仅使用少于200个样本的情况下，对4B参数模型微调后性能超越完整基准微调：数学推理(GSMHard)提升+5.9%，科学推理(GenomeBench)提升+3.6%，医学推理(MultiMedQA)提升+16.6%；计算需求减少3-14倍；消融研究揭示问题复杂度与性能之间的非单调关系，存在领域特定最优点。

Conclusion: NanoFlux证明了通过智能合成小规模、精准定位的训练数据可有效提升LLM推理能力，未来模型改进的关键可能在于高质量数据的自动化构建而非大规模数据堆砌。

Abstract: We present NanoFlux, a novel adversarial framework for generating targeted
training data to improve LLM reasoning, where adversarially-generated datasets
containing fewer than 200 examples outperform conventional fine-tuning
approaches. The framework employs a competitive dynamic between models
alternating as Attacker and Defender, supervised by a tool-augmented Judge,
synthesizing multi-step questions with explanatory annotations that target
specific reasoning capabilities. Fine-tuning a 4B-parameter model on
NanoFlux-generated data yields performance gains across diverse domains
compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning
(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical
reasoning (MultiMedQA), while reducing computational requirements by 3-14x.
Ablation studies reveal a non-monotonic relationship between dataset
characteristics and model performance, uncovering domain-specific optimal
points for question complexity and reasoning quality. NanoFlux automates
training data generation through embedding-based novelty filtering,
tool-augmented evaluation, and multi-hop reasoning, suggesting that future
model improvements may lie in the intelligent synthesis of small, precisely
targeted training datasets.

</details>


### [657] [ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction](https://arxiv.org/abs/2509.23254)
*Zhang-Yu You,Jiahao Ma,Hongzong Li,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Conformer架构的新型模型ABCONFORMER，用于仅从序列数据中准确预测抗体-抗原相互作用界面，包括表位和互补位，在无结构信息条件下实现了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测抗体-抗原相互作用界面对于疫苗设计、免疫诊断和治疗性抗体开发至关重要，但仅从序列出发进行可靠预测仍具挑战性。

Method: 采用Conformer作为主干网络，并引入受物理启发的滑动注意力机制，以捕捉生物序列的局部和全局特征，实现不依赖三维结构的残基级接触恢复。

Result: ABCONFORMER在最新的SARS-CoV-2抗体-抗原数据集上达到最先进水平，在抗体无关的表位预测任务上优于现有主流序列方法，消融实验表明滑动注意力显著提升预测精度。

Conclusion: ABCONFORMER是一种高效、可靠的序列驱动抗体-抗原相互作用预测工具，滑动注意力机制有效增强了模型对关键结合位点的识别能力。

Abstract: Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for
vaccine design, immunodiagnostics, and therapeutic antibody development.
However, achieving reliable predictions from sequences alone remains a
challenge. In this paper, we present ABCONFORMER, a model based on the
Conformer backbone that captures both local and global features of a
biosequence. To accurately capture Ab-Ag interactions, we introduced the
physics-inspired sliding attention, enabling residue-level contact recovery
without relying on three-dimensional structural data. ABConformer can
accurately predict paratopes and epitopes given the antibody and antigen
sequence, and predict pan-epitopes on the antigen without antibody information.
In comparison experiments, ABCONFORMER achieves state-of-the-art performance on
a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based
methods for antibody-agnostic epitope prediction. Ablation studies further
quantify the contribution of each component, demonstrating that, compared to
conventional cross-attention, sliding attention significantly enhances the
precision of epitope prediction. To facilitate reproducibility, we will release
the code under an open-source license upon acceptance.

</details>


### [658] [CREPE: Controlling Diffusion with Replica Exchange](https://arxiv.org/abs/2509.23265)
*Jiajun He,Paul Jeha,Peter Potaptchik,Leo Zhang,José Miguel Hernández-Lobato,Yuanqi Du,Saifuddin Syed,Francisco Vargas*

Main category: cs.LG

TL;DR: 提出基于副本交换的CREPE方法，用于扩散模型推理时控制，具有高样本多样性和在线优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖启发式引导或与SMC结合进行偏差校正，缺乏灵活性和多样性。

Method: 采用副本交换算法（replica exchange），在推理时生成序列样本，支持在线精炼和早期终止。

Result: 在温度退火、奖励倾斜、模型组合和无分类器引导去偏等任务中表现出色，性能媲美SMC方法。

Conclusion: CREPE提供了一种灵活、高效的扩散模型推理控制方案，优于传统SMC方法。

Abstract: Inference-time control of diffusion models aims to steer model outputs to
satisfy new constraints without retraining. Previous approaches have mostly
relied on heuristic guidance or have been coupled with Sequential Monte Carlo
(SMC) for bias correction. In this paper, we propose a flexible alternative
based on replica exchange, an algorithm designed initially for sampling
problems. We refer to this method as the CREPE (Controlling with REPlica
Exchange). Unlike SMC, CREPE: (1) generates particles sequentially, (2)
maintains high diversity in the generated samples after a burn-in period, and
(3) enables online refinement or early termination. We demonstrate its
versatility across various tasks, including temperature annealing,
reward-tilting, model composition and classifier-free guidance debiasing, with
competitive performance compared to prior SMC methods.

</details>


### [659] [Transfer Learning and Machine Learning for Training Five Year Survival Prognostic Models in Early Breast Cancer](https://arxiv.org/abs/2509.23268)
*Lisa Pilgram,Kai Yang,Ana-Alicia Beltran-Bless,Gregory R. Pond,Lisa Vandermeer,John Hilton,Marie-France Savard,Andréanne Leblanc,Lois Sheperd,Bingshu E. Chen,John M. S. Bartlett,Karen J. Taylor,Jane Bayani,Sarah L. Barker,Melanie Spears,Cornelis J. H. van der Velde,Elma Meershoek-Klein Kranenbarg,Luc Dirix,Elizabeth Mallon,Annette Hasenburg,Christos Markopoulos,Lamin Juwara,Fida K. Dankar,Mark Clemons,Khaled El Emam*

Main category: cs.LG

TL;DR: 该研究比较了从头机器学习、迁移学习和集成方法在乳腺癌生存预后预测中的表现，发现这些方法在信息缺失或数据分布变化的情况下优于传统的PREDICT v3模型。


<details>
  <summary>Details</summary>
Motivation: 为了提升乳腺癌预后预测的准确性与适用性，尤其是在临床病理数据不完整或存在数据分布偏移的情况下，探索机器学习方法的潜力。

Method: 使用MA.27试验数据训练模型，并在TEAM试验和SEER队列中进行外部验证；采用迁移学习（微调PREDICT v3）、从头学习（随机生存森林和XGBoost）以及加权集成方法；通过校准度和区分度评估模型性能，并使用SHAP值分析特征重要性。

Result: 迁移学习、从头RSF和集成方法在MA.27中显著改善校准度（ICI从0.042降至≤0.007），区分度略有提升（AUC从0.738升至0.744–0.799）；ML模型能处理PREDICT v3因缺失信息无法预测的情况（23.8%-25.8%）；SHAP值显示年龄、淋巴结状态、分级和肿瘤大小最重要；在SEER中验证了优势，但在TEAM中未显著。

Conclusion: 迁移学习、从头机器学习和集成方法可有效改进乳腺癌生存预后预测，尤其适用于传统模型因信息缺失或数据偏移而受限的场景。

Abstract: Prognostic information is essential for decision-making in breast cancer
management. Recently trials have predominantly focused on genomic
prognostication tools, even though clinicopathological prognostication is less
costly and more widely accessible. Machine learning (ML), transfer learning and
ensemble integration offer opportunities to build robust prognostication
frameworks. We evaluate this potential to improve survival prognostication in
breast cancer by comparing de-novo ML, transfer learning from a pre-trained
prognostic tool and ensemble integration. Data from the MA.27 trial was used
for model training, with external validation on the TEAM trial and a SEER
cohort. Transfer learning was applied by fine-tuning the pre-trained prognostic
tool PREDICT v3, de-novo ML included Random Survival Forests and Extreme
Gradient Boosting, and ensemble integration was realized through a weighted sum
of model predictions. Transfer learning, de-novo RSF, and ensemble integration
improved calibration in MA.27 over the pre-trained model (ICI reduced from
0.042 in PREDICT v3 to <=0.007) while discrimination remained comparable (AUC
increased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3
predictions were observed in 23.8-25.8% of MA.27 individuals due to missing
information. In contrast, ML models and ensemble integration could predict
survival regardless of missing information. Across all models, patient age,
nodal status, pathological grading and tumor size had the highest SHAP values,
indicating their importance for survival prognostication. External validation
in SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and
ensemble integration. This study demonstrates that transfer learning, de-novo
RSF, and ensemble integration can improve prognostication in situations where
relevant information for PREDICT v3 is lacking or where a dataset shift is
likely.

</details>


### [660] [Continuous-Time Reinforcement Learning for Asset-Liability Management](https://arxiv.org/abs/2509.23280)
*Yilie Huang*

Main category: cs.LG

TL;DR: 提出一种基于连续时间强化学习的资产-负债管理（ALM）新方法，采用线性二次型框架，结合中间和终端目标，通过模型无关的策略梯度软演员-评论家算法实现动态资产与负债同步。


<details>
  <summary>Details</summary>
Motivation: 传统ALM方法依赖模型假设且调参复杂，现有强化学习方法在探索与利用平衡上表现不佳，需更高效、无需环境建模的ALM策略学习方法。

Method: 设计了一种面向ALM的软演员-评论家算法，引入演员的自适应探索和评论家的调度探索机制，在连续时间LQ框架下实现模型无关的策略优化。

Result: 在200种随机市场情景下，该方法在平均奖励上优于两种改进的传统金融策略、一种基于模型的连续时间RL方法及三种先进RL算法，表现出快速初期收益和持续优异性能。

Conclusion: 该方法的优势不来自复杂神经网络或参数估计提升，而在于直接学习最优ALM策略而不学习环境，实现了更高效稳定的资产-负债动态匹配。

Abstract: This paper proposes a novel approach for Asset-Liability Management (ALM) by
employing continuous-time Reinforcement Learning (RL) with a linear-quadratic
(LQ) formulation that incorporates both interim and terminal objectives. We
develop a model-free, policy gradient-based soft actor-critic algorithm
tailored to ALM for dynamically synchronizing assets and liabilities. To ensure
an effective balance between exploration and exploitation with minimal tuning,
we introduce adaptive exploration for the actor and scheduled exploration for
the critic. Our empirical study evaluates this approach against two enhanced
traditional financial strategies, a model-based continuous-time RL method, and
three state-of-the-art RL algorithms. Evaluated across 200 randomized market
scenarios, our method achieves higher average rewards than all alternative
strategies, with rapid initial gains and sustained superior performance. The
outperformance stems not from complex neural networks or improved parameter
estimation, but from directly learning the optimal ALM strategy without
learning the environment.

</details>


### [661] [A Neural ODE Approach to Aircraft Flight Dynamics Modelling](https://arxiv.org/abs/2509.23307)
*Gabriel Jarry,Ramon Dalmau,Xavier Olive,Philippe Very*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经常微分方程（NODE-FDM）的飞机飞行动态模型，利用快速存取记录器（QAR）数据进行训练，在飞行轨迹预测中表现出优于现有BADA模型的精度，尤其在下降阶段对高度、速度和质量动态的模拟有显著提升，展示了物理信息引导的神经网络在高保真飞机性能建模中的潜力。


<details>
  <summary>Details</summary>
Motivation: 准确的飞机轨迹预测对空中交通管理、航空公司运营和环境评估至关重要。传统模型如BADA在复杂飞行阶段（如下降段）存在精度不足的问题，因此需要一种更高保真度的数据驱动建模方法。

Method: 提出NODE-FDM模型，结合解析运动学关系与数据驱动的神经常微分方程，利用QAR飞行数据进行训练，实现对飞机纵向动力学的高精度建模。

Result: NODE-FDM在高度、速度和质量动态的再现上显著优于BADA4结合轨迹控制例程的现有方法，尤其在下降阶段表现更优。

Conclusion: 尽管存在物理约束不足和QAR数据有限等局限性，研究验证了物理信息引导的神经常微分方程在飞机性能建模中的有效性与潜力，未来将扩展至完整的横向动力学建模。

Abstract: Accurate aircraft trajectory prediction is critical for air traffic
management, airline operations, and environmental assessment. This paper
introduces NODE-FDM, a Neural Ordinary Differential Equations-based Flight
Dynamics Model trained on Quick Access Recorder (QAR) data. By combining
analytical kinematic relations with data-driven components, NODE-FDM achieves a
more accurate reproduction of recorded trajectories than state-of-the-art
models such as a BADA-based trajectory generation methodology (BADA4
performance model combined with trajectory control routines), particularly in
the descent phase of the flight. The analysis demonstrates marked improvements
across altitude, speed, and mass dynamics. Despite current limitations,
including limited physical constraints and the limited availability of QAR
data, the results demonstrate the potential of physics-informed neural ordinary
differential equations as a high-fidelity, data-driven approach to aircraft
performance modelling. Future work will extend the framework to incorporate a
full modelling of the lateral dynamics of the aircraft.

</details>


### [662] [ASTGI: Adaptive Spatio-Temporal Graph Interactions for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.23313)
*Xvyuan Liu,Xiangfei Qiu,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: 提出了一种名为自适应时空图交互（ASTGI）的框架，用于解决不规则多变量时间序列预测中的信息表示和动态依赖建模问题，在多个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不规则多变量时间序列在医疗和金融等领域广泛存在，但其异步采样和不规则间隔给现有方法带来挑战：如何准确表示原始信息而不引入失真，以及如何有效捕捉观测点间的复杂动态依赖关系。

Method: 设计了四个模块：时空点表示模块将每个观测编码为可学习的嵌入空间中的点；邻域自适应图构建模块通过最近邻搜索为每个点自适应构建因果图；时空动态传播模块基于点之间的相对时空位置生成消息并计算交互权重，迭代更新图上的信息；基于查询点的预测模块通过聚合邻域信息并回归生成最终预测。

Result: 在多个基准数据集上的实验表明，ASTGI在预测性能上显著优于多种现有的先进方法。

Conclusion: ASTGI能有效处理不规则时间序列的表示与依赖建模，提升了预测精度，具有在关键领域应用的潜力。

Abstract: Irregular multivariate time series (IMTS) are prevalent in critical domains
like healthcare and finance, where accurate forecasting is vital for proactive
decision-making. However, the asynchronous sampling and irregular intervals
inherent to IMTS pose two core challenges for existing methods: (1) how to
accurately represent the raw information of irregular time series without
introducing data distortion, and (2) how to effectively capture the complex
dynamic dependencies between observation points. To address these challenges,
we propose the Adaptive Spatio-Temporal Graph Interaction (ASTGI) framework.
Specifically, the framework first employs a Spatio-Temporal Point
Representation module to encode each discrete observation as a point within a
learnable spatio-temporal embedding space. Second, a Neighborhood-Adaptive
Graph Construction module adaptively builds a causal graph for each point in
the embedding space via nearest neighbor search. Subsequently, a
Spatio-Temporal Dynamic Propagation module iteratively updates information on
these adaptive causal graphs by generating messages and computing interaction
weights based on the relative spatio-temporal positions between points.
Finally, a Query Point-based Prediction module generates the final forecast by
aggregating neighborhood information for a new query point and performing
regression. Extensive experiments on multiple benchmark datasets demonstrate
that ASTGI outperforms various state-of-the-art methods.

</details>


### [663] [Two-Scale Latent Dynamics for Recurrent-Depth Transformers](https://arxiv.org/abs/2509.23314)
*Francesco Pappone,Donato Crisostomi,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 本文研究了递归深度Transformer中迭代计算的几何特性，提出了一个两尺度操作模型：块内为小尺度优化，块间为大尺度漂移。随着训练进行，迭代步长变小且趋向正交，表明模型更精细地捕捉局部结构。基于此，提出一种基于二阶梯度的早退机制，在性能、稳定性和时间效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解递归深度Transformer在测试时通过重复迭代潜在表示来扩展计算的过程中的动态行为，尤其是迭代过程中的几何结构变化，以改进现有的退出机制。

Method: 分析不同检查点下循环块中隐藏状态的更新几何性质，提出两尺度模型，并基于第二阶步长差设计新的早退机制，与基于KL散度和一阶梯度的方法进行比较。

Result: 发现随着训练深入，迭代步长减小且逐步趋于相互正交，支持了细粒度建模的观点；所提出的二阶早退机制在性能、稳定性和计算效率方面均优于对比方法。

Conclusion: 递归深度Transformer的迭代过程呈现出从小尺度优化到大尺度漂移的双重动态，利用这种结构可设计更高效的早退策略，提升模型推理效率与稳定性。

Abstract: Recurrent-depth transformers scale test-time compute by iterating latent
computations before emitting tokens. We study the geometry of these iterates
and argue for a simple, \emph{two-scale} operational picture: (i) within a
looped block, updates act as \emph{small-scale refinements}; (ii) across
consecutive blocks, states undergo a \emph{larger-scale drift}. Across
checkpoints, our measurements show that loop steps become \emph{smaller} and
increasingly \emph{orthogonal} to one another, indicating better local modeling
of fine structure rather than merely pushing in a single direction. These
dynamics motivate an early-exit mechanism based on the model's second-order
difference in step-size, which we show is superior in terms of performance,
stability and time-efficiency, when compared to the KL-divergence exit strategy
of Geiping et al. and its naive first-order counterpart.

</details>


### [664] [MELCOT: A Hybrid Learning Architecture with Marginal Preservation for Matrix-Valued Regression](https://arxiv.org/abs/2509.23315)
*Khang Tran,Hieu Cao,Thinh Pham,Nghiem Diep,Tri Cao,Binh Nguyen*

Main category: cs.LG

TL;DR: 提出MELCOT模型，结合边际估计与可学习代价最优传输，用于高效高维矩阵回归。


<details>
  <summary>Details</summary>
Motivation: 解决高维矩阵回归中空间结构丢失和存储开销大的问题。

Method: 结合基于经典机器学习的边际估计（ME）模块和基于深度学习的可学习代价最优传输（LCOT）模块。

Result: 在多个数据集和领域上实验表明，MELCOT在保持高效率的同时始终优于所有基线方法。

Conclusion: MELCOT有效融合了经典方法与深度学习的优势，适用于高维矩阵回归任务。

Abstract: Regression is essential across many domains but remains challenging in
high-dimensional settings, where existing methods often lose spatial structure
or demand heavy storage. In this work, we address the problem of matrix-valued
regression, where each sample is naturally represented as a matrix. We propose
MELCOT, a hybrid model that integrates a classical machine learning-based
Marginal Estimation (ME) block with a deep learning-based Learnable-Cost
Optimal Transport (LCOT) block. The ME block estimates data marginals to
preserve spatial information, while the LCOT block learns complex global
features. This design enables MELCOT to inherit the strengths of both classical
and deep learning methods. Extensive experiments across diverse datasets and
domains demonstrate that MELCOT consistently outperforms all baselines while
remaining highly efficient.

</details>


### [665] [LLM Interpretability with Identifiable Temporal-Instantaneous Representation](https://arxiv.org/abs/2509.23323)
*Xiangchen Song,Jiaqi Sun,Zijian Li,Yujia Zheng,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种可识别的时间因果表示学习框架，用于大规模语言模型（LLM）的高维概念空间，结合稀疏自编码器技术，有效捕捉时延和即时因果关系，提升LLM可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器缺乏时间依赖性和理论保证，因果表示学习方法难以扩展到LLM的复杂概念空间，亟需一种兼具理论基础和高效计算的可解释方法。

Method: 设计了一个可识别的时间因果表示学习框架，能够建模时间延迟和即时因果关系，并与稀疏自编码器结合，应用于LLM激活的高维概念空间。

Result: 在合成数据集上验证了方法的有效性，成功发现了LLM激活中有意义的概念关系，证明了同时建模时间与即时关系能提升可解释性。

Conclusion: 所提出的框架为LLM内部表示的可解释性提供了更强的理论支持和实践效果，弥合了机械可解释性与因果表示学习之间的差距。

Abstract: Despite Large Language Models' remarkable capabilities, understanding their
internal representations remains challenging. Mechanistic interpretability
tools such as sparse autoencoders (SAEs) were developed to extract
interpretable features from LLMs but lack temporal dependency modeling,
instantaneous relation representation, and more importantly theoretical
guarantees, undermining both the theoretical foundations and the practical
confidence necessary for subsequent analyses. While causal representation
learning (CRL) offers theoretically grounded approaches for uncovering latent
concepts, existing methods cannot scale to LLMs' rich conceptual space due to
inefficient computation. To bridge the gap, we introduce an identifiable
temporal causal representation learning framework specifically designed for
LLMs' high-dimensional concept space, capturing both time-delayed and
instantaneous causal relations. Our approach provides theoretical guarantees
and demonstrates efficacy on synthetic datasets scaled to match real-world
complexity. By extending SAE techniques with our temporal causal framework, we
successfully discover meaningful concept relationships in LLM activations. Our
findings show that modeling both temporal and instantaneous conceptual
relationships advances the interpretability of LLMs.

</details>


### [666] [Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling](https://arxiv.org/abs/2509.23325)
*Jonas Ngnawé,Maxime Heuillet,Sabyasachi Sahoo,Yann Pequignot,Ola Ahmad,Audrey Durand,Frédéric Precioso,Christian Gagné*

Main category: cs.LG

TL;DR: 本文研究了从非鲁棒预训练模型进行鲁棒微调（RFT）的挑战，发现直接使用鲁棒目标微调会导致性能不佳，称为“次优迁移”。为此，作者提出了一种新的启发式方法“Epsilon-Scheduling”，通过调整训练中的扰动强度来促进最优迁移，并引入“期望鲁棒性”作为评估指标，在多种配置下验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管开源库中存在大量非鲁棒预训练模型，但它们在鲁棒微调中的潜力尚不明确。本文旨在填补这一知识空白，系统研究从非鲁棒模型出发进行鲁棒微调的问题与解决方案。

Method: 通过实验分析非鲁棒模型在鲁棒目标下微调的表现，提出“Epsilon-Scheduling”策略，即在训练过程中动态调整扰动强度，并引入“期望鲁棒性”作为综合评估指标。

Result: 实验表明，直接对非鲁棒模型进行鲁棒微调会阻碍任务适应，导致次优迁移甚至迁移失败；而采用Epsilon-Scheduling可有效避免此问题，显著提升期望鲁棒性和整体性能。

Conclusion: Epsilon-Scheduling是一种有效的鲁棒微调策略，能够充分利用非鲁棒预训练模型实现良好的鲁棒性和任务性能平衡，为实际应用提供了可行路径。

Abstract: Fine-tuning pretrained models is a standard and effective workflow in modern
machine learning. However, robust fine-tuning (RFT), which aims to
simultaneously achieve adaptation to a downstream task and robustness to
adversarial examples, remains challenging. Despite the abundance of non-robust
pretrained models in open-source repositories, their potential for RFT is less
understood. We address this knowledge gap by systematically examining RFT from
such non-robust models. Our experiments reveal that fine-tuning non-robust
models with a robust objective, even under small perturbations, can lead to
poor performance, a phenomenon that we dub \emph{suboptimal transfer}. In
challenging scenarios (eg, difficult tasks, high perturbation), the resulting
performance can be so low that it may be considered a transfer failure. We find
that fine-tuning using a robust objective impedes task adaptation at the
beginning of training and eventually prevents optimal transfer. However, we
propose a novel heuristic, \emph{Epsilon-Scheduling}, a schedule over
perturbation strength used during training that promotes optimal transfer.
Additionally, we introduce \emph{expected robustness}, a metric that captures
performance across a range of perturbations, providing a more comprehensive
evaluation of the accuracy-robustness trade-off for diverse models at test
time. Extensive experiments on a wide range of configurations (six pretrained
models and five datasets) show that \emph{Epsilon-Scheduling} successfully
prevents \emph{suboptimal transfer} and consistently improves expected
robustness.

</details>


### [667] [Entering the Era of Discrete Diffusion Models: A Benchmark for Schrödinger Bridges and Entropic Optimal Transport](https://arxiv.org/abs/2509.23348)
*Xavier Aramayo Carrasco,Grigoriy Ksenofontov,Aleksei Leonov,Iaroslav Sergeevich Koshelev,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出了一个用于离散空间上薛定谔桥（SB）问题的基准，提供了具有解析解的概率分布对，以评估现有和新提出的算法在高维离散设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可靠的方法来评估离散域中薛定谔桥方法的实际求解效果，因此需要一个可量化、可重复的评估基准。

Method: 构建具有已知解析解的SB问题实例，提出新的SB算法DLightSB、DLightSB-M和α-CSBM，并在高维离散空间中评估多个求解器的性能。

Result: 成功构造了可用于严格评估的基准测试集，验证了新算法的有效性，并展示了不同求解器在高维离散设置中的表现差异。

Conclusion: 该工作为离散空间上的SB方法提供了首个可靠的评估框架，推动了该领域研究的可重复性和进一步发展。

Abstract: The Entropic Optimal Transport (EOT) problem and its dynamic counterpart, the
Schr\"odinger bridge (SB) problem, play an important role in modern machine
learning, linking generative modeling with optimal transport theory. While
recent advances in discrete diffusion and flow models have sparked growing
interest in applying SB methods to discrete domains, there is still no reliable
way to evaluate how well these methods actually solve the underlying problem.
We address this challenge by introducing a benchmark for SB on discrete spaces.
Our construction yields pairs of probability distributions with analytically
known SB solutions, enabling rigorous evaluation. As a byproduct of building
this benchmark, we obtain two new SB algorithms, DLightSB and DLightSB-M, and
additionally extend prior related work to construct the $\alpha$-CSBM
algorithm. We demonstrate the utility of our benchmark by evaluating both
existing and new solvers in high-dimensional discrete settings. This work
provides the first step toward proper evaluation of SB methods on discrete
spaces, paving the way for more reproducible future studies.

</details>


### [668] [Landing with the Score: Riemannian Optimization through Denoising](https://arxiv.org/abs/2509.23357)
*Andrey Kharitenko,Zebang Shen,Riccardo de Santi,Niao He,Florian Doerfler*

Main category: cs.LG

TL;DR: 提出了一种基于去噪分数函数连接数据分布与黎曼优化操作的新方法，适用于仅通过数据隐式定义的流形优化问题。


<details>
  <summary>Details</summary>
Motivation: 在数据流形假设下，高维数据集中在低维流形附近，但传统黎曼优化所需的标准几何操作不可用，因此需要一种无需显式流形结构即可进行优化的方法。

Method: 引入一个链接函数（基于去噪分数）来恢复关键的流形操作（如回缩和黎曼梯度计算），并利用扩散模型中的预训练得分网络构建两种新算法：DLF 和 DRGD。

Result: 提出了 DLF 和 DRGD 两种推理时算法，并在理论上证明了其可行性和最优性；实验显示该方法在数据驱动控制中的参考跟踪任务中有效。

Conclusion: 该方法成功将扩散模型中的得分函数用于隐式流形上的黎曼优化，为生成式AI和数据驱动设计提供了高效且理论可靠的优化框架。

Abstract: Under the data manifold hypothesis, high-dimensional data are concentrated
near a low-dimensional manifold. We study the problem of Riemannian
optimization over such manifolds when they are given only implicitly through
the data distribution, and the standard manifold operations required by
classical algorithms are unavailable. This formulation captures a broad class
of data-driven design problems that are central to modern generative AI. Our
key idea is to introduce a link function that connects the data distribution to
the geometric operations needed for optimization. We show that this function
enables the recovery of essential manifold operations, such as retraction and
Riemannian gradient computation. Moreover, we establish a direct connection
between our construction and the score function in diffusion models of the data
distribution. This connection allows us to leverage well-studied
parameterizations, efficient training procedures, and even pretrained score
networks from the diffusion model literature to perform optimization. Building
on this foundation, we propose two efficient inference-time algorithms --
Denoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD)
-- and provide theoretical guarantees for both feasibility (approximate
manifold adherence) and optimality (small Riemannian gradient norm). Finally,
we demonstrate the effectiveness of our approach on finite-horizon reference
tracking tasks in data-driven control, highlighting its potential for practical
generative and design applications.

</details>


### [669] [Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought](https://arxiv.org/abs/2509.23365)
*Hanlin Zhu,Shibo Hao,Zhiting Hu,Jiantao Jiao,Stuart Russell,Yuandong Tian*

Main category: cs.LG

TL;DR: 本文研究了连续思维（continuous CoT）在简化两层Transformer模型训练过程中的动态机制，揭示了超位置机制如何通过两个训练阶段自然形成，并通过实验验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究显示连续思维能提升大语言模型的推理能力，但其背后的超位置机制如何通过基于梯度的训练方法自然学习仍不清楚，本文旨在填补这一理论空白。

Method: 通过对简化版两层Transformer在有向图可达性问题上的训练动态进行理论分析，识别出‘思维生成’和‘预测’两个阶段，并引入索引匹配logit来刻画模型局部搜索能力的变化。

Result: 发现索引匹配logit在训练中先增加后保持有界，这种有界性在探索与利用之间实现平衡，促使模型在不确定时对多个合理推理路径赋予可比权重，从而形成超位置状态；实验结果支持该理论。

Conclusion: 连续思维中的超位置机制可通过标准梯度训练自然涌现，其核心在于训练过程中索引匹配logit的有界动态，该机制有效平衡了推理过程中的探索与利用。

Abstract: Previous work shows that the chain of continuous thought (continuous CoT)
improves the reasoning capability of large language models (LLMs) by enabling
implicit parallel thinking, and a subsequent work provided theoretical insight
by showing that a two-layer transformer equipped with continuous CoT can
efficiently solve directed graph reachability by maintaining a superposition of
multiple reasoning traces in the continuous thought. However, it remains
unclear how the superposition mechanism is naturally learned from
gradient-based training methods. To fill this gap, we theoretically analyze the
training dynamics of a simplified two-layer transformer on the directed graph
reachability problem to unveil how the superposition mechanism emerges during
training in two training stages -- (i) a thought-generation stage that
autoregressively expands the continuous thought, and (ii) a prediction stage
that converts the thought into the final answer. Our analysis reveals that
during training using continuous thought, the index-matching logit, an
important quantity which reflects the strength of the model's local search
ability, will first increase and then remain bounded under mild assumptions.
The bounded index-matching logit effectively balances exploration and
exploitation during the reasoning process: the model will exploit local problem
structures to identify plausible search traces, and assign comparable weights
to multiple such traces to explore when it is uncertain about which solution is
correct, which results in superposition. Our experimental results tracking the
growth of logits further validate our theory.

</details>


### [670] [Splines-Based Feature Importance in Kolmogorov-Arnold Networks: A Framework for Supervised Tabular Data Dimensionality Reduction](https://arxiv.org/abs/2509.23366)
*Ange-Clément Akazan,Verlon Roel Mbingui*

Main category: cs.LG

TL;DR: 本文提出并评估了基于Kolmogorov-Arnold网络（KAN）的四种特征选择方法，能够在表格数据中有效识别非线性与多变量特征重要性，具有良好的可解释性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 高维数据需要有效的特征选择来提升模型性能和可解释性，而传统方法在捕捉复杂特征交互方面存在局限。

Method: 提出四种基于KAN的特征选择器（KAN-L1、KAN-L2、KAN-SI、KAN-KO），利用样条参数化实现可解释的重要性度量，并在多个分类与回归任务上与LASSO、随机森林等经典方法对比。

Result: KAN-based方法在多种数据集上表现优于或媲美经典方法，其中KAN-L2和KAN-SI在噪声回归任务中表现稳健，KAN-L1、KAN-KO和KAN-SI在高维分类任务中能更好消除冗余；但KAN-L1在回归中过于激进，KAN-L2在分类中表现不佳。

Conclusion: 基于KAN的特征选择提供了一种强大且可解释的替代方案，能够揭示超越稀疏性或纯度指标的非线性与多变量特征相关性。

Abstract: High-dimensional datasets require effective feature selection to improve
predictive performance, interpretability, and robustness. We propose and
evaluate feature selection methods for tabular datasets based on
Kolmogorov-Arnold networks (KANs), which parameterize feature transformations
through splines, enabling direct access to interpretable importance measures.
We introduce four KAN-based selectors ($\textit{KAN-L1}$, $\textit{KAN-L2}$,
$\textit{KAN-SI}$, $\textit{KAN-KO}$) and compare them against classical
baselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple
classification and regression tabular dataset benchmarks. Average (over three
retention levels: 20\%, 40\%, and 60\%) F1 scores and $R^2$ score results
reveal that KAN-based selectors, particularly $\textit{KAN-L2}$,
$\textit{KAN-L1}$, $\textit{KAN-SI}$, and $\textit{KAN-KO}$, are competitive
with and sometimes superior to classical baselines in structured and synthetic
datasets. However, $\textit{KAN-L1}$ is often too aggressive in regression,
removing useful features, while $\textit{KAN-L2}$ underperforms in
classification, where simple coefficient shrinkage misses complex feature
interactions. $\textit{KAN-L2}$ and $\textit{KAN-SI}$ provide robust
performance on noisy regression datasets and heterogeneous datasets, aligning
closely with ensemble predictors. In classification tasks, KAN selectors such
as $\textit{KAN-L1}$, $\textit{KAN-KO}$, and $\textit{KAN-SI}$ sometimes
surpass the other selectors by eliminating redundancy, particularly in
high-dimensional multi-class data. Overall, our findings demonstrate that
KAN-based feature selection provides a powerful and interpretable alternative
to traditional methods, capable of uncovering nonlinear and multivariate
feature relevance beyond sparsity or impurity-based measures.

</details>


### [671] [Graph Your Own Prompt](https://arxiv.org/abs/2509.23373)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.LG

TL;DR: 提出了一种名为图一致性正则化（GCR）的新框架，通过将模型预测生成的关系图结构引入学习过程，提升特征表示的语义一致性和类别感知能力。


<details>
  <summary>Details</summary>
Motivation: 深度网络学到的特征表示常包含与预测语义矛盾的噪声类间相似性，需要一种机制来确保特征关系与预测行为一致。

Method: 在任意深度插入无参数的图一致性层（GCL），构建批级特征相似性图，并与基于softmax预测和类内指示符调制的全局类感知掩码预测图对齐；采用多层跨空间图对齐机制和自适应加权，根据图差异大小学习各层重要性。

Result: GCR在多个网络和数据集上均提升了特征结构的清晰度、类内聚性和泛化性能，且无需修改模型架构或训练流程。

Conclusion: GCR是一种轻量、模型无关的正则化方法，通过利用模型自身预测结构来改善语义特征学习，为从预测结构中学习提供了新视角。

Abstract: We propose Graph Consistency Regularization (GCR), a novel framework that
injects relational graph structures, derived from model predictions, into the
learning process to promote class-aware, semantically meaningful feature
representations. Functioning as a form of self-prompting, GCR enables the model
to refine its internal structure using its own outputs. While deep networks
learn rich representations, these often capture noisy inter-class similarities
that contradict the model's predicted semantics. GCR addresses this issue by
introducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths.
Each GCL builds a batch-level feature similarity graph and aligns it with a
global, class-aware masked prediction graph, derived by modulating softmax
prediction similarities with intra-class indicators. This alignment enforces
that feature-level relationships reflect class-consistent prediction behavior,
acting as a semantic regularizer throughout the network. Unlike prior work, GCR
introduces a multi-layer, cross-space graph alignment mechanism with adaptive
weighting, where layer importance is learned from graph discrepancy magnitudes.
This allows the model to prioritize semantically reliable layers and suppress
noisy ones, enhancing feature quality without modifying the architecture or
training procedure. GCR is model-agnostic, lightweight, and improves semantic
structure across various networks and datasets. Experiments show that GCR
promotes cleaner feature structure, stronger intra-class cohesion, and improved
generalization, offering a new perspective on learning from prediction
structure. [Project website](https://darcyddx.github.io/gcr/)
[Code](https://github.com/Darcyddx/graph-prompt)

</details>


### [672] [Planner Aware Path Learning in Diffusion Language Models Training](https://arxiv.org/abs/2509.23405)
*Fred Zhangzhi Peng,Zachary Bezemek,Jarrid Rector-Brooks,Shuibai Zhang,Anru R. Zhang,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: 本文提出了Planned Evidence Lower Bound (P-ELBO) 和 Planner Aware Path Learning (PAPL)，以解决扩散语言模型在非均匀规划路径下训练与推理不一致的问题，显著提升了文本、蛋白质序列和代码生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在推理时使用基于规划的非均匀去噪路径，但训练时仍采用均匀随机路径，导致训练与推理之间的不匹配。

Method: 作者推导出新的P-ELBO，将规划器引入训练目标，并提出PAPL方法，修改标准的掩码离散扩散损失，使训练与推理路径保持一致。

Result: PAPL在多个领域均取得显著提升：蛋白质序列建模相对提升40%，文本生成的MAUVE指标最高提升4倍，代码生成的HumanEval pass@10相对提升23%。

Conclusion: 通过显式建模规划路径，PAPL有效弥合了训练与推理间的差距，为基于规划的扩散语言模型提供了更优的训练框架。

Abstract: Diffusion language models have emerged as a powerful alternative to
autoregressive models, enabling fast inference through flexible and parallel
generation paths. This flexibility is enabled by new sampling strategies, or
planners, that iteratively choose where to denoise along the sequence rather
than sampling uniformly at random. However, by modifying reverse paths,
planners introduce a mismatch between the uniformly random denoising paths used
during training and the planning-based paths used at inference. In this work,
we systematically investigate this mismatch and theoretically show that the
standard discrete diffusion training evidence lower bound (ELBO) does not
accurately describe a denoiser under non-uniform planning. To bridge this gap,
we derive a new Planned Evidence Lower Bound (P-ELBO) that directly
incorporates planner-based reverse dynamics into the training objective.
Building on this, we propose Planner Aware Path Learning (PAPL), a simple and
effective modification of the standard masked discrete diffusion loss that
aligns training and inference under planned denoisers. Empirically, PAPL
delivers consistent improvements across domains, including a 40% relative gain
in protein sequence modeling, up to a 4x improvement in MAUVE for text
generation, and a 23% relative gain in HumanEval pass@10 for code generation.

</details>


### [673] [Mind the Links: Cross-Layer Attention for Link Prediction in Multiplex Networks](https://arxiv.org/abs/2509.23409)
*Devesh Sharma,Aditya Kishore,Ayush Garg,Debajyoti Mazumder,Debasis Mohapatra,Jasabanta Patro*

Main category: cs.LG

TL;DR: 提出一种基于多视图边分类的多层网络链接预测框架，利用跨层自注意力融合信息，包含Trans-SLE和Trans-GAT两种模型，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多层图时要么忽略层间依赖，要么缺乏可扩展性，难以有效捕捉节点对在不同层中的复杂关系。

Method: 将多层链接预测视为多视图边分类问题，为每对节点构建每层的边视图序列，并使用跨层自注意力机制融合目标层的证据；提出了Trans-SLE（基于静态嵌入的轻量级Transformer）和Trans-GAT（结合GAT编码器与Transformer融合）两种模型。

Result: 在六个公开多层数据集上实验表明，该方法在macro-F1指标上 consistently 超过MELL、HOPLP-MUL、RMNE等强基线；引入Union--Set候选池和两种无泄漏协议确保了公平性和可扩展性。

Conclusion: 所提框架简单、可扩展，且兼容预计算嵌入和GNN编码器，有效提升了多层图链接预测性能。

Abstract: Multiplex graphs capture diverse relations among shared nodes. Most
predictors either collapse layers or treat them independently. This loses
crucial inter-layer dependencies and struggles with scalability. To overcome
this, we frame multiplex link prediction as multi-view edge classification. For
each node pair, we construct a sequence of per-layer edge views and apply
cross-layer self-attention to fuse evidence for the target layer. We present
two models as instances of this framework: Trans-SLE, a lightweight transformer
over static embeddings, and Trans-GAT, which combines layer-specific GAT
encoders with transformer fusion. To ensure scalability and fairness, we
introduce a Union--Set candidate pool and two leakage-free protocols:
cross-layer and inductive subgraph generalization. Experiments on six public
multiplex datasets show consistent macro-F_1 gains over strong baselines (MELL,
HOPLP-MUL, RMNE). Our approach is simple, scalable, and compatible with both
precomputed embeddings and GNN encoders.

</details>


### [674] [PATCH: Learnable Tile-level Hybrid Sparsity for LLMs](https://arxiv.org/abs/2509.23410)
*Younes Hourri,Mohammad Mozaffari,Maryam Mehri Dehnavi*

Main category: cs.LG

TL;DR: 提出PATCH，一种混合稀疏框架，可在0%到50%之间连续调节稀疏度，兼顾模型压缩的准确性与硬件加速效率，在保持甚至提升精度的同时实现显著端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有模型剪枝方法在准确性和硬件加速之间难以平衡：非结构化稀疏保留精度但无法有效利用GPU加速，而半结构化2:4稀疏虽硬件友好但强制50%稀疏率导致性能下降。因此需要一种兼具灵活性与效率的剪枝方案。

Method: 将权重矩阵划分为块，通过可学习的掩码机制为每个块动态选择密集或2:4稀疏模式，实现细粒度控制和层间非均匀稀疏，从而在部署效率与模型质量之间取得更好平衡。

Result: 在0.5B至8B参数规模的模型上验证有效；以LLaMA-2 7B在A6000 GPU为例，相比密集模型获得1.18x-1.38x端到端加速，同时比最先进的2:4剪枝方法MaskLLM提升0.37%-2.96%的精度。

Conclusion: PATCH通过灵活的混合稀疏策略，成功弥合了高精度与高效推理之间的鸿沟，为大模型部署提供了一种更优的剪枝解决方案。

Abstract: Large language models (LLMs) deliver impressive performance but incur
prohibitive memory and compute costs at deployment. Model pruning is an
effective way to reduce these overheads, yet existing approaches face
challenges: unstructured sparsity, where nonzeros can appear anywhere,
preserves accuracy but yields irregular access patterns that prevent GPU
acceleration, while semi-structured 2:4 sparsity is hardware-friendly but
enforces a rigid 50% pattern that degrades model quality. To bridge this gap,
we introduce PATCH, a hybrid sparsity framework that enables a continuous
sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,
assigning each tile to be either dense or 2:4 sparse via a learnable mask
selection mechanism. This design provides fine-grained control over
accuracy-acceleration tradeoffs and supports non-uniform sparsity across
layers, leading to superior overall quality. Across models from 0.5B to 8B
parameters, PATCH consistently narrows the gap to dense accuracy while
delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,
PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while
improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning
method, MaskLLM.

</details>


### [675] [URS: A Unified Neural Routing Solver for Cross-Problem Zero-Shot Generalization](https://arxiv.org/abs/2509.23413)
*Changliang Zhou,Canhong Yu,Shunyu Yao,Xi Lin,Zhenkun Wang,Yu Zhou,Qingfu Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为URS的统一神经路由求解器，能够通过单一模型在无需微调的情况下实现对上百种未见过的车辆路径问题（VRP）变体的零样本泛化求解。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器依赖预定义约束或需针对每种问题微调，限制了其在未见VRP变体上的零样本泛化能力。

Method: 提出统一数据表示（UDR）、混合偏置模块（MBM）、自适应参数生成器以及LLM驱动的约束满足机制，以增强模型对多种VRP问题的适应性和可行性保障。

Result: 在超过100种VRP变体上验证，其中包含90多种未见变体，URS均能生成高质量解，且无需任何微调。

Conclusion: URS是首个可通过单一模型处理超100种VRP变体的神经求解器，显著提升了零样本泛化能力和实际应用潜力。

Abstract: Multi-task neural routing solvers have emerged as a promising paradigm for
their ability to solve multiple vehicle routing problems (VRPs) using a single
model. However, existing neural solvers typically rely on predefined problem
constraints or require per-problem fine-tuning, which substantially limits
their zero-shot generalization ability to unseen VRP variants. To address this
critical bottleneck, we propose URS, a unified neural routing solver capable of
zero-shot generalization across a wide range of unseen VRPs using a single
model without any fine-tuning. The key component of URS is the unified data
representation (UDR), which replaces problem enumeration with data unification,
thereby broadening the problem coverage and reducing reliance on domain
expertise. In addition, we propose a Mixed Bias Module (MBM) to efficiently
learn the geometric and relational biases inherent in various problems. On top
of the proposed UDR, we further develop a parameter generator that adaptively
adjusts the decoder and bias weights of MBM to enhance zero-shot
generalization. Moreover, we propose an LLM-driven constraint satisfaction
mechanism, which translates raw problem descriptions into executable stepwise
masking functions to ensure solution feasibility. Extensive experiments
demonstrate that URS can consistently produce high-quality solutions for more
than 100 distinct VRP variants without any fine-tuning, which includes more
than 90 unseen variants. To the best of our knowledge, URS is the first neural
solver capable of handling over 100 VRP variants with a single model.

</details>


### [676] [LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport](https://arxiv.org/abs/2509.23436)
*Ashkan Shahbazi,Chayne Thrash,Yikun Bai,Keaton Hamm,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文提出了LOTFormer，一种同时具有线性时间和双随机特性的注意力机制，通过低秩运输计划和可学习的枢轴测度来提升长序列建模的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 标准的softmax注意力机制存在二次复杂度，且通常只进行行归一化，容易过度关注少数token，影响模型鲁棒性和信息流动。现有双随机注意力方法计算开销大，难以扩展。

Method: 将注意力图视为查询与键之间的传输计划，引入一个支持集较小的可学习枢轴测度，通过求解两个熵最优传输问题（查询→枢轴 和 枢轴→键）并组合成条件耦合，实现低秩、双随机的注意力矩阵。

Result: LOTFormer在Long Range Arena基准上达到了最先进的性能，优于先前的线性及基于传输的注意力方法，在准确率和效率方面均有提升。

Conclusion: LOTFormer提供了一种兼具线性计算复杂度和双随机性质的注意力机制，有效平衡了token参与度，提升了模型的可扩展性与表现力。

Abstract: Transformers have proven highly effective across a wide range of modalities.
However, the quadratic complexity of the standard softmax attention mechanism
poses a fundamental barrier to scaling them to long context windows. A large
body of work addresses this with linear attention, which reformulates attention
as a kernel function and approximates it with finite feature maps to achieve
linear-time computation. Orthogonal to computational scaling, most attention
mechanisms -- both quadratic and linear -- produce row-normalized maps that can
over-focus on a few tokens, degrading robustness and information flow.
Enforcing doubly-stochastic attention alleviates this by balancing token
participation across rows and columns, but existing doubly-stochastic attention
mechanisms typically introduce substantial overhead, undermining scalability.
We propose LOTFormer, a principled attention mechanism that is simultaneously
linear-time and doubly-stochastic. Our approach exploits the connection between
attention maps and transportation plans between query and key measures. The
central idea is to constrain the transport plan to be low-rank by conditioning
it on a learnable pivot measure with small support. Concretely, we solve two
entropic optimal transport problems (queries $\to$ pivot and pivot $\to$ keys)
and compose them into a conditional (glued) coupling. This yields an attention
matrix that is provably doubly-stochastic, has rank at most $r \ll n$, and
applies to values in $O(nr)$ time without forming the full $n \times n$ map.
The pivot locations and masses are learned end-to-end. Empirically, LOTFormer
achieves state-of-the-art results on the Long Range Arena benchmark, surpassing
prior linear and transport-based attention methods in both accuracy and
efficiency.

</details>


### [677] [Better Hessians Matter: Studying the Impact of Curvature Approximations in Influence Functions](https://arxiv.org/abs/2509.23437)
*Steve Hong,Runa Eschenhagen,Bruno Mlodozeniec,Richard Turner*

Main category: cs.LG

TL;DR: 本文研究了在深度学习中使用Hessian近似对影响函数数据归因性能的影响，发现更精确的Hessian近似能持续提升影响分数的质量，并分析了不同近似步骤对归因准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习中Hessian矩阵的逆计算困难，常用近似方法如GGN和K-FAC来计算影响函数，但这些近似对数据归因性能的影响尚不明确。

Method: 在受控的分类设置下，通过实验评估不同Hessian近似方法的质量对影响函数归因效果的影响，并分解现有近似方法的各个步骤以分析其对准确性的作用。

Result: 更高质量的Hessian近似 consistently 提升影响分数的准确性；K-FAC与GGN/EK-FAC在特征值上的不匹配是主要误差来源。

Conclusion: 改进Hessian近似的质量有助于提升数据归因性能，研究结果支持了当前相关研究方向，并指出了最关键的近似环节，为未来在计算效率与归因精度之间的权衡提供了指导。

Abstract: Influence functions offer a principled way to trace model predictions back to
training data, but their use in deep learning is hampered by the need to invert
a large, ill-conditioned Hessian matrix. Approximations such as Generalised
Gauss-Newton (GGN) and Kronecker-Factored Approximate Curvature (K-FAC) have
been proposed to make influence computation tractable, yet it remains unclear
how the departure from exactness impacts data attribution performance.
Critically, given the restricted regime in which influence functions are
derived, it is not necessarily clear better Hessian approximations should even
lead to better data attribution performance. In this paper, we investigate the
effect of Hessian approximation quality on influence-function attributions in a
controlled classification setting. Our experiments show that better Hessian
approximations consistently yield better influence score quality, offering
justification for recent research efforts towards that end. We further
decompose the approximation steps for recent Hessian approximation methods and
evaluate each step's influence on attribution accuracy. Notably, the mismatch
between K-FAC eigenvalues and GGN/EK-FAC eigenvalues accounts for the majority
of the error and influence loss. These findings highlight which approximations
are most critical, guiding future efforts to balance computational tractability
and attribution accuracy.

</details>


### [678] [Factor Decorrelation Enhanced Data Removal from Deep Predictive Models](https://arxiv.org/abs/2509.23443)
*Wenhao Yang,Lin Li,Xiaohui Tao,Kaize Shi*

Main category: cs.LG

TL;DR: 提出一种通过因子去相关和损失扰动来增强深度预测模型的数据移除新方法，有效减少特征冗余和数据泄露风险，提升模型在分布内和分布外场景下的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 保护用户隐私和合规要求需要在模型训练中移除敏感数据，但这一过程常导致分布偏移，影响模型性能，尤其是在分布外场景下。

Method: 引入两个关键组件：一是采用动态自适应权重调整和迭代表示更新的判别性保持因子去相关模块，以减少特征冗余和特征间相关性；二是结合损失扰动的平滑数据移除机制，提供信息理论上的防数据泄露保障。

Result: 在五个基准数据集上的实验表明，该方法在显著分布偏移下仍能保持高预测精度和鲁棒性，优于其他基线方法。

Conclusion: 所提方法在效率和适应性方面表现优越，适用于多种分布条件下的数据移除任务，有助于构建更安全、稳健的机器学习模型。

Abstract: The imperative of user privacy protection and regulatory compliance
necessitates sensitive data removal in model training, yet this process often
induces distributional shifts that undermine model performance-particularly in
out-of-distribution (OOD) scenarios. We propose a novel data removal approach
that enhances deep predictive models through factor decorrelation and loss
perturbation. Our approach introduces: (1) a discriminative-preserving factor
decorrelation module employing dynamic adaptive weight adjustment and iterative
representation updating to reduce feature redundancy and minimize inter-feature
correlations. (2) a smoothed data removal mechanism with loss perturbation that
creates information-theoretic safeguards against data leakage during removal
operations. Extensive experiments on five benchmark datasets show that our
approach outperforms other baselines and consistently achieves high predictive
accuracy and robustness even under significant distribution shifts. The results
highlight its superior efficiency and adaptability in both in-distribution and
out-of-distribution scenarios.

</details>


### [679] [PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations](https://arxiv.org/abs/2509.23453)
*Dawei Gao,Dali Wang,Zhuowei Gu,Qinglei Cao,Xiao Wang,Peter Thornton,Dan Ricciuto,Yunhe Feng*

Main category: cs.LG

TL;DR: PHASE是一个模块化的深度学习框架，通过融合异构数据和多层级物理约束，实现了科学模拟中物理一致且对异构性敏感的AI代理模型，在E3SM土地模型的生物地球化学自旋上升任务中实现了超过60倍的有效加速。


<details>
  <summary>Details</summary>
Motivation: 大规模数值模拟因计算成本高昂而受限，现有AI代理模型在关键科学应用中面临物理合理性、可信度及异构数据融合的挑战。

Method: 提出PHASE框架，结合数据类型感知的编码器与多层次物理约束，整合异构输入并确保从局部动力学到全局行为的物理一致性。

Result: 在E3SM Land Model的BGC spin-up任务中，仅用前20年模拟数据即可推断出需1200年以上才能达到的近平衡状态，实现至少60倍的积分长度缩减，并展现出对更高空间分辨率的良好泛化能力。

Conclusion: PHASE能够捕捉控制性物理规律而非表面相关性，为陆面建模等复杂科学工作流提供了实用且物理一致的加速方案。

Abstract: Large-scale numerical simulations underpin modern scientific discovery but
remain constrained by prohibitive computational costs. AI surrogates offer
acceleration, yet adoption in mission-critical settings is limited by concerns
over physical plausibility, trustworthiness, and the fusion of heterogeneous
data. We introduce PHASE, a modular deep-learning framework for
physics-integrated, heterogeneity-aware surrogates in scientific simulations.
PHASE combines data-type-aware encoders for heterogeneous inputs with
multi-level physics-based constraints that promote consistency from local
dynamics to global system behavior. We validate PHASE on the biogeochemical
(BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth
System Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first
scientifically validated AI-accelerated solution for this task. Using only the
first 20 simulation years, PHASE infers a near-equilibrium state that otherwise
requires more than 1,200 years of integration, yielding an effective reduction
in required integration length by at least 60x. The framework is enabled by a
pipeline for fusing heterogeneous scientific data and demonstrates strong
generalization to higher spatial resolutions with minimal fine-tuning. These
results indicate that PHASE captures governing physical regularities rather
than surface correlations, enabling practical, physically consistent
acceleration of land-surface modeling and other complex scientific workflows.

</details>


### [680] [Data-Efficient Training by Evolved Sampling](https://arxiv.org/abs/2509.23461)
*Ziheng Cheng,Zhong Li,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了一种简单而有效的动态采样框架Evolved Sampling (ES)，基于损失动态和增强的损失差异进行批次级数据选择，显著减少反向传播时间并保持模型性能，可扩展至集合级数据选择（ESWP）以进一步加速，实现最高近45%的训练时间节省。


<details>
  <summary>Details</summary>
Motivation: 为了在保持模型性能的同时加速训练过程，需要识别对训练有重要贡献的高信息量数据样本，现有方法在动态性和效率方面仍有提升空间。

Method: 提出Evolved Sampling (ES) 框架，基于训练过程中损失的变化和增强的损失差异，在批次级别进行动态数据选择，并支持灵活的频率调节；进一步扩展为ESWP，结合集合级数据选择与剪枝策略。

Result: ES(WP) 作为一种即插即用框架，在多种预训练和后训练任务中均实现了无性能损失的训练加速，最多节省近45%的壁钟时间。

Conclusion: Evolved Sampling 提供了一种高效、简洁且可扩展的数据选择方案，显著提升了训练效率，同时揭示了大规模机器学习中数据效率的重要研究方向。

Abstract: Data selection is designed to accelerate learning with preserved performance.
To achieve this, a fundamental thought is to identify informative data samples
with significant contributions to the training. In this work, we propose
\textbf{Evolved Sampling} (\textbf{ES}), a simple yet effective framework for
\emph{dynamic} sampling along the training process. This method conducts \em
batch \em level data selection based on the dynamics of losses and augmented
\emph{loss differences}, which enables flexible \emph{frequency tuning}, and
hence significantly reduces the back propagation time with maintained model
performance. Due to its conciseness, ES is also readily extensible to
incorporate \em set \em level data selection (to form ES with pruning,
\textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP)
consistently achieves lossless training accelerations across various
pre-training and post-training tasks, saving up to nearly 45\% wall-clock time.
Our results motivate further investigations on the data efficiency aspect of
modern large-scale machine learning.

</details>


### [681] [Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning](https://arxiv.org/abs/2509.23462)
*Alakh Sharma,Gaurish Trivedi,Kartikey Bhandari,Yash Sinha,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: 提出了一种名为GEMS的可扩展多智能体强化学习框架，通过使用潜在锚点和单个摊销生成器替代显式策略群体，并利用蒙特卡洛采样和自适应策略扩展，在保持PSRO理论保证的同时显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的多智能体强化学习方法（如PSRO）存在存储和计算成本高的问题，难以扩展，因此需要一种更高效的框架来实现可扩展的多智能体学习。

Method: GEMS采用紧凑的潜在锚点和一个摊销生成器代替显式策略群体，使用无偏蒙特卡洛 rollout、乘法权重元动态和模型无关的经验Bernstein UCB oracle来自适应扩展策略集，并在生成器内部通过基于优势的信任区域目标训练最佳响应。

Result: 在双人和多人游戏（如Deceptive Messages Game、Kuhn Poker和Multi-Particle环境）中，GEMS比PSRO快约6倍，内存使用减少1.3倍，同时获得更高的奖励。

Conclusion: GEMS在保留PSRO博弈论保证的同时，克服了其计算和内存效率低下的问题，实现了多领域中可扩展的多智能体学习。

Abstract: Scalable multi-agent reinforcement learning (MARL) remains a central
challenge for AI. Existing population-based methods, like Policy-Space Response
Oracles, PSRO, require storing explicit policy populations and constructing
full payoff matrices, incurring quadratic computation and linear memory costs.
We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free
framework that replaces explicit populations with a compact set of latent
anchors and a single amortized generator. Instead of exhaustively constructing
the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,
multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB
oracle to adaptively expand the policy set. Best responses are trained within
the generator using an advantage-based trust-region objective, eliminating the
need to store and train separate actors. We evaluated GEMS in a variety of
Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn
Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster,
has 1.3x less memory usage than PSRO, while also reaps higher rewards
simultaneously. These results demonstrate that GEMS retains the game theoretic
guarantees of PSRO, while overcoming its fundamental inefficiencies, hence
enabling scalable multi-agent learning in multiple domains.

</details>


### [682] [Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving](https://arxiv.org/abs/2509.23470)
*Rui Ai,Hugo De Oliveira Barbalho,Sirui Li,Alexei Robsky,David Simchi-Levi,Ishai Menache*

Main category: cs.LG

TL;DR: 提出一种结合变化点检测的近端策略优化框架（POC），用于实时混合整数线性规划（MILP）问题的重求解决策，平衡性能与计算成本，在八组数据上优于基线方法2%-17%。


<details>
  <summary>Details</summary>
Motivation: 在实时操作中，频繁重求解MILP问题计算代价高，而现有方法多集中于启发式、低数据或光滑目标场景，缺乏对NP难MILP问题的有效重求解时机决策机制。

Method: 提出POC框架，结合变化点检测识别环境变化并选择有益样本，利用强化学习（PPO）决定重求解时机，并理论分析重求解次数与成本的关系。

Result: 在八个合成与真实世界数据集上验证，POC在性能上持续超越现有基线方法2%至17%，并引入了适用于实时MILP问题的基准和评估标准。

Conclusion: POC能有效平衡MILP重求解的性能增益与计算开销，为实时优化提供了一种系统化决策方案，并填补了相关领域在真实MILP基准和评估方面的空白。

Abstract: A common challenge in real-time operations is deciding whether to re-solve an
optimization problem or continue using an existing solution. While modern data
platforms may collect information at high frequencies, many real-time
operations require repeatedly solving computationally intensive optimization
problems formulated as Mixed-Integer Linear Programs (MILPs). Determining when
to re-solve is, therefore, an economically important question. This problem
poses several challenges: 1) How to characterize solution optimality and
solving cost; 2) How to detect environmental changes and select beneficial
samples for solving the MILP; 3) Given the large time horizon and non-MDP
structure, vanilla reinforcement learning (RL) methods are not directly
applicable and tend to suffer from value function explosion. Existing
literature largely focuses on heuristics, low-data settings, and smooth
objectives, with little focus on common NP-hard MILPs. We propose a framework
called Proximal Policy Optimization with Change Point Detection (POC), which
systematically offers a solution for balancing performance and cost when
deciding appropriate re-solving times. Theoretically, we establish the
relationship between the number of re-solves and the re-solving cost. To test
our framework, we assemble eight synthetic and real-world datasets, and show
that POC consistently outperforms existing baselines by 2%-17%. As a side
benefit, our work fills the gap in the literature by introducing real-time MILP
benchmarks and evaluation criteria.

</details>


### [683] [Memory-Efficient Fine-Tuning via Low-Rank Activation Compression](https://arxiv.org/abs/2509.23472)
*Jiang-Xin Shi,Wen-Da Wei,Jin-Fei Qi,Xuanyu Chen,Tong Wei,Yu-Feng Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为LoRAct的低秩激活压缩方法，用于在微调大模型时显著降低激活内存消耗，同时保持良好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法虽然减少了可训练参数，但激活值带来的内存开销仍然很高，尤其是在大批次和长上下文场景下，限制了实际部署。

Method: 基于观察到激活矩阵的秩较低，提出LoRAct方法，在前向传播过程中在线进行低秩压缩，并设计了一种适用于低秩矩阵的采样正交分解算法，无需校准数据。

Result: 实验显示，与LoRA相比，LoRAct在视觉和语言任务上平均减少约80%的激活内存，且性能相当甚至更优。

Conclusion: LoRAct是一种灵活、高效的微调压缩方法，能有效缓解大模型微调中的内存瓶颈，具有较强的实用性与广泛适用性。

Abstract: The parameter-efficient fine-tuning paradigm has garnered significant
attention with the advancement of foundation models. Although numerous methods
have been proposed to reduce the number of trainable parameters, their
substantial memory overhead remains a critical bottleneck that hinders
practical deployment. In this paper, we observe that model activations
constitute a major source of memory consumption, especially under large batch
sizes and long context lengths; however, the rank of the activations remains
consistently low. Motivated by this insight, we propose a memory-efficient
fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior
work, LoRAct provides a more flexible and versatile compressing strategy that
can be applied online during the forward pass without the need for any
calibration data. Moreover, LoRAct incorporates a novel sampling-based
orthogonal decomposition algorithm specifically designed for low-rank matrices,
offering improved computational efficiency and a tighter error bound compared
to the widely used RSVD. Experiments on both vision and language tasks
demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces
activation memory by approximately 80% in comparison with the widely adopted
LoRA method, while maintaining competitive performance. The source code is
available at https://github.com/shijxcs/meft.

</details>


### [684] [Statistical Learning Guarantees for Group-Invariant Barron Functions](https://arxiv.org/abs/2509.23474)
*Yahong Yang,Wei Zhu*

Main category: cs.LG

TL;DR: 本文在Barron框架下研究了群不变神经网络的泛化误差，表明引入群不变结构可改善逼近精度，且估计误差不增加，从而在学习具有对称性的函数时显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 为了理解群不变神经网络在学习具有内在对称性函数时的统计优势，需从理论上分析其对泛化误差的影响。

Method: 在Barron框架下分析群不变神经网络的逼近速率和Rademacher复杂度，引入群依赖因子δ_{G,Γ,σ}来量化对逼近率的影响。

Result: 群不变结构引入的因子δ_{G,Γ,σ} ≤ 1可显著提升逼近精度；Rademacher复杂度不增大，估计误差不受影响；整体泛化误差在对称函数学习中明显改善；并给出了δ接近|G|^{-1}（有利）和接近1（不利）的示例。

Conclusion: 编码群不变结构能为学习对称目标函数带来明确的统计优势，提供了坚实的理论依据。

Abstract: We investigate the generalization error of group-invariant neural networks
within the Barron framework. Our analysis shows that incorporating
group-invariant structures introduces a group-dependent factor
$\delta_{G,\Gamma,\sigma} \le 1$ into the approximation rate. When this factor
is small, group invariance yields substantial improvements in approximation
accuracy. On the estimation side, we establish that the Rademacher complexity
of the group-invariant class is no larger than that of the non-invariant
counterpart, implying that the estimation error remains unaffected by the
incorporation of symmetry. Consequently, the generalization error can improve
significantly when learning functions with inherent group symmetries. We
further provide illustrative examples demonstrating both favorable cases, where
$\delta_{G,\Gamma,\sigma}\approx |G|^{-1}$, and unfavorable ones, where
$\delta_{G,\Gamma,\sigma}\approx 1$. Overall, our results offer a rigorous
theoretical foundation showing that encoding group-invariant structures in
neural networks leads to clear statistical advantages for symmetric target
functions.

</details>


### [685] [Temporal Generalization: A Reality Check](https://arxiv.org/abs/2509.23487)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 本文研究了在仅依赖过去数据的情况下，机器学习模型是否能在分布变化下实现泛化，比较了参数插值和参数外推两种方法，在多种时序任务中发现没有方法能始终优于直接使用最新模型参数的简单基线，表明对未来数据进行泛化的难度。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在分布偏移下表现不佳，本文旨在探究仅利用历史数据时，模型在何种条件下能够实现对未来数据的泛化。

Method: 研究了参数插值（凸组合历史模型参数）和参数外推（超出历史参数凸包范围）两种方法，并在多个时间序列任务上进行基准测试。

Result: 实验结果表明，在所有场景中，现有方法均无法一致地超越使用最新模型参数这一简单基线。

Conclusion: 在无法获取未来数据或缺乏对数据生成过程强假设的情况下，模型难以可靠地外推到未来数据，因此应谨慎对待关于未来泛化的主张。

Abstract: Machine learning (ML) models often struggle to maintain performance under
distribution shifts, leading to inaccurate predictions on unseen future data.
In this work, we investigate whether and under what conditions models can
achieve such a generalization when relying solely on past data. We explore two
primary approaches: convex combinations of past model parameters
(\emph{parameter interpolation}) and explicit extrapolation beyond the convex
hull of past parameters (\emph{parameter extrapolation}). We benchmark several
methods within these categories on a diverse set of temporal tasks, including
language modeling, news summarization, news tag prediction, academic paper
categorization, satellite image-based land use classification over time, and
historical yearbook photo gender prediction. Our empirical findings show that
none of the evaluated methods consistently outperforms the simple baseline of
using the latest available model parameters in all scenarios. In the absence of
access to future data or robust assumptions about the underlying
data-generating process, these results underscore the inherent difficulties of
generalizing and extrapolating to future data and warrant caution when
evaluating claims of such generalization.

</details>


### [686] [Revisiting Multivariate Time Series Forecasting with Missing Values](https://arxiv.org/abs/2509.23494)
*Jie Yang,Yifan Hu,Kexin Zhang,Luyang Niu,Yushun Dong,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 本文提出了一种新的多变量时间序列预测框架CRIB，直接从部分观测数据进行预测，避免了传统插补方法带来的误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理缺失值时采用先插补再预测的框架，但由于缺失值无真实标签，插补过程易引入错误，影响预测精度。

Method: 提出Consistency-Regularized Information Bottleneck (CRIB)框架，基于信息瓶颈原理，结合单变量注意力机制与一致性正则化，学习鲁棒表征以过滤缺失值噪声并保留关键预测信号。

Result: 在四个真实世界数据集上的实验表明，CRIB在高缺失率下仍能保持准确预测，显著优于现有方法。

Conclusion: CRIB通过摒弃插补步骤、直接建模部分观测序列，实现了更可靠的时间序列预测，为MTSF-M任务提供了新范式。

Abstract: Missing values are common in real-world time series, and multivariate time
series forecasting with missing values (MTSF-M) has become a crucial area of
research for ensuring reliable predictions. To address the challenge of missing
data, current approaches have developed an imputation-then-prediction framework
that uses imputation modules to fill in missing values, followed by forecasting
on the imputed data. However, this framework overlooks a critical issue: there
is no ground truth for the missing values, making the imputation process
susceptible to errors that can degrade prediction accuracy. In this paper, we
conduct a systematic empirical study and reveal that imputation without direct
supervision can corrupt the underlying data distribution and actively degrade
prediction accuracy. To address this, we propose a paradigm shift that moves
away from imputation and directly predicts from the partially observed time
series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a
novel framework built on the Information Bottleneck principle. CRIB combines a
unified-variate attention mechanism with a consistency regularization scheme to
learn robust representations that filter out noise introduced by missing values
while preserving essential predictive signals. Comprehensive experiments on
four real-world datasets demonstrate the effectiveness of CRIB, which predicts
accurately even under high missing rates. Our code is available in
https://github.com/Muyiiiii/CRIB.

</details>


### [687] [Beyond Outliers: A Study of Optimizers Under Quantization](https://arxiv.org/abs/2509.23500)
*Georgios Vlassis,Saleh Ashkboos,Alexandra Volkova,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 研究了不同优化器在量化（PTQ和QAT）下的模型性能影响，发现Shampoo在量化感知训练中表现最优，具有最高的参数效率。


<details>
  <summary>Details</summary>
Motivation: 随着新型优化器的兴起和模型量化的普及，亟需系统研究优化器选择与量化之间的相互作用对模型性能的影响。

Method: 训练了50M到1.5B参数的模型，使用六种优化器，评估其在后训练量化（PTQ）和量化感知训练（QAT）下的表现，并分析异常值指标与量化鲁棒性的关系。

Result: 发现MMR和峰度等异常值指标无法预测不同优化器下的PTQ性能；Shampoo在QAT中准确率下降最少，且展现出最佳的参数效率。

Conclusion: 优化器的选择显著影响量化后的模型性能，Shampoo在量化感知训练中优于其他优化器，具备更高的参数效率。

Abstract: As new optimizers gain traction and model quantization becomes standard for
efficient deployment, a key question arises: how does the choice of optimizer
affect model performance in the presence of quantization? Despite progress in
both areas, systematic evidence on optimizer-quantization interactions remains
limited. To fill this gap, we study the impact of optimizer choice on model
robustness under quantization, considering both post-training quantization
(PTQ), and quantization-aware training (QAT). We first train full-precision
models, ranging from 50M to 1.5B parameters, with six optimizers, to explore
the hyperparameter landscape, and establish well-tuned baselines. We then apply
PTQ to evaluate how model performance degrades when trained with different
optimizers. We find that outlier-related metrics, such as the max-to-mean ratio
(MMR) and Kurtosis, fail to predict the PTQ performance across different
optimizers. We show analytically that this is due to the MMR capturing only
isolated layer errors, while ignoring how quantization errors accumulate and
propagate through the network. To study the QAT degradation, we train quantized
models from scratch and compare them to our original-precision baselines. We
find that optimizers performing well in the original pretraining setup may not
remain optimal under QAT, and that models trained with Shampoo show the lowest
accuracy degradation. Finally, we derive scaling laws for quantization-aware
training under different optimizers, showing that Shampoo achieves the highest
parameter efficiency of all tested optimizers.

</details>


### [688] [Disentanglement of Variations with Multimodal Generative Modeling](https://arxiv.org/abs/2509.23548)
*Yijie Zhang,Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 本文提出了信息解耦的多模态VAE（IDMVAE），通过基于互信息的正则化方法，显式分离共享和私有信息，在挑战性数据集上实现了更优的生成质量和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理异质性和模态间关联时，难以在具有不足似然模型的复杂数据集上实现共享与私有信息的有效解耦。

Method: 提出IDMVAE，采用跨视图互信息最大化提取共享变量，引入循环一致性损失和生成增强去除冗余，并结合扩散模型提升潜在先验的表达能力。

Result: IDMVAE在多个挑战性数据集上实现了共享与私有信息的清晰分离，生成质量与语义一致性优于现有方法。

Conclusion: IDMVAE通过严谨的信息正则化策略有效解决了多模态数据中共享与私有信息解耦的难题，提升了生成性能。

Abstract: Multimodal data are prevalent across various domains, and learning robust
representations of such data is paramount to enhancing generation quality and
downstream task performance. To handle heterogeneity and interconnections among
different modalities, recent multimodal generative models extract shared and
private (modality-specific) information with two separate variables. Despite
attempts to enforce disentanglement between these two variables, these methods
struggle with challenging datasets where the likelihood model is insufficient.
In this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to
explicitly address this issue, with rigorous mutual information-based
regularizations, including cross-view mutual information maximization for
extracting shared variables, and a cycle-consistency style loss for redundancy
removal using generative augmentations. We further introduce diffusion models
to improve the capacity of latent priors. These newly proposed components are
complementary to each other. Compared to existing approaches, IDMVAE shows a
clean separation between shared and private information, demonstrating superior
generation quality and semantic coherence on challenging datasets.

</details>


### [689] [Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble](https://arxiv.org/abs/2509.23552)
*Md. Saiful Bari Siddiqui,Nowshin Tarannum*

Main category: cs.LG

TL;DR: 提出AMR-EnsembleNet，结合1D CNN和XGBoost，用于抗菌素耐药性预测，在E. coli数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略基因组序列顺序或计算成本高，难以在中等规模数据集上有效预测抗菌素耐药性。

Method: 构建一个融合模型：使用轻量级1D CNN捕捉SNP序列模式，结合XGBoost捕获非局部特征交互，形成CNN-XGBoost集成框架。

Result: 在809个E. coli菌株数据集上，对四种抗生素的预测性能优越，CIP的MCC达0.926，GEN的Macro F1-score为0.691，并验证模型关注已知AMR相关基因。

Conclusion: 序列感知的1D CNN与XGBoost融合可有效克服单独模型的局限，是中等规模AMR数据集上的高效预测方案。

Abstract: Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis.
While genomic sequencing enables rapid prediction of resistance phenotypes,
current computational methods have limitations. Standard machine learning
models treat the genome as an unordered collection of features, ignoring the
sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art
sequence models like Transformers are often too data-hungry and computationally
expensive for the moderately-sized datasets that are typical in this domain. To
address these challenges, we propose AMR-EnsembleNet, an ensemble framework
that synergistically combines sequence-based and feature-based learning. We
developed a lightweight, custom 1D Convolutional Neural Network (CNN) to
efficiently learn predictive sequence motifs from high-dimensional SNP data.
This sequence-aware model was ensembled with an XGBoost model, a powerful
gradient boosting system adept at capturing complex, non-local feature
interactions. We trained and evaluated our framework on a benchmark dataset of
809 E. coli strains, predicting resistance across four antibiotics with varying
class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier
performance across all the antibiotics, reaching a Matthews Correlation
Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro
F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also
show that our model consistently focuses on SNPs within well-known AMR genes
like fusA and parC, confirming it learns the correct genetic signals for
resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a
feature-based XGBoost model creates a powerful ensemble, overcoming the
limitations of using either an order-agnostic or a standalone sequence model.

</details>


### [690] [Improving constraint-based discovery with robust propagation and reliable LLM priors](https://arxiv.org/abs/2509.23570)
*Ruiqi Lyu,Alistair Turcan,Martin Jinye Zhang,Bryan Wilder*

Main category: cs.LG

TL;DR: 提出MosaCD方法，结合条件独立性检验和大语言模型（LLM）注释来提高因果结构发现的准确性，通过过滤LLM幻觉并采用置信度下降传播策略，显著提升图构建的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统基于约束的方法依赖完美的条件独立性测试和完全搜索，现实中易导致级联错误；而使用LLM作为专家的方法又假设LLM无误，不切实际。因此需要一种更鲁棒的方法结合两者优势并抑制LLM幻觉。

Method: MosaCD首先利用CI测试和LLM注释生成高置信度边方向种子集，通过打乱查询顺序检测并过滤LLM的幻觉；然后采用置信度下降传播策略，优先传播最可靠的边方向，并可与任何骨架学习方法结合。

Result: 在多个真实世界图上的实验表明，MosaCD相比现有基于约束的方法在最终图结构构建上具有更高准确率，尤其体现在初始种子的可靠性和传播过程的稳健性。

Conclusion: MosaCD通过融合CI测试与经校正的LLM知识，有效提升了因果发现的精度和鲁棒性，为利用不完美专家信息提供了新思路。

Abstract: Learning causal structure from observational data is central to scientific
modeling and decision-making. Constraint-based methods aim to recover
conditional independence (CI) relations in a causal directed acyclic graph
(DAG). Classical approaches such as PC and subsequent methods orient
v-structures first and then propagate edge directions from these seeds,
assuming perfect CI tests and exhaustive search of separating subsets --
assumptions often violated in practice, leading to cascading errors in the
final graph. Recent work has explored using large language models (LLMs) as
experts, prompting sets of nodes for edge directions, and could augment edge
orientation when assumptions are not met. However, such methods implicitly
assume perfect experts, which is unrealistic for hallucination-prone LLMs. We
propose MosaCD, a causal discovery method that propagates edges from a
high-confidence set of seeds derived from both CI tests and LLM annotations. To
filter hallucinations, we introduce shuffled queries that exploit LLMs'
positional bias, retaining only high-confidence seeds. We then apply a novel
confidence-down propagation strategy that orients the most reliable edges
first, and can be integrated with any skeleton-based discovery method. Across
multiple real-world graphs, MosaCD achieves higher accuracy in final graph
construction than existing constraint-based methods, largely due to the
improved reliability of initial seeds and robust propagation strategies.

</details>


### [691] [EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations](https://arxiv.org/abs/2509.23585)
*Emerald Zhang,Julian Weaver,Edward Castillo*

Main category: cs.LG

TL;DR: EVO-LRP使用CMA-ES优化LRP超参数，通过定量可解释性指标提升归因质量，在可视化和解释性能上优于传统XAI方法。


<details>
  <summary>Details</summary>
Motivation: 现有LRP方法依赖启发式规则，缺乏对清晰度和模型行为一致性的优化，难以平衡细节与可解释性。

Method: 提出EVO-LRP，采用协方差矩阵自适应进化策略（CMA-ES）根据忠实性、稀疏性等可解释性指标自动优化LRP超参数。

Result: EVO-LRP在可解释性指标和视觉连贯性方面优于传统XAI方法，且对类别特异性特征具有强敏感性。

Conclusion: 通过基于任务的系统化优化可显著提升归因图的质量，为模型解释提供更可靠的方法。

Abstract: Explainable AI (XAI) methods help identify which image regions influence a
model's prediction, but often face a trade-off between detail and
interpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware
alternative. However, LRP implementations commonly rely on heuristic rule sets
that are not optimized for clarity or alignment with model behavior. We
introduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) to tune LRP hyperparameters based on quantitative
interpretability metrics, such as faithfulness or sparseness. EVO-LRP
outperforms traditional XAI approaches in both interpretability metric
performance and visual coherence, with strong sensitivity to class-specific
features. These findings demonstrate that attribution quality can be
systematically improved through principled, task-specific optimization.

</details>


### [692] [Sketching Low-Rank Plus Diagonal Matrices](https://arxiv.org/abs/2509.23587)
*Andres Fernandez,Felix Dangel,Philipp Hennig,Frank Schneider*

Main category: cs.LG

TL;DR: 本文提出了SKETCHLORD方法，通过联合估计低秩和对角成分，有效逼近高维线性算子，相比传统分离或顺序方法具有更高精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能单独构建低秩或对角近似，难以准确表示更复杂的低秩加对角结构，导致近似误差较大。

Method: 将低秩与对角成分的联合估计建模为凸优化问题，利用少量矩阵-向量乘积实现高效求解。

Result: 理论和实验表明，SKETCHLORD在合成数据上能更准确地恢复低秩加对角结构，性能优于顺序估计方法。

Conclusion: SKETCHLORD为高维线性算子的高保真近似提供了一种有效工具，特别适用于大规模问题如深度学习Hessian矩阵的逼近。

Abstract: Many relevant machine learning and scientific computing tasks involve
high-dimensional linear operators accessible only via costly matrix-vector
products. In this context, recent advances in sketched methods have enabled the
construction of *either* low-rank *or* diagonal approximations from few
matrix-vector products. This provides great speedup and scalability, but
approximation errors arise due to the assumed simpler structure. This work
introduces SKETCHLORD, a method that simultaneously estimates both low-rank
*and* diagonal components, targeting the broader class of Low-Rank *plus*
Diagonal (LoRD) linear operators. We demonstrate theoretically and empirically
that this joint estimation is superior also to any sequential variant
(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as
a convex optimization problem, leading to a scalable algorithm. Comprehensive
experiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's
performance in accurately recovering these structures. This positions it as a
valuable addition to the structured approximation toolkit, particularly when
high-fidelity approximations are desired for large-scale operators, such as the
deep learning Hessian.

</details>


### [693] [Toward a Holistic Approach to Continual Model Merging](https://arxiv.org/abs/2509.23592)
*Hoang Phan,Sungmin Cha,Tung Lam Tran,Qi Lei*

Main category: cs.LG

TL;DR: 提出了一种在持续学习中进行模型合并的综合框架，通过在合并前、中、后三个阶段的干预，解决了可扩展性和功能信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统持续学习方法中存在的可扩展性差和无法访问旧数据时丢失功能信息的问题。

Method: 在主模型的切线空间内对领域特定数据进行微调以增强任务权重解耦；利用优化器状态中的功能信息进行合并；通过合并后的校正减少表示差异。

Result: 在标准的类增量和域增量基准上表现出竞争性性能，且在恒定内存约束下无需访问历史数据。

Conclusion: 该方法提供了一种可扩展、高效的灾难性遗忘问题解决方案。

Abstract: We present a holistic framework for continual model merging that intervenes
at three critical stages: pre-merging, during merging, and post-merging-to
address two fundamental challenges in continual learning. In particular,
conventional approaches either maintain a growing list of per-domain task
vectors, leading to scalability issues or rely solely on weight-space merging
when old data is inaccessible, thereby losing crucial functional information.
Our method overcomes these limitations by first fine-tuning the main model
within its tangent space on domain-specific data; this linearization amplifies
per-task weight disentanglement, effectively mitigating across-task
interference. During merging, we leverage functional information from available
optimizer states beyond mere parameter averages to avoid the need to revisit
old data. Finally, a post-merging correction aligns the representation
discrepancy between pre- and post-merged models, reducing bias and enhancing
overall performance-all while operating under constant memory constraints
without accessing historical data. Extensive experiments on standard
class-incremental and domain-incremental benchmarks demonstrate that our
approach not only achieves competitive performance but also provides a scalable
and efficient solution to the catastrophic forgetting problem.

</details>


### [694] [Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models](https://arxiv.org/abs/2509.23593)
*Zekun Wang,Anant Gupta,Zihan Dong,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型梯度几何的秩-1 EWC 方法，结合回放有效缓解灾难性遗忘，在多个图像生成数据集上显著减少遗忘并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法如回放和弹性权重固化（EWC）存在局限：回放依赖强生成器且易受分布漂移影响，EWC假设任务间共享最优参数并使用对角Fisher近似，表达能力有限。

Method: 研究扩散模型在低信噪比下的梯度几何特性，发现样本梯度高度共线，经验Fisher矩阵近似秩-1且与均值梯度对齐；据此提出秩-1版本的EWC，并与回放方法结合，利用其主导曲率方向进行正则化。

Result: 在MNIST、FashionMNIST、CIFAR-10和ImageNet-1k等类增量图像生成任务上，该方法显著优于仅回放和对角EWC基线，平均FID更优，遗忘程度大幅降低（在MNIST和FashionMNIST上几乎消除，在ImageNet-1k上减少约一半）。

Conclusion: 扩散模型具有近似秩-1的Fisher结构，利用更准确的Fisher估计后，EWC可成为回放的有效补充：回放促进跨任务参数共享，而EWC有效抑制回放引起的漂移。

Abstract: Catastrophic forgetting remains a central obstacle for continual learning in
neural models. Popular approaches -- replay and elastic weight consolidation
(EWC) -- have limitations: replay requires a strong generator and is prone to
distributional drift, while EWC implicitly assumes a shared optimum across
tasks and typically uses a diagonal Fisher approximation. In this work, we
study the gradient geometry of diffusion models, which can already produce
high-quality replay data. We provide theoretical and empirical evidence that,
in the low signal-to-noise ratio (SNR) regime, per-sample gradients become
strongly collinear, yielding an empirical Fisher that is effectively rank-1 and
aligned with the mean gradient. Leveraging this structure, we propose a rank-1
variant of EWC that is as cheap as the diagonal approximation yet captures the
dominant curvature direction. We pair this penalty with a replay-based approach
to encourage parameter sharing across tasks while mitigating drift. On
class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10,
ImageNet-1k), our method consistently improves average FID and reduces
forgetting relative to replay-only and diagonal-EWC baselines. In particular,
forgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved
on ImageNet-1k. These results suggest that diffusion models admit an
approximately rank-1 Fisher. With a better Fisher estimate, EWC becomes a
strong complement to replay: replay encourages parameter sharing across tasks,
while EWC effectively constrains replay-induced drift.

</details>


### [695] [Characteristic Root Analysis and Regularization for Linear Time Series Forecasting](https://arxiv.org/abs/2509.23597)
*Zheng Wang,Kaixuan Zhang,Wanfang Chen,Xiaonan Lu,Longyuan Li,Tobias Schlagenhauf*

Main category: cs.LG

TL;DR: 本文系统研究了线性模型在时间序列预测中的作用，强调特征根对时序动态的影响，并提出两种鲁棒的根重构策略（低秩回归与Root Purge），在理论与实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂模型在不同数据集上表现不稳定，而简单线性模型表现出惊人竞争力，但其背后机制缺乏深入理论分析，尤其是特征根在时序建模中的作用。

Method: 通过分析无噪声和含噪声情形下的特征根行为，揭示归一化、通道独立等设计的影响；提出基于低秩回归的降维方法和自适应的Root Purge算法以抑制噪声引起的虚假根。

Result: 理论分析表明噪声会导致模型产生虚假特征根，且抑制噪声需大量训练数据；所提方法在多个标准数据集上实现SOTA性能。

Conclusion: 结合经典线性系统理论与现代学习方法，可构建更鲁棒、可解释且数据高效的预测模型。

Abstract: Time series forecasting remains a critical challenge across numerous domains,
yet the effectiveness of complex models often varies unpredictably across
datasets. Recent studies highlight the surprising competitiveness of simple
linear models, suggesting that their robustness and interpretability warrant
deeper theoretical investigation. This paper presents a systematic study of
linear models for time series forecasting, with a focus on the role of
characteristic roots in temporal dynamics. We begin by analyzing the noise-free
setting, where we show that characteristic roots govern long-term behavior and
explain how design choices such as instance normalization and channel
independence affect model capabilities. We then extend our analysis to the
noisy regime, revealing that models tend to produce spurious roots. This leads
to the identification of a key data-scaling property: mitigating the influence
of noise requires disproportionately large training data, highlighting the need
for structural regularization. To address these challenges, we propose two
complementary strategies for robust root restructuring. The first uses rank
reduction techniques, including Reduced-Rank Regression and Direct Weight Rank
Reduction, to recover the low-dimensional latent dynamics. The second, a novel
adaptive method called Root Purge, encourages the model to learn a
noise-suppressing null space during training. Extensive experiments on standard
benchmarks demonstrate the effectiveness of both approaches, validating our
theoretical insights and achieving state-of-the-art results in several
settings. Our findings underscore the potential of integrating classical
theories for linear systems with modern learning techniques to build robust,
interpretable, and data-efficient forecasting models.

</details>


### [696] [GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning](https://arxiv.org/abs/2509.23616)
*Fanlong Zeng,Wensheng Gan,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GraphIFE框架以解决图神经网络中因类别不平衡导致的合成节点质量不一致问题，通过图不变特征提取提升模型对少数类的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在类别不平衡情况下常因忽略少数类而导致性能下降，且合成节点存在质量不一致问题。

Method: 引入图不变学习中的两个关键概念，设计GraphIFE框架，增强嵌入空间表示以提取不变特征。

Result: 在多个数据集上实验表明，GraphIFE在效率和泛化性方面均优于多种基线方法。

Conclusion: GraphIFE能有效缓解图数据中的类别不平衡问题，显著提升少数类的分类性能。

Abstract: The class imbalance problem refers to the disproportionate distribution of
samples across different classes within a dataset, where the minority classes
are significantly underrepresented. This issue is also prevalent in
graph-structured data. Most graph neural networks (GNNs) implicitly assume a
balanced class distribution and therefore often fail to account for the
challenges introduced by class imbalance, which can lead to biased learning and
degraded performance on minority classes. We identify a quality inconsistency
problem in synthesized nodes, which leads to suboptimal performance under graph
imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph
Invariant Feature Extraction), a novel framework designed to mitigate quality
inconsistency in synthesized nodes. Our approach incorporates two key concepts
from graph invariant learning and introduces strategies to strengthen the
embedding space representation, thereby enhancing the model's ability to
identify invariant features. Extensive experiments demonstrate the framework's
efficiency and robust generalization, as GraphIFE consistently outperforms
various baselines across multiple datasets. The code is publicly available at
https://github.com/flzeng1/GraphIFE.

</details>


### [697] [DRIK: Distribution-Robust Inductive Kriging without Information Leakage](https://arxiv.org/abs/2509.23631)
*Chen Yang,Changhao Zhao,Chen Wang,Jiansheng Fan*

Main category: cs.LG

TL;DR: 提出了一种分布鲁棒的归纳克里金法DRIK，通过3x3数据划分避免信息泄露，并在节点、边和子图层级提升OOD泛化能力，在多个时空数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统归纳克里金训练评估存在信息泄露和OOD泛化差的问题，尤其是2x2时空划分导致测试数据影响模型选择。

Method: 提出3x3数据划分方式以消除泄露，并设计DRIK方法：在节点级扰动坐标、边级随机删边、子图级添加伪标签子图，增强分布鲁棒性。

Result: 在六个时空数据集上实验表明，DRIK显著优于现有方法，MAE最高降低12.48%，且具有良好可扩展性。

Conclusion: DRIK通过改进评估设置和多层级正则化策略，有效提升了归纳克里金的OOD泛化能力和实际应用可靠性。

Abstract: Inductive kriging supports high-resolution spatio-temporal estimation with
sparse sensor networks, but conventional training-evaluation setups often
suffer from information leakage and poor out-of-distribution (OOD)
generalization. We find that the common 2x2 spatio-temporal split allows test
data to influence model selection through early stopping, obscuring the true
OOD characteristics of inductive kriging. To address this issue, we propose a
3x3 partition that cleanly separates training, validation, and test sets,
eliminating leakage and better reflecting real-world applications. Building on
this redefined setting, we introduce DRIK, a Distribution-Robust Inductive
Kriging approach designed with the intrinsic properties of inductive kriging in
mind to explicitly enhance OOD generalization, employing a three-tier strategy
at the node, edge, and subgraph levels. DRIK perturbs node coordinates to
capture continuous spatial relationships, drops edges to reduce ambiguity in
information flow and increase topological diversity, and adds pseudo-labeled
subgraphs to strengthen domain generalization. Experiments on six diverse
spatio-temporal datasets show that DRIK consistently outperforms existing
methods, achieving up to 12.48% lower MAE while maintaining strong scalability.

</details>


### [698] [PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference](https://arxiv.org/abs/2509.23638)
*Enda Yu,Zhaoning Zhang,Dezun Dong,Yongwei Wu,Xiangke Liao*

Main category: cs.LG

TL;DR: PreScope 是一种预测驱动的专家调度系统，用于解决 MoE 模型在商品硬件上部署时的内存和 PCIe 延迟瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: MoE 模型在商品硬件上部署时面临内存和 PCIe 延迟瓶颈，专家权重卸载到 CPU 内存会导致 PCIe 传输延迟远超 GPU 计算时间。

Method: 提出 PreScope 系统，包括三层核心设计：1) Learnable Layer-Aware Predictor (LLaPor) 捕捉层特定的专家激活模式；2) Prefetch-Aware Cross-Layer Scheduling (PreSched) 生成全局最优预取计划；3) Asynchronous I/O Optimizer (AsyncIO) 解耦 I/O 与计算。

Result: 相比现有最先进方法，PreScope 实现了 141% 的吞吐量提升和 74.6% 的延迟降低。

Conclusion: PreScope 有效缓解了 MoE 模型在异构硬件上的调度瓶颈，显著提升了推理效率。

Abstract: Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when
deployed on commodity hardware. Offloading expert weights to CPU memory results
in PCIe transfer latency that exceeds GPU computation by several folds. We
present PreScope, a prediction-driven expert scheduling system that addresses
three key challenges: inaccurate activation prediction, PCIe bandwidth
competition, and cross-device scheduling complexity. Our solution includes: 1)
Learnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert
activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that
generates globally optimal plans balancing prefetching costs and loading
overhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from
computation, eliminating waiting bubbles. PreScope achieves 141% higher
throughput and 74.6% lower latency than state-of-the-art solutions.

</details>


### [699] [Virtual Nodes based Heterogeneous Graph Convolutional Neural Network for Efficient Long-Range Information Aggregation](https://arxiv.org/abs/2509.23660)
*Ranhui Yan,Jia cai*

Main category: cs.LG

TL;DR: 提出基于虚拟节点的异构图卷积网络（VN-HGCN），通过引入虚拟节点增强信息流动，有效聚合长距离信息，仅用4层即可实现高性能，并具有良好的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有异构图模型难以捕捉长距离依赖，或需堆叠多层导致计算复杂度高和过平滑问题。

Method: 引入虚拟节点作为辅助节点，连接特定类型的所有节点，促进跨节点和边类型的长距离信息聚合，并在图结构中集成虚拟节点以提升信息流动效率。

Result: 在三个真实异构图数据集上实验表明，VN-HGCN优于多种先进基线方法，且仅需4层即可有效聚合信息。

Conclusion: VN-HGCN能有效解决长距离信息传播问题，降低模型深度需求，避免过平滑，同时具备良好泛化能力，可作为通用框架应用于其他HGNN模型。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) have exhibited powerful
performance in heterogeneous graph learning by aggregating information from
various types of nodes and edges. However, existing heterogeneous graph models
often struggle to capture long-range information or necessitate stacking
numerous layers to learn such dependencies, resulting in high computational
complexity and encountering over-smoothing issues. In this paper, we propose a
Virtual Nodes based Heterogeneous Graph Convolutional Network (VN-HGCN), which
leverages virtual nodes to facilitate enhanced information flow within the
graph. Virtual nodes are auxiliary nodes interconnected with all nodes of a
specific type in the graph, facilitating efficient aggregation of long-range
information across different types of nodes and edges. By incorporating virtual
nodes into the graph structure, VN-HGCN achieves effective information
aggregation with only $4$ layers. Additionally, we demonstrate that VN-HGCN can
serve as a versatile framework that can be seamlessly applied to other HGNN
models, showcasing its generalizability. Empirical evaluations validate the
effectiveness of VN-HGCN, and extensive experiments conducted on three
real-world heterogeneous graph datasets demonstrate the superiority of our
model over several state-of-the-art baselines.

</details>


### [700] [Pure Node Selection for Imbalanced Graph Node Classification](https://arxiv.org/abs/2509.23662)
*Fanlong Zeng,Wensheng Gan,Jiayang Wu,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为PNS（Pure Node Sampling）的即插即用模块，用于解决图神经网络中由随机种子引起的随机性异常连接问题（RACP），并在节点生成阶段有效缓解类别不平衡和拓扑不平衡带来的性能下降。


<details>
  <summary>Details</summary>
Motivation: 图神经网络通常假设类别平衡，忽视了图数据中普遍存在的类别不平衡问题，尤其是随机种子对模型性能的影响（RACP），亟需一种稳定且通用的解决方案。

Method: 提出PNS模块，在节点合成阶段直接操作，通过消除算法中的随机因素来解决RACP问题，同时缓解邻居节点分布异常导致的性能退化，具有良好的兼容性和稳定性。

Result: 实验表明PNS能有效消除不利随机种子的影响，在多个基准数据集和不同GNN主干网络下均优于基线方法，展现出优异的性能和稳定性。

Conclusion: PNS是一种有效的即插即用式方法，能够稳定图神经网络在类别不平衡场景下的训练过程，显著提升模型性能，具有广泛的应用前景。

Abstract: The problem of class imbalance refers to an uneven distribution of quantity
among classes in a dataset, where some classes are significantly
underrepresented compared to others. Class imbalance is also prevalent in
graph-structured data. Graph neural networks (GNNs) are typically based on the
assumption of class balance, often overlooking the issue of class imbalance. In
our investigation, we identified a problem, which we term the Randomness
Anomalous Connectivity Problem (RACP), where certain off-the-shelf models are
affected by random seeds, leading to a significant performance degradation. To
eliminate the influence of random factors in algorithms, we proposed PNS (Pure
Node Sampling) to address the RACP in the node synthesis stage. Unlike existing
approaches that design specialized algorithms to handle either quantity
imbalance or topological imbalance, PNS is a novel plug-and-play module that
operates directly during node synthesis to mitigate RACP. Moreover, PNS also
alleviates performance degradation caused by abnormal distribution of node
neighbors. We conduct a series of experiments to identify what factors are
influenced by random seeds. Experimental results demonstrate the effectiveness
and stability of our method, which not only eliminates the effect of
unfavorable random seeds but also outperforms the baseline across various
benchmark datasets with different GNN backbones. Data and code are available at
https://github.com/flzeng1/PNS.

</details>


### [701] [Calibration Meets Reality: Making Machine Learning Predictions Trustworthy](https://arxiv.org/abs/2509.23665)
*Kristina P. Sinaga,Arjun S. Nair*

Main category: cs.LG

TL;DR: 本文对Platt缩放和等距回归两种后校准方法进行了理论分析，研究了特征质量对校准性能的影响，并通过合成与真实数据实验验证了方法在不同场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管后校准方法被广泛使用，但其在不同数据集和模型架构下的理论理解仍不充分，尤其是特征质量与校准性能之间的关系缺乏深入研究。

Method: 提出对Platt缩放和等距回归的严格理论分析，推导收敛性保证、计算复杂度界限和有限样本性能指标，并通过控制合成实验和多组真实数据实验评估特征信息性对校准的影响。

Result: 实验表明，在仅使用信息特征与包含噪声的全特征空间下，不同校准方法表现出不同的鲁棒性，且所提方法在多种场景下均提升校准指标。

Conclusion: 研究为基于数据特性和计算约束选择合适的校准方法提供了实用指南，弥合了不确定性量化中理论与实践之间的差距。

Abstract: Post-hoc calibration methods are widely used to improve the reliability of
probabilistic predictions from machine learning models. Despite their
prevalence, a comprehensive theoretical understanding of these methods remains
elusive, particularly regarding their performance across different datasets and
model architectures. Input features play a crucial role in shaping model
predictions and, consequently, their calibration. However, the interplay
between feature quality and calibration performance has not been thoroughly
investigated. In this work, we present a rigorous theoretical analysis of
post-hoc calibration methods, focusing on Platt scaling and isotonic
regression. We derive convergence guarantees, computational complexity bounds,
and finite-sample performance metrics for these methods. Furthermore, we
explore the impact of feature informativeness on calibration performance
through controlled synthetic experiments. Our empirical evaluation spans a
diverse set of real-world datasets and model architectures, demonstrating
consistent improvements in calibration metrics across various scenarios. By
examining calibration performance under varying feature conditions utilizing
only informative features versus complete feature spaces including noise
dimensions, we provide fundamental insights into the robustness and reliability
of different calibration approaches. Our findings offer practical guidelines
for selecting appropriate calibration methods based on dataset characteristics
and computational constraints, bridging the gap between theoretical
understanding and practical implementation in uncertainty quantification. Code
and experimental data are available at:
https://github.com/Ajwebdevs/calibration-analysis-experiments.

</details>


### [702] [Clebsch-Gordan Transformer: Fast and Global Equivariant Attention](https://arxiv.org/abs/2509.24093)
*Owen Lewis Howell,Linfeng Zhao,Xupeng Zhu,Yaoyao Qian,Haojie Huang,Lingfeng Sun,Wil Thomason,Robert Platt,Robin Walters*

Main category: cs.LG

TL;DR: 提出了一种基于SO(3)不可约表示上Clebsch-Gordan卷积的Clebsch-Gordan Transformer，实现了高效的全局注意力机制，具有O(N log N)的复杂度，并在多个基准任务上优于现有等变Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有等变Transformer受限于计算成本，仅支持低阶特征和局部上下文，限制了表达能力和性能。

Method: 引入Clebsch-Gordan卷积，在SO(3)不可约表示上实现高效全局注意力，并利用Clebsch-Gordan矩阵的稀疏性支持高阶特征；结合权重共享或数据增强实现令牌排列等变性。

Result: 在n-body模拟、QM9、ModelNet点云分类和机器人抓取数据集上验证，相比现有方法在GPU内存、速度和精度上均有提升。

Conclusion: Clebsch-Gordan Transformer实现了高效、可扩展的全局注意力，支持全阶等变特征建模，显著提升了等变Transformer的性能与效率。

Abstract: The global attention mechanism is one of the keys to the success of
transformer architecture, but it incurs quadratic computational costs in
relation to the number of tokens. On the other hand, equivariant models, which
leverage the underlying geometric structures of problem instance, often achieve
superior accuracy in physical, biochemical, computer vision, and robotic tasks,
at the cost of additional compute requirements. As a result, existing
equivariant transformers only support low-order equivariant features and local
context windows, limiting their expressiveness and performance. This work
proposes Clebsch-Gordan Transformer, achieving efficient global attention by a
novel Clebsch-Gordon Convolution on $\SO(3)$ irreducible representations. Our
method enables equivariant modeling of features at all orders while achieving
${O}(N \log N)$ input token complexity. Additionally, the proposed method
scales well with high-order irreducible features, by exploiting the sparsity of
the Clebsch-Gordon matrix. Lastly, we also incorporate optional token
permutation equivariance through either weight sharing or data augmentation. We
benchmark our method on a diverse set of benchmarks including n-body
simulation, QM9, ModelNet point cloud classification and a robotic grasping
dataset, showing clear gains over existing equivariant transformers in GPU
memory size, speed, and accuracy.

</details>


### [703] [Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability](https://arxiv.org/abs/2509.23666)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 提出UAT框架，利用多臂老虎机动态调整早退阈值，实现在线、无监督的自适应推理，在保证性能的同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有早退策略依赖静态阈值，易导致模型对错误类别过度自信，且在分布偏移下鲁棒性差，影响可信度和准确性。

Method: 提出UAT，采用多臂老虎机框架，基于新的奖励函数在线、无监督地调整早退阈值，综合评估预测置信度及其可靠性，平衡计算效率与预测质量。

Result: 在多种任务（视觉-语言理解、文本生成、分类）上验证了UAT的有效性，相比完整模型实现了1.70-2.10倍加速，性能下降小于2%。

Conclusion: UAT能有效提升早退模型的自适应性和鲁棒性，在不同任务和分布下均表现出优越的效率与精度权衡。

Abstract: Early-Exit Deep Neural Networks enable adaptive inference by allowing
prediction at intermediary layers, significantly reducing computational costs
and latency. Most of the early exit strategies greedily exit a sample at an
intermediary layer if the confidence in class prediction exceeds a predefined
threshold that is set using a static validation set. This is problematic as the
model might be overconfident in a wrong class. Also, they are not robust to
distribution shifts encountered in deployment, which can undermine model
trustworthiness and accuracy. To address these challenges, we propose UAT that
adapts the threshold for exit decisions using a Multi-Armed Bandit framework,
enabling online, unsupervised adjustment of exit decisions. UAT makes decisions
based on a new reward function that assesses predictive certainty and its
reliability to balance computational efficiency and prediction quality while
penalizing unnecessary late exits. We provide guarantees on risk achieved by
UAT and validate its performance on diverse tasks spanning vision-language
understanding, text generation, and classification. Our framework demonstrates
consistent improvements in speedup (1.70-2.10x) with a minimal performance drop
(<2%) as compared to full model performance. Our source code is available at
https://github.com/Div290/UAT.

</details>


### [704] [Why Alignment Must Precede Distillation: A Minimal Working Explanation](https://arxiv.org/abs/2509.23667)
*Sungmin Cha,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 本文提出在偏好对齐中，应先对高召回率的参考模型进行对齐，再进行知识蒸馏（Align -> KD），而非传统的先蒸馏后对齐（KD -> Align），并通过理论、合成实验和大模型实验证明该方法更优。


<details>
  <summary>Details</summary>
Motivation: 传统做法在知识蒸馏后的紧凑模型上进行偏好对齐，忽略了参考模型的分布召回能力，导致难以对齐罕见但期望的行为。

Method: 提出Align -> KD新流程，首先在高召回率模型上进行对齐，然后进行知识蒸馏；通过Mixture-of-Gaussians可控实验和SmolLM2系列模型验证。

Result: 实验表明，先对齐后蒸馏显著提升目标行为的对齐效果，获得更高的奖励得分和目标精度，且方差更低。

Conclusion: 参考模型的召回能力是影响对齐效果的关键因素，对齐应在蒸馏之前进行，Align -> KD 是更优范式。

Abstract: For efficiency, preference alignment is often performed on compact,
knowledge-distilled (KD) models. We argue this common practice introduces a
significant limitation by overlooking a key property of the alignment's
reference model: its distributional recall. We show that the standard KD ->
Align workflow diminishes the model's capacity to align rare yet desirable
behaviors, even under strong preference signals. We instead demonstrate that
reversing the pipeline (i.e., Align -> KD) is essential: alignment must first
be performed on a high-recall reference before distillation. Our contributions
are threefold. First, we provide a minimal working explanation of how the
reference model constrains preference alignment objectives at a fundamental
level. Second, we validate this theory in a controllable Mixture-of-Gaussians
experiment, where low-recall anchoring consistently results in suboptimal model
performance. Finally, we demonstrate that the same phenomenon holds in LLM
alignment with the SmolLM2 family: models aligned after KD fail to effectively
align target behaviors, resulting in substantially lower reward and target
precision. In contrast, our proposed Align -> KD pipeline robustly aligns these
behaviors, yielding models with superior target-oriented metrics and lower
variance. Together, these results establish reference-model recall as a
first-order design choice in alignment, offering a clear principle: alignment
must precede distillation.

</details>


### [705] [Multi-Scale Spatial-Temporal Hypergraph Network with Lead-Lag Structures for Stock Time Series Forecasting](https://arxiv.org/abs/2509.23668)
*Xiangfei Qiu,Liu Yang,Hanyin Cheng,Xingjian Wu,Rongjia Wu,Zhigang Zhang,Ding Tu,Chenjuan Guo,Bin Yang,Christian S. Jensen,Jilin Hu*

Main category: cs.LG

TL;DR: 提出Hermes框架，通过超图网络中的移动聚合和多尺度融合模块，更深入地利用行业间相关性进行股票时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于超图的方法只能浅层捕捉行业相关性，未能充分考虑行业间的领先-滞后关系及多尺度信息建模。

Method: 在超图网络中引入基于超边的移动聚合模块（滑动窗口+动态时序聚合）以捕捉领先-滞后关系，并采用跨尺度、边到边的消息传递机制实现多尺度信息融合。

Result: 在多个真实股市数据集上的实验表明，Hermes在预测准确性和效率方面均优于现有的最先进方法。

Conclusion: Hermes框架能有效提升股票时间序列预测性能，通过深入建模行业相关性、领先-滞后关系和多尺度信息，为金融时序预测提供了新的解决方案。

Abstract: Time series forecasting occurs in a range of financial applications providing
essential decision-making support to investors, regulatory institutions, and
analysts. Unlike multivariate time series from other domains, stock time series
exhibit industry correlation. Exploiting this kind of correlation can improve
forecasting accuracy. However, existing methods based on hypergraphs can only
capture industry correlation relatively superficially. These methods face two
key limitations: they do not fully consider inter-industry lead-lag
interactions, and they do not model multi-scale information within and among
industries. This study proposes the Hermes framework for stock time series
forecasting that aims to improve the exploitation of industry correlation by
eliminating these limitations. The framework integrates moving aggregation and
multi-scale fusion modules in a hypergraph network. Specifically, to more
flexibly capture the lead-lag relationships among industries, Hermes proposes a
hyperedge-based moving aggregation module. This module incorporates a sliding
window and utilizes dynamic temporal aggregation operations to consider
lead-lag dependencies among industries. Additionally, to effectively model
multi-scale information, Hermes employs cross-scale, edge-to-edge message
passing to integrate information from different scales while maintaining the
consistency of each scale. Experimental results on multiple real-world stock
datasets show that Hermes outperforms existing state-of-the-art methods in both
efficiency and accuracy.

</details>


### [706] [Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.23671)
*Jingqi Xu,Guibin Chen,Jingxi Lu,Yuzhang Lin*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的多变量时间序列预测模型DIMIGNN，通过多样性感知的邻居选择和动态多尺度融合机制提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的方法在多变量时间序列预测中忽略了邻居间信息的多样性，且仅依赖单一时间尺度进行预测，可能导致冗余信息聚合和预测不准确。

Method: 设计了多样性感知邻居选择机制（DNSM）以平衡邻居间的相似性与多样性，并引入动态多尺度融合模块（DMFM）来动态整合不同时间尺度的预测结果。

Result: 在多个真实世界数据集上的实验表明，DIMIGNN consistently 优于现有的先进方法。

Conclusion: DIMIGNN通过有效建模变量间依赖关系并融合多尺度信息，显著提升了多变量时间序列预测的准确性。

Abstract: Recently, numerous deep models have been proposed to enhance the performance
of multivariate time series (MTS) forecasting. Among them, Graph Neural
Networks (GNNs)-based methods have shown great potential due to their
capability to explicitly model inter-variable dependencies. However, these
methods often overlook the diversity of information among neighbors, which may
lead to redundant information aggregation. In addition, their final prediction
typically relies solely on the representation from a single temporal scale. To
tackle these issues, we propose a Graph Neural Networks (GNNs) with
Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN).
DIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to
ensure that each variable shares high informational similarity with its
neighbors while maintaining diversity among neighbors themselves. Furthermore,
a Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust
the contributions of prediction results from different temporal scales to the
final forecasting result. Extensive experiments on real-world datasets
demonstrate that DIMIGNN consistently outperforms prior methods.

</details>


### [707] [Towards a Comprehensive Scaling Law of Mixture-of-Experts](https://arxiv.org/abs/2509.23678)
*Guoliang Zhao,Yuhan Fu,Shuaipeng Li,Xingwu Sun,Ruobing Xie,An Wang,Weidong Han,Zhen Yang,Weixuan Sun,Yudong Zhang,Cheng-zhong Xu,Di Wang,Jie Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种针对Mixture-of-Experts（MoE）模型的细粒度联合扩展定律，通过446次受控实验识别出影响性能的五个关键因素，并推导出最优配置，为未来MoE模型的设计与训练提供精准指导。


<details>
  <summary>Details</summary>
Motivation: 现有的密集模型扩展规律不适用于MoE模型，因其存在多重影响因素、复杂的耦合关系以及非单调性能影响，亟需针对MoE模型建立专用的扩展规律。

Method: 系统性分解MoE设置，识别出数据大小、总模型大小、激活模型大小、活跃专家数量和共享专家比例五个关键因素，设计446次受控实验以刻画其边际效应，并构建综合的联合扩展定律。

Result: 提出了一个全面精确的MoE联合扩展定律，得出了G、S和Na/N的理论最优与效率感知最优配置，发现G和S的最优设置与模型结构和数据大小无关，且随着N增大，Na/N的最优激活比例变得更稀疏。

Conclusion: 所提出的MoE扩展定律能有效指导未来MoE模型的设计与训练，在参数高效扩展和成本效益部署方面具有重要应用价值。

Abstract: Mixture-of-Experts (MoE) models have become the consensus approach for
enabling parameter-efficient scaling and cost-effective deployment in large
language models. However, existing scaling laws for dense models are
inapplicable to MoE models, which stems from three critical challenges: the
multiplicity of influencing factors, their intricate coupling relationships and
the non-monotonic nature of their performance impacts. They collectively
necessitate a fine-grained investigation into MoE-specific scaling laws. In
this work, we perform a systematic decomposition of MoE settings, identifying
five key factors that influence model performance from both size and structural
perspectives (data size ($D$), total model size ($N$), activated model size
($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).
Specifically, we design $446$ controlled experiments to characterize their
marginal effects, ultimately constructing a comprehensive and precise joint MoE
scaling law that considers all essential factors. Furthermore, we derive the
theoretically optimal and practically efficiency-aware optimal configurations
for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that
the optimal settings for $G$ and $S$ are independent of both the model
architecture and data size. With the scaling of $N$, the optimal activation
parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could
function as an accurate and insightful guidance to facilitate future MoE model
design and training.

</details>


### [708] [Discrete Variational Autoencoding via Policy Search](https://arxiv.org/abs/2509.24716)
*Michael Drolet,Firas Al-Hafez,Aditya Bhatt,Jan Peters,Oleg Arenz*

Main category: cs.LG

TL;DR: 提出了一种基于自然梯度和非参数编码器的离散VAE训练框架，无需重参数化，结合自适应步长和Transformer编码器，在ImageNet等高维任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离散VAE因缺乏精确可微参数化而依赖近似梯度方法，限制了其在高维数据（如图像重建）上的性能和应用。

Method: 利用非参数编码器的自然梯度来更新参数化编码器，避免使用Gumbel-Softmax或REINFORCE等近似梯度方法，并结合自动步长调整和基于Transformer的编码器。

Result: 在ImageNet 256上实现了比现有离散VAE方法更好的重建效果，FID分数提升20%，且能有效处理高维数据。

Conclusion: 该方法为离散VAE提供了一种更高效、可扩展的训练方式，显著提升了在复杂数据集上的表现，推动了离散瓶颈模型在多模态建模中的应用。

Abstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit
efficiency and can be modeled with autoregressive discrete distributions,
enabling parameter-efficient multimodal search with transformers. However,
discrete random variables do not allow for exact differentiable
parameterization; therefore, discrete VAEs typically rely on approximations,
such as Gumbel-Softmax reparameterization or straight-through gradient
estimates, or employ high-variance gradient-free methods such as REINFORCE that
have had limited success on high-dimensional tasks such as image
reconstruction. Inspired by popular techniques in policy search, we propose a
training framework for discrete VAEs that leverages the natural gradient of a
non-parametric encoder to update the parametric encoder without requiring
reparameterization. Our method, combined with automatic step size adaptation
and a transformer-based encoder, scales to challenging datasets such as
ImageNet and outperforms both approximate reparameterization methods and
quantization-based discrete autoencoders in reconstructing high-dimensional
data from compact latent spaces, achieving a 20% improvement on FID Score for
ImageNet 256.

</details>


### [709] [Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning](https://arxiv.org/abs/2509.23683)
*Danni Yang,Zhikang Chen,Sen Cui,Mengyue Yang,Ding Li,Abudukelimu Wuerkaixi,Haoxuan Li,Jinke Ren,Mingming Gong*

Main category: cs.LG

TL;DR: 提出了一种去中心化的动态合作框架（DCFCL），通过动态联盟和选择性合作缓解联邦持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦持续学习方法在处理跨客户端数据分布变化和任务演化时，容易因知识干扰导致性能下降，尤其在异构场景下表现不佳。

Method: 采用基于联盟亲和博弈的动态合作机制，结合梯度一致性与模型相似性评估客户收益，并设计合并阻塞算法和动态合作演化算法实现合作平衡。

Result: 实验表明该方法在多个基准上优于现有基线方法，能有效提升个性化模型性能。

Conclusion: 所提出的DCFCL框架通过动态、选择性合作显著改善了联邦持续学习中的知识保留与迁移效果。

Abstract: Federated continual learning (FCL) has garnered increasing attention for its
ability to support distributed computation in environments with evolving data
distributions. However, the emergence of new tasks introduces both temporal and
cross-client shifts, making catastrophic forgetting a critical challenge. Most
existing works aggregate knowledge from clients into a global model, which may
not enhance client performance since irrelevant knowledge could introduce
interference, especially in heterogeneous scenarios. Additionally, directly
applying decentralized approaches to FCL suffers from ineffective group
formation caused by task changes. To address these challenges, we propose a
decentralized dynamic cooperation framework for FCL, where clients establish
dynamic cooperative learning coalitions to balance the acquisition of new
knowledge and the retention of prior learning, thereby obtaining personalized
models. To maximize model performance, each client engages in selective
cooperation, dynamically allying with others who offer meaningful performance
gains. This results in non-overlapping, variable coalitions at each stage of
the task. Moreover, we use coalitional affinity game to simulate coalition
relationships between clients. By assessing both client gradient coherence and
model similarity, we quantify the client benefits derived from cooperation. We
also propose a merge-blocking algorithm and a dynamic cooperative evolution
algorithm to achieve cooperative and dynamic equilibrium. Comprehensive
experiments demonstrate the superiority of our method compared to various
baselines. Code is available at: https://github.com/ydn3229/DCFCL.

</details>


### [710] [Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs](https://arxiv.org/abs/2509.23684)
*Tanya Chowdhury,Atharva Nijasure,Yair Zick,James Allan*

Main category: cs.LG

TL;DR: 提出基于合作博弈论的机械可解释性框架，通过识别神经元之间的协同作用来揭示大语言模型中更高级的结构。


<details>
  <summary>Details</summary>
Motivation: 现有的研究对微调后大模型中MLP层的特征表示形式不清楚，且传统方法难以发现神经元间的协同效应。

Method: 引入基于hedonic博弈的游戏理论框架，使用top-responsive utilities和PAC-Top-Cover算法提取具有非加性效应的稳定神经元联盟，并追踪其跨层动态变化。

Result: 在LLaMA、Mistral和Pythia重排序器上验证，发现相比聚类基线具有更高协同性的神经元群组，揭示了跨任务和模型的一致模式。

Conclusion: 该方法揭示了神经元如何协作编码特征，提供了超越独立神经元分析的功能性、可解释且跨领域可预测的计算单元。

Abstract: Fine-tuned Large Language Models (LLMs) encode rich task-specific features,
but the form of these representations, especially within MLP layers, remains
unclear. Empirical inspection of LoRA updates shows that new features
concentrate in mid-layer MLPs, yet the scale of these layers obscures
meaningful structure. Prior probing suggests that statistical priors may
strengthen, split, or vanish across depth, motivating the need to study how
neurons work together rather than in isolation.
  We introduce a mechanistic interpretability framework based on coalitional
game theory, where neurons mimic agents in a hedonic game whose preferences
capture their synergistic contributions to layer-local computations. Using
top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable
coalitions of neurons: groups whose joint ablation has non-additive effects. We
then track their transitions across layers as persistence, splitting, merging,
or disappearance.
  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR
tasks, our method finds coalitions with consistently higher synergy than
clustering baselines. By revealing how neurons cooperate to encode features,
hedonic coalitions uncover higher-order structure beyond disentanglement and
yield computational units that are functionally important, interpretable, and
predictive across domains.

</details>


### [711] [FedDAPL: Toward Client-Private Generalization in Federated Learning](https://arxiv.org/abs/2509.23688)
*Soroosh Safari Loaliyan,Jose-Luis Ambite,Paul M. Thompson,Neda Jahanshad,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 提出一种结合领域对抗神经网络（DANN）与联邦学习的框架，通过引入近端正则化方法解决其在跨站点脑龄预测中的收敛问题，在保护数据隐私的同时显著提升模型跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽保护医疗数据隐私，但设备差异导致的域偏移阻碍模型泛化；现有域适应方法需共享数据，违背隐私要求；领域泛化方法通常仍依赖集中式多站点数据，无法满足联邦学习的隐私保障。

Method: 将领域对抗神经网络（DANN）集成到联邦学习流程中，并提出一种近端正则化方法以稳定客户端间的对抗训练过程，实现无需共享原始数据的跨站点不变表示学习。

Result: 在OpenBHB脑MRI数据集上进行实验，使用15个站点训练并在19个未见站点测试，结果显示该方法在脑龄预测任务中优于FedAvg和ERM，具有更优的跨站点泛化性能且保障数据隐私。

Conclusion: 所提出的联邦DANN结合近端正则化方法有效解决了隐私与域偏移之间的矛盾，为多中心医学影像分析提供了一种兼具隐私保护和强泛化能力的可行方案。

Abstract: Federated Learning (FL) trains models locally at each research center or
clinic and aggregates only model updates, making it a natural fit for medical
imaging, where strict privacy laws forbid raw data sharing. A major obstacle is
scanner-induced domain shift: non-biological variations in hardware or
acquisition protocols can cause models to fail on external sites. Most
harmonization methods correct this shift by directly comparing data across
sites, conflicting with FL's privacy constraints. Domain Generalization (DG)
offers a privacy-friendly alternative - learning site-invariant representations
without sharing raw data - but standard DG pipelines still assume centralized
access to multi-site data, again violating FL's guarantees. This paper meets
these difficulties with a straightforward integration of a Domain-Adversarial
Neural Network (DANN) within the FL process. After demonstrating that a naive
federated DANN fails to converge, we propose a proximal regularization method
that stabilizes adversarial training among clients. Experiments on T1-weighted
3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on
participants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79
y (mean 19+/-13 y; 55 percent male) in validation, show that training on 15
sites and testing on 19 unseen sites yields superior cross-site generalization
over FedAvg and ERM while preserving data privacy.

</details>


### [712] [Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability](https://arxiv.org/abs/2509.23689)
*Ankit Gangwal,Aaryan Ajay Sharma*

Main category: cs.LG

TL;DR: 本研究探讨了模型合并（MM）对对抗样本迁移性的影响，通过8种MM方法、7个数据集和6种攻击方法的广泛实验，发现MM并不能可靠防御迁移攻击，且某些更强的MM方法反而更易受攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管模型合并（MM）在多任务学习中展现出潜力，但其对抗迁移攻击的鲁棒性尚未被充分研究，尤其是基于对抗样本的黑盒攻击场景。

Method: 对8种MM方法、7个数据集和6种攻击方法进行了综合评估与统计分析，共覆盖336种攻击设置，系统研究MM对对抗样本迁移性的影响。

Result: 发现MM无法可靠防御迁移攻击，平均迁移攻击成功率超过95%；更强的MM方法、缓解表征偏移的方法以及权重平均法均不同程度地增加了对迁移攻击的脆弱性。

Conclusion: MM并不提供可靠的对抗鲁棒性，反而可能增加系统对迁移攻击的敏感性，研究揭示了三种关键现象并提出了潜在解决方案，为设计更安全的MM系统提供了重要指导。

Abstract: Model Merging (MM) has emerged as a promising alternative to multi-task
learning, where multiple fine-tuned models are combined, without access to
tasks' training data, into a single model that maintains performance across
tasks. Recent works have explored the impact of MM on adversarial attacks,
particularly backdoor attacks. However, none of them have sufficiently explored
its impact on transfer attacks using adversarial examples, i.e., a black-box
adversarial attack where examples generated for a surrogate model successfully
mislead a target model.
  In this work, we study the effect of MM on the transferability of adversarial
examples. We perform comprehensive evaluations and statistical analysis
consisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336
distinct attack settings. Through it, we first challenge the prevailing notion
of MM conferring free adversarial robustness, and show MM cannot reliably
defend against transfer attacks, with over 95% relative transfer attack success
rate. Moreover, we reveal 3 key insights for machine-learning practitioners
regarding MM and transferability for a robust system design: (1) stronger MM
methods increase vulnerability to transfer attacks; (2) mitigating
representation bias increases vulnerability to transfer attacks; and (3) weight
averaging, despite being the weakest MM method, is the most vulnerable MM
method to transfer attacks. Finally, we analyze the underlying reasons for this
increased vulnerability, and provide potential solutions to the problem. Our
findings offer critical insights for designing more secure systems employing
MM.

</details>


### [713] [Estimating Time Series Foundation Model Transferability via In-Context Learning](https://arxiv.org/abs/2509.23695)
*Qingren Yao,Ming Jin,Chengqi Zhang,Chao-Han Huck Yang,Jun Qi,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出了TimeTic，一个通过将模型选择转化为上下文学习问题来估计时间序列基础模型（TSFM）可迁移性的框架，利用表格基础模型和新的模型特征化方法，在多个数据集和任务上实现了对下游微调性能的有效预测。


<details>
  <summary>Details</summary>
Motivation: 随着TSFM数量的增加，如何高效识别最适合特定下游任务微调的模型成为一个挑战，尤其是在公开数据有限的情况下。

Method: TimeTic利用源数据集上的观测结果，构建包含数据集元特征、模型特性和微调性能的表格结构，并采用基于熵演化的新型模型表征，使用表格基础模型作为上下文学习器进行迁移性估计。

Result: 在包含10个数据集、10个基础模型和3个预测任务的基准上，TimeTic的估计结果与实际微调性能高度一致，平均秩相关系数达约0.6，相比仅用零样本性能提升30%。

Conclusion: TimeTic为TSFM的迁移性评估提供了一种灵活且高效的方法，能够有效指导下游任务中的模型选择。

Abstract: Time series foundation models (TSFMs) offer strong zero-shot forecasting via
large-scale pre-training, yet fine-tuning remains critical for boosting
performance in domains with limited public data. With the growing number of
TSFMs, efficiently identifying the best model for downstream fine-tuning
becomes increasingly challenging. In this work, we introduce TimeTic, a
transferability estimation framework that recasts model selection as an
in-context-learning problem: given observations on known (source) datasets, it
predicts how a TSFM will perform after fine-tuning on a downstream (target)
dataset. TimeTic flexibly organizes the observed model-data relationships as
contextual information, allowing it to adapt seamlessly to various test-time
scenarios. Leveraging the natural tabular structure formed by dataset
meta-features, model characteristics, and fine-tuned performance, we employ
tabular foundation models to serve as in-context learners. We further introduce
a novel model characterization based on entropy evolution across model layers,
capturing embedding-space distinctions and enabling TimeTic to generalize
across arbitrary model sets. We establish a comprehensive benchmark for
transferability estimation including 10 datasets, 10 foundation models, and 3
forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong
alignment with actual fine-tuned performance for previously unseen datasets,
achieving a mean rank correlation of approximately 0.6 and a 30% improvement
compared to using zero-shot performance as the transferability score.

</details>


### [714] [Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization](https://arxiv.org/abs/2509.23711)
*Ziheng Cheng,Xin Guo,Yufei Zhang*

Main category: cs.LG

TL;DR: 本文研究了连续时间强化学习中的确定性策略梯度方法，提出了CT-DDPG算法，具有更好的稳定性和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 将离散时间强化学习算法扩展到连续时间环境面临稳定性差和收敛慢的问题，尤其是在处理复杂、连续的真实世界应用时。

Method: 基于优势函数的类比推导出连续时间策略梯度公式，并通过鞅刻画建立理论基础，提出CT-DDPG算法。

Result: 实验表明，CT-DDPG在不同时间离散化和噪声水平下，相比现有方法具有更高的稳定性和更快的收敛速度。

Conclusion: CT-DDPG为连续时间强化学习提供了一种有效的确定性策略梯度方法，克服了传统方法对时间离散化的敏感性。

Abstract: The theory of discrete-time reinforcement learning (RL) has advanced rapidly
over the past decades. Although primarily designed for discrete environments,
many real-world RL applications are inherently continuous and complex. A major
challenge in extending discrete-time algorithms to continuous-time settings is
their sensitivity to time discretization, often leading to poor stability and
slow convergence. In this paper, we investigate deterministic policy gradient
methods for continuous-time RL. We derive a continuous-time policy gradient
formula based on an analogue of the advantage function and establish its
martingale characterization. This theoretical foundation leads to our proposed
algorithm, CT-DDPG, which enables stable learning with deterministic policies
in continuous-time environments. Numerical experiments show that the proposed
CT-DDPG algorithm offers improved stability and faster convergence compared to
existing discrete-time and continuous-time methods, across a wide range of
control tasks with varying time discretizations and noise levels.

</details>


### [715] [FraudTransformer: Time-Aware GPT for Transaction Fraud Detection](https://arxiv.org/abs/2509.23712)
*Gholamali Aminian,Andrew Elliott,Tiger Li,Timothy Cheuk Hin Wong,Victor Claude Dehon,Lukasz Szpruch,Carsten Maple,Christopher Read,Martin Brown,Gesine Reinert,Mo Mamouei*

Main category: cs.LG

TL;DR: 提出了一种名为FraudTransformer的序列模型，用于检测现实世界银行流中的支付欺诈，该模型结合了时间编码器和学习的位置编码器，在大规模工业数据集上表现优于传统基线模型。


<details>
  <summary>Details</summary>
Motivation: 需要能够利用事件顺序和不规则时间间隔的模型来有效检测现实世界银行流中的支付欺诈。

Method: 引入FraudTransformer模型，基于GPT风格架构，增加专门的时间编码器（处理绝对时间戳或事件间时间）和学习的位置编码器（保持相对顺序）。

Result: 在包含数千万笔交易和辅助事件的大规模工业数据集上实验表明，FraudTransformer在AUROC和PRAUC指标上均优于逻辑回归、XGBoost、LightGBM等基线模型及缺少时间或位置组件的消融版本。

Conclusion: FraudTransformer通过融合时间与顺序信息，在真实银行交易流的欺诈检测任务中展现出优越性能，验证了建模时间动态和相对位置的重要性。

Abstract: Detecting payment fraud in real-world banking streams requires models that
can exploit both the order of events and the irregular time gaps between them.
We introduce FraudTransformer, a sequence model that augments a vanilla
GPT-style architecture with (i) a dedicated time encoder that embeds either
absolute timestamps or inter-event values, and (ii) a learned positional
encoder that preserves relative order. Experiments on a large industrial
dataset -- tens of millions of transactions and auxiliary events -- show that
FraudTransformer surpasses four strong classical baselines (Logistic
Regression, XGBoost and LightGBM) as well as transformer ablations that omit
either the time or positional component. On the held-out test set it delivers
the highest AUROC and PRAUC.

</details>


### [716] [A Self-Adaptive Frequency Domain Network for Continuous Intraoperative Hypotension Prediction](https://arxiv.org/abs/2509.23720)
*Xian Zeng,Tianze Xu,Kai Yang,Jie Sun,Youran Wang,Jun Xu,Mucheng Ren*

Main category: cs.LG

TL;DR: 提出了一种新的自适应频域网络（SAFDNet），用于术中低血压的早期预警，结合频域和时域特征，具有高鲁棒性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于人工智能的术中低血压预测模型在融合时频信息、捕捉长短时间依赖关系以及抗噪能力方面存在局限性。

Method: 设计了自适应频谱模块，利用傅里叶分析提取频域特征并采用自适应阈值去噪；引入交互注意力模块以捕获数据中的长短时依赖关系。

Result: 在两个大规模真实世界数据集上进行内外部验证，SAFDNet在术中低血压早期预警中最高达到97.3%的AUROC，优于当前先进模型，且对噪声不敏感。

Conclusion: SAFDNet在术中低血压早期预测中表现出优越性能和强健性，具备良好的临床应用潜力。

Abstract: Intraoperative hypotension (IOH) is strongly associated with postoperative
complications, including postoperative delirium and increased mortality, making
its early prediction crucial in perioperative care. While several artificial
intelligence-based models have been developed to provide IOH warnings, existing
methods face limitations in incorporating both time and frequency domain
information, capturing short- and long-term dependencies, and handling noise
sensitivity in biosignal data. To address these challenges, we propose a novel
Self-Adaptive Frequency Domain Network (SAFDNet). Specifically, SAFDNet
integrates an adaptive spectral block, which leverages Fourier analysis to
extract frequency-domain features and employs self-adaptive thresholding to
mitigate noise. Additionally, an interactive attention block is introduced to
capture both long-term and short-term dependencies in the data. Extensive
internal and external validations on two large-scale real-world datasets
demonstrate that SAFDNet achieves up to 97.3\% AUROC in IOH early warning,
outperforming state-of-the-art models. Furthermore, SAFDNet exhibits robust
predictive performance and low sensitivity to noise, making it well-suited for
practical clinical applications.

</details>


### [717] [Time-Shifted Token Scheduling for Symbolic Music Generation](https://arxiv.org/abs/2509.23749)
*Ting-Kang Wang,Chih-Pin Tan,Yi-Hsuan Yang*

Main category: cs.LG

TL;DR: 提出一种基于延迟的调度机制（DP），在保持高效性的同时建模符号音乐生成中令牌内的依赖关系，提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决符号音乐生成中细粒度标记化与紧凑标记化之间的效率与质量权衡问题。

Method: 采用延迟-based调度机制（DP），在解码过程中展开类复合标记，实现对令牌内依赖的自回归建模，且不增加额外参数。

Result: 在符号管弦乐MIDI数据集上的实验表明，该方法在所有指标上均优于标准复合标记化，并缩小了与细粒度标记化的差距。

Conclusion: DP是一种轻量级、可无缝集成到现有表示中的有效策略，兼顾了生成效率与质量。

Abstract: Symbolic music generation faces a fundamental trade-off between efficiency
and quality. Fine-grained tokenizations achieve strong coherence but incur long
sequences and high complexity, while compact tokenizations improve efficiency
at the expense of intra-token dependencies. To address this, we adapt a
delay-based scheduling mechanism (DP) that expands compound-like tokens across
decoding steps, enabling autoregressive modeling of intra-token dependencies
while preserving efficiency. Notably, DP is a lightweight strategy that
introduces no additional parameters and can be seamlessly integrated into
existing representations. Experiments on symbolic orchestral MIDI datasets show
that our method improves all metrics over standard compound tokenizations and
narrows the gap to fine-grained tokenizations.

</details>


### [718] [An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms](https://arxiv.org/abs/2509.23750)
*Li Wang,Sudun,Xingjian Zhang,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: 本文探讨了批归一化（BN）在深度强化学习（DRL）中的应用，提出了一种模式感知的BN方法（MA-BN），通过实证研究验证其在提升训练稳定性、加速收敛和增强性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管BN在深度学习中成功广泛应用，但在DRL中因数据非独立同分布及动态分布变化而受限，本文旨在探索BN在DRL中仍具有的潜在优势。

Method: 对离策略Actor-Critic算法中BN的使用进行了系统的实证研究，分析不同训练与评估模式的影响，识别导致不稳定或发散的失败模式，并提出MA-BN方法及实用建议。

Result: 实验表明，在RL设置下，MA-BN能够加速并稳定训练过程，扩大有效学习率范围，增强探索能力，并降低优化难度。

Conclusion: 适当应用BN可在DRL中带来显著优势，MA-BN为在DRL流程中鲁棒集成BN提供了可行方案。

Abstract: Batch Normalization (BN) has played a pivotal role in the success of deep
learning by improving training stability, mitigating overfitting, and enabling
more effective optimization. However, its adoption in deep reinforcement
learning (DRL) has been limited due to the inherent non-i.i.d. nature of data
and the dynamically shifting distributions induced by the agent's learning
process. In this paper, we argue that, despite these challenges, BN retains
unique advantages in DRL settings, particularly through its stochasticity and
its ability to ease training. When applied appropriately, BN can adapt to
evolving data distributions and enhance both convergence speed and final
performance. To this end, we conduct a comprehensive empirical study on the use
of BN in off-policy actor-critic algorithms, systematically analyzing how
different training and evaluation modes impact performance. We further identify
failure modes that lead to instability or divergence, analyze their underlying
causes, and propose the Mode-Aware Batch Normalization (MA-BN) method with
practical actionable recommendations for robust BN integration in DRL
pipelines. We also empirically validate that, in RL settings, MA-BN accelerates
and stabilizes training, broadens the effective learning rate range, enhances
exploration, and reduces overall optimization difficulty. Our code is available
at: https://github.com/monster476/ma-bn.git.

</details>


### [719] [Anchored Supervised Fine-Tuning](https://arxiv.org/abs/2509.23753)
*He Zhu,Junyou Su,Peng Lai,Ren Ma,Wenjia Zhang,Linyi Yang,Guanhua Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的后训练方法ASFT，通过在动态微调（DFT）中引入轻量级KL正则化来解决其训练不稳定性问题，在数学推理、医学知识对齐和代码生成任务上优于SFT和DFT。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型后训练方法在监督微调（SFT）和强化学习（RL）之间存在效率与泛化能力的权衡；DFT虽尝试折中但存在训练不稳定问题，需更稳定且高效的方法。

Method: 基于奖励加权回归（RWR）框架分析DFT，发现其缺乏分布锚定导致漂移问题，进而提出ASFT，在DFT基础上加入KL正则化以实现分布锚定，提升稳定性。

Result: ASFT在多个任务（如数学推理、医学知识、代码生成）上显著优于SFT和DFT，且计算开销极小，同时理论分析表明其具有更紧致的RL界和更好的训练稳定性。

Conclusion: 通过理论驱动的设计，ASFT在保持DFT优势的同时解决了其根本缺陷，验证了RWR框架对理解与改进后训练方法的有效性。

Abstract: Post-training of large language models involves a fundamental trade-off
between supervised fine-tuning (SFT), which efficiently mimics demonstrations
but tends to memorize, and reinforcement learning (RL), which achieves better
generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently
emerged as a promising middle ground, reweighting SFT objectives with token
probabilities and achieving improvements in certain reasoning domains, though
it exhibits instability in other tasks. We provide a analysis of DFT through
the reward-weighted regression (RWR) framework, revealing that it corresponds
to a specific auxiliary distribution choice that yields provably tighter RL
bounds than standard SFT. However, our analysis also uncovers a critical
limitation: this construction lacks distributional anchoring, leading to
progressive drift that undermines training stability. To address this, we
propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's
reweighting with lightweight KL regularization to preserve tightness while
ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT
across mathematical reasoning, medical knowledge grounding, and code
generation, achieving substantial improvements with minimal computational
overhead. Our RWR framework provides a systematic lens for understanding
post-training methods and demonstrates that principled theoretical analysis
leads to both stronger guarantees and practical gains.

</details>


### [720] [SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values](https://arxiv.org/abs/2509.23756)
*Tomer D. Meirman,Bracha Shapira,Noa Dagan,Lior S. Rokach*

Main category: cs.LG

TL;DR: SHAPoint是一个新颖的、与任务无关的框架，将梯度提升树的预测准确性与基于点的风险评分的可解释性相结合，适用于分类、回归和生存分析任务。


<details>
  <summary>Details</summary>
Motivation: 传统风险评分方法依赖手动预处理和简化假设，限制了灵活性和预测能力，需要一种更灵活、自动化且高效的方法。

Method: 提出SHAPoint框架，结合梯度提升树的优势，自动生成点积风险评分，支持多种任务类型，并具备对缺失数据的原生处理和单调性约束支持。

Result: SHAPoint在保持与最先进方法相当的预测性能的同时，生成简洁可解释的评分，运行时间显著缩短，灵活性更高，预处理依赖更少。

Conclusion: SHAPoint是一种高效、灵活且可解释的临床决策支持工具，适用于透明且可扩展的风险分层。

Abstract: Interpretable risk scores play a vital role in clinical decision support, yet
traditional methods for deriving such scores often rely on manual
preprocessing, task-specific modeling, and simplified assumptions that limit
their flexibility and predictive power. We present SHAPoint, a novel,
task-agnostic framework that integrates the predictive accuracy of gradient
boosted trees with the interpretability of point-based risk scores. SHAPoint
supports classification, regression, and survival tasks, while also inheriting
valuable properties from tree-based models, such as native handling of missing
data and support for monotonic constraints. Compared to existing frameworks,
SHAPoint offers superior flexibility, reduced reliance on manual preprocessing,
and faster runtime performance. Empirical results show that SHAPoint produces
compact and interpretable scores with predictive performance comparable to
state-of-the-art methods, but at a fraction of the runtime, making it a
powerful tool for transparent and scalable risk stratification.

</details>


### [721] [Knowledge Homophily in Large Language Models](https://arxiv.org/abs/2509.23773)
*Utkarsh Sahu,Zhisheng Qi,Mahantesh Halappanavar,Nedim Lipka,Ryan A. Rossi,Franck Dernoncourt,Yu Zhang,Yao Ma,Yu Wang*

Main category: cs.LG

TL;DR: 该论文研究了大语言模型（LLM）中知识的结构组织，发现存在类似认知神经科学中的“知识同质性”现象，即相关知识在模型中更可能被共同掌握。作者通过构建知识图谱并利用图神经网络（GNN）预测实体的知识掌握程度，提升了知识注入和多跳问答的效率。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型中知识的结构化组织方式，受认知神经科学中语义聚类和启动效应启发，试图揭示模型是否也存在类似的知识关联模式。

Method: 将LLM的知识映射为图结构，通过三元组和实体层面的知识检查构建知识图，并分析邻近实体之间的知识掌握关系；基于发现的同质性，使用图神经网络（GNN）回归模型预测实体的知识掌握得分。

Result: 发现LLM对图中邻近实体的知识掌握程度具有高度相关性（知识同质性）；提出的GNN模型能有效预测实体知识性，用于优先检查知识薄弱的三元组，在相同标注预算下提升知识覆盖效率，并改善多跳问答中的路径检索性能。

Conclusion: 大语言模型中的知识呈现结构化的同质性分布，利用这种特性可通过图神经网络优化知识获取与注入过程，提升知识密集型任务的效率与性能。

Abstract: Large Language Models (LLMs) have been increasingly studied as neural
knowledge bases for supporting knowledge-intensive applications such as
question answering and fact checking. However, the structural organization of
their knowledge remains unexplored. Inspired by cognitive neuroscience
findings, such as semantic clustering and priming, where knowing one fact
increases the likelihood of recalling related facts, we investigate an
analogous knowledge homophily pattern in LLMs. To this end, we map LLM
knowledge into a graph representation through knowledge checking at both the
triplet and entity levels. After that, we analyze the knowledgeability
relationship between an entity and its neighbors, discovering that LLMs tend to
possess a similar level of knowledge about entities positioned closer in the
graph. Motivated by this homophily principle, we propose a Graph Neural Network
(GNN) regression model to estimate entity-level knowledgeability scores for
triplets by leveraging their neighborhood scores. The predicted
knowledgeability enables us to prioritize checking less well-known triplets,
thereby maximizing knowledge coverage under the same labeling budget. This not
only improves the efficiency of active labeling for fine-tuning to inject
knowledge into LLMs but also enhances multi-hop path retrieval in
reasoning-intensive question answering.

</details>


### [722] [Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression](https://arxiv.org/abs/2509.23779)
*Jiarui Jiang,Wei Huang,Miao Zhang,Taiji Suzuki,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文研究了Mamba在线性回归上下文学习（ICL）任务中的训练动态，提出了新的非凸优化分析技术，证明了其以指数速度收敛到ICL解，并揭示了Mamba通过一种在线梯度下降机制学习上下文中的潜在函数，机制上不同于Transformer。


<details>
  <summary>Details</summary>
Motivation: 尽管Mamba在经验上展现出与Transformer相当的上下文学习能力，但其理论理解仍不足，尤其是缺乏对基础任务（如线性回归ICL）的深入分析，限制了对其内在机制的理解。

Method: 通过发展针对Mamba结构的非凸优化与梯度下降的新分析技术，研究其在线性回归ICL任务上的训练动态，并建立收敛性理论和损失界。

Result: 建立了Mamba在ICL任务上的指数收敛速率，推导出可与Transformer相比的损失界，并发现Mamba通过类似在线梯度下降的机制更新上下文中的函数估计。

Conclusion: Mamba不仅在效率上优于Transformer，且在理论上具备强大的上下文学习能力，其通过不同于Transformer的机制实现ICL，为构建高效基础模型提供了新视角。

Abstract: State-space models (SSMs), particularly Mamba, emerge as an efficient
Transformer alternative with linear complexity for long-sequence modeling.
Recent empirical works demonstrate Mamba's in-context learning (ICL)
capabilities competitive with Transformers, a critical capacity for large
foundation models. However, theoretical understanding of Mamba's ICL remains
limited, restricting deeper insights into its underlying mechanisms. Even
fundamental tasks such as linear regression ICL, widely studied as a standard
theoretical benchmark for Transformers, have not been thoroughly analyzed in
the context of Mamba. To address this gap, we study the training dynamics of
Mamba on the linear regression ICL task. By developing novel techniques
tackling non-convex optimization with gradient descent related to Mamba's
structure, we establish an exponential convergence rate to ICL solution, and
derive a loss bound that is comparable to Transformer's. Importantly, our
results reveal that Mamba can perform a variant of \textit{online gradient
descent} to learn the latent function in context. This mechanism is different
from that of Transformer, which is typically understood to achieve ICL through
gradient descent emulation. The theoretical results are verified by
experimental simulation.

</details>


### [723] [Visual CoT Makes VLMs Smarter but More Fragile](https://arxiv.org/abs/2509.23789)
*Chunxue Xu,Yiwei Wang,Yujun Cai,Bryan Hooi,Songze Li*

Main category: cs.LG

TL;DR: 本文首次系统评估了视觉思维链（Visual CoT）在图像噪声下的鲁棒性，发现其虽提升准确性，但对输入扰动更敏感，并提出一种基于Grounding DINO的即插即用方法以增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管Visual CoT在多模态推理中表现优异，但其在图像噪声下的鲁棒性尚未被研究，亟需系统评估并解决其脆弱性问题。

Method: 构建包含12种图像损坏类型和4个VQA数据集的基准，比较使用与不使用Visual CoT的VLMs表现，并分析中间推理组件（编辑图像块）带来的脆弱性；提出将Grounding DINO集成到Visual CoT流程中以提供高置信度局部视觉线索。

Result: 实验表明Visual CoT在干净和噪声图像上均提升准确率，但性能下降更显著；编辑后的图像块是主要脆弱来源；引入Grounding DINO可有效缓解该问题。

Conclusion: Visual CoT存在对输入扰动敏感的缺陷，通过引入高置信度视觉定位模块可有效增强其鲁棒性，为后续工作提供了可通用的改进方案。

Abstract: Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in
Vision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates
explicit visual edits, such as cropping or annotating regions of interest, into
the reasoning process, achieving superior multimodal performance. However, the
robustness of Visual CoT-based VLMs against image-level noise remains
unexplored. In this paper, we present the first systematic evaluation of Visual
CoT robustness under visual perturbations. Our benchmark spans 12 image
corruption types across 4 Visual Question Answering (VQA) datasets, enabling a
comprehensive comparison between VLMs that use Visual CoT, and VLMs that do
not. The results reveal that integrating Visual CoT consistently improves
absolute accuracy regardless of whether the input images are clean or corrupted
by noise; however, it also increases sensitivity to input perturbations,
resulting in sharper performance degradation compared to standard VLMs. Through
extensive analysis, we identify the intermediate reasoning components of Visual
CoT, i.e., the edited image patches , as the primary source of fragility.
Building on this analysis, we propose a plug-and-play robustness enhancement
method that integrates Grounding DINO model into the Visual CoT pipeline,
providing high-confidence local visual cues to stabilize reasoning. Our work
reveals clear fragility patterns in Visual CoT and offers an effective,
architecture-agnostic solution for enhancing visual robustness.

</details>


### [724] [Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement](https://arxiv.org/abs/2509.23799)
*Anyi Wang,Xuansheng Wu,Dong Shu,Yunpu Ma,Ninghao Liu*

Main category: cs.LG

TL;DR: 提出了一种通过稀疏自编码器（SAE-RSV）精炼小数据集上学习到的引导向量的方法，以去噪并增强语义相关特征，显著提升了在有限数据下的大模型控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有引导方法依赖大规模数据集，在小数据场景下效果受限，且易受任务无关噪声影响。

Method: 利用稀疏自编码器（SAE）对引导向量进行语义去噪，并通过语义相似性补充缺失的任务相关特征。

Result: 实验表明SAE-RSV在多种基准上显著优于现有方法，包括监督微调，能有效从小样本数据构建高性能引导向量。

Conclusion: 通过SAE精炼可从有限数据中构建有效的引导向量，提升了小样本场景下LLM引导的可行性与性能。

Abstract: Steering has emerged as a promising approach in controlling large language
models (LLMs) without modifying model parameters. However, most existing
steering methods rely on large-scale datasets to learn clear behavioral
information, which limits their applicability in many real-world scenarios. The
steering vectors extracted from small dataset often contain task-irrelevant
noising features, which degrades their effectiveness. To refine the steering
vectors learned from limited data, we introduce Refinement of Steering Vector
via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise
and augment the steering vectors. In our framework, we first remove
task-irrelevant features according to their semantics provided by SAEs, and
then enrich task-relevant features missing from the small dataset through their
semantic similarity to the identified relevant features. Extensive experiments
demonstrate that the proposed SAE-RSV substantially outperforms all the
baseline methods including supervised fine-tuning. Our findings show that
effective steering vector can be constructed from limited training data by
refining the original steering vector through SAEs.

</details>


### [725] [STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning](https://arxiv.org/abs/2509.23802)
*Yao Luan,Ni Mu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 本文提出了一种新的偏好强化学习方法STAIR，通过时间距离进行对比学习来自动划分任务阶段，并优先在相同阶段内进行比较，从而解决多阶段任务中的阶段错位问题。


<details>
  <summary>Details</summary>
Motivation: 在多阶段任务中，传统的偏好强化学习因跨阶段比较导致反馈信息不足，影响策略学习效果，因此需要一种能够缓解阶段错位问题的方法。

Method: 提出STAIR方法，首先利用对比学习根据时间距离自动学习阶段划分，然后优先在同一阶段内的片段之间进行偏好比较，以提高奖励学习的有效性。

Result: 实验表明，STAIR在多阶段任务中显著优于现有方法，在单阶段任务中也具有竞争力；人类研究表明其划分的阶段符合人类认知。

Conclusion: STAIR能有效缓解多阶段任务中的阶段错位问题，提升偏好强化学习的性能，并具备良好的可扩展性和实用性。

Abstract: Preference-based reinforcement learning (PbRL) bypasses complex reward
engineering by learning rewards directly from human preferences, enabling
better alignment with human intentions. However, its effectiveness in
multi-stage tasks, where agents sequentially perform sub-tasks (e.g.,
navigation, grasping), is limited by stage misalignment: Comparing segments
from mismatched stages, such as movement versus manipulation, results in
uninformative feedback, thus hindering policy learning. In this paper, we
validate the stage misalignment issue through theoretical analysis and
empirical experiments. To address this issue, we propose STage-AlIgned Reward
learning (STAIR), which first learns a stage approximation based on temporal
distance, then prioritizes comparisons within the same stage. Temporal distance
is learned via contrastive learning, which groups temporally close states into
coherent stages, without predefined task knowledge, and adapts dynamically to
policy changes. Extensive experiments demonstrate STAIR's superiority in
multi-stage tasks and competitive performance in single-stage tasks.
Furthermore, human studies show that stages approximated by STAIR are
consistent with human cognition, confirming its effectiveness in mitigating
stage misalignment.

</details>


### [726] [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
*Fanding Huang,Guanbo Huang,Xiao Fan,Yi He,Xiao Liang,Xiao Chen,Qinting Jiang,Faisal Nadeem Khan,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 本文挑战了强化学习中可验证奖励领域普遍存在的探索-利用权衡观点，提出该权衡可能是度量层级的产物而非根本约束。通过在隐藏状态空间中使用有效秩及其导数（ERV和ERA）进行分析，发现探索与利用可解耦，并据此提出VERL方法，首次实现协同增强两者性能，实验显示显著准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究基于token级指标将RLVR中的进展解释为探索与利用的权衡，但这种视角可能掩盖了更深层次的动态机制。作者质疑这一权衡是否为根本限制，还是测量方式带来的假象，因而需要在更具语义意义的隐藏状态空间中重新审视该问题。

Method: 引入Effective Rank (ER) 来量化隐藏状态空间中的探索程度，并提出其一阶和二阶导数——Effective Rank Velocity (ERV) 与 Effective Rank Acceleration (ERA)，用以刻画利用动态。基于ERA设计VERL方法，将其作为稳定的元控制器，通过调节RL优势函数构建双通道激励机制，同时增强探索与利用。

Result: 实验证明，在多个LLM和推理基准上，VERL均带来持续性能提升，在Gaokao 2024数据集上最高实现21.4%的绝对准确率提升。分析表明，探索与利用在隐藏状态层面可以解耦，且二者可被同时增强。

Conclusion: 探索与利用之间的权衡并非不可逾越的瓶颈，而是取决于分析层级的现象；通过在隐藏状态空间中建模其动态关系，VERL实现了两者的协同优化，为RLVR提供了新的设计范式。

Abstract: A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)
interprets recent progress through the lens of an exploration-exploitation
trade-off, a perspective largely shaped by token-level metrics. We re-examine
this perspective, proposing that this perceived trade-off may not be a
fundamental constraint but rather an artifact of the measurement level. To
investigate this, we shift the analysis to the semantically rich hidden-state
space, adopting Effective Rank (ER) to quantify exploration and proposing its
novel first- and second-order derivatives, named Effective Rank Velocity (ERV)
and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our
analysis reveals that at the hidden-state level, exploration and exploitation
could be decoupled (Sec. 4). This finding reveals an opportunity to enhance
both capacities simultaneously. This insight motivates our method,
Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the
principle of synergistic exploration-exploitation enhancement by directly
shaping the RL advantage function. The key innovation is leveraging the
theoretically stable ERA as a predictive meta-controller to create a
synergistic, dual-channel incentive structure. Instead of forcing a trade-off,
VERL prospectively amplifies rewards for exploration to preempt overconfidence
and reinforces exploitative gains to consolidate reasoning. Experiments across
diverse LLMs and reasoning benchmarks show consistent gains, including up to
21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.

</details>


### [727] [Tequila: Trapping-free Ternary Quantization for Large Language Models](https://arxiv.org/abs/2509.23809)
*Hong Huang,Decheng Wu,Rui Cen,Guanghua Yu,Zonghang Li,Kai Liu,Jianchen Zhu,Peng Chen,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Tequila的无捕获量化优化方法，通过将陷入死区的三值权重重新用作动态偏置，解决了大型语言模型在边缘设备上部署时因量化导致的精度下降问题，实现了接近全精度模型性能且推理速度提升3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的三值量化方法由于权重陷入死区导致梯度噪声大、优化困难，造成显著的精度损失，限制了大型语言模型在资源受限设备上的高效部署。

Method: 提出Tequila方法，将被困在死区的权重重新利用为动态偏置，使其在前向传播中提供连续信号，并在反向传播中获得有意义的梯度，从而增强模型容量和优化能力，同时几乎不增加推理开销。

Result: 在五个基准测试中，Tequila优于当前最先进的三值量化方法；在ARC基准上，相比SOTA基线提升了超过4%的准确率，性能接近全精度模型（差距<1%），并实现了3.0倍的推理加速。

Conclusion: Tequila为在资源受限环境中高效部署大型语言模型提供了一个高度实用且高效的解决方案，显著缓解了三值量化中的死区捕获问题。

Abstract: Quantization techniques are essential for the deployment of Large Language
Models (LLMs) on edge devices. However, prevailing methods often rely on
mixed-precision multiplication that lacks efficient hardware support, making it
not feasible. Ternary weight quantization addresses this by constraining
weights to {-1, 0, 1}, replacing expensive multiplications with
hardware-efficient additions. However, such aggressive compression leads to
significant accuracy degradation, even after costly quantization-aware training
with massive data. We identify the core issue as deadzone trapping: a large
number of weights are trapped at the deadzone boundary. This occurs because
these weights receive only noisy, uninformative gradients, preventing stable
escape from the deadzone and severely impeding model capacity and optimization.
To address this issue, we propose Tequila, a trapping-free quantization
optimization method that reactivates deadzone-trapped weights by repurposing
them as dynamic biases. This allows the repurposed weights to provide a
continuous signal in the forward pass and, critically, receive direct,
meaningful gradient signals during backpropagation, thereby enhancing model
capacity and optimization with nearly zero inference overhead. Extensive
evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)
ternary quantization methods across five benchmarks. Specifically, on the ARC
benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly
matching full-precision performance (within <1% gap) with a 3.0x inference
speedup. Consequently, Tequila offers a highly practical and efficient
implementation for the deployment of advanced LLMs in resource-constrained
environments. The code is available at https://github.com/Tencent/AngelSlim.

</details>


### [728] [IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting](https://arxiv.org/abs/2509.23813)
*Beiliang Wu,Peiyuan Liu,Yifan Hu,Luyan Zhang,Ao Hu,Zenglin Xu*

Main category: cs.LG

TL;DR: 提出了一种基于MLP的多变量时间序列预测框架IndexNet，通过引入时间戳嵌入和通道嵌入模块，有效利用索引相关描述信息，提升了模型对复杂周期模式的捕捉能力和变量区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽略了包含丰富上下文语义的索引相关信息（如时间戳和变量索引），限制了模型对长期复杂周期模式的建模和变量间异质性的识别。

Method: 设计了一个名为IndexNet的MLP架构，包含时间戳嵌入（TE）和通道嵌入（CE）模块：TE将时间戳转化为嵌入向量并注入输入序列，增强周期性建模；CE为每个变量分配可学习的身份嵌入，提升变量区分度。

Result: 在12个真实世界数据集上实验表明，IndexNet性能与主流基线相当，且具备良好的通用性和可解释性，插件式实验和可视化分析进一步验证了其有效性。

Conclusion: IndexNet通过融合索引相关的上下文信息，在不增加复杂性的前提下显著提升了多变量时间序列预测的性能与可解释性，为未来研究提供了新的方向。

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in a wide
range of real-world applications, such as weather prediction and traffic flow
forecasting. Although recent advances have significantly improved the modeling
of temporal dynamics and inter-variable dependencies, most existing methods
overlook index-related descriptive information, such as timestamps and variable
indices, which carry rich contextual semantics. To unlock the potential of such
information and take advantage of the lightweight and powerful periodic capture
ability of MLP-based architectures, we propose IndexNet, an MLP-based framework
augmented with an Index Embedding (IE) module. The IE module consists of two
key components: Timestamp Embedding (TE) and Channel Embedding (CE).
Specifically, TE transforms timestamps into embedding vectors and injects them
into the input sequence, thereby improving the model's ability to capture
long-term complex periodic patterns. In parallel, CE assigns each variable a
unique and trainable identity embedding based on its index, allowing the model
to explicitly distinguish between heterogeneous variables and avoid homogenized
predictions when input sequences seem close. Extensive experiments on 12
diverse real-world datasets demonstrate that IndexNet achieves comparable
performance across mainstream baselines, validating the effectiveness of our
temporally and variably aware design. Moreover, plug-and-play experiments and
visualization analyses further reveal that IndexNet exhibits strong generality
and interpretability, two aspects that remain underexplored in current MTSF
research.

</details>


### [729] [Test-time GNN Model Evaluation on Dynamic Graphs](https://arxiv.org/abs/2509.23816)
*Bo Li,Xin Zheng,Ming Jin,Can Wang,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出了一个用于评估动态图神经网络（DGNN）在未见测试图上性能的新框架DyGEval，通过两阶段方法模拟测试时的图分布差异并训练评估器，有效提升了模型在分布偏移下的性能评估准确性。


<details>
  <summary>Details</summary>
Motivation: 由于动态图数据分布随时间变化，已训练好的DGNN在实际部署中对未见无标签测试图的推理性能存在不确定性，因此需要在测试时评估其性能。

Method: 提出DyGEval，采用两阶段框架：第一阶段进行测试时动态图模拟以捕捉训练-测试分布差异；第二阶段开发并训练评估器来估计DGNN在测试图上的性能。

Result: 实验表明DyGEval能有效评估多种DGNN骨干网络在不同动态图上的性能，尤其在存在分布偏移的情况下表现优异。

Conclusion: DyGEval为动态图神经网络在测试时的性能评估提供了一个有效的解决方案，有助于提升模型在真实场景中的可靠性与适用性。

Abstract: Dynamic graph neural networks (DGNNs) have emerged as a leading paradigm for
learning from dynamic graphs, which are commonly used to model real-world
systems and applications. However, due to the evolving nature of dynamic graph
data distributions over time, well-trained DGNNs often face significant
performance uncertainty when inferring on unseen and unlabeled test graphs in
practical deployment. In this case, evaluating the performance of deployed
DGNNs at test time is crucial to determine whether a well-trained DGNN is
suited for inference on an unseen dynamic test graph. In this work, we
introduce a new research problem: DGNN model evaluation, which aims to assess
the performance of a specific DGNN model trained on observed dynamic graphs by
estimating its performance on unseen dynamic graphs during test time.
Specifically, we propose a Dynamic Graph neural network Evaluator, dubbed
DyGEval, to address this new problem. The proposed DyGEval involves a two-stage
framework: (1) test-time dynamic graph simulation, which captures the
training-test distributional differences as supervision signals and trains an
evaluator; and (2) DyGEval development and training, which accurately estimates
the performance of the well-trained DGNN model on the test-time dynamic graphs.
Extensive experiments demonstrate that the proposed DyGEval serves as an
effective evaluator for assessing various DGNN backbones across different
dynamic graphs under distribution shifts.

</details>


### [730] [Space Group Conditional Flow Matching](https://arxiv.org/abs/2509.23822)
*Omri Puny,Yaron Lipman,Benjamin Kurt Miller*

Main category: cs.LG

TL;DR: 提出了一种基于空间群和Wyckoff位置条件的生成框架Space Group Conditional Flow Matching，显著提升了高对称稳定晶体结构的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有晶体结构生成模型常忽略晶体对称性约束，导致生成晶体对称性偏低、不真实。

Method: 通过在给定空间群和Wyckoff位置条件下构建对称噪声分布和群条件等变向量场，限制原子在其初始Wyckoff位置上的运动，并采用高效群平均方法降低对称化计算开销。

Result: 在晶体结构预测和从头生成任务上达到最优性能，并通过消融实验验证了关键设计的有效性。

Conclusion: 该方法有效融合晶体对称性先验，显著提升生成晶体的对称性和稳定性，推动了晶体材料的高效生成设计。

Abstract: Inorganic crystals are periodic, highly-symmetric arrangements of atoms in
three-dimensional space. Their structures are constrained by the symmetry
operations of a crystallographic \emph{space group} and restricted to lie in
specific affine subspaces known as \emph{Wyckoff positions}. The frequency an
atom appears in the crystal and its rough positioning are determined by its
Wyckoff position. Most generative models that predict atomic coordinates
overlook these symmetry constraints, leading to unrealistically high
populations of proposed crystals exhibiting limited symmetry. We introduce
Space Group Conditional Flow Matching, a novel generative framework that
samples significantly closer to the target population of highly-symmetric,
stable crystals. We achieve this by conditioning the entire generation process
on a given space group and set of Wyckoff positions; specifically, we define a
conditionally symmetric noise base distribution and a group-conditioned,
equivariant, parametric vector field that restricts the motion of atoms to
their initial Wyckoff position. Our form of group-conditioned equivariance is
achieved using an efficient reformulation of \emph{group averaging} tailored
for symmetric crystals. Importantly, it reduces the computational overhead of
symmetrization to a negligible level. We achieve state of the art results on
crystal structure prediction and de novo generation benchmarks. We also perform
relevant ablations.

</details>


### [731] [Electric Currents for Discrete Data Generation](https://arxiv.org/abs/2509.23825)
*Alexander Kolesov,Stepan Manukhov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出了一种基于电路电流理论的离散数据生成方法ECD²G，通过神经网络学习概率流并实现源分布到目标分布的传输。


<details>
  <summary>Details</summary>
Motivation: 现有离散数据生成方法缺乏物理可解释性与理论保证，因此需要一种基于物理原理且能确保分布迁移的新方法。

Method: 将源分布样本视为电路输入节点，目标分布样本视为输出节点，利用神经网络学习电路中的电流以表示概率流，并沿电路路径传输样本。

Result: 该方法在概念验证实验中展示了有效的数据分布转移能力，且理论上保证了分布间的传输。

Conclusion: ECD²G为离散数据生成提供了一个有理论支撑、物理意义明确的新框架，具有潜在广泛应用价值。

Abstract: We propose $\textbf{E}$lectric $\textbf{C}$urrent $\textbf{D}$iscrete
$\textbf{D}$ata $\textbf{G}$eneration (ECD$^{2}$G), a pioneering method for
data generation in discrete settings that is grounded in electrical engineering
theory. Our approach draws an analogy between electric current flow in a
circuit and the transfer of probability mass between data distributions. We
interpret samples from the source distribution as current input nodes of a
circuit and samples from the target distribution as current output nodes. A
neural network is then used to learn the electric currents to represent the
probability flow in the circuit. To map the source distribution to the target,
we sample from the source and transport these samples along the circuit
pathways according to the learned currents. This process provably guarantees
transfer between data distributions. We present proof-of-concept experiments to
illustrate our ECD$^{2}$G method.

</details>


### [732] [Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know](https://arxiv.org/abs/2509.23830)
*Albus Yizhuo Li*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯混合专家（MoE）路由框架，通过在权重空间、logit空间和选择空间引入不确定性，提升大语言模型的校准性和分布外检测能力。


<details>
  <summary>Details</summary>
Motivation: 标准的确定性MoE路由机制存在脆弱性，导致模型校准差和过度自信，缺乏对未知的识别能力。

Method: 构建贝叶斯MoE路由框架，在路由过程的不同阶段（权重空间、logit空间、选择空间）建模路由决策的概率分布，引入结构化不确定性。

Result: 在30亿参数MoE模型上的实验表明，该方法显著提升了路由稳定性、分布内校准性和分布外检测性能。

Conclusion: 通过改进MoE路由机制中的不确定性建模，可有效增强大语言模型的可靠性和自知之明，为构建更鲁棒、自我感知的模型提供了可行路径。

Abstract: The Mixture-of-Experts (MoE) architecture has enabled the creation of massive
yet efficient Large Language Models (LLMs). However, the standard deterministic
routing mechanism presents a significant limitation: its inherent brittleness
is a key contributor to model miscalibration and overconfidence, resulting in
systems that often do not know what they don't know.
  This thesis confronts this challenge by proposing a structured
\textbf{Bayesian MoE routing framework}. Instead of forcing a single,
deterministic expert selection, our approach models a probability distribution
over the routing decision itself. We systematically investigate three families
of methods that introduce this principled uncertainty at different stages of
the routing pipeline: in the \textbf{weight-space}, the \textbf{logit-space},
and the final \textbf{selection-space}.
  Through a series of controlled experiments on a 3-billion parameter MoE
model, we demonstrate that this framework significantly improves routing
stability, in-distribution calibration, and out-of-distribution (OoD)
detection. The results show that by targeting this core architectural
component, we can create a more reliable internal uncertainty signal. This work
provides a practical and computationally tractable pathway towards building
more robust and self-aware LLMs, taking a crucial step towards making them know
what they don't know.

</details>


### [733] [Adversarial Diffusion for Robust Reinforcement Learning](https://arxiv.org/abs/2509.23846)
*Daniele Foffano,Alessio Russo,Alexandre Proutiere*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的鲁棒强化学习方法AD-RRL，通过引导扩散过程生成最坏情况下的轨迹来优化累积回报的CVaR，从而提高策略对环境动态不确定性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强化学习中模型误差和不确定性导致的鲁棒性问题是一个核心挑战，现有基于模型的方法容易产生逐步累积的预测误差。

Method: 利用扩散模型能够一次性生成完整轨迹并支持条件采样的特性，结合CVaR优化与鲁棒强化学习的关系，提出AD-RRL方法，在训练中通过对抗性条件采样生成最坏情况轨迹以提升鲁棒性。

Result: 在多个标准基准上的实验结果表明，AD-RRL在鲁棒性和性能方面均优于现有的鲁棒强化学习方法。

Conclusion: AD-RRL有效提升了强化学习策略在环境动态不确定性下的鲁棒性，展示了扩散模型在鲁棒决策任务中的潜力。

Abstract: Robustness to modeling errors and uncertainties remains a central challenge
in reinforcement learning (RL). In this work, we address this challenge by
leveraging diffusion models to train robust RL policies. Diffusion models have
recently gained popularity in model-based RL due to their ability to generate
full trajectories "all at once", mitigating the compounding errors typical of
step-by-step transition models. Moreover, they can be conditioned to sample
from specific distributions, making them highly flexible. We leverage
conditional sampling to learn policies that are robust to uncertainty in
environment dynamics. Building on the established connection between
Conditional Value at Risk (CVaR) optimization and robust RL, we introduce
Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides
the diffusion process to generate worst-case trajectories during training,
effectively optimizing the CVaR of the cumulative return. Empirical results
across standard benchmarks show that AD-RRL achieves superior robustness and
performance compared to existing robust RL methods.

</details>


### [734] [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://arxiv.org/abs/2509.23866)
*Pengxiang Li,Zechen Hu,Zirui Shang,Jingrong Wu,Yang Liu,Hui Liu,Zhi Gao,Chenrui Shi,Bofei Zhang,Zihao Zhang,Xiaochuan Shi,Zedong YU,Yuwei Wu,Xinxiao Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.LG

TL;DR: 提出DART框架，通过解耦的异步模块设计和自适应数据管理策略，显著提升基于视觉语言模型的GUI智能体在强化学习中的训练效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在GUI任务中应用强化学习时面临的交互速度慢和高质量数据不足的问题。

Method: 将训练系统解耦为四个异步模块（环境集群、rollout服务、数据管理器和训练器），并引入自适应数据筛选机制，包括预收集成功轨迹、动态调整rollout参数、选择高熵步骤训练以及使用截断重要性采样稳定学习过程。

Result: 在OSWorld基准上，DART-GUI-7B达到42.13%的任务成功率，比基础模型提高14.61%，优于开源SOTA模型7.34%；系统效率提升显著：GPU利用率达1.6倍，训练吞吐量1.9倍，环境利用率5.5倍。

Conclusion: DART框架有效提升了GUI智能体在复杂任务中的强化学习效率与性能，且将全面开源其代码、数据和模型，推动开放社区发展。

Abstract: Vision-language model (VLM) based GUI agents show promise for automating
complex desktop and mobile tasks, but face significant challenges in applying
reinforcement learning (RL): (1) slow multi-turn interactions with GUI
environments for policy rollout, and (2) insufficient high-quality
agent-environment interactions for policy learning. To address these
challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI
agents, which coordinates heterogeneous modules in a highly decoupled manner.
DART separates the training system into four asynchronous modules: environment
cluster, rollout service, data manager, and trainer. This design enables
non-blocking communication, asynchronous training, rollout-wise trajectory
sampling, and per-worker model synchronization, significantly improving the
system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,
and 5.5* environment utilization. To facilitate effective learning from
abundant samples, we introduce an adaptive data curation scheme: (1)
pre-collecting successful trajectories for challenging tasks to supplement
sparse success in online sampling; (2) dynamically adjusting rollout numbers
and trajectory lengths based on task difficulty; (3) training selectively on
high-entropy steps to prioritize critical decisions; (4) stabilizing learning
via truncated importance sampling for policy mismatch between policy rollout
and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task
success rate, a 14.61% absolute gain over the base model, and 7.34% higher than
open-source SOTA. We will fully open-source our training framework, data, and
model checkpoints via computer-use-agents.github.io/dart-gui, which we believe
is a timely contribution to the open-source community of agentic RL training.

</details>


### [735] [Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer](https://arxiv.org/abs/2509.23886)
*Simon Schrodi,Elias Kempf,Fazl Barez,Thomas Brox*

Main category: cs.LG

TL;DR: 本文研究了在硬蒸馏中隐藏偏见传递的“潜意识学习”现象，发现其并不依赖于全局标记纠缠或logit泄漏，而是由少数“分歧标记”驱动，并揭示早期层的关键作用，且该现象对微小变化（如提示改写）非常脆弱。


<details>
  <summary>Details</summary>
Motivation: 探究在硬蒸馏过程中，为何教师模型的隐藏偏见仍能传递给学生模型，尤其是在仅使用采样标记的情况下，潜意识学习的发生机制和条件。

Method: 通过受控实验和机制分析，识别导致偏见传递的分歧标记，研究其在不同蒸馏设置下的影响，并分析模型早期层在其中的作用。

Result: 发现潜意识学习主要由少量分歧标记驱动，掩盖这些标记可大幅减少偏见传递；早期层在该过程中起关键作用，微调单个早期层即可实现潜意识学习；此外，该现象对提示改写等小扰动极为敏感。

Conclusion: 潜意识学习并非源于全局机制，而是依赖特定分歧标记和模型早期层，且极易被干扰抑制，表明该现象具有高度脆弱性。

Abstract: Language models can transfer hidden biases during distillation. For example,
a teacher that "likes owls" can make its student "like owls" too, even when the
training data consists only of lists of numbers. This surprising phenomenon is
called subliminal learning. Subliminal learning can be expected under soft
distillation, where the student is trained on the teacher's full next-token
distribution. But the fact that this also occurs under hard distillation-where
the student only sees sampled tokens-raises a deeper question: when and how
does subliminal learning actually occur? We answer this question through
controlled experiments and mechanistic analysis. Our results show that
subliminal learning does not need (global) token entanglement or logit leakage.
Instead, it comes down to a small set of divergence tokens-rare cases where
teachers with different biases would predict different tokens. Masking out
these tokens mostly removes the hidden bias transfer. Mechanistically,
divergence tokens reveal that early layers are critical. Surprisingly,
finetuning even a single such early layer is sufficient for subliminal
learning. Finally, we find that subliminal learning is fragile. Even small
changes, like paraphrasing prompts, are usually sufficient to suppress it.

</details>


### [736] [Gradient Flow Convergence Guarantee for General Neural Network Architectures](https://arxiv.org/abs/2509.23887)
*Yash Jakhmola*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论，证明了在训练具有分段非零多项式激活函数或ReLU、Sigmoid激活函数的神经网络时，连续梯度下降（梯度流）具有线性收敛性。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习理论中的一个关键挑战是解释基于梯度的优化方法在训练大规模复杂深度神经网络时为何能取得显著成功，但目前仍缺乏统一的理论支持。

Method: 通过提出一个通用定理，在更弱的假设下统一证明了任意具有特定激活函数的神经网络在梯度流下的线性收敛性，并涵盖此前未知的架构。

Result: 该理论不仅整合了已有结果，还扩展到新的网络结构，且在无穷小步长极限下具有严格成立的线性收敛保证，实验显示其预测与实际梯度下降行为高度一致。

Conclusion: 本文为多种常见神经网络提供了统一的线性收敛理论基础，推动了对梯度下降优化机制的深入理解。

Abstract: A key challenge in modern deep learning theory is to explain the remarkable
success of gradient-based optimization methods when training large-scale,
complex deep neural networks. Though linear convergence of such methods has
been proved for a handful of specific architectures, a united theory still
evades researchers. This article presents a unified proof for linear
convergence of continuous gradient descent, also called gradient flow, while
training any neural network with piecewise non-zero polynomial activations or
ReLU, sigmoid activations. Our primary contribution is a single, general
theorem that not only covers architectures for which this result was previously
unknown but also consolidates existing results under weaker assumptions. While
our focus is theoretical and our results are only exact in the infinitesimal
step size limit, we nevertheless find excellent empirical agreement between the
predictions of our result and those of the practical step-size gradient descent
method.

</details>


### [737] [Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings](https://arxiv.org/abs/2509.23893)
*Zhixin Zhang,Zeming Wei,Meng Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为动态正交持续学习（DOC）的微调方法，用于解决大语言模型在持续学习中的灾难性遗忘问题。通过追踪功能方向的漂移并动态更新，同时调整新任务梯度与历史功能方向正交，有效减轻了新旧任务间的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有基于正则化的方法在长期持续学习中表现不佳，主要原因是微调过程中功能方向的漂移，导致模型难以保持对历史任务的性能。

Method: 提出动态正交持续（DOC）微调方法，通过动态跟踪和更新功能方向，并使新任务的梯度与历史功能方向正交，以减少任务间干扰。

Result: 在多个大语言模型持续学习基准上的实验表明，该方法优于先前方法，显著减少了灾难性遗忘。

Conclusion: DOC为大语言模型的持续微调提供了一个有效的解决方案，能够有效缓解灾难性遗忘，适用于长期持续学习场景。

Abstract: Catastrophic forgetting remains a critical challenge in continual learning
for large language models (LLMs), where models struggle to retain performance
on historical tasks when fine-tuning on new sequential data without access to
past datasets. In this paper, we first reveal that the drift of functional
directions during the fine-tuning process is a key reason why existing
regularization-based methods fail in long-term LLM continual learning. To
address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a
novel approach that tracks the drift of these functional directions and
dynamically updates them during the fine-tuning process. Furthermore, by
adjusting the gradients of new task parameters to be orthogonal to the tracked
historical function directions, our method mitigates interference between new
and old tasks. Extensive experiments on various LLM continual learning
benchmarks demonstrate that this approach outperforms prior methods,
effectively reducing catastrophic forgetting and providing a robust tool for
continuous LLM fine-tuning. Our code is available at
https://github.com/meloxxxxxx/DOC.

</details>


### [738] [Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization](https://arxiv.org/abs/2509.23898)
*Chris Kolb,Laetitia Frost,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: 提出D-Gating方法，通过可微结构化过参数化实现神经网络的结构稀疏，理论证明其与传统非光滑群稀疏正则化等价，并在实践中表现出优异的稀疏-性能权衡。


<details>
  <summary>Details</summary>
Motivation: 解决结构化稀疏正则化因非可微性导致无法与标准随机梯度下降兼容的问题，避免使用专用优化器或后处理剪枝。

Method: 设计D-Gating，将每组权重拆分为主权重向量和多个标量门控因子，实现完全可微的结构化过参数化，并理论分析其与L_{2,2/D}正则化的等价性及收敛性。

Result: 证明D-Gating的局部极小点对应于原问题的局部极小点，目标函数在梯度流下指数收敛至正则化损失，并在多种任务上验证了其优越的稀疏性和性能。

Conclusion: D-Gating在理论上等价于求解原始结构稀疏问题，但具有更优的学习动态，能从非稀疏状态演化为稀疏优化，且无需复杂优化策略即可实现高效稀疏化。

Abstract: Structured sparsity regularization offers a principled way to compact neural
networks, but its non-differentiability breaks compatibility with conventional
stochastic gradient descent and requires either specialized optimizers or
additional post-hoc pruning without formal guarantees. In this work, we propose
$D$-Gating, a fully differentiable structured overparameterization that splits
each group of weights into a primary weight vector and multiple scalar gating
factors. We prove that any local minimum under $D$-Gating is also a local
minimum using non-smooth structured $L_{2,2/D}$ penalization, and further show
that the $D$-Gating objective converges at least exponentially fast to the
$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results
show that $D$-Gating is theoretically equivalent to solving the original group
sparsity problem, yet induces distinct learning dynamics that evolve from a
non-sparse regime into sparse optimization. We validate our theory across
vision, language, and tabular tasks, where $D$-Gating consistently delivers
strong performance-sparsity tradeoffs and outperforms both direct optimization
of structured penalties and conventional pruning baselines.

</details>


### [739] [Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.23905)
*Tianjiao Sun,Ningyan Guo,Haozhe Gu,Yanyan Peng,Zhiyong Feng*

Main category: cs.LG

TL;DR: 提出了一种通信与控制协同设计机制，通过多智能体强化学习框架和新型MAHPPO-AM算法优化无人机群的资源分配与三维轨迹，在复杂地理环境中显著提升通信质量并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 复杂地理环境导致无人机到地面链路频繁中断，影响无人机群辅助通信的可靠性与服务质量，需提高通信质量与能源效率。

Method: 将联合资源分配与三维轨迹控制问题建模为马尔可夫决策过程，设计多智能体强化学习框架，并提出带有动作掩码的多智能体混合近端策略优化（MAHPPO-AM）算法以处理高维混合动作空间中的硬约束。

Result: 实验结果表明，所提方法在公平性指数达到0.99的同时，能耗比基线方法降低最多25%。

Conclusion: 该协同设计机制有效平衡了地面用户通信公平性与无人机能耗，在复杂环境下实现了高效可靠的无人机群通信。

Abstract: The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication
networks has become an increasingly vital approach for remediating coverage
limitations in infrastructure-deficient environments, with especially pressing
applications in temporary scenarios, such as emergency rescue, military and
security operations, and remote area coverage. However, complex geographic
environments lead to unpredictable and highly dynamic wireless channel
conditions, resulting in frequent interruptions of air-to-ground (A2G) links
that severely constrain the reliability and quality of service in UAV
swarm-assisted mobile communications. To improve the quality of UAV
swarm-assisted communications in complex geographic environments, we propose an
integrated communication and control co-design mechanism. Given the stringent
energy constraints inherent in UAV swarms, our proposed mechanism is designed
to optimize energy efficiency while maintaining an equilibrium between
equitable communication rates for mobile ground users (GUs) and UAV energy
expenditure. We formulate the joint resource allocation and 3D trajectory
control problem as a Markov decision process (MDP), and develop a multi-agent
reinforcement learning (MARL) framework to enable real-time coordinated actions
across the UAV swarm. To optimize the action policy of UAV swarms, we propose a
novel multi-agent hybrid proximal policy optimization with action masking
(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action
spaces. The algorithm incorporates action masking to enforce hard constraints
in high-dimensional action spaces. Experimental results demonstrate that our
approach achieves a fairness index of 0.99 while reducing energy consumption by
up to 25% compared to baseline methods.

</details>


### [740] [Graph Mixing Additive Networks](https://arxiv.org/abs/2509.23923)
*Maya Bechler-Speicher,Andrea Zerio,Maor Huri,Marie Vibeke Vestergaard,Ran Gilad-Bachrach,Tine Jess,Samir Bhatt,Aleksejs Sazonovs*

Main category: cs.LG

TL;DR: GMAN是一种灵活、可解释且表达能力强的框架，扩展了图神经加性网络（GNANs），用于从稀疏时间序列数据集中学习。


<details>
  <summary>Details</summary>
Motivation: 为了在保持模型可解释性的同时提升对稀疏时间序列数据的建模能力，特别是在需要领域对齐解释的应用中。

Method: 将每个时间依赖轨迹表示为有向图，并在其上应用增强版的GNAN；通过特征和图分组引入先验知识，实现多层级可解释性。

Result: 在真实数据集（如死亡率预测和假新闻检测）上，GMAN优于强非解释性黑箱基线模型。

Conclusion: GMAN在性能和可解释性之间实现了良好平衡，能够提供可操作且与领域一致的解释。

Abstract: We introduce GMAN, a flexible, interpretable, and expressive framework that
extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse
time-series data. GMAN represents each time-dependent trajectory as a directed
graph and applies an enriched, more expressive GNAN to each graph. It allows
users to control the interpretability-expressivity trade-off by grouping
features and graphs to encode priors, and it provides feature, node, and
graph-level interpretability. On real-world datasets, including mortality
prediction from blood tests and fake-news detection, GMAN outperforms strong
non-interpretable black-box baselines while delivering actionable,
domain-aligned explanations.

</details>


### [741] [HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.23928)
*Zhinan Xie,Peisong Wang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出HiViS框架，通过显式-隐式输入分解和多步自反馈训练策略，显著加速视觉-语言模型的推测解码，实现最高2.65倍的推理加速且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码在视觉-语言模型中面临视觉标记导致的语义错位和解码效率低下的问题，难以直接应用。

Method: 将视觉标记从草案模型输入中移除，仅保留文本标记作为显式输入，并复用目标模型最后一层隐藏状态作为隐式视觉信息；采用多步自反馈训练策略进行高效训练。

Result: 草案模型prefill序列长度压缩至目标模型的0.7%-1.3%，在多种模型和任务上实现最高2.65倍推理加速，且生成质量无损。

Conclusion: HiViS有效解决了视觉-语言模型中推测解码的效率与语义对齐问题，显著提升了推理速度，具有良好的通用性和实用性。

Abstract: Speculative decoding is an effective approach for accelerating inference in
Large Language models (LLMs), but its adaptation to Vision-Language models
(VLMs) remains challenging for additional visual tokens in multimodal inputs.
First, owing to the fact that the drafter and the target VLM may derived from
different families, the semantic representations of visual tokens in the target
VLM are misaligned with those in the drafter, introducing bias into the
KV-cache during the prefill stage. Second, the large number of visual tokens
substantially slows down the drafter's self-attention during the decoding
stage. We propose Hiding Visual Tokens from the Drafter for Speculative
Decoding in Vision-Language Models (HiViS), an explicit-implicit input
decomposition framework that alleviates the above inefficiency. All visual
tokens are removed from the drafter's input, retaining only textual tokens as
explicit inputs, while directly reusing the target VLM's corresponding
last-layer hidden states as implicit visual information without additional
processing. To train the drafter efficiently, we introduces multi-step
self-feedback training strategy with dynamic data selection and sequential
embedding supervision to simulate reasoning during training. Our approach
compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the
target VLM's input, while maintaining lossless generation quality. Extensive
experiments across diverse models and tasks demonstrate up to 2.65x speedup,
confirming the effectiveness of HiViS in accelerating VLM inference.

</details>


### [742] [Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms](https://arxiv.org/abs/2509.23933)
*Jiahao Ying,Mingbao Lin,Qianru Sun,Yixin Cao*

Main category: cs.LG

TL;DR: 本文通过引入一种内部度量方法（MUI）研究MoE架构的内在机制，揭示了专家协作、神经元利用率与模型泛化能力之间的关系，表明MUI可作为基准性能之外的补充指标。


<details>
  <summary>Details</summary>
Motivation: 现有对MoE架构的研究过于关注性能，缺乏对其内部机制的理解，限制了进一步发展。因此需要从内部指标出发，深入分析其路由机制和专家行为。

Method: 提出并使用MUI（专家利用率）作为内部度量指标，系统分析多种公开MoE模型的专家激活模式、训练动态及任务完成过程中的专家协作行为。

Result: 发现：(1) 模型演进中神经元利用率下降，反映更强泛化能力；(2) 训练过程中MUI比基准性能提供更深层洞察；(3) 多专家协同完成任务，共享专家起主导作用；(4) 神经元级激活模式可反映数据多样性。

Conclusion: MUI是一种有效的内部分析工具，有助于理解MoE模型的容量、动态演化和专家专业化，为未来设计更高效的MoE系统提供指导。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising direction,
offering efficiency and scalability by activating only a subset of parameters
during inference. However, current research remains largely
performance-centric, with limited understanding of its internal mechanisms,
thereby constraining broader progress. In this work, we use an internal metric
to investigate the mechanisms of MoE architecture by explicitly incorporating
routing mechanisms and analyzing expert-level behaviors. Through systematic
analyses of a wide range of publicly available MoE models, we uncover several
findings: (1) neuron utilization decreases as models evolve, reflecting
stronger generalization; (2) training exhibits a dynamic trajectory, where
benchmark performance alone provides limited signal while MUI reveals deeper
insights; (3) task completion emerges from collaborative contributions of
multiple experts, with shared experts driving concentration; and (4) activation
patterns at the neuron level provide a fine-grained proxy for data diversity.
Together, these results demonstrate the potential of MUI as a complementary
indicator to benchmark performance, offering new insights into the capacity,
dynamics, and specialization of MoE models. Our project can be found at
https://yingjiahao14.github.io/MoE-MUI/.

</details>


### [743] [Diffusion Models are Kelly Gamblers](https://arxiv.org/abs/2509.23937)
*Akhil Premkumar*

Main category: cs.LG

TL;DR: 本文揭示了扩散模型与凯利准则之间的联系，指出条件扩散模型通过存储信号X与条件信息Y之间的互信息来工作，分类器自由引导在采样时增强了这种互信息，尤其有助于图像模型中低互信息的问题，并讨论了扩散模型作为无限深自编码器的流行观点的一些细微差别，将去噪损失与量子力学中的费米黄金规则联系起来。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型与信息论、赌博中的凯利准则之间的理论联系，理解条件扩散模型如何利用互信息，并澄清对扩散模型作为无限深自编码器的理解误区。

Method: 通过理论分析，建立扩散模型与凯利准则的联系，分析条件扩散模型中互信息的作用，探讨分类器自由引导对互信息的影响，并类比量子力学中的费米黄金规则解释去噪损失。

Result: 发现条件扩散模型存储X与Y的互信息；分类器自由引导可提升X与Y的互信息；图像与标签间互信息低与流形假设密切相关；扩散模型并非简单的无限深自编码器；去噪损失可类比费米黄金规则。

Conclusion: 扩散模型不仅与信息论紧密相关，还与决策理论（如凯利准则）和物理定律（如费米黄金规则）存在深刻联系，这为理解和改进扩散模型提供了新的理论视角。

Abstract: We draw a connection between diffusion models and the Kelly criterion for
maximizing returns in betting games. We find that conditional diffusion models
store additional information to bind the signal $X$ with the conditioning
information $Y$, equal to the mutual information between them. Classifier-free
guidance effectively boosts the mutual information between $X$ and $Y$ at
sampling time. This is especially helpful in image models, since the mutual
information between images and their labels is low, a fact which is intimately
connected to the manifold hypothesis. Finally, we point out some nuances in the
popular perspective that diffusion models are infinitely deep autoencoders. In
doing so, we relate the denoising loss to the Fermi Golden Rule from quantum
mechanics.

</details>


### [744] [Brain-language fusion enables interactive neural readout and in-silico experimentation](https://arxiv.org/abs/2509.23941)
*Victoria Bosch,Daniel Anthes,Adrien Doerig,Sushrut Thorat,Peter König,Tim Christian Kietzmann*

Main category: cs.LG

TL;DR: CorText是一个将神经活动直接整合到大语言模型（LLM）潜在空间中的新框架，实现了基于脑数据的开放、自然语言交互，在图像描述生成和问题回答方面表现优于对照，并展现出训练未见语义类别的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的神经解码方法多为静态且缺乏交互性，限制了脑-机交互的灵活性，因此需要一种能够实现动态、开放式语言交互的新型解码框架。

Method: 提出CorText框架，将fMRI记录的神经活动嵌入大语言模型的潜在空间，仅使用神经数据即可生成图像描述并回答问题，并通过反事实分析模拟皮层微刺激。

Result: CorText在生成准确图像描述和回答细节问题上优于对照模型，展现出对训练中未见语义类别的零样本泛化能力，并可通过反事实分析模拟神经干预效果。

Conclusion: CorText实现了从被动解码向生成式、灵活的脑-语言接口转变，推动了神经解码技术向更具交互性和泛化能力的方向发展。

Abstract: Large language models (LLMs) have revolutionized human-machine interaction,
and have been extended by embedding diverse modalities such as images into a
shared language space. Yet, neural decoding has remained constrained by static,
non-interactive methods. We introduce CorText, a framework that integrates
neural activity directly into the latent space of an LLM, enabling open-ended,
natural language interaction with brain data. Trained on fMRI data recorded
during viewing of natural scenes, CorText generates accurate image captions and
can answer more detailed questions better than controls, while having access to
neural data only. We showcase that CorText achieves zero-shot generalization
beyond semantic categories seen during training. Furthermore, we present a
counterfactual analysis that emulates in-silico cortical microstimulation.
These advances mark a shift from passive decoding toward generative, flexible
interfaces between brain activity and language.

</details>


### [745] [Efficient Identification of High Similarity Clusters in Polygon Datasets](https://arxiv.org/abs/2509.23942)
*John N. Daras*

Main category: cs.LG

TL;DR: 提出一个结合动态相似性索引阈值、监督调度和召回约束优化的框架，以减少大规模地理空间分析中的计算负载。


<details>
  <summary>Details</summary>
Motivation: 在极大数据集下，现有几何操作优化工具仍面临计算量过大的挑战，难以高效完成空间相似性计算。

Method: 采用核密度估计（KDE）动态确定相似性阈值，并结合机器学习模型进行聚类优先级排序，通过动态阈值、监督调度和召回约束优化来减少需验证的聚类数量。

Result: 实验结果表明该方法显著降低了计算成本，同时保持高精度，具备良好的可扩展性和有效性。

Conclusion: 所提框架能在满足用户定义的精度和召回要求的前提下，有效提升大规模空间相似性计算的效率，为大规模地理空间分析提供了实用解决方案。

Abstract: Advancements in tools like Shapely 2.0 and Triton can significantly improve
the efficiency of spatial similarity computations by enabling faster and more
scalable geometric operations. However, for extremely large datasets, these
optimizations may face challenges due to the sheer volume of computations
required. To address this, we propose a framework that reduces the number of
clusters requiring verification, thereby decreasing the computational load on
these systems. The framework integrates dynamic similarity index thresholding,
supervised scheduling, and recall-constrained optimization to efficiently
identify clusters with the highest spatial similarity while meeting
user-defined precision and recall requirements. By leveraging Kernel Density
Estimation (KDE) to dynamically determine similarity thresholds and machine
learning models to prioritize clusters, our approach achieves substantial
reductions in computational cost without sacrificing accuracy. Experimental
results demonstrate the scalability and effectiveness of the method, offering a
practical solution for large-scale geospatial analysis.

</details>


### [746] [Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm](https://arxiv.org/abs/2509.23946)
*Kaisen Yang,Lixuan He,Rushi Shah,Kaicheng Yang,Qinwei Ma,Dianbo Liu,Alex Lamb*

Main category: cs.LG

TL;DR: 本文提出了一种名为Explore-Execute Chain（E²C）的新推理框架，将大语言模型的推理过程解耦为探索和执行两个阶段，提升了效率、可解释性和跨领域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Chain-of-Thought方法将战略规划与逐步执行混杂，导致计算效率低、推理路径探索有限且可解释性差。

Method: E²C框架分为两个阶段：首先随机生成高层计划（探索阶段），然后确定性地执行该计划（执行阶段）；训练上采用结合改进的监督微调（EF-SFT）和强化学习的两阶段方法。

Result: 在AIME'2024上，E²C测试时扩展仅用不到10%的解码token即达到58.1%的准确率；在医疗基准上，EF-SFT仅用3.5%的token就比标准SFT最高提升14.5%准确率，实现SOTA性能。

Conclusion: 通过分离规划与执行，E²C显著提升了推理效率、泛化能力和模型可解释性，为大模型推理提供了更高效且可扩展的架构。

Abstract: Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning
abilities of Large Language Models (LLMs), yet their monolithic and
auto-regressive architecture inherently conflates high-level strategic planning
with low-level step-by-step execution, leading to computational inefficiency,
limited exploration of reasoning paths, and reduced interpretability. To
overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a
structured reasoning framework that decouples reasoning into two distinct
phases: an exploratory phase that stochastically generates succinct high-level
plans, followed by an execution phase that deterministically carries out the
chosen plan. Our approach incorporates a two-stage training methodology, which
combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation
algorithm enforcing strict plan adherence - with a subsequent Reinforcement
Learning (RL) stage that capitalizes on the informativeness of exploration and
reinforces the determinism of execution.This decomposition enables an efficient
test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches
58.1% accuracy using <10% of the decoding tokens required by comparable methods
(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For
cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with
only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher
accuracy than standard SFT on medical benchmarks, delivering state-of-the-art
performance, strong generalization, and greater interpretability by separating
planning from execution. The code and pre-trained models for the project are
available at: https://github.com/yks23/Explore-Execute-Chain.git

</details>


### [747] [DiBS-MTL: Transformation-Invariant Multitask Learning with Direction Oracles](https://arxiv.org/abs/2509.23948)
*Surya Murthy,Kushagra Gupta,Mustafa O. Karabag,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 本文提出了一种基于方向的讨价还价解法DiBS-MTL，用于多任务学习中避免任务损失非仿射缩放导致的任务主导问题，并证明其在非凸情况下的收敛性，实验表明该方法对现有方法具有鲁棒性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中不同任务损失的任意非仿射缩放会导致某些任务主导训练，从而降低整体性能，现有方法对此缺乏不变性。

Method: 引入具备单调非仿射变换不变性的方向型讨价还价解法（DiBS），理论证明其在非凸MTL设置下子序列收敛至Pareto平稳点，并提出适用于MTL的高效算法DiBS-MTL。

Result: 在标准MTL基准上验证了DiBS-MTL的有效性，其性能与最先进方法相当，且对显著降低其他方法性能的非仿射单调变换具有鲁棒性。

Conclusion: DiBS-MTL能有效应对多任务学习中因损失尺度差异引起的问题，在非凸情形下收敛并保持稳定性能，为MTL提供了一种鲁棒的优化框架。

Abstract: Multitask learning (MTL) algorithms typically rely on schemes that combine
different task losses or their gradients through weighted averaging. These
methods aim to find Pareto stationary points by using heuristics that require
access to task loss values, gradients, or both. In doing so, a central
challenge arises because task losses can be arbitrarily, nonaffinely scaled
relative to one another, causing certain tasks to dominate training and degrade
overall performance. A recent advance in cooperative bargaining theory, the
Direction-based Bargaining Solution (DiBS), yields Pareto stationary solutions
immune to task domination because of its invariance to monotonic nonaffine task
loss transformations. However, the convergence behavior of DiBS in nonconvex
MTL settings is currently not understood. To this end, we prove that under
standard assumptions, a subsequence of DiBS iterates converges to a Pareto
stationary point when task losses are possibly nonconvex, and propose DiBS-MTL,
a computationally efficient adaptation of DiBS to the MTL setting. Finally, we
validate DiBS-MTL empirically on standard MTL benchmarks, showing that it
achieves competitive performance with state-of-the-art methods while
maintaining robustness to nonaffine monotonic transformations that
significantly degrade the performance of existing approaches, including prior
bargaining-inspired MTL methods. Code available at
https://github.com/suryakmurthy/dibs-mtl.

</details>


### [748] [Evaluating the Robustness of Chinchilla Compute-Optimal Scaling](https://arxiv.org/abs/2509.23963)
*Rylan Schaeffer,Noam Levi,Andreas Kirsch,Theo Guenais,Brando Miranda,Elyas Obbad,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文重新审视了Chinchilla论文中的模型参数不确定性及其对关键结果的影响，发现尽管存在多种参数解释和潜在扰动，其核心结论（如计算最优的token-to-parameter比率）依然稳健，从而为语言模型的可扩展性提供了持续可靠的指导。


<details>
  <summary>Details</summary>
Motivation: 针对Chinchilla论文中存在的置信区间宽、方法间结果不一致及与其他缩放定律不符等问题，研究其核心结论是否仍可信赖。

Method: 分析Chinchilla中模型参数的三种不同解释，并通过四种结构化方式故意扰动参数，评估其对缩放律估计和最优token-to-parameter比率的影响。

Result: 发现不同参数解释对关键结果影响不大；最优token-to-parameter比率在一种解释下更稳定；结果对加性或系统性误差较敏感，但整体上Chinchilla的核心结论经受住了显著扰动。

Conclusion: Chinchilla的关键结果具有鲁棒性，仍可作为语言模型扩展的可靠指南。

Abstract: Hoffman et al (2022)'s Chinchilla paper introduced the principle of
compute-optimal scaling, laying a foundation for future scaling of language
models. In the years since, however, valid concerns about Chinchilla have been
raised: wide confidence intervals, discrepancies between its three approaches,
and incongruities with other scaling laws. This raises a critical question for
the field: Can practitioners still rely on Chinchilla's prescriptions? Our work
demonstrates the answer is yes. We begin by uncovering that the model
parameters central to Chinchilla's analyses were ambiguous: three
interpretations are possible, with relative differences between different
interpretations of model parameters as high as 15.2%. We find that, perhaps
surprisingly, which model parameters are used for the analyses do not
meaningfully affect key results: the scaling law estimates and the
compute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,
the tokens-to-parameter ratio becomes more constant with the target compute
budget. We then ask how distorted the Chinchilla model parameters could have
been without meaningfully affecting the key results. By deliberately perturbing
model parameters in four structured ways, we find that key Chinchilla results
are most sensitive to additive or systematic errors, which can alter the
otherwise flat trend of the optimal tokens-to-parameter ratio, but overall,
Chinchilla's key results withstand sizable perturbations. Altogether, our
findings offer the field renewed confidence in Chinchilla as a durable guide
for scaling language models.

</details>


### [749] [Detecting and Rectifying Noisy Labels: A Similarity-based Approach](https://arxiv.org/abs/2509.23964)
*Dang Huu-Tien,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出一种基于神经网络倒数第二层特征的模型无关的标签错误检测与修正方法，通过分析特征相似性在紧密聚类中识别并自动修正标签噪声。


<details>
  <summary>Details</summary>
Motivation: 标签噪声会损害神经网络训练性能，随着模型规模增大，亟需自动化工具来检测和修正数据集中的标签错误。

Method: 利用神经网络的倒数第二层特征，计算误标数据点与其真实类别及其他类别数据点之间的特征相似性，基于同类聚类内的标签分布信息进行错误检测与修正。

Result: 实验表明该方法在多种噪声场景下均表现出高性能，并能自动修正标签错误，有效提升数据集质量。

Conclusion: 所提方法是一种有效的后处理、模型无关的标签噪声检测与修正方案，具有广泛适用性和实用价值。

Abstract: Label noise in datasets could damage the performance of neural net training.
As the size of modern deep networks grows, there is a growing demand for
automated tools for detecting such errors. In this paper, we propose post-hoc,
model-agnostic error detection and rectification methods utilizing the
penultimate feature from a neural network. Our idea is based on the observation
that the similarity between the penultimate feature of a mislabeled data point
and its true class data points is higher than that for data points from other
classes, making the probability of label occurrence within a tight, similar
cluster informative for detecting and rectifying errors. Extensive experiments
show our method not only demonstrates high performance across various noises
but also automatically rectifies these errors to improve the quality of
datasets.

</details>


### [750] [Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts](https://arxiv.org/abs/2509.23976)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习（PPO）的框架，从Common Domain Model（CDM）规范自动生成功能正确且Gas优化的Solidity智能合约，采用两阶段课程学习策略，在测试数据上实现了最高35.59%的Gas成本降低。


<details>
  <summary>Details</summary>
Motivation: 将金融衍生品自动化通过智能合约实现虽具效率潜力，但受限于将高级金融规范（如CDM）转化为功能正确且Gas高效的可执行代码的复杂性。

Method: 采用基于Proximal Policy Optimization（PPO）的强化学习框架，结合预定义代码片段库和两阶段课程学习：第一阶段训练功能正确性，第二阶段优化Gas消耗。

Result: 实验结果表明，该方法在未见过的测试数据上相比未优化基线最多减少了35.59%的Gas成本，能够有效生成功能正确且经济可行的智能合约。

Conclusion: 该研究提供了一种可行的方法，弥合了高级金融协议与高效链上执行之间的鸿沟，推动了金融衍生品智能合约的自动化和可持续部署。

Abstract: Smart contract-based automation of financial derivatives offers substantial
efficiency gains, but its real-world adoption is constrained by the complexity
of translating financial specifications into gas-efficient executable code. In
particular, generating code that is both functionally correct and economically
viable from high-level specifications, such as the Common Domain Model (CDM),
remains a significant challenge. This paper introduces a Reinforcement Learning
(RL) framework to generate functional and gas-optimized Solidity smart
contracts directly from CDM specifications. We employ a Proximal Policy
Optimization (PPO) agent that learns to select optimal code snippets from a
pre-defined library. To manage the complex search space, a two-phase curriculum
first trains the agent for functional correctness before shifting its focus to
gas optimization. Our empirical results show the RL agent learns to generate
contracts with significant gas savings, achieving cost reductions of up to
35.59% on unseen test data compared to unoptimized baselines. This work
presents a viable methodology for the automated synthesis of reliable and
economically sustainable smart contracts, bridging the gap between high-level
financial agreements and efficient on-chain execution.

</details>


### [751] [Guide: Generalized-Prior and Data Encoders for DAG Estimation](https://arxiv.org/abs/2509.23992)
*Amartya Roy,Devharish N,Shreya Ganguly,Kripabandhu Ghosh*

Main category: cs.LG

TL;DR: 提出了一种名为GUIDE的新框架，结合大语言模型生成的邻接矩阵与观测数据，通过双编码器架构在计算效率、准确性和可扩展性方面显著优于现有因果发现方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在可扩展性、计算效率和混合数据类型适应性方面存在严重局限，尤其是在节点数超过70时性能下降明显，且对连续与非连续数据处理能力不足。

Method: 采用双编码器架构，将大语言模型（LLM）生成的邻接矩阵与观测数据融合，并引入强化学习代理在训练中动态平衡准确性奖励与DAG约束惩罚，以优化因果结构发现过程。

Result: 相比RL-BIC和KCRL方法平均减少约42%运行时间，相较于NOTEARS和GraN-DAG分别平均提升约117%的准确性，且能扩展至70个及以上节点，在混合数据类型上表现稳健。

Conclusion: GUIDE在保持DAG结构约束的同时，显著提升了因果发现的效率、准确性和可扩展性，尤其适用于大规模和混合数据类型的场景，克服了传统方法的关键瓶颈。

Abstract: Modern causal discovery methods face critical limitations in scalability,
computational efficiency, and adaptability to mixed data types, as evidenced by
benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational
energy demands, and continuous/non-continuous data handling. While traditional
algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,
exhibiting prohibitive energy costs for higher-order nodes and poor scalability
beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large
Language Model (LLM)-generated adjacency matrices with observational data
through a dual-encoder architecture. GUIDE uniquely optimizes computational
efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and
KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy
over both NOTEARS and GraN-DAG individually. During training, GUIDE's
reinforcement learning agent dynamically balances reward maximization
(accuracy) and penalty avoidance (DAG constraints), enabling robust performance
across mixed data types and scalability to $\ge 70$ nodes -- a setting where
baseline methods fail.

</details>


### [752] [Does Weak-to-strong Generalization Happen under Spurious Correlations?](https://arxiv.org/abs/2509.24005)
*Chenruo Liu,Yijun Dong,Qi Lei*

Main category: cs.LG

TL;DR: 本文研究了弱到强泛化（W2S）中在存在虚假相关性的情况下，使用弱教师生成的伪标签微调强学生模型的表现，并提出了一种简单有效的算法来提升失败情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 在具有群体不平衡导致的虚假相关性的下游任务中，探究弱教师生成的伪标签是否能成功实现弱到强泛化，并分析失败原因。

Method: 理论分析在比例渐近极限下W2S增益的表现，识别出标注数据与未标注数据中少数群体比例不一致（η_u ≠ η_ℓ）会导致W2S失败；提出一种无需群体标签的后处理方法，在高置信度子集上重新训练强学生模型以提升性能。

Result: 理论表明当η_u = η_ℓ时W2S总能发生，而差异越大W2S增益越小；实验验证了理论发现，并显示所提算法在多种基准和教师-学生组合上显著优于标准W2S微调。

Conclusion: 群体比例不匹配是W2S失败的关键因素，所提出的高置信度重训练策略可有效缓解该问题，提升W2S鲁棒性和性能。

Abstract: We initiate a unified theoretical and algorithmic study of a key problem in
weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained
student with pseudolabels from a weaker teacher on a downstream task with
spurious correlations, does W2S happen, and how to improve it upon failures? We
consider two sources of spurious correlations caused by group imbalance: (i) a
weak teacher fine-tuned on group-imbalanced labeled data with a minority group
of fraction $\eta_\ell$, and (ii) a group-imbalanced unlabeled set
pseudolabeled by the teacher with a minority group of fraction $\eta_u$.
Theoretically, a precise characterization of W2S gain at the proportional
asymptotic limit shows that W2S always happens with sufficient pseudolabels
when $\eta_u = \eta_\ell$ but may fail when $\eta_u \ne \eta_\ell$, where W2S
gain diminishes as $(\eta_u - \eta_\ell)^2$ increases. Our theory is
corroborated by extensive experiments on various spurious correlation
benchmarks and teacher-student pairs. To boost W2S performance upon failures,
we further propose a simple, effective algorithmic remedy that retrains the
strong student on its high-confidence data subset after W2S fine-tuning. Our
algorithm is group-label-free and achieves consistent, substantial improvements
over vanilla W2S fine-tuning.

</details>


### [753] [SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention](https://arxiv.org/abs/2509.24006)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Shuo Yang,Kaiwen Zheng,Haocheng Xi,Ziteng Wang,Hongzhou Zhu,Min Zhao,Ion Stoica,Joseph E. Gonzalez,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: 提出SLA（Sparse-Linear Attention）方法，融合稀疏与线性注意力机制，显著加速扩散模型中的注意力计算，尤其在视频生成中实现高达20倍的计算减少和2.2倍端到端加速，且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer（DiT）模型在视频生成中因序列长和注意力计算复杂度为二次方，导致注意力延迟成为主要瓶颈，需高效加速方法。

Method: 基于注意力权重可分离为高秩的大权重和低秩的小权重的观察，SLA将权重分为关键、边缘和可忽略三类，分别采用O(N^2)、O(N)计算和跳过处理，并融合为单一GPU内核，支持前向和反向传播。

Result: SLA在仅少量微调下实现注意力计算减少95%，注意力计算加速13.7倍，端到端视频生成加速2.2倍，在Wan2.1-1.3B上表现优异，且生成质量无损。

Conclusion: SLA通过结合稀疏与线性注意力，有效降低DiT模型的注意力计算开销，是适用于长序列扩散模型的高效注意力机制。

Abstract: In Diffusion Transformer (DiT) models, particularly for video generation,
attention latency is a major bottleneck due to the long sequence length and the
quadratic complexity. We find that attention weights can be separated into two
parts: a small fraction of large weights with high rank and the remaining
weights with very low rank. This naturally suggests applying sparse
acceleration to the first part and low-rank acceleration to the second. Based
on this finding, we propose SLA (Sparse-Linear Attention), a trainable
attention method that fuses sparse and linear attention to accelerate diffusion
models. SLA classifies attention weights into critical, marginal, and
negligible categories, applying O(N^2) attention to critical weights, O(N)
attention to marginal weights, and skipping negligible ones. SLA combines these
computations into a single GPU kernel and supports both forward and backward
passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x
reduction in attention computation, resulting in significant acceleration
without loss of generation quality. Experiments show that SLA reduces attention
computation by 95% without degrading end-to-end generation quality,
outperforming baseline methods. In addition, we implement an efficient GPU
kernel for SLA, which yields a 13.7x speedup in attention computation and a
2.2x end-to-end speedup in video generation on Wan2.1-1.3B.

</details>


### [754] [Pretraining Scaling Laws for Generative Evaluations of Language Models](https://arxiv.org/abs/2509.24012)
*Rylan Schaeffer,Noam Levi,Brando Miranda,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出了三种预训练扩展定律，用于拟合和预测生成性评估中的pass-at-$k$性能，并分析了不同协变量对扩展定律稳定性与预测能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注预训练损失和判别式任务的扩展定律，而对生成式任务（如数学解题、软件工程）的研究较少，因此需要建立适用于生成性评估的扩展定律。

Method: 提出并比较了三种基于不同协变量（计算量、模型参数与token数、黄金参考解的对数似然）的扩展定律，通过实验分析其在pass-at-$k$指标上的拟合与预测性能。

Result: 发现计算量和参数+token的扩展定律在最后1.5-2.5个数量级趋于稳定，而基于对数似然的定律在5个数量级内更稳定；三者预测性能相近，但各有优劣；并从理论上证明计算量定律是参数-令牌定律的计算最优包络。

Conclusion: 该框架为研究人员提供了预测生成性任务性能的方法和洞察，有助于指导模型扩展策略。

Abstract: Neural scaling laws have played a central role in modern machine learning,
driving the field's ever-expanding scaling of parameters, data and compute.
While much research has gone into fitting scaling laws and predicting
performance on pretraining losses and on discriminative evaluations such as
multiple-choice question-answering, comparatively little research has been done
on fitting scaling laws and predicting performance on generative evaluations
such as mathematical problem-solving or software engineering. We propose and
evaluate three different pretraining scaling laws for fitting pass-at-$k$ on
generative evaluations and for predicting pass-at-$k$ of the most expensive
model using the performance of cheaper models. Our three scaling laws differ in
the covariates used: (1) compute, (2) model parameters and tokens, (3) log
likelihoods of gold reference solutions. We make four main contributions: (1)
We show how generative evaluations offer new hyperparameters (in our setting,
$k$) that researchers can use to control the scaling laws parameters and the
predictability of performance. (2) In terms of scaling law parameters, we find
that the compute scaling law and parameters\,+\,tokens scaling law stabilize
for the last ~$1.5{-}2.5$ orders of magnitude, whereas the gold reference
likelihood scaling law stabilizes for the last ~$5$ orders of magnitude. (3) In
terms of predictive performance, we find all three scaling laws perform
comparably, although the compute scaling law predicts slightly worse for small
$k$ and the log likelihoods of gold reference solutions predicts slightly worse
for large $k$. (4) We establish a theoretical connection that the compute
scaling law emerges as the compute-optimal envelope of the
parameters-and-tokens scaling law. Our framework provides researchers and
practitioners with insights and methodologies to forecast generative
performance.

</details>


### [755] [Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.24047)
*Runyu Zhang,Na Li,Asuman Ozdaglar,Jeff Shamma,Gioele Zardini*

Main category: cs.LG

TL;DR: 提出了一种基于风险寻求的乐观框架，用于多智能体强化学习中的合作问题，通过乐观值函数和策略梯度定理实现去中心化的乐观Actor-Critic算法，在合作任务中优于风险中性和启发式乐观方法。


<details>
  <summary>Details</summary>
Motivation: 在合作型多智能体强化学习中，传统风险规避方法容易导致次优均衡，而现有乐观方法缺乏理论基础，因此需要一个有理论支撑的乐观建模框架。

Method: 基于凸风险测度的对偶表示，提出将风险寻求目标解释为乐观，并引入乐观值函数；推导出对应的策略梯度定理，设计了去中心化的乐观Actor-Critic算法。

Result: 在合作型基准任务上验证了所提方法的有效性，风险寻求的乐观方法在协调性能上优于风险中性基线和启发式乐观方法。

Conclusion: 该框架统一了风险敏感学习与乐观主义，为多智能体合作提供了理论严谨且实践有效的解决方案。

Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL),
where convex risk measures and robust formulations provide principled ways to
model preferences beyond expected return. Recent extensions to multi-agent RL
(MARL) have largely emphasized the risk-averse setting, prioritizing robustness
to uncertainty. In cooperative MARL, however, such conservatism often leads to
suboptimal equilibria, and a parallel line of work has shown that optimism can
promote cooperation. Existing optimistic methods, though effective in practice,
are typically heuristic and lack theoretical grounding. Building on the dual
representation for convex risk measures, we propose a principled framework that
interprets risk-seeking objectives as optimism. We introduce optimistic value
functions, which formalize optimism as divergence-penalized risk-seeking
evaluations. Building on this foundation, we derive a policy-gradient theorem
for optimistic value functions, including explicit formulas for the entropic
risk/KL-penalty setting, and develop decentralized optimistic actor-critic
algorithms that implement these updates. Empirical results on cooperative
benchmarks demonstrate that risk-seeking optimism consistently improves
coordination over both risk-neutral baselines and heuristic optimistic methods.
Our framework thus unifies risk-sensitive learning and optimism, offering a
theoretically grounded and practically effective approach to cooperation in
MARL.

</details>


### [756] [Collaborative Device-Cloud LLM Inference through Reinforcement Learning](https://arxiv.org/abs/2509.24050)
*Wenzhi Fang,Dong-Jun Han,Liangqi Yuan,Christopher Brinton*

Main category: cs.LG

TL;DR: 提出一种基于设备端大语言模型自身推理过程末尾进行路由决策的框架，通过后训练赋予其判断任务是否需要卸载到云端的能力，并设计了奖励最大化问题和群自适应策略梯度算法，在减少云资源使用的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有外部路由器难以从提示词表面模式准确判断任务难度，导致设备-云协作中路由决策效果不佳。

Method: 将路由决策交由设备端LLM在推理结束后自主完成，通过后训练引入能力；构建奖励最大化问题，设计包含群级别策略梯度和自适应提示过滤的算法来优化路由策略。

Result: 实验表明该方法在多个模型和基准上均优于现有基线方法，显著缩小了与完全使用云LLM之间的性能差距，同时有效控制了对云LLM的调用次数。

Conclusion: 该框架能更精准地判断任务卸载时机，提升了设备-云协作效率，为轻量级本地推理与高性能云端模型的平衡提供了有效解决方案。

Abstract: Device-cloud collaboration has emerged as a promising paradigm for deploying
large language models (LLMs), combining the efficiency of lightweight on-device
inference with the superior performance of powerful cloud LLMs. An essential
problem in this scenario lies in deciding whether a given query is best handled
locally or delegated to the cloud. Existing approaches typically rely on
external routers, implemented as binary classifiers, which often struggle to
determine task difficulty from the prompt's surface pattern. To address these
limitations, we propose a framework where the on-device LLM makes routing
decisions at the end of its solving process, with this capability instilled
through post-training. In particular, we formulate a reward maximization
problem with carefully designed rewards that encourage effective problem
solving and judicious offloading to the cloud. To solve this problem, we
develop a group-adaptive policy gradient algorithm, featuring a group-level
policy gradient, designed to yield an unbiased gradient estimator of the
reward, and adaptive prompt filtering, developed to enforce the constraint on
cloud LLM usage. Extensive experiments across models and benchmarks show that
the proposed methodology consistently outperforms existing baselines and
significantly narrows the gap to full cloud LLM performance.

</details>


### [757] [On The Variability of Concept Activation Vectors](https://arxiv.org/abs/2509.24058)
*Julia Wenkmann,Damien Garreau*

Main category: cs.LG

TL;DR: 本文对概念激活向量（CAVs）的构建进行了细粒度的理论分析，量化了其因随机采样带来的变异性，并发现CAVs的方差随样本数N以1/N的速度下降，据此提出了资源高效的应用建议。


<details>
  <summary>Details</summary>
Motivation: 提高人工智能模型的透明度是当前AI领域的重要挑战，概念-based解释方法（如CAVs）虽有前景，但其因随机采样导致结果不一致，需深入分析其变异性。

Method: 通过理论分析和在多个真实数据集上的实验，研究CAVs构建过程中随机采样带来的方差变化规律。

Result: 发现CAVs的方差随随机样本数量N的增加以1/N的速度减小，具有普适性。

Conclusion: CAVs的变异性可通过增加样本数有效控制，建议在实际应用中根据资源合理选择样本数量以提升稳定性。

Abstract: One of the most pressing challenges in artificial intelligence is to make
models more transparent to their users. Recently, explainable artificial
intelligence has come up with numerous method to tackle this challenge. A
promising avenue is to use concept-based explanations, that is, high-level
concepts instead of plain feature importance score. Among this class of
methods, Concept Activation vectors (CAVs), Kim et al. (2018) stands out as one
of the main protagonists. One interesting aspect of CAVs is that their
computation requires sampling random examples in the train set. Therefore, the
actual vectors obtained may vary from user to user depending on the randomness
of this sampling. In this paper, we propose a fine-grained theoretical analysis
of CAVs construction in order to quantify their variability. Our results,
confirmed by experiments on several real-life datasets, point out towards an
universal result: the variance of CAVs decreases as $1/N$, where $N$ is the
number of random examples. Based on this we give practical recommendations for
a resource-efficient application of the method.

</details>


### [758] [In-Context Compositional Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2509.24067)
*Qiushui Xu,Yuhao Huang,Yushu Jiang,Lei Song,Jinyu Wang,Wenliang Zheng,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了一种新的离线强化学习框架ICQL，通过将Q学习形式化为上下文推断问题，利用线性Transformer从检索到的转换中自适应地推断局部Q函数，无需显式子任务标签，在多个任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一全局Q函数，难以捕捉包含多样化子任务的任务的组合性质，因此需要一种能更好处理任务组合性的Q函数估计方法。

Method: 提出In-context Compositional Q-Learning (ICQL)，使用线性Transformer将Q学习视为上下文推断问题，从检索到的转换中自适应推断局部Q函数，无需显式子任务标签，并在理论上证明其具有有界逼近误差和近似最优策略提取能力。

Result: 在厨房任务中性能提升最高达16.4%，在Gym和Adroit任务中分别提升8.6%和6.3%，验证了ICQL在离线设置下的有效性。

Conclusion: ICQL是一种原理清晰且有效的离线强化学习框架，展示了上下文学习在鲁棒且组合式的值估计中的潜力。

Abstract: Accurately estimating the Q-function is a central challenge in offline
reinforcement learning. However, existing approaches often rely on a single
global Q-function, which struggles to capture the compositional nature of tasks
involving diverse subtasks. We propose In-context Compositional Q-Learning
(\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a
contextual inference problem, using linear Transformers to adaptively infer
local Q-functions from retrieved transitions without explicit subtask labels.
Theoretically, we show that under two assumptions--linear approximability of
the local Q-function and accurate weight inference from retrieved
context--\texttt{ICQL} achieves bounded Q-function approximation error, and
supports near-optimal policy extraction. Empirically, \texttt{ICQL}
substantially improves performance in offline settings: improving performance
in kitchen tasks by up to 16.4\%, and in Gym and Adroit tasks by up to 8.6\%
and 6.3\%. These results highlight the underexplored potential of in-context
learning for robust and compositional value estimation, positioning
\texttt{ICQL} as a principled and effective framework for offline RL.

</details>


### [759] [A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture](https://arxiv.org/abs/2509.24068)
*Roussel Rahman,Jeff Shrager*

Main category: cs.LG

TL;DR: 本文将策略选择理论（SCT）重构为基于神经网络架构的“小数学模型”（SMM），引入计数练习、符号嵌入和门控注意力机制，模拟儿童算术学习中的策略波动与干扰现象，并计划扩展至策略自适应选择与发现，为数学推理能力在类LLM代理中的发展提供统一研究平台。


<details>
  <summary>Details</summary>
Motivation: 旨在将经典的策略选择理论现代化，结合当前神经网络与大语言模型的思想，以更好地解释儿童算术学习过程，并为未来数学认知建模提供可扩展的计算框架。

Method: 采用类大语言模型的神经网络架构构建‘小数学模型’（SMM），整合计数实践、数字符号嵌入和 gated attention 机制，复现 SCT 中的策略动态（如波浪式手指计数使用）及加法与计数间的干扰效应。

Result: SMM 成功再现了计数与加法之间的建设性和破坏性干扰，以及随着加法记忆提升，手指计数呈现‘波浪式’使用的现象，验证了模型对原 SCT 核心特征的继承与扩展。

Conclusion: SMM 为策略选择理论提供了新的计算实现路径，不仅增强了对早期数学学习的模拟能力，也为未来在 LLM 框架下研究策略自适应与发现奠定了基础，推动数学认知发展的统一建模。

Abstract: Strategy Choice Theory (SCT)\footnote{``Strategy Choice Theory'',
``Distributions of Associations'', and ``Overlapping Wave Theory'' have been
used to refer to this line of work, emphasizing different
aspects.}\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}
explains important aspects of children's arithmetic learning based upon
principles including learning from developmentally naturalistic data,
probabilistic representation, confidence-based retrieval, and the phase-like
importance of scaffolding strategies, such as finger-counting. Here we recast
SCT as a ``Small Math Model'' (SMM), employing a neural-network-based
architecture analogous to LLMs. The SMM extends SCT to include counting
practice\footnote{The original SCT model was pre-biased in accordance with the
supposed experience of counting.}, symbol (number) embedding, and gated
attention. Similar to earlier work, the SMM demonstrates constructive and
destructive interference between counting and addition, and the ``wave-like''
use of finger-counting as sum recall improves. We plan to extend the SMM to
later aspects of the decades-long SCT program, including adaptive strategy
choice and eventually strategy discovery, providing a unified platform to
investigate the understanding of numerical characteristics and relationships
essential for mathematical reasoning -- as it can emerge in LLM-based agents.

</details>


### [760] [AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring](https://arxiv.org/abs/2509.24069)
*Youssef Sabiri,Walid Houmaidi,Ouail El Maadi,Yousra Chtouki*

Main category: cs.LG

TL;DR: 本文介绍了AQUAIR，一个公开的室内环境质量数据集，记录了摩洛哥某鱼类养殖设施内的空气温湿度、二氧化碳、挥发性有机物和颗粒物浓度等六项指标，采样频率为每5分钟一次，时间跨度超过三个月，包含2.3万多个观测数据。该数据集经过质量控制，可用于短时预测、事件检测和传感器漂移研究，填补了智能水产养殖领域头空环境数据的空白。


<details>
  <summary>Details</summary>
Motivation: 智能水产养殖系统依赖于丰富的环境数据流来保障鱼类福利、优化投喂并降低能耗，但目前关于室内养殖池上方空气环境的公开数据集十分稀缺，限制了结合气相条件与水质动态的预测和异常检测工具的发展。因此，亟需一个高质量、公开的室内环境数据集以推动相关研究。

Method: 在摩洛哥阿姆加斯的一处水产养殖设施中，使用一台Awair HOME监测设备以每5分钟一次的频率采集六项室内环境质量（IEQ）参数，包括空气温度、相对湿度、二氧化碳、总挥发性有机化合物、PM2.5和PM10。数据采集时间为2024年10月14日至2025年1月9日。传感器安装符合ISO标准高度，并通过参考仪器进行校准验证。作者还开发了一个开源处理流程，用于时间戳归一化、填补短时数据缺失，并生成可供分析的数据表。

Result: 获得了超过23,000个带时间戳的质量控制后的观测数据。探索性统计显示环境条件总体稳定（中位CO2浓度为758 ppm，PM2.5为12 μg/m³），但在投喂时段出现明显峰值，数据具有丰富的结构特征，适合用于短时预测、事件检测和传感器漂移分析。数据已公开存档于Figshare平台。

Conclusion: AQUAIR数据集填补了智能水产养殖信息学中关于养殖头上方空气环境数据的空白，为数据驱动的机器学习教学和循环水养殖系统中头空环境动态的传感研究提供了可复现的基准数据集。

Abstract: Smart aquaculture systems depend on rich environmental data streams to
protect fish welfare, optimize feeding, and reduce energy use. Yet public
datasets that describe the air surrounding indoor tanks remain scarce, limiting
the development of forecasting and anomaly-detection tools that couple
head-space conditions with water-quality dynamics. We therefore introduce
AQUAIR, an open-access public dataset that logs six Indoor Environmental
Quality (IEQ) variables--air temperature, relative humidity, carbon dioxide,
total volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture
facility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every
five minutes from 14 October 2024 to 9 January 2025, producing more than 23,000
time-stamped observations that are fully quality-controlled and publicly
archived on Figshare. We describe the sensor placement, ISO-compliant mounting
height, calibration checks against reference instruments, and an open-source
processing pipeline that normalizes timestamps, interpolates short gaps, and
exports analysis-ready tables. Exploratory statistics show stable conditions
(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time
peaks, offering rich structure for short-horizon forecasting, event detection,
and sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture
informatics and provides a reproducible benchmark for data-centric machine
learning curricula and environmental sensing research focused on head-space
dynamics in recirculating aquaculture systems.

</details>


### [761] [A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks](https://arxiv.org/abs/2509.24076)
*Bo Hu,José C. Príncipe*

Main category: cs.LG

TL;DR: 本文提出了一种结合Mixture Density Networks（MDN）与对比损失的方法，利用四种基于核的矩阵代价函数进行数据密度估计，以学习定义混合密度所需的多个中心。


<details>
  <summary>Details</summary>
Motivation: 为了提升自监督和对比特征学习中对数据分布的建模能力，需要有效的密度估计方法。传统MDN虽能生成多中心高斯混合模型，但缺乏与对比学习目标的有效结合。

Method: 将MDN与基于成对距离的对比损失相结合，提出了四种核化矩阵代价函数：标量代价、向量-矩阵代价、矩阵-矩阵代价（Schur补的迹）和SVD代价（核范数），用于优化混合密度网络中的中心学习。

Result: 所提出的四种核化矩阵代价能够有效支持MDN在密度估计任务中的训练，并提升了在对比学习框架下的特征表示能力。

Conclusion: 通过引入新的矩阵形式的对比代价函数，该方法成功地将对比学习与混合密度估计相结合，在理论上和实验上验证了其在学习多中心特征表示方面的有效性。

Abstract: Pairwise distance-based costs are crucial for self-supervised and contrastive
feature learning. Mixture Density Networks (MDNs) are a widely used approach
for generative models and density approximation, using neural networks to
produce multiple centers that define a Gaussian mixture. By combining MDNs with
contrastive costs, this paper proposes data density approximation using four
types of kernelized matrix costs: the scalar cost, the vector-matrix cost, the
matrix-matrix cost (the trace of Schur complement), and the SVD cost (the
nuclear norm), for learning multiple centers required to define a mixture
density.

</details>


### [762] [Demographic-Agnostic Fairness without Harm](https://arxiv.org/abs/2509.24077)
*Zhongteng Cai,Mohammad Mahdi Khalili,Xueru Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种无需依赖人口统计信息的公平性优化算法DAFH，通过联合学习群体分类器和解耦分类器，在不损害准确率的前提下实现偏好型公平。


<details>
  <summary>Details</summary>
Motivation: 现有的基于平等的公平性方法往往以牺牲模型准确性为代价，且多数偏好型公平方法依赖于可获取的人口统计信息，限制了其在现实场景中的应用。因此，需要一种在无人口统计信息情况下仍能保证公平且不降低准确性的新方法。

Method: 提出了一种名为DAFH（Demographic-Agnostic Fairness without Harm）的新型优化算法，该算法同时学习一个将人群划分为多个群体的分类器以及与这些群体相关联的一组解耦分类器，并在理论层面进行了样本复杂度分析。

Result: 理论分析表明，所提方法在已知人口统计信息并用于训练解耦分类器的情况下优于基线方法；在合成数据和真实数据上的实验验证了该方法的有效性。

Conclusion: DAFH能够在无需访问个体人口统计信息的情况下实现偏好型公平，同时保持甚至提升模型准确性，为高风险领域中的公平机器学习提供了可行方案。

Abstract: As machine learning (ML) algorithms are increasingly used in social domains
to make predictions about humans, there is a growing concern that these
algorithms may exhibit biases against certain social groups. Numerous notions
of fairness have been proposed in the literature to measure the unfairness of
ML. Among them, one class that receives the most attention is
\textit{parity-based}, i.e., achieving fairness by equalizing treatment or
outcomes for different social groups. However, achieving parity-based fairness
often comes at the cost of lowering model accuracy and is undesirable for many
high-stakes domains like healthcare. To avoid inferior accuracy, a line of
research focuses on \textit{preference-based} fairness, under which any group
of individuals would experience the highest accuracy and collectively prefer
the ML outcomes assigned to them if they were given the choice between various
sets of outcomes. However, these works assume individual demographic
information is known and fully accessible during training. In this paper, we
relax this requirement and propose a novel \textit{demographic-agnostic
fairness without harm (DAFH)} optimization algorithm, which jointly learns a
group classifier that partitions the population into multiple groups and a set
of decoupled classifiers associated with these groups. Theoretically, we
conduct sample complexity analysis and show that our method can outperform the
baselines when demographic information is known and used to train decoupled
classifiers. Experiments on both synthetic and real data validate the proposed
method.

</details>


### [763] [PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM](https://arxiv.org/abs/2509.24085)
*Ju-Hyung Lee,Yanqing Lu,Klaus Doppler*

Main category: cs.LG

TL;DR: PEARL 是一个利用设备间状态进行Wi-Fi Aware参数选择的协作式跨层优化框架，通过上下文感知奖励和轻量级变体实现低延迟、节能的设备端LLM控制。


<details>
  <summary>Details</summary>
Motivation: 为了提升设备到设备通信中的跨层优化效率，尤其是在资源受限的情况下，需要一种能够结合发布者和订阅者状态并自适应调整无线参数的智能框架。

Method: 提出 PEARL 框架，利用发布者与订阅者状态指导 Wi-Fi Aware 参数选择；设计上下文感知奖励函数，结合应用延迟容忍度和设备电池状态进行KL散度微调；研究两种轻量级变体：PEARL（Head+LoRA）和 PEARL-Lite（仅Head）。

Result: 在基于真实测量的合成场景中，PEARL 在目标得分上优于启发式和紧凑模型基线，并在合作低电量情况下最多减少16%的能耗；PEARL-Lite 实现了低于20毫秒的推理延迟且性能接近完整模型。

Conclusion: 实验结果表明，对等设备感知的上下文信息、奖励对齐的训练方法以及基于头部的高效架构使大语言模型在始终在线的设备端跨层控制中变得可行且高效。

Abstract: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a
framework for cooperative cross-layer optimization in device-to-device (D2D)
communication. Building on our previous work on single-device on-device LLMs,
PEARL extends the paradigm by leveraging both publisher and subscriber states
to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which
normalizes latency by application tolerances and modulates energy by device
battery states, provides richer supervision for KL-based finetuning. We study
two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves
the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms
inference at near-identical objective scores. Across synthetic scenarios
grounded in real measurements, PEARL improves objective scores over heuristic
and compact model baselines and reduces energy by up to 16% in cooperative
low-battery cases. These results demonstrate that peer-aware context,
reward-aligned training, and head-based efficiency make LLMs practical for
always-on, on-device cross-layer control.

</details>


### [764] [ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs](https://arxiv.org/abs/2509.24115)
*Evan Dramko,Yihuang Xiong,Yizhi Zhu,Geoffroy Hautier,Thomas Reps,Christopher Jermaine,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的机器学习力场ADAPT，用于加速点缺陷材料的第一性原理结构弛豫计算，在硅点缺陷数据集上显著降低了能量和力的预测误差，且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图神经网络的机器学习力场在建模点缺陷时存在过平滑和长程相互作用表征不足的问题，限制了其精度和效率。

Method: 提出ADAPT模型，采用直接坐标空间表示取代图结构，将原子视为Token，利用Transformer编码器显式建模所有原子对之间的相互作用。

Result: 在硅点缺陷数据集上，ADAPT相比最先进的GNN模型在力和能量预测误差上均降低了约33%，同时计算成本显著降低。

Conclusion: ADAPT通过避免图表示和显式建模原子间相互作用，在点缺陷体系中实现了更高精度和更高效的结构弛豫，为大规模材料模拟提供了新工具。

Abstract: Point defects play a central role in driving the properties of materials.
First-principles methods are widely used to compute defect energetics and
structures, including at scale for high-throughput defect databases. However,
these methods are computationally expensive, making machine-learning force
fields (MLFFs) an attractive alternative for accelerating structural
relaxations. Most existing MLFFs are based on graph neural networks (GNNs),
which can suffer from oversmoothing and poor representation of long-range
interactions. Both of these issues are especially of concern when modeling
point defects. To address these challenges, we introduce the Accelerated Deep
Atomic Potential Transformer (ADAPT), an MLFF that replaces graph
representations with a direct coordinates-in-space formulation and explicitly
considers all pairwise atomic interactions. Atoms are treated as tokens, with a
Transformer encoder modeling their interactions. Applied to a dataset of
silicon point defects, ADAPT achieves a roughly 33 percent reduction in both
force and energy prediction errors relative to a state-of-the-art GNN-based
model, while requiring only a fraction of the computational cost.

</details>


### [765] [GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries](https://arxiv.org/abs/2509.24117)
*Sifan Wang,Zhikai Wu,David van Dijk,Lu Lu*

Main category: cs.LG

TL;DR: 提出了一种用于复杂几何形状上逆问题的几何扩散模型框架GeoFunFlow，结合了几何函数自编码器和潜在扩散模型，实现了高精度重建、校准的不确定性量化和高效推断。


<details>
  <summary>Details</summary>
Motivation: 解决由偏微分方程控制的逆问题在不规则几何形状下的挑战，包括不适定性、数据稀疏性和计算成本高的问题。

Method: 提出GeoFunFlow框架，包含一个基于Perceiver模块的几何函数自编码器（GeoFAE）用于处理非结构化网格，并结合潜空间中的扩散模型通过修正流进行训练，以实现从稀疏和噪声数据中采样后验分布。

Result: 在五个基准测试中，GeoFunFlow在复杂几何形状上实现了最先进的重构精度，提供了校准的不确定性量化，并且相比算子学习和扩散模型基线方法具有更高效的推理能力。

Conclusion: GeoFunFlow为复杂几何上的PDE约束逆问题提供了一个高效、准确且可扩展的解决方案，尤其适用于需要多次后验采样的实际应用场景。

Abstract: Inverse problems governed by partial differential equations (PDEs) are
crucial in science and engineering. They are particularly challenging due to
ill-posedness, data sparsity, and the added complexity of irregular geometries.
Classical PDE-constrained optimization methods are computationally expensive,
especially when repeated posterior sampling is required. Learning-based
approaches improve efficiency and scalability, yet most are designed for
regular domains or focus on forward modeling. Here, we introduce {\em
GeoFunFlow}, a geometric diffusion model framework for inverse problems on
complex geometries. GeoFunFlow combines a novel geometric function autoencoder
(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE
employs a Perceiver module to process unstructured meshes of varying sizes and
produces continuous reconstructions of physical fields, while the diffusion
model enables posterior sampling from sparse and noisy data. Across five
benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over
complex geometries, provides calibrated uncertainty quantification, and
delivers efficient inference compared to operator-learning and diffusion model
baselines.

</details>


### [766] [HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning](https://arxiv.org/abs/2509.24118)
*Md Mozaharul Mottalib,Thao-Ly T. Phan,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 提出了一种名为HyMaTE的混合Mamba和Transformer模型，用于电子健康记录（EHR）表征学习，结合了状态空间模型的高效性和Transformer的注意力机制，在多个临床数据集上验证了其有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在处理长序列、稀疏且多变量的EHR数据时存在计算复杂度高和上下文长度受限的问题，而现有的状态空间模型（如Mamba）主要关注序列级信息混合，忽视通道级特征建模。

Method: 设计HyMaTE模型，融合Mamba的线性时间序列建模能力和Transformer的自注意力机制，专门针对纵向EHR数据进行优化，兼顾效率与表征能力。

Result: 在多个临床预测任务中，HyMaTE优于现有模型，能够捕捉更丰富、更精细的EHR数据统一表征，并通过自注意力机制提供良好的结果可解释性。

Conclusion: HyMaTE是一种可扩展、可泛化的EHR表征学习解决方案，兼具高效性与强表达能力，适用于真实世界医疗场景。

Abstract: Electronic health Records (EHRs) have become a cornerstone in modern-day
healthcare. They are a crucial part for analyzing the progression of patient
health; however, their complexity, characterized by long, multivariate
sequences, sparsity, and missing values poses significant challenges in
traditional deep learning modeling. While Transformer-based models have
demonstrated success in modeling EHR data and predicting clinical outcomes,
their quadratic computational complexity and limited context length hinder
their efficiency and practical applications. On the other hand, State Space
Models (SSMs) like Mamba present a promising alternative offering linear-time
sequence modeling and improved efficiency for handling long sequences, but
focus mostly on mixing sequence-level information rather than channel-level
data. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and
Transformer Model for EHR Representation Learning), a novel hybrid model
tailored for representing longitudinal data, combining the strengths of SSMs
with advanced attention mechanisms. By testing the model on predictive tasks on
multiple clinical datasets, we demonstrate HyMaTE's ability to capture an
effective, richer, and more nuanced unified representation of EHR data.
Additionally, the interpretability of the outcomes achieved by self-attention
illustrates the effectiveness of our model as a scalable and generalizable
solution for real-world healthcare applications. Codes are available at:
https://github.com/healthylaife/HyMaTE.

</details>


### [767] [Echo Flow Networks](https://arxiv.org/abs/2509.24122)
*Hongbo Liu,Jia Xu*

Main category: cs.LG

TL;DR: 本文提出了Echo Flow Networks (EFNs)，一种基于扩展回声状态网络（X-ESNs）和新型矩阵门控复合激活函数（MCRA）的高效时间序列预测框架，通过双流架构实现长期记忆特征选择，在多个基准数据集上实现了更小模型、更快训练和更低预测误差的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在处理长序列时间序列预测时面临计算复杂性与信息累积能力之间的权衡，而传统回声状态网络因非线性表达能力不足难以达到先进性能，因此需要一种既高效又具强表达能力的新架构。

Method: 提出Echo Flow Networks (EFNs)，包含多组带MLP读出层的扩展回声状态网络（X-ESNs），引入Matrix-Gated Composite Random Activation (MCRA) 增强神经元级动态表达能力，并设计双流架构使近期输入动态选择无限视野记忆中的关键特征。

Result: 在五个基准数据集（ETTh, ETTm, DMV, Weather, Air Quality）上，EFNs相比PatchTST等方法训练速度快4倍、模型小3倍，预测误差从43%降至35%，相对提升20%，其中变体EchoFormer取得一致的最先进性能。

Conclusion: EFNs在保持回声状态网络高效特性的基础上，显著提升了表达能力和长期稳定性，为高效且高性能的时间序列预测提供了新的可行路径。

Abstract: At the heart of time-series forecasting (TSF) lies a fundamental challenge:
how can models efficiently and effectively capture long-range temporal
dependencies across ever-growing sequences? While deep learning has brought
notable progress, conventional architectures often face a trade-off between
computational complexity and their ability to retain accumulative information
over extended horizons.
  Echo State Networks (ESNs), a class of reservoir computing models, have
recently regained attention for their exceptional efficiency, offering constant
memory usage and per-step training complexity regardless of input length. This
makes them particularly attractive for modeling extremely long-term event
history in TSF. However, traditional ESNs fall short of state-of-the-art
performance due to their limited nonlinear capacity, which constrains both
their expressiveness and stability.
  We introduce Echo Flow Networks (EFNs), a framework composed of a group of
extended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel
Matrix-Gated Composite Random Activation (MCRA), which enables complex,
neuron-specific temporal dynamics, significantly expanding the network's
representational capacity without compromising computational efficiency. In
addition, we propose a dual-stream architecture in which recent input history
dynamically selects signature reservoir features from an infinite-horizon
memory, leading to improved prediction accuracy and long-term stability.
  Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to
4x faster training and 3x smaller model size compared to leading methods like
PatchTST, reducing forecasting error from 43% to 35%, a 20% relative
improvement. One instantiation of our framework, EchoFormer, consistently
achieves new state-of-the-art performance across five benchmark datasets: ETTh,
ETTm, DMV, Weather, and Air Quality.

</details>


### [768] [The Impossibility of Inverse Permutation Learning in Transformer Models](https://arxiv.org/abs/2509.24125)
*Rohan Alur,Chris Hays,Manish Raghavan,Devavrat Shah*

Main category: cs.LG

TL;DR: 本文研究了在仅解码器Transformer中学习逆置换的问题，提出了一种不可能性结果：任意深度的仅解码器Transformer无法完成该任务，并给出了两种可行的学习构造方法。


<details>
  <summary>Details</summary>
Motivation: 逆置换学习建模了多种推理任务中的自然鲁棒性属性，如长上下文检索、多项选择问答和上下文学习，因此研究其在Transformer架构中的可学习性具有重要意义。

Method: 通过理论分析证明仅解码器Transformer无法学习逆置换任务，并提出两种替代构造：一是利用编码器-解码器结构的优势，二是通过在输入中添加“草稿标记”（scratch tokens）实现可行性。

Result: 证明了仅解码器Transformer在表达能力上无法完成逆置换学习；同时发现使用scratch tokens可以使其成为可能，揭示了链式思维提示（chain-of-thought prompting）等中间‘思考’标记可能通过非语义机制促进推理。

Conclusion: 仅解码器Transformer存在表达能力局限，而引入额外结构或标记（如scratch tokens）可突破这一限制，为理解大模型推理机制提供了新视角。

Abstract: In this technical note, we study the problem of inverse permutation learning
in decoder-only transformers. Given a permutation and a string to which that
permutation has been applied, the model is tasked with producing the original
(``canonical'') string. We argue that this task models a natural robustness
property across a variety of reasoning tasks, including long-context retrieval,
multiple choice QA and in-context learning. Our primary contribution is an
impossibility result: we show that an arbitrary depth, decoder-only transformer
cannot learn this task. This result concerns the expressive capacity of
decoder-only transformer models and is agnostic to training dynamics or sample
complexity. We give a pair of alternative constructions under which inverse
permutation learning is feasible. The first of these highlights the fundamental
role of the causal attention mask, and reveals a gap between the expressivity
of encoder-decoder transformers and the more popular decoder-only architecture.
The latter result is more surprising: we show that simply padding the input
with ``scratch tokens" yields a construction under which inverse permutation
learning is possible. We conjecture that this may suggest an alternative
mechanism by which chain-of-thought prompting or, more generally, intermediate
``thinking'' tokens can enable reasoning in large language models, even when
these tokens encode no meaningful semantic information (e.g., the results of
intermediate computations).

</details>


### [769] [A signal separation view of classification](https://arxiv.org/abs/2509.24140)
*H. N. Mhaskar,Ryan O'Dowd*

Main category: cs.LG

TL;DR: 提出一种基于局部三角多项式核的分类方法，能够在任意紧致度量空间中自动确定类别数并实现完美分类，且所需查询标签最少。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法多基于函数逼近，难以在复杂空间中有效分离类别，尤其是当类别边界接触或重叠时。因此需要一种新方法来更精确地分离不同类别的分布支持。

Method: 采用最初用于信号处理中点源分离的局部三角多项式核技术，将其应用于分类问题，通过MASC算法以层次化方式分离各类别对应的概率分布支持域。

Result: 在多个模拟和真实数据集（如Salinas、Indian Pines高光谱数据和文档数据集）上验证了该方法的有效性，能够准确估计类别数量并实现高精度分类。

Conclusion: 该方法为任意紧致度量空间中的分类问题提供了理论上有保证的新解决方案，能有效处理重叠或接触的类别边界，且标签需求最小。

Abstract: The problem of classification in machine learning has often been approached
in terms of function approximation. In this paper, we propose an alternative
approach for classification in arbitrary compact metric spaces which, in
theory, yields both the number of classes, and a perfect classification using a
minimal number of queried labels. Our approach uses localized trigonometric
polynomial kernels initially developed for the point source signal separation
problem in signal processing. Rather than point sources, we argue that the
various classes come from different probability distributions. The localized
kernel technique developed for separating point sources is then shown to
separate the supports of these distributions. This is done in a hierarchical
manner in our MASC algorithm to accommodate touching/overlapping class
boundaries. We illustrate our theory on several simulated and real life
datasets, including the Salinas and Indian Pines hyperspectral datasets and a
document dataset.

</details>


### [770] [Evaluation of Machine and Deep Learning Techniques for Cyclone Trajectory Regression and Status Classification by Time Series Data](https://arxiv.org/abs/2509.24146)
*Ethan Zachary Lo,Dan Chie-Tien Lo*

Main category: cs.LG

TL;DR: 本研究提出了一种基于机器学习的两阶段管道模型，用于热带气旋路径和状态预测，结合回归与分类方法，在精度和鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报模型计算成本高且易受大气系统混沌特性影响，难以准确预测气旋，因此需要更高效、准确的替代方法。

Method: 采用梯度提升回归模型预测气旋特征（如最大风速、最低气压、轨迹长度和方向变化），然后将结果输入随机森林、支持向量机和多层感知机等分类器预测气旋状态类别，并使用SMOTE处理类别不平衡问题。

Result: 随机森林分类器准确率达到93%，在精确率、召回率和F1分数上均优于SVM和MLP，尤其在识别少数类状态和减少误报方面表现突出；回归模型对气压和风速的预测平均绝对误差分别为约2.2 mb和2.4 kt。

Conclusion: 基于集成学习的机器学习模型可作为传统气象预报方法的有效、可扩展替代方案，具备实现实时气旋预测并集成到决策支持系统的潜力。

Abstract: Accurate cyclone forecasting is essential for minimizing loss of life,
infrastructure damage, and economic disruption. Traditional numerical weather
prediction models, though effective, are computationally intensive and prone to
error due to the chaotic nature of atmospheric systems. This study proposes a
machine learning (ML) approach to forecasting tropical cyclone trajectory and
status using time series data from the National Hurricane Center, including
recently added best track wind radii. A two-stage ML pipeline is developed: a
regression model first predicts cyclone features maximum wind speed, minimum
pressure, trajectory length, and directional change using a sliding window of
historical data. These outputs are then input into classification models to
predict the cyclone's categorical status. Gradient boosting regression and
three classifiers random forest (RF), support vector machine (SVM), and
multilayer perceptron (MLP) are evaluated. After hyperparameter tuning and
synthetic minority oversampling (SMOTE), the RF classifier achieves the highest
performance with 93% accuracy, outperforming SVM and MLP across precision,
recall, and F1 score. The RF model is particularly robust in identifying
minority cyclone statuses and minimizing false negatives. Regression results
yield low mean absolute errors, with pressure and wind predictions within about
2.2 mb and 2.4 kt, respectively. These findings demonstrate that ML models,
especially ensemble-based classifiers, offer an effective, scalable alternative
to traditional forecasting methods, with potential for real-time cyclone
prediction and integration into decision support systems.

</details>


### [771] [Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs](https://arxiv.org/abs/2509.24166)
*Arpit Garg,Hemanth Saratchandran,Ravi Garg,Simon Lucey*

Main category: cs.LG

TL;DR: 提出了一种名为Bounded Parameter-Efficient Unlearning的新方法，通过在MLP适配器上应用有界函数来稳定LoRA微调，解决了大语言模型中梯度差异法导致的权重和梯度无界增长问题，在多个基准测试中显著提升了遗忘效果同时保持了保留性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型机器遗忘方法（如梯度差异法）在使用交叉熵损失时会导致权重和梯度的无界增长，造成训练不稳定，影响遗忘和保留效果，亟需一种更稳定可靠的方法。

Method: 基于对梯度上升在遗忘集上破坏优化过程的理论分析，提出Bounded Parameter-Efficient Unlearning方法，通过对LoRA中的MLP适配器应用有界函数来控制权重动态，从而稳定梯度差异法的训练过程。

Result: 在TOFU、TDEC和MUSE等多个基准测试中，从125M到8B参数的不同架构和规模模型上，该方法均显著改善了遗忘效果并保持了良好的保留能力，表现出稳定可靠的收敛性。

Conclusion: Bounded Parameter-Efficient Unlearning为大语言模型提供了一个理论可解释且实际可扩展的高效遗忘框架，有效解决了传统梯度差异法的不稳定性问题。

Abstract: Machine unlearning in large language models (LLMs) is essential for privacy
and safety; however, existing approaches remain unstable and unreliable. A
widely used strategy, the gradient difference method, applies gradient descent
on retained data while performing gradient ascent on forget data, the data
whose influence should be removed. However, when combined with cross-entropy
loss, this procedure causes unbounded growth of weights and gradients, leading
to training instability and degrading both forgetting and retention. We provide
a theoretical framework that explains this failure, explicitly showing how
ascent on the forget set destabilizes optimization in the feedforward MLP
layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient
Unlearning, a parameter-efficient approach that stabilizes LoRA-based
fine-tuning by applying bounded functions to MLP adapters. This simple
modification controls the weight dynamics during ascent, enabling the gradient
difference method to converge reliably. Across the TOFU, TDEC, and MUSE
benchmarks, and across architectures and scales from 125M to 8B parameters, our
method achieves substantial improvements in forgetting while preserving
retention, establishing a novel theoretically grounded and practically scalable
framework for unlearning in LLMs.

</details>


### [772] [Multi-Scale Geometric Autoencoder](https://arxiv.org/abs/2509.24168)
*Qipeng Zhan,Zhuoping Zhou,Zexuan Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种多尺度几何自编码器（MAE），通过非对称结构同时保持数据的全局和局部几何特性，在合成流形和真实数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器方法通常只关注数据的全局或局部几何结构，难以兼顾两者，导致距离近似误差累积或大尺度关系失真。

Method: 提出多尺度几何自编码器（MAE），在编码器中引入全局距离约束，在解码器中引入局部几何约束，采用非对称架构同时保留数据的多尺度几何结构，并通过理论分析证明该设计符合编解码器的自然分工。

Result: 在合成流形和多个真实世界数据集上的实验表明，MAE在多种评估指标上 consistently 优于现有的自编码器方法。

Conclusion: MAE通过非对称的多尺度约束有效保留了数据的几何结构，提升了降维与可视化效果，为自编码器设计提供了新的有效范式。

Abstract: Autoencoders have emerged as powerful models for visualization and
dimensionality reduction based on the fundamental assumption that
high-dimensional data is generated from a low-dimensional manifold. A critical
challenge in autoencoder design is to preserve the geometric structure of data
in the latent space, with existing approaches typically focusing on either
global or local geometric properties separately. Global approaches often
encounter errors in distance approximation that accumulate, while local methods
frequently converge to suboptimal solutions that distort large-scale
relationships. We propose Multi-Scale Geometric Autoencoder (MAE), which
introduces an asymmetric architecture that simultaneously preserves both scales
of the geometric structure by applying global distance constraints to the
encoder and local geometric constraints to the decoder. Through theoretical
analysis, we establish that this asymmetric design aligns naturally with the
distinct roles of the encoder and decoder components. Our comprehensive
experiments on both synthetic manifolds and real-world datasets demonstrate
that MAE consistently outperforms existing methods across various evaluation
metrics.

</details>


### [773] [Model Correlation Detection via Random Selection Probing](https://arxiv.org/abs/2509.24171)
*Ruibo Chen,Sheng Zhang,Yihan Wu,Tong Zheng,Peihua Mai,Heng Huang*

Main category: cs.LG

TL;DR: 提出了一种名为随机选择探测（RSP）的假设检验框架，用于检测大语言模型和视觉语言模型之间的相关性，通过生成严格的p值来量化相关性证据，具有高鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性的方法通常需要访问模型参数或产生无原则阈值的启发式分数，限制了其适用性，因此需要一种更可靠、无需参数访问且具有统计基础的模型相关性检测方法。

Method: 提出随机选择探测（RSP），在参考模型上优化文本或视觉前缀以执行随机选择任务，并评估这些前缀在目标模型上的可迁移性，结合无关基线模型过滤通用特征，从而生成具有统计意义的p值。

Result: 实验表明，RSP在细调和开源模型上对相关模型 consistently 产生小p值，对无关模型保持高p值，且在多种访问条件下均表现稳健，显著降低假阳性率。

Conclusion: RSP是首个原理性强且通用的模型相关性检测统计框架，为现代机器学习系统中的透明和可解释决策提供了有效工具。

Abstract: The growing prevalence of large language models (LLMs) and vision-language
models (VLMs) has heightened the need for reliable techniques to determine
whether a model has been fine-tuned from or is even identical to another.
Existing similarity-based methods often require access to model parameters or
produce heuristic scores without principled thresholds, limiting their
applicability. We introduce Random Selection Probing (RSP), a
hypothesis-testing framework that formulates model correlation detection as a
statistical test. RSP optimizes textual or visual prefixes on a reference model
for a random selection task and evaluates their transferability to a target
model, producing rigorous p-values that quantify evidence of correlation. To
mitigate false positives, RSP incorporates an unrelated baseline model to
filter out generic, transferable features. We evaluate RSP across both LLMs and
VLMs under diverse access conditions for reference models and test models.
Experiments on fine-tuned and open-source models show that RSP consistently
yields small p-values for related models while maintaining high p-values for
unrelated ones. Extensive ablation studies further demonstrate the robustness
of RSP. These results establish RSP as the first principled and general
statistical framework for model correlation detection, enabling transparent and
interpretable decisions in modern machine learning ecosystems.

</details>


### [774] [FM-FoG: A Real-Time Foundation Model-based Wearable System for Freezing-of-Gait Mitigation](https://arxiv.org/abs/2509.24176)
*Chuntian Chi,John Clapham,Leslie Cloud,Ingrid Pretzer-Aboff,GinaMari Blackwell,Huajie Shao,Gang Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于基础模型的实时可穿戴系统FM-FoG，用于在未见过的帕金森病患者中实现无需患者特异性训练的冻结步态（FoG）检测，具有高F1分数、低延迟和节能优势。


<details>
  <summary>Details</summary>
Motivation: 现有的FoG检测系统依赖大量患者特定训练数据且泛化能力差，限制了其临床应用。

Method: 采用自监督预训练结合传感器上下文整合，并利用轻量级CNN-LSTM活动分类器仅在行走或站立时激活基础模型，以减少计算开销。

Result: 在VCU FoG-IMU数据集上，对未见患者测试达到98.5%的F1分数，相比基线方法显著提升；部署于手机端可延长电池寿命达72%，并保持低于20ms的干预延迟。

Conclusion: FM-FoG能够在无需个体训练的情况下跨患者泛化，具备临床实用性和能效优势，适用于实际医疗场景。

Abstract: Freezing-of-Gait (FoG) affects over 50% of mid-to-late stage Parkinson's
disease (PD) patients, significantly impairing patients' mobility independence
and reducing quality of life. FoG is characterized by sudden episodes where
walking cannot start or is interrupted, occurring exclusively during standing
or walking, and never while sitting or lying down. Current FoG detection
systems require extensive patient-specific training data and lack
generalization, limiting clinical deployment. To address these issues, we
introduce FM-FoG, a real-time foundation model-based wearable system achieving
FoG detection in unseen patients without patient-specific training. Our
approach combines self-supervised pretraining on diverse Inertial Measurement
Unit (IMU) datasets with sensor context integration. Since FoG occurs only
during ambulatory activities, a lightweight CNN-LSTM activity classifier
selectively activates the foundation model only during walking or standing,
avoiding unnecessary computation. Evaluated on the VCU FoG-IMU dataset with 23
PD patients, FM-FoG achieves a 98.5% F1-score when tested on previously unseen
patients, substantially outperforming competitive baseline methods. Deployed on
a Google Pixel 8a smartphone, the system extends battery life by up to 72%
while maintaining sub-20ms intervention latency. The results indicate that our
FM-FoG can enable practical, energy-efficient healthcare applications that
generalize across patients without individual training requirements.

</details>


### [775] [Negative Pre-activations Differentiate Syntax](https://arxiv.org/abs/2509.24198)
*Linghao Kong,Angelina Ning,Micah Adler,Nir Shavit*

Main category: cs.LG

TL;DR: Wasserstein神经元在大语言模型中通过负预激活空间的稀疏分化机制对语法结构起关键作用，尽管数量极少，但其特异性干预会显著破坏模型的语法行为。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型中为何少量特定神经元（Wasserstein神经元）对模型功能至关重要，尤其是它们在区分相似输入和语法处理中的作用。

Method: 分析Wasserstein神经元在平滑激活函数前的负预激活行为，进行符号特异性干预（仅清零负预激活），并与随机和困惑度匹配对照比较，结合词性分析和逐层干预研究其功能。

Result: 发现Wasserstein神经元在早期层中通过将相似句法词（如冠词、介词）映射到显著不同的负预激活值来实现分化；清零这些负值会严重损害语法性能，而控制组无此效应；该影响随训练进程与神经元稳定同步出现，且局部损伤随网络深度累积。

Conclusion: 负预激活空间中的稀疏分化是语言模型实现语法处理的关键机制，Wasserstein神经元在此过程中扮演不可替代的角色。

Abstract: A recently discovered class of entangled neurons, known as Wasserstein
neurons, is disproportionately critical in large language models despite
constituting only a very small fraction of the network: their targeted removal
collapses the model, consistent with their unique role in differentiating
similar inputs. Interestingly, in Wasserstein neurons immediately preceding
smooth activation functions, such differentiation manifests in the negative
pre-activation space, especially in early layers. Pairs of similar inputs are
driven to highly distinct negative values, and these pairs involve syntactic
tokens such as determiners and prepositions. We show that this negative region
is functional rather than simply favorable for optimization. A minimal,
sign-specific intervention that zeroes only the negative pre-activations of a
small subset of entangled neurons significantly weakens overall model function
and disrupts grammatical behavior, while both random and perplexity-matched
controls leave grammatical performance largely unchanged. Part of speech
analysis localizes the excess surprisal to syntactic scaffolding tokens, and
layer-specific interventions reveal that small local degradations accumulate
across depth. Over training checkpoints, the same ablation impairs grammatical
behavior as Wasserstein neurons emerge and stabilize. Together, these results
identify negative differentiation in a sparse subset of entangled neurons as a
crucial mechanism that language models rely on for syntax.

</details>


### [776] [Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends](https://arxiv.org/abs/2509.24203)
*Chaorui Yao,Yanxi Chen,Yuchang Sun,Yushuo Chen,Wenhao Zhang,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 本文提出了一种基于REINFORCE的off-policy强化学习新视角，通过第一性原理推导出群相对REINFORCE可自然解释为off-policy算法，并提出了正则化策略更新和主动调整数据分布两大原则，统一解释了OPMD和AsymRE等近期算法，为大语言模型的off-policy RL提供了理论基础与实用设计指导。


<details>
  <summary>Details</summary>
Motivation: 由于实际应用中的限制、LLM-RL基础设施的复杂性以及对强化学习方法进一步创新的需求，针对大语言模型的off-policy强化学习受到越来越多关注。而传统上REINFORCE及其变体被视为on-policy算法，难以适应off-policy场景，存在理论认知上的局限。

Method: 从第一性原理出发，推导不依赖特定数据分布假设的群相对REINFORCE公式，揭示其内在的off-policy解释；提出通过正则化策略更新和主动塑造数据分布两种机制来适配off-policy设置，并以此重新解释OPMD和AsymRE等算法。

Result: 澄清了重要性采样与截断在GRPO中的作用误区，统一并重新诠释了OPMD和AsymRE为REINFORCE损失的正则化形式，为数据加权策略提供了理论依据，并通过大量实验验证了所提方法的有效性。

Conclusion: REINFORCE类算法具备天然的off-policy潜力，通过适当的正则化和数据分布控制可在off-policy设置下有效运行，该工作为大语言模型的off-policy强化学习提供了新的理论视角和可操作的设计原则。

Abstract: Off-policy reinforcement learning (RL) for large language models (LLMs) is
attracting growing interest, driven by practical constraints in real-world
applications, the complexity of LLM-RL infrastructure, and the need for further
innovations of RL methodologies. While classic REINFORCE and its modern
variants like Group Relative Policy Optimization (GRPO) are typically regarded
as on-policy algorithms with limited tolerance of off-policyness, we present in
this work a first-principles derivation for group-relative REINFORCE without
assuming a specific training data distribution, showing that it admits a native
off-policy interpretation. This perspective yields two general principles for
adapting REINFORCE to off-policy settings: regularizing policy updates, and
actively shaping the data distribution. Our analysis demystifies some myths
about the roles of importance sampling and clipping in GRPO, unifies and
reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and
Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,
and offers theoretical justification for seemingly heuristic data-weighting
strategies. Our findings lead to actionable insights that are validated with
extensive empirical studies, and open up new opportunities for principled
algorithm design in off-policy RL for LLMs. Source code for this work is
available at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.

</details>


### [777] [MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2509.24217)
*Yuyang Sha,Hongxin Pan,Gang Luo,Caijuan Shi,Jing Wang,Kefeng Li*

Main category: cs.LG

TL;DR: 本研究提出了一种基于大语言模型的MDD诊断框架MDD-Thinker，结合监督微调和强化学习提升推理能力与可解释性，在真实大规模临床数据上表现出高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前抑郁症诊断依赖主观评估，缺乏整合多模态临床信息的能力，且现有大语言模型存在可解释性差、易产生幻觉等问题。

Method: 开发MDD-Thinker框架，结合监督微调（SFT）与强化学习（RL），在UK Biobank及公开心理健康数据集生成的5万条推理样本上进行训练，并评估其诊断与推理性能。

Result: MDD-Thinker在准确率（0.8268）、F1分数（0.8081）和AUC上显著优于传统模型和主流大模型，结合SFT与RL带来最高性能提升，且推理能力媲美更大模型的同时保持计算高效。

Conclusion: MDD-Thinker是首个在真实世界数据上训练的推理增强型LLM抑郁症诊断框架，平衡了准确性、可解释性与效率，为智能精神健康诊断提供了可扩展的解决方案。

Abstract: Background Major depressive disorder (MDD) is a leading cause of global
disability, yet current diagnostic approaches often rely on subjective
assessments and lack the ability to integrate multimodal clinical information.
Large language models (LLMs) hold promise for enhancing diagnostic accuracy
through advanced reasoning but face challenges in interpretability,
hallucination, and reliance on synthetic data.
  Methods We developed MDD-Thinker, an LLM-based diagnostic framework that
integrates supervised fine-tuning (SFT) with reinforcement learning (RL) to
strengthen reasoning ability and interpretability. Using the UK Biobank
dataset, we generated 40,000 reasoning samples, supplemented with 10,000
samples from publicly available mental health datasets. The model was
fine-tuned on these reasoning corpora, and its diagnostic and reasoning
performance was evaluated against machine learning, deep learning, and
state-of-the-art LLM baselines.
  Findings MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081,
significantly outperforming traditional baselines such as SVM and MLP, as well
as general-purpose LLMs. Incorporating both SFT and RL yielded the greatest
improvements, with relative gains of 29.0% in accuracy, 38.1% in F1-score, and
34.8% in AUC. Moreover, the model demonstrated comparable reasoning performance
compared to much larger LLMs, while maintaining computational efficiency.
  Interpretation This study presents the first reasoning-enhanced LLM framework
for MDD diagnosis trained on large-scale real-world clinical data. By
integrating SFT and RL, MDD-Thinker balances accuracy, interpretability, and
efficiency, offering a scalable approach for intelligent psychiatric
diagnostics. These findings suggest that reasoning-oriented LLMs can provide
clinically reliable support for MDD detection and may inform broader
applications in mental health care.

</details>


### [778] [CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TL;DR: 本文提出了一种名为CAOTE的新型缓存令牌驱逐方法，通过结合注意力分数和值向量来衡量令牌对注意力输出的贡献，从而优化驱逐误差。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力分数的令牌重要性度量方法缺乏对令牌在注意力输出中实际贡献的考量，导致在资源受限设备上的长上下文处理效率不高。

Method: 提出CAOTE方法，将注意力分数与值向量结合，在闭式解中首次引入值向量信息，并可作为元启发式方法与其他驱逐方法结合使用。

Result: 实验表明，CAOTE与最先进的基于注意力分数的方法结合时，在下游任务中始终提升准确率。

Conclusion: 利用值向量信息能更有效地评估令牌重要性，CAOTE为长上下文场景下的高效推理提供了更优的令牌驱逐策略。

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value tokens on top of
attention-based eviction scores in closed-form. Additionally, CAOTE can act as
a meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [779] [Conda: Column-Normalized Adam for Training Large Language Models Faster](https://arxiv.org/abs/2509.24218)
*Junjie Wang,Pan Zhou,Yiming Dong,Huan Li,Jia Li,Xun Zhou,Qicheng Lao,Cong Fang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种新的优化器Column-Normalized Adam (Conda)，结合了Adam的坐标级自适应性和全局谱归一化的优点，显著提升了大语言模型预训练的收敛速度，在LLaMA系列上比AdamW快2~2.5倍。


<details>
  <summary>Details</summary>
Motivation: Adam类优化器存在谱条件差和低秩结构问题，影响训练效率；Muon虽改善谱条件但缺乏坐标级自适应性，因此需要一种兼顾两者的优化方法。

Method: Conda将梯度更新投影到正交子空间，并基于投影梯度进行列方向的二阶矩归一化，实现谱条件改进的同时保持坐标级学习率自适应。

Result: 在LLaMA和GPT-2系列模型上，Conda在训练步数和时间上均比AdamW、Muon等基线方法收敛更快，速度提升达2~2.5倍，且在不同训练设置下具有鲁棒性。

Conclusion: Conda是一种高效且广泛适用的大规模大语言模型训练优化器，平衡了谱归一化与坐标自适应性的优势。

Abstract: Large language models (LLMs) have demonstrated impressive generalization and
emergent capabilities, yet their pre-training remains computationally expensive
and sensitive to optimization dynamics. While Adam-based optimizers offer fast
convergence by adapting learning rates coordinate-wise, recent studies reveal
that their updates often suffer from poor spectral conditioning and low-rank
structures, hindering efficiency. Muon addresses this issue via global spectral
normalization but lacks the per-coordinate adaptivity of Adam. In this work, we
propose \textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges
the strengths of both approaches. Conda projects updates into an orthogonal
subspace and applies column-wise second moment normalization based on the
projected gradients, thereby achieving both improved spectral conditioning and
maintaining coordinate-wise adaptivity. This design alleviates the spectral
pathologies of Adam while preserving its fast convergence behavior. Extensive
experiments on the LLaMA and GPT-2 series show that Conda consistently
outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on
the LLaMA series, \textbf{Conda achieves $2{\sim}2.5\times$ the convergence
speed of AdamW, measured in both training steps and training time.} Further
ablations demonstrate its robustness under diverse training setups. These
results collectively highlight Conda as an effective and broadly applicable
optimizer for large-scale LLM training. The code is released on
https://github.com/jie040109/Conda

</details>


### [780] [Semantic Editing with Coupled Stochastic Differential Equations](https://arxiv.org/abs/2509.24223)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 提出使用耦合随机微分方程（coupled SDEs）来引导预训练生成模型的采样过程，实现图像编辑，无需重新训练或辅助网络，保持高提示保真度和像素级一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法容易失真或引入伪影，难以在语义修改与细节保留之间取得平衡。

Method: 通过耦合随机微分方程（coupled SDEs），在生成过程中使用相同关联噪声同时驱动源图像和编辑图像，从而在保持视觉相似性的同时实现语义控制。

Result: 该方法即插即用，无需训练或额外网络，实现了高提示符合度和接近像素级的一致性，在多种预训练生成模型上表现优异。

Conclusion: coupled SDEs 是一种简单而强大的工具，适用于基于SDE的扩散模型和整流流模型，为可控生成AI提供了新思路。

Abstract: Editing the content of an image with a pretrained text-to-image model remains
challenging. Existing methods often distort fine details or introduce
unintended artifacts. We propose using coupled stochastic differential
equations (coupled SDEs) to guide the sampling process of any pre-trained
generative model that can be sampled by solving an SDE, including diffusion and
rectified flow models. By driving both the source image and the edited image
with the same correlated noise, our approach steers new samples toward the
desired semantics while preserving visual similarity to the source. The method
works out-of-the-box-without retraining or auxiliary networks-and achieves high
prompt fidelity along with near-pixel-level consistency. These results position
coupled SDEs as a simple yet powerful tool for controlled generative AI.

</details>


### [781] [Proposing a Framework for Machine Learning Adoption on Legacy Systems](https://arxiv.org/abs/2509.24224)
*Ashiqur Rahman,Hamed Alhoori*

Main category: cs.LG

TL;DR: 提出一种基于API的轻量级框架，将机器学习模型生命周期与生产环境解耦，通过浏览器界面为领域专家提供交互式ML能力，无需硬件升级且支持零停机维护，降低中小企业的采用门槛。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业中具有重要价值，但传统集成方式成本高、易造成生产中断，尤其对中小企业而言难以负担，亟需一种低成本、低风险的集成方案。

Method: 设计并实现一个基于API的框架，将机器学习模型生命周期与生产系统分离，通过轻量级浏览器界面提供人机协作的交互式模型控制，支持在线维护和参数调整。

Result: 该框架成功实现了机器学习功能的无缝集成，无需本地硬件升级，保障生产零中断，并提升了领域专家对模型的信任与操作便利性。

Conclusion: 该框架有效降低了机器学习在工业场景中的部署门槛，为制造业提供了可扩展、易访问的智能化路径，增强了企业竞争力。

Abstract: The integration of machine learning (ML) is critical for industrial
competitiveness, yet its adoption is frequently stalled by the prohibitive
costs and operational disruptions of upgrading legacy systems. The financial
and logistical overhead required to support the full ML lifecycle presents a
formidable barrier to widespread implementation, particularly for small and
medium-sized enterprises. This paper introduces a pragmatic, API-based
framework designed to overcome these challenges by strategically decoupling the
ML model lifecycle from the production environment. Our solution delivers the
analytical power of ML to domain experts through a lightweight, browser-based
interface, eliminating the need for local hardware upgrades and ensuring model
maintenance can occur with zero production downtime. This human-in-the-loop
approach empowers experts with interactive control over model parameters,
fostering trust and facilitating seamless integration into existing workflows.
By mitigating the primary financial and operational risks, this framework
offers a scalable and accessible pathway to enhance production quality and
safety, thereby strengthening the competitive advantage of the manufacturing
sector.

</details>


### [782] [Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms](https://arxiv.org/abs/2509.24228)
*Wei Wang,Dong-Dong Wu,Ming Li,Jingxiong Zhang,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了首个正例-无标签（PU）学习基准，以系统比较不同PU学习算法，并识别影响公平评估的关键因素，包括模型选择中负样本的使用问题以及单样本与双样本设置间的偏差，提出校准方法以实现更公平的比较。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习算法的实验设置不一致，且评估协议存在不现实和不公平的问题，难以准确判断算法优劣。

Method: 构建统一的PU学习基准，研究适用于PU场景的模型选择准则，并针对单样本设置中的内部标签偏移问题提出校准方法，以实现跨设置的公平比较。

Result: 揭示了当前PU学习评估中存在的关键问题，提出了有效的校准策略，并建立了更真实、公平的评估框架。

Conclusion: 所提出的基准和校准方法为未来PU学习算法的评估提供了可访问、现实且公平的环境。

Abstract: Positive-unlabeled (PU) learning is a weakly supervised binary classification
problem, in which the goal is to learn a binary classifier from only positive
and unlabeled data, without access to negative data. In recent years, many PU
learning algorithms have been developed to improve model performance. However,
experimental settings are highly inconsistent, making it difficult to identify
which algorithm performs better. In this paper, we propose the first PU
learning benchmark to systematically compare PU learning algorithms. During our
implementation, we identify subtle yet critical factors that affect the
realistic and fair evaluation of PU learning algorithms. On the one hand, many
PU learning algorithms rely on a validation set that includes negative data for
model selection. This is unrealistic in traditional PU learning settings, where
no negative data are available. To handle this problem, we systematically
investigate model selection criteria for PU learning. On the other hand, the
problem settings and solutions of PU learning have different families, i.e.,
the one-sample and two-sample settings. However, existing evaluation protocols
are heavily biased towards the one-sample setting and neglect the significant
difference between them. We identify the internal label shift problem of
unlabeled training data for the one-sample setting and propose a simple yet
effective calibration approach to ensure fair comparisons within and across
families. We hope our framework will provide an accessible, realistic, and fair
environment for evaluating PU learning algorithms in the future.

</details>


### [783] [ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2509.24239)
*Jincheng Liu,Sijun He,Jingjing Wu,Xiangsen Wang,Yang Chen,Zhaoqi Kuang,Siqi Bao,Yuan Yao*

Main category: cs.LG

TL;DR: 本文提出了一个名为ChessArena的测试平台，用于评估大语言模型（LLMs）在国际象棋中的战略推理能力，发现当前模型在复杂战略推理方面存在显著不足，即使微调后的Qwen3-8B也仅接近更大规模的先进推理模型。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否具备真正的复杂战略推理能力，还是仅仅依赖训练数据中的模式识别。

Method: 构建了一个名为ChessArena的竞争性测试框架，包含四种对战模式、排名算法和排行榜，评估了13个不同LLM在超过800场比赛中的表现，并测试其基本理解、走子选择和解题等细粒度能力。

Result: 实验结果显示，现有LLM无法击败人类业余水平的棋手Maia-1100，部分模型甚至输给随机走子的玩家；但经过微调的Qwen3-8B表现出显著提升，接近最先进的大型推理模型。

Conclusion: 当前大语言模型在复杂战略推理（如国际象棋）方面仍存在明显缺陷，主要依赖模式匹配而非真正推理，而针对性微调可显著提升其表现。

Abstract: Recent large language models (LLMs) have shown strong reasoning capabilities.
However, a critical question remains: do these models possess genuine reasoning
skills particularly complex strategic reasoning or are they primarily excelling
at sophisticated pattern recognition within their training data? To address
this question, this paper presents a chess testbed, ChessArena, to evaluate the
strategic reasoning capabilities of LLMs. Chess requires complex strategic
reasoning capabilities including long-term planning, strict rule comprehension,
and multi-turn conversation memorization. Specifically, ChessArena is a
competitive framework where LLMs play against each other, under four different
play modes. The testbed is equipped with a ranking algorithm and a leaderboard.
The testbed can also evaluate fine-grained capabilities including basic
understanding, move selection, and puzzle solving. Over 13 LLMs with different
modes are evaluated in ChessArena, playing over 800 games. The results reveal
significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess
engine at human amateur level), while some even failed to defeat a random
player that selects moves arbitrarily. We also present a strong baseline to the
testbed: our fine-tuned Qwen3-8B substantially improved performance,
approaching much larger state-of-the-art reasoning models.

</details>


### [784] [Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization](https://arxiv.org/abs/2509.24256)
*Yunhao Liang,Pujun Zhang,Yuan Qu,Shaochong Lin,Zuo-jun Max Shen*

Main category: cs.LG

TL;DR: 提出图基础模型（GFM），首次实现基于预训练范式解决图上所有基于距离的优化问题，通过随机游走路径的自监督学习，有效捕捉图的拓扑与组合结构，在多种任务中表现接近专用求解器但推理更快。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型成功的预训练迁移范式应用于运筹学中的图优化问题面临挑战，因语言的统计灵活性与图的严格组合约束存在冲突，需构建能理解图结构规则的基础模型。

Method: 提出图基础模型（GFM），在图中通过随机游走生成路径，并采用类似LLM的自监督预训练范式，使模型内化图的拓扑和组合规则；利用预训练后的GFM作为图结构的基础表示，结合简单的生成式启发策略求解各类优化问题。

Result: 在20至893个节点的网络上实验表明，GFM在多种距离型优化任务中性能媲美专用求解器，同时推理速度显著更快。

Conclusion: GFM建立了适用于图优化的预训练迁移新范式，为将基础模型的进展应用于运筹学领域开辟了新路径。

Abstract: The pretrain-transfer paradigm, which underpins the success of large language
models (LLMs), has demonstrated the immense power of creating foundation models
that learn generalizable representations from vast datasets. However, extending
this paradigm to Operations Research (OR) problems on graph structures remains
challenging due to the fundamental conflict between the statistical flexibility
of language and the strict combinatorial constraints of graphs. To bridge this
gap, we introduce the Graph Foundation Model (GFM), the first framework capable
of solving all distance-based optimization problems on graph structures. By
introducing the LLM-like self-supervised pre-training paradigm on the paths
generated from random walks in the graph, GFM is compelled to internalize the
graph's complex topological and combinatorial rules, where the connectivity of
the structure itself can be treated as the supervisory signal. Unlike existing
neural methods that learn complex and task-specific solving policies, our
approach leverages the pre-trained GFM as a foundational model of the graph's
intrinsic structure, which in turn enables a simple generative heuristic to
tackle a diverse range of optimization challenges effectively. Comprehensive
experiments on networks ranging from 20 to 893 nodes demonstrate that GFM
achieves competitive performance against specialized solvers across a variety
of distinct optimization task classes, while maintaining significantly faster
inference times. Our work establishes a new paradigm of adapting the
pretrain-transfer framework to graph optimization, opening the door for
applying foundation model innovations to OR.

</details>


### [785] [Adversarial Reinforcement Learning Framework for ESP Cheater Simulation](https://arxiv.org/abs/2509.24274)
*Inkyu Park,Jeong-Gwan Lee,Taehwan Kwon,Juheon Choi,Seungku Kim,Junsu Kim,Kimin Lee*

Main category: cs.LG

TL;DR: 提出一种基于强化学习和对抗博弈的模拟框架，用于建模自适应的ESP作弊行为及检测器，实现可控且可扩展的反作弊研究平台。


<details>
  <summary>Details</summary>
Motivation: 由于ESP作弊行为难以观测且数据标注困难，传统方法难以有效检测；同时作弊者会动态调整行为以规避检测，增加了反作弊系统开发的难度。

Method: 将作弊者和正常玩家建模为具有不同观测量的强化学习智能体，检测器通过行为轨迹进行分类，并将双方交互建模为对抗博弈，引入能根据检测风险动态切换行为的结构化作弊者模型。

Result: 实验表明该框架能成功模拟出在获取奖励与规避检测之间进行策略性权衡的自适应作弊行为。

Conclusion: 所提框架为研究自适应作弊行为和开发有效的反作弊检测器提供了一个可控且可扩展的平台。

Abstract: Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game
information such as enemy locations, are difficult to detect because their
effects are not directly observable in player behavior. The lack of observable
evidence makes it difficult to collect reliably labeled data, which is
essential for training effective anti-cheat systems. Furthermore, cheaters
often adapt their behavior by limiting or disguising their cheat usage, which
further complicates detection and detector development. To address these
challenges, we propose a simulation framework for controlled modeling of ESP
cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and
non-cheaters as reinforcement learning agents with different levels of
observability, while detectors classify their behavioral trajectories. Next, we
formulate the interaction between the cheater and the detector as an
adversarial game, allowing both players to co-adapt over time. To reflect
realistic cheater strategies, we introduce a structured cheater model that
dynamically switches between cheating and non-cheating behaviors based on
detection risk. Experiments demonstrate that our framework successfully
simulates adaptive cheater behaviors that strategically balance reward
optimization and detection evasion. This work provides a controllable and
extensible platform for studying adaptive cheating behaviors and developing
effective cheat detectors.

</details>


### [786] [ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying](https://arxiv.org/abs/2509.24302)
*Muyun Jiang,Shuailei Zhang,Zhenjie Yang,Mengjun Wu,Weibang Jiang,Zhiwei Guo,Wei Zhang,Rui Liu,Shangen Zhang,Yong Li,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: 本文提出了一种新的脑电图-语言对齐基础模型ELASTIQ，通过引入任务感知的语义指导和指令调节机制，在多种BCI任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型难以将语言指令作为先验约束融入表示学习，限制了语义知识的利用。因此，需要一种能够结合语言指令来提升EEG表示统一性和可迁移性的方法。

Method: 提出ELASTIQ模型，包含预训练阶段的频谱-时间重建（STR）模块，以及指令调优阶段的指令条件化Q-Former（IQF），通过可学习查询实现EEG与文本标签的对齐。

Result: 在20个涵盖多种BCI任务的数据集上评估，ELASTIQ在14个数据集中达到最先进性能，并在五类任务中取得最佳平均结果。分析首次表明，显式任务指令能有效引导EEG嵌入到连贯且语言可解释的空间中。

Conclusion: ELASTIQ成功实现了EEG与语言指令的语义对齐，显著提升了EEG表示的鲁棒性与跨任务迁移能力，为基于语义引导的通用脑机接口提供了新路径。

Abstract: Recent advances in electroencephalography (EEG) foundation models, which
capture transferable EEG representations, have greatly accelerated the
development of brain-computer interfaces (BCI). However, existing approaches
still struggle to incorporate language instructions as prior constraints for
EEG representation learning, limiting their ability to leverage the semantic
knowledge inherent in language to unify different labels and tasks. To address
this challenge, we present ELASTIQ, a foundation model for EEG-Language
Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates
task-aware semantic guidance to produce structured and linguistically aligned
EEG embeddings, thereby enhancing decoding robustness and transferability. In
the pretraining stage, we introduce a joint Spectral-Temporal Reconstruction
(STR) module, which combines frequency masking as a global spectral
perturbation with two complementary temporal objectives: random masking to
capture contextual dependencies and causal masking to model sequential
dynamics. In the instruction tuning stage, we propose the
Instruction-conditioned Q-Former (IQF), a query-based cross-attention
transformer that injects instruction embeddings into EEG tokens and aligns them
with textual label embeddings through learnable queries. We evaluate ELASTIQ on
20 datasets spanning motor imagery, emotion recognition, steady-state visual
evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves
state-of-the-art performance on 14 of the 20 datasets and obtains the best
average results across all five task categories. Importantly, our analyses
reveal for the first time that explicit task instructions serve as semantic
priors guiding EEG embeddings into coherent and linguistically grounded spaces.
The code and pre-trained weights will be released.

</details>


### [787] [Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning](https://arxiv.org/abs/2509.24305)
*Alexander Tyurin,Andrei Spiridonov,Varvara Rudenko*

Main category: cs.LG

TL;DR: 本文提出了两种新的分布式强化学习算法Rennala NIGT和Malenia NIGT，分别在同构和异构环境下实现了高效的异步策略梯度聚合，理论和实验结果均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分布式强化学习在异步并行计算和通信条件下研究较少，尤其在存在计算异构性和通信瓶颈的情况下缺乏高效算法。

Method: 提出Rennala NIGT和Malenia NIGT两种新算法，实现异步策略梯度聚合；前者支持AllReduce操作，后者针对异构环境设计，具备更强的理论保证。

Result: 在同构设置下，Rennala NIGT降低了总计算与通信复杂度；在异构设置下，Malenia NIGT同时处理异步计算与异构环境，理论性能更优；实验表明新方法显著优于先前方法。

Conclusion: 所提出的两种算法在理论和实践中均展现出优越性，为分布式策略梯度方法提供了更高效、更具鲁棒性的解决方案。

Abstract: We study distributed reinforcement learning (RL) with policy gradient methods
under asynchronous and parallel computations and communications. While
non-distributed methods are well understood theoretically and have achieved
remarkable empirical success, their distributed counterparts remain less
explored, particularly in the presence of heterogeneous asynchronous
computations and communication bottlenecks. We introduce two new algorithms,
Rennala NIGT and Malenia NIGT, which implement asynchronous policy gradient
aggregation and achieve state-of-the-art efficiency. In the homogeneous
setting, Rennala NIGT provably improves the total computational and
communication complexity while supporting the AllReduce operation. In the
heterogeneous setting, Malenia NIGT simultaneously handles asynchronous
computations and heterogeneous environments with strictly better theoretical
guarantees. Our results are further corroborated by experiments, showing that
our methods significantly outperform prior approaches.

</details>


### [788] [A study of Universal ODE approaches to predicting soil organic carbon](https://arxiv.org/abs/2509.24306)
*Satyanarayana Raju G. V. V,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本研究利用基于通用微分方程（UDE）的科学机器学习框架预测土壤有机碳（SOC）动态，结合物理机制与神经网络，在低噪声条件下表现优异，但在高噪声下出现过拟合或平滑失效，需进一步改进以适应实地应用。


<details>
  <summary>Details</summary>
Motivation: 土壤有机碳（SOC）对土壤健康和气候韧性至关重要，但其动态受复杂过程影响，传统预测方法困难，亟需结合物理机制与数据驱动的新方法提升预测能力。

Method: 采用通用微分方程（UDE）框架，融合对流-扩散等物理模型与神经网络，用于学习微生物非线性过程；在合成数据集上设计六种实验场景，评估模型在不同噪声水平下的预测性能。

Result: 在无噪声和中等噪声（7%）条件下，模型准确重建SOC动态（R2达0.9999以上）；但在35%高噪声条件下，出现过拟合或过度平滑问题，导致泛化能力下降甚至负R2。

Conclusion: UDE在噪声可控环境下具有潜力，适用于可扩展且抗噪的SOC预测，但要应用于实际田间条件，需引入抗噪损失函数、概率建模及更精细的微生物过程整合。

Abstract: Soil Organic Carbon (SOC) is a foundation of soil health and global climate
resilience, yet its prediction remains difficult because of intricate physical,
chemical, and biological processes. In this study, we explore a Scientific
Machine Learning (SciML) framework built on Universal Differential Equations
(UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend
mechanistic physics, such as advection diffusion transport, with neural
networks that learn nonlinear microbial production and respiration. Using
synthetic datasets, we systematically evaluated six experimental cases,
progressing from clean, noise free benchmarks to stress tests with high (35%)
multiplicative, spatially correlated noise. Our results highlight both the
potential and limitations of the approach. In noise free and moderate noise
settings, the UDE accurately reconstructed SOC dynamics. In clean terminal
profile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5,
and R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 =
0.99998), capturing depth wise SOC trends while tolerating realistic
measurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear
evidence of overfitting: the model reproduced noisy inputs with high accuracy
but lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise
at t = 50) collapsed toward overly smooth mean profiles, failing to capture
depth wise variability and yielding negative R2, underscoring the limits of
standard training under severe uncertainty. These findings suggest that UDEs
are well suited for scalable, noise tolerant SOC forecasting, though advancing
toward field deployment will require noise aware loss functions, probabilistic
modelling, and tighter integration of microbial dynamics.

</details>


### [789] [Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers](https://arxiv.org/abs/2509.24317)
*Xianhang Li,Chen Huang,Chun-Liang Li,Eran Malach,Josh Susskind,Vimal Thilak,Etai Littwin*

Main category: cs.LG

TL;DR: SALT是一种用于视频表征学习的两阶段无正则化方法，通过使用冻结的目标编码器替代EMA更新的教师模型，实现了更高的计算效率、可扩展性和透明度，同时保持了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的V-JEPA方法依赖指数移动平均（EMA）更新教师模型，虽能防止表征崩溃，但导致模型选择复杂且教师与学生架构耦合，限制了可扩展性与优化灵活性。因此，需要一种更简单、高效且可扩展的方法。

Method: 提出SALT（Static-teacher Asymmetric Latent Training）：第一阶段训练一个仅通过像素重建目标获得的目标编码器；第二阶段冻结该目标编码器，训练学生模型来预测被掩码区域的潜在表示。两个阶段分别处理像素重建和潜在空间预测，实现解耦训练。

Result: 学生模型在冻结主干的评估下优于最新的V-JEPA 2编码器，在多个基准上表现更优；在相同预训练FLOPs下，SALT具有更高的探针准确率，其扩展曲线在准确率-FLOPs帕累托前沿上优于V-JEPA；此外，学生模型性能对教师质量鲁棒，即使使用小而次优的教师也能训练出高性能学生。

Conclusion: SALT作为一种简单、可扩展且计算高效的替代方案，克服了EMA机制的局限，为自监督视频表征学习提供了新的可行路径。

Abstract: Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable
off-the-shelf video representation by predicting masked regions in latent space
with an exponential moving average (EMA)-updated teacher. While EMA prevents
representation collapse, it complicates scalable model selection and couples
teacher and student architectures. We revisit masked-latent prediction and show
that a frozen teacher suffices. Concretely, we (i) train a target encoder with
a simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze
it and train a student to predict the teacher's latents on masked regions. This
leads to a two-stage, unregularized scheme that we refer to as SALT
(Static-teacher Asymmetric Latent Training). SALT decouples optimization into
pixel reconstruction (teacher) and masked latent prediction (student),
increasing transparency, efficiency, and scalability while preserving the
ability of representation to generalize under frozen evaluation. Empirically,
our student models outperform recently proposed V-JEPA 2 encoders under frozen
backbone evaluation across diverse benchmarks. They are also more
compute-optimal: at matched pretraining FLOPs, our method achieves higher
probing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs
Pareto frontier. Finally, we find that student quality is remarkably robust to
teacher quality: high-performing students emerge even with small, sub-optimal
teachers. This points to a compute budget allocation that should overwhelmingly
favor the student. These results position SALT as a simple, scalable, and
compute-efficient alternative to EMA-based self-distillation for video
representation learning.

</details>


### [790] [AuON: A Linear-time Alternative to Semi-Orthogonal Momentum Updates](https://arxiv.org/abs/2509.24320)
*Dipan Maity*

Main category: cs.LG

TL;DR: 提出AuON，一种线性时间复杂度的优化器，通过归一化非线性缩放实现动量更新的单位范数约束，避免显式半正交化，在保持结构对齐的同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统正交梯度更新方法（如SVD/QR）计算成本高（O(n^3)），而现有改进方法（如Muon）虽降至O(n^2)，但仍存在二次复杂度瓶颈，且动量应用顺序影响性能。

Method: 提出AuON，利用双曲余弦RMS缩放与归一化相结合，在谱范数信任域下约束动量更新，保持方向信息，无需构造半正交矩阵；并提出混合变体Hybrid-AuON，结合一次Newton-Schulz迭代。

Result: 在视觉和语言任务上，AuON及其混合版本性能媲美AdamW和Muon，且计算效率更高，复杂度为线性。

Conclusion: AuON通过避免显式半正交化，在保持优化性能的同时显著降低计算复杂度，为高效优化器设计提供了新方向。

Abstract: Orthogonal gradient updates have emerged as a promising direction in
optimization for machine learning. However, traditional approaches such as
SVD/QR decomposition incur prohibitive computational costs of O(n^3) and
underperform compared to well-tuned SGD with momentum, since momentum is
applied only after strict orthogonalization. Recent advances, such as Muon,
improve efficiency by applying momentum before orthogonalization and producing
semi-orthogonal matrices via Newton-Schulz iterations, reducing complexity to
O(n^2). Nevertheless, quadratic costs remain a bottleneck.
  In this work, we study the semi-orthogonal properties of momentum-based
updates and develop a method to bound momentum updates under a spectral-norm
trust region, preserving directional information without requiring explicit
semi-orthogonalization.
  We propose AuON (Alternative Unit-norm momentum updates by Normalized
nonlinear scaling), a linear-time optimizer that achieves strong performance
without constructing semi-orthogonal matrices, while preserving structural
alignment and reconditioning ill-posed updates. Our approach combines
hyperbolic-cosine RMS scaling transformations with normalization, demonstrating
both effectiveness and computational efficiency compared to Newton-Schulz
methods. We further introduce a hybrid variant (Hybrid-AuON) that applies a
single Newton-Schulz iteration. Experiments across vision and language
benchmarks show that AuON and its hybrid variant achieve performance comparable
to strong baselines such as AdamW and Muon.
  Code is available at: https://github.com/ryyzn9/AuON

</details>


### [791] [H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning](https://arxiv.org/abs/2509.24330)
*Shiyuan Zuo,Rongfei Fan,Cheng Zhan,Jie Xu,Puning Zhao,Han Hu*

Main category: cs.LG

TL;DR: 本文提出了一种名为H+的新型相似性感知聚合方法，用于增强联邦学习在面对拜占庭攻击时的鲁棒性，该方法在有无干净数据的情况下均适用，并具有低计算复杂度和优越的防御性能。


<details>
  <summary>Details</summary>
Motivation: 现有的相似性感知聚合方法仅适用于拥有干净数据的联邦学习系统，而在缺乏干净数据的实际场景中无法应用，因此需要一种更通用且高效的防御机制。

Method: H+方法通过从客户端上传的高维参数向量中随机选取低维片段，利用相似性检测函数H将其与参考向量进行比较，保留最相似的客户端参数用于聚合；参考向量可来自鲁棒算法或干净数据，并通过K次重复操作提高诚实客户端识别准确性。

Result: 实验表明，H+在多种拜占庭攻击类型和不同攻击比例下均显著优于现有方法，在多个基准数据集上实现了最先进的鲁棒性，且时间复杂度仅为O(KMr)，计算开销低。

Conclusion: H+是一种高效、通用且鲁棒的联邦学习聚合方法，能够在有无干净数据的场景下有效抵御拜占庭攻击，具有实际应用价值。

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data. However, it remains vulnerable to Byzantine attacks, which can
compromise the aggregation of locally updated parameters at the central server.
Similarity-aware aggregation has emerged as an effective strategy to mitigate
such attacks by identifying and filtering out malicious clients based on
similarity between client model parameters and those derived from clean data,
i.e., data that is uncorrupted and trustworthy. However, existing methods adopt
this strategy only in FL systems with clean data, making them inapplicable to
settings where such data is unavailable. In this paper, we propose H+, a novel
similarity-aware aggregation approach that not only outperforms existing
methods in scenarios with clean data, but also extends applicability to FL
systems without any clean data. Specifically, H+ randomly selects
$r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to
the server and applies a similarity check function $H$ to compare each segment
against a reference vector, preserving the most similar client vectors for
aggregation. The reference vector is derived either from existing robust
algorithms when clean data is unavailable or directly from clean data.
Repeating this process $K$ times enables effective identification of honest
clients. Moreover, H+ maintains low computational complexity, with an
analytical time complexity of $\mathcal{O}(KMr)$, where $M$ is the number of
clients and $Kr \ll p$. Comprehensive experiments validate H+ as a
state-of-the-art (SOTA) method, demonstrating substantial robustness
improvements over existing approaches under varying Byzantine attack ratios and
multiple types of traditional Byzantine attacks, across all evaluated scenarios
and benchmark datasets.

</details>


### [792] [Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning](https://arxiv.org/abs/2509.24332)
*Siyang Li,Yize Chen,Yan Guo,Ming Huang,Hui Xiong*

Main category: cs.LG

TL;DR: 提出一种基于物理引导的不变学习方法iMOOE，用于提升深度学习模型在偏微分方程（PDE）系统中对未见分布外场景的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的PDE系统参数多变，现有方法在零样本OOD泛化上表现不足，且缺乏对PDE系统中基本物理不变性的建模。

Method: 明确定义了双重PDE不变性原理，并提出iMOOE方法，包含不变性对齐的操作符专家混合架构和频域增强的不变学习目标。

Result: 在多个模拟基准和真实应用场景中验证了iMOOE在分布内预测性能和零样本OOD泛化能力上的优越性。

Conclusion: 通过显式建模PDE系统的不变性，iMOOE显著提升了模型在未见分布外场景下的泛化能力，无需测试时适应即可实现高效预测。

Abstract: Advanced deep learning-based approaches have been actively applied to
forecast the spatiotemporal physical dynamics governed by partial differential
equations (PDEs), which acts as a critical procedure in tackling many science
and engineering problems. As real-world physical environments like PDE system
parameters are always capricious, how to generalize across unseen
out-of-distribution (OOD) forecasting scenarios using limited training data is
of great importance. To bridge this barrier, existing methods focus on
discovering domain-generalizable representations across various PDE dynamics
trajectories. However, their zero-shot OOD generalization capability remains
deficient, since extra test-time samples for domain-specific adaptation are
still required. This is because the fundamental physical invariance in PDE
dynamical systems are yet to be investigated or integrated. To this end, we
first explicitly define a two-fold PDE invariance principle, which points out
that ingredient operators and their composition relationships remain invariant
across different domains and PDE system evolution. Next, to capture this
two-fold PDE invariance, we propose a physics-guided invariant learning method
termed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert
architecture and a frequency-enriched invariant learning objective. Extensive
experiments across simulated benchmarks and real-world applications validate
iMOOE's superior in-distribution performance and zero-shot generalization
capabilities on diverse OOD forecasting scenarios.

</details>


### [793] [Expanding Horizons of Level Diversity via Multi-objective Evolutionary Learning](https://arxiv.org/abs/2509.24341)
*Qingquan Zhang,Ziqi Wang,Yuchen Li,Keyuan Zhang,Bo Yuan,Jialin Liu*

Main category: cs.LG

TL;DR: 本文提出了一种多目标进化学习框架，用于在训练生成模型时同时优化多个关卡多样性指标，从而提升游戏关卡的多维多样性，并在超级马里奥兄弟的案例中展示了该方法能有效提供可玩性与多样性之间的权衡选择。


<details>
  <summary>Details</summary>
Motivation: 现有关卡生成方法往往未能全面评估多维度的关卡多样性，而多样性指标之间存在冲突或互补关系，因此需要一种能够综合优化多维多样性的方法。

Method: 将生成模型的训练建模为多目标学习问题，每个多样性指标作为一个独立目标，提出一个多目标进化学习框架，在训练过程中同时优化多个多样性指标。

Result: 在超级马里奥兄弟基准上的实验表明，该框架能够增强多维多样性，并识别出一组在可玩性与两种代表性多样性（基于内容和以玩家为中心）之间具有不同权衡的生成模型帕累托前沿。

Conclusion: 所提框架能够有效提升游戏关卡的多维多样性，为决策者在不同应用场景和用户需求下选择合适的生成器提供了支持。

Abstract: In recent years, the generation of diverse game levels has gained increasing
interest, contributing to a richer and more engaging gaming experience. A
number of level diversity metrics have been proposed in literature, which are
naturally multi-dimensional, leading to conflicted, complementary, or both
relationships among these dimensions. However, existing level generation
approaches often fail to comprehensively assess diversity across those
dimensions. This paper aims to expand horizons of level diversity by
considering multi-dimensional diversity when training generative models. We
formulate the model training as a multi-objective learning problem, where each
diversity metric is treated as a distinct objective. Furthermore, a
multi-objective evolutionary learning framework that optimises multiple
diversity metrics simultaneously throughout the model training process is
proposed. Our case study on the commonly used benchmark Super Mario Bros.
demonstrates that our proposed framework can enhance multi-dimensional
diversity and identify a Pareto front of generative models, which provides a
range of tradeoffs among playability and two representative diversity metrics,
including a content-based one and a player-centered one. Such capability
enables decision-makers to make informed choices when selecting generators
accommodating a variety of scenarios and the diverse needs of players and
designers.

</details>


### [794] [Watermarking Diffusion Language Models](https://arxiv.org/abs/2509.24368)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了首个针对扩散语言模型（DLM）的水印技术，解决了传统水印方法依赖已生成上下文、难以应用于非自回归生成顺序的挑战。通过在期望上应用水印并增强上下文中的水印强度，实现了高检测准确率且对生成质量影响小。


<details>
  <summary>Details</summary>
Motivation: 由于扩散语言模型（DLM）以非顺序方式生成文本，传统依赖先前生成token的水印方法无法直接适用，因此需要设计专用于DLM的新水印机制。

Method: 提出在上下文尚未完全确定时，基于期望应用水印；同时鼓励使用能增强其他token水印强度的token作为上下文，保持检测器不变。

Result: 实验表明该DLM水印在极低生成质量影响下，实现超过99%的真阳性检测率，并具备与现有ARLM水印相当的鲁棒性。

Conclusion: 该工作首次实现了对扩散语言模型的有效水印嵌入，为DLM的版权保护和内容溯源提供了可靠解决方案。

Abstract: We introduce the first watermark tailored for diffusion language models
(DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in
contrast to standard autoregressive language models (ARLMs) which generate
tokens sequentially. While there has been much work in ARLM watermarking, a key
challenge when attempting to apply these schemes directly to the DLM setting is
that they rely on previously generated tokens, which are not always available
with DLM generation. In this work we address this challenge by: (i) applying
the watermark in expectation over the context even when some context tokens are
yet to be determined, and (ii) promoting tokens which increase the watermark
strength when used as context for other tokens. This is accomplished while
keeping the watermark detector unchanged. Our experimental evaluation
demonstrates that the DLM watermark leads to a >99% true positive rate with
minimal quality impact and achieves similar robustness to existing ARLM
watermarks, enabling for the first time reliable DLM watermarking.

</details>


### [795] [Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning](https://arxiv.org/abs/2509.24372)
*Xin Qiu,Yulu Gan,Conor F. Hayes,Qiyao Liang,Elliot Meyerson,Babak Hodjat,Risto Miikkulainen*

Main category: cs.LG

TL;DR: 本研究首次成功将进化策略（ES）扩展到大规模语言模型（LLM）的全参数微调，证明ES在数十亿参数上高效搜索，并在样本效率、长时奖励容忍度、鲁棒性、抗奖励欺骗性和训练稳定性方面优于现有强化学习（RL）方法，为LLM微调提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）是当前主流的LLM微调方法，但进化策略（ES）因被认为难以扩展到大模型而被忽视。本文旨在挑战这一观点，探索ES在大规模模型上的潜力。

Method: 采用进化策略（ES）对完整参数的大型语言模型进行微调，通过大规模实验验证其在不同任务和基础模型上的可扩展性与有效性，并与现有RL方法进行系统比较。

Result: ES在数十亿参数的LLM上实现了高效搜索，在样本效率、长时奖励处理、对不同基础模型的鲁棒性、减少奖励欺骗倾向以及跨运行稳定性方面均优于现有RL方法。

Conclusion: 进化策略是一种可扩展且高效的LLM微调方法，具备超越当前RL方法的多方面优势，为LLM微调开辟了新的技术路径。

Abstract: Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is
a critical step in the AI deployment pipeline. Reinforcement learning (RL) is
arguably the most prominent fine-tuning method, contributing to the birth of
many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once
showed comparable performance to RL on models with a few million parameters,
was neglected due to the pessimistic perception of its scalability to larger
models. In this work, we report the first successful attempt to scale up ES for
fine-tuning the full parameters of LLMs, showing the surprising fact that ES
can search efficiently over billions of parameters and outperform existing RL
fine-tuning methods in multiple respects, including sample efficiency,
tolerance to long-horizon rewards, robustness to different base LLMs, less
tendency to reward hacking, and more stable performance across runs. It
therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond
what current RL techniques provide. The source codes are provided at:
https://github.com/VsonicV/es-fine-tuning-paper.

</details>


### [796] [AXIS: Explainable Time Series Anomaly Detection with Large Language Models](https://arxiv.org/abs/2509.24378)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: 本文提出AXIS框架，通过结合符号化数值提示、上下文集成的步对齐提示和任务先验提示，利用冻结的大语言模型实现高质量的时间序列异常检测与解释。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列到文本转换方法缺乏模态间的上下文对齐和表征一致性，难以让大语言模型有效理解连续时间信号。

Method: AXIS框架不直接将时间序列转为文本，而是从序列中提取三种互补提示：符号化数值提示用于数值锚定，基于预训练时间序列编码器生成的细粒度动态提示，以及编码全局异常特征的任务先验提示，共同增强大语言模型的输入。

Result: 实验表明，AXIS在解释质量和异常检测性能上均优于通用大模型、专用时间序列大模型以及时序视觉语言模型，且在新构建的多格式问答基准上表现出色。

Conclusion: AXIS有效弥合了离散语言模型与连续时间序列之间的语义鸿沟，实现了兼具高可解释性与竞争力检测精度的异常检测。

Abstract: Time-series anomaly detection (TSAD) increasingly demands explanations that
articulate not only if an anomaly occurred, but also what pattern it exhibits
and why it is anomalous. Leveraging the impressive explanatory capabilities of
Large Language Models (LLMs), recent works have attempted to treat time series
as text for explainable TSAD. However, this approach faces a fundamental
challenge: LLMs operate on discrete tokens and struggle to directly process
long, continuous signals. Consequently, naive time-to-text serialization
suffers from a lack of contextual grounding and representation alignment
between the two modalities. To address this gap, we introduce AXIS, a framework
that conditions a frozen LLM for nuanced time-series understanding. Instead of
direct serialization, AXIS enriches the LLM's input with three complementary
hints derived from the series: (i) a symbolic numeric hint for numerical
grounding, (ii) a context-integrated, step-aligned hint distilled from a
pretrained time-series encoder to capture fine-grained dynamics, and (iii) a
task-prior hint that encodes global anomaly characteristics. Furthermore, to
facilitate robust evaluation of explainability, we introduce a new benchmark
featuring multi-format questions and rationales that supervise contextual
grounding and pattern-level semantics. Extensive experiments, including both
LLM-based and human evaluations, demonstrate that AXIS yields explanations of
significantly higher quality and achieves competitive detection accuracy
compared to general-purpose LLMs, specialized time-series LLMs, and time-series
Vision Language Models.

</details>


### [797] [Muon: Training and Trade-offs with Latent Attention and MoE](https://arxiv.org/abs/2509.24406)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: Muon优化器在小到中等规模解码器上展现出优越的收敛性和数据效率，结合现代架构优化可显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 探索Muon优化器在中小规模解码器上的理论基础与实证表现，填补其在数学原理和实际应用间协同效应的研究空白。

Method: 通过理论分析（收敛性、谱正则化、自然梯度联系）与大规模实验（100+次训练），结合MLA和MoE等现代架构验证其性能。

Result: Muon在保持或提升最终困惑度的同时，仅需AdamW 48-52%的训练计算量；结合MLA+MoE可实现68%内存减少和3.2倍推理加速，困惑度改善8-12%。

Conclusion: Muon是一种原理性强、鲁棒的AdamW替代方案，在大批次训练和现代效率技术结合下表现尤为突出。

Abstract: We present a comprehensive theoretical and empirical study of the Muon
optimizer for training transformers only with a small to medium decoder (30M -
200M parameters), with an emphasis on its mathematical foundations, convergence
properties and synergistic interactions with modern architectural
optimizations. Building on recent work showing Muon's scalability, we provide
rigorous theoretical analysis including: (i)showing the convergence rate under
standard assumptions, (ii) spectral regularization properties that prevent
gradient explosion, (iii) connection to natural gradient descent on the Stiefel
manifold, and (iv) equivalence to steepest gradient descent under the spectral
norm. Crucially, we demonstrate that Muon expands the Pareto frontier in the
compute-time trade-off by maintaining superior data efficiency at large batch
sizes, a key finding of~\cite{essentialai2025muon} that we validate across our
model scales. Empirically, Muon reaches the target loss with 48-52\% of the
training calculated by AdamW while maintaining or improving the final
perplexity, consistent with larger-scale results. When combined with Multi-Head
Latent Attention (MLA) and Mixture-of-Experts (MoE), we observe multiplicative
efficiency gains: MLA+MoE+Muon achieves 68\% memory reduction and 3.2$\times$
inference speedup, while improving perplexity by 8-12\%. We provide detailed
procedures on 15 architectural and optimizer components, stability analyzes
across 100+ training runs, and practical implementation guidelines including
Newton-Schulz coefficients $(3.4445, -4.7750, 2.0315)$ optimized
by~\cite{su2024muonblog}. Our theoretical analysis and comprehensive
experiments establish Muon as a principled, robust alternative to AdamW that
particularly excels when combined with modern efficiency techniques and
large-batch training regimes.

</details>


### [798] [ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection](https://arxiv.org/abs/2509.24414)
*Tao Yin,Xiaohong Zhang,Shaochen Fu,Zhibin Zhang,Li Huang,Yiyuan Yang,Kaixiang Yang,Meng Yan*

Main category: cs.LG

TL;DR: 本文提出了ScatterAD，一种利用表示散射现象（scattering）来增强时空异常检测的新方法，通过建模时间与拓扑维度上的散射特性，在多个公开基准上实现了最先进的多变量时间序列异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法独立建模空间或时间依赖关系，忽略了高维空间中正常和异常样本的分散特性，导致表征学习不充分且对异常不够敏感。

Method: 提出ScatterAD模型，包含用于捕捉图结构散射的拓扑编码器和通过最小化相邻时间步间均方误差来约束过度散射的时间编码器，并引入对比融合机制以确保时序与拓扑表征的互补性。

Result: 在多个公开数据集上实验表明，ScatterAD在多变量时间序列异常检测任务中达到最先进水平，显著优于现有方法。

Conclusion: 利用表示散射作为归纳信号可有效提升多变量时间序列异常检测性能，所提出的ScatterAD框架通过联合建模时空散射与对比融合机制，增强了特征表示的判别能力。

Abstract: One main challenge in time series anomaly detection for industrial IoT lies
in the complex spatio-temporal couplings within multivariate data. However,
traditional anomaly detection methods focus on modeling spatial or temporal
dependencies independently, resulting in suboptimal representation learning and
limited sensitivity to anomalous dispersion in high-dimensional spaces. In this
work, we conduct an empirical analysis showing that both normal and anomalous
samples tend to scatter in high-dimensional space, especially anomalous samples
are markedly more dispersed. We formalize this dispersion phenomenon as
scattering, quantified by the mean pairwise distance among sample
representations, and leverage it as an inductive signal to enhance
spatio-temporal anomaly detection. Technically, we propose ScatterAD to model
representation scattering across temporal and topological dimensions. ScatterAD
incorporates a topological encoder for capturing graph-structured scattering
and a temporal encoder for constraining over-scattering through mean squared
error minimization between neighboring time steps. We introduce a contrastive
fusion mechanism to ensure the complementarity of the learned temporal and
topological representations. Additionally, we theoretically show that
maximizing the conditional mutual information between temporal and topological
views improves cross-view consistency and enhances more discriminative
representations. Extensive experiments on multiple public benchmarks show that
ScatterAD achieves state-of-the-art performance on multivariate time series
anomaly detection. Code is available at this repository:
https://github.com/jk-sounds/ScatterAD.

</details>


### [799] [BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification](https://arxiv.org/abs/2509.24425)
*Jingtao Zhang,Yi Liu,Qi Shen,Changhong Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为BiHDTrans的高效神经符号二值超维Transformer模型，结合了超维计算的高效性与Transformer在时序建模上的优势，显著提升了多变量时间序列分类的准确性与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的超维计算难以捕捉复杂时序模式，而Transformer虽擅长序列建模但计算开销大，难以部署于资源受限的边缘环境，因此需要一种兼具效率与表达能力的新方法。

Method: 将自注意力机制融入超维计算框架，构建二值化的高维表示，并在FPGA上实现流水线硬件加速，利用高维表示的独立同分布特性优化推理性能。

Result: BiHDTrans在准确率上平均比现有二值Transformer高出6.67%，比现有超维模型至少提升14.47%；在FPGA上实现39.4倍的推理延迟降低；即使维度减少64%，仍保持竞争力，模型大小减少4.4倍，延迟进一步降低49.8%。

Conclusion: BiHDTrans成功融合了Transformer的时间建模能力和超维计算的效率优势，为资源受限环境下的多变量时间序列分类提供了高精度、低延迟、可扩展的解决方案。

Abstract: The proliferation of Internet-of-Things (IoT) devices has led to an
unprecedented volume of multivariate time series (MTS) data, requiring
efficient and accurate processing for timely decision-making in
resource-constrained edge environments. Hyperdimensional (HD) computing, with
its inherent efficiency and parallelizability, has shown promise in
classification tasks but struggles to capture complex temporal patterns, while
Transformers excel at sequence modeling but incur high computational and memory
overhead. We introduce BiHDTrans, an efficient neurosymbolic binary
hyperdimensional Transformer that integrates self-attention into the HD
computing paradigm, unifying the representational efficiency of HD computing
with the temporal modeling power of Transformers. Empirically, BiHDTrans
outperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and
achieves 6.67% higher accuracy on average than SOTA binary Transformers. With
hardware acceleration on FPGA, our pipelined implementation leverages the
independent and identically distributed properties of high-dimensional
representations, delivering 39.4 times lower inference latency than SOTA binary
Transformers. Theoretical analysis shows that binarizing in holographic
high-dimensional space incurs significantly less information distortion than
directly binarizing neural networks, explaining BiHDTrans's superior accuracy.
Furthermore, dimensionality experiments confirm that BiHDTrans remains
competitive even with a 64% reduction in hyperspace dimensionality, surpassing
SOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as
well as further reducing the latency by 49.8% compare to the full-dimensional
baseline. Together, these contributions bridge the gap between the
expressiveness of Transformers and the efficiency of HD computing, enabling
accurate, scalable, and low-latency MTS classification.

</details>


### [800] [Semantic Compression via Multimodal Representation Learning](https://arxiv.org/abs/2509.24431)
*Eleonora Grassucci,Giordano Cicchetti,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 本文提出了一种基于模态对齐的语义压缩新方法，通过减少不同模态嵌入之间的模态差距，实现高效压缩，同时保持多模态表示的语义一致性。


<details>
  <summary>Details</summary>
Motivation: 多模态表示学习产生高维嵌入，带来存储和处理的可扩展性挑战，亟需在不损失语义信息的前提下进行语义压缩。

Method: 利用预训练编码器，通过减小模态间隙，使表达相同语义的不同模态嵌入共享空间区域，并用其中心点作为语义代表进行压缩。

Result: 在多种大规模多模态下游任务中验证了该方法的有效性，实现了显著的内存节省且未牺牲性能。

Conclusion: 模态对齐是实现语义压缩的关键，所提方法能在保持性能的同时有效压缩多模态嵌入。

Abstract: Multimodal representation learning produces high-dimensional embeddings that
align diverse modalities in a shared latent space. While this enables strong
generalization, it also introduces scalability challenges, both in terms of
storage and downstream processing. A key open problem is how to achieve
semantic compression, reducing the memory footprint of multimodal embeddings
while preserving their ability to represent shared semantic content across
modalities. In this paper, we prove a strong connection between reducing the
modality gap, which is the residual separation of embeddings from different
modalities, and the feasibility of post-training semantic compression. When the
gap is sufficiently reduced, embeddings from different modalities but
expressing the same semantics share a common portion of the space. Therefore,
their centroid is a faithful representation of such a semantic concept. This
enables replacing multiple embeddings with a single centroid, yielding
significant memory savings. We propose a novel approach for semantic
compression grounded on the latter intuition, operating directly on pretrained
encoders. We demonstrate its effectiveness across diverse large-scale
multimodal downstream tasks. Our results highlight that modality alignment is a
key enabler for semantic compression, showing that the proposed approach
achieves significant compression without sacrificing performance.

</details>


### [801] [EOE: Evolutionary Optimization of Experts for Training Language Models](https://arxiv.org/abs/2509.24436)
*Yingshi Chen*

Main category: cs.LG

TL;DR: 提出一种基于进化框架的大语言模型训练方法，通过分专家训练和进化操作提升效率并减少推理模型大小。


<details>
  <summary>Details</summary>
Motivation: 为了降低大语言模型训练的内存消耗和提高吞吐量，同时保持较高的精度。

Method: 将模型划分为多个结构相同但参数不同的专家，每步仅训练一个专家，并在AdamW优化后引入交叉、PSO和变异等进化操作，使当前专家从最优专家中学习，最终只保存最优专家的权重。

Result: 最优专家能达到接近完整模型的精度，显著减小推理模型大小，训练吞吐量提升十倍以上，且框架为纯C++/CUDA实现，便于部署。

Conclusion: 该进化训练框架有效提升了训练效率和资源利用率，适合在资源受限设备上部署。

Abstract: This paper presents an evolutionary framework for the training of large
language models(LLM). The models are divided into several
experts(sub-networks), which have the same structure but different parameter
values. Only one expert is trained at each step. After the classical AdamW
optimization, some evolutionary operators(crossover, PSO, and mutation) act on
the tensor weights between the current expert and the best expert. So current
expert would learn the experience of best expert. The direction of best expert
would help current expert's loss decrease faster. Finally, only save the weight
of the best expert. Experiments show that best expert would achieve nearly the
same accuracy as the full model. This would greatly reduce the size of the
model for inference. Since only one expert is trained at each step, the
training needs much less memory and has much higher throughput. Experiments
show that the throughput would accelerate more than ten times! Our source code
is available. It's a pure c++/cu framework, which is suitable for easy
deployment on PCs and edge computing devices.

</details>


### [802] [Distributionally Robust Federated Learning with Outlier Resilience](https://arxiv.org/abs/2509.24462)
*Zifan Wang,Xinlei Yi,Xenia Konti,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种基于非平衡Wasserstein距离和KL惩罚的分布鲁棒联邦学习方法，以同时应对数据分布偏移和局部数据集中异常值的影响，并设计了可证明收敛的分布式算法。


<details>
  <summary>Details</summary>
Motivation: 现有基于DRO的联邦学习方法忽视了局部数据集中异常值对模型训练的负面影响，导致模型性能下降。

Method: 引入基于非平衡Wasserstein距离的新模糊集，结合KL散度惩罚来抑制异常值影响，将原问题转化为可处理的拉格朗日罚函数优化问题，并提出分布式鲁棒联邦学习算法。

Result: 在合成和真实数据集上的实验表明，所提方法在存在异常值和分布偏移时显著优于现有方法，且具备鲁棒性保证。

Conclusion: 该方法有效提升了联邦学习在复杂数据环境下的鲁棒性，兼顾了分布偏移和异常值抵抗能力。

Abstract: Federated learning (FL) enables collaborative model training without direct
data sharing, but its performance can degrade significantly in the presence of
data distribution perturbations. Distributionally robust optimization (DRO)
provides a principled framework for handling this by optimizing performance
against the worst-case distributions within a prescribed ambiguity set.
However, existing DRO-based FL methods often overlook the detrimental impact of
outliers in local datasets, which can disproportionately bias the learned
models. In this work, we study distributionally robust federated learning with
explicit outlier resilience. We introduce a novel ambiguity set based on the
unbalanced Wasserstein distance, which jointly captures geometric
distributional shifts and incorporates a non-geometric Kullback--Leibler
penalization to mitigate the influence of outliers. This formulation naturally
leads to a challenging min--max--max optimization problem. To enable
decentralized training, we reformulate the problem as a tractable Lagrangian
penalty optimization, which admits robustness certificates. Building on this
reformulation, we propose the distributionally outlier-robust federated
learning algorithm and establish its convergence guarantees. Extensive
experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our approach.

</details>


### [803] [Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nyström Approximation](https://arxiv.org/abs/2509.24467)
*Maedeh Zarvandi,Michael Timothy,Theresa Wasserer,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文提出了KREPES，一种基于Nyström近似的大规模核表示学习统一框架，解决了传统核方法在可扩展性上的局限，支持多种无监督和自监督损失，并在大规模图像和表格数据上验证了其高效性与表示的可解释性。


<details>
  <summary>Details</summary>
Motivation: 核方法虽有坚实的理论基础，但受限于计算和内存开销，难以扩展到大规模表示学习任务，尤其是在需要从大量无标签数据中学习表示的基础模型时代缺乏有效的可扩展核方法框架。

Method: 提出KREPES框架，利用Nyström近似实现核方法的可扩展性，支持多种无监督和自监督损失函数，用于高效学习核空间中的表示。

Result: 在大型图像和表格数据集上的实验表明，KREPES具有良好的计算效率和可扩展性，同时学到的表示具备良好的可解释性。

Conclusion: KREPES为核方法在大规模表示学习中的应用提供了可行路径，兼具高效性、灵活性和可解释性，有望推动核方法在现代机器学习中的复兴。

Abstract: Kernel methods provide a theoretically grounded framework for non-linear and
non-parametric learning, with strong analytic foundations and statistical
guarantees. Yet, their scalability has long been limited by prohibitive time
and memory costs. While progress has been made in scaling kernel regression, no
framework exists for scalable kernel-based representation learning, restricting
their use in the era of foundation models where representations are learned
from massive unlabeled data. We introduce KREPES -- a unified, scalable
framework for kernel-based representation learning via Nystr\"om approximation.
KREPES accommodates a wide range of unsupervised and self-supervised losses,
and experiments on large image and tabular datasets demonstrate its efficiency.
Crucially, KREPES enables principled interpretability of the learned
representations, an immediate benefit over deep models, which we substantiate
through dedicated analysis.

</details>


### [804] [FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing](https://arxiv.org/abs/2509.24472)
*Ran Elbaz,Guy Bar-Shalom,Yam Eitan,Fabrizio Frasca,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出了Function Sharing KAN（FS-KAN），一种构建适用于任意置换对称群的等变和不变Kolmogorov-Arnold网络层的原则性方法，通过推广参数共享机制并保持KAN的可解释性和表达能力，在低数据场景下表现出卓越的数据效率。


<details>
  <summary>Details</summary>
Motivation: 尽管KAN在可解释性和表达能力上优于传统MLP，但在置换对称数据上的等变扩展仍缺乏统一且通用的框架，限制了其在对称性建模中的应用。

Method: 通过将参数共享方案推广到Kolmogorov-Arnold结构中，构造出FS-KAN层，并理论分析其表达能力与传统参数共享网络相当。

Result: FS-KAN在多种数据类型和对称群上的实验表明，相比标准参数共享层具有显著更高的数据效率，尤其在数据稀缺情况下优势明显，同时保留了KAN的可解释性和适应性。

Conclusion: FS-KAN为处理任意置换对称性的KAN提供了统一且有效的框架，兼具强表达力、高数据效率和良好可解释性，是低数据场景下的理想架构选择。

Abstract: Permutation equivariant neural networks employing parameter-sharing schemes
have emerged as powerful models for leveraging a wide range of data symmetries,
significantly enhancing the generalization and computational efficiency of the
resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated
promise through their improved interpretability and expressivity compared to
traditional architectures based on MLPs. While equivariant KANs have been
explored in recent literature for a few specific data types, a principled
framework for applying them to data with permutation symmetries in a general
context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a
principled approach to constructing equivariant and invariant KA layers for
arbitrary permutation symmetry groups, unifying and significantly extending
previous work in this domain. We derive the basic construction of these FS-KAN
layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup
and provide a theoretical analysis demonstrating that FS-KANs have the same
expressive power as networks that use standard parameter-sharing layers,
allowing us to transfer well-known and important expressivity results from
parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data
types and symmetry groups show that FS-KANs exhibit superior data efficiency
compared to standard parameter-sharing layers, by a wide margin in certain
cases, while preserving the interpretability and adaptability of KANs, making
them an excellent architecture choice in low-data regimes.

</details>


### [805] [One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning](https://arxiv.org/abs/2509.24483)
*Minh Le,Bao-Ngoc Dao,Huy Nguyen,Quyen Tran,Anh Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出SMoPE框架，结合任务特定与共享提示的优势，通过稀疏MoE结构和动态专家选择机制，在持续学习中实现高效、低干扰的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于提示的持续学习方法在计算开销和知识干扰之间的权衡问题。

Method: 将共享提示组织为多个‘提示专家’，采用稀疏MoE架构；引入提示注意力得分聚合机制进行动态专家选择，并设计自适应噪声机制和基于原型的损失函数以平衡专家使用并促进专业化。

Result: 在多个持续学习基准上实验表明，SMoPE在显著降低参数量和计算成本的同时，性能优于任务特定提示方法，并与最先进方法相当。

Conclusion: SMoPE有效平衡了效率与性能，为持续学习中的提示工程提供了新思路。

Abstract: Prompt-based methods have recently gained prominence in Continual Learning
(CL) due to their strong performance and memory efficiency. A prevalent
strategy in this paradigm assigns a dedicated subset of prompts to each task,
which, while effective, incurs substantial computational overhead and causes
memory requirements to scale linearly with the number of tasks. Conversely,
approaches employing a single shared prompt across tasks offer greater
efficiency but often suffer from degraded performance due to knowledge
interference. To reconcile this trade-off, we propose SMoPE, a novel framework
that integrates the benefits of both task-specific and shared prompt
strategies. Inspired by recent findings on the relationship between Prefix
Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into
multiple "prompt experts" within a sparse MoE architecture. For each input,
only a select subset of relevant experts is activated, effectively mitigating
interference. To facilitate expert selection, we introduce a prompt-attention
score aggregation mechanism that computes a unified proxy score for each
expert, enabling dynamic and sparse activation. Additionally, we propose an
adaptive noise mechanism to encourage balanced expert utilization while
preserving knowledge from prior tasks. To further enhance expert
specialization, we design a prototype-based loss function that leverages prefix
keys as implicit memory representations. Extensive experiments across multiple
CL benchmarks demonstrate that SMoPE consistently outperforms task-specific
prompt methods and achieves performance competitive with state-of-the-art
approaches, all while significantly reducing parameter counts and computational
costs.

</details>


### [806] [Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model](https://arxiv.org/abs/2509.24492)
*Charmaine Barker,Daniel Bethell,Simos Gerasimou*

Main category: cs.LG

TL;DR: 本文提出了一种名为GUIDE的轻量级证据学习元模型方法，用于在不重新训练或修改预训练深度学习模型的情况下，显式地学习何时及如何表达不确定性，显著提升了分布外检测和对抗攻击检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的后处理方法无法有效解决预训练模型在分布偏移下的过度自信问题，缺乏对模型何时应表现出不确定性的引导机制。

Method: GUIDE通过校准阶段识别显著内部特征，并利用这些特征构建噪声驱动的课程学习策略，附加于冻结的深度学习模型上，以学习不确定性表达。

Result: GUIDE在无需重新训练、无需架构修改的前提下，将分布外检测性能提升约77%，对抗攻击检测提升约80%，同时保持原有模型在分布内数据上的性能。

Conclusion: 主动引导模型学习不确定性表达是提升预测置信度与可靠性一致性的关键，GUIDE具有广泛适用性和优越性能。

Abstract: Reliable uncertainty quantification remains a major obstacle to the
deployment of deep learning models under distributional shift. Existing
post-hoc approaches that retrofit pretrained models either inherit misplaced
confidence or merely reshape predictions, without teaching the model when to be
uncertain. We introduce GUIDE, a lightweight evidential learning meta-model
approach that attaches to a frozen deep learning model and explicitly learns
how and when to be uncertain. GUIDE identifies salient internal features via a
calibration stage, and then employs these features to construct a noise-driven
curriculum that teaches the model how and when to express uncertainty. GUIDE
requires no retraining, no architectural modifications, and no manual
intermediate-layer selection to the base deep learning model, thus ensuring
broad applicability and minimal user intervention. The resulting model avoids
distilling overconfidence from the base model, improves out-of-distribution
detection by ~77% and adversarial attack detection by ~80%, while preserving
in-distribution performance. Across diverse benchmarks, GUIDE consistently
outperforms state-of-the-art approaches, evidencing the need for actively
guiding uncertainty to close the gap between predictive confidence and
reliability.

</details>


### [807] [LLM DNA: Tracing Model Evolution via Functional Representations](https://arxiv.org/abs/2509.24496)
*Zhaomin Wu,Haodong Zhao,Ziyang Wang,Jizhou Guo,Qian Wang,Bingsheng He*

Main category: cs.LG

TL;DR: 提出LLM DNA概念，用于揭示大语言模型间的进化关系，具有继承性和遗传决定性，并构建了无需训练的通用提取方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型数量激增但其演化关系不清晰，现有方法受限于任务特定性或架构假设，难以有效管理模型谱系。

Method: 定义LLM DNA为功能行为的低维双Lipschitz表示，理论上证明其存在性及遗传性质，并设计无需训练的可扩展提取流程。

Result: 在305个LLM上验证DNA的有效性，与已有研究一致，并发现新的模型关联；构建出符合架构演进和时间顺序的LLM进化树。

Conclusion: LLM DNA提供了一种通用、可扩展的方式来理解模型演化，有助于未来大模型的管理与分析。

Abstract: The explosive growth of large language models (LLMs) has created a vast but
opaque landscape: millions of models exist, yet their evolutionary
relationships through fine-tuning, distillation, or adaptation are often
undocumented or unclear, complicating LLM management. Existing methods are
limited by task specificity, fixed model sets, or strict assumptions about
tokenizers or architectures. Inspired by biological DNA, we address these
limitations by mathematically defining LLM DNA as a low-dimensional,
bi-Lipschitz representation of functional behavior. We prove that LLM DNA
satisfies inheritance and genetic determinism properties and establish the
existence of DNA. Building on this theory, we derive a general, scalable,
training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA
aligns with prior studies on limited subsets and achieves superior or
competitive performance on specific tasks. Beyond these tasks, DNA comparisons
uncover previously undocumented relationships among LLMs. We further construct
the evolutionary tree of LLMs using phylogenetic algorithms, which align with
shifts from encoder-decoder to decoder-only architectures, reflect temporal
progression, and reveal distinct evolutionary speeds across LLM families.

</details>


### [808] [Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models](https://arxiv.org/abs/2509.24510)
*Jonas Hübotter,Patrik Wolf,Alexander Shevchenko,Dennis Jüni,Andreas Krause,Gil Kur*

Main category: cs.LG

TL;DR: 本文提出，尽管基础模型经过大规模训练，但在全局上仍存在欠参数化问题，测试时训练（TTT）通过在泛化后进行专门化，使模型聚焦于与测试任务相关的概念，从而提升性能。作者基于线性表示假设建立理论模型，并通过稀疏自编码器实验和跨模态扩展研究验证了该机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对测试时训练（TTT）为何有效的解释主要集中在分布外适应或特权数据的使用，但随着基础模型规模增大，多数测试数据为分布内，这些解释不再充分。因此，需要新的理论框架来理解TTT在分布内任务中的有效性。

Method: 提出基础模型仍处于全局欠参数化状态，TTT实现了从泛化到专门化的转变；在线性表示假设下构建理论模型，预测TTT能显著降低分布内测试误差；通过在ImageNet上训练稀疏自编码器验证语义相关样本仅由少数共享概念解释的关键假设；并在图像与语言任务中开展扩展性研究以识别最有效的专门化场景。

Result: 实验证明，语义相关的数据点确实由少量共享概念解释，支持了线性表示假设下的模型设定；扩展研究表明，在特定规模和任务条件下，TTT相比全局训练可显著降低分布内测试误差，验证了理论预测。

Conclusion: 测试时训练的有效性源于基础模型的全局欠参数化，TTT作为一种任务特定的专门化机制，在分布内任务中也能带来性能提升，其效果取决于模型容量、任务结构与数据规模之间的匹配关系。

Abstract: Recent empirical studies have explored the idea of continuing to train a
model at test-time for a given task, known as test-time training (TTT), and
have found it to yield significant performance improvements. However, there is
limited understanding of why and when TTT is effective. Earlier explanations
mostly focused on the observation that TTT may help when applied to
out-of-distribution adaptation or used with privileged data. However, the
growing scale of foundation models with most test data being in-distribution
questions these explanations. We instead posit that foundation models remain
globally underparameterized, with TTT providing a mechanism for specialization
after generalization, focusing capacity on concepts relevant to the test task.
Specifically, under the linear representation hypothesis, we propose a model in
which TTT achieves a substantially smaller in-distribution test error than
global training. We empirically validate our model's key assumptions by
training a sparse autoencoder on ImageNet, showing that semantically related
data points are explained by only a few shared concepts. Finally, we perform
scaling studies across image and language tasks that confirm the practical
implications of our model, identifying the regimes where specialization is most
effective.

</details>


### [809] [Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.24517)
*Sophia N. Wilson,Jens Hesselbjerg Christensen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文探讨了在深度学习模型中引入物理归纳偏置，以在保持或提升模型效果的同时显著提高计算和能源效率，减少碳足迹，主张将效率与效果共同作为模型开发的核心考量。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习发展过度关注模型效果，导致大规模、高资源消耗的模型产生巨大碳排放，因此需要探索在保证性能的前提下提升模型效率的新方法。

Method: 通过在时空预测任务中引入不同程度的物理归纳偏置（包括传统物理信息模型和新兴的流匹配模型），系统比较不同模型在效果与效率之间的权衡。

Result: 实验表明，嵌入物理归纳偏置不仅能保持甚至提升预测精度，还能显著降低计算资源消耗和碳排放，实现更高效的模型设计。

Conclusion: 物理归纳偏置为构建高效、低碳的机器学习模型提供了有效且原则性的路径，未来模型开发应同时重视效果与效率。

Abstract: Development of modern deep learning methods has been driven primarily by the
push for improving model efficacy (accuracy metrics). This sole focus on
efficacy has steered development of large-scale models that require massive
resources, and results in considerable carbon footprint across the model
life-cycle. In this work, we explore how physics inductive biases can offer
useful trade-offs between model efficacy and model efficiency (compute, energy,
and carbon). We study a variety of models for spatio-temporal forecasting, a
task governed by physical laws and well-suited for exploring different levels
of physics inductive bias. We show that embedding physics inductive biases into
the model design can yield substantial efficiency gains while retaining or even
improving efficacy for the tasks under consideration. In addition to using
standard physics-informed spatio-temporal models, we demonstrate the usefulness
of more recent models like flow matching as a general purpose method for
spatio-temporal forecasting. Our experiments show that incorporating physics
inductive biases offer a principled way to improve the efficiency and reduce
the carbon footprint of machine learning models. We argue that model
efficiency, along with model efficacy, should become a core consideration
driving machine learning model development and deployment.

</details>


### [810] [LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection](https://arxiv.org/abs/2509.24547)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Linh Ngo Van*

Main category: cs.LG

TL;DR: 本文提出了一种名为LEAF的新框架，用于解决少样本持续事件检测（FCED）中的灾难性遗忘和数据不足问题。该方法采用基于低秩适配的专家混合结构和语义感知专家选择机制，结合对比学习和知识蒸馏，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有FCED方法在全量微调共享模型时容易发生知识干扰，且依赖可能引入噪声的数据增强策略，导致灾难性遗忘和泛化能力差。

Method: 提出LEAF框架：1）引入基于LoRA的专家混合结构；2）设计语义感知的专家选择机制；3）利用标签描述进行对比学习；4）使用知识蒸馏防止记忆缓冲过拟合。

Result: 在多个FCED基准上的实验表明，LEAF在少样本持续学习场景下显著优于现有方法，实现了最先进的性能。

Conclusion: LEAF通过专家隔离、语义引导学习和知识保留机制，有效缓解了FCED中的知识干扰与遗忘问题，具有良好的鲁棒性和泛化能力。

Abstract: Few-shot Continual Event Detection (FCED) poses the dual challenges of
learning from limited data and mitigating catastrophic forgetting across
sequential tasks. Existing approaches often suffer from severe forgetting due
to the full fine-tuning of a shared base model, which leads to knowledge
interference between tasks. Moreover, they frequently rely on data augmentation
strategies that can introduce unnatural or semantically distorted inputs. To
address these limitations, we propose LEAF, a novel and robust expert-based
framework for FCED. LEAF integrates a specialized mixture of experts
architecture into the base model, where each expert is parameterized with
low-rank adaptation (LoRA) matrices. A semantic-aware expert selection
mechanism dynamically routes instances to the most relevant experts, enabling
expert specialization and reducing knowledge interference. To improve
generalization in limited-data settings, LEAF incorporates a contrastive
learning objective guided by label descriptions, which capture high-level
semantic information about event types. Furthermore, to prevent overfitting on
the memory buffer, our framework employs a knowledge distillation strategy that
transfers knowledge from previous models to the current one. Extensive
experiments on multiple FCED benchmarks demonstrate that LEAF consistently
achieves state-of-the-art performance.

</details>


### [811] [Training-Free Multimodal Guidance for Video to Audio Generation](https://arxiv.org/abs/2509.24550)
*Eleonora Grassucci,Giuliano Galadini,Giordano Cicchetti,Aurelio Uncini,Fabio Antonacci,Danilo Comminiello*

Main category: cs.LG

TL;DR: 提出了一种无需训练的多模态引导机制（MDG），利用模态嵌入的体积空间来增强视频到音频生成中的跨模态一致性，可在不重新训练的情况下提升生成音质和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法依赖大规模配对数据联合训练或成对相似性，难以捕捉全局多模态一致性且成本高。

Method: 提出多模态扩散引导（MDG），通过视频、音频和文本模态嵌入所张成的体积空间来实现统一的跨模态对齐，作为一种轻量级、即插即用的控制信号应用于预训练音频扩散模型。

Result: 在VGGSound和AudioCaps上的实验表明，MDG在感知质量和多模态对齐方面均优于基线方法。

Conclusion: MDG能有效提升视频到音频生成的感知质量与跨模态一致性，是一种通用、高效的训练-free多模态引导方案。

Abstract: Video-to-audio (V2A) generation aims to synthesize realistic and semantically
aligned audio from silent videos, with potential applications in video editing,
Foley sound design, and assistive multimedia. Although the excellent results,
existing approaches either require costly joint training on large-scale paired
datasets or rely on pairwise similarities that may fail to capture global
multimodal coherence. In this work, we propose a novel training-free multimodal
guidance mechanism for V2A diffusion that leverages the volume spanned by the
modality embeddings to enforce unified alignment across video, audio, and text.
The proposed multimodal diffusion guidance (MDG) provides a lightweight,
plug-and-play control signal that can be applied on top of any pretrained audio
diffusion model without retraining. Experiments on VGGSound and AudioCaps
demonstrate that our MDG consistently improves perceptual quality and
multimodal alignment compared to baselines, proving the effectiveness of a
joint multimodal guidance for V2A.

</details>


### [812] [Short window attention enables long-term memorization](https://arxiv.org/abs/2509.24552)
*Loïc Cabannes,Maximilian Beck,Gergely Szilvasy,Matthijs Douze,Maria Lomeli,Jade Copet,Pierre-Emmanuel Mazaré,Gabriel Synnaeve,Hervé Jégou*

Main category: cs.LG

TL;DR: 本文提出了一种结合滑动窗口注意力和xLSTM线性RNN的混合架构SWAX，发现较大的滑动窗口并不提升长上下文性能，反而短窗口有助于训练xLSTM的长期记忆。为兼顾长短上下文任务，采用随机变窗大小训练，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 研究滑动窗口长度对混合架构中softmax注意力与线性RNN层协同作用的影响，解决现有方法在长短期上下文处理上的权衡问题。

Method: 提出SWAX模型，结合滑动窗口注意力与xLSTM；通过随机改变训练时的滑动窗口大小，促使模型同时利用长上下文窗口和xLSTM的记忆能力。

Result: 实验发现较大滑动窗口并未改善长上下文性能，而短窗口更利于激发xLSTM的长期记忆；采用随机窗口大小训练的SWAX在短和长上下文任务上均显著优于固定窗口注意力模型。

Conclusion: 滑动窗口大小的选择对混合架构性能有重要影响，随机变窗训练策略能有效平衡短程与长程依赖建模，提升整体性能。

Abstract: Recent works show that hybrid architectures combining sliding window softmax
attention layers with linear recurrent neural network (RNN) layers outperform
both of these architectures taken separately. However, the impact of the window
length and the interplay between softmax attention and linear RNN layers remain
under-studied. In this work, we introduce SWAX, a hybrid architecture
consisting of sliding-window attention and xLSTM linear RNN layers.
  A counter-intuitive finding with SWAX is that larger sliding windows do not
improve the long-context performance. In fact, short window attention
encourages the model to better train the long-term memory of the xLSTM, by
relying less on the softmax attention mechanism for long context-retrieval.
  The issue with small sliding windows is that they are detrimental for
short-context tasks, which could be solved with information from moderately
larger sliding windows otherwise. Therefore, we train SWAX by stochastically
changing the sliding window size, forcing the model to leverage both a longer
context window and the xLSTM memory. SWAX trained with stochastic window sizes
significantly outperforms regular window attention both on short and
long-context problems.

</details>


### [813] [Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations](https://arxiv.org/abs/2509.24556)
*Hussam Sababha,Bernat Font,Mohammed Daqaq*

Main category: cs.LG

TL;DR: 本研究在高雷诺数（Re = 3000）下，通过旋转驱动实验部署深度强化学习（DRL）实现对圆柱体涡激振动（VIV）的主动流动控制，克服了执行器延迟等实际限制，实现了超过95%的振动抑制。


<details>
  <summary>Details</summary>
Motivation: 以往研究多基于低雷诺数数值模拟，缺乏真实实验环境下的验证。本研究旨在探索DRL在高雷诺数、真实实验条件下对复杂流动控制问题的适用性和鲁棒性，特别是应对执行器延迟等工程挑战。

Method: 采用深度强化学习（DRL）框架，利用振荡圆柱的位移和速度作为状态反馈进行实时控制；通过引入历史控制动作增强状态输入，以补偿执行器延迟，使DRL代理学习高频旋转控制策略。

Result: 仅使用状态反馈时，DRL实现最高80%的振动抑制，依赖于锁频现象；加入历史控制动作后，系统学习到高频控制策略，有效调控涡脱落，振动衰减超过95%。

Conclusion: DRL能够适应复杂的实验流动控制环境，并可通过输入设计克服硬件延迟等实际限制，在真实主动流动控制中展现出巨大潜力。

Abstract: This study showcases an experimental deployment of deep reinforcement
learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV)
in a circular cylinder at a high Reynolds number (Re = 3000) using rotary
actuation. Departing from prior work that relied on low-Reynolds-number
numerical simulations, this research demonstrates real-time control in a
challenging experimental setting, successfully addressing practical constraints
such as actuator delay. When the learning algorithm is provided with state
feedback alone (displacement and velocity of the oscillating cylinder), the DRL
agent learns a low-frequency rotary control strategy that achieves up to 80%
vibration suppression which leverages the traditional lock-on phenomenon. While
this level of suppression is significant, it remains below the performance
achieved using high-frequency rotary actuation. The reduction in performance is
attributed to actuation delays and can be mitigated by augmenting the learning
algorithm with past control actions. This enables the agent to learn a
high-frequency rotary control strategy that effectively modifies vortex
shedding and achieves over 95% vibration attenuation. These results demonstrate
the adaptability of DRL for AFC in real-world experiments and its ability to
overcome instrumental limitations such as actuation lag.

</details>


### [814] [Emergent World Representations in OpenVLA](https://arxiv.org/abs/2509.24559)
*Marco Molinari,Leonardo Nevali,Saharsha Navani,Omar G. Younis*

Main category: cs.LG

TL;DR: 本文研究了基于策略的强化学习训练的视觉语言动作模型（VLAs）是否隐式地学习了世界模型，通过在状态表示上使用嵌入算术的方法探测OpenVLA中是否存在潜在的状态转移知识。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Language Action模型在不显式建模环境动态的情况下，是否隐含地学习到了状态转移的世界模型，这是模型基础强化学习的一个标志。

Method: 提出了一种实验方法，利用连续环境状态的嵌入差异来检测过渡向量是否可以从中间模型激活中恢复，并使用线性和非线性探针训练各层的激活以评估预测能力。

Result: 发现OpenVLA在状态转换上的预测能力显著超过基线，表明其编码了一个内部世界模型；此外，早期检查点的结果提示该世界模型随着训练进展而出现。

Conclusion: OpenVLA确实隐式地学习并编码了内部世界模型，且这种能力随训练逐步显现，为理解VLAs的工作机制提供了新视角。

Abstract: Vision Language Action models (VLAs) trained with policy-based reinforcement
learning (RL) encode complex behaviors without explicitly modeling
environmental dynamics. However, it remains unclear whether VLAs implicitly
learn world models, a hallmark of model-based RL. We propose an experimental
methodology using embedding arithmetic on state representations to probe
whether OpenVLA, the current state of the art in VLAs, contains latent
knowledge of state transitions. Specifically, we measure the difference between
embeddings of sequential environment states and test whether this transition
vector is recoverable from intermediate model activations. Using linear and non
linear probes trained on activations across layers, we find statistically
significant predictive ability on state transitions exceeding baselines
(embeddings), indicating that OpenVLA encodes an internal world model (as
opposed to the probes learning the state transitions). We investigate the
predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that
the world model emerges as training progresses. Finally, we outline a pipeline
leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.

</details>


### [815] [Learning to Solve Optimization Problems Constrained with Partial Differential Equations](https://arxiv.org/abs/2509.24573)
*Yusuf Guven,Vincenzo Di Vito,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: 本文提出了一种基于学习的双网络框架，结合动态预测器和优化代理，用于高效求解偏微分方程约束优化问题，在保持解质量的同时显著提升计算速度。


<details>
  <summary>Details</summary>
Motivation: PDE约束优化问题因决策变量与状态变量紧密耦合且需处理高维离散化和动态约束，导致计算成本高昂，传统方法难以实现实时求解。

Method: 提出一种学习框架，包含时间离散神经算子作为动态预测器以近似PDE系统轨迹，以及基于代理优化器技术的优化代理来近似最优决策，实现端到端的联合训练与实时推断。

Result: 在Burgers方程、热传导方程和电压调节等基准任务上验证了方法的有效性，解的质量与直接法和模型预测控制（MPC）相当，计算速度提升了多达四个数量级。

Conclusion: 该双网络框架能有效捕捉决策与PDE动力学之间的耦合关系，为PDE约束优化提供了一种高效、可扩展的替代方案，适用于需要快速响应的实际应用。

Abstract: Partial differential equation (PDE)-constrained optimization arises in many
scientific and engineering domains, such as energy systems, fluid dynamics and
material design. In these problems, the decision variables (e.g., control
inputs or design parameters) are tightly coupled with the PDE state variables,
and the feasible set is implicitly defined by the governing PDE constraints.
This coupling makes the problems computationally demanding, as it requires
handling high dimensional discretization and dynamic constraints. To address
these challenges, this paper introduces a learning-based framework that
integrates a dynamic predictor with an optimization surrogate. The dynamic
predictor, a novel time-discrete Neural Operator (Lu et al.), efficiently
approximate system trajectories governed by PDE dynamics, while the
optimization surrogate leverages proxy optimizer techniques (Kotary et al.) to
approximate the associated optimal decisions. This dual-network design enables
real-time approximation of optimal strategies while explicitly capturing the
coupling between decisions and PDE dynamics. We validate the proposed approach
on benchmark PDE-constrained optimization tasks inlacing Burgers' equation,
heat equation and voltage regulation, and demonstrate that it achieves solution
quality comparable to classical control-based algorithms, such as the Direct
Method and Model Predictive Control (MPC), while providing up to four orders of
magnitude improvement in computational speed.

</details>


### [816] [SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems](https://arxiv.org/abs/2509.24580)
*Lingyu Wang,Xiangming Meng*

Main category: cs.LG

TL;DR: 提出SAIP，一种即插即用的自适应模块，用于在每一步动态调整先验与似然得分之间的平衡尺度，提升扩散模型在图像恢复任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定的、手动调参的尺度来平衡先验和似然得分，这种静态设计在不同时间和任务上表现次优，限制了性能和泛化能力。

Method: 提出SAIP模块，通过无需重新训练或修改扩散模型结构的方式，在每个时间步自适应地优化尺度，以更好地融合先验和 likelihood 得分。

Result: SAIP能无缝集成到现有采样器中，在多种图像恢复任务（包括具有挑战性的场景）中显著提升重建质量。

Conclusion: SAIP通过自适应尺度调节机制，有效解决了扩散模型中先验与似然不平衡的问题，具备良好的通用性和即插即用特性。

Abstract: Solving inverse problems with diffusion models has shown promise in tasks
such as image restoration. A common approach is to formulate the problem in a
Bayesian framework and sample from the posterior by combining the prior score
with the likelihood score. Since the likelihood term is often intractable,
estimators like DPS, DMPS, and $\pi$GDM are widely adopted. However, these
methods rely on a fixed, manually tuned scale to balance prior and likelihood
contributions. Such a static design is suboptimal, as the ideal balance varies
across timesteps and tasks, limiting performance and generalization. To address
this issue, we propose SAIP, a plug-and-play module that adaptively refines the
scale at each timestep without retraining or altering the diffusion backbone.
SAIP integrates seamlessly into existing samplers and consistently improves
reconstruction quality across diverse image restoration tasks, including
challenging scenarios.

</details>


### [817] [CURA: Size Isnt All You Need -- A Compact Universal Architecture for On-Device Intelligence](https://arxiv.org/abs/2509.24601)
*Jae-Bum Seo,Muhammad Salman,Lismer Andres Caceres-Najarro*

Main category: cs.LG

TL;DR: 本文提出了一种受模拟音频信号处理电路启发的新型轻量级AI架构CURA，能够在多种任务（如回归、分类、NLP和计算机视觉）中实现高通用性、高紧凑性和强复杂模式识别能力，且参数量最多减少2500倍，性能接近专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有设备端AI架构在资源受限环境中存在模型参数随任务复杂度增长而增加（缺乏紧凑性）以及难以跨领域泛化（如无法同时处理回归和NLP任务）的问题。

Method: 设计了一种名为CURA的新架构，灵感来自模拟音频信号处理电路，通过结构创新实现低参数量和跨域适应能力，适用于多种机器学习任务。

Result: 实验表明，CURA在保持同等准确率下最多仅需基线模型1/2500的参数；在四个NLP基准和一个计算机视觉数据集上表现稳定，F1分数高达90%；在复杂模式预测任务中，MAE降低1.6倍，MSE降低2.1倍。

Conclusion: CURA在紧凑性、通用性和复杂模式捕捉方面优于现有方法，为资源受限设备提供了一种高效、多用途的AI解决方案。

Abstract: Existing on-device AI architectures for resource-constrained environments
face two critical limitations: they lack compactness, with parameter
requirements scaling proportionally to task complexity, and they exhibit poor
generalizability, performing effectively only on specific application domains
(e.g., models designed for regression tasks cannot adapt to natural language
processing (NLP) applications). In this paper, we propose CURA, an architecture
inspired by analog audio signal processing circuits that provides a compact and
lightweight solution for diverse machine learning tasks across multiple
domains. Our architecture offers three key advantages over existing approaches:
(1) Compactness: it requires significantly fewer parameters regardless of task
complexity; (2) Generalizability: it adapts seamlessly across regression,
classification, complex NLP, and computer vision tasks; and (3) Complex pattern
recognition: it can capture intricate data patterns while maintaining extremely
low model complexity. We evaluated CURA across diverse datasets and domains.
For compactness, it achieved equivalent accuracy using up to 2,500 times fewer
parameters compared to baseline models. For generalizability, it demonstrated
consistent performance across four NLP benchmarks and one computer vision
dataset, nearly matching specialized existing models (achieving F1-scores up to
90%). Lastly, it delivers superior forecasting accuracy for complex patterns,
achieving 1.6 times lower mean absolute error and 2.1 times lower mean squared
error than competing models.

</details>


### [818] [Evaluating classification performance across operating contexts: A comparison of decision curve analysis and cost curves](https://arxiv.org/abs/2509.24608)
*Louise AC Millard,Peter A Flach*

Main category: cs.LG

TL;DR: 本文比较了决策曲线分析（DCA）和代价曲线在评估分类模型中的关系与优缺点，指出决策曲线与Brier曲线密切相关，两者在模型选择上具有一致性，但Brier曲线更具通用性且其曲线下面积即为Brier得分。


<details>
  <summary>Details</summary>
Motivation: 为了更合理地评估分类模型，需考虑模型使用情境中正负样本分类的相对价值，进而影响决策阈值的选择。现有方法如DCA和代价曲线分别评估模型的期望效用和期望损失，但二者关系尚不清晰。

Method: 通过理论推导和假设校准模型得分，分析DCA中的净收益与Brier曲线中的Brier损失之间的关系，并比较两种曲线在不同阈值下的表现及其可比性。

Result: 发现决策曲线与Brier曲线在x轴和最优模型选择上一致；净收益和Brier损失在任一阈值下总选择相同最优模型；Brier损失在不同阈值间具有可比性，而净收益不可比；Brier曲线下面积等于Brier得分，且Brier曲线适用范围更广。

Conclusion: Brier曲线比DCA更具通用性，且能更好地支持跨阈值比较，建议在模型评估中结合使用Brier曲线和上包络决策曲线以评估再校准潜力。

Abstract: Classification models typically predict a score and use a decision threshold
to produce a classification. Appropriate model evaluation should carefully
consider the context in which a model will be used, including the relative
value of correct classifications of positive versus negative examples, which
affects the threshold that should be used. Decision curve analysis (DCA) and
cost curves are model evaluation approaches that assess the expected utility
and expected loss of prediction models, respectively, across decision
thresholds. We compared DCA and cost curves to determine how they are related,
and their strengths and limitations. We demonstrate that decision curves are
closely related to a specific type of cost curve called a Brier curve. Both
curves are derived assuming model scores are calibrated and setting the
classification threshold using the relative value of correct positive and
negative classifications, and the x-axis of both curves are equivalent. Net
benefit (used for DCA) and Brier loss (used for Brier curves) will always
choose the same model as optimal at any given threshold. Across thresholds,
differences in Brier loss are comparable whereas differences in net benefit
cannot be compared. Brier curves are more generally applicable (when a wider
range of thresholds are plausible), and the area under the Brier curve is the
Brier score. We demonstrate that reference lines common in each space can be
included in either and suggest the upper envelope decision curve as a useful
comparison for DCA showing the possible gain in net benefit that could be
achieved through recalibration alone.

</details>


### [819] [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
*Liang Lin,Zhihao Xu,Junhao Dong,Jian Zhao,Yuchen Yuan,Guibin Zhang,Miao Yu,Yiming Zhang,Zhengtao Yao,Huahui Yi,Dongrui Liu,Xinfeng Li,Kun Wang*

Main category: cs.LG

TL;DR: 本文提出了OrthAlign，一种通过正交子空间分解来解决多目标偏好对齐中梯度冲突的新方法，实现了在帮助性、无害性和真实性等多个维度上的稳定优化与性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对齐多个人类偏好时面临各目标间相互冲突的难题，传统方法未能从根本上解决参数层面的冲突问题。

Method: 提出OrthAlign方法，利用正交子空间分解将参数更新空间分离，使不同偏好的优化方向互不干扰，并结合谱范数约束提供理论收敛保证。

Result: 实验表明，OrthAlign在单个偏好上的最大改进达34.61%至50.89%，平均总体奖励提升13.96%，且优化过程更稳定。

Conclusion: OrthAlign通过在参数层面解耦多目标优化方向，有效缓解了对齐中的权衡问题，为多偏好对齐提供了新的有效范式。

Abstract: Large language model (LLM) alignment faces a critical dilemma when addressing
multiple human preferences: improvements in one dimension frequently come at
the expense of others, creating unavoidable trade-offs between competing
objectives like helpfulness and harmlessness. While prior work mainly focuses
on constraint-based optimization algorithms and data selection strategies to
mitigate conflicts, these approaches overlook the fundamental issue of
resolving conflicts directly at the parameter level. In this paper, we present
OrthAlign, an innovative approach that pioneers a new paradigm by leveraging
orthogonal subspace decomposition to fundamentally resolve gradient-level
conflicts in multi-objective preference alignment. OrthAlign strategically
decomposes parameter update spaces into orthogonal subspaces, ensuring that
optimization toward different preferences occurs in mathematically
non-interfering directions. Building upon this, we provide theoretical
guarantees demonstrating that when parameter increments satisfy both orthogonal
subspace constraints and spectral norm bounds, the resulting updates exhibit
linear Lipschitz growth rather than exponential instability, ensuring stable
convergence across all preference dimensions. Extensive experiments show that:
I. OrthAlign achieves maximum single-preference improvements ranging from
34.61% to 50.89% after multiple-objective alignment across helpful, harmless,
and truthful dimensions. II. With an average overall reward improvement of
13.96%.

</details>


### [820] [Learning Hamiltonian Dynamics at Scale: A Differential-Geometric Approach](https://arxiv.org/abs/2509.24627)
*Katharina Friedl,Noémie Jaquier,Mika Liao,Danica Kragic*

Main category: cs.LG

TL;DR: 本文提出了一种名为几何降阶哈密顿神经网络（RO-HNN）的新方法，结合了哈密顿力学的守恒定律与模型降阶的可扩展性，用于高效、稳定地建模高维物理系统的动力学。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的神经网络在保持能量守恒等基本性质方面表现良好，但在处理高维系统时面临扩展性难题。因此，亟需一种既能保持物理一致性又具备良好可扩展性的模型。

Method: RO-HNN包含两个核心组件：一种新颖的几何约束辛自编码器，用于学习低维、结构保持的辛子流形；以及一个几何哈密顿神经网络，用于在该子流形上建模动力学。该方法融合了辛几何与降阶建模的思想。

Result: 实验表明，RO-HNN能够对复杂高维动力学提供物理一致、稳定且具有良好泛化能力的预测。

Conclusion: RO-HNN有效拓展了哈密顿神经网络在高维物理系统中的应用范围，实现了物理保真性与计算可扩展性的良好平衡。

Abstract: By embedding physical intuition, network architectures enforce fundamental
properties, such as energy conservation laws, leading to plausible predictions.
Yet, scaling these models to intrinsically high-dimensional systems remains a
significant challenge. This paper introduces Geometric Reduced-order
Hamiltonian Neural Network (RO-HNN), a novel physics-inspired neural network
that combines the conservation laws of Hamiltonian mechanics with the
scalability of model order reduction. RO-HNN is built on two core components: a
novel geometrically-constrained symplectic autoencoder that learns a
low-dimensional, structure-preserving symplectic submanifold, and a geometric
Hamiltonian neural network that models the dynamics on the submanifold. Our
experiments demonstrate that RO-HNN provides physically-consistent, stable, and
generalizable predictions of complex high-dimensional dynamics, thereby
effectively extending the scope of Hamiltonian neural networks to
high-dimensional physical systems.

</details>


### [821] [Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory](https://arxiv.org/abs/2509.24653)
*Pengxiao Lin,Zheng-An Chen,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 本文提出了Identity Bridge机制，通过监督模型在零跳身份任务上来解决大语言模型在双跳推理任务中的组合性缺陷，并通过理论分析和实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合推理任务上表现不佳，尤其是在双跳推理任务中存在严重问题，需要一种有效的方法来提升其推理能力。

Method: 引入Identity Bridge机制，在零跳身份任务上进行监督；使用简化Emb-MLP模型进行理论分析，证明身份监督通过隐式的核范数正则化重塑模型潜在几何结构；对复杂任务采用小初始化或权重衰减增强正则化效果。

Result: 实验证明该方法使模型能够成功执行分布外的双跳推理任务；理论分析表明身份监督促使潜在空间对齐并减缓泛化性能下降；在大规模模型上的扩展研究表明其仍可通过潜在记忆实现双跳推理。

Conclusion: Identity Bridge机制有效解决了大模型在组合推理上的局限，为提升其隐式推理能力提供了重要启示。

Abstract: Despite remarkable advances, large language models often fail at
compositional reasoning tasks, a phenomenon exemplified by the ``curse of
two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet
powerful mechanism that resolves this compositionality gap by supervising the
model on a zero-hop identity task. We demonstrate empirically that this
addition enables models to successfully perform out-of-distribution two-hop
reasoning, a task they otherwise completely fail. To explain this phenomenon,
we provide a theoretical analysis using a simplified Emb-MLP model, proving
that identity supervision reshapes the model's latent geometry. We show this
alignment is induced by an implicit nuclear-norm regularization during
optimization, which favors low-rank solutions that share structure across
tasks. For complex tasks, we use small initialization or weight decay to
enhance the regularization effect, which enhances the latent space alignment
effect and slows down the generalization decay. Finally, we extend our
investigation to large-scale models, observing that they still achieve two-hop
reasoning through the latent memory, which provides crucial inspiration for
enhancing their implicit reasoning abilities.

</details>


### [822] [HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling](https://arxiv.org/abs/2509.24655)
*Max van Spengler,Artem Moskalev,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: 本文提出了HyperHELM，一种在双曲空间中对mRNA序列进行掩码语言模型预训练的框架，通过结合欧几里得主干与双曲层，更好地建模mRNA与氨基酸之间的生物层级结构，在多项任务中优于传统欧几里得模型。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理如蛋白质和mRNA等生物序列时，其默认的欧几里得几何可能无法匹配生物数据内在的层次结构；而双曲几何更适合表达层级数据，但尚未被应用于mRNA序列的语言建模，因此需要一种能利用双曲几何优势的新框架。

Method: 提出HyperHELM框架，采用混合设计，在欧几里得主干网络之上引入双曲层，实现mRNA序列在双曲空间中的掩码语言模型预训练，并使学习到的表示与mRNA和氨基酸间的生物学层级关系对齐。

Result: 在多个跨物种数据集上，HyperHELM在10项任务中的9项性能超过欧几里得基线模型，平均提升10%，在长序列和低GC含量序列的分布外泛化表现优异；在抗体区域注释任务中，比考虑层级结构的欧几里得模型准确率提高3%。

Conclusion: 双曲几何可作为mRNA序列层次化语言建模的有效归纳偏置，HyperHELM为生物序列建模提供了新的有效范式。

Abstract: Language models are increasingly applied to biological sequences like
proteins and mRNA, yet their default Euclidean geometry may mismatch the
hierarchical structures inherent to biological data. While hyperbolic geometry
provides a better alternative for accommodating hierarchical data, it has yet
to find a way into language modeling for mRNA sequences. In this work, we
introduce HyperHELM, a framework that implements masked language model
pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with
hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned
representations with the biological hierarchy defined by the relationship
between mRNA and amino acids. Across multiple multi-species datasets, it
outperforms Euclidean baselines on 9 out of 10 tasks involving property
prediction, with 10% improvement on average, and excels in out-of-distribution
generalization to long and low-GC content sequences; for antibody region
annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation
accuracy. Our results highlight hyperbolic geometry as an effective inductive
bias for hierarchical language modeling of mRNA sequences.

</details>


### [823] [T-POP: Test-Time Personalization with Online Preference Feedback](https://arxiv.org/abs/2509.24696)
*Zikun Qu,Min Zhang,Mingze Kong,Xiang Li,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Shuang Qiu,Yao Shu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: 提出T-POP算法，通过在线偏好反馈实现大模型实时个性化，无需微调且数据高效。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法对新用户存在冷启动问题，需大量用户数据或昂贵微调。

Method: 提出T-POP，结合测试时对齐与对决bandits，在解码过程中在线学习奖励函数以引导生成。

Result: 实验表明T-POP在少量交互下快速实现个性化，显著优于基线方法，且随交互增多持续提升。

Conclusion: T-POP为大语言模型的实时个性化提供了一种高效、无需参数更新的新范式。

Abstract: Personalizing large language models (LLMs) to individual user preferences is
a critical step beyond generating generically helpful responses. However,
current personalization methods are ill-suited for new users, as they typically
require either slow, resource-intensive fine-tuning or a substantial amount of
pre-existing user data, creating a significant cold-start problem. To address
this challenge, we introduce a new paradigm for real-time personalization by
learning from online pairwise preference feedback collected during text
generation. We propose T-POP (Test-Time Personalization with Online Preference
Feedback}), a novel algorithm that synergistically combines test-time alignment
with dueling bandits. Without updating the LLM parameters, T-POP steers the
decoding process of a frozen LLM by learning a reward function online that
captures user preferences. By leveraging dueling bandits, T-POP intelligently
queries the user to efficiently balance between exploring their preferences and
exploiting the learned knowledge to generate personalized text. Extensive
experiments demonstrate that T-POP achieves rapid and data-efficient
personalization, significantly outperforming existing baselines and showing
consistent improvement with more user interactions.

</details>


### [824] [FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits](https://arxiv.org/abs/2509.24701)
*Pingchen Lu,Zhi Hong,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Yao Shu,Min Zhang,Shuang Qiu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: 本文提出了一种基于多臂赌博机（MAB）的联邦提示优化框架，以解决黑箱模型、样本效率和隐私保护协作三大挑战。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型对输入提示敏感，提示优化至关重要，但面临黑箱模型、查询成本高和隐私保护协作困难的问题。

Method: 提出了FedPOB算法及其扩展FedPOB-Pref，分别基于线性UCB和联邦对决带反馈的多臂赌博机框架，通过共享模型参数实现协作学习。

Result: 实验表明，FedPOB和FedPOB-Pref显著优于现有基线方法，并且随着参与代理数量增加，性能持续提升。

Conclusion: 所提出的联邦提示优化框架在样本效率、隐私保护和协作增益方面均表现出色，适用于实际应用场景。

Abstract: The performance of large language models (LLMs) is highly sensitive to the
input prompt, making prompt optimization a critical task. However, real-world
application is hindered by three major challenges: (1) the black-box nature of
powerful proprietary LLMs, (2) the need for high sample efficiency due to query
costs, and (3) the desire for privacy-preserving collaboration among multiple
users. To address these challenges simultaneously, we introduce a novel
framework for sample-efficient federated prompt optimization based on
multi-armed bandits (MABs). The MAB framework is uniquely suited for this
problem as it is (1) inherently a black-box optimization method, (2)
practically sample-efficient, and (3) enables collaborative learning with
theoretically guaranteed benefit from more participating agents. We first
propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a
federated variant of the Linear UCB algorithm, where agents collaborate by
sharing model parameters instead of raw data. We then extend our approach to
the practical setting of comparative user feedback by introducing FedPOB with
Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated
dueling bandits. Extensive experiments demonstrate that both FedPOB and
FedPOB-Pref significantly outperform existing baselines and that their
performance consistently improves as more agents participate in the
collaboration, validating the effectiveness of our federated approach.

</details>


### [825] [Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF](https://arxiv.org/abs/2509.24713)
*Jing Liu*

Main category: cs.LG

TL;DR: 提出了一种基于机制可解释性的框架CART，用于识别奖励模型中处理长尾分布的专用神经回路，并通过电路分析指导数据增强、正则化和集成策略，提升长尾场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习（RLHF）奖励模型在长尾分布上存在系统性失效，易导致奖励黑客和模型失对齐问题。

Method: 借鉴语言模型中稀有词元的分布式专业化研究，假设奖励模型也存在针对长尾事件的功能性独立电路；建立理论框架连接电路特化、奖励泛化边界与长尾性能；提出Circuit-Aware Reward Training (CART)，利用电路分析指导数据增强、正则化和集成方法。

Result: 该框架揭示了奖励模型在长尾场景下失败的机制，并提供了提升长尾性能的实用训练策略。

Conclusion: 通过识别并利用奖励模型中的专用神经电路，CART能有效改善其在长尾分布上的泛化能力与鲁棒性，缓解奖励欺骗和失对齐问题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) reward models exhibit
systematic failures on longtail distributions, leading to reward hacking and
misalignment. We propose a mechanistic interpretability framework that
identifies specialized neural circuits responsible for rare-event processing in
reward models. Drawing from recent advances showing distributed specialization
for rare tokens in language models\citep{liu2025no, liu2025emergent}, we
hypothesize that reward models also develop functionally distinct circuits for
longtail scenarios. Our theoretical framework establishes formal connections
between circuit specialization, reward generalization bounds, and longtail
performance. We introduce \textbf{Circuit-Aware Reward Training (CART)}, which
uses circuit analysis to guide data augmentation, regularization, and ensemble
strategies. This approach provides both theoretical insights into reward model
failures and practical interventions for improving longtail robustness.

</details>


### [826] [Q-Net: Transferable Queue Length Estimation via Kalman-based Neural Networks](https://arxiv.org/abs/2509.24725)
*Ting Gao,Elvin Isufi,Winnie Daamen,Erik-Sander Smits,Serge Hoogendoorn*

Main category: cs.LG

TL;DR: 本文提出了一种名为Q-Net的高效、可解释框架，用于在部分观测条件下估计信号交叉口的排队长度，结合环形检测器数据和聚合浮动车数据，利用改进的KalmanNet滤波器实现高精度、可迁移的实时估计。


<details>
  <summary>Details</summary>
Motivation: 在交通流未被完全捕捉的部分观测条件下，准确估计信号交叉口的排队长度仍具挑战性，尤其当传统交通守恒假设不成立时。

Method: Q-Net融合环形检测器的车辆计数和聚合浮动车数据（aFCD）的分段平均速度，采用定制的状态空间模型和AI增强的KalmanNet滤波器学习卡尔曼增益，解耦测量维度与路段长度，提升空间可迁移性。

Result: 在荷兰鹿特丹主干道上的实验表明，Q-Net在均方根误差（RMSE）上比基线方法提升超60%，能准确追踪排队形成与消散，并校正aFCD引起的延迟，具备良好的时空可迁移性。

Conclusion: Q-Net是一种无需依赖摄像头或雷达等昂贵设备的数据高效、可解释排队估计方案，具备部署于实时动态交通控制系统的潜力。

Abstract: Estimating queue lengths at signalized intersections remains a challenge in
traffic management, especially under partially observed conditions where
vehicle flows are not fully captured. This paper introduces Q-Net, a
data-efficient and interpretable framework for queue length estimation that
performs robustly even when traffic conservation assumptions are violated.
Q-Net integrates two widely available and privacy-friendly data sources: (i)
vehicle counts from loop detectors near stop lines, and (ii) aggregated
floating car data (aFCD), which divides each road section into segments and
provides segment-wise average speed measurements. These data sources often
differ in spatial and temporal resolution, creating fusion challenges. Q-Net
addresses this by employing a tailored state-space model and an AI-augmented
Kalman filter, KalmanNet, which learns the Kalman gain from data without
requiring prior knowledge of noise covariances or full system dynamics. We
build on the vanilla KalmanNet pipeline to decouple measurement dimensionality
from section length, enabling spatial transferability across road segments.
Unlike black-box models, Q-Net maintains physical interpretability, with
internal variables linked to real-world traffic dynamics. Evaluations on main
roads in Rotterdam, the Netherlands, demonstrate that Q-Net outperforms
baseline methods by over 60\% in Root Mean Square Error (RMSE), accurately
tracking queue formation and dissipation while correcting aFCD-induced delays.
Q-Net also demonstrates strong spatial and temporal transferability, enabling
deployment without costly sensing infrastructure like cameras or radar.
Additionally, we propose a real-time variant of Q-Net, highlighting its
potential for integration into dynamic, queue-based traffic control systems.

</details>


### [827] [Beyond Softmax: A Natural Parameterization for Categorical Random Variables](https://arxiv.org/abs/2509.24728)
*Alessandro Manenti,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出一种名为catnat的新函数，通过层次化二分分裂替代softmax，改善梯度下降性能，在多种任务中提升学习效率和模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决离散隐变量在深度学习中因softmax函数的信息几何局限性导致的梯度下降困难问题。

Method: 引入catnat函数，采用层次化二分结构，并证明其能产生对角化的Fisher信息矩阵，从而优化梯度估计。

Result: 在图结构学习、变分自编码器和强化学习等多个实验中，catnat显著提高学习效率和测试性能。

Conclusion: catnat是比softmax更优的选择，易于集成且兼容现有训练技术。

Abstract: Latent categorical variables are frequently found in deep learning
architectures. They can model actions in discrete reinforcement-learning
environments, represent categories in latent-variable models, or express
relations in graph neural networks. Despite their widespread use, their
discrete nature poses significant challenges to gradient-descent learning
algorithms. While a substantial body of work has offered improved gradient
estimation techniques, we take a complementary approach. Specifically, we: 1)
revisit the ubiquitous $\textit{softmax}$ function and demonstrate its
limitations from an information-geometric perspective; 2) replace the
$\textit{softmax}$ with the $\textit{catnat}$ function, a function composed of
a sequence of hierarchical binary splits; we prove that this choice offers
significant advantages to gradient descent due to the resulting diagonal Fisher
Information Matrix. A rich set of experiments - including graph structure
learning, variational autoencoders, and reinforcement learning - empirically
show that the proposed function improves the learning efficiency and yields
models characterized by consistently higher test performance. $\textit{Catnat}$
is simple to implement and seamlessly integrates into existing codebases.
Moreover, it remains compatible with standard training stabilization techniques
and, as such, offers a better alternative to the $\textit{softmax}$ function.

</details>


### [828] [Who invented deep residual learning?](https://arxiv.org/abs/2509.24732)
*Juergen Schmidhuber*

Main category: cs.LG

TL;DR: 本文追溯了深度残差学习的发展历程，并探讨了残差连接的发明者。


<details>
  <summary>Details</summary>
Motivation: 明确深度残差学习的起源和关键贡献者，澄清学术史上的归属问题。

Method: 通过梳理相关文献，构建深度残差学习的技术发展时间线。

Result: 呈现了残差连接技术演进的关键节点和重要研究工作。

Conclusion: 虽然2015年何恺明等人的论文影响最大，但残差结构的思想有更早的源头，其发展是多位研究者共同推动的结果。

Abstract: Modern AI is based on deep artificial neural networks (NNs). As of 2025, the
most cited scientific article of the 21st century is an NN paper on deep
residual learning with residual connections. Who invented this? We present a
timeline of the evolution of deep residual learning.

</details>


### [829] [A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity](https://arxiv.org/abs/2509.24734)
*Giordano Cicchetti,Eleonora Grassucci,Danilo Comminiello*

Main category: cs.LG

TL;DR: 本文提出了TRIANGLE，一种新的三模态神经几何学习方法，通过在高维空间中计算三角形面积相似性来改进多模态对齐，避免了额外的融合层或成对相似性计算，在视频-文本、音频-文本检索和音频-视频分类任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在对齐多个模态方面存在不足，导致下游任务中信息利用不充分，缺乏有效指标来确保所有模态都被正确对齐。

Method: 提出TRIANGLE方法，利用模态嵌入所张成的高维空间中的三角形面积作为相似性度量，并将其应用于对比损失函数中以替代余弦相似性，从而实现三模态的联合对齐。

Result: 在多个三模态任务（如视频-文本检索、音频-文本检索和音频-视频分类）中，TRIANGLE在不同数据集上均达到最先进的性能，相比基于余弦相似性的方法Recall@1最高提升达9个百分点。

Conclusion: TRIANGLE通过几何驱动的相似性度量有效提升了多模态模型的对齐效果和可解释性，为多模态学习提供了一种无需复杂融合结构的高效解决方案。

Abstract: Multimodal learning plays a pivotal role in advancing artificial intelligence
systems by incorporating information from multiple modalities to build a more
comprehensive representation. Despite its importance, current state-of-the-art
models still suffer from severe limitations that prevent the successful
development of a fully multimodal model. Such methods may not provide
indicators that all the involved modalities are effectively aligned. As a
result, some modalities may not be aligned, undermining the effectiveness of
the model in downstream tasks where multiple modalities should provide
additional information that the model fails to exploit. In this paper, we
present TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed
similarity measure that is directly computed in the higher-dimensional space
spanned by the modality embeddings. TRIANGLE improves the joint alignment of
three modalities via a triangle-area similarity, avoiding additional fusion
layers or pairwise similarities. When incorporated in contrastive losses
replacing cosine similarity, TRIANGLE significantly boosts the performance of
multimodal modeling, while yielding interpretable alignment rationales.
Extensive evaluation in three-modal tasks such as video-text and audio-text
retrieval or audio-video classification, demonstrates that TRIANGLE achieves
state-of-the-art results across different datasets improving the performance of
cosine-based methods up to 9 points of Recall@1.

</details>


### [830] [Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption](https://arxiv.org/abs/2509.24748)
*Longxiang He,Deheng Ye,Junbo Tan,Xueqian Wang,Li Shen*

Main category: cs.LG

TL;DR: 本文提出了一种新的离线到在线强化学习方法RPEX，通过引入逆概率加权来缓解数据污染导致的策略重尾问题，提升了在线探索效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实际环境中的离线数据集和在线交互常含有噪声或恶意损坏，严重影响O2O RL性能，现有工作未充分探讨数据污染下的鲁棒性问题。

Method: 在在线探索策略中引入逆概率加权（IPW），以减轻数据污染引起的重尾效应，并提出了RPEX方法。

Result: 在D4RL数据集上的实验表明，RPEX在多种数据污染场景下均达到最先进的O2O性能。

Conclusion: RPEX有效提高了离线到在线强化学习在面对状态、动作、奖励和动态污染时的鲁棒性和探索效率。

Abstract: Pretraining a policy on offline data followed by fine-tuning through online
interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has
emerged as a promising paradigm for real-world RL deployment. However, both
offline datasets and online interactions in practical environments are often
noisy or even maliciously corrupted, severely degrading the performance of O2O
RL. Existing works primarily focus on mitigating the conservatism of offline
policies via online exploration, while the robustness of O2O RL under data
corruption, including states, actions, rewards, and dynamics, is still
unexplored. In this work, we observe that data corruption induces heavy-tailed
behavior in the policy, thereby substantially degrading the efficiency of
online exploration. To address this issue, we incorporate Inverse Probability
Weighted (IPW) into the online exploration policy to alleviate
heavy-tailedness, and propose a novel, simple yet effective method termed
$\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion.
Extensive experimental results on D4RL datasets demonstrate that RPEX achieves
SOTA O2O performance across a wide range of data corruption scenarios. Code is
available at
$\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.

</details>


### [831] [In-Context Learning of Temporal Point Processes with Foundation Inference Models](https://arxiv.org/abs/2509.24762)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,César Ojeda,Ramsés J. Sánchez*

Main category: cs.LG

TL;DR: 提出一种基于预训练的深度神经网络模型FIM-PP，用于通过上下文学习推断点过程的条件强度函数，在无需额外训练或快速微调的情况下实现对多类型事件序列的建模与预测。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法需为每个目标系统训练独立的专用模型，缺乏通用性和可迁移性，因此需要一种能够跨系统泛化的点过程推断方法。

Method: 采用摊销推断和上下文学习思想，基于大量从霍克斯过程分布中采样的合成数据预训练一个深度神经网络，使其能根据输入事件序列上下文推断条件强度函数；该模型可直接应用于真实数据，也可快速微调。

Result: 在多个常见基准数据集上的实验表明，该方法在下一事件预测任务中性能媲美专用模型，且具备零样本迁移和快速适应能力。

Conclusion: FIM-PP通过预训练实现了点过程推断的通用性，为事件序列建模提供了一种高效、可迁移的新范式。

Abstract: Modeling event sequences of multiple event types with marked temporal point
processes (MTPPs) provides a principled way to uncover governing dynamical
rules and predict future events. Current neural network approaches to MTPP
inference rely on training separate, specialized models for each target system.
We pursue a radically different approach: drawing on amortized inference and
in-context learning, we pretrain a deep neural network to infer, in-context,
the conditional intensity functions of event histories from a context defined
by sets of event sequences. Pretraining is performed on a large synthetic
dataset of MTPPs sampled from a broad distribution of Hawkes processes. Once
pretrained, our Foundation Inference Model for Point Processes (FIM-PP) can
estimate MTPPs from real-world data without any additional training, or be
rapidly finetuned to target systems. Experiments show that this amortized
approach matches the performance of specialized models on next-event prediction
across common benchmark datasets.
  Our pretrained model, repository and tutorials will soon be available online

</details>


### [832] [Neural Message-Passing on Attention Graphs for Hallucination Detection](https://arxiv.org/abs/2509.24770)
*Fabrizio Frasca,Guy Bar-Shalom,Yftah Ziser,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出了一种名为CHARM的新方法，通过将注意力流、激活和注意力分数等计算轨迹统一表示为属性图，并利用图神经网络（GNN）进行幻觉检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常生成错误或无根据的内容（即幻觉），现有检测方法依赖于启发式规则或对孤立的计算轨迹（如激活、注意力图）的简单建模，缺乏系统性整合。

Method: 将token作为节点、注意力流作为边、注意力分数和激活作为节点与边的特征，构建属性图；将幻觉检测转化为图学习任务，使用GNN在该图上进行训练与推理。

Result: CHARM在理论上涵盖了先前基于注意力的启发式方法，在多个基准测试中 consistently 优于其他先进方法，并展现出良好的跨数据集零样本迁移能力。

Conclusion: 整合多种计算轨迹并利用图结构信息能有效提升幻觉检测性能，CHARM为LLM输出的真实性验证提供了更强大且可解释的框架。

Abstract: Large Language Models (LLMs) often generate incorrect or unsupported content,
known as hallucinations. Existing detection methods rely on heuristics or
simple models over isolated computational traces such as activations, or
attention maps. We unify these signals by representing them as attributed
graphs, where tokens are nodes, edges follow attentional flows, and both carry
features from attention scores and activations. Our approach, CHARM, casts
hallucination detection as a graph learning task and tackles it by applying
GNNs over the above attributed graphs. We show that CHARM provably subsumes
prior attention-based heuristics and, experimentally, it consistently
outperforms other leading approaches across diverse benchmarks. Our results
shed light on the relevant role played by the graph structure and on the
benefits of combining computational traces, whilst showing CHARM exhibits
promising zero-shot performance on cross-dataset transfer.

</details>


### [833] [MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models](https://arxiv.org/abs/2509.24779)
*Kacper Kapuśniak,Cristian Gabellini,Michael Bronstein,Prudencio Tossou,Francesco Di Giovanni*

Main category: cs.LG

TL;DR: 提出了一种新的生成模型类别——MSM模拟器（MSM Emulators），通过基于马尔可夫状态模型（MSM）的离散状态转移采样，显著加速分子动力学轨迹生成，相比传统MD方法提速两个数量级以上，并在多种蛋白质体系中表现出优越的统计复现能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）虽然能深入探究蛋白质功能，但计算成本高昂，现有生成模型通常学习固定滞后转移密度，导致训练信号被频繁但信息量低的转换主导，限制了效率和泛化能力。

Method: 引入MSM模拟器这一新模型类别，利用马尔可夫状态模型（MSM）定义离散状态，并在其基础上学习转移采样；具体实现为Markov Space Flow Matching（MarS-FM），结合流匹配技术进行高效生成。

Result: MarS-FM在RMSD、回转半径、二级结构等结构可观测量上均优于现有方法，能够准确再现MD统计特性；在多达500个残基的蛋白质域上验证，涵盖去折叠事件，且训练集与测试集序列严格不重叠，表现出良好泛化性；采样速度比显式或隐式溶剂MD快两个数量级以上。

Conclusion: MSM模拟器，特别是MarS-FM，提供了一种高效、可推广的分子动力学替代方案，能够在大幅降低计算成本的同时保持高保真度的动态行为建模。

Abstract: Molecular Dynamics (MD) is a powerful computational microscope for probing
protein functions. However, the need for fine-grained integration and the long
timescales of biomolecular events make MD computationally expensive. To address
this, several generative models have been proposed to generate surrogate
trajectories at lower cost. Yet, these models typically learn a fixed-lag
transition density, causing the training signal to be dominated by frequent but
uninformative transitions. We introduce a new class of generative models, MSM
Emulators, which instead learn to sample transitions across discrete states
defined by an underlying Markov State Model (MSM). We instantiate this class
with Markov Space Flow Matching (MarS-FM), whose sampling offers more than two
orders of magnitude speedup compared to implicit- or explicit-solvent MD
simulations. We benchmark Mars-FM ability to reproduce MD statistics through
structural observables such as RMSD, radius of gyration, and secondary
structure content. Our evaluation spans protein domains (up to 500 residues)
with significant chemical and structural diversity, including unfolding events,
and enforces strict sequence dissimilarity between training and test sets to
assess generalization. Across all metrics, MarS-FM outperforms existing
methods, often by a substantial margin.

</details>


### [834] [Quantifying Generalisation in Imitation Learning](https://arxiv.org/abs/2509.24784)
*Nathan Gavenski,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: Labyrinth是一个用于评估模仿学习中泛化能力的基准环境，具有精确控制结构、起始与目标位置及任务复杂度的特点。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习基准在训练和评估之间缺乏足够的变化，限制了对泛化能力的有效评估。

Method: 设计了一个名为Labyrinth的基准环境，提供离散且完全可观测的状态空间、已知最优动作，并支持部分可观测、钥匙门任务和冰面障碍等变体。

Result: 实现了可验证的分离训练、评估和测试设置，支持细粒度、可解释的评估以及受控、可重复的实验。

Conclusion: Labyrinth推进了模仿学习中泛化能力的评估，为开发更鲁棒的智能体提供了有力工具。

Abstract: Imitation learning benchmarks often lack sufficient variation between
training and evaluation, limiting meaningful generalisation assessment. We
introduce Labyrinth, a benchmarking environment designed to test generalisation
with precise control over structure, start and goal positions, and task
complexity. It enables verifiably distinct training, evaluation, and test
settings. Labyrinth provides a discrete, fully observable state space and known
optimal actions, supporting interpretability and fine-grained evaluation. Its
flexible setup allows targeted testing of generalisation factors and includes
variants like partial observability, key-and-door tasks, and ice-floor hazards.
By enabling controlled, reproducible experiments, Labyrinth advances the
evaluation of generalisation in imitation learning and provides a valuable tool
for developing more robust agents.

</details>


### [835] [Assessing the risk of future Dunkelflaute events for Germany using generative deep learning](https://arxiv.org/abs/2509.24788)
*Felix Strnad,Jonathan Schmidt,Fabian Mockert,Philipp Hennig,Nicole Ludwig*

Main category: cs.LG

TL;DR: 本研究利用生成式深度学习框架降尺度CMIP6气候模拟数据，评估德国未来“Dunkelflaute”事件（低风能和太阳能发电期）的发生频率和持续时间，发现在SSP2-4.5和SSP5-8.5排放情景下，其风险在整个21世纪将保持稳定。


<details>
  <summary>Details</summary>
Motivation: 可再生能源对天气的依赖性带来电网稳定性挑战，特别是Dunkelflaute事件可能导致电力供应短缺，因此需评估未来此类事件的变化趋势以支持能源规划。

Method: 采用生成式深度学习模型对CMIP6气候模拟数据进行降尺度处理，并与ERA5历史数据统计特征对比，进而分析未来不同排放情景下德国Dunkelflaute事件的可能变化。

Result: 在SSP2-4.5和SSP5-8.5两种排放情景下，在多模型集合平均意义上，德国Dunkelflaute事件的频率和持续时间预计与历史时期相比基本保持不变。

Conclusion: 在所考虑的气候情景下，未来德国Dunkelflaute事件带来的电力系统风险在整个本世纪内预计将保持稳定，无需预期其显著增加。

Abstract: The European electricity power grid is transitioning towards renewable energy
sources, characterized by an increasing share of off- and onshore wind and
solar power. However, the weather dependency of these energy sources poses a
challenge to grid stability, with so-called Dunkelflaute events -- periods of
low wind and solar power generation -- being of particular concern due to their
potential to cause electricity supply shortages. In this study, we investigate
the impact of these events on the German electricity production in the years
and decades to come. For this purpose, we adapt a recently developed generative
deep learning framework to downscale climate simulations from the CMIP6
ensemble. We first compare their statistics to the historical record taken from
ERA5 data. Next, we use these downscaled simulations to assess plausible future
occurrences of Dunkelflaute events in Germany under the optimistic low
(SSP2-4.5) and high (SSP5-8.5) emission scenarios. Our analysis indicates that
both the frequency and duration of Dunkelflaute events in Germany in the
ensemble mean are projected to remain largely unchanged compared to the
historical period. This suggests that, under the considered climate scenarios,
the associated risk is expected to remain stable throughout the century.

</details>


### [836] [Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.24789)
*Zhijian Xu,Wanxu Cai,Xilin Dai,Zhaorong Deng,Qiang Xu*

Main category: cs.LG

TL;DR: 本文提出了Fidel-TS，一个基于实时API构建的高质量时间序列预测基准，强调数据来源完整性、因果合理性和结构清晰性，揭示了现有基准中的偏见，并证明文本信息的因果相关性是提升多模态预测性能的关键。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型评估受限于高质量基准的缺乏，且存在数据污染、因果泄露等问题，导致研究进展可能被高估。

Method: 提出高保真基准的核心原则，构建基于实时API的新大规模基准Fidel-TS，并通过实验验证其有效性。

Result: 实验证明Fidel-TS能有效暴露先前基准的偏差和设计缺陷，并证实文本信息的因果相关性对多模态预测性能提升至关重要。

Conclusion: 构建符合数据完整性与因果原则的基准对推动时间序列预测研究至关重要，Fidel-TS为未来模型评估提供了更可靠的标准。

Abstract: The evaluation of time series forecasting models is hindered by a critical
lack of high-quality benchmarks, leading to a potential illusion of progress.
Existing datasets suffer from issues ranging from pre-training data
contamination in the age of LLMs to the causal and description leakage
prevalent in early multimodal designs. To address this, we formalize the core
principles of high-fidelity benchmarking, focusing on data sourcing integrity,
strict causal soundness, and structural clarity. We introduce Fidel-TS, a new
large-scale benchmark built from the ground up on these principles by sourcing
data from live APIs. Our extensive experiments validate this approach by
exposing the critical biases and design limitations of prior benchmarks.
Furthermore, we conclusively demonstrate that the causal relevance of textual
information is the key factor in unlocking genuine performance gains in
multimodal forecasting.

</details>


### [837] [DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.24800)
*Zixu Wang,Hongbin Dong,Xiaoping Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列预测模型DSAT-HD，结合混合分解、多尺度自适应路径和双流残差学习框架，在多个数据集上优于现有方法并表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法难以捕捉跨不同范围的多样化特征，且传统分解方法需预设季节周期并仅处理单一固定季节性。

Method: 1) 结合EMA和傅里叶分解的混合分解机制，通过噪声Top-k门控动态平衡季节性和趋势成分；2) 利用稀疏分配器将特征路由到四个并行Transformer层的多尺度自适应路径，并通过稀疏组合器融合特征，结合局部CNN和全局交互的混合注意力机制增强性能；3) 双流残差学习框架，CNN和MLP分支分别处理季节性和趋势成分，由平衡损失函数协调。

Result: 在九个数据集上的实验表明，DSAT-HD整体优于现有方法，并在部分数据集上达到最先进性能，同时在多种迁移场景中表现出更强的泛化能力。

Conclusion: DSAT-HD有效解决了现有方法在建模多尺度特征和复杂季节性-趋势分解方面的局限性，具有优越的预测性能和泛化能力。

Abstract: Time series forecasting is crucial for various applications, such as weather,
traffic, electricity, and energy predictions. Currently, common time series
forecasting methods are based on Transformers. However, existing approaches
primarily model limited time series or fixed scales, making it more challenging
to capture diverse features cross different ranges. Additionally, traditional
methods like STL for complex seasonality-trend decomposition require
pre-specified seasonal periods and typically handle only single, fixed
seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive
Transformer (DSAT-HD), which integrates three key innovations to address the
limitations of existing methods: 1) A hybrid decomposition mechanism combining
EMA and Fourier decomposition with RevIN normalization, dynamically balancing
seasonal and trend components through noise Top-k gating; 2) A multi-scale
adaptive pathway leveraging a sparse allocator to route features to four
parallel Transformer layers, followed by feature merging via a sparse combiner,
enhanced by hybrid attention combining local CNNs and global interactions; 3) A
dual-stream residual learning framework where CNN and MLP branches separately
process seasonal and trend components, coordinated by a balanced loss function
minimizing expert collaboration variance. Extensive experiments on nine
datasets demonstrate that DSAT-HD outperforms existing methods overall and
achieves state-of-the-art performance on some datasets. Notably, it also
exhibits stronger generalization capabilities across various transfer
scenarios.

</details>


### [838] [Physics-informed learning under mixing: How physical knowledge speeds up learning](https://arxiv.org/abs/2509.24801)
*Anna Scampicchio,Leonardo F. Toso,Rahel Rickenbach,James Anderson,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 该论文研究了在数据依赖的情况下，物理信息正则化如何影响经验风险最小化的学习速率，并证明当物理先验信息对齐时，学习速率可从较慢的Sobolev极小极大速率提升到快速的最优i.i.d.速率。


<details>
  <summary>Details</summary>
Motivation: 理解在数据依赖情况下，融入物理领域先验知识如何影响机器学习的学习速率。

Method: 基于物理信息正则化的经验风险最小化，推导出复杂度相关的过量风险的概率和期望界限。

Result: 证明当物理先验信息对齐时，学习速率显著提升，且无需因数据依赖而导致样本量折损。

Conclusion: 物理先验信息的正确对齐能显著加快学习速率，克服数据依赖带来的负面影响。

Abstract: A major challenge in physics-informed machine learning is to understand how
the incorporation of prior domain knowledge affects learning rates when data
are dependent. Focusing on empirical risk minimization with physics-informed
regularization, we derive complexity-dependent bounds on the excess risk in
probability and in expectation. We prove that, when the physical prior
information is aligned, the learning rate improves from the (slow) Sobolev
minimax rate to the (fast) optimal i.i.d. one without any sample-size deflation
due to data dependence.

</details>


### [839] [DyMoDreamer: World Modeling with Dynamic Modulation](https://arxiv.org/abs/2509.24804)
*Boxuan Zhang,Runqing Wang,Wei Xiao,Weipu Zhang,Jian Sun,Gao Huang,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: DyMoDreamer是一种新的基于模型的强化学习算法，通过动态调制机制和帧间差异掩码提取动态特征和时间信息，显著提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统世界模型在处理视觉任务时未能分离动态对象与静态背景，导致计算低效且影响决策性能，因此需要更高效地捕捉奖励相关动态。

Method: 提出DyMoDreamer，引入帧间差分掩码生成微分观测，结合随机分类分布的动态调制机制，并集成到循环状态空间模型（RSSM）中以增强对动态特征的关注。

Result: 在Atari 100k基准上达到156.6%的平均人类归一化得分，DeepMind视觉控制套件上创下832分新纪录，在Crafter基准1M步后性能提升9.5%。

Conclusion: DyMoDreamer通过显式建模对象级运动线索和时间动态，显著提升了MBRL的样本效率和决策性能，成为多个视觉强化学习基准的新标杆。

Abstract: A critical bottleneck in deep reinforcement learning (DRL) is sample
inefficiency, as training high-performance agents often demands extensive
environmental interactions. Model-based reinforcement learning (MBRL) mitigates
this by building world models that simulate environmental dynamics and generate
synthetic experience, improving sample efficiency. However, conventional world
models process observations holistically, failing to decouple dynamic objects
and temporal features from static backgrounds. This approach is computationally
inefficient, especially for visual tasks where dynamic objects significantly
influence rewards and decision-making performance. To address this, we
introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic
modulation mechanism to improve the extraction of dynamic features and enrich
the temporal information. DyMoDreamer employs differential observations derived
from a novel inter-frame differencing mask, explicitly encoding object-level
motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic
categorical distributions and integrated into a recurrent state-space model
(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments
demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k
benchmark with a $156.6$\% mean human-normalized score, establishes a new
record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\%
performance improvement after $1$M steps on the Crafter benchmark. Our code is
released at https://github.com/Ultraman-Tiga1/DyMoDreamer.

</details>


### [840] [Putnam-like dataset summary: LLMs as mathematical competition contestants](https://arxiv.org/abs/2509.24827)
*Bartosz Bieganowski,Daniel Strzelecki,Robert Skiba,Mateusz Topolewski*

Main category: cs.LG

TL;DR: 本文总结了Google DeepMind发布的类似普特南竞赛的基准测试结果，包含96道数学竞赛风格的问题和576个大语言模型（LLM）的解答，用于评估LLM在解决数学竞赛问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 评估当前大语言模型在解决高难度数学竞赛问题上的能力，检验其推理与解题水平。

Method: 使用Google DeepMind发布的包含96道原创问题和576个LLM解答的数据集，分析模型在这些问题上的表现。

Result: 分析揭示了LLM在数学竞赛问题上的整体表现水平，但具体性能指标未在摘要中提及。

Conclusion: 该研究为衡量LLM在复杂数学问题求解方面的能力提供了基准，表明需进一步提升模型的数学推理能力。

Abstract: In this paper we summarize the results of the Putnam-like benchmark published
by Google DeepMind. This dataset consists of 96 original problems in the spirit
of the Putnam Competition and 576 solutions of LLMs. We analyse the performance
of models on this set of problems to verify their ability to solve problems
from mathematical contests.

</details>


### [841] [Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data](https://arxiv.org/abs/2509.24840)
*Oussama Kharouiche,Aris Markogiannakis,Xiao Fei,Michail Chatzianastasis,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: Cell2Text是一个多模态生成框架，将单细胞RNA测序数据转化为结构化自然语言描述，结合单细胞基础模型和大语言模型，实现对细胞身份、组织来源、疾病关联和通路活性的可解释性建模。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型局限于离散预测头，无法充分捕捉生物学家所需的丰富上下文信息，缺乏对细胞复杂性的可解释表达。

Method: 整合基因水平的单细胞基础模型嵌入与预训练大语言模型，构建多模态生成框架Cell2Text，将scRNA-seq表达谱直接翻译为自然语言描述。

Result: Cell2Text在分类准确率上优于基线模型，PageRank相似性指标显示其具有强本体一致性，文本生成语义保真度高，能泛化到未见细胞类型。

Conclusion: 将基因表达数据与自然语言结合，不仅能提升预测性能，还能提供内在可解释的输出，为无需大量标注的细胞表征提供了可扩展路径。

Abstract: Single-cell RNA sequencing has transformed biology by enabling the
measurement of gene expression at cellular resolution, providing information
for cell types, states, and disease contexts. Recently, single-cell foundation
models have emerged as powerful tools for learning transferable representations
directly from expression profiles, improving performance on classification and
clustering tasks. However, these models are limited to discrete prediction
heads, which collapse cellular complexity into predefined labels that fail to
capture the richer, contextual explanations biologists need. We introduce
Cell2Text, a multimodal generative framework that translates scRNA-seq profiles
into structured natural language descriptions. By integrating gene-level
embeddings from single-cell foundation models with pretrained large language
models, Cell2Text generates coherent summaries that capture cellular identity,
tissue origin, disease associations, and pathway activity, generalizing to
unseen cells. Empirically, Cell2Text outperforms baselines on classification
accuracy, demonstrates strong ontological consistency using PageRank-based
similarity metrics, and achieves high semantic fidelity in text generation.
These results demonstrate that coupling expression data with natural language
offers both stronger predictive performance and inherently interpretable
outputs, pointing to a scalable path for label-efficient characterization of
unseen cells.

</details>


### [842] [Beyond the Hook: Predicting Billboard Hot 100 Chart Inclusion with Machine Learning from Streaming, Audio Signals, and Perceptual Features](https://arxiv.org/abs/2509.24856)
*Christos Mountzouris*

Main category: cs.LG

TL;DR: 本研究探讨了影响歌曲进入Billboard Hot 100榜单的关键因素，发现流媒体 popularity 是最强预测指标，音频特征如器乐性、效价、时长和语音度也有显著影响。三种机器学习模型（逻辑回归、随机森林和XGBoost）均实现了约90%的准确率，其中随机森林表现略优。


<details>
  <summary>Details</summary>
Motivation: 随着数字流媒体平台的发展，音乐产业数据日益结构化，为研究音乐流行度和主流成功提供了新机会。本文旨在识别影响歌曲进入Billboard Hot 100榜单的最关键预测因素。

Method: 采用逻辑回归、随机森林和XGBoost三种机器学习模型，基于流媒体 popularity、音频信号特征（如器乐性、效价、时长、语音度）以及人类听觉感知的概率指标，预测歌曲是否能进入Billboard Hot 100榜单。

Result: 流媒体 popularity 是最显著的预测因子，其次是器乐性、效价、时长和语音度。逻辑回归准确率达90.0%，随机森林达到90.4%，XGBoost为90.3%。随机森林在非上榜歌曲上的精确率接近完美（0.990），且对上榜歌曲召回率高（0.992）。

Conclusion: 歌曲的流媒体 popularity 是预测其能否进入Billboard Hot 100的最重要因素，结合音频特征可有效建模流行趋势，随机森林模型在整体性能上表现最佳。

Abstract: The advent of digital streaming platforms have recently revolutionized the
landscape of music industry, with the ensuing digitalization providing
structured data collections that open new research avenues for investigating
popularity dynamics and mainstream success. The present work explored which
determinants hold the strongest predictive influence for a track's inclusion in
the Billboard Hot 100 charts, including streaming popularity, measurable audio
signal attributes, and probabilistic indicators of human listening. The
analysis revealed that popularity was by far the most decisive predictor of
Billboard Hot 100 inclusion, with considerable contribution from
instrumentalness, valence, duration and speechiness. Logistic Regression
achieved 90.0% accuracy, with very high recall for charting singles (0.986) but
lower recall for non-charting ones (0.813), yielding balanced F1-scores around
0.90. Random Forest slightly improved performance to 90.4% accuracy,
maintaining near-perfect precision for non-charting singles (0.990) and high
recall for charting ones (0.992), with F1-scores up to 0.91. Gradient Boosting
(XGBoost) reached 90.3% accuracy, delivering a more balanced trade-off by
improving recall for non-charting singles (0.837) while sustaining high recall
for charting ones (0.969), resulting in F1-scores comparable to the other
models.

</details>


### [843] [DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning](https://arxiv.org/abs/2509.24868)
*Jiayi Li,Flora D. Salim*

Main category: cs.LG

TL;DR: 本文提出DRIFT-Net，一种用于求解PDE的双分支神经网络架构，通过频谱分支和图像分支分别捕捉全局低频信息和局部高频细节，实现更高效、准确的PDE建模。


<details>
  <summary>Details</summary>
Motivation: 现有基于多尺度窗口自注意力的PDE基础模型（如scOT）因局部性限制，难以实现真正的全局频谱耦合，导致在闭环 rollout 中出现误差累积和漂移问题。

Method: DRIFT-Net采用双分支结构：频谱分支处理低频全局信息，进行轻量级控制混合；图像分支捕捉局部细节；两分支通过逐带加权融合，并将结果返回空间域与图像分支相加，实现多尺度信息保留。

Result: 在相同训练设置下，DRIFT-Net相比强注意力基线模型误差更低、吞吐量更高且参数更少；在Navier-Stokes基准上，相对L1误差降低7%–54%，参数减少约15%，吞吐量仍高于scOT。

Conclusion: DRIFT-Net通过有效的频谱-空间双路径设计，增强了全局耦合稳定性，缓解了误差漂移，显著提升了PDE神经求解器的精度与效率。

Abstract: Learning PDE dynamics with neural solvers can significantly improve
wall-clock efficiency and accuracy compared with classical numerical solvers.
In recent years, foundation models for PDEs have largely adopted multi-scale
windowed self-attention, with the scOT backbone in \textsc{Poseidon} serving as
a representative example.
  However, because of their locality, truly globally consistent spectral
coupling can only be propagated gradually through deep stacking and window
shifting. This weakens global coupling and leads to error accumulation and
drift during closed-loop rollouts. To address this, we propose
\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral
branch and an image branch. The spectral branch is responsible for capturing
global, large-scale low-frequency information, whereas the image branch focuses
on local details and nonstationary structures. Specifically, we first perform
controlled, lightweight mixing within the low-frequency range. Then we fuse the
spectral and image paths at each layer via bandwise weighting, which avoids the
width inflation and training instability caused by naive concatenation. The
fused result is transformed back into the spatial domain and added to the image
branch, thereby preserving both global structure and high-frequency details
across scales. Compared with strong attention-based baselines, DRIFT-Net
achieves lower error and higher throughput with fewer parameters under
identical training settings and budget. On Navier--Stokes benchmarks, the
relative $L_{1}$ error is reduced by 7\%--54\%, the parameter count decreases
by about 15\%, and the throughput remains higher than scOT. Ablation studies
and theoretical analyses further demonstrate the stability and effectiveness of
this design. The code is available at
https://github.com/cruiseresearchgroup/DRIFT-Net.

</details>


### [844] [Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation](https://arxiv.org/abs/2509.24873)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 本文将共形预测应用于多模态多任务土壤剖面描述模型SoilNet，通过模拟人在回路的标注流程，验证了在有限专家标注预算下，共形预测能提高回归任务的标注效率，并在分类任务中保持相当的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升人机协作中机器学习模型的可信度和协作效率，需要对模型不确定性进行可靠校准，从而实现更有效的专家干预和负责任的模型使用。

Method: 采用共形预测这一模型无关的不确定性校准框架，应用于SoilNet模型，并设计了一个模拟的人在回路标注流程，在标注预算有限的情况下根据模型不确定性决定是否请求专家标注。

Result: 实验表明，经过共形化处理的SoilNet在回归任务中显著提高了标注效率，在相同标注预算下分类任务的表现与非共形版本相当。

Conclusion: 共形预测能够有效提升多模态多任务模型在实际人机协作场景中的标注效率和实用性，特别是在专家资源有限的情况下。

Abstract: Uncertainty quantification is essential in human-machine collaboration, as
human agents tend to adjust their decisions based on the confidence of the
machine counterpart. Reliably calibrated model uncertainties, hence, enable
more effective collaboration, targeted expert intervention and more responsible
usage of Machine Learning (ML) systems. Conformal prediction has become a well
established model-agnostic framework for uncertainty calibration of ML models,
offering statistically valid confidence estimates for both regression and
classification tasks. In this work, we apply conformal prediction to
$\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.
We design a simulated human-in-the-loop (HIL) annotation pipeline, where a
limited budget for obtaining ground truth annotations from domain experts is
available when model uncertainty is high. Our experiments show that
conformalizing SoilNet leads to more efficient annotation in regression tasks
and comparable performance scores in classification tasks under the same
annotation budget when tested against its non-conformal counterpart. All code
and experiments can be found in our repository:
https://github.com/calgo-lab/BGR

</details>


### [845] [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
*Sanxing Chen,Xiaoyin Chen,Yukun Huang,Roy Xie,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 该论文研究了通过监督微调（SFT）和强化学习（RL）提升大语言模型在多臂老虎机任务中的探索能力，发现尽管性能可媲美UCB和汤普森采样，但模型倾向于过早 exploitation，导致早期灾难性失败，强调需针对性设计奖励机制并超越平均遗憾进行评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在序贯决策中探索效率低，现有方法改进其在多臂老虎机任务上的表现，但缺乏对探索策略如何被塑造及泛化能力的深入理解。

Method: 采用监督微调（SFT）和多种定制奖励信号的强化学习（RL），包括降低方差的遗憾 shaping 奖励和实现 oracle 模仿的算法奖励，训练LLMs并分析其行为。

Result: 训练后的模型性能优于预训练模型，接近UCB和Thompson Sampling，在6倍长 horizon 和不同老虎机家族中表现出强泛化能力；但更倾向于贪婪利用，易早期放弃探索；模仿UCB的模型甚至能通过更激进的利用超越教师模型。

Conclusion: SFT和RL可显著提升LLMs在探索任务中的表现，但可能导致过度利用；应根据任务需求选择训练范式，并设计更精细的奖励机制与评估指标以促进稳健的探索行为。

Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

</details>


### [846] [Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime](https://arxiv.org/abs/2509.24882)
*Leonardo Defilippis,Yizhou Xu,Julius Girardin,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová,Bruno Loureiro,Florent Krzakala*

Main category: cs.LG

TL;DR: 本文研究了二次和对角神经网络在特征学习机制下的缩放律，通过与矩阵压缩感知和LASSO的联系，推导出过量风险随样本复杂度和权重衰减变化的相图，揭示了不同缩放区域之间的交叉和平台行为，并将其与训练后网络权重的谱特性精确关联，从第一性原理上解释了权重谱中幂律尾部与泛化性能的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管神经缩放律在深度学习中取得了许多进展，但其理论理解仍主要局限于线性模型。因此，需要在更复杂的非线性模型（如二次和对角神经网络）中系统分析缩放律的机制。

Method: 利用与矩阵压缩感知和LASSO方法的联系，对二次和对角神经网络在特征学习机制下的缩放指数进行理论分析，构建过量风险在不同样本复杂度和权重衰减条件下的相图，并分析训练后网络权重的谱特性。

Result: 推导出缩放指数的详细相图，发现不同缩放区域之间的交叉和平台行为；建立了缩放行为与网络权重谱特性之间的精确联系；揭示了权重谱中幂律尾部的出现与网络泛化性能之间的理论关联。

Conclusion: 为神经网络缩放律中的复杂现象提供了理论解释，并从第一性原理出发验证了权重谱幂律尾部与泛化性能之间的经验观察，增强了对特征学习机制下非线性模型缩放行为的理解。

Abstract: Neural scaling laws underlie many of the recent advances in deep learning,
yet their theoretical understanding remains largely confined to linear models.
In this work, we present a systematic analysis of scaling laws for quadratic
and diagonal neural networks in the feature learning regime. Leveraging
connections with matrix compressed sensing and LASSO, we derive a detailed
phase diagram for the scaling exponents of the excess risk as a function of
sample complexity and weight decay. This analysis uncovers crossovers between
distinct scaling regimes and plateau behaviors, mirroring phenomena widely
reported in the empirical neural scaling literature. Furthermore, we establish
a precise link between these regimes and the spectral properties of the trained
network weights, which we characterize in detail. As a consequence, we provide
a theoretical validation of recent empirical observations connecting the
emergence of power-law tails in the weight spectrum with network generalization
performance, yielding an interpretation from first principles.

</details>


### [847] [Scaling with Collapse: Efficient and Predictable Training of LLM Families](https://arxiv.org/abs/2509.25087)
*Shane Bergsma,Bin Claire Zhang,Nolan Dey,Shaheer Muhammad,Gurpreet Gosal,Joel Hestness*

Main category: cs.LG

TL;DR: 训练损失曲线在适当归一化后可坍缩为一条普适轨迹，这种坍缩现象在采用实际扩展策略的LLM训练中依然成立，并可作为计算高效训练的标志。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在实际扩展策略下训练损失曲线是否仍能坍缩，以及该现象能否用于诊断训练问题和优化超参数调优。

Method: 基于最优超参数设置下的数据预算，对不同规模模型的训练损失曲线进行归一化，观察其是否坍缩；利用坍缩现象进行早期诊断与提前停止。

Result: 发现当超参数根据数据预算最优设置时，损失曲线在不同尺度上精确坍缩；坍缩可作为计算效率的指标，并成功应用于训练诊断和超参数调优；基于此训练了高效的LLM家族Celerity。

Conclusion: 损失曲线的坍缩是计算高效训练的标志，在实际扩展条件下依然成立，且可用于提升LLM开发效率。

Abstract: Effective LLM training relies on *consistency*, meaning that key quantities
-- such as final losses and optimal hyperparameters -- scale predictably across
model sizes. Qiu et al. (2025) recently showed that this consistency extends
beyond scalars: whole training loss curves can *collapse* onto a universal
trajectory after a simple normalization. What remains unclear is whether this
phenomenon holds for LLM families trained under *practical scaling recipes*,
where width, depth, learning rate, batch size, and weight decay are scaled
jointly. We show that it does: loss curves collapse across scales precisely
when optimization hyperparameters are set optimally for the given data budget,
in accordance with recent empirical scaling laws. Collapse thus emerges as a
signature of compute-efficient training. We demonstrate two applications at
scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of
training pathologies, and (2) the predictability of collapsed curves enables
early stopping in large-scale hyperparameter tuning. Finally, we train a
competitive LLM family, *Celerity*, using these insights, highlighting collapse
as an effective tool for developing efficient LLMs.

</details>


### [848] [Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks](https://arxiv.org/abs/2509.24886)
*Ya-Wei Eileen Lin,Ron Levie*

Main category: cs.LG

TL;DR: 本文提出了自适应规范化（adaptive canonicalization）框架，通过依赖输入和网络的动态标准形式来解决等变机器学习中的不连续性问题。


<details>
  <summary>Details</summary>
Motivation: 传统规范化方法在等变机器学习中常引入不连续性，影响训练稳定性、泛化能力和通用逼近性质。

Method: 提出基于先验最大化的自适应规范化，选择最大化网络预测置信度的输入标准形式，并证明其连续性和对称性保持特性。

Result: 该方法在谱图神经网络和点云旋转对称处理中有效，实验显示在分子、蛋白质及点云分类任务上优于数据增强、标准规范化和等变架构。

Conclusion: 自适应规范化能构建连续且对称的模型，具备通用逼近能力，并在多个任务上表现优越。

Abstract: Canonicalization is a widely used strategy in equivariant machine learning,
enforcing symmetry in neural networks by mapping each input to a standard form.
Yet, it often introduces discontinuities that can affect stability during
training, limit generalization, and complicate universal approximation
theorems. In this paper, we address this by introducing \emph{adaptive
canonicalization}, a general framework in which the canonicalization depends
both on the input and the network. Specifically, we present the adaptive
canonicalization based on prior maximization, where the standard form of the
input is chosen to maximize the predictive confidence of the network. We prove
that this construction yields continuous and symmetry-respecting models that
admit universal approximation properties.
  We propose two applications of our setting: (i) resolving eigenbasis
ambiguities in spectral graph neural networks, and (ii) handling rotational
symmetries in point clouds. We empirically validate our methods on molecular
and protein classification, as well as point cloud classification tasks. Our
adaptive canonicalization outperforms the three other common solutions to
equivariant machine learning: data augmentation, standard canonicalization, and
equivariant architectures.

</details>


### [849] [ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation](https://arxiv.org/abs/2509.25100)
*Aasheesh Singh,Vishal Vaddina,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出ORPO-Distill，一种基于偏好优化的跨架构大模型蒸馏方法，通过多样化的推理轨迹和奇比偏好优化目标提升知识迁移效果。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒知识蒸馏在跨架构模型压缩中效率有限，难以有效传递教师模型的推理过程知识。

Method: 将知识蒸馏建模为偏好优化问题，使用奇比偏好优化（ORPO）目标函数对比师生推理轨迹，并采用混合策略利用学生生成的输出进行训练。

Result: 在五个数据集和多种学生模型上实验表明，该方法显著优于标准的思维链蒸馏和传统知识蒸馏基线。

Conclusion: ORPO-Distill能更有效地实现跨架构大模型的知识迁移，尤其在推理轨迹的利用和学习效率方面表现优越。

Abstract: We introduce ORPO-Distill, a general-purpose method for cross-architecture
LLM distillation that formulates the problem as a preference optimization task.
Unlike standard CoT distillation, the approach transfers knowledge through
diverse reasoning traces. It employs an Odds-Ratio Preference Optimization
objective that contrasts teacher and student traces for more effective
learning, and adopts a mixed-policy strategy for utilizing student-generated
outputs, outperforming both off- and on-policy alternatives. Experiments on
five datasets and multiple student models show consistent improvements over
conventional black-box KD baselines.

</details>


### [850] [Towards Understanding the Shape of Representations in Protein Language Models](https://arxiv.org/abs/2509.24895)
*Kosio Beshkov,Anders Malthe-Sørenssen*

Main category: cs.LG

TL;DR: 本研究通过SRV表示和图过滤方法，分析蛋白质语言模型（PLMs）在不同层中如何转换序列空间并编码结构信息，发现结构保真度最高的表示出现在接近但早于最后一层，提示在此处训练折叠模型可能提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前对蛋白质语言模型如何转换整个序列空间及其关系尚不清楚，缺乏对模型内部表示空间的系统性理解。

Method: 采用平方根速度（SRV）表示和图过滤方法，将蛋白质及其表示映射到度量空间，并利用Karcher均值和有效维度分析ESM2模型各层的特征。

Result: SRV形状空间的Karcher均值和有效维度随ESM2模型层数呈非线性变化；图过滤结果显示PLMs优先编码残基间的局部关系，长程上下文编码效果下降；结构保真度最高的表示出现在接近但早于最后一层。

Conclusion: PLMs在中间靠后层最能忠实反映蛋白质结构，建议在此类层上训练蛋白质折叠模型以提升性能。

Abstract: While protein language models (PLMs) are one of the most promising avenues of
research for future de novo protein design, the way in which they transform
sequences to hidden representations, as well as the information encoded in such
representations is yet to be fully understood. Several works have attempted to
propose interpretability tools for PLMs, but they have focused on understanding
how individual sequences are transformed by such models. Therefore, the way in
which PLMs transform the whole space of sequences along with their relations is
still unknown. In this work we attempt to understand this transformed space of
sequences by identifying protein structure and representation with square-root
velocity (SRV) representations and graph filtrations. Both approaches naturally
lead to a metric space in which pairs of proteins or protein representations
can be compared with each other.
  We analyze different types of proteins from the SCOP dataset and show that
the Karcher mean and effective dimension of the SRV shape space follow a
non-linear pattern as a function of the layers in ESM2 models of different
sizes. Furthermore, we use graph filtrations as a tool to study the context
lengths at which models encode the structural features of proteins. We find
that PLMs preferentially encode immediate as well as local relations between
residues, but start to degrade for larger context lengths. The most
structurally faithful encoding tends to occur close to, but before the last
layer of the models, indicating that training a folding model ontop of these
layers might lead to improved folding performance.

</details>


### [851] [Rethinking Entropy Regularization in Large Reasoning Models](https://arxiv.org/abs/2509.25133)
*Yuxian Jiang,Yafu Li,Guanxu Chen,Dongrui Liu,Yu Cheng,Jing Shao*

Main category: cs.LG

TL;DR: 提出SIREN方法，通过选择性熵正则化解决大推理模型中强化学习的熵崩溃和过早收敛问题，在数学基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统熵正则化在大推理模型的强化学习中因动作空间大、轨迹长导致全局熵爆炸，无法有效促进探索，需新方法解决熵崩溃与过早收敛问题。

Method: 提出SIREN，采用两步熵掩码机制（top-p掩码和峰值熵掩码）限制在有意义的动作和状态子集上进行探索，并引入自锚定形式的正则化以稳定训练。

Result: 在五个数学基准测试中，SIREN相比以往熵相关RLVR方法取得更好平均表现，如在AIME24/25上使用Qwen2.5-Math-7B时maj@k提升+6.6，且能保持验证pass@k，增加响应多样性并维持适当熵水平。

Conclusion: SIREN有效缓解了大推理模型中基于强化学习的可验证奖励训练中的过早收敛问题，为大规模动作空间下的探索提供了更优的熵控制方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great promise
in enhancing the reasoning abilities of large reasoning models (LRMs). However,
it suffers from a critical issue: entropy collapse and premature convergence.
Naive entropy regularization, a common approach for encouraging exploration in
the traditional RL literature, fails to address this problem in the context of
LRM. Our analysis reveals that this failure stems from the vast action space
and long trajectories in LRMs, which easily trigger a global entropy explosion
as the model indiscriminately explores all possible actions and states. To
address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method
that confines exploration to a meaningful subset of actions and states. SIREN
achieves this through a two-step entropy masking mechanism, consisting of a
top-p mask and a peak-entropy mask. In addition, regularization is transformed
into a self-anchored form to stabilize training. Across five mathematical
benchmarks, SIREN attains superior average performance over previous
entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on
AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes
greater response diversity and maintains entropy at an appropriate level, which
helps to preserve the validation pass@k throughout training. This effectively
mitigates the premature convergence problem common in RLVR for LRM.

</details>


### [852] [Is Sequence Information All You Need for Bayesian Optimization of Antibodies?](https://arxiv.org/abs/2509.24933)
*Sebastian W. Ober,Calvin McCarter,Aniruddh Raghu,Yucen Lily Li,Alan N. Amin,Andrew Gordon Wilson,Hunter Elliott*

Main category: cs.LG

TL;DR: 本研究探讨了在抗体贝叶斯优化中引入结构信息的不同方法，并与仅使用序列的方法在结合亲和力和稳定性两个性质上进行比较，发现特定结构信息可提高早期数据效率，但在引入蛋白质语言模型“软约束”后，结构信息的优势减弱甚至消失。


<details>
  <summary>Details</summary>
Motivation: 抗体治疗性质的工程优化通常迭代且昂贵，贝叶斯优化虽适用，但如何选择适合高度结构化抗体空间的代理模型仍具挑战，且目前尚无工作将结构信息融入抗体贝叶斯优化中。

Method: 探索多种将结构信息融入贝叶斯优化的方法，与多种仅基于序列的方法进行对比，并提出一种基于蛋白质语言模型的“软约束”来引导优化方向，实验针对抗体的结合亲和力和稳定性两种性质进行评估。

Result: 特定类型的结构信息可提升稳定性的早期优化数据效率，但峰值性能相当；引入蛋白质语言模型软约束后，亲和力的数据效率差距缩小，稳定性则完全消除，使得仅用序列的方法表现与结构方法相当。

Conclusion: 在引入蛋白质语言模型软约束的情况下，仅使用序列的贝叶斯优化方法可达到与结构方法相当的性能，这引发了对在抗体优化中使用结构信息必要性的质疑。

Abstract: Bayesian optimization is a natural candidate for the engineering of antibody
therapeutic properties, which is often iterative and expensive. However,
finding the optimal choice of surrogate model for optimization over the highly
structured antibody space is difficult, and may differ depending on the
property being optimized. Moreover, to the best of our knowledge, no prior
works have attempted to incorporate structural information into antibody
Bayesian optimization. In this work, we explore different approaches to
incorporating structural information into Bayesian optimization, and compare
them to a variety of sequence-only approaches on two different antibody
properties, binding affinity and stability. In addition, we propose the use of
a protein language model-based ``soft constraint,'' which helps guide the
optimization to promising regions of the space. We find that certain types of
structural information improve data efficiency in early optimization rounds for
stability, but have equivalent peak performance. Moreover, when incorporating
the protein language model soft constraint we find that the data efficiency gap
is diminished for affinity and eliminated for stability, resulting in
sequence-only methods that match the performance of structure-based methods,
raising questions about the necessity of structure in Bayesian optimization for
antibodies.

</details>


### [853] [OAT-FM: Optimal Acceleration Transport for Improved Flow Matching](https://arxiv.org/abs/2509.24936)
*Angxiao Yue,Anqi Dong,Hongteng Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优加速传输（OAT）理论的Flow Matching新方法OAT-FM，通过优化样本与速度联合空间中的加速度传输，实现了对现有FM方法的改进，并引入两阶段生成模型微调范式，有效提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于最优传输（OT）的Flow Matching方法隐含了流线性化目标，但未充分考虑加速度的影响，限制了生成质量。作者希望从物理作用量角度重新建模，提升FM的理论解释性与实际性能。

Method: 将Flow Matching与最优加速传输（OAT）理论结合，提出OAT-FM方法，其目标函数对应于流线性的充要条件；设计低复杂度算法实现该方法，并提出先训练再用OAT-FM微调的两阶段范式。

Result: OAT-FM在多个生成任务中均能显著提升已有FM模型的性能，且无需重新生成大量噪声数据，避免分布漂移风险，具有高效性和通用性。

Conclusion: OAT-FM为Flow Matching提供了新的理论视角和优化路径，所提出的两阶段范式可作为通用增强策略，推动生成模型性能提升。

Abstract: As a powerful technique in generative modeling, Flow Matching (FM) aims to
learn velocity fields from noise to data, which is often explained and
implemented as solving Optimal Transport (OT) problems. In this study, we
bridge FM and the recent theory of Optimal Acceleration Transport (OAT),
developing an improved FM method called OAT-FM and exploring its benefits in
both theory and practice. In particular, we demonstrate that the straightening
objective hidden in existing OT-based FM methods is mathematically equivalent
to minimizing the physical action associated with acceleration defined by OAT.
Accordingly, instead of enforcing constant velocity, OAT-FM optimizes the
acceleration transport in the product space of sample and velocity, whose
objective corresponds to a necessary and sufficient condition of flow
straightness. An efficient algorithm is designed to achieve OAT-FM with low
complexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative
model trained by an arbitrary FM method, whose velocity information has been
relatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm
eliminates the risk of data distribution drift and the need to generate a large
number of noise data pairs, which consistently improves model performance in
various generative tasks. Code is available at:
https://github.com/AngxiaoYue/OAT-FM

</details>


### [854] [Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer](https://arxiv.org/abs/2509.24947)
*Sooraj Sathish,Keshav Goyal,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 本文提出了一种新的深度Q学习方法，通过引入正则化项减少状态表征间的正相关性，从而提升线性函数逼近在迁移学习中的效果，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习训练成本高且需要大量超参数调优，迁移学习虽可缓解问题，但标准模型学习到的特征高度相关，限制了其在线性函数逼近中的应用。

Method: 提出一种新的深度Q学习算法，引入正则化项以减少神经网络最后一层隐藏层中状态表征的正相关性，并将去相关的特征用于线性函数逼近器进行迁移学习。

Result: 在标准RL基准和MinAtar游戏上的实验和消融研究表明，该方法能有效提升迁移学习性能，减少计算开销。

Conclusion: 通过减少特征间的正相关性，改进的深度Q学习方法显著提升了迁移学习中线性模型的性能，为高效、低代价的强化学习提供了可行路径。

Abstract: Deep Reinforcement Learning (RL) has demonstrated success in solving complex
sequential decision-making problems by integrating neural networks with the RL
framework. However, training deep RL models poses several challenges, such as
the need for extensive hyperparameter tuning and high computational costs.
Transfer learning has emerged as a promising strategy to address these
challenges by enabling the reuse of knowledge from previously learned tasks for
new, related tasks. This avoids the need for retraining models entirely from
scratch. A commonly used approach for transfer learning in RL is to leverage
the internal representations learned by the neural network during training.
Specifically, the activations from the last hidden layer can be viewed as
refined state representations that encapsulate the essential features of the
input. In this work, we investigate whether these representations can be used
as input for training simpler models, such as linear function approximators, on
new tasks. We observe that the representations learned by standard deep RL
models can be highly correlated, which limits their effectiveness when used
with linear function approximation. To mitigate this problem, we propose a
novel deep Q-learning approach that introduces a regularization term to reduce
positive correlations between feature representation of states. By leveraging
these reduced correlated features, we enable more effective use of linear
function approximation in transfer learning. Through experiments and ablation
studies on standard RL benchmarks and MinAtar games, we demonstrate the
efficacy of our approach in improving transfer learning performance and thereby
reducing computational overhead.

</details>


### [855] [Intra-request branch orchestration for efficient LLM reasoning](https://arxiv.org/abs/2509.24957)
*Weifan Jiang,Rana Shahout,Yilun Du,Michael Mitzenmacher,Minlan Yu*

Main category: cs.LG

TL;DR: DUCHESS是一种面向大语言模型的推理服务系统，通过基于预测的分支调度机制，在不牺牲准确率的前提下显著降低推理成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时推理算法（如思维链、多分支推理）虽然提升了复杂任务的准确性，但带来了更高的token消耗和请求延迟；此前工作主要关注减少token使用，常以牺牲准确率为代价，且忽略了其他延迟因素。

Method: DUCHESS采用轻量级线性探针模型，利用LLM层激活值来预测各推理分支的正确性，并据此动态决定分支的终止、复制或继续；在多请求场景下，结合提示词难度估计，优先处理较简单的推理任务，实现更优的调度。

Result: 在三个推理基准上的实验表明，与自洽性方法相比，DUCHESS在相同准确率下减少了42%-63%的token使用；在vLLM服务中，使用先来先服务调度时，平均、中位数和尾部延迟分别降低了57%-81%、58%-85%和52%-84%，并在高请求率下的难度感知调度中取得进一步增益。

Conclusion: DUCHESS通过预测引导的分支编排策略，有效平衡了大模型推理中的准确性、成本与延迟，显著提升了推理服务效率。

Abstract: Large Language Models (LLMs) increasingly rely on inference-time reasoning
algorithms such as chain-of-thought and multi-branch reasoning to improve
accuracy on complex tasks. These methods, however, substantially increase token
usage and per-request latency. Prior work has largely focused on reducing token
usage, often at the expense of accuracy, while overlooking other latency
factors. We present DUCHESS, an LLM serving system that reduces cost and
latency without sacrificing accuracy through intra-request branch orchestration
guided by predictions. DUCHESS employs a lightweight linear probing model over
LLM layer activations to estimate branch correctness, and its orchestration
policy decides whether to terminate, duplicate, or continue a branch. When
handling multiple requests, DUCHESS further reduces latency by prioritizing
easier reasoning tasks when complexity can be estimated from the prompt.
Experiments on three reasoning benchmarks show that DUCHESS consistently
improves the token-accuracy Pareto frontier, reducing token usage by 42-63% at
matched accuracy compared to self-consistency. In serving with vLLM, DUCHESS
reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with
First-Come-First-Served scheduling, and achieves additional gains under
difficulty-aware scheduling at higher request rates.

</details>


### [856] [Overlap-Adaptive Regularization for Conditional Average Treatment Effect Estimation](https://arxiv.org/abs/2509.24962)
*Valentyn Melnychuk,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出了一种新的重叠自适应正则化方法（OAR），用于改善低重叠区域下条件平均处理效应（CATE）估计的性能，该方法可与现有元学习器结合使用，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的CATE估计方法在低重叠情况下表现不佳，因此需要一种能在低重叠区域提升估计性能的新方法。

Method: 提出重叠自适应正则化（OAR），根据重叠权重对目标模型进行正则化，使低重叠区域受到更强的正则化，并可应用于参数和非参数第二阶段模型，同时设计了保持Neyman正交性的去偏版本。

Result: 在多个（半）合成实验中，OAR相比恒定正则化显著提升了低重叠场景下的CATE估计精度。

Conclusion: OAR是首个利用重叠权重进行正则化的CATE估计方法，具有灵活性和鲁棒性，能有效改善低重叠区域的治疗效应估计。

Abstract: The conditional average treatment effect (CATE) is widely used in
personalized medicine to inform therapeutic decisions. However,
state-of-the-art methods for CATE estimation (so-called meta-learners) often
perform poorly in the presence of low overlap. In this work, we introduce a new
approach to tackle this issue and improve the performance of existing
meta-learners in the low-overlap regions. Specifically, we introduce
Overlap-Adaptive Regularization (OAR) that regularizes target models
proportionally to overlap weights so that, informally, the regularization is
higher in regions with low overlap. To the best of our knowledge, our OAR is
the first approach to leverage overlap weights in the regularization terms of
the meta-learners. Our OAR approach is flexible and works with any existing
CATE meta-learner: we demonstrate how OAR can be applied to both parametric and
non-parametric second-stage models. Furthermore, we propose debiased versions
of our OAR that preserve the Neyman-orthogonality of existing meta-learners and
thus ensure more robust inference. Through a series of (semi-)synthetic
experiments, we demonstrate that our OAR significantly improves CATE estimation
in low-overlap settings in comparison to constant regularization.

</details>


### [857] [SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression](https://arxiv.org/abs/2509.25176)
*Haoming Wen,Yushi Bai,Juanzi Li,Jie Tang*

Main category: cs.LG

TL;DR: 本文提出了SIRI方法，通过在训练中交替压缩和扩展推理长度，提升大推理模型的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少大推理模型冗余思维模式时往往牺牲性能，本文旨在克服这一权衡。

Method: 提出一种迭代交替压缩与扩展推理预算的训练机制，动态调整最大生成长度。压缩阶段缩短生成长度以提高推理密度，扩展阶段恢复长度以支持长视野规划。

Result: 在DeepSeek-R1-Distill-Qwen-1.5B上实验显示，SIRI-low三次迭代后在AIME24上性能提升43.2%，用 token 减少46.9%；SIRI-high达到最高准确率。

Conclusion: 周期性调整输出截断长度可有效平衡探索与效率，使模型逼近性能-效率帕累托前沿。

Abstract: We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved
Compression, a simple yet effective RL approach for Large Reasoning Models
(LRMs) that enables more efficient and accurate reasoning. Existing studies
have observed repetitive thinking patterns in LRMs, and attempts to reduce them
often come at the cost of performance. In this paper, we show that this
trade-off can be overcome through a training regime that iteratively alternates
between compressing and expanding the reasoning budget, by dynamically
adjusting the maximum rollout length during training. The compression phase
cuts the rollout length, forcing the model to make precise and valuable
decisions within a limited context, which effectively reduces redundant tokens
and increases reasoning density. The expansion phase then relaxes the length
limit, providing space for the model to explore and plan in long-horizon
settings. Remarkably, we find that after each compression-expansion cycle, the
model's performance improves even as its output length decreases, steadily
pushing it closer to the Pareto frontier in the performance-efficiency
trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves
performance on AIME24 by 43.2% while reducing token usage by 46.9% after three
iterations, and SIRI-high achieves the highest accuracy compared to all other
methods (Figure 1). Our findings shed light on the potential of periodically
oscillating the LRM's output truncation length during training to dynamically
balance exploration and efficiency in reasoning, converging towards an optimal
"sweet spot" between the two. Our models are publicly available.

</details>


### [858] [Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models](https://arxiv.org/abs/2509.24974)
*Ahmad Fraij,Sam Dauncey*

Main category: cs.LG

TL;DR: 本文研究了离散扩散模型与自回归模型在小样本条件下的效率差异，发现自回归模型在小规模数据集上更具样本效率，而离散扩散模型需要更大的模型容量和更多训练轮次才能达到可比性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺，需要更高效的大型语言模型，因此需要比较不同模型的样本效率。

Method: 利用双下降现象，全面比较离散扩散模型和自回归模型的样本效率。

Result: 离散扩散模型需要更大的容量和更多训练轮次才能脱离欠参数化状态并达到插值阈值；在强过参数化状态下，两类模型表现相似，测试损失均未出现明显的二次下降。

Conclusion: 自回归模型在小规模数据集上更具样本效率，而离散扩散模型只有在具备足够容量和计算资源时才具有竞争力。

Abstract: Data scarcity drives the need for more sample-efficient large language
models. In this work, we use the double descent phenomenon to holistically
compare the sample efficiency of discrete diffusion and autoregressive models.
We show that discrete diffusion models require larger capacity and more
training epochs to escape their underparameterized regime and reach the
interpolation threshold. In the strongly overparameterized regime, both models
exhibit similar behavior, with neither exhibiting a pronounced second descent
in test loss across a large range of model sizes. Overall, our results indicate
that autoregressive models are more sample-efficient on small-scale datasets,
while discrete diffusion models only become competitive when given sufficient
capacity and compute.

</details>


### [859] [Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards](https://arxiv.org/abs/2509.24981)
*Haoran He,Yuxiao Ye,Qingpeng Cai,Chen Hu,Binxing Jiao,Daxin Jiang,Ling Pan*

Main category: cs.LG

TL;DR: 本文提出了一种名为ROVER的简化强化学习方法，用于提升大语言模型在数学推理中的表现。通过形式化RLVR为具有特定结构的马尔可夫决策过程，作者证明了最优动作可以从一个固定均匀随机策略的Q函数中恢复，从而绕开传统策略迭代框架。ROVER方法在保持高多样性和持续探索能力的同时，在多个基准上显著优于现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于PPO等策略优化框架的RLVR方法存在训练不稳定和多样性崩溃问题，需要复杂的启发式技巧和精细调参。作者希望设计一种更简单、稳定且高效的方法来解决数学推理中的这些问题。

Method: 将标准RLVR形式化为具有确定性状态转移、树状结构动态和二元终端奖励的有限视界马尔可夫决策过程，并证明最优动作可从固定均匀随机策略的Q函数中恢复。基于此提出ROVER算法，使用softmax对均匀策略下的Q值进行动作采样。

Result: 在多个基础模型和标准数学推理基准上，ROVER相较于现有复杂方法显著提升了性能：pass@1提升+8.2，pass@256提升+16.8，多样性提升+17.6%。

Conclusion: 通过利用RLVR在数学推理中的特殊结构，ROVER以极简的设计实现了更优的性能与多样性，表明在特定任务中可省去传统强化学习算法中的复杂机制。

Abstract: RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for
improving the reasoning abilities of large language models (LLMs). Current
methods rely primarily on policy optimization frameworks like PPO and GRPO,
which follow generalized policy iteration that alternates between evaluating
the current policy's value and improving the policy based on evaluation. While
effective, they often suffer from training instability and diversity collapse,
requiring complex heuristic tricks and careful tuning. We observe that standard
RLVR in math reasoning can be formalized as a specialized finite-horizon Markov
Decision Process with deterministic state transitions, tree-structured
dynamics, and binary terminal rewards. Though large in scale, the underlying
structure is simpler than general-purpose control settings for which popular RL
algorithms (e.g., PPO) were developed, suggesting that several sophisticated
techniques in existing methods may be reduced or even omitted. Based on this
insight, we prove a surprising result: the optimal action can be recovered from
the Q-function of a fixed uniformly random policy, thereby bypassing the
generalized policy iteration loop and its associated heuristics. We introduce
Random Policy Valuation for Diverse Reasoning (ROVER) to translate this
principle into a practical and scalable algorithm for LLM math reasoning, a
minimalist yet highly effective RL method that samples actions from a softmax
over these uniform-policy Q-values. ROVER preserves diversity throughout
training, allowing sustained exploration of multiple valid pathways. Across
multiple base models and standard math reasoning benchmarks, ROVER demonstrates
superior performance in both \textbf{quality} (\textbf{+8.2} on pass@1,
\textbf{+16.8} on pass@256) and \textbf{diversity} (\textbf{+17.6\%}), despite
its radical simplification compared to strong, complicated existing methods.

</details>


### [860] [Sampling Complexity of TD and PPO in RKHS](https://arxiv.org/abs/2509.24991)
*Lu Zou,Wendi Ren,Weizhong Zhang,Liang Ding,Shuang Li*

Main category: cs.LG

TL;DR: 本文从函数空间的角度重新审视近端策略优化（PPO），在再生核希尔伯特空间（RKHS）中解耦策略评估与改进，提出基于核TD的批评家和KL正则化自然梯度策略更新，提供了非渐近、实例自适应的理论保证，并通过实验验证了其在稳定性、采样效率和吞吐量方面的优势。


<details>
  <summary>Details</summary>
Motivation: PPO通常基于有限维假设，缺乏在连续状态-动作空间中的理论基础。本文旨在从函数空间角度为PPO提供更坚实的理论支持，并提升其在实际应用中的稳定性和效率。

Method: 在RKHS中解耦策略评估与改进：使用核化时序差分（TD）批评家进行高效的RKHS梯度更新；采用KL正则化的自然梯度策略更新，通过对评估的动作值函数进行指数化操作来实现类似PPO/TRPO的近端更新。

Result: 提供了依赖于RKHS熵的非渐近、实例自适应收敛保证，统一了多种核设定下的分析；推导出确保最优k^{-1/2}收敛速率的采样规则；实验表明理论对齐的调度提升了常见控制任务上的稳定性和采样效率，且核TD批评家相比GAE基线具有更高吞吐量。

Conclusion: 本文为PPO在连续状态-动作空间中建立了更坚实的理论基础，阐明了在何种条件下基于RKHS的近端更新结合核TD批评家能够实现全局策略改进并保持实际效率。

Abstract: We revisit Proximal Policy Optimization (PPO) from a function-space
perspective. Our analysis decouples policy evaluation and improvement in a
reproducing kernel Hilbert space (RKHS): (i) A kernelized temporal-difference
(TD) critic performs efficient RKHS-gradient updates using only one-step
state-action transition samples; (ii) a KL-regularized, natural-gradient policy
step exponentiates the evaluated action-value, recovering a PPO/TRPO-style
proximal update in continuous state-action spaces. We provide non-asymptotic,
instance-adaptive guarantees whose rates depend on RKHS entropy, unifying
tabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes,
and we derive a sampling rule for the proximal update that ensures the optimal
$k^{-1/2}$ convergence rate for stochastic optimization. Empirically, the
theory-aligned schedule improves stability and sample efficiency on common
control tasks (e.g., CartPole, Acrobot), while our TD-based critic attains
favorable throughput versus a GAE baseline. Altogether, our results place PPO
on a firmer theoretical footing beyond finite-dimensional assumptions and
clarify when RKHS-proximal updates with kernel-TD critics yield global policy
improvement with practical efficiency.

</details>


### [861] [Score-based Membership Inference on Diffusion Models](https://arxiv.org/abs/2509.25003)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 本文研究了针对扩散模型的基于分数的成员推断攻击（MIA），提出了一种高效的单查询攻击方法SimA，并发现潜在扩散模型（LDM）由于其潜在编码器的信息瓶颈而相对更鲁棒，同时提出了增强LDM抗攻击能力的策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能泄露训练数据的成员信息，引发隐私问题，因此需要深入理解基于分数的MIA机制并设计有效攻击与防御方法。

Method: 通过理论分析噪声预测向量的性质，发现去噪器输出的范数可反映样本与训练集的接近程度；基于此提出SimA攻击方法，仅需一次查询即可判断成员身份，并在多种扩散模型上进行实验验证。

Result: SimA在DDPM和LDM等模型上均表现出优异的攻击性能；发现LDM比像素空间模型更不易受MIA影响，归因于其潜在空间的信息瓶颈；通过调节β-VAE中的β参数可进一步提升防御效果。

Conclusion: 基于分数的MIA可通过去噪器输出范数有效实现，SimA是一种高效且原理清晰的攻击方法；LDM具有天然防御优势，但需进一步研究其潜在空间的可逆性以提升安全性。

Abstract: Membership inference attacks (MIAs) against diffusion models have emerged as
a pressing privacy concern, as these models may inadvertently reveal whether a
given sample was part of their training set. We present a theoretical and
empirical study of score-based MIAs, focusing on the predicted noise vectors
that diffusion models learn to approximate. We show that the expected denoiser
output points toward a kernel-weighted local mean of nearby training samples,
such that its norm encodes proximity to the training set and thereby reveals
membership. Building on this observation, we propose SimA, a single-query
attack that provides a principled, efficient alternative to existing
multi-query methods. SimA achieves consistently strong performance across
variants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent
Diffusion Models are surprisingly less vulnerable than pixel-space models, due
to the strong information bottleneck imposed by their latent auto-encoder. We
further investigate this by differing the regularization hyperparameters
($\beta$ in $\beta$-VAE) in latent channel and suggest a strategy to make LDM
training more robust to MIA. Our results solidify the theory of score-based
MIAs, while highlighting that Latent Diffusion class of methods requires better
understanding of inversion for VAE, and not simply inversion of the Diffusion
process

</details>


### [862] [Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting](https://arxiv.org/abs/2509.25017)
*Spyros Kondylatos,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.LG

TL;DR: 提出一种能够同时捕捉模型不确定性和数据不确定性的深度学习框架，用于提升野火危险性预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在野火预测中缺乏不确定性量化，影响其可靠性，限制了实际应用。

Method: 构建一个联合建模认知不确定性（epistemic）和偶然不确定性（aleatoric）的深度学习框架，用于短期野火危险预测，并生成带有不确定性图层的风险地图。

Result: 相比确定性基线模型，F1分数提高2.3%，校准误差降低2.1%；实验证明不确定性估计可靠，且在决策支持中具有实用价值；长期预测显示偶然不确定性随时间增加，而认知不确定性保持稳定。

Conclusion: 联合建模两种不确定性不仅提升了预测性能和模型校准，还在复杂条件下提供互补信息，增强了野火预测系统的可信度和鲁棒性。

Abstract: Wildfires are among the most severe natural hazards, posing a significant
threat to both humans and natural ecosystems. The growing risk of wildfires
increases the demand for forecasting models that are not only accurate but also
reliable. Deep Learning (DL) has shown promise in predicting wildfire danger;
however, its adoption is hindered by concerns over the reliability of its
predictions, some of which stem from the lack of uncertainty quantification. To
address this challenge, we present an uncertainty-aware DL framework that
jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance
short-term wildfire danger forecasting. In the next-day forecasting, our
best-performing model improves the F1 Score by 2.3% and reduces the Expected
Calibration Error by 2.1% compared to a deterministic baseline, enhancing both
predictive skill and calibration. Our experiments confirm the reliability of
the uncertainty estimates and illustrate their practical utility for decision
support, including the identification of uncertainty thresholds for rejecting
low-confidence predictions and the generation of well-calibrated wildfire
danger maps with accompanying uncertainty layers. Extending the forecast
horizon up to ten days, we observe that aleatoric uncertainty increases with
time, showing greater variability in environmental conditions, while epistemic
uncertainty remains stable. Finally, we show that although the two uncertainty
types may be redundant in low-uncertainty cases, they provide complementary
insights under more challenging conditions, underscoring the value of their
joint modeling for robust wildfire danger prediction. In summary, our approach
significantly improves the accuracy and reliability of wildfire danger
forecasting, advancing the development of trustworthy wildfire DL systems.

</details>


### [863] [MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts](https://arxiv.org/abs/2509.25020)
*Jiayu Liu,Zhenya Huang,Anya Sims,Enhong Chen,Yee Whye Teh,Ning Miao*

Main category: cs.LG

TL;DR: 本文提出了一种新的大语言模型推理范式MARCOS，将推理建模为连续高维‘思维’的隐马尔可夫链，而非传统的token级思维链（CoT），从而在保持甚至超越CoT性能的同时实现高达15.7倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法存在推理速度慢、信息瓶颈和推理与生成过程纠缠等问题，限制了模型的效率和推理质量，因此需要一种更高效、解耦的推理范式。

Method: 将推理过程建模为隐马尔可夫链中的连续高维‘思维’状态，显式的推理步骤作为可观测变量；提出两阶段变分训练方法以适配这种潜在变量模型。

Result: 在三个基准上实验表明，MARCOS性能优于现有连续推理方法，与token-based CoT相当，并在GSM8K上以最高15.7倍的推理速度提升实现了4.7%的性能超越。

Conclusion: MARCOS提供了一种高效、解耦的新推理范式，不仅提升了推理效率和性能，还支持按步骤控制随机性，为强化学习和更复杂的推理任务提供了新机会。

Abstract: The current paradigm for reasoning in large language models (LLMs) involves
models "thinking out loud" via a sequence of tokens, known as chain-of-thought
(CoT). This approach, while effective, has several significant drawbacks.
Firstly, inference requires autoregressive generation of often thousands of CoT
tokens, which is slow and computationally expensive. Secondly, it constrains
reasoning to the discrete space of tokens, creating an information bottleneck
across reasoning steps. Thirdly, it fundamentally entangles reasoning with
token generation, forcing LLMs to "think while speaking," which causes
potentially short-sighted reasoning. In light of these limitations, we
re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our
approach, rather than autoregressively generating tokens, we model reasoning as
a hidden Markov chain of continuous, high-dimensional "thoughts". Each
reasoning step involves a transition of the internal thoughts, where explicit
reasoning steps (which may consist of hundreds of tokens) serve as observable
variables, which are windows to peek into the implicit thoughts. Since this
latent process is incompatible with the standard supervised learning, we
further propose a two-phase variational training scheme. Our experiments on
three benchmarks demonstrate that MARCOS outperforms existing continuous
reasoning methods and, for the first time, achieves performance comparable to
token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup
in inference. Beyond this, MARCOS offers additional advantages, such as
step-level instead of token-level control over randomness, opening significant
opportunities for reinforcement learning and reasoning in LLMs.

</details>


### [864] [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](https://arxiv.org/abs/2509.25031)
*Sophia V. Kuhn,Rafael Bischof,Marius Weber,Antoine Binggeli,Michael A. Kraus,Walter Kaufmann,Fernando Pérez-Cruz*

Main category: cs.LG

TL;DR: 提出基于贝叶斯神经网络（BNN）的代理模型，用于快速评估老化桥梁结构的安全性，通过预测规范合规因子并量化不确定性，实现高效、低成本的基础设施优先级排序。


<details>
  <summary>Details</summary>
Motivation: 老化基础设施的维护面临资源分配难题，传统高精度模拟成本高且难以大规模应用，需平衡分析精度与计算成本。

Method: 利用参数化流程生成大规模非线性有限元分析数据，基于瑞士联邦铁路桥梁组合成数据库，训练BNN代理模型以预测代码合规因子，并校准认知不确定性。

Result: 模型能高效准确地估计高保真分析结果，实现快速、不确定性感知的初步筛选，识别关键结构并指导是否需要精细分析，在实际铁路下穿案例中验证了其减少不必要分析和干预的潜力。

Conclusion: BNN代理模型可显著降低基础设施组合评估的成本和碳排放，为大规模结构预评估提供了一种高效、可靠的新方法。

Abstract: Aging infrastructure portfolios pose a critical resource allocation
challenge: deciding which structures require intervention and which can safely
remain in service. Structural assessments must balance the trade-off between
cheaper, conservative analysis methods and accurate but costly simulations that
do not scale portfolio-wide. We propose Bayesian neural network (BNN)
surrogates for rapid structural pre-assessment of worldwide common bridge
types, such as reinforced concrete frame bridges. Trained on a large-scale
database of non-linear finite element analyses generated via a parametric
pipeline and developed based on the Swiss Federal Railway's bridge portfolio,
the models accurately and efficiently estimate high-fidelity structural
analysis results by predicting code compliance factors with calibrated
epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware
triage: flagging likely critical structures and providing guidance where
refined analysis is pertinent. We demonstrate the framework's effectiveness in
a real-world case study of a railway underpass, showing its potential to
significantly reduce costs and emissions by avoiding unnecessary analyses and
physical interventions across entire infrastructure portfolios.

</details>


### [865] [A multiscale analysis of mean-field transformers in the moderate interaction regime](https://arxiv.org/abs/2509.25040)
*Giuseppe Bruno,Federico Pasqualotto,Andrea Agazzi*

Main category: cs.LG

TL;DR: 本文研究了在推理过程中，仅编码器的Transformer模型中token随深度演化的动态行为，将其建模为均场相互作用的粒子系统，并在中等相互作用体制下分析其多尺度演化过程。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型中token在不同层之间的动态演化机制，尤其是在大规模token和特定参数缩放下的行为。

Method: 将token视为均场相互作用的粒子系统，在中等相互作用体制下（token数量N较大且逆温度参数β随N缩放），分析系统的多尺度动力学，并严格刻画各阶段的极限动态。

Result: 发现系统存在三个阶段：快速阶段（token经验测度坍缩到低维空间）、中间阶段（测度进一步聚集成簇）和慢速阶段（簇逐步合并为单一簇），并证明了在相应极限下的收敛性，通过模拟验证了结果。

Conclusion: 该工作为理解Transformer模型深层的token动态提供了理论框架，揭示了其多尺度演化特性。

Abstract: In this paper, we study the evolution of tokens through the depth of
encoder-only transformer models at inference time by modeling them as a system
of particles interacting in a mean-field way and studying the corresponding
dynamics. More specifically, we consider this problem in the moderate
interaction regime, where the number $N$ of tokens is large and the inverse
temperature parameter $\beta$ of the model scales together with $N$. In this
regime, the dynamics of the system displays a multiscale behavior: a fast
phase, where the token empirical measure collapses on a low-dimensional space,
an intermediate phase, where the measure further collapses into clusters, and a
slow one, where such clusters sequentially merge into a single one. We provide
a rigorous characterization of the limiting dynamics in each of these phases
and prove convergence in the above mentioned limit, exemplifying our results
with some simulations.

</details>


### [866] [Efficient Hyperparameter Tuning via Trajectory Invariance Principle](https://arxiv.org/abs/2509.25049)
*Bingrui Li,Jiaxin Wen,Zhanpeng Zhou,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: 本文提出了轨迹不变性现象，揭示了学习率和权重衰减组合下训练损失曲线、梯度噪声和梯度范数的不变性，从而将超参数调优从二维空间简化为一维，提供了一种高效的调优规则。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模扩大，超参数调优成本越来越高，但目前缺乏有效的指导原则。因此，需要建立适用于批量大小、学习率和权重衰减等广泛超参数的高效调优方法。

Method: 通过分析预训练过程中的损失曲线、梯度噪声和梯度范数，识别出学习率与权重衰减组合下的轨迹不变性现象，并基于此提出简化的调优方向。同时对现有缩放定律进行改进和重新评估。

Result: 发现了轨迹不变性现象，有效压缩了超参数搜索空间；提出了沿不变性方向进行调优的新准则；改进了先前的缩放定律，并挑战了若干现有观点。

Conclusion: 本工作为大规模超参数调优提供了新的高效原则，推动了对缩放定律的进一步研究，具有实际应用价值和理论启发意义。

Abstract: As hyperparameter tuning becomes increasingly costly at scale, efficient
tuning methods are essential. Yet principles for guiding hyperparameter tuning
remain limited. In this work, we seek to establish such principles by
considering a broad range of hyperparameters, including batch size, learning
rate, and weight decay. We identify a phenomenon we call trajectory invariance,
where pre-training loss curves, gradient noise, and gradient norm exhibit
invariance--closely overlapping--with respect to a quantity that combines
learning rate and weight decay. This phenomenon effectively reduces the
original two-dimensional hyperparameter space to one dimension, yielding an
efficient tuning rule: follow the salient direction revealed by trajectory
invariance. Furthermore, we refine previous scaling laws and challenge several
existing viewpoints. Overall, our work proposes new principles for efficient
tuning and inspires future research on scaling laws.

</details>


### [867] [Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models](https://arxiv.org/abs/2509.25050)
*Shuchen Xue,Chongjian Ge,Shilong Zhang,Yichen Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 本文提出了一种名为Advantage Weighted Matching (AWM)的强化学习方法，用于扩散模型，通过统一预训练和强化学习的目标函数，降低方差并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型强化学习方法（如DDPO）使用与预训练不同的目标函数，导致训练效率低下。本文旨在建立一个更高效、统一的框架。

Method: 提出AWM方法，基于得分/流匹配损失（与预训练一致），并通过优势函数对样本加权，从而在保持建模目标一致的同时提升高奖励样本的影响。

Result: 在Stable Diffusion 3.5 Medium和FLUX上，AWM相比Flow-GRPO实现了最高24倍的加速，且不牺牲生成质量。

Conclusion: AWM通过统一预训练与强化学习目标，提供了一种低方差、快速收敛且理论一致的扩散模型强化学习新范式。

Abstract: Reinforcement Learning (RL) has emerged as a central paradigm for advancing
Large Language Models (LLMs), where pre-training and RL post-training share the
same log-likelihood formulation. In contrast, recent RL approaches for
diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),
optimize an objective different from the pretraining objectives--score/flow
matching loss. In this work, we establish a novel theoretical analysis: DDPO is
an implicit form of score/flow matching with noisy targets, which increases
variance and slows convergence. Building on this analysis, we introduce
\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for
diffusion. It uses the same score/flow-matching loss as pretraining to obtain a
lower-variance objective and reweights each sample by its advantage. In effect,
AWM raises the influence of high-reward samples and suppresses low-reward ones
while keeping the modeling objective identical to pretraining. This unifies
pretraining and RL conceptually and practically, is consistent with
policy-gradient theory, reduces variance, and yields faster convergence. This
simple yet effective design yields substantial benefits: on GenEval, OCR, and
PickScore benchmarks, AWM delivers up to a $24\times$ speedup over Flow-GRPO
(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,
without compromising generation quality. Code is available at
https://github.com/scxue/advantage_weighted_matching.

</details>


### [868] [Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific AI](https://arxiv.org/abs/2509.25080)
*Bogdan Raonić,Siddhartha Mishra,Samuel Lanthaler*

Main category: cs.LG

TL;DR: 提出一种基于得分扩散模型估计联合似然的OOD检测方法，用于评估科学机器学习中回归任务的预测可信度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型在科学领域广泛应用，但在分布外（OOD）数据上可能失败，如何检测回归任务中的此类失败仍是一个开放问题。

Method: 利用基于得分的扩散模型估计输入和模型预测的联合似然，生成任务感知的可靠性评分。

Result: 在多个科学数据集（包括PDE、卫星图像和脑肿瘤分割）上验证，该方法的似然评分与预测误差强相关。

Conclusion: 该方法为构建可验证的‘信任证书’提供了基础，是评估AI科学预测可信度的实用工具。

Abstract: Data-driven models are increasingly adopted in critical scientific fields
like weather forecasting and fluid dynamics. These methods can fail on
out-of-distribution (OOD) data, but detecting such failures in regression tasks
is an open challenge. We propose a new OOD detection method based on estimating
joint likelihoods using a score-based diffusion model. This approach considers
not just the input but also the regression model's prediction, providing a
task-aware reliability score. Across numerous scientific datasets, including
PDE datasets, satellite imagery and brain tumor segmentation, we show that this
likelihood strongly correlates with prediction error. Our work provides a
foundational step towards building a verifiable 'certificate of trust', thereby
offering a practical tool for assessing the trustworthiness of AI-based
scientific predictions. Our code is publicly available at
https://github.com/bogdanraonic3/OOD_Detection_ScientificML

</details>


### [869] [Towards generalizable deep ptychography neural networks](https://arxiv.org/abs/2509.25104)
*Albert Vong,Steven Henke,Oliver Hoidn,Hanna Ruth,Junjing Deng,Alexander Hexemer,Apurva Mehta,Arianna Gleason,Levi Hancock,Nicholas Schwarz*

Main category: cs.LG

TL;DR: 提出一种以探针学习为中心的无监督训练工作流，通过结合实测探针与合成物体，实现X射线叠层成像中跨实验条件的鲁棒重建，并支持实时反馈。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在多样化的实验条件下缺乏鲁棒性，难以满足下一代光源快速采集下的实时反馈需求。

Method: 采用以探针学习为核心的无监督训练策略，结合实测探针与程序化生成的合成物体，构建物理信息神经网络进行重建。

Result: 该方法实现了在多个光束线上对未见实验的重建，具备良好的泛化能力；重建精度可媲美仅使用实验数据训练的模型，且支持动态实验条件下的实时反馈。

Conclusion: 探针学习与分布内学习同等重要，所提方法为X射线叠层成像提供了具有跨实验泛化能力的高效重建方案，适用于未来高通量光源的实时调控。

Abstract: X-ray ptychography is a data-intensive imaging technique expected to become
ubiquitous at next-generation light sources delivering many-fold increases in
coherent flux. The need for real-time feedback under accelerated acquisition
rates motivates surrogate reconstruction models like deep neural networks,
which offer orders-of-magnitude speedup over conventional methods. However,
existing deep learning approaches lack robustness across diverse experimental
conditions. We propose an unsupervised training workflow emphasizing probe
learning by combining experimentally-measured probes with synthetic,
procedurally generated objects. This probe-centric approach enables a single
physics-informed neural network to reconstruct unseen experiments across
multiple beamlines; among the first demonstrations of multi-probe
generalization. We find probe learning is equally important as in-distribution
learning; models trained using this synthetic workflow achieve reconstruction
fidelity comparable to those trained exclusively on experimental data, even
when changing the type of synthetic training object. The proposed approach
enables training of experiment-steering models that provide real-time feedback
under dynamic experimental conditions.

</details>


### [870] [Learning in an Echo Chamber: Online Learning with Replay Adversary](https://arxiv.org/abs/2509.25135)
*Daniil Dmitriev,Harald Eskelund Franck,Carolin Heinzler,Amartya Sanyal*

Main category: cs.LG

TL;DR: 本文提出了“重放设置下的在线学习”框架，用于建模机器学习系统在自标注数据上训练时可能陷入错误累积的问题。作者引入了扩展阈值维度（ExThD）作为该模型中可学习性的精确度量，并设计了一种基于闭包的算法，可在对抗任意自适应对手时实现最优错误界。结果表明，重放设置比经典错误界设置更具挑战性，且正确学习与类别是否近似交集封闭密切相关。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统越来越多地使用自我标注的数据进行训练，存在错误被反复强化、形成“回声室”的风险。为理解这一现象，需要建立理论框架来分析模型在接收历史预测作为标签（而非真实标签）时的学习能力。

Method: 提出“重放设置下的在线学习”模型：每轮学习者输出假设，对手则返回真实标签或之前某轮的预测标签。引入扩展阈值维度（ExThD(H)）作为可学习性的刻画工具，并设计基于闭包的算法以最小化真实标签出现时的错误次数。对随机对手和正确学习情形也进行了理论分析。

Result: 证明了ExThD(H)是重放设置下可学习性的精确度量：存在最多犯ExThD(H)次错误的算法，且任何算法都无法超越此界限。对于随机对手，在交集封闭类上可获得类似紧界。研究发现某些Littlestone维数很小的类其ExThD可任意大，说明该设定更难。此外，仅当假设类几乎交集封闭时才可正确学习，否则所有正确学习器将承受线性错误，而本文的非正确算法仍可达ExThD(H)界。

Conclusion: 扩展阈值维度ExThD(H)是重放设置下在线学习的完全可学习性度量。基于闭包的算法在该模型中表现最优，且非正确学习在该设定下具有显著优势。该工作为理解模型在自我反馈环境中的学习极限提供了新的理论基础。

Abstract: As machine learning systems increasingly train on self-annotated data, they
risk reinforcing errors and becoming echo chambers of their own beliefs. We
model this phenomenon by introducing a learning-theoretic framework: Online
Learning in the Replay Setting. In round $t$, the learner outputs a hypothesis
$\hat{h}_t$; the adversary then reveals either the true label $f^\ast(x_t)$ or
a replayed label $\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is
counted only when the true label is shown, yet classical algorithms such as the
SOA or the halving algorithm are easily misled by the replayed errors.
  We introduce the Extended Threshold dimension, $\mathrm{ExThD}(\mathcal{H})$,
and prove matching upper and lower bounds that make
$\mathrm{ExThD}(\mathcal{H})$ the exact measure of learnability in this model.
A closure-based learner makes at most $\mathrm{ExThD}(\mathcal{H})$ mistakes
against any adaptive adversary, and no algorithm can perform better. For
stochastic adversaries, we prove a similar bound for every intersection-closed
class. The replay setting is provably harder than the classical mistake bound
setting: some classes have constant Littlestone dimension but arbitrarily large
$\mathrm{ExThD}(\mathcal{H})$. Proper learning exhibits an even sharper
separation: a class is properly learnable under replay if and only if it is
(almost) intersection-closed. Otherwise, every proper learner suffers
$\Omega(T)$ errors, whereas our improper algorithm still achieves the
$\mathrm{ExThD}(\mathcal{H})$ bound. These results give the first tight
analysis of learning against replay adversaries, based on new results for
closure-type algorithms.

</details>


### [871] [BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression](https://arxiv.org/abs/2509.25136)
*David González Martínez*

Main category: cs.LG

TL;DR: 提出了一种无需微调的高效模型压缩方法BALF，结合激活感知分解和可扩展的预算秩分配器，在多种架构上实现了显著的压缩效果且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络压缩技术通常需要昂贵的微调或搜索过程，限制了在普通硬件上的实用性，因此需要一种高效、无需微调的通用压缩方法。

Method: 提出激活感知的因子分解框架和可扩展的预算化秩分配器，构成BALF管道，适用于多种层类型，灵活控制压缩目标且无额外开销。

Result: 在ResNet-20、ResNeXt-101和Vision Transformer等多种模型和数据集上验证了BALF的有效性，例如在ImageNet上将ResNeXt-101的FLOPs减少45%的同时仅导致1%的top-1准确率下降。

Conclusion: BALF是一种高效、通用且无需微调的模型压缩方案，能够在广泛模型架构中实现高压缩率与低精度损失的平衡。

Abstract: Neural network compression techniques typically require expensive fine-tuning
or search procedures, rendering them impractical on commodity hardware.
Inspired by recent LLM compression research, we present a general
activation-aware factorization framework that can be applied to a broad range
of layers. Moreover, we introduce a scalable budgeted rank allocator that
allows flexible control over compression targets (e.g., retaining 50% of
parameters) with no overhead. Together, these components form BALF, an
efficient pipeline for compressing models without fine-tuning. We demonstrate
its effectiveness across multiple scales and architectures, from ResNet-20 on
CIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it
achieves excellent results in the fine-tuning-free regime. For instance, BALF
reduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1
accuracy drop.

</details>


### [872] [High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification](https://arxiv.org/abs/2509.25153)
*Nicholas Barnfield,Hugo Cui,Yue M. Lu*

Main category: cs.LG

TL;DR: 该论文研究了注意力机制在长序列中如何学会选择性关注信息性标记，从而检测弱、稀疏且罕见的特征。理论分析表明，单层注意力分类器在信号强度仅随序列长度对数增长时即可实现趋近于零的测试误差，优于需要平方根增长的线性分类器。


<details>
  <summary>Details</summary>
Motivation: 探讨注意力机制在稀疏信号检测中的优势，尤其是在信号微弱、稀少且分布稀疏的情况下，传统方法难以有效识别，而注意力机制可能通过自适应聚焦提升性能。

Method: 构建一个稀疏标记分类模型，在正样本中嵌入弱信号向量，负样本为纯噪声；在长序列极限下分析注意力分类器的表征能力，并在高维有限序列长度下研究梯度训练动态，推导测试误差和训练损失的渐近表达式。

Result: 证明单层注意力分类器在信号强度仅以log L增长时可实现可忽略的测试误差；仅需两次梯度更新即可使查询权重向量与隐藏信号对齐，形成选择性注意力图；并量化了其容量优势。

Conclusion: 注意力机制通过自适应地放大信息性标记，在检测弱稀疏信号方面显著优于非自适应线性分类器，其成功源于早期梯度驱动的信号对齐与高效的信息提取能力。

Abstract: When and how can an attention mechanism learn to selectively attend to
informative tokens, thereby enabling detection of weak, rare, and sparsely
located features? We address these questions theoretically in a sparse-token
classification model in which positive samples embed a weak signal vector in a
randomly chosen subset of tokens, whereas negative samples are pure noise. In
the long-sequence limit, we show that a simple single-layer attention
classifier can in principle achieve vanishing test error when the signal
strength grows only logarithmically in the sequence length $L$, whereas linear
classifiers require $\sqrt{L}$ scaling. Moving from representational power to
learnability, we study training at finite $L$ in a high-dimensional regime,
where sample size and embedding dimension grow proportionally. We prove that
just two gradient updates suffice for the query weight vector of the attention
classifier to acquire a nontrivial alignment with the hidden signal, inducing
an attention map that selectively amplifies informative tokens. We further
derive an exact asymptotic expression for the test error and training loss of
the trained attention-based classifier, and quantify its capacity -- the
largest dataset size that is typically perfectly separable -- thereby
explaining the advantage of adaptive token selection over nonadaptive linear
baselines.

</details>


### [873] [Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation](https://arxiv.org/abs/2509.25157)
*Jinhao Liang,Yixuan Sun,Anirban Samaddar,Sandeep Madireddy,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: 提出了一种无需训练的新型采样方法Chance-constrained Flow Matching (CCFM)，在生成过程中结合随机优化以有效满足硬约束，同时保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 生成模型常违反来自物理规律或任务要求的硬约束，传统投影方法会扭曲分布，而多阶段方法复杂且易累积误差。

Method: 将随机优化融入采样过程，在噪声中间样本上直接操作，理论上等价于对干净样本的可行集进行投影，避免分布失真。

Result: 实验表明，CCFM在偏微分方程描述的物理系统和分子对接问题中，相比现有最先进方法具有更高的可行性与保真度。

Conclusion: CCFM提供了一种高效、无需训练的方式来满足生成过程中的硬约束，在保证样本质量的同时确保可行性。

Abstract: Generative models excel at synthesizing high-fidelity samples from complex
data distributions, but they often violate hard constraints arising from
physical laws or task specifications. A common remedy is to project
intermediate samples onto the feasible set; however, repeated projection can
distort the learned distribution and induce a mismatch with the data manifold.
Thus, recent multi-stage procedures attempt to defer projection to clean
samples during sampling, but they increase algorithmic complexity and
accumulate errors across steps. This paper addresses these challenges by
proposing a novel training-free method, Chance-constrained Flow Matching
(CCFM), that integrates stochastic optimization into the sampling process,
enabling effective enforcement of hard constraints while maintaining
high-fidelity sample generation. Importantly, CCFM guarantees feasibility in
the same manner as conventional repeated projection, yet, despite operating
directly on noisy intermediate samples, it is theoretically equivalent to
projecting onto the feasible set defined by clean samples. This yields a
sampler that mitigates distributional distortion. Empirical experiments show
that CCFM outperforms current state-of-the-art constrained generative models in
modeling complex physical systems governed by partial differential equations
and molecular docking problems, delivering higher feasibility and fidelity.

</details>


### [874] [Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids](https://arxiv.org/abs/2509.25158)
*Ehimare Okoyomon,Arbel Yaniv,Christoph Goebel*

Main category: cs.LG

TL;DR: 本研究系统探讨了三种物理信息策略在提升图神经网络对配电网电压预测的归纳偏置作用，通过ENGAGE数据集验证了各策略在标准性能和分布外泛化能力上的效果。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据有限或不完整，现有机器学习模型在配电网电压预测中泛化能力差，难以保证电力系统稳定性。

Method: 采用三种物理信息策略：(i) 基于潮流约束的损失函数，(ii) 复数神经网络，(iii) 残差任务重构，并在ENGAGE数据集上进行受控实验以评估各策略的影响。

Result: 实验表明，引入物理先验知识的归纳偏置能显著提升模型在标准预测性能和分布外泛化能力，其中残差任务重构和复数神经网络表现尤为突出。

Conclusion: 融合物理知识的归纳偏置可有效提升GNN在电压预测中的可靠性与鲁棒性，为实际配电网应用提供了可行的建模指导。

Abstract: Voltage prediction in distribution grids is a critical yet difficult task for
maintaining power system stability. Machine learning approaches, particularly
Graph Neural Networks (GNNs), offer significant speedups but suffer from poor
generalization when trained on limited or incomplete data. In this work, we
systematically investigate the role of inductive biases in improving a model's
ability to reliably learn power flow. Specifically, we evaluate three
physics-informed strategies: (i) power-flow-constrained loss functions, (ii)
complex-valued neural networks, and (iii) residual-based task reformulation.
Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid
configurations, we conduct controlled experiments to isolate the effect of each
inductive bias and assess both standard predictive performance and
out-of-distribution generalization. Our study provides practical insights into
which model assumptions most effectively guide learning for reliable and
efficient voltage prediction in modern distribution networks.

</details>


### [875] [GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models](https://arxiv.org/abs/2509.25170)
*Peter Holderrieth,Uriel Singer,Tommi Jaakkola,Ricky T. Q. Chen,Yaron Lipman,Brian Karrer*

Main category: cs.LG

TL;DR: 本文提出了GLASS Flows，一种新的采样范式，通过在预训练模型中构建“流匹配模型内的流匹配模型”来高效模拟马尔可夫转移，结合了ODE的效率与SDE的随机性，在文本到图像生成任务中消除了随机演化与效率之间的权衡，并提升了推理时的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDE采样的奖励对齐算法在推理时效率较低，限制了流匹配和扩散模型的实际应用，因此需要一种更高效的采样方法。

Method: 提出GLASS Flows，利用预训练模型无需重训练即可构建“内层”流匹配模型，通过ODE方式模拟原本需SDE采样的马尔可夫过程，实现高效且具随机性的生成。

Result: 在大规模文本到图像模型上验证了GLASS Flows的有效性，显著提升采样效率和生成性能，结合Feynman-Kac Steering达到当前最优的生成效果。

Conclusion: GLASS Flows是一种简单、即插即用的解决方案，解决了流匹配与扩散模型在推理时效率与性能之间的权衡，为奖励对齐和推理时扩展提供了新范式。

Abstract: The performance of flow matching and diffusion models can be greatly improved
at inference time using reward alignment algorithms, yet efficiency remains a
major limitation. While several algorithms were proposed, we demonstrate that a
common bottleneck is the sampling method these algorithms rely on: many
algorithms require to sample Markov transitions via SDE sampling, which is
significantly less efficient and often less performant than ODE sampling. To
remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that
simulates a "flow matching model within a flow matching model" to sample Markov
transitions. As we show in this work, this "inner" flow matching model can be
retrieved from a pre-trained model without any re-training, combining the
efficiency of ODEs with the stochastic evolution of SDEs. On large-scale
text-to-image models, we show that GLASS Flows eliminate the trade-off between
stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS
Flows improve state-of-the-art performance in text-to-image generation, making
it a simple, drop-in solution for inference-time scaling of flow and diffusion
models.

</details>


### [876] [TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion](https://arxiv.org/abs/2509.25171)
*Sophia Tang,Yuchen Zhu,Molei Tao,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 提出TR2-D2框架，结合蒙特卡洛树搜索与离散扩散模型，通过构建回放缓冲区实现奖励引导的序列生成优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的扩散微调方法容易陷入次优轨迹，因依赖当前模型rollout训练而难以获得最优奖励信号。

Method: 引入树搜索（MCTS）生成高质量轨迹并构建回放缓冲区，用于离散扩散模型在随机最优控制目标下的轨迹感知微调。

Result: 在生物序列生成任务中验证了单目标和多目标微调的有效性，TR2-D2显著提升了奖励表现与生成质量。

Conclusion: TR2-D2通过结合树搜索与轨迹感知微调，实现了更可靠、高效的奖励引导离散序列生成。

Abstract: Reinforcement learning with stochastic optimal control offers a promising
framework for diffusion fine-tuning, where a pre-trained diffusion model is
optimized to generate paths that lead to a reward-tilted distribution. While
these approaches enable optimization without access to explicit samples from
the optimal distribution, they require training on rollouts under the current
fine-tuned model, making them susceptible to reinforcing sub-optimal
trajectories that yield poor rewards. To overcome this challenge, we introduce
TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion
(TR2-D2), a novel framework that optimizes reward-guided discrete diffusion
trajectories with tree search to construct replay buffers for trajectory-aware
fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS)
and subsequently used to fine-tune a pre-trained discrete diffusion model under
a stochastic optimal control objective. We validate our framework on single-
and multi-objective fine-tuning of biological sequence diffusion models,
highlighting the overall effectiveness of TR2-D2 for reliable reward-guided
fine-tuning in discrete sequence generation.

</details>


### [877] [XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning](https://arxiv.org/abs/2509.25174)
*Daniel Palenicek,Florian Vogt,Joe Watson,Ingmar Posner,Jan Peters*

Main category: cs.LG

TL;DR: 本文提出了一种基于优化感知原则的深度演员-评论家算法XQC，通过改进评论家网络的优化景观（如使用批归一化、权重归一化和分布交叉熵损失），显著提升了样本效率，并在多种连续控制任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 提高深度强化学习算法的样本效率，采用更原则性的方法关注评论家网络的优化景观，而非依赖经验性但复杂的模型设计。

Method: 利用Hessian矩阵的特征谱和条件数分析常见架构设计对训练动态的影响，提出结合批归一化、权重归一化和分布交叉熵损失的方法，并构建XQC算法。

Result: XQC在55个本体感觉和15个基于视觉的连续控制任务上达到最先进的样本效率，且使用的参数量显著少于现有方法。

Conclusion: 通过优化感知的设计可显著提升强化学习算法的样本效率和训练稳定性，XQC为高效算法设计提供了新方向。

Abstract: Sample efficiency is a central property of effective deep reinforcement
learning algorithms. Recent work has improved this through added complexity,
such as larger models, exotic network architectures, and more complex
algorithms, which are typically motivated purely by empirical performance. We
take a more principled approach by focusing on the optimization landscape of
the critic network. Using the eigenspectrum and condition number of the
critic's Hessian, we systematically investigate the impact of common
architectural design decisions on training dynamics. Our analysis reveals that
a novel combination of batch normalization (BN), weight normalization (WN), and
a distributional cross-entropy (CE) loss produces condition numbers orders of
magnitude smaller than baselines. This combination also naturally bounds
gradient norms, a property critical for maintaining a stable effective learning
rate under non-stationary targets and bootstrapping. Based on these insights,
we introduce XQC: a well-motivated, sample-efficient deep actor-critic
algorithm built upon soft actor-critic that embodies these optimization-aware
principles. We achieve state-of-the-art sample efficiency across 55
proprioception and 15 vision-based continuous control tasks, all while using
significantly fewer parameters than competing methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [878] [Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence](https://arxiv.org/abs/2509.23144)
*Atma Anand*

Main category: cs.AI

TL;DR: 本文提出了热力学协调理论（TCT），指出多智能体系统在协调过程中受热力学约束，必须以信息损失换取可发现性而非精度，揭示了协调协议的最小描述长度随系统复杂度增长的规律，并解释了从神经网络到官僚体系中的临界现象、滞后效应及对齐伪造等行为。


<details>
  <summary>Details</summary>
Motivation: 理解多智能体多目标系统中协调的本质热力学限制，解释现实中普遍存在的协调困难、路径依赖和对齐失败现象。

Method: 结合信息论、热力学与拓扑学方法，推导协调协议的最小描述长度下限，引入协调温度概念以预测相变与临界行为，并扩展Arrow偏好聚合不可能定理至多层递归情形。

Result: 发现协调过程本质上需要信息损失；推导出协调协议复杂度的下限公式L(P)≥NKlog₂K+N²d²log(1/ε)；提出协调温度与功耗估计框架；解释了多目标优化中的循环行为和大模型对齐伪造现象。

Conclusion: 协调不是追求最优或精确，而是通过信息简化形成可共同发现的焦点，这一过程具有普适的热力学基础，TCT为分析分布式智能系统的集体行为提供了统一框架。

Abstract: Information-processing systems coordinating across multiple agents and
objectives face fundamental thermodynamic constraints. We show that solutions
with maximum utility to act as coordination focal points have much higher
selection pressure for being findable across agents rather than accuracy. We
derive that the information-theoretic minimum description length of
coordination protocols to precision $\varepsilon$ scales as $L(P)\geq NK\log_2
K+N^2d^2\log (1/\varepsilon)$ for $N$ agents with $d$ potentially conflicting
objectives and internal model complexity $K$. This scaling forces progressive
simplification, with coordination dynamics changing the environment itself and
shifting optimization across hierarchical levels. Moving from established focal
points requires re-coordination, creating persistent metastable states and
hysteresis until significant environmental shifts trigger phase transitions
through spontaneous symmetry breaking. We operationally define coordination
temperature to predict critical phenomena and estimate coordination work costs,
identifying measurable signatures across systems from neural networks to
restaurant bills to bureaucracies. Extending the topological version of Arrow's
theorem on the impossibility of consistent preference aggregation, we find it
recursively binds whenever preferences are combined. This potentially explains
the indefinite cycling in multi-objective gradient descent and alignment faking
in Large Language Models trained with reinforcement learning with human
feedback. We term this framework Thermodynamic Coordination Theory (TCT), which
demonstrates that coordination requires radical information loss.

</details>


### [879] [HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis](https://arxiv.org/abs/2509.25112)
*Yiquan Wang,Tin-Yeh Huang,Qingyun Gao,Jialin Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为HeDA的智能多智能体系统，通过构建知识图谱和多层风险传播分析，自动发现热浪相关的级联风险路径。该系统处理了超过1万篇论文，构建了包含两万多个节点的知识图谱，并发现了五条此前未被识别的重要风险链，为气候适应策略提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 科学文献中关于热浪级联风险的知识碎片化，阻碍了对复杂跨系统风险路径的全面理解。

Method: 开发HeDA（热浪发现代理）系统，利用多智能体框架从10,247篇学术论文中提取信息，构建包含23,156个节点和89,472个关系的知识图谱，并采用多层风险传播分析方法识别被忽视的风险传递路径。

Result: 在复杂问答任务中达到78.9%的准确率，比GPT-4等先进基线高出13.7%；成功发现五条新的高影响风险链，例如热浪→用水需求激增→工业用水限制→小企业中断，且经历史案例和专家验证。

Conclusion: HeDA提供了一种AI驱动科学发现的新范式，有助于揭示复杂的级联风险路径，支持更具韧性的气候适应策略制定。

Abstract: Heatwaves pose complex cascading risks across interconnected climate, social,
and economic systems, but knowledge fragmentation in scientific literature
hinders comprehensive understanding of these risk pathways. We introduce HeDA
(Heatwave Discovery Agent), an intelligent multi-agent system designed for
automated scientific discovery through knowledge graph construction and
multi-layer risk propagation analysis. HeDA processes over 10,247 academic
papers to construct a comprehensive knowledge graph with 23,156 nodes and
89,472 relationships, employing novel multi-layer risk propagation analysis to
systematically identify overlooked risk transmission pathways. Our system
achieves 78.9% accuracy on complex question-answering tasks, outperforming
state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA
successfully discovered five previously unidentified high-impact risk chains,
such as the pathway where a heatwave leads to a water demand surge, resulting
in industrial water restrictions and ultimately causing small business
disruption, which were validated through historical case studies and domain
expert review. This work presents a new paradigm for AI-driven scientific
discovery, providing actionable insights for developing more resilient climate
adaptation strategies.

</details>


### [880] [ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration](https://arxiv.org/abs/2509.24230)
*Shaobin Ling,Yun Wang,Chenyou Fan,Tin Lun Lam,Junjie Hu*

Main category: cs.AI

TL;DR: 本文提出了一种名为ELHPlan的新框架，通过引入“动作链”作为规划基本单元，在大语言模型驱动的多机器人协作中实现了效率与适应性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的多机器人协作方法在动态环境中要么缺乏适应性（声明式方法），要么计算成本过高（迭代式方法），难以兼顾效率与性能。

Method: ELHPlan采用基于子目标意图绑定的动作链作为基本规划单元，通过构建、验证、修正和执行四个循环步骤进行协同规划，避免了频繁的整体重规划，提升了效率。

Result: 在TDW-MAT和C-WAH两个基准上的实验表明，ELHPlan在任务成功率相当的情况下，仅消耗最先进的方法24%的token，并显著降低了规划时间。

Conclusion: ELHPlan在保持高任务成功率的同时大幅降低了计算开销，为基于大语言模型的多智能体规划系统建立了新的效率-有效性前沿。

Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but
face fundamental trade-offs: declarative methods lack adaptability in dynamic
environments, while iterative methods incur prohibitive computational costs
that scale poorly with team size and task complexity. In this paper, we propose
ELHPlan, a novel framework that introduces Action Chains--sequences of actions
explicitly bound to sub-goal intentions--as the fundamental planning primitive.
ELHPlan operates via a cyclical process: 1) constructing intention-bound action
sequences, 2) proactively validating for conflicts and feasibility, 3) refining
issues through targeted mechanisms, and 4) executing validated actions. This
design balances adaptability and efficiency by providing sufficient planning
horizons while avoiding expensive full re-planning. We further propose
comprehensive efficiency metrics, including token consumption and planning
time, to more holistically evaluate multi-agent collaboration. Our experiments
on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable
task success rates while consuming only 24% of the tokens required by
state-of-the-art methods. Our research establishes a new
efficiency-effectiveness frontier for LLM-based multi-agent planning systems.

</details>


### [881] [Training Agents Inside of Scalable World Models](https://arxiv.org/abs/2509.24527)
*Danijar Hafner,Wilson Yan,Timothy Lillicrap*

Main category: cs.AI

TL;DR: Dreamer 4 是一种可扩展的智能体，通过在快速且准确的世界模型中进行强化学习，首次仅从离线数据中在 Minecraft 中获得钻石，展示了基于想象训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 以往的世界模型在复杂环境中难以准确预测物体交互，限制了智能体的学习能力。本文旨在构建一个更准确、可扩展的世界模型，以支持无需环境交互的高效行为学习，特别是在交互成本高或不安全的实际应用（如机器人）中。

Method: 提出 Dreamer 4，结合快捷强制目标和高效 Transformer 架构，在单个 GPU 上实现实时推理；利用少量带动作数据和大量无标签视频进行通用动作条件建模，在世界模型内部通过强化学习训练策略。

Result: 在 Minecraft 中，该世界模型显著优于先前方法，能准确预测物体交互和游戏机制；Dreamer 4 首次仅从离线数据中完成‘获取钻石’任务，需从原始像素中选择超过 20,000 个操作序列。

Conclusion: Dreamer 4 提供了一种可扩展的想象训练范式，推动了基于世界模型的智能体发展，为脱离环境交互的离线强化学习提供了可行路径。

Abstract: World models learn general knowledge from videos and simulate experience for
training behaviors in imagination, offering a path towards intelligent agents.
However, previous world models have been unable to accurately predict object
interactions in complex environments. We introduce Dreamer 4, a scalable agent
that learns to solve control tasks by reinforcement learning inside of a fast
and accurate world model. In the complex video game Minecraft, the world model
accurately predicts object interactions and game mechanics, outperforming
previous world models by a large margin. The world model achieves real-time
interactive inference on a single GPU through a shortcut forcing objective and
an efficient transformer architecture. Moreover, the world model learns general
action conditioning from only a small amount of data, allowing it to extract
the majority of its knowledge from diverse unlabeled videos. We propose the
challenge of obtaining diamonds in Minecraft from only offline data, aligning
with practical applications such as robotics where learning from environment
interaction can be unsafe and slow. This task requires choosing sequences of
over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors
in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft
purely from offline data, without environment interaction. Our work provides a
scalable recipe for imagination training, marking a step towards intelligent
agents.

</details>


### [882] [When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?](https://arxiv.org/abs/2509.24927)
*An Guo,Shuoxiao Zhang,Enyi Tang,Xinyu Gao,Haomin Pang,Haoxiang Tian,Yanzhou Mu,Wu Wen,Chunrong Fang,Zhenyu Chen*

Main category: cs.AI

TL;DR: 本文通过大规模实证研究，分析了车联网协同感知系统中的六种常见错误模式，并评估了其关键组件的性能，揭示了系统在通信干扰、融合方案和错误传播方面的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 由于V2X协同感知系统结构复杂，且在出现错误时其错误类型和成因尚不明确，因此需要系统性研究以识别主要问题并提升系统可靠性。

Method: 通过大规模实证研究，识别并分析六种常见的协同感知错误模式，并评估不同传感器配置、通信方式（V2I/V2V）和融合方案对感知性能的影响。

Result: 1) 基于LiDAR的协作配置表现最佳；2) V2I与V2V在不同融合方案下性能差异显著；3) 协同感知错误增多会导致驾驶违规频率上升；4) 在线运行时系统对通信干扰缺乏鲁棒性。

Conclusion: 研究揭示了V2X协同感知系统关键组件中的潜在风险与脆弱性，为未来系统设计与修复提供了重要依据。

Abstract: With the tremendous advancement of deep learning and communication
technology, Vehicle-to-Everything (V2X) cooperative perception has the
potential to address limitations in sensing distant objects and occlusion for a
single-agent perception system. V2X cooperative perception systems are software
systems characterized by diverse sensor types and cooperative agents, varying
fusion schemes, and operation under different communication conditions.
Therefore, their complex composition gives rise to numerous operational
challenges. Furthermore, when cooperative perception systems produce erroneous
predictions, the types of errors and their underlying causes remain
insufficiently explored. To bridge this gap, we take an initial step by
conducting an empirical study of V2X cooperative perception. To systematically
evaluate the impact of cooperative perception on the ego vehicle's perception
performance, we identify and analyze six prevalent error patterns in
cooperative perception systems. We further conduct a systematic evaluation of
the critical components of these systems through our large-scale study and
identify the following key findings: (1) The LiDAR-based cooperation
configuration exhibits the highest perception performance; (2)
Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication
exhibit distinct cooperative perception performance under different fusion
schemes; (3) Increased cooperative perception errors may result in a higher
frequency of driving violations; (4) Cooperative perception systems are not
robust against communication interference when running online. Our results
reveal potential risks and vulnerabilities in critical components of
cooperative perception systems. We hope that our findings can better promote
the design and repair of cooperative perception systems.

</details>


### [883] [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
*Sean Trott*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型（LLM）中机制性发现的可推广性问题，提出了五个可能的对应轴（功能、发展、位置、关系和构型），并通过Pythia模型的不同随机种子训练过程分析了“1-back注意力头”的发展轨迹，发现其在不同模型间具有高度一致性，尤其在较大模型中更早出现且更强。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的机制解释研究缺乏判断某一模型发现是否可推广到其他模型的原则，本文旨在解决这一认识论挑战，明确机制性结论在何种条件下及沿哪些维度可跨模型成立。

Method: 提出五个机制可推广性的对应轴，并通过分析多个Pythia模型（不同规模和随机种子）在预训练过程中1-back注意力头的发展轨迹进行实证验证。

Result: 1-back注意力头的发展轨迹在不同模型间表现出高度一致性，发展越早、增长越快、峰值越高，尤其在较大模型中更为显著；而位置一致性较弱。

Conclusion: 机制性发现的可推广性研究应致力于将LLM的设计属性与其涌现行为和机制联系起来，从而建立系统化的跨模型解释框架。

Abstract: Research on Large Language Models (LLMs) increasingly focuses on identifying
mechanistic explanations for their behaviors, yet the field lacks clear
principles for determining when (and how) findings from one model instance
generalize to another. This paper addresses a fundamental epistemological
challenge: given a mechanistic claim about a particular model, what justifies
extrapolating this finding to other LLMs -- and along which dimensions might
such generalizations hold? I propose five potential axes of correspondence
along which mechanistic claims might generalize, including: functional (whether
they satisfy the same functional criteria), developmental (whether they develop
at similar points during pretraining), positional (whether they occupy similar
absolute or relative positions), relational (whether they interact with other
model components in similar ways), and configurational (whether they correspond
to particular regions or structures in weight-space). To empirically validate
this framework, I analyze "1-back attention heads" (components attending to
previous tokens) across pretraining in random seeds of the Pythia models (14M,
70M, 160M, 410M). The results reveal striking consistency in the developmental
trajectories of 1-back attention across models, while positional consistency is
more limited. Moreover, seeds of larger models systematically show earlier
onsets, steeper slopes, and higher peaks of 1-back attention. I also address
possible objections to the arguments and proposals outlined here. Finally, I
conclude by arguing that progress on the generalizability of mechanistic
interpretability research will consist in mapping constitutive design
properties of LLMs to their emergent behaviors and mechanisms.

</details>


### [884] [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
*Louie Hong Yao,Nicholas Jarvis,Tiffany Zhan,Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.AI

TL;DR: JE-IRT是一个几何项目反应框架，将大语言模型和问题嵌入到共享空间中，通过方向和范数分别编码语义和难度，提供对模型能力与问题结构之间关系的可解释统一视角。


<details>
  <summary>Details</summary>
Motivation: 标准的大语言模型评估方法将多样的能力压缩为单一分数，掩盖了其多维本质，因此需要一种更细粒度、可解释的评估方式。

Method: 提出JE-IRT框架，将模型和问题共同嵌入几何空间，利用向量方向表示语义、范数表示难度，并通过几何交互判断回答正确性。

Result: 实验表明，分布外行为可通过方向对齐解释，范数越大问题越难；新模型可通过拟合单个嵌入加入空间，且学习到的空间揭示出与人类分类部分一致的模型内部分类体系。

Conclusion: JE-IRT提供了一种统一且可解释的几何视角，连接模型能力与问题结构，为模型评估和泛化提供了新方法。

Abstract: Standard LLM evaluation practices compress diverse abilities into single
scores, obscuring their inherently multidimensional nature. We present JE-IRT,
a geometric item-response framework that embeds both LLMs and questions in a
shared space. For question embeddings, the direction encodes semantics and the
norm encodes difficulty, while correctness on each question is determined by
the geometric interaction between the model and question embeddings. This
geometry replaces a global ranking of LLMs with topical specialization and
enables smooth variation across related questions. Building on this framework,
our experimental results reveal that out-of-distribution behavior can be
explained through directional alignment, and that larger norms consistently
indicate harder questions. Moreover, JE-IRT naturally supports generalization:
once the space is learned, new LLMs are added by fitting a single embedding.
The learned space further reveals an LLM-internal taxonomy that only partially
aligns with human-defined subject categories. JE-IRT thus establishes a unified
and interpretable geometric lens that connects LLM abilities with the structure
of questions, offering a distinctive perspective on model evaluation and
generalization.

</details>


### [885] [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
*Yu Wu,Shuo Wu,Ye Tao,Yansong Li,Anand D. Sarwate*

Main category: cs.AI

TL;DR: Inter-Cascade是一种在线交互式LLM级联框架，通过强模型作为长期教师向弱模型传递可复用的解题策略，实现上下文中的知识迁移，在提升弱模型性能的同时显著减少对昂贵模型的调用和成本。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM Cascade是非自适应的，会因重复查询反复调用昂贵模型，导致成本高且效率低，因此需要一种能动态提升弱模型能力并降低调用开销的方法。

Method: 提出Inter-Cascade，强模型在解决难题后将其解法提炼为通用、可复用的策略，并附加到后续查询中以增强弱模型的能力，实现无需微调的动态性能提升。

Result: 在多个基准上，Inter-Cascade显著提升了弱模型准确率（最高+33.06个百分点）和系统整体性能（最高+5.53个百分点），同时减少强模型调用达48.05%，节省费用达49.63%。

Conclusion: Inter-Cascade实现了高效的LLM间上下文知识转移，提供了一个通用、可扩展的框架，适用于开源和API型大模型，有效平衡了性能与成本。

Abstract: Large Language Models (LLMs) vary widely in their capabilities, with larger
models often having better performance but higher cost: choosing an LLM model
often involves trading off performance and cost. The LLM Cascade is a paradigm
that defers difficult queries from weak/cheap to strong/expensive models. This
approach is nonadaptive: the deferral decision is trained offline. When
confronted with similar or repeated queries, the LLM Cascade may then
repeatedly consult the expensive model and incur higher cost. To improve the
cascading efficiency, we propose Inter-Cascade, an online and interactive LLM
Cascade that extends the role of strong model from a backup helper to a
long-term teacher. In our system, when a strong model resolves a difficult
query, it also distills its solution into a generalized, reusable
problem-solving strategy that boosts the weak model on subsequent queries.
Adding strategies to queries enables the weak model to dynamically improve its
performance over time, avoiding computationally and time-intensive fine-tuning.
Empirically, compared with standard LLM Cascade baselines across multiple
benchmarks, the Inter-Cascade significantly improves the accuracy of the weak
model (by up to 33.06 absolute percentage points) and the overall system (by up
to 5.53 absolute percentage points), while reducing the calls to strong models
(by up to 48.05% relative reduction) and saving the corresponding fees (by up
to 49.63% relative reduction). Inter-Cascade demonstrates the effective
in-context knowledge transfer between LLMs, and provides a general, scalable
framework applicable to both open-source and API-based LLMs.

</details>


### [886] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: 本文提出了一种结合Agentless训练和SWE-Agent框架的方法，通过结构化技能先验提升代码生成代理的性能，在SWE-bench上取得了与Claude 3.5 Sonnet相当的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的SWE领域大模型方法分为多轮交互的SWE-Agent和单步验证的Agentless流程，二者被视为互斥范式。作者认为Agentless训练中蕴含的推理能力可为SWE-Agent提供有效的技能先验，从而实现两者的协同增效。

Method: 首先优化Agentless训练方案，构建开源SWE大模型Kimi-Dev；然后在其基础上使用5000条公开轨迹进行SFT适配，使其支持SWE-Agent框架。

Result: Kimi-Dev在SWE-bench Verified上达到60.4%的成绩（当时最佳workflow方法），经SFT后驱动SWE-Agent取得48.6% pass@1，性能与Claude 3.5 Sonnet（241022版）相当。

Conclusion: Agentless训练产生的结构化技能先验能够有效桥接workflow与agentic两类框架，提升编码代理的迁移能力和效率。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [887] [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)
*Fang Wu,Xu Huang,Weihao Xuan,Zhiwei Zhang,Yijia Xiao,Guancheng Wan,Xiaomin Li,Bing Hu,Peng Xia,Jure Leskovec,Yejin Choi*

Main category: cs.AI

TL;DR: 本文提出了Multiplayer Nash Preference Optimization (MNPO)，将大语言模型的对齐问题推广到多玩家Nash博弈框架，克服了传统奖励方法和两玩家Nash方法在建模复杂、非传递性人类偏好时的局限性，实验证明MNPO在多种场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Bradley-Terry假设的奖励方法难以捕捉真实世界中非传递性和异质性的偏好，而现有的Nash学习方法仅限于两玩家博弈，存在单一对手偏差，无法充分建模复杂的偏好结构。

Method: 将对齐问题建模为n人博弈，每个策略在与对手群体竞争的同时受到参考模型的正则化约束，并引入多玩家Nash均衡和扩展的对偶间隙来衡量逼近质量。

Result: MNPO在指令遵循基准上持续优于INPO、ONPO、EGPO等NLHF基线方法，尤其在异质标注者条件和混合策略评估中表现更优。

Conclusion: MNPO是一个原则性强且可扩展的框架，能有效处理复杂、非传递的人类偏好，推动了基于人类反馈的强化学习在多玩家博弈方向的发展。

Abstract: Reinforcement learning from human feedback (RLHF) has emerged as the standard
paradigm for aligning large language models (LLMs) with human preferences.
However, reward-based methods built on the Bradley-Terry assumption struggle to
capture the non-transitive and heterogeneous nature of real-world preferences.
To address this, recent studies have reframed alignment as a two-player Nash
game, giving rise to Nash learning from human feedback (NLHF). While this
perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong
theoretical and empirical guarantees, they remain fundamentally restricted to
two-player interactions, creating a single-opponent bias that fails to capture
the full complexity of realistic preference structures. In this work, we
introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework
that generalizes NLHF to the multiplayer regime. It formulates alignment as an
$n$-player game, where each policy competes against a population of opponents
while being regularized toward a reference model. Our framework establishes
well-defined Nash equilibria in multiplayer settings and extends the concept of
duality gap to quantify approximation quality. We demonstrate that MNPO
inherits the equilibrium guarantees of two-player methods while enabling richer
competitive dynamics and improved coverage of diverse preference structures.
Through comprehensive empirical evaluation, we show that MNPO consistently
outperforms existing NLHF baselines on instruction-following benchmarks,
achieving superior alignment quality under heterogeneous annotator conditions
and mixed-policy evaluation scenarios. Together, these results establish MNPO
as a principled and scalable framework for aligning LLMs with complex,
non-transitive human preferences. Code is available at
https://github.com/smiles724/MNPO.

</details>


### [888] [$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding](https://arxiv.org/abs/2509.23234)
*Runyan Tan,Shuang Wu,Phillip Howard*

Main category: cs.AI

TL;DR: 本文提出了一种名为$p$-less采样的新方法，该方法基于信息论动态设置解码过程中的截断阈值，无需超参数调节，且在不同温度下均能保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有的采样方法对超参数选择敏感，且在不同任务和温度配置下表现不稳定，因此需要一种更鲁棒、无需调参的采样策略。

Method: 提出$p$-less采样，利用整个词汇概率分布动态计算每一步的截断阈值，基于信息熵理论自适应调整采样范围，无需人工设定超参数。

Result: $p$-less在数学、逻辑推理和创意写作等多个任务中 consistently 优于现有采样方法，尤其在高温下文本质量下降更少，并具有更高的推理效率（更低的平均采样时间和更短的生成长度）。

Conclusion: $p$-less采样是一种高效、稳定、无需超参数的解码方法，在多种生成任务和高温度场景中均表现出优越性能。

Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often
depends upon the choice of a sampling-based decoding strategy to
probabilistically choose the next token at each generation step. While a
variety of such sampling methods have been proposed, their performance can be
sensitive to the selection of hyperparameters which may require different
settings depending upon the generation task and temperature configuration. In
this work, we introduce $p$-less sampling: an information-theoretic approach to
sampling which dynamically sets a truncation threshold at each decoding step
based on the entire token probability distribution. Unlike existing methods,
$p$-less sampling has no hyperparameters and consistently produces high-quality
outputs as temperature increases. We provide theoretical perspectives on
$p$-less sampling to ground our proposed method and conduct experiments to
empirically validate its effectiveness across a range of math, logical
reasoning, and creative writing tasks. Our results demonstrate how $p$-less
sampling consistently outperforms existing sampling approaches while exhibiting
much less degradation in text quality at higher temperature values. We further
show how $p$-less achieves greater inference-time efficiency than alternative
methods through lower average token sampling times and shorter generation
lengths, without sacrificing accuracy. Finally, we provide analyses to
highlight the benefits of $p$-less through qualitative examples, case studies,
and diversity assessments.

</details>


### [889] [Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning](https://arxiv.org/abs/2509.23292)
*Ningning Xu,Yuxuan Jiang,Shubhashis Roy Dipta*

Main category: cs.AI

TL;DR: 提出一种模式感知的工具集成推理方法，通过结合计算器模式和算法模式，提升大模型在复杂数学问题中的代码使用率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理研究多关注何时调用工具，而忽视了如何应用工具，导致即使推理正确也常因工具使用不当而失败。

Method: 构建一个两阶段框架：第一阶段从两种常见模式（计算器模式和算法模式）中建立代码能力；第二阶段根据教师偏好对齐模式选择。

Result: 在多个挑战性数学数据集上，该方法显著提升了代码生成准确率，例如在MATH500上Code@1从64.0%提升至70.5%，在AIME24上从26.7%提升至50.0%。

Conclusion: 模式感知的方法能有效提升工具集成推理的性能，合理选择工具应用模式对提高模型表现至关重要。

Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large
reasoning models (LRMs) on complex problems. Prior work has mainly studied when
to invoke tools, while overlooking how tools are applied. We identify two
common patterns: a calculator pattern that uses code for direct computation,
and an algorithmic pattern that encodes problems as programs. Misaligned
choices often cause failures even when reasoning is sound. We propose a
two-stage framework that first builds code competence from both patterns and
then aligns pattern selection with teacher preferences. Across challenging math
datasets, our pattern-aware method substantially improves both code usage and
accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on
AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a
pattern-aware approach for tool-integrated reasoning.

</details>


### [890] [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
*Jinyi Han,Ying Huang,Ying Liao,Zishang Jiang,Xikun Lu,Haiquan Zhao,Xinyi Wang,Guanghao Zhou,Sihang Jiang,Jiaqing Liang,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出了一种名为Just-Enough Thinking (JET)的方法，通过截断推理路径和质量控制的长度奖励，提升大推理模型的推理效率，同时保持甚至提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型虽然性能强大，但深层推理计算成本高，现有强化学习方法难以在 rollout 阶段构建短推理路径，导致学习效率低下。作者受证据积累模型启发，发现模型在早期已积累足够信息，后续推理步骤冗余。

Method: 提出JET方法，包含 rollout 阶段的轨迹截断机制，使模型接触更短且分布一致的推理路径，并设计质量控制的长度奖励函数，鼓励简洁且正确的推理。

Result: 实验表明，JET显著提升了推理效率，减少输出长度达46.3%，同时准确率提升4.6%（如DeepSeek-Distill-Qwen-1.5B在Olympiad基准上）。

Conclusion: JET能有效训练模型主动终止不必要的推理，在保持或提升准确率的同时大幅提高推理效率，为高效推理提供了新思路。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on
challenging tasks, yet their deep reasoning often incurs substantial
computational costs. To achieve efficient reasoning, existing reinforcement
learning methods still struggle to construct short reasoning path during the
rollout stage, limiting effective learning. Inspired by Evidence Accumulation
Models, we find that LRMs have accumulated sufficient information early in
reasoning, making further reasoning steps redundant. Based on this insight, we
propose Just-Enough Thinking (JET), which trains models to proactively
terminate unnecessary reasoning. JET performs trajectory truncation during
rollout to expose the model to short, distributionally consistent reasoning
paths. Besides, it uses a quality-controlled length reward to better encourage
concise reasoning while maintaining correctness. Extensive experiments
demonstrate that JET significantly improves reasoning efficiency without
sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%
accuracy gain while reducing output length by 46.3% on the Olympiad benchmark.
Our code is available in the GitHub.

</details>


### [891] [Mapping Overlaps in Benchmarks through Perplexity in the Wild](https://arxiv.org/abs/2509.23488)
*Siyang Wu,Honglin Bao,Sida Li,Ari Holtzman,James A. Evans*

Main category: cs.AI

TL;DR: 提出了一种基于容量熟悉度的基准测试特征（benchmark signatures），通过大规模元评估分析88个基准和32个大语言模型，揭示了不同任务间的重叠与差异，发现逻辑、数学、语言等任务存在跨功能重叠，而编码领域最不重叠，且该方法能有效排除问题格式等非能力因素的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试之间的语义和性能相关性研究难以准确反映模型真实能力，且易受问题格式等非能力因素影响，缺乏对基准间深层关联和LLM能力结构的机制性理解。

Method: 定义基准特征为来自自然语料中对LLM预训练暴露程度敏感的显著词元集合，使用基于线性回归的逐步前向选择方法，在32个LLM和88个多样化基准上进行大规模元评估，提取并分析这些特征。

Result: 基准特征能有效捕捉基准间的变异、重叠与分歧；发现知识与推理子任务有重叠，多语言和文化类基准相似性较低；编码领域最独立；跨功能重叠存在于逻辑、数学、语言等任务；性能相关性受问题格式等正交因素强烈影响，但特征方法对此具有鲁棒性。

Conclusion: 基准特征提供了一种更稳健、更具解释性的工具来理解LLM能力结构和基准有效性，揭示了当前基准研究的局限性，并描绘了大语言模型能力之间的内在关联图景。

Abstract: We develop signatures of capacity familiarity to characterize large language
model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures
probe the capacity required for benchmark performance. We formally define them
as a set of salient tokens drawn from in-the-wild, naturally authored corpora,
where LLM token perplexity, reflecting more or less pre-training exposure,
becomes highly predictive of LLM benchmark performance. Through a large-scale
meta-evaluation, we extract benchmark signatures via stepwise forward selection
with linear regressions across 32 LLMs and 88 benchmarks spanning diverse
knowledge, coding, logic, instruction following, math, language, reasoning, and
world modeling. Our analysis situates signatures in relation to both the
semantic similarity of benchmark questions and the correlation of model
performance. While performance overlaps are universally high and semantic
overlaps remain confined to a narrow mid-range, benchmark signatures prove
highly informative in capturing variation, overlap, and divergence. We observe
overlap in knowledge and reasoning subtasks, whereas multilingual and cultural
benchmarks exhibit less similarity, even compared to cross-task overlap.
Notably, performance-level results are strongly influenced by
benchmark-orthogonal factors such as question format, highlighting limitations
in LLM generalization, the conflation of performance with ability, and issues
inherent in current mainstream benchmark agreement studies. Benchmark
signatures, however, remain robust to such effects. Ultimately, we identify
cross-functional overlaps across logic, math, language, instruction following,
and world modeling, with coding emerging as the least overlapping domain.
Together, these findings provide mechanistic insights into benchmark validity
and LLM sensitivities, and sketch the underlying landscape of interconnected
LLM capabilities.

</details>


### [892] [Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment](https://arxiv.org/abs/2509.23564)
*Min-Hsuan Yeh,Yixuan Li*

Main category: cs.AI

TL;DR: 本文提出了PrefCleanBench，首个用于评估13种偏好数据清洗方法在大语言模型对齐中效果的综合基准，旨在解决人类反馈噪声问题，提升对齐性能与泛化能力，并开源了所有方法的模块化实现。


<details>
  <summary>Details</summary>
Motivation: 人类反馈常存在噪声和不一致性，影响奖励模型质量，现有自动化数据清洗方法缺乏系统性评估，因此需要一个统一基准来衡量不同方法的有效性与泛化性。

Method: 构建了一个标准化评估协议PrefCleanBench，整合13种数据清洗方法，在多种数据集、模型架构和优化算法下统一评估其在对齐性能和泛化能力上的表现。

Result: 通过系统比较揭示了决定数据清洗成功的关键因素，验证了不同方法在提升模型对齐效果上的差异性和适用场景。

Conclusion: PrefCleanBench为基于高质量数据的LLM对齐提供了可复现、原则性的评估基础，强调了数据预处理在负责任AI发展中的关键作用。

Abstract: Human feedback plays a pivotal role in aligning large language models (LLMs)
with human preferences. However, such feedback is often noisy or inconsistent,
which can degrade the quality of reward models and hinder alignment. While
various automated data cleaning methods have been proposed to mitigate this
issue, a systematic evaluation of their effectiveness and generalizability
remains lacking. To bridge this gap, we introduce the first comprehensive
benchmark for evaluating 13 preference data cleaning methods in the context of
LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning
strategies in terms of alignment performance and generalizability across
diverse datasets, model architectures, and optimization algorithms. By unifying
disparate methods and rigorously comparing them, we uncover key factors that
determine the success of data cleaning in alignment tasks. This benchmark lays
the groundwork for principled and reproducible approaches to improving LLM
alignment through better data quality-highlighting the crucial but
underexplored role of data preprocessing in responsible AI development. We
release modular implementations of all methods to catalyze further research:
https://github.com/deeplearning-wisc/PrefCleanBench.

</details>


### [893] [From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models](https://arxiv.org/abs/2509.23676)
*Jue Zhang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本研究通过三阶段分析探究大型推理模型中推理过程与答案生成的关系，发现显式推理显著提升答案质量，且答案生成依赖于推理过程中的信息流。


<details>
  <summary>Details</summary>
Motivation: 明确大型推理模型中显式推理轨迹对答案生成的影响程度尚不清晰，需要系统研究其内在机制。

Method: 结合实证评估、注意力分析和基于激活补丁的机械干预，在三个蒸馏版DeepSeek R1模型上进行三阶段分析。

Result: 显式推理提升答案质量；答案token关注推理token，特定中层注意力头跟踪推理轨迹；扰动关键推理token可改变最终答案。

Conclusion: 推理过程与答案生成存在定向且功能性的信息流动，中间推理在塑造模型输出中具有功能性作用。

Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside
final answers, yet the extent to which these traces influence answer generation
remains unclear. In this work, we conduct a three-stage investigation into the
interplay between reasoning and answer generation in three distilled DeepSeek
R1 models. First, through empirical evaluation, we demonstrate that including
explicit reasoning consistently improves answer quality across diverse domains.
Second, attention analysis reveals that answer tokens attend substantially to
reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely
tracking the reasoning trajectory, including self-reflective cues. Third, we
apply mechanistic interventions using activation patching to assess the
dependence of answer tokens on reasoning activations. Our results show that
perturbations to key reasoning tokens can reliably alter the final answers,
confirming a directional and functional flow of information from reasoning to
answer. These findings deepen our understanding of how LRMs leverage reasoning
tokens for answer generation, highlighting the functional role of intermediate
reasoning in shaping model outputs. Our data and code are publicly available at
\href{https://aka.ms/R2A-code}{this URL}.

</details>


### [894] [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
*Jianshuo Dong,Sheng Guo,Hao Wang,Zhuotao Liu,Tianwei Zhang,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 本文提出了一种自动化红队测试框架SafeSearch，用于评估基于大语言模型的搜索代理在面对低质量搜索结果时的安全性，并构建了包含300个测试用例的基准，揭示了现有搜索代理存在严重漏洞且常见防御措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 不可靠的搜索结果可能对用户造成安全威胁，成为新的攻击面，因此需要系统评估搜索代理的安全性。

Method: 设计并实施了两个真实环境实验，开发了自动化的红队测试框架SafeSearch，并构建涵盖五类风险的300个测试案例，评估了多种搜索代理架构和15种大语言模型。

Result: 发现LLM-based搜索代理存在显著漏洞，在某些设置下最高攻击成功率达90.5%，且常见防御手段如提醒提示效果有限。

Conclusion: 所提出的框架有助于提升搜索代理系统的透明度与安全性，推动更安全的代理系统发展。

Abstract: Search agents connect LLMs to the Internet, enabling access to broader and
more up-to-date information. However, unreliable search results may also pose
safety threats to end users, establishing a new threat surface. In this work,
we conduct two in-the-wild experiments to demonstrate both the prevalence of
low-quality search results and their potential to misguide agent behaviors. To
counter this threat, we introduce an automated red-teaming framework that is
systematic, scalable, and cost-efficient, enabling lightweight and harmless
safety assessments of search agents. Building on this framework, we construct
the SafeSearch benchmark, which includes 300 test cases covering five
categories of risks (e.g., misinformation and indirect prompt injection). Using
this benchmark, we evaluate three representative search agent scaffolds,
covering search workflow, tool-calling, and deep research, across 7 proprietary
and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities
of LLM-based search agents: when exposed to unreliable websites, the highest
ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,
our analysis highlights the limited effectiveness of common defense practices,
such as reminder prompting. This emphasizes the value of our framework in
promoting transparency for safer agent development. Our codebase and test cases
are publicly available: https://github.com/jianshuod/SafeSearch.

</details>


### [895] [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
*Cheng Yang,Jiaxuan Lu,Haiyuan Wan,Junchi Yu,Feiwei Qin*

Main category: cs.AI

TL;DR: 提出ChemMAS，一个多智能体系统，通过基于证据的推理实现可解释的化学反应条件推荐，在准确性和可信度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推荐化学反应条件时缺乏对推荐结果的合理解释，限制了其在高风险科研流程中的应用。

Method: 将条件预测重构为基于证据的推理任务，采用多智能体系统进行机理锚定、多通道回忆、约束感知的智能体辩论和理由聚合。

Result: 实验显示ChemMAS在Top-1准确率上比领域专用基线高出20-35%，比通用大模型高出10-15%，并提供可验证、人类可信任的推理依据。

Conclusion: ChemMAS建立了科学发现中可解释AI的新范式，兼顾高性能与高可解释性。

Abstract: The chemical reaction recommendation is to select proper reaction condition
parameters for chemical reactions, which is pivotal to accelerating chemical
science. With the rapid development of large language models (LLMs), there is
growing interest in leveraging their reasoning and planning capabilities for
reaction condition recommendation. Despite their success, existing methods
rarely explain the rationale behind the recommended reaction conditions,
limiting their utility in high-stakes scientific workflows. In this work, we
propose ChemMAS, a multi-agent system that reframes condition prediction as an
evidence-based reasoning task. ChemMAS decomposes the task into mechanistic
grounding, multi-channel recall, constraint-aware agentic debate, and rationale
aggregation. Each decision is backed by interpretable justifications grounded
in chemical knowledge and retrieved precedents. Experiments show that ChemMAS
achieves 20-35% gains over domain-specific baselines and outperforms
general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,
human-trustable rationales, which establishes a new paradigm for explainable AI
in scientific discovery.

</details>


### [896] [Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models](https://arxiv.org/abs/2509.23962)
*Guanxu Chen,Yafu Li,Yuxian Jiang,Chen Qian,Qihan Ren,Jingyi Yang,Yu Cheng,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: 本文提出了Conditional advANtage estimatiON (CANON) 方法，用于在无需预设方向性偏好的情况下增强大语言模型的强化学习效果，尤其适用于可验证奖励的任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计的方向性先验（如熵或响应长度），可能导致过度偏差，需精细调参。作者希望提出一种更鲁棒、无需手动指定高低优劣方向的方法。

Method: CANON将采样响应按目标指标（如熵或长度）分为高低两组，通过组间比较判断哪个趋势提升性能，并在组内选择更优响应，从而放大指标影响而不假设其方向。

Result: 基于熵的CANON在三个大模型上均优于先前方法，在数学推理和复杂逻辑任务中表现更佳；应用于响应长度时，提升了令牌效率，优化了性能与成本的帕累托前沿。

Conclusion: CANON是一种无需方向先验、可灵活适配不同指标的强化学习优势估计方法，有效提升大语言模型在可验证任务中的推理能力与训练稳定性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for large language
models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning
capabilities on tasks with clear correctness criteria, such as mathematical
reasoning tasks. Several training metrics, such as entropy or response length,
have been observed to correlate with different reasoning behaviors in
reinforcement learning. Prior approaches incorporate such priors through reward
or advantage shaping, which often relies on hand-crafted penalties and
preferences (e.g., higher-is-better or lower-is-better). However, without
careful hyperparameter tuning, these directional priors can be overly biased
and may lead to failure. To this end, we introduce Conditional advANtage
estimatiON (CANON), amplifying the impact of the target metric without
presuming its direction. Specifically, CANON regroups the sampled responses
into two groups based on the higher or lower value of a target metric, measures
which metric trend contributes to better performance through inter-group
comparison, and identifies the better response within the same group. In
summary, CANON based on entropy consistently outperforms prior methods across
three LLMs on both math reasoning and high-complexity logic tasks. When applied
to response length, CANON further improves token efficiency, yielding a more
favorable Pareto frontier in the performance-cost trade-off.

</details>


### [897] [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
*Miguel Angel Alvarado Gonzalez,Michelle Bruno Hernandez,Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Sandra Malagon*

Main category: cs.AI

TL;DR: 单次运行的LLM排行榜不可靠，通过三次独立运行发现83%的配对排名会发生反转；建议至少进行两次重复实验以提高评估稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM排行榜通常依赖单次随机运行结果，但其可靠性存疑，需要探究多次重复运行对排名稳定性的影响。

Method: 在AI4Math基准上对八个先进模型进行三次独立运行，采用混合效应逻辑回归、领域边际均值、排名不稳定性分析和运行间可靠性评估重复次数的价值。

Result: 单次运行的排行榜具有脆性，12个切片中有10个出现至少一个配对排名反转；两次运行可消除约83%的单次运行排名反转，且标准误仅缩小约5%。

Conclusion: 建议将模型评估视为实验，报告不确定性，并在随机解码下至少使用两次重复，以提升评估的稳健性和现实可靠性。

Abstract: LLM leaderboards often rely on single stochastic runs, but how many
repetitions are required for reliable conclusions remains unclear. We
re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three
independent runs per setting. Using mixed-effects logistic regression,
domain-level marginal means, rank-instability analysis, and run-to-run
reliability, we assessed the value of additional repetitions. Our findings
shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at
least one pairwise rank relative to the three-run majority, despite a zero
sign-flip rate for pairwise significance and moderate overall interclass
correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to
three) but large ranking gains; two runs remove $\sim$83\% of single-run
inversions. We provide cost-aware guidance for practitioners: treat evaluation
as an experiment, report uncertainty, and use $\geq 2$ repetitions under
stochastic decoding. These practices improve robustness while remaining
feasible for small teams and help align model comparisons with real-world
reliability.

</details>


### [898] [Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models](https://arxiv.org/abs/2509.24156)
*Yuhui Wang,Changjiang Li,Guangke Chen,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 本文研究了大型推理模型（LRM）在链式思维（CoT）推理中存在推理过程与最终答案不一致的问题，提出其根源在于推理与记忆检索两种机制的冲突，并引入FARL框架通过记忆去学习与强化学习结合来抑制检索捷径，提升真正推理能力。


<details>
  <summary>Details</summary>
Motivation: 发现当前大型推理模型在生成答案时，其推理过程与最终答案存在矛盾，推测这是由于模型同时依赖链式推理和记忆检索两种机制所致，需验证并解决这一机制冲突。

Method: 通过设计控制实验，在推理阶段引入误导线索、在检索阶段引入错误答案，分析不同因素（问题领域、模型规模、微调方法）对两种机制主导性的影响，并提出FARL框架，在微调过程中结合记忆去学习与强化学习以抑制检索捷径。

Result: 实验证实推理与记忆检索机制共存且相互竞争，模型规模和微调方式显著影响机制主导性；FARL能有效抑制检索捷径，增强模型的推理主导行为和泛化推理能力。

Conclusion: 当前推理微调范式存在被记忆检索机制绕过的风险，FARL提供了一种有效路径，通过抑制检索捷径促进模型发展出更可靠、可泛化的推理能力。

Abstract: Large reasoning models (LRMs) exhibit unprecedented capabilities in solving
complex problems through Chain-of-Thought (CoT) reasoning. However, recent
studies reveal that their final answers often contradict their own reasoning
traces. We hypothesize that this inconsistency stems from two competing
mechanisms for generating answers: CoT reasoning and memory retrieval. To test
this hypothesis, we conduct controlled experiments that challenge LRMs with
misleading cues during reasoning and/or corrupted answers during retrieval. Our
results across models and datasets confirm that both mechanisms operate
simultaneously, with their relative dominance influenced by multiple factors:
problem domains, model scales, and fine-tuning approaches (e.g., reinforcement
learning vs. distillation). The findings reveal a critical limitation in
current reasoning fine-tuning paradigms: models can exploit the retrieval
mechanism as a shortcut, effectively "hacking" the reward signal and
undermining genuine reasoning development. To address this challenge, we
introduce FARL, a novel fine-tuning framework that integrates memory unlearning
with reinforcement learning. By carefully suppressing retrieval shortcuts
during the fine-tuning process, FARL promotes reasoning-dominant behavior and
enhances generalizable reasoning capabilities.

</details>


### [899] [Learning to Ponder: Adaptive Reasoning in Latent Space](https://arxiv.org/abs/2509.24238)
*Yixin He,Lumingyuan Tang*

Main category: cs.AI

TL;DR: FR-Ponder是一种无需修改主干网络权重的单图框架，通过潜在控制实现推理计算量的实例自适应分配，在保持准确性的同时优化了计算资源的使用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法如Best-of-N和多数投票对所有输入采用相同的推理深度，导致简单问题浪费计算资源，复杂问题则可能推理不足。

Method: 提出FR-Ponder框架，利用一个少于100万参数的控制器观察隐藏状态，决定是否停止或通过添加预计算的潜在引导向量进行小步推理，并结合GRPO奖励机制调节推理深度。

Result: 在GSM8K和MATH500数据集上，FR-Ponder在更低FLOPs下实现了更高的准确率，优于早期退出基线方法，且计算分配与问题难度相关。

Conclusion: FR-Ponder有效实现了按需推理，提升了计算效率与模型性能的平衡，适用于资源受限场景下的复杂推理任务。

Abstract: Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,
yet prevailing approaches like Best-of-N and majority voting apply uniform
depth across inputs, wasting computation on simple queries while potentially
under-thinking complex ones. We present FR-Ponder, a single-graph,
backbone-training-free framework that allocates instance-adaptive reasoning
compute via latent steering. A less than 1M-param controller observes hidden
states and decides to halt or apply a small ponder step by adding a
pre-computed steering vector to frozen representations. Our method extracts the
latent steering vector associated with deeper reasoning outputs and direct IO
from LLM and re-applies it through a tunable scaling factor, allowing the model
to adapt its reasoning depth to the complexity of each input. To balance
performance and computational cost, we employ Group Relative Policy
Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth,
achieving task accuracy while mitigating overreasoning. Through curriculum
learning and careful reward engineering, FR-Ponder learns calibrated compute
allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder
improves the compute-accuracy frontier, delivering lower FLOPs with better
matched accuracy and comparing favorably to early-exit baselines, without
modifying backbone weights. Analyses visualize interpretable steering
directions and show learned compute allocation correlates with problem
difficulty.

</details>


### [900] [SpecExit: Accelerating Large Reasoning Model via Speculative Exit](https://arxiv.org/abs/2509.24248)
*Rubing Yang,Huajun Bai,Song Liu,Guanghua Yu,Runzhi Fan,Yanbin Dang,Jiejing Zhang,Kai Liu,Jianchen Zhu,Peng Chen*

Main category: cs.AI

TL;DR: 提出SpecExit框架，利用轻量级草稿模型直接从隐藏状态预测未来token和早退信号，有效减少大推理模型的过思考问题，在不损失精度的情况下显著缩短生成长度并提升端到端速度。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在过思考问题，导致输出过长和高延迟，现有早退机制因探测开销限制了性能提升。

Method: 受推测解码中隐藏状态使用的启发，设计SpecExit框架，通过轻量级草稿模型直接预测未来token和早退信号，避免探测开销。

Result: 相比推测解码基线，平均生成长度减少66%，端到端延迟实现2.5倍加速，且保持准确率。

Conclusion: SpecExit能有效利用隐藏状态提供早退信号，在不牺牲准确性的前提下大幅提升推理效率，具有广泛的应用潜力。

Abstract: Despite their strong performance on reasoning tasks, large reasoning models
(LRMs) often suffer from overthinking, producing unnecessarily long outputs and
incurring high end-to-end latency, a significant limitation to their real-world
deployment. To address overthinking, early-exit mechanisms have been proposed
to terminate reasoning before typical completion, showing that this approach
can effectively shorten generation length with minimal impact on accuracy.
However, their reliance on probing mechanisms introduces a detection overhead
that limits their end-to-end latency gains and compromises their
generalizability across diverse problems. Inspired by the use of hidden states
in speculative decoding, we propose SpecExit, a novel framework that predicts
both future tokens and an early-exit signal directly from a lightweight draft
model without probing overhead. Our method offers significant improvements,
reducing average generation length by 66\% and achieving a 2.5x speedup in
end-to-end latency compared to the speculative decoding baseline, without
compromising accuracy. Our method leverages the inherent signals from hidden
states to provide effective early-exit signals, suggesting broader use of
hidden states for efficient reasoning. Our code is available at
https://github.com/Tencent/AngelSlim.

</details>


### [901] [PAME-AI: Patient Messaging Creation and Optimization using Agentic AI](https://arxiv.org/abs/2509.24263)
*Junjie Luo,Yihong Guo,Anqi Liu,Ritu Agarwal,Gordon,Gao*

Main category: cs.AI

TL;DR: PAME-AI 是一种基于代理AI的患者消息生成与优化新方法，利用DIKW层次结构从数据中提取高绩效的消息设计策略，在大规模医疗沟通中实现了点击率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统移动消息设计受限于无法探索高维设计空间，难以优化患者参与度。

Method: 提出PAME-AI框架，基于DIKW层次结构构建多个专用计算代理，逐步将实验数据转化为可操作的消息设计策略，并通过两阶段实验验证效果。

Result: 在包含超过51万次患者接触的实验中，最佳生成消息的参与率达到68.76%，相比61.27%的基线提升了12.2%的相对点击率。

Conclusion: PAME-AI的代理架构支持并行处理、假设验证和持续学习，适用于大规模医疗沟通的持续优化。

Abstract: Messaging patients is a critical part of healthcare communication, helping to
improve things like medication adherence and healthy behaviors. However,
traditional mobile message design has significant limitations due to its
inability to explore the high-dimensional design space. We develop PAME-AI, a
novel approach for Patient Messaging Creation and Optimization using Agentic
AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI
offers a structured framework to move from raw data to actionable insights for
high-performance messaging design. PAME-AI is composed of a system of
specialized computational agents that progressively transform raw experimental
data into actionable message design strategies. We demonstrate our approach's
effectiveness through a two-stage experiment, comprising of 444,691 patient
encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated
message achieved 68.76% engagement compared to the 61.27% baseline,
representing a 12.2\% relative improvement in click-through rates. This agentic
architecture enables parallel processing, hypothesis validation, and continuous
learning, making it particularly suitable for large-scale healthcare
communication optimization.

</details>


### [902] [AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](https://arxiv.org/abs/2509.24269)
*Zihao Zhu,Xinyu Wu,Gehan Hu,Siwei Lyu,Ke Xu,Baoyuan Wu*

Main category: cs.AI

TL;DR: 提出AdvChain方法，通过对抗性思维链调优来增强大推理模型的自我纠正能力，解决安全问题中的“雪球效应”。


<details>
  <summary>Details</summary>
Motivation: 现有安全思维链调优方法存在“雪球效应”，即微小的推理偏差会逐步放大，导致有害响应或过度拒绝，缺乏自我纠正机制。

Method: 构建包含诱惑-纠正和犹豫-纠正样本的数据集，采用对抗性思维链调优训练模型，使其学会从有害推理偏移和不必要的谨慎中恢复。

Result: 实验表明，AdvChain显著提升了对越狱攻击和思维链劫持的鲁棒性，大幅减少了对良性提示的过度拒绝，在不牺牲推理能力的前提下实现了更好的安全性与效用平衡。

Conclusion: AdvChain为构建更稳健、可靠的大推理模型提供了新方向，强调动态自我纠正在安全对齐中的重要性。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the
multi-step nature of CoT introduces new safety challenges that extend beyond
conventional language model alignment. We identify a failure mode in current
safety CoT tuning methods: the \textit{snowball effect}, where minor reasoning
deviations progressively amplify throughout the thought process, leading to
either harmful compliance or excessive refusal. This effect stems from models
being trained to imitate perfect reasoning scripts without learning to
self-correct. To address this limitation, we propose AdvChain, an alignment
paradigm that teaches models dynamic self-correction through adversarial CoT
tuning. Our method involves constructing a dataset containing
Temptation-Correction and Hesitation-Correction samples, where models learn to
recover from harmful reasoning drifts and unnecessary cautions. Extensive
experiments show that AdvChain significantly enhances robustness against
jailbreak attacks and CoT hijacking while substantially reducing over-refusal
on benign prompts, achieving a superior safety-utility balance without
compromising reasoning capabilities. Our work establishes a new direction for
building more robust and reliable reasoning models.

</details>


### [903] [SCI-Verifier: Scientific Verifier with Thinking](https://arxiv.org/abs/2509.24285)
*Shenghe Zheng,Chenyu Huang,Fangchen Yu,Junchi Yao,Jingqi Ye,Tao Chen,Yun Luo,Ning Ding,LEI BAI,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: 本文提出了SCI-VerifyBench和SCI-Verifier，分别是一个跨学科科学验证基准和一个增强推理的统一验证模型，旨在解决现有科学领域答案验证中存在的评估标准不系统、依赖繁琐规则设计等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的科学推理答案验证研究缺乏系统的评估标准和足够的学科覆盖，且过度依赖手工规则或提示工程，难以应对复杂推理场景和跨学科泛化需求。

Method: 在数据层面构建了涵盖数学、物理、生物、化学等领域的SCI-VerifyBench基准，基于真实LLM响应并引入领域特定的等价变换；在模型层面提出SCI-Verifier，通过后训练增强其逻辑推理与等价判断能力。

Result: SCI-VerifyBench提供了高质量、多样化的验证数据，支持严格的验证能力评估；SCI-Verifier展现出强大的推理与等价判断能力，输出简洁稳定，具备良好的跨学科适用性。

Conclusion: SCI-VerifyBench与SCI-Verifier共同构成了科学答案验证的系统化框架，为提升大语言模型在科学领域的可靠性与应用性提供了有效路径。

Abstract: As large language models (LLMs) are increasingly applied to scientific
reasoning, the complexity of answer formats and the diversity of equivalent
expressions make answer verification a critical yet challenging task. Existing
verification studies in scientific domains suffer from two major limitations:
(a) the absence of systematic evaluation standards and insufficient
disciplinary coverage, which hinders their comprehensive assessment; and (b)
heavy reliance on cumbersome rule design or prompt engineering, which reduces
their effectiveness in complex reasoning scenarios or limits their
cross-disciplinary generalization. To address these challenges, we propose
solutions at both the data and model levels. On the data side, we construct
SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics,
biology, chemistry, and general scientific QA. The benchmark is built from real
LLM responses and enhanced with domain-specific equivalence transformations
that generate challenging and realistic data. Model-based and expert
annotations ensure both quality and diversity, enabling rigorous evaluation of
verification ability. On the model side, we emphasize the importance of
reasoning for verification and introduce SCI-Verifier, a unified
reasoning-augmented verifier for scientific domains. Through post-training,
SCI-Verifier demonstrates strong logical reasoning and equivalence judgment
capabilities while maintaining concise and stable outputs. Together,
SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific
verification, offering both systematic evaluation and practical pathways to
enhance the reliability and applicability of LLMs in scientific domains.

</details>


### [904] [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/abs/2509.24393)
*Yichi Zhang,Yue Ding,Jingwen Yang,Tianwei Luo,Dongbai Li,Ranjie Duan,Qiang Liu,Hang Su,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种新的对齐方法IPO，通过干预偏好优化来增强大推理模型在推理过程中的安全性，有效减少有害内容的生成，同时保持良好的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大推理模型虽然在复杂问题求解上取得进展，但其思维链中常含有潜在有害内容，现有方法忽视了推理过程本身的安全性，存在安全隐患。

Method: 提出Intervened Preference Optimization (IPO)，通过识别安全触发步骤和合规线索，用安全步骤替换易导致不安全延续的步骤，并构建强信号的偏好学习对进行训练。

Result: 在越狱和对抗性安全基准测试中，IPO相比SFT和RL基线方法在推理和响应的安全性上均有显著提升，有害性相对降低超过30%，且在多种推理任务中保持优异性能。

Conclusion: 显式对齐推理过程的安全性至关重要，IPO为构建更安全的大推理模型提供了有效且实用的路径。

Abstract: Although Large Reasoning Models (LRMs) have progressed in solving complex
problems, their chain-of-thought (CoT) reasoning often contains harmful content
that can persist even when the final responses appear safe. We show that this
issue still remains in existing methods which overlook the unique significance
of safe reasoning, undermining their trustworthiness and posing potential risks
in applications if unsafe reasoning is accessible for and exploited by
malicious users. We therefore shift our focus to aligning the safety of
reasoning itself in this paper and explore process supervision as the solution.
However, simply rewarding safe reasoning proves inadequate due to low rollout
diversity and limited training signals. To tackle this challenge, we first
delve into the characteristics of safe reasoning and uncover several critical
insights that 1) safe reasoning is often consolidated by a few critical steps
of safety triggers; 2) compliance cues strongly correlate with unsafe
continuations; and 3) corrective interventions reliably steer unsafe
trajectories towards safer traces. Motivated by these, we propose Intervened
Preference Optimization (IPO), an alignment method that enforces safe reasoning
by substituting compliance steps with safety triggers and constructing pairs
for preference learning with strong signals. Experiments on jailbreak and
adversarial safety benchmarks demonstrate that IPO remarkably improves overall
safety regarding both reasoning and responses, outperforming SFT-based and
RL-based baselines with a relative reduction of over 30% in harmfulness, while
preserving excellent performance across diverse reasoning tasks. The results
highlight the importance of explicit alignment for reasoning and provide a
practical path to safer LRMs.

</details>


### [905] [Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design](https://arxiv.org/abs/2509.24509)
*Yihong Liu,Junyi Li,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 提出EvoPH框架，结合岛屿迁移模型和精英选择算法，协同进化提示与启发式算法，显著提升组合优化问题的自动算法设计性能。


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法设计依赖专家经验且易陷入局部最优，现有LLM驱动方法缺乏多样性导致优化停滞。

Method: 引入经验引导的反射性协同进化机制，将提示与启发式算法共同进化，并采用岛屿模型和精英选择维持种群多样性。

Result: 在旅行商和装箱问题上，EvoPH相比基线方法取得最低相对误差。

Conclusion: EvoPH有效提升了LLM在自动算法设计中的探索能力，推动了组合优化领域的发展。

Abstract: Combinatorial optimization problems are traditionally tackled with
handcrafted heuristic algorithms, which demand extensive domain expertise and
significant implementation effort. Recent progress has highlighted the
potential of automatic heuristics design powered by large language models
(LLMs), enabling the automatic generation and refinement of heuristics. These
approaches typically maintain a population of heuristics and employ LLMs as
mutation operators to evolve them across generations. While effective, such
methods often risk stagnating in local optima. To address this issue, we
propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics
(EvoPH) for automatic algorithm design, a novel framework that integrates the
island migration model with the elites selection algorithm to simulate diverse
heuristics populations. In EvoPH, prompts are co-evolved with heuristic
algorithms, guided by performance feedback. We evaluate our framework on two
problems, i.e., Traveling Salesman Problem and Bin Packing Problem.
Experimental results demonstrate that EvoPH achieves the lowest relative error
against optimal solutions across both datasets, advancing the field of
automatic algorithm design with LLMs.

</details>


### [906] [On the Self-awareness of Large Reasoning Models' Capability Boundaries](https://arxiv.org/abs/2509.24711)
*Qingjie Zhang,Yujia Fu,Yang Wang,Liu Yan,Tao Wei,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 本文研究了大型推理模型（LRM）在面对难题时缺乏对自身能力边界认知的问题，提出通过监测推理表达和隐藏状态来识别边界，并设计优化策略以避免无效推理，显著提升模型效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的推理范式忽视了问题与模型能力边界之间的关系，导致LRM在无法解决的问题上浪费大量计算资源。因此，需要探究LRM是否具备对自身能力边界的自我认知，并加以利用。

Method: 通过分析黑箱模型中的推理置信度变化轨迹，以及白箱模型中输入令牌隐藏状态的线性可分性，提取能力边界信号，并提出两种监控策略：推理表达监控和隐藏状态监控。

Result: 实验表明，所提出的边界感知策略能在不牺牲准确率的前提下，减少62.7%到93.6%的token使用量，有效避免无谓的长链推理。

Conclusion: LRM在推理前或推理过程中已蕴含对其能力边界的隐含认知，利用这一信号可实现高效、可靠的推理终止机制。

Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex
reasoning tasks such as mathematics, yet they also display misbehaviors that
expose their limitations. In particular, when faced with hard questions, LRMs
often engage in unproductive reasoning until context limit, producing wrong
answers while wasting substantial computation. This phenomenon reflects a
fundamental issue: current answering paradigms overlook the relationship
between questions and LRMs' capability boundaries. In this paper, we
investigate whether LRMs possess self-awareness of capability boundaries. We
begin by an observation that LRMs may know what they cannot solve through
expressed reasoning confidence. For black-box models, we find that reasoning
expressions reveal boundary signals, with accelerated growing confidence
trajectory for solvable problems but convergent uncertainty trajectory for
unsolvable ones. For white-box models, we show that hidden states of the last
input token encode boundary information, with solvable and unsolvable problems
linearly separable even before reasoning begins. Building on these findings, we
propose two simple yet effective optimization strategies: reasoning expression
monitoring and hidden states monitoring. Experiments demonstrate that these
boundary-aware strategies enable LRMs to avoid unproductive reasoning without
sacrificing accuracy, significantly improving reliability and efficiency by
cutting token usage up to 62.7 - 93.6%.

</details>


### [907] [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
*Zhen Bi,Zhenlin Hu,Jinnan Yang,Mingyang Chen,Cheng Deng,Yida Xue,Zeyu Yang,Qing Shen,Zhenfang Liu,Kang Zhao,Ningyu Zhang,Jungang Lou*

Main category: cs.AI

TL;DR: 提出数据推理强度（DRI）度量方法，通过重构训练数据的逻辑结构提升大模型推理能力，强调数据推理复杂性比数据量更重要。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视训练样本内部的推理复杂性，导致数据的推理潜力未被充分利用，限制了大语言模型的逻辑推理能力。

Method: 引入数据推理强度（DRI）指标，量化样本的潜在逻辑复杂性，并提出再认知优化策略，重构现有数据以匹配模型的推理边界。

Result: 实验表明该方法显著优于以数据为中心的基线策略，在性能和泛化能力上均有提升，并在强化学习框架下验证有效。

Conclusion: 提升训练数据的逻辑推理复杂性而非单纯增加数据量，是释放大语言模型认知潜力的关键。

Abstract: Recent advances in large language models (LLMs) highlight the importance of
training data structure and quality in shaping reasoning behavior. However,
most existing approaches focus on transforming data formats while neglecting
the internal reasoning complexity of training samples, leaving the reasoning
potential of data under-explored and underutilized. In this work, we posit that
LLM logical reasoning performance is jointly constrained by the potential of
the training data and the cognitive capacity of the model. To make this
relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel
metric that quantifies the latent logical reasoning complexity of samples by
decomposing and aggregating their logical structures. This allows us to analyze
how well current LLMs utilize logical reasoning signals and identify
performance gaps relative to data potential. Based on this insight, we
introduce a re-cognizing optimization strategy that systematically enhances the
logical reasoning intensity of training data.Rather than increasing data
volume, our method re-optimizes existing samples to better align with the LLM's
logical reasoning boundary. Extensive experiments show that our approach
significantly improves performance and generalization over data-centric
strategies. We further validate our method under a reinforcement learning
framework. Our results indicate that prioritizing reasoning complexity in data
rather than sheer scale or superficial form is essential to realizing LLMs'
full cognitive potential.

</details>


### [908] [Neural network embeddings recover value dimensions from psychometric survey items on par with human data](https://arxiv.org/abs/2509.24906)
*Max Pellert,Clemens M. Lechner,Indira Sen,Markus Strohmaier*

Main category: cs.AI

TL;DR: 本研究提出了一种名为SQuID的新方法，利用大语言模型的神经网络嵌入有效恢复心理测量问卷中的潜在维度结构，无需领域微调即可捕捉负相关关系，解释了55%的人类判断数据方差，且在成本、可扩展性和灵活性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的心理测量方法依赖大量人工评分，成本高且难以扩展。研究旨在开发一种基于语义嵌入的自动化方法，以低成本、高效率地复现已有的心理结构。

Method: 提出SQuID（Survey and Questionnaire Item Embeddings Differentials）方法，使用大语言模型生成问卷项目的嵌入表示，并通过差分处理恢复潜在维度结构。比较多种嵌入模型在多个评估指标上的表现。

Result: SQuID方法成功恢复了PVQ-RR问卷中人类价值观的结构，解释了55%的维度间相似性方差；多维尺度分析显示与人类数据具有合理的因子一致性，且能捕捉负相关关系，无需领域微调。

Conclusion: 语义嵌入结合SQuID可有效复现传统心理测量结构，提供一种更具成本效益和可扩展性的替代方案，对心理测量学和社会科学研究具有重要意义。

Abstract: This study introduces "Survey and Questionnaire Item Embeddings
Differentials" (SQuID), a novel methodological approach that enables neural
network embeddings to effectively recover latent dimensions from psychometric
survey items. We demonstrate that embeddings derived from large language
models, when processed with SQuID, can recover the structure of human values
obtained from human rater judgments on the Revised Portrait Value Questionnaire
(PVQ-RR). Our experimental validation compares multiple embedding models across
a number of evaluation metrics. Unlike previous approaches, SQuID successfully
addresses the challenge of obtaining negative correlations between dimensions
without requiring domain-specific fine-tuning. Quantitative analysis reveals
that our embedding-based approach explains 55% of variance in
dimension-dimension similarities compared to human data. Multidimensional
scaling configurations from both types of data show fair factor congruence
coefficients and largely follow the underlying theory. These results
demonstrate that semantic embeddings can effectively replicate psychometric
structures previously established through extensive human surveys. The approach
offers substantial advantages in cost, scalability and flexibility while
maintaining comparable quality to traditional methods. Our findings have
significant implications for psychometrics and social science research,
providing a complementary methodology that could expand the scope of human
behavior and experience represented in measurement tools.

</details>


### [909] [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
*Huihao Jing,Wenbin Hu,Hongyu Luo,Jianhui Yang,Wei Fan,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出MASLegalBench，一个面向多智能体系统（MAS）的法律领域基准，基于GDPR场景，支持任务分解与角色分工，填补了现有评估方法的空白。


<details>
  <summary>Details</summary>
Motivation: 现有法律任务的LLM基准未充分考虑多智能体系统的优势（如任务分解、角色专业化），缺乏针对性评估方法，限制了MAS在法律领域的应用潜力。

Method: 设计MASLegalBench，采用演绎推理框架，以GDPR为应用场景，构建包含丰富背景知识和复杂推理过程的测试集；手动设计多种基于角色的多智能体系统，并结合多个先进大模型进行广泛实验。

Result: 实验揭示了当前模型和MAS架构在法律任务中的优势、局限性及改进方向，验证了MASLegalBench对评估和推动MAS在法律领域发展的有效性。

Conclusion: MASLegalBench为多智能体系统在法律领域的应用提供了有效的评估工具，有助于推动其在复杂法律推理任务中的发展。

Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large
Language Models (LLMs), show great potential in addressing complex tasks. In
this context, integrating MAS with legal tasks is a crucial step. While
previous studies have developed legal benchmarks for LLM agents, none are
specifically designed to consider the unique advantages of MAS, such as task
decomposition, agent specialization, and flexible training. In fact, the lack
of evaluation methods limits the potential of MAS in the legal domain. To
address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS
and designed with a deductive reasoning approach. Our benchmark uses GDPR as
the application scenario, encompassing extensive background knowledge and
covering complex reasoning processes that effectively reflect the intricacies
of real-world legal situations. Furthermore, we manually design various
role-based MAS and conduct extensive experiments using different
state-of-the-art LLMs. Our results highlight the strengths, limitations, and
potential areas for improvement of existing models and MAS architectures.

</details>


### [910] [From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones](https://arxiv.org/abs/2509.25123)
*Lifan Yuan,Weize Chen,Yuchen Zhang,Ganqu Cui,Hanbin Wang,Ziming You,Ning Ding,Zhiyuan Liu,Maosong Sun,Hao Peng*

Main category: cs.AI

TL;DR: 该论文研究了强化学习（RL）是否能赋予大语言模型（LLM）真正的新技能，发现RL能够通过组合已有技能使LLM获得新的、可泛化的技能，而不仅仅是重新加权已有策略。


<details>
  <summary>Details</summary>
Motivation: 探讨强化学习在LLM后训练中的作用，澄清其是激活已有能力还是带来真正新技能的争议。

Method: 构建一个合成框架，将技能定义为推断字符串变换函数输出的能力，通过实验测试LLM在RL前后对函数组合（如h(x)=g(f(x))）的学习与泛化能力，并对比next-token训练的效果。

Result: 实验表明，RL能使LLM学会训练中未见的函数组合，且该组合能力可泛化到更复杂的多层组合任务，并能在不同任务间迁移；定性分析显示RL改变了模型的推理行为，而next-token训练则无此效果。

Conclusion: RL能够促使LLM通过组合已有技能获得真正的新技能，支持先构建具备基础技能的基座模型，再用RL发展高级、可泛化能力的训练范式。

Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing
ones? This question lies at the core of ongoing debates about the role of RL in
LLM post-training. On one side, strong empirical results can be achieved with
RL even without preceding supervised finetuning; on the other, critics argue
that RL contributes little beyond reweighting existing reasoning strategies.
This work provides concrete evidence that LLMs can acquire genuinely new skills
during RL by composing existing ones, mirroring one of the central mechanisms
by which humans acquire new cognitive skills. To mitigate data contamination
and other confounding factors, and to allow precise control over task
complexity, we develop a synthetic framework for our investigation.
Specifically, we define a skill as the ability to infer the output of a string
transformation function f(x) given x. When an LLM has already learned f and g
prior to RL, our experiments reveal that RL enables it to learn unseen
compositions of them h(x)=g(f(x)). Further, this compositional ability
generalizes to more difficult problems such as compositions of >2 functions
unseen during RL training. Surprisingly, our experiments show that
compositional skill acquired on a source task transfers to a different target
task. This transfer happens even without compositional training on the target,
requiring only prior knowledge of the target's atomic skills. Our qualitative
analysis shows that RL fundamentally changes the reasoning behaviors of the
models. In contrast, next-token training with the same data yields none of
these findings. Our systematic experiments provide fresh insights into LLM
learning, suggesting the value of first building base models with basic skills,
then using RL to incentivize advanced, generalizable skills for complex
problems.

</details>


### [911] [The Era of Real-World Human Interaction: RL from User Conversations](https://arxiv.org/abs/2509.25137)
*Chuanyang Jin,Jing Xu,Bo Liu,Leitian Tao,Olga Golovneva,Tianmin Shu,Wenting Zhao,Xian Li,Jason Weston*

Main category: cs.AI

TL;DR: 本文提出了从人类交互中进行强化学习（RLHI）的新范式，通过用户引导的重写和基于用户的奖励两种方法，利用真实用户对话数据实现模型的个性化对齐和持续改进。


<details>
  <summary>Details</summary>
Motivation: 现有的对话模型依赖专家标注的反馈数据，难以实现持续学习和多维度对齐，因此需要一种能从自然人类交互中学习的新方法。

Method: 提出RLHI框架，包括两种方法：1）基于用户引导重写的RLHI，根据用户的自然语言后续回复修改模型输出；2）基于用户奖励的RLHI，利用结合用户长期交互历史（即 persona）的奖励模型进行学习，并通过人物设定条件偏好优化将长期用户特征与回合级偏好关联。

Result: 在WildChat对话数据上训练的两种RLHI变体在个性化和指令遵循方面优于强基线模型，且类似反馈还能提升推理基准上的表现。

Conclusion: 自然的人类交互可为个性化对齐提供可扩展且有效的监督信号，RLHI为未来模型的持续改进提供了可行路径。

Abstract: We posit that to achieve continual model improvement and multifaceted
alignment, future models must learn from natural human interaction. Current
conversational models are aligned using pre-annotated, expert-generated human
feedback. In this work, we introduce Reinforcement Learning from Human
Interaction (RLHI), a paradigm that learns directly from in-the-wild user
conversations. We develop two complementary methods: (1) RLHI with User-Guided
Rewrites, which revises unsatisfactory model outputs based on users'
natural-language follow-up responses, (2) RLHI with User-Based Rewards, which
learns via a reward model conditioned on knowledge of the user's long-term
interaction history (termed persona). Together, these methods link long-term
user personas to turn-level preferences via persona-conditioned preference
optimization. Trained on conversations derived from WildChat, both RLHI
variants outperform strong baselines in personalization and
instruction-following, and similar feedback enhances performance on reasoning
benchmarks. These results suggest organic human interaction offers scalable,
effective supervision for personalized alignment.

</details>


### [912] [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)
*Siru Ouyang,Jun Yan,I-Hung Hsu,Yanfei Chen,Ke Jiang,Zifeng Wang,Rujun Han,Long T. Le,Samira Daruki,Xiangru Tang,Vishy Tirumalashetty,George Lee,Mahsan Rofouei,Hangfei Lin,Jiawei Han,Chen-Yu Lee,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出ReasoningBank和MaTTS，通过从历史交互中提炼可泛化的推理策略并结合测试时扩展，实现智能体的持续自我进化。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理在持续任务中无法有效利用历史经验，导致重复错误。

Method: 设计ReasoningBank框架以存储和检索通用推理策略，并引入记忆感知的测试时扩展（MaTTS）来生成多样化经验以提升记忆质量。

Result: 在网页浏览和软件工程基准上，ReasoningBank优于仅存储原始轨迹或成功流程的方法，MaTTS进一步提升了性能。

Conclusion: 记忆驱动的经验扩展是一种新的扩展维度，能使代理在长期任务中不断自我增强并涌现出新行为。

Abstract: With the growing adoption of large language model agents in persistent
real-world roles, they naturally encounter continuous streams of tasks. A key
limitation, however, is their failure to learn from the accumulated interaction
history, forcing them to discard valuable insights and repeat past errors. We
propose ReasoningBank, a novel memory framework that distills generalizable
reasoning strategies from an agent's self-judged successful and failed
experiences. At test time, an agent retrieves relevant memories from
ReasoningBank to inform its interaction and then integrates new learnings back,
enabling it to become more capable over time. Building on this powerful
experience learner, we further introduce memory-aware test-time scaling
(MaTTS), which accelerates and diversifies this learning process by scaling up
the agent's interaction experience. By allocating more compute to each task,
the agent generates abundant, diverse experiences that provide rich contrastive
signals for synthesizing higher-quality memory. The better memory in turn
guides more effective scaling, establishing a powerful synergy between memory
and test-time scaling. Across web browsing and software engineering benchmarks,
ReasoningBank consistently outperforms existing memory mechanisms that store
raw trajectories or only successful task routines, improving both effectiveness
and efficiency; MaTTS further amplifies these gains. These findings establish
memory-driven experience scaling as a new scaling dimension, enabling agents to
self-evolve with emergent behaviors naturally arise.

</details>


### [913] [Hilbert: Recursively Building Formal Proofs with Informal Reasoning](https://arxiv.org/abs/2509.22819)
*Sumanth Varambally,Thomas Voice,Yanchao Sun,Zhifeng Chen,Rose Yu,Ke Ye*

Main category: cs.AI

TL;DR: Hilbert是一个结合非正式推理与形式化验证的智能体框架，通过整合四个组件（非正式LLM、专用证明LLM、形式验证器和语义定理检索器），在数学问题求解上显著超越现有方法，在miniF2F和PutnamBench基准上取得当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备一定的数学推理能力，但其解题过程常含错误且难以自动验证；而形式化证明系统虽可精确验证但求解能力有限。因此，亟需弥合非形式化推理与形式化证明之间的性能差距。

Method: 提出Hilbert框架，采用代理式架构协调四个模块：擅长数学推理的非正式LLM、专用于Lean 4战术的证明LLM、形式验证器和语义定理检索器；通过递归分解问题为子目标，并利用验证反馈修正错误证明。

Result: 在miniF2F基准上达到99.2%准确率，超过此前最佳公开方法6.6个百分点；在PutnamBench上解决462/660题（70.0%），优于SeedProver（50.4%）并相较最佳公开基线提升422%。

Conclusion: Hilbert有效融合了非形式化推理与形式化验证的优势，显著缩小了两者在数学问题求解上的性能差距，实现了当前最先进的形式化证明生成效果。

Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning
abilities, but their solutions frequently contain errors that cannot be
automatically verified. Formal theorem proving systems such as Lean 4 offer
automated verification with complete accuracy, motivating recent efforts to
build specialized prover LLMs that generate verifiable proofs in formal
languages. However, a significant gap remains: current prover LLMs solve
substantially fewer problems than general-purpose LLMs operating in natural
language. We introduce Hilbert, an agentic framework that bridges this gap by
combining the complementary strengths of informal reasoning and formal
verification. Our system orchestrates four components: an informal LLM that
excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4
tactics, a formal verifier, and a semantic theorem retriever. Given a problem
that the prover is unable to solve, Hilbert employs recursive decomposition to
split the problem into subgoals that it solves with the prover or reasoner LLM.
It leverages verifier feedback to refine incorrect proofs as necessary.
Experimental results demonstrate that Hilbert substantially outperforms
existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points
above the best publicly available method. Hilbert achieves the best known
result on PutnamBench. It solves 462/660 problems (70.0%), outperforming
proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement
over the best publicly available baseline. Thus, Hilbert effectively narrows
the gap between informal reasoning and formal proof generation.

</details>


### [914] [Risk Profiling and Modulation for LLMs](https://arxiv.org/abs/2509.23058)
*Yikai Wang,Xiaocheng Li,Guanting Chen*

Main category: cs.AI

TL;DR: 本研究提出了一种新的方法来引导、控制和调节大语言模型（LLM）的风险偏好，利用行为经济学和金融学工具，发现指令微调模型更符合标准效用理论，而预训练和RLHF对齐模型则偏离较大，且后训练能最有效稳定地调节风险偏好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不确定性决策任务中应用广泛，但其风险特征及提示和对齐方法对其影响尚不清楚，特别是后训练如何影响LLM的风险行为仍缺乏研究。

Method: 采用效用理论模型，比较预训练、指令微调和RLHF对齐的LLM，并评估提示工程、上下文学习和后训练等调节策略的效果。

Result: 指令微调模型表现出与标准效用理论一致的行为，而预训练和RLHF对齐模型偏离效用模型；后训练是最稳定有效的风险偏好调节方式。

Conclusion: 不同阶段和类型的LLM具有不同的风险特征，后训练能有效调节这些特征，为未来行为对齐和风险感知型LLM设计提供了基础。

Abstract: Large language models (LLMs) are increasingly used for decision-making tasks
under uncertainty; however, their risk profiles and how they are influenced by
prompting and alignment methods remain underexplored. Existing studies have
primarily examined personality prompting or multi-agent interactions, leaving
open the question of how post-training influences the risk behavior of LLMs. In
this work, we propose a new pipeline for eliciting, steering, and modulating
LLMs' risk profiles, drawing on tools from behavioral economics and finance.
Using utility-theoretic models, we compare pre-trained, instruction-tuned, and
RLHF-aligned LLMs, and find that while instruction-tuned models exhibit
behaviors consistent with some standard utility formulations, pre-trained and
RLHF-aligned models deviate more from any utility models fitted. We further
evaluate modulation strategies, including prompt engineering, in-context
learning, and post-training, and show that post-training provides the most
stable and effective modulation of risk preference. Our findings provide
insights into the risk profiles of different classes and stages of LLMs and
demonstrate how post-training modulates these profiles, laying the groundwork
for future research on behavioral alignment and risk-aware LLM design.

</details>


### [915] [MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning](https://arxiv.org/abs/2509.23143)
*Charles L. Wang*

Main category: cs.AI

TL;DR: 本文提出了MathBode，一种用于评估大语言模型数学推理能力的动态诊断方法，通过频率响应分析（增益和相位）揭示模型在参数变化下的推理动态特性，发现模型存在低通行为和相位滞后，并开源数据与代码。


<details>
  <summary>Details</summary>
Motivation: 传统的一次性准确率无法充分反映模型在数学推理中的动态行为和一致性，因此需要一种更精细、可解释的诊断工具来揭示模型在不同参数变化下的推理表现。

Method: 将每个参数化问题视为一个系统，对单一参数施加正弦驱动，拟合模型输出与精确解的一阶谐波响应，提取增益和相位作为Bode式指纹指标，应用于五类闭式问题家族。

Result: 在五类数学问题中，该方法揭示了模型普遍存在的低通特性和相位滞后现象；前沿模型在动态响应上优于中端模型；符号计算基线用于校准（增益≈1，相位≈0）。

Conclusion: MathBode提供了一种紧凑、可复现的协议，补充了标准基准，能够量化推理保真度与一致性，有助于深入理解LLMs的数学推理动态。

Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning
in large language models (LLMs). Instead of one-shot accuracy, MathBode treats
each parametric problem as a system: we drive a single parameter sinusoidally
and fit first-harmonic responses of model outputs and exact solutions. This
yields interpretable, frequency-resolved metrics -- gain (amplitude tracking)
and phase (lag) -- that form Bode-style fingerprints. Across five closed-form
families (linear solve, ratio/saturation, compound interest, 2x2 linear
systems, similar triangles), the diagnostic surfaces systematic low-pass
behavior and growing phase lag that accuracy alone obscures. We compare several
models against a symbolic baseline that calibrates the instrument ($G \approx
1$, $\phi \approx 0$). Results separate frontier from mid-tier models on
dynamics, providing a compact, reproducible protocol that complements standard
benchmarks with actionable measurements of reasoning fidelity and consistency.
We open-source the dataset and code to enable further research and adoption.

</details>


### [916] [AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8](https://arxiv.org/abs/2509.23154)
*Jinzhe Pan,Jingqing Wang,Yuehui Ouyang,Wenchi Cheng,Wei Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体强化学习的分布式信道接入机制，通过动态退避选择、公平性量化指标和集中训练-分散执行架构，在保证与传统Wi-Fi设备兼容的同时，显著降低碰撞概率并提升信道访问公平性。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi系统依赖二进制指数退避（BEB），在密集部署下存在碰撞解决效率低和公平性差的问题，难以满足日益增长的无线设备数量和高可靠性应用需求。

Method: 提出一个融合AI优化与传统设备共存的多智能体强化学习框架：1）设计自适应实时信道条件的动态退避选择机制；2）引入符合EDCA原则的公平性量化指标；3）采用基于邻域活动模式的集中训练-分散执行（CTDE）架构，并通过约束多智能体近端策略优化（MAPPO）进行联合优化。

Result: 实验结果表明，该方案相比传统BEB显著降低了碰撞概率，有效消除了异构场景中的饥饿风险，同时保持了与商用Wi-Fi设备的向后兼容性。

Conclusion: 所提出的多智能体强化学习框架能够在不牺牲兼容性的前提下，有效提升密集网络中的信道接入效率与公平性，为未来智能无线接入提供了可行路径。

Abstract: The exponential growth of wireless devices and stringent reliability
requirements of emerging applications demand fundamental improvements in
distributed channel access mechanisms for unlicensed bands. Current Wi-Fi
systems, which rely on binary exponential backoff (BEB), suffer from suboptimal
collision resolution in dense deployments and persistent fairness challenges
due to inherent randomness. This paper introduces a multi-agent reinforcement
learning framework that integrates artificial intelligence (AI) optimization
with legacy device coexistence. We first develop a dynamic backoff selection
mechanism that adapts to real-time channel conditions through access deferral
events while maintaining full compatibility with conventional CSMA/CA
operations. Second, we introduce a fairness quantification metric aligned with
enhanced distributed channel access (EDCA) principles to ensure equitable
medium access opportunities. Finally, we propose a centralized training
decentralized execution (CTDE) architecture incorporating neighborhood activity
patterns as observational inputs, optimized via constrained multi-agent
proximal policy optimization (MAPPO) to jointly minimize collisions and
guarantee fairness. Experimental results demonstrate that our solution
significantly reduces collision probability compared to conventional BEB while
preserving backward compatibility with commercial Wi-Fi devices. The proposed
fairness metric effectively eliminates starvation risks in heterogeneous
scenarios.

</details>


### [917] [Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction](https://arxiv.org/abs/2509.23186)
*Qimin Zhong,Hao Liao,Siwei Wang,Mingyang Zhou,Xiaoqun Wu,Rui Mao,Wei Chen*

Main category: cs.AI

TL;DR: 本文研究了多标记预测（MTP）范式对大语言模型学习传递关系的影响，提出通过改进转移层（如下一标记注入和基于Transformer的结构）来增强模型在复杂规划任务中的路径规划能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多种任务中表现优异，但在学习传递关系方面仍存在困难，而这对于复杂规划至关重要。因此，需要探索提升模型捕捉传递关系能力的方法。

Method: 理论分析采用带有共享输出头和转移层的Transformer架构，研究MTP范式下传递关系的学习机制，并提出两种改进策略：下一标记注入（NTI）和基于Transformer的转移层。

Result: 在合成图和Blocksworld规划基准上的实验验证了理论分析，表明所提方法显著提升了模型的路径规划能力，转移层能逐步学习多步邻接信息，从而推断训练数据中未见的传递可达关系。

Conclusion: 本研究深化了对MTP范式下Transformer学习机制的理解，提供了克服传递性瓶颈的有效策略，推动了具备结构感知能力的通用规划模型的发展。

Abstract: Large Language Models (LLMs) have achieved impressive performance across
diverse tasks but continue to struggle with learning transitive relations, a
cornerstone for complex planning. To address this issue, we investigate the
Multi-Token Prediction (MTP) paradigm and its impact to transitive relation
learning. We theoretically analyze the MTP paradigm using a Transformer
architecture composed of a shared output head and a transfer layer. Our
analysis reveals that the transfer layer gradually learns the multi-step
adjacency information, which in turn enables the backbone model to capture
unobserved transitive reachability relations beyond those directly present in
the training data, albeit with some inevitable noise in adjacency estimation.
Building on this foundation, we propose two strategies to enhance the transfer
layer and overall learning quality: Next-Token Injection (NTI) and a
Transformer-based transfer layer. Our experiments on both synthetic graphs and
the Blocksworld planning benchmark validate our theoretical findings and
demonstrate that the improvements significantly enhance the model's
path-planning capability. These findings deepen our understanding of how
Transformers with MTP learn in complex planning tasks, and provide practical
strategies to overcome the transitivity bottleneck, paving the way toward
structurally aware and general-purpose planning models.

</details>


### [918] [Democratizing AI scientists using ToolUniverse](https://arxiv.org/abs/2509.23426)
*Shanghua Gao,Richard Zhu,Pengwei Sui,Zhenglun Kong,Sufian Aldogom,Yepeng Huang,Ayush Noori,Reza Shamji,Krishna Parvataneni,Theodoros Tsiligkaridis,Marinka Zitnik*

Main category: cs.AI

TL;DR: 本文提出了ToolUniverse，一个用于构建AI科学家的生态系统，支持跨语言和推理模型的工具集成，标准化工具调用方式，并实现自动接口优化、自然语言生成工具及智能工作流组合，在高胆固醇血症案例中成功应用于药物类似物发现。


<details>
  <summary>Details</summary>
Motivation: 现有的AI科学家系统定制化程度高、依赖固定工作流且缺乏统一的工具、数据和分析环境，限制了其开发与协作；受组学领域统一生态系统成功的启发，亟需构建类似的基础设施以促进AI科学家的发展。

Method: 设计并实现ToolUniverse生态系统，标准化AI科学家识别与调用工具的方式，集成超过600个机器学习模型、数据集、API和科学计算包，支持工具接口自动优化、从自然语言描述生成新工具、迭代优化工具规范，并将工具组合为自主工作流。

Result: 在高胆固醇血症的案例研究中，使用ToolUniverse构建的AI科学家成功识别出一种具有优良预测特性的强效药物类似物。

Conclusion: ToolUniverse提供了一个开放、通用且可扩展的生态系统，显著降低了构建AI科学家的门槛，推动了AI在科学研究中的协作与自动化。

Abstract: AI scientists are emerging computational systems that serve as collaborative
partners in discovery. These systems remain difficult to build because they are
bespoke, tied to rigid workflows, and lack shared environments that unify
tools, data, and analyses into a common ecosystem. In omics, unified ecosystems
have transformed research by enabling interoperability, reuse, and
community-driven development; AI scientists require comparable infrastructure.
We present ToolUniverse, an ecosystem for building AI scientists from any
language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes
how AI scientists identify and call tools, integrating more than 600 machine
learning models, datasets, APIs, and scientific packages for data analysis,
knowledge retrieval, and experimental design. It automatically refines tool
interfaces for correct use by AI scientists, creates new tools from natural
language descriptions, iteratively optimizes tool specifications, and composes
tools into agentic workflows. In a case study of hypercholesterolemia,
ToolUniverse was used to create an AI scientist to identify a potent analog of
a drug with favorable predicted properties. The open-source ToolUniverse is
available at https://aiscientist.tools.

</details>


### [919] [Dynamic Trust Calibration Using Contextual Bandits](https://arxiv.org/abs/2509.23497)
*Bruno M. Henrique,Eugene Santos Jr*

Main category: cs.AI

TL;DR: 提出了一种基于上下文的动态信任校准新方法，利用Contextual Bandits算法实现人与AI之间的有效信任评估，显著提升了决策性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化、客观的方法来衡量人与AI之间的信任校准，且现有方法无法区分意见形成与决策过程。

Method: 引入基于Contextual Bandits的自适应算法，构建标准化的信任校准指标，动态评估在何种情境下应信任AI。

Result: 在三个不同数据集上验证，信任校准确保了10%到38%的奖励指标提升，显著改善决策表现。

Conclusion: 该方法为构建可信赖AI系统提供了理论支持和实践指导，适用于疾病诊断、司法等关键领域。

Abstract: Trust calibration between humans and Artificial Intelligence (AI) is crucial
for optimal decision-making in collaborative settings. Excessive trust can lead
users to accept AI-generated outputs without question, overlooking critical
flaws, while insufficient trust may result in disregarding valuable insights
from AI systems, hindering performance. Despite its importance, there is
currently no definitive and objective method for measuring trust calibration
between humans and AI. Current approaches lack standardization and consistent
metrics that can be broadly applied across various contexts, and they don't
distinguish between the formation of opinions and subsequent human decisions.
In this work, we propose a novel and objective method for dynamic trust
calibration, introducing a standardized trust calibration measure and an
indicator. By utilizing Contextual Bandits-an adaptive algorithm that
incorporates context into decision-making-our indicator dynamically assesses
when to trust AI contributions based on learned contextual information. We
evaluate this indicator across three diverse datasets, demonstrating that
effective trust calibration results in significant improvements in
decision-making performance, as evidenced by 10 to 38% increase in reward
metrics. These findings not only enhance theoretical understanding but also
provide practical guidance for developing more trustworthy AI systems
supporting decisions in critical domains, for example, disease diagnoses and
criminal justice.

</details>


### [920] [BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2509.23589)
*Shu Liu,Wenlin Chen,Weihao Li,Zheng Wang,Lijin Yang,Jianing Huang,Yipin Zhang,Zhongzhan Huang,Ze Cheng,Hao Yang*

Main category: cs.AI

TL;DR: 提出了一种名为BridgeDrive的新型基于锚点引导的扩散桥策略，用于闭环轨迹规划，在Bench2Drive基准上实现了最先进的性能，成功率比之前的方法提高了5%。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在复杂动态驾驶场景中难以有效引导，简单条件控制效果有限，而现有锚点引导方法存在理论不一致性和性能损失问题。

Method: 引入BridgeDrive，一种基于扩散桥策略的新型锚点引导方法，构建了原则性的扩散框架，将典型专家驾驶行为（锚点）转化为细粒度轨迹计划，并兼容高效ODE求解器以支持实时部署。

Result: 在Bench2Drive基准测试中达到最先进水平，成功率较先前方法提升5%，且能有效应对不同交通状况。

Conclusion: BridgeDrive为自动驾驶中的闭环轨迹规划提供了一个有效且理论上一致的解决方案，兼具高性能与实际部署可行性。

Abstract: Diffusion-based planners have shown great promise for autonomous driving due
to their ability to capture multi-modal driving behaviors. However, guiding
these models effectively in reactive, closed-loop environments remains a
significant challenge. Simple conditioning often fails to provide sufficient
guidance in complex and dynamic driving scenarios. Recent work attempts to use
typical expert driving behaviors (i.e., anchors) to guide diffusion models but
relies on a truncated schedule, which introduces theoretical inconsistencies
and can compromise performance. To address this, we introduce BridgeDrive, a
novel anchor-guided diffusion bridge policy for closed-loop trajectory
planning. Our approach provides a principled diffusion framework that
effectively translates anchors into fine-grained trajectory plans,
appropriately responding to varying traffic conditions. Our planner is
compatible with efficient ODE solvers, a critical factor for real-time
autonomous driving deployment. We achieve state-of-the-art performance on the
Bench2Drive benchmark, improving the success rate by 5% over prior arts.

</details>


### [921] [How LLMs Learn to Reason: A Complex Network Perspective](https://arxiv.org/abs/2509.23629)
*Sihan Hu,Xiansheng Cai,Yuan Huang,Zhiyuan Yao,Linfeng Zhang,Pan Zhang,Youjin Deng,Kun Chen*

Main category: cs.AI

TL;DR: 本文提出了一种统一理论，解释了基于可验证奖励的强化学习（RLVR）训练大语言模型时出现的独特现象，如两阶段学习曲线和V形响应长度轨迹，并引入了Annealed-RLVR算法以改善模型推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中存在多个未被充分理解的现象，如两阶段学习、V形响应长度变化和灾难性遗忘，需要一个统一的理论来解释并改进训练方法。

Method: 作者提出语义复杂网络的自组织理论，认为模型推理过程形成稀疏拓扑结构（平均度接近2），导致先陷入最大挫败态（技能孤岛形成），再进入快速学习阶段；基于此，设计了包含SFT‘加热’步骤的Annealed-RLVR算法。

Result: 在15亿参数模型上的实验表明，Annealed-RLVR在分布内和分布外基准上均优于标准RLVR。

Conclusion: 将RLVR视为结构自组织过程而非黑箱优化，为理解和提升AI系统的涌现推理能力提供了新的物理直觉和工程路径。

Abstract: Training large language models with Reinforcement Learning from Verifiable
Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain
poorly understood, including a two-stage learning curve, V-shaped
response-length trajectories, and a pronounced vulnerability to catastrophic
forgetting. In this work, we propose that these seemingly disparate phenomena
can be explained using a single unifying theory: the model's reasoning process
maps to the self-organization of a semantic complex network whose topology
remains persistently sparse, with the average degree pinned close to two. This
topology imposes a fundamental mechanism for forgetting and learning: it first
drives the system into a maximally frustrated state where ``skill islands''
form, slow-learning happens, and forgetting is induced; then it enters a sharp
growth phase where the new skills are ``bolted on'', driven by
phase-transition-like learning at the web's frontier. Equipped with the theory,
we propose \textit{Annealed-RLVR}, a principled algorithm that introduces an
SFT-based ``heating'' step at the point of maximal frustration to resolve the
competitive bottleneck and enhance the reasoning capability of the model.
Experiments on a 1.5B-parameter model demonstrate that the approach outperforms
standard RLVR on both in-distribution and out-of-distribution benchmarks. By
recasting RLVR from black-box optimization into a predictable process of
structural self-organization, our work provides a new physical intuition for
engineering the emergent reasoning capabilities of future AI systems.

</details>


### [922] [Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning](https://arxiv.org/abs/2509.22746)
*Zejun Li,Yingxiu Zhao,Jiwen Zhang,Siyuan Wang,Yang Yao,Runzhou Zhao,Jun Song,Bo Zheng,Zhongyu Wei*

Main category: cs.AI

TL;DR: 提出了一种新的自适应视觉推理范式Mixture-of-Visual-Thoughts（MoVT），通过统一多种推理模式并实现上下文感知的模式选择，提升了模型的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法局限于特定推理模式，难以具备通用推理能力，因此需要一种能够根据上下文自适应选择推理模式的新范式。

Method: 提出了AdaVaR框架，包含两个阶段：监督冷启动阶段统一学习多种推理模式；通过设计AdaGRPO算法的强化学习阶段实现自适应模式选择。

Result: 实验表明AdaVaR能有效学习和区分多种推理模式，并实现上下文自适应的模式选择，在多种场景下均取得一致提升。

Conclusion: MoVT是一种有效的构建通用视觉推理模型的解决方案。

Abstract: Current visual reasoning methods mainly focus on exploring specific reasoning
modes. Although improvements can be achieved in particular domains, they
struggle to develop general reasoning capabilities. Inspired by this, we
propose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),
which unifies different reasoning modes within a single model and guides it to
select the appropriate mode based on context. To achieve this, we introduce
AdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different
modes are unified and learned during the supervised cold-start stage, and the
mode selection capability is induced via an RL process with a carefully
designed AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively
guides the model to learn and differentiate multiple modes and perform
context-adaptive mode selection, achieving consistent improvement across
various scenarios, highlighting MoVT as an effective solution for building
general visual reasoning models.

</details>


### [923] [Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs](https://arxiv.org/abs/2509.24107)
*Shreyas Singh,Kunal Singh,Pradeep Moturi*

Main category: cs.AI

TL;DR: 本文提出了Fathom-DeepResearch，一个由两个专用模型组成的智能体系统，用于复杂的信息检索任务。其中Fathom-Search-4B通过多代理自博弈数据集和改进的强化学习方法优化了基于网页搜索的推理能力，而Fathom-Synthesizer-4B则将搜索轨迹合成为结构化、引用丰富的研究报告，在多个基准测试中达到开源模型中的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 为了提升智能体在复杂、开放性信息检索任务中的表现，需要构建能够进行深度网络搜索并有效综合信息的系统。现有方法在搜索依赖性和结果合成方面存在不足，因此需要更强大的工具集成推理框架。

Method: 提出Fathom-DeepResearch系统，包含两个基于Qwen3-4B训练的模型：Fathom-Search-4B采用DUETQA数据集、RAPO强化学习算法和可调控的逐步骤奖励机制来优化多轮网页搜索；Fathom-Synthesizer-4B负责将多轮搜索过程转化为结构化的深度研究报告。

Result: 该系统在DeepSearch系列基准（如SimpleQA、FRAMES等）和DeepResearch-Bench上表现出色，达到开源权重类别中的最先进性能，并在HLE、AIME-25、GPQA-Diamond和MedQA等多样化推理任务中展现出良好的泛化能力。

Conclusion: Fathom-DeepResearch通过专门化的模型分工与创新的训练方法，显著提升了工具集成推理系统的搜索深度与信息综合能力，为复杂信息检索任务提供了高效且可扩展的开源解决方案。

Abstract: Tool-integrated reasoning has emerged as a key focus for enabling agentic
applications. Among these, DeepResearch Agents have gained significant
attention for their strong performance on complex, open-ended
information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system
composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch
model trained from Qwen3-4B and optimized for evidence-based investigation
through live web search and targeted webpage querying. Its training combines
three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent
self-play that enforces strict web-search dependence and heterogeneous source
grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes
multi-turn Reinforcement Learning with Verifiable Rewards through curriculum
pruning, reward-aware advantage scaling, and per-prompt replay buffers; and
(iii) a steerable step-level reward that classifies each tool call by cognitive
behavior and marginal utility, enabling explicit control over search trajectory
breadth, depth, and horizon. These improvements enable reliable extension of
tool-calling beyond 20 calls when warranted. The second is
Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn
DeepSearch traces into structured, citation-dense DeepResearch Reports for
comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,
WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves
state-of-the-art performance in the open-weights category while demonstrating
strong generalization to diverse reasoning tasks including HLE, AIME-25,
GPQA-Diamond, and MedQA.

</details>


### [924] [AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors](https://arxiv.org/abs/2509.23109)
*Junyang Zhang,Tianyi Zhu,Thierry Tambe*

Main category: cs.AI

TL;DR: 提出了一种名为Attention Anchor（AttAnchor）的无参数框架，通过跨模态语义分组提升视觉语言模型中的token局部性和跨模态对齐，显著减少幻觉并提高推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因直接拼接图像和文本token并使用模态盲的位置编码，导致跨模态语义相关token间不必要的长距离注意力，引发幻觉和性能下降，亟需增强token局部性和跨模态对齐机制。

Method: 提出Attention Anchor框架，在相关视觉patch附近插入文本token作为语义路标，动态揭示基于内容的跨模态注意力关系，实现文本与图像token的联合聚类分组，无需额外参数或对齐损失。

Result: 在15项指标和基准中13项取得提升，推理任务最高提升32%，幻觉基准最高降低15%；TinyLLaVA 1B模型超越LLaVA 7B和QwenVL 3B在POPE上的表现，推理开销仅增加0.1%。

Conclusion: Attention Anchor首次探索了跨模态token的联合分组机制，有效改善了视觉语言模型的注意力局部性与跨模态对齐，在几乎不增加计算成本的情况下显著提升了模型性能并减少了幻觉。

Abstract: A fundamental reason for the dominance of attention over RNNs and LSTMs in
LLMs is its ability to capture long-range dependencies by modeling direct
interactions between all tokens, overcoming the sequential limitations of
recurrent architectures. Similarly, a key reason why today's vision language
models (VLMs) hallucinate and underperform pure language models is that they
rely on direct concatenation of image and text tokens with a modality-blinded
positional encoding, which conveniently adopts the pretrained LLM backbone but
forces unnecessary long-distance attention between semantically related tokens
across modalities. This underscores the urgent need for mechanisms that
efficiently enhance token locality and cross-modal alignment. In response, we
propose Attention Anchor, a parameter-free framework that efficiently groups
semantically similar tokens across modalities, improving cross-modal locality.
By inserting text tokens near relevant visual patches, we create semantic
signposts that reveal true content-based cross-modal attention scores, guiding
the model to focus on the correct image regions for tasks such as VQA, MMBench
and POPE. This improves answer accuracy and reduces hallucinations without
disrupting the prompt's semantic flow. AttAnchor achieves improvements across
13 out of 15 different metrics and benchmarks, including up to 32% gains on
reasoning tasks and up to 15% improvements on hallucination benchmarks.
AttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B
and QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of
our knowledge, this work is among the first to investigate mixed-modal token
grouping, where text and image tokens are clustered jointly into shared groups
rather than being grouped within a single modality or merely aligned post-hoc
with additional alignment losses.

</details>


### [925] [Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned](https://arxiv.org/abs/2509.23250)
*Brandon Ong,Tej Deep Pala,Vernon Toh,William Chandra Tjhi,Soujanya Poria*

Main category: cs.AI

TL;DR: 本文提出了一种用于视觉语言模型的新型过程奖励模型（VL-PRM），通过混合数据合成、感知导向监督和测试时扩展策略，提升了多模态推理的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言过程奖励模型依赖蒙特卡洛树搜索构建数据，易产生噪声且泛化能力有限，本文旨在探索更优的VL-PRM设计空间。

Method: 提出结合MCTS与强VLM判断的混合数据合成框架、引入感知层面的监督机制，并系统评估多种测试时扩展策略。

Result: 在五个多模态基准上验证了方法的有效性，发现VL-PRM作为结果奖励模型表现更优，小模型可媲美大模型，且感知监督显著提升性能。

Conclusion: 所提方法显著提升视觉语言模型的推理可靠性，揭示了VL-PRM在多任务上的潜力，推动VLM的发展。

Abstract: Process Reward Models (PRMs) provide step-level supervision that improves the
reliability of reasoning in large language models. While PRMs have been
extensively studied in text-based domains, their extension to Vision Language
Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on
Monte Carlo Tree Search (MCTS) for data construction, which can often produce
noisy supervision signals and limit generalization across tasks. In this work,
we aim to elucidate the design space of VL-PRMs by exploring diverse strategies
for dataset construction, training, and test-time scaling. First, we introduce
a hybrid data synthesis framework that combines MCTS with judgments from a
strong VLM, producing more accurate step-level labels. Second, we propose
perception-focused supervision, enabling our PRM to explicitly detect errors at
the visual grounding stage of reasoning. Third, we systematically evaluate
multiple test-time scaling strategies, showing that our PRMs can reliably guide
VLMs toward more accurate solutions. Our experiments covering five diverse
multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and
MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome
Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM
guided process step selection, (ii) smaller VL-PRMs can match or even surpass
larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning
abilities in stronger VLM backbones, (iv) perception-level supervision leads to
significant gains in test-time scaling, and (v) TTS performance of different
policies improve on advanced math reasoning datasets despite not training
VL-PRMs on such datasets. We hope our work will motivate further research and
support the advancement of VLMs.

</details>


### [926] [Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations](https://arxiv.org/abs/2509.24250)
*Edward Kim,Daniel He,Jorge Chao,Wiktor Rajca,Mohammed Amin,Nishant Malpani,Ruta Desai,Antti Oulasvirta,Bjoern Hartmann,Sanjit Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种将协作任务学习框架化为程序合成问题的新方法，利用叙述性示范（动作与自然语言结合）作为统一的教学模态，使用户无需编写代码即可教授、检查和修正系统行为。


<details>
  <summary>Details</summary>
Motivation: 协作任务中用户需要推断队友意图，而这种推断具有模糊性和动态性，因此需要可解释且可修正的系统表示方式。现有方法难以支持用户有效理解和修改系统行为。

Method: 将系统行为表示为可编辑的程序，使用叙述性示范（即配对的物理动作与自然语言）作为教学、检查和修正系统逻辑的统一输入模态，并通过同一模态向用户反馈学习结果。

Result: 在20名用户的被试内研究中，70%（14人）成功将学习到的程序 refine 到符合其意图，90%（18人）认为修正程序很容易。研究揭示了以程序形式表示学习所面临的挑战以及教授协作性物理活动的困难。

Conclusion: 叙述性示范结合程序合成为协作性物理任务的教学提供了有效途径，具备良好的可解释性与可修正性，未来需进一步解决程序表示的可用性与协作意图建模的问题。

Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most
prior work has focused on non collaborative physical activities. Collaborative
tasks introduce added complexity, requiring systems to infer users assumptions
about their teammates intent, which is an inherently ambiguous and dynamic
process. This necessitates representations that are interpretable and
correctable, enabling users to inspect and refine system behavior. We address
this challenge by framing collaborative task learning as a program synthesis
problem. Our system represents behavior as editable programs and uses narrated
demonstrations, i.e. paired physical actions and natural language, as a unified
modality for teaching, inspecting, and correcting system logic without
requiring users to see or write code. The same modality is used for the system
to communicate its learning to users. In a within subjects study, 20 users
taught multiplayer soccer tactics to our system. 70 percent (14/20) of
participants successfully refined learned programs to match their intent and 90
percent (18/20) found it easy to correct the programs. The study surfaced
unique challenges in representing learning as programs and in enabling users to
teach collaborative physical activities. We discuss these issues and outline
mitigation strategies.

</details>


### [927] [Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models](https://arxiv.org/abs/2509.24261)
*Yuhua Jiang,Jiawei Huang,Yufeng Yuan,Xin Mao,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.AI

TL;DR: 提出了一种风险敏感的强化学习框架RS-GRPO，用于提升大语言模型在复杂推理任务中的多解性能（pass@k），同时保持或提高单解准确率（pass@1）。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法因预训练模型初始策略过于集中，导致探索不足，抑制了解法多样性，难以发现新的推理策略。

Method: 引入风险寻求目标函数，在平均奖励和最大奖励之间插值，设计了风险敏感的GRPO算法（RS-GRPO），通过加强对困难提示的学习来促进深度探索。

Result: 在六个数学推理基准和五种不同大语言模型上，RS-GRPO consistently 提升了pass@k性能，同时保持或提高了pass@1准确率。

Conclusion: RS-GRPO有效缓解了强化学习中的探索困境，不仅蒸馏已有能力，还能发现新推理策略，显著提升解法多样性和多解性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for enhancing Large Language Models (LLMs) on complex reasoning tasks. However,
existing methods suffer from an exploration dilemma: the sharply peaked initial
policies of pre-trained LLMs confine standard RL algorithms to a narrow set of
solutions, boosting single-solution accuracy (pass@1) but suppressing solution
diversity and multi-solution performance (pass@k). As a result, RLVR often
distills existing capabilities rather than discovering new reasoning
strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement
Learning framework. Our approach employs a risk-seeking objective that
interpolates between mean and maximum rewards, leading to a novel algorithm,
Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying
learning from challenging prompts. Remarkably, RS-GRPO is simple to implement,
requiring only minor code modifications. On six mathematical reasoning
benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k
performance while maintaining or enhancing pass@1 accuracy.

</details>


### [928] [Overcoming Over-Fitting in Constraint Acquisition via Query-Driven Interactive Refinement](https://arxiv.org/abs/2509.24489)
*Vasileios Balafas,Dimos Tsouros,Nikolaos Ploskas,Kostas Stergiou*

Main category: cs.AI

TL;DR: 提出了一种混合约束获取（CA）框架，通过结合被动学习、主动查询和概率置信度评分，有效缓解了数据有限情况下的过拟合问题，提升了模型准确性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 手动建模在约束编程中效率低下，现有被动CA方法易在数据不足时产生过拟合，而主动方法查询成本高，因此需要一种兼顾准确性与效率的混合方法。

Method: 框架结合被动学习生成候选约束，利用机器学习先验初始化的置信度评分指导交互式精炼过程，识别并剔除过拟合约束，并通过子集探索机制恢复有效子结构，最后通过主动学习确保模型完整性。

Result: 在多个基准测试上实验表明，该方法能在有限样本下显著提高目标模型覆盖率和整体准确性，同时保持可控的查询复杂度。

Conclusion: 所提出的混合CA框架在数据受限场景下实现了更鲁棒、实用的约束获取，是CA领域的重要进展。

Abstract: Manual modeling in Constraint Programming is a substantial bottleneck, which
Constraint Acquisition (CA) aims to automate. However, passive CA methods are
prone to over-fitting, often learning models that include spurious global
constraints when trained on limited data, while purely active methods can be
query-intensive. We introduce a hybrid CA framework specifically designed to
address the challenge of over-fitting in CA. Our approach integrates passive
learning for initial candidate generation, a query-driven interactive
refinement phase that utilizes probabilistic confidence scores (initialized by
machine learning priors) to systematically identify over-fitted constraints,
and a specialized subset exploration mechanism to recover valid substructures
from rejected candidates. A final active learning phase ensures model
completeness. Extensive experiments on diverse benchmarks demonstrate that our
interactive refinement phase is crucial for achieving high target model
coverage and overall model accuracy from limited examples, doing so with
manageable query complexity. This framework represents a substantial
advancement towards robust and practical constraint acquisition in data-limited
scenarios.

</details>


### [929] [Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting](https://arxiv.org/abs/2509.24495)
*Mateusz Żarski,Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 提出一种受神经可塑性启发的动态人工神经网络NMT-Net，用于多任务需求预测，通过训练时结构自适应提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法缺乏在训练过程中对网络结构的动态调整能力，限制了模型对新任务的适应性。

Method: 引入基于任务相似性的任务识别机制，动态选择并训练候选ANN头，并根据性能评估将其集成到模型中，实现计算图的结构可塑性。

Result: 在三个真实世界数据集上验证，NMT-Net相比传统基线和前沿方法具有更低的RMSE和标准差，表现出更优的性能与稳定性。

Conclusion: NMT-Net提供了一种可扩展、可适应的多任务及时序预测解决方案，具备持续学习潜力。

Abstract: This paper introduces a novel approach to Dynamic Artificial Neural Networks
(D-ANNs) for multi-task demand forecasting called Neuroplastic Multi-Task
Network (NMT-Net). Unlike conventional methods focusing on inference-time
dynamics or computational efficiency, our proposed method enables structural
adaptability of the computational graph during training, inspired by
neuroplasticity as seen in biological systems. Each new task triggers a dynamic
network adaptation, including similarity-based task identification and
selective training of candidate ANN heads, which are then assessed and
integrated into the model based on their performance. We evaluated our
framework using three real-world multi-task demand forecasting datasets from
Kaggle. We demonstrated its superior performance and consistency, achieving
lower RMSE and standard deviation compared to traditional baselines and
state-of-the-art multi-task learning methods. NMT-Net offers a scalable,
adaptable solution for multi-task and continual learning in time series
prediction. The complete code for NMT-Net is available from our GitHub
repository.

</details>


### [930] [Transparent Visual Reasoning via Object-Centric Agent Collaboration](https://arxiv.org/abs/2509.23757)
*Benjamin Teoh,Ben Glocker,Francesca Toni,Avinash Kori*

Main category: cs.AI

TL;DR: OCEAN是一种基于对象中心表示和多智能体协商的可解释AI框架，通过博弈论推理生成直观可信的视觉解释。


<details>
  <summary>Details</summary>
Motivation: 解决可解释AI中缺乏以人类可理解概念为基础的解释问题，尤其是在视觉领域。

Method: 提出OCEAN框架，采用对象中心表征和透明的多智能体博弈论推理过程，实现端到端训练。

Result: 在两个多目标数据集上表现与黑箱模型相当，且用户研究显示其解释更直观、更可信。

Conclusion: OCEAN通过结构化的推理过程实现了既准确又可解释的视觉决策，优于传统事后解释方法。

Abstract: A central challenge in explainable AI, particularly in the visual domain, is
producing explanations grounded in human-understandable concepts. To tackle
this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a
novel, inherently interpretable framework built on object-centric
representations and a transparent multi-agent reasoning process. The
game-theoretic reasoning process drives agents to agree on coherent and
discriminative evidence, resulting in a faithful and interpretable
decision-making process. We train OCEAN end-to-end and benchmark it against
standard visual classifiers and popular posthoc explanation tools like GradCAM
and LIME across two diagnostic multi-object datasets. Our results demonstrate
competitive performance with respect to state-of-the-art black-box models with
a faithful reasoning process, which was reflected by our user study, where
participants consistently rated OCEAN's explanations as more intuitive and
trustworthy.

</details>


### [931] [Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG](https://arxiv.org/abs/2509.24761)
*Yueming Sun,Long Yang*

Main category: cs.AI

TL;DR: 提出了一种基于图结构和对比学习的Transformer框架（SFTG），用于提升脑电图（EEG）视觉解码性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号具有高维、噪声大、非欧几里得特性，导致视觉神经表征解码困难，现有方法难以应对个体内部变异性和特征一致性问题。

Method: 提出EEG Graph Transformer（EGT）编码脑区空间连接与时间动态，并引入Graph Archetype Contrastive Learning（GAC）学习个体特异性的图原型以增强特征一致性和类别可分性。

Result: 在Things-EEG数据集上进行被试依赖和被试独立实验，结果显示所提方法显著优于先前最先进方法。

Conclusion: 结合图学习与对比学习目标能有效提升EEG解码的鲁棒性与泛化能力，为神经表征建模提供了新方向。

Abstract: Decoding visual neural representations from Electroencephalography (EEG)
signals remains a formidable challenge due to their high-dimensional, noisy,
and non-Euclidean nature. In this work, we propose a Spatial-Functional
Awareness Transformer-based Graph Archetype Contrastive Learning (SFTG)
framework to enhance EEG-based visual decoding. Specifically, we introduce the
EEG Graph Transformer (EGT), a novel graph-based neural architecture that
simultaneously encodes spatial brain connectivity and temporal neural dynamics.
To mitigate high intra-subject variability, we propose Graph Archetype
Contrastive Learning (GAC), which learns subject-specific EEG graph archetypes
to improve feature consistency and class separability. Furthermore, we conduct
comprehensive subject-dependent and subject-independent evaluations on the
Things-EEG dataset, demonstrating that our approach significantly outperforms
prior state-of-the-art EEG decoding methods.The results underscore the
transformative potential of integrating graph-based learning with contrastive
objectives to enhance EEG-based brain decoding, paving the way for more
generalizable and robust neural representations.

</details>


### [932] [Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning](https://arxiv.org/abs/2509.25052)
*Sai Wang,Yu Wu,Zhongwen Xu*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型的新型智能体架构CEL，通过交互与反思循环，从零开始自主学习环境规则并制定有效策略，在多种网格任务中实现了可解释的强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法依赖大量经验且知识隐式存储于网络权重中，缺乏透明性和泛化能力，因此需要一种能通过显式推理和规划进行学习的新范式。

Method: 提出Cogito, ergo ludo（CEL）架构，利用大语言模型在每次交互后进行规则归纳和策略总结，构建对环境动力学的显式语言模型，并形成可执行的战略手册。

Result: 在Minesweeper、Frozen Lake和Sokoban等网格世界任务中，CEL智能体能够从稀疏奖励中自主发现规则并掌握游戏，实现有效策略；消融实验表明迭代学习过程对持续学习至关重要。

Conclusion: CEL为构建更具通用性和可解释性的智能体提供了新路径，使其不仅能有效行动，还能通过对原始经验的显式推理不断构建和完善对世界的理解。

Abstract: The pursuit of artificial agents that can learn to master complex
environments has led to remarkable successes, yet prevailing deep reinforcement
learning methods often rely on immense experience, encoding their knowledge
opaquely within neural network weights. We propose a different paradigm, one in
which an agent learns to play by reasoning and planning. We introduce Cogito,
ergo ludo (CEL), a novel agent architecture that leverages a Large Language
Model (LLM) to build an explicit, language-based understanding of its
environment's mechanics and its own strategy. Starting from a tabula rasa state
with no prior knowledge (except action set), CEL operates on a cycle of
interaction and reflection. After each episode, the agent analyzes its complete
trajectory to perform two concurrent learning processes: Rule Induction, where
it refines its explicit model of the environment's dynamics, and Strategy and
Playbook Summarization, where it distills experiences into an actionable
strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,
Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent
successfully learns to master these games by autonomously discovering their
rules and developing effective policies from sparse rewards. Ablation studies
confirm that the iterative process is critical for sustained learning. Our work
demonstrates a path toward more general and interpretable agents that not only
act effectively but also build a transparent and improving model of their world
through explicit reasoning on raw experience.

</details>


### [933] [Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs](https://arxiv.org/abs/2509.25139)
*Yue Zhang,Tianyi Ma,Zun Wang,Yanyuan Qiao,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 提出一种基于文本类比推理的视觉-语言导航方法，通过多视角文本描述增强LLM对场景的理解和空间推理能力，在R2R数据集上显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的视觉-语言导航方法在处理图像时存在过度简化视觉细节或缺乏高层语义理解的问题，难以实现有效的上下文理解和类比推理。

Method: 引入来自多个视角的文本描述，利用文本驱动的类比推理机制，帮助代理更好地理解全局场景结构和空间关系，从而做出更准确的动作决策。

Result: 在R2R数据集上的实验表明，所提方法显著优于现有的零样本VLN方法，提升了导航成功率和路径质量。

Conclusion: 通过融合多视角文本描述和类比推理，可有效增强大语言模型在具身导航任务中的场景理解与推理能力。

Abstract: Integrating large language models (LLMs) into embodied AI models is becoming
increasingly prevalent. However, existing zero-shot LLM-based
Vision-and-Language Navigation (VLN) agents either encode images as textual
scene descriptions, potentially oversimplifying visual details, or process raw
image inputs, which can fail to capture abstract semantics required for
high-level reasoning. In this paper, we improve the navigation agent's
contextual understanding by incorporating textual descriptions from multiple
perspectives that facilitate analogical reasoning across images. By leveraging
text-based analogical reasoning, the agent enhances its global scene
understanding and spatial reasoning, leading to more accurate action decisions.
We evaluate our approach on the R2R dataset, where our experiments demonstrate
significant improvements in navigation performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [934] [Mobile Robot Localization via Indoor Positioning System and Odometry Fusion](https://arxiv.org/abs/2509.22693)
*Muhammad Hafil Nugraha,Fauzi Abdul,Lastiko Bramantyo,Estiko Rijanto,Roni Permana Saputra,Oka Mahendra*

Main category: cs.RO

TL;DR: 本文提出了一种融合超声波室内定位系统（IPS）与轮式里程计的移动机器人定位方法，通过扩展卡尔曼滤波（EKF）实现传感器数据融合，显著提高了定位精度和轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服单一定位方法（如IPS或轮式里程计）在室内环境中存在的局限性，如轮子打滑和传感器噪声，提升移动机器人的定位准确性。

Method: 采用扩展卡尔曼滤波（EKF）对超声波IPS和轮式里程计数据进行融合，结合两者优势以提高定位鲁棒性。

Result: 实验结果表明，融合系统相比单独使用IPS或里程计显著提升了定位精度和轨迹跟踪效果，有效减少了由打滑和噪声引起的误差。

Conclusion: 基于EKF的传感器融合方法为室内移动机器人提供了更可靠、精确的定位解决方案，适用于复杂动态环境中的自主导航。

Abstract: Accurate localization is crucial for effectively operating mobile robots in
indoor environments. This paper presents a comprehensive approach to mobile
robot localization by integrating an ultrasound-based indoor positioning system
(IPS) with wheel odometry data via sensor fusion techniques. The fusion
methodology leverages the strengths of both IPS and wheel odometry,
compensating for the individual limitations of each method. The Extended Kalman
Filter (EKF) fusion method combines the data from the IPS sensors and the
robot's wheel odometry, providing a robust and reliable localization solution.
Extensive experiments in a controlled indoor environment reveal that the
fusion-based localization system significantly enhances accuracy and precision
compared to standalone systems. The results demonstrate significant
improvements in trajectory tracking, with the EKF-based approach reducing
errors associated with wheel slippage and sensor noise.

</details>


### [935] [Nonlinear Model Predictive Control with Single-Shooting Method for Autonomous Personal Mobility Vehicle](https://arxiv.org/abs/2509.22694)
*Rakha Rahmadani Pratama,Catur Hilman A. H. B. Baskoro,Joga Dharma Setiawan,Dyah Kusuma Dewi,P Paryanto,Mochammad Ariyanto,Roni Permana Saputra*

Main category: cs.RO

TL;DR: 本文提出了一种基于非线性模型预测控制（NMPC）的自主个人交通工具SEATER的控制方法，采用单次射击法求解最优控制问题，并在Gazebo仿真环境中结合ROS框架进行验证，结果表明该方法能有效实现目标定位与避障，具备良好的实时性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了提升自主个人 mobility 车辆在复杂环境下的控制精度与安全性，需要一种能够处理非完整约束并满足多目标和障碍物规避的先进控制方法。

Method: 采用非线性模型预测控制（NMPC），结合单次射击法通过非线性规划求解最优控制问题，利用里程计数据作为反馈，应用于差动驱动的非完整车辆模型。

Result: 在无障碍和静态障碍环境下的一系列仿真结果显示，所提出的NMPC方法能够成功引导车辆到达目标位姿，同时满足系统约束和避障要求，且具有良好的实时性能。

Conclusion: NMPC结合单次射击法在SEATER车辆控制中表现出良好的有效性、鲁棒性和实时性，适用于自主车辆在受限环境中的精确运动控制。

Abstract: This paper introduces a proposed control method for autonomous personal
mobility vehicles, specifically the Single-passenger Electric Autonomous
Transporter (SEATER), using Nonlinear Model Predictive Control (NMPC). The
proposed method leverages a single-shooting approach to solve the optimal
control problem (OCP) via non-linear programming (NLP). The proposed NMPC is
implemented to a non-holonomic vehicle with a differential drive system, using
odometry data as localization feedback to guide the vehicle towards its target
pose while achieving objectives and adhering to constraints, such as obstacle
avoidance. To evaluate the performance of the proposed method, a number of
simulations have been conducted in both obstacle-free and static obstacle
environments. The SEATER model and testing environment have been developed in
the Gazebo Simulation and the NMPC are implemented within the Robot Operating
System (ROS) framework. The simulation results demonstrate that the NMPC-based
approach successfully controls the vehicle to reach the desired target location
while satisfying the imposed constraints. Furthermore, this study highlights
the robustness and real-time effectiveness of NMPC with a single-shooting
approach for autonomous vehicle control in the evaluated scenarios.

</details>


### [936] [ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows](https://arxiv.org/abs/2509.22695)
*Zhitao Wang,Yanke Wang,Jiangtao Wen,Roberto Horowitz,Yuxing Han*

Main category: cs.RO

TL;DR: 本文提出了ReSeFlow，一种结合SE(3)-不变性与修正流的快速生成策略学习方法，能在单步推理中实现更低误差和更优轨迹生成，显著提升数据与推理效率。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人操作需要鲁棒且长视野的策略生成，现有SE(3)-等变扩散模型虽数据高效但推理耗时，亟需提升推理效率。

Method: 受修正流推理高效的启发，将修正流引入SE(3)-扩散模型，构建了ReSeFlow模型，利用SE(3)-等变网络保持旋转和平移对称性，实现快速、测地线一致且计算量最小的策略生成。

Result: 在模拟基准上验证，ReSeFlow仅用一步推理即可超越基线方法在100步推理下的表现，在涂漆任务上误差降低48.5%，旋转三角任务上降低21.9%。

Conclusion: ReSeFlow融合了SE(3)等变性和修正流的优势，实现了高效的数据利用和快速推理，推动了生成式策略学习模型在真实场景中的应用。

Abstract: Robotic manipulation in unstructured environments requires the generation of
robust and long-horizon trajectory-level policy with conditions of perceptual
observations and benefits from the advantages of SE(3)-equivariant diffusion
models that are data-efficient. However, these models suffer from the inference
time costs. Inspired by the inference efficiency of rectified flows, we
introduce the rectification to the SE(3)-diffusion models and propose the
ReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing
fast, geodesic-consistent, least-computational policy generation. Crucially,
both components employ SE(3)-equivariant networks to preserve rotational and
translational symmetry, enabling robust generalization under rigid-body
motions. With the verification on the simulated benchmarks, we find that the
proposed ReSeFlow with only one inference step can achieve better performance
with lower geodesic distance than the baseline methods, achieving up to a 48.5%
error reduction on the painting task and a 21.9% reduction on the rotating
triangle task compared to the baseline's 100-step inference. This method takes
advantages of both SE(3) equivariance and rectified flow and puts it forward
for the real-world application of generative policy learning models with the
data and inference efficiency.

</details>


### [937] [Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments](https://arxiv.org/abs/2509.22698)
*Hailong Zhang,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.RO

TL;DR: 本文提出了MASTAVN，一种可扩展的多智能体音频-视觉导航框架，通过跨智能体通信和联合音视频融合机制，实现两个智能体在共享3D环境中协同定位并导航至音频目标，在真实感3D模拟器中验证了其在任务完成时间和成功率上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉导航研究主要集中在单智能体系统，难以应对动态3D环境中需要快速多智能体协作的复杂任务，尤其是在应急响应等时间敏感场景中。

Method: 提出MASTAVN框架，结合跨智能体通信协议和联合音视频融合机制，提升空间推理与时间同步能力，并在Replica和Matterport3D等真实感3D模拟器中进行评估。

Result: 相比单智能体和非协作基线方法，MASTAVN显著缩短了任务完成时间，提高了导航成功率，尤其在时间敏感的紧急场景中表现突出。

Conclusion: 多智能体间的时空协调对复杂3D环境中的协作任务至关重要，MASTAVN为可扩展的多智能体具身智能提供了有效范式。

Abstract: Intelligent agents often require collaborative strategies to achieve complex
tasks beyond individual capabilities in real-world scenarios. While existing
audio-visual navigation (AVN) research mainly focuses on single-agent systems,
their limitations emerge in dynamic 3D environments where rapid multi-agent
coordination is critical, especially for time-sensitive applications like
emergency response. This paper introduces MASTAVN (Multi-Agent Scalable
Transformer Audio-Visual Navigation), a scalable framework enabling two agents
to collaboratively localize and navigate toward an audio target in shared 3D
environments. By integrating cross-agent communication protocols and joint
audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal
synchronization. Through rigorous evaluation in photorealistic 3D simulators
(Replica and Matterport3D), MASTAVN achieves significant reductions in task
completion time and notable improvements in navigation success rates compared
to single-agent and non-collaborative baselines. This highlights the essential
role of spatiotemporal coordination in multi-agent systems. Our findings
validate MASTAVN's effectiveness in time-sensitive emergency scenarios and
establish a paradigm for advancing scalable multi-agent embodied intelligence
in complex 3D environments.

</details>


### [938] [Large Language Models for 3D IC Space Planning](https://arxiv.org/abs/2509.22716)
*Hung-Ying Chu,Guan-Wei Chen,Shao-Yu Wei,Yu-Cheng Lin*

Main category: cs.RO

TL;DR: 本研究提出了一种基于大语言模型（LLM）的3D IC空间规划方法，采用后序切分树表示法，在保证布局合法性的同时有效减少死区空间。


<details>
  <summary>Details</summary>
Motivation: 随着二维设计接近缩放极限，3D IC成为提高集成密度和性能的关键方案，但其复杂性对空间规划提出了更高要求，需有效减少死区并保证布局质量。

Method: 使用开源大语言模型，并在大规模合成数据集上进行微调，结合后序切分树表示法生成合法的3D IC空间布局，同时在MCNC派生的3D基准上进行评估。

Result: 实验表明该方法在运行时间、合法性和死区减少之间取得了良好平衡，部分测试案例实现了零死区布局，并能推广至ami33、ami49等实际基准，但在更大或不规则实例上仍有挑战。

Conclusion: LLM驱动的空间规划可作为传统EDA工具的数据驱动补充，为可扩展的3D布局生成提供新思路，并具备向物流、3D物体放置等跨领域应用的潜力。

Abstract: Three-dimensional integrated circuits (3D ICs) have emerged as a promising
solution to the scaling limits of two-dimensional designs, offering higher
integration density, shorter interconnects, and improved performance. As design
complexity increases, effective space planning becomes essential to reduce dead
space and ensure layout quality. This study investigates the use of large
language models (LLMs) for 3D IC space planning through a post-order slicing
tree representation, which guarantees legal space plans while aiming to
minimize dead space. Open-source LLMs were fine-tuned on large-scale synthetic
datasets and further evaluated on MCNC-derived 3D benchmarks. Experimental
results indicate that the proposed framework achieves a favorable balance
between runtime efficiency, legality, and dead-space reduction, with
zero-dead-space layouts obtained in a significant portion of test cases under
practical runtime budgets. Beyond synthetic benchmarks, the method generalizes
to MCNC cases such as ami33 and ami49, though larger and irregular instances
remain challenging. The approach also shows potential for cross-domain
applications, including logistics and 3D object placement, where spatial
efficiency is critical. Overall, the results suggest that LLM-based space
planning can serve as a data-driven complement to traditional electronic design
automation (EDA) methods, providing new insights for scalable 3D layout
generation.

</details>


### [939] [Self-driving cars: Are we there yet?](https://arxiv.org/abs/2509.22754)
*Merve Atasever,Zhuochen Liu,Qingpei Li,Akshay Hitendra Shah,Hans Walker,Jyotirmoy V. Deshmukh,Rahul Jain*

Main category: cs.RO

TL;DR: 本文对CARLA、nuPlan和Waymo Open Dataset三大自动驾驶运动规划基准平台上的方法进行了综合比较分析，采用CARLA v2.0作为统一评估平台，评估并对比了不同模型的性能，指出现有方法的优势与不足，总结了当前趋势与共性挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了推动自动驾驶运动规划领域的发展，需要对现有主流基准平台上的方法进行公平、系统的比较，以识别共性问题和未来研究方向。

Method: 选取CARLA、nuPlan和Waymo Open Dataset三大领先榜单中的运动规划方法，采用CARLA leaderboard v2.0作为统一评估平台，对所选模型进行适配后进行标准化评估，开展横向对比分析。

Result: 实现了对多个先进运动规划方法的统一评估，揭示了各方法在不同驾驶场景下的性能差异，识别出当前技术的优势与局限性。

Conclusion: 当前运动规划方法在复杂场景下仍面临挑战，统一评估有助于发现共性问题，未来应关注泛化能力、安全性与效率的平衡以及真实世界部署的可行性。

Abstract: Autonomous driving remains a highly active research domain that seeks to
enable vehicles to perceive dynamic environments, predict the future
trajectories of traffic agents such as vehicles, pedestrians, and cyclists and
plan safe and efficient future motions. To advance the field, several
competitive platforms and benchmarks have been established to provide
standardized datasets and evaluation protocols. Among these, leaderboards by
the CARLA organization and nuPlan and the Waymo Open Dataset have become
leading benchmarks for assessing motion planning algorithms. Each offers a
unique dataset and challenging planning problems spanning a wide range of
driving scenarios and conditions. In this study, we present a comprehensive
comparative analysis of the motion planning methods featured on these three
leaderboards. To ensure a fair and unified evaluation, we adopt CARLA
leaderboard v2.0 as our common evaluation platform and modify the selected
models for compatibility. By highlighting the strengths and weaknesses of
current approaches, we identify prevailing trends, common challenges, and
suggest potential directions for advancing motion planning research.

</details>


### [940] [Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving](https://arxiv.org/abs/2509.22756)
*Shiyi Liang,Xinyuan Chang,Changjie Wu,Huiyuan Yan,Yifan Bai,Xinran Liu,Hang Zhang,Yujian Yuan,Shuang Zeng,Mu Xu,Xing Wei*

Main category: cs.RO

TL;DR: 本文提出了一种名为PAMR的新框架，用于从视觉观测中自回归地协同构建车道矢量和交通规则，实现了在长序列驾驶中的持续准确地图构建与交通规则感知。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注几何元素或仅将交通规则视为临时分类，无法捕捉规则在长时间驾驶中的持续有效性。因此需要一种能够同时精确构建高精地图并保持对交通规则持久感知的方法。

Method: 提出PAMR框架，包含两个关键机制：一是Map-Rule Co-Construction，用于在时间片段中处理驾驶场景；二是Map-Rule Cache，用于跨片段保持规则一致性。此外，还构建了改进的MapDRv2数据集以支持连续一致的地图生成评估。

Result: 实验表明，PAMR在联合矢量-规则建图任务中表现优越，并在整个长序列驾驶过程中保持了交通规则的持续有效性。

Conclusion: PAMR通过协同构建地图与规则并维护其时序一致性，有效解决了自动驾驶中地图构建与规则持久性脱节的问题，提升了复杂环境下的安全性与可靠性。

Abstract: Safe autonomous driving requires both accurate HD map construction and
persistent awareness of traffic rules, even when their associated signs are no
longer visible. However, existing methods either focus solely on geometric
elements or treat rules as temporary classifications, failing to capture their
persistent effectiveness across extended driving sequences. In this paper, we
present PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel
framework that performs autoregressive co-construction of lane vectors and
traffic rules from visual observations. Our approach introduces two key
mechanisms: Map-Rule Co-Construction for processing driving scenes in temporal
segments, and Map-Rule Cache for maintaining rule consistency across these
segments. To properly evaluate continuous and consistent map generation, we
develop MapDRv2, featuring improved lane geometry annotations. Extensive
experiments demonstrate that PAMR achieves superior performance in joint
vector-rule mapping tasks, while maintaining persistent rule effectiveness
throughout extended driving sequences.

</details>


### [941] [Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse](https://arxiv.org/abs/2509.23778)
*Zeyuan Zhang,Chaoran Li,Shao Zhang,Ying Wen*

Main category: cs.RO

TL;DR: 本文提出了一种基于序列建模的多智能体寻路方法SePar，通过Transformer架构实现隐式信息交换，在降低决策复杂度的同时保持全局感知能力，在多种MAPF任务及其变体中表现优于现有学习方法，并能良好泛化到未见环境。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的MAPD方法在狭窄通道和长走廊的仓库环境中仅依赖局部观测时性能较差，且显式通信机制带来高计算复杂度，因此需要一种既能减少信息缺失又能控制计算开销的方法。

Method: 将MAPF建模为序列建模问题，提出Sequential Pathfinder (SePar)，利用Transformer架构实现隐式信息共享，实现分布式决策中的全局感知，并证明了序列建模范式下的路径策略具有顺序不变最优性。

Result: 实验表明SePar在多种MAPF任务及变体上持续优于现有学习方法，决策复杂度从指数级降至线性，具备良好的环境泛化能力，并验证了在复杂仓库地图中引入模仿学习的必要性。

Conclusion: SePar通过序列建模和Transformer架构有效解决了MAPD中局部观测不足与通信开销高的问题，为多智能体路径规划提供了一种高效、可扩展的新范式。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of
Multi-Agent Path Finding (MAPF), where agents are required to sequentially
complete tasks with fixed-location pickup and delivery demands. Although
learning-based methods have made progress in MAPD, they often perform poorly in
warehouse-like environments with narrow pathways and long corridors when
relying only on local observations for distributed decision-making.
Communication learning can alleviate the lack of global information but
introduce high computational complexity due to point-to-point communication. To
address this challenge, we formulate MAPF as a sequence modeling problem and
prove that path-finding policies under sequence modeling possess
order-invariant optimality, ensuring its effectiveness in MAPD. Building on
this, we propose the Sequential Pathfinder (SePar), which leverages the
Transformer paradigm to achieve implicit information exchange, reducing
decision-making complexity from exponential to linear while maintaining
efficiency and global awareness. Experiments demonstrate that SePar
consistently outperforms existing learning-based methods across various MAPF
tasks and their variants, and generalizes well to unseen environments.
Furthermore, we highlight the necessity of integrating imitation learning in
complex maps like warehouses.

</details>


### [942] [Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem](https://arxiv.org/abs/2509.22801)
*Huajing Zhao,Brian Flynn,Adam Norton,Holly Yanco*

Main category: cs.RO

TL;DR: 本文介绍了COMPARE生态系统的工作进展，旨在通过制定组件级和流程级的模块化标准和指南，提升机器人操作领域开源产品的兼容性和可比性。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人操作中开源软件的互操作性和可重复性，需要建立统一的模块化标准和集成规范。

Method: 通过构建开源产品库分析各组件共性，调研现有模块化流程以总结最佳实践，并开发符合新标准的模块化流程。

Result: 初步建立了针对感知、抓取规划、运动规划等组件的模块化框架，提出了组件级和流程级的标准与指南，并开发了新的模块化管道。

Conclusion: 所提出的标准化方法有助于提升机器人操作系统的可重用性与可扩展性，为未来开源项目的协作提供了基础。

Abstract: The COMPARE Ecosystem aims to improve the compatibility and benchmarking of
open-source products for robot manipulation through a series of activities. One
such activity is the development of standards and guidelines to specify
modularization practices at the component-level for individual modules (e.g.,
perception, grasp planning, motion planning) and integrations of components
that form robot manipulation capabilities at the pipeline-level. This paper
briefly reviews our work-in-progress to date to (1) build repositories of
open-source products to identify common characteristics of each component in
the pipeline, (2) investigate existing modular pipelines to glean best
practices, and (3) develop new modular pipelines that advance prior work while
abiding by the proposed standards and guidelines.

</details>


### [943] [Teleoperator-Aware and Safety-Critical Adaptive Nonlinear MPC for Shared Autonomy in Obstacle Avoidance of Legged Robots](https://arxiv.org/abs/2509.22815)
*Ruturaj Sambhus,Muneeb Ahmad,Basit Muhammad Imran,Sujith Vijayan,Dylan P. Losey,Kaveh Akbari Hamed*

Main category: cs.RO

TL;DR: 提出了一种面向四足机器人共享自主的自适应非线性模型预测控制（ANMPC）框架，结合控制屏障函数（CBF）保障安全性，并通过在线学习人类输入参数实现人机协同避障。


<details>
  <summary>Details</summary>
Motivation: 传统共享控制方法在动态腿式运动和复杂环境中难以兼顾安全性和协作效率，缺乏对人类操作意图的动态适应能力。

Method: 采用分层控制架构：高层CBF-ANMPC（10 Hz）生成人机融合速度指令；中层NMPC（60 Hz）跟踪简化动力学；底层非线性全身控制器（500 Hz）执行全动力学跟踪。引入噪声理性Boltzmann模型在线估计人类意图，并通过投影梯度下降法实时更新参数，结合CBF约束确保系统安全性。

Result: 在Unitree Go2四足机器人上进行了数值仿真、实物实验和用户研究，验证了框架的实时避障能力、人类意图参数的在线学习效果以及安全的人机协作性能。

Conclusion: 所提出的自适应、安全关键的共享控制框架有效提升了人机协作的安全性与灵活性，适用于复杂环境下的四足机器人遥操作。

Abstract: Ensuring safe and effective collaboration between humans and autonomous
legged robots is a fundamental challenge in shared autonomy, particularly for
teleoperated systems navigating cluttered environments. Conventional
shared-control approaches often rely on fixed blending strategies that fail to
capture the dynamics of legged locomotion and may compromise safety. This paper
presents a teleoperator-aware, safety-critical, adaptive nonlinear model
predictive control (ANMPC) framework for shared autonomy of quadrupedal robots
in obstacle-avoidance tasks. The framework employs a fixed arbitration weight
between human and robot actions but enhances this scheme by modeling the human
input with a noisily rational Boltzmann model, whose parameters are adapted
online using a projected gradient descent (PGD) law from observed joystick
commands. Safety is enforced through control barrier function (CBF) constraints
integrated into a computationally efficient NMPC, ensuring forward invariance
of safe sets despite uncertainty in human behavior. The control architecture is
hierarchical: a high-level CBF-based ANMPC (10 Hz) generates blended
human-robot velocity references, a mid-level dynamics-aware NMPC (60 Hz)
enforces reduced-order single rigid body (SRB) dynamics to track these
references, and a low-level nonlinear whole-body controller (500 Hz) imposes
the full-order dynamics via quadratic programming to track the mid-level
trajectories. Extensive numerical and hardware experiments, together with a
user study, on a Unitree Go2 quadrupedal robot validate the framework,
demonstrating real-time obstacle avoidance, online learning of human intent
parameters, and safe teleoperator collaboration.

</details>


### [944] [Parameter Identification of a Differentiable Human Arm Musculoskeletal Model without Deep Muscle EMG Reconstruction](https://arxiv.org/abs/2509.22825)
*Philip Sanderink,Yingfan Zhou,Shuzhen Luo,Cheng Fang*

Main category: cs.RO

TL;DR: 提出了一种无需重建深层肌肉EMG的可微优化方法，用于识别人体手臂骨骼肌模型的骨和浅层肌肉参数。


<details>
  <summary>Details</summary>
Motivation: 准确的个体化肌肉骨骼模型参数识别对协作机器人系统（如辅助外骨骼）至关重要，但现有基于EMG的方法受限于深层肌肉信号难以无创测量。

Method: 利用深层肌肉力的最小二乘解计算模型参数损失梯度，在可微优化框架下同时识别骨骼和浅层肌肉参数，避免直接重建深层肌肉EMG。

Result: 通过大量对比仿真表明，该方法在仅使用浅层EMG的情况下，能达到与拥有全部肌肉EMG数据的类似方法相当的估计精度。

Conclusion: 所提方法有效避免了对深层肌肉行为建模的假设，提高了参数识别的可靠性与实用性，适用于无创个性化肌肉骨骼建模。

Abstract: Accurate parameter identification of a subject-specific human musculoskeletal
model is crucial to the development of safe and reliable physically
collaborative robotic systems, for instance, assistive exoskeletons.
Electromyography (EMG)-based parameter identification methods have demonstrated
promising performance for personalized musculoskeletal modeling, whereas their
applicability is limited by the difficulty of measuring deep muscle EMGs
invasively. Although several strategies have been proposed to reconstruct deep
muscle EMGs or activations for parameter identification, their reliability and
robustness are limited by assumptions about the deep muscle behavior. In this
work, we proposed an approach to simultaneously identify the bone and
superficial muscle parameters of a human arm musculoskeletal model without
reconstructing the deep muscle EMGs. This is achieved by only using the
least-squares solution of the deep muscle forces to calculate a loss gradient
with respect to the model parameters for identifying them in a framework of
differentiable optimization. The results of extensive comparative simulations
manifested that our proposed method can achieve comparable estimation accuracy
compared to a similar method, but with all the muscle EMGs available.

</details>


### [945] [Prompting Robot Teams with Natural Language](https://arxiv.org/abs/2509.24575)
*Nicolas Pfitzer,Eduardo Sebastián,Ajay Shankar,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出了一种利用自然语言提示多机器人团队执行高层任务的框架，通过将任务表示为确定性有限自动机（DFA），并利用循环神经网络（RNN）编码其逻辑结构，结合图神经网络（GNN）策略实现去中心化的实时协作控制。


<details>
  <summary>Details</summary>
Motivation: 由于个体在集体中的行为难以明确描述和解释，并需持续适应其他成员的行为，因此需要一种既能表达任务逻辑语义、又能支持去中心化和实时交互操作的框架。

Method: 将任务建模为确定性有限自动机（DFA），利用RNN学习语言模型分解出的子任务逻辑与序列结构，并将其隐藏状态与任务语义对齐；进一步训练一个依赖于RNN隐藏状态和语言嵌入的图神经网络（GNN）控制策略，实现机器人的去中心化协同执行。

Result: 在多个需要顺序性和协作性的模拟和真实世界多机器人任务中验证了该方法的有效性，仅使用一个轻量级且可解释的模型即可实现复杂的团队协作行为。

Conclusion: 该框架成功融合了语言模型的意图理解能力与形式化任务表示，实现了自然语言到多机器人协同行为的高效、可解释映射，适用于动态、分布式的多智能体系统。

Abstract: This paper presents a framework towards prompting multi-robot teams with
high-level tasks using natural language expressions. Our objective is to use
the reasoning capabilities demonstrated by recent language models in
understanding and decomposing human expressions of intent, and repurpose these
for multi-robot collaboration and decision-making. The key challenge is that an
individual's behavior in a collective can be hard to specify and interpret, and
must continuously adapt to actions from others. This necessitates a framework
that possesses the representational capacity required by the logic and
semantics of a task, and yet supports decentralized and interactive real-time
operation. We solve this dilemma by recognizing that a task can be represented
as a deterministic finite automaton (DFA), and that recurrent neural networks
(RNNs) can encode numerous automata. This allows us to distill the logic and
sequential decompositions of sub-tasks obtained from a language model into an
RNN, and align its internal states with the semantics of a given task. By
training a graph neural network (GNN) control policy that is conditioned on the
hidden states of the RNN and the language embeddings, our method enables robots
to execute task-relevant actions in a decentralized manner. We present
evaluations of this single light-weight interpretable model on various
simulated and real-world multi-robot tasks that require sequential and
collaborative behavior by the team -- sites.google.com/view/prompting-teams.

</details>


### [946] [Dynamic Buffers: Cost-Efficient Planning for Tabletop Rearrangement with Stacking](https://arxiv.org/abs/2509.22828)
*Arman Barghi,Hamed Hosseini,Seraj Ghasemi,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: 提出了一种名为动态缓冲（Dynamic Buffer）的新型规划原语，通过构建可移动的临时堆叠来提升机器人在密集和大规模环境中重排物体的效率和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统规划器在处理杂乱环境中的物体重排时效率低下，使用固定缓冲区导致高成本和低效操作，尤其是在密集场景中。

Method: 引入动态缓冲原语，模仿人类分组策略，允许机器人创建可移动的临时堆栈，并将其作为整体运输。

Result: 与现有最先进方法相比，在密集场景中机械臂移动成本降低11.89%，在大规模低密度环境中降低5.69%。实验在Delta并联机器人上验证了该方法的实用性。

Conclusion: 动态缓冲是一种关键的规划原语，能显著提升重排任务的成本效益和鲁棒性。

Abstract: Rearranging objects in cluttered tabletop environments remains a
long-standing challenge in robotics. Classical planners often generate
inefficient, high-cost plans by shuffling objects individually and using fixed
buffers--temporary spaces such as empty table regions or static stacks--to
resolve conflicts. When only free table locations are used as buffers, dense
scenes become inefficient, since placing an object can restrict others from
reaching their goals and complicate planning. Allowing stacking provides extra
buffer capacity, but conventional stacking is static: once an object supports
another, the base cannot be moved, which limits efficiency. To overcome these
issues, a novel planning primitive called the Dynamic Buffer is introduced.
Inspired by human grouping strategies, it enables robots to form temporary,
movable stacks that can be transported as a unit. This improves both
feasibility and efficiency in dense layouts, and it also reduces travel in
large-scale settings where space is abundant. Compared with a state-of-the-art
rearrangement planner, the approach reduces manipulator travel cost by 11.89%
in dense scenarios with a stationary robot and by 5.69% in large, low-density
settings with a mobile manipulator. Practicality is validated through
experiments on a Delta parallel robot with a two-finger gripper. These findings
establish dynamic buffering as a key primitive for cost-efficient and robust
rearrangement planning.

</details>


### [947] [Curriculum Imitation Learning of Distributed Multi-Robot Policies](https://arxiv.org/abs/2509.25097)
*Jesús Roche,Eduardo Sebastián,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出了一种基于模仿学习的多机器人系统控制策略，通过课程学习提升长期协调能力，并利用全局状态演示估计局部感知，实现鲁棒分布控制。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人系统中长期协调困难和真实训练数据获取难的问题。

Method: 采用课程学习逐步增加专家轨迹长度以提升长期协调；通过过滤邻居、转换坐标系和模拟传感器噪声，将全局演示转化为局部可观测信息。

Result: 实验表明所提方法在不同任务、团队规模和噪声水平下均能提高长期行为准确性，并使策略对现实不确定性具有鲁棒性。

Conclusion: 该方法能够仅从全局演示中学习到鲁棒、分布式的控制器，无需专家动作或机载测量数据。

Abstract: Learning control policies for multi-robot systems (MRS) remains a major
challenge due to long-term coordination and the difficulty of obtaining
realistic training data. In this work, we address both limitations within an
imitation learning framework. First, we shift the typical role of Curriculum
Learning in MRS, from scalability with the number of robots, to focus on
improving long-term coordination. We propose a curriculum strategy that
gradually increases the length of expert trajectories during training,
stabilizing learning and enhancing the accuracy of long-term behaviors. Second,
we introduce a method to approximate the egocentric perception of each robot
using only third-person global state demonstrations. Our approach transforms
idealized trajectories into locally available observations by filtering
neighbors, converting reference frames, and simulating onboard sensor
variability. Both contributions are integrated into a physics-informed
technique to produce scalable, distributed policies from observations. We
conduct experiments across two tasks with varying team sizes and noise levels.
Results show that our curriculum improves long-term accuracy, while our
perceptual estimation method yields policies that are robust to realistic
uncertainty. Together, these strategies enable the learning of robust,
distributed controllers from global demonstrations, even in the absence of
expert actions or onboard measurements.

</details>


### [948] [Empart: Interactive Convex Decomposition for Converting Meshes to Parts](https://arxiv.org/abs/2509.22847)
*Brandon Vu,Shameek Ganguly,Pushkar Joshi*

Main category: cs.RO

TL;DR: 本文提出了一种名为Empart的交互式工具，用于在3D网格的不同区域应用差异化的简化误差容限，以优化机器人应用中的仿真性能。


<details>
  <summary>Details</summary>
Motivation: 现有网格简化方法通常使用统一的误差容限，导致在非关键区域过度保留细节或在关键接触区域精度不足，影响运动规划和仿真的效率与准确性。

Method: Empart基于现有的凸分解算法，引入一种新颖的并行框架，支持用户对网格的特定区域设置不同的简化容差，并提供误差和性能的可视化反馈，实现交互式优化。

Result: 相比当前最先进的V-HACD方法，在相同误差阈值下，Empart显著减少了生成的凸部件数量；在机器人抓取任务中，仿真时间减少了69%。

Conclusion: Empart通过区域自适应的网格简化，在保证关键区域几何精度的同时提升了仿真效率，验证了交互式、区域差异化简化在机器人应用中的重要价值。

Abstract: Simplifying complex 3D meshes is a crucial step in robotics applications to
enable efficient motion planning and physics simulation. Common methods, such
as approximate convex decomposition, represent a mesh as a collection of simple
parts, which are computationally inexpensive to simulate. However, existing
approaches apply a uniform error tolerance across the entire mesh, which can
result in a sub-optimal trade-off between accuracy and performance. For
instance, a robot grasping an object needs high-fidelity geometry in the
vicinity of the contact surfaces but can tolerate a coarser simplification
elsewhere. A uniform tolerance can lead to excessive detail in non-critical
areas or insufficient detail where it's needed most.
  To address this limitation, we introduce Empart, an interactive tool that
allows users to specify different simplification tolerances for selected
regions of a mesh. Our method leverages existing convex decomposition
algorithms as a sub-routine but uses a novel, parallelized framework to handle
region-specific constraints efficiently. Empart provides a user-friendly
interface with visual feedback on approximation error and simulation
performance, enabling designers to iteratively refine their decomposition. We
demonstrate that our approach significantly reduces the number of convex parts
compared to a state-of-the-art method (V-HACD) at a fixed error threshold,
leading to substantial speedups in simulation performance. For a robotic
pick-and-place task, Empart-generated collision meshes reduced the overall
simulation time by 69% compared to a uniform decomposition, highlighting the
value of interactive, region-specific simplification for performant robotics
applications.

</details>


### [949] [Multi-Robot Allocation for Information Gathering in Non-Uniform Spatiotemporal Environments](https://arxiv.org/abs/2509.22883)
*Kaleb Ben Naveed,Haejoon Lee,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出了一种用于多机器人场估计的两阶段框架，以解决非均匀时空环境中高斯过程（GP）模型因长度尺度不准确而导致的不确定性估计误差问题。


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程方法通常假设全局长度尺度或更新不及时，难以适应具有不同时空动态的非均匀环境，导致场估计中的不确定性建模不准确。

Method: 第一阶段使用基于变差函数的规划器学习区域特定的空间长度尺度；第二阶段采用基于当前不确定性的分配策略，并随着时序长度尺度的优化动态调整采样。利用之前工作中提出的清晰度（clarity）信息度量来编码不确定性。

Result: 在多种环境中进行了评估，验证了方法的有效性；提供了空间长度尺度估计的收敛性分析，并给出了动态遗憾界以量化与最优分配序列的差距。

Conclusion: 所提框架能更准确地估计非均匀时空场的不确定性，优于传统单一全局尺度或静态更新的方法，适用于复杂动态环境下的多机器人协同感知。

Abstract: Autonomous robots are increasingly deployed to estimate spatiotemporal fields
(e.g., wind, temperature, gas concentration) that vary across space and time.
We consider environments divided into non-overlapping regions with distinct
spatial and temporal dynamics, termed non-uniform spatiotemporal environments.
Gaussian Processes (GPs) can be used to estimate these fields. The GP model
depends on a kernel that encodes how the field co-varies in space and time,
with its spatial and temporal lengthscales defining the correlation. Hence,
when these lengthscales are incorrect or do not correspond to the actual field,
the estimates of uncertainty can be highly inaccurate. Existing GP methods
often assume one global lengthscale or update only periodically; some allow
spatial variation but ignore temporal changes. To address these limitations, we
propose a two-phase framework for multi-robot field estimation. Phase 1 uses a
variogram-driven planner to learn region-specific spatial lengthscales. Phase 2
employs an allocation strategy that reassigns robots based on the current
uncertainty, and updates sampling as temporal lengthscales are refined. For
encoding uncertainty, we utilize clarity, an information metric from our
earlier work. We evaluate the proposed method across diverse environments and
provide convergence analysis for spatial lengthscale estimation, along with
dynamic regret bounds quantifying the gap to the oracle's allocation sequence.

</details>


### [950] [Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM](https://arxiv.org/abs/2509.22910)
*Yanwei Du,Jing-Chen Peng,Patricio A. Vela*

Main category: cs.RO

TL;DR: 提出了一种名为“Good Weights”（GW）的算法，通过自适应融合视觉SLAM与航位推算（DR），提升在纹理缺失或视觉退化环境中的定位鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 视觉SLAM在缺乏纹理或光照不良的环境中性能下降，容易丢失轨迹；而DR虽短期可靠但长期误差累积，需一种自适应融合方法以兼顾两者优势。

Method: 设计了一个自适应加权框架（Good Weights），动态调整视觉SLAM与DR的贡献权重：当视觉跟踪不可靠时增强DR影响，视觉特征强时则降低DR权重，并对SLAM系统的各模块进行相应修改以集成DR。

Result: 在真实数据集和实际部署中验证了该方法的有效性，能够在挑战性环境中保持连续且准确的帧级位姿估计，减少跟踪丢失。

Conclusion: Good Weights算法有效提升了视觉SLAM在复杂环境下的鲁棒性和准确性，为移动机器人导航提供了一种实用的解决方案。

Abstract: Given that Visual SLAM relies on appearance cues for localization and scene
understanding, texture-less or visually degraded environments (e.g., plain
walls or low lighting) lead to poor pose estimation and track loss. However,
robots are typically equipped with sensors that provide some form of dead
reckoning odometry with reasonable short-time performance but unreliable
long-time performance. The Good Weights (GW) algorithm described here provides
a framework to adaptively integrate dead reckoning (DR) with passive visual
SLAM for continuous and accurate frame-level pose estimation. Importantly, it
describes how all modules in a comprehensive SLAM system must be modified to
incorporate DR into its design. Adaptive weighting increases DR influence when
visual tracking is unreliable and reduces when visual feature information is
strong, maintaining pose track without overreliance on DR. Good Weights yields
a practical solution for mobile navigation that improves visual SLAM
performance and robustness. Experiments on collected datasets and in real-world
deployment demonstrate the benefits of Good Weights.

</details>


### [951] [ARMimic: Learning Robotic Manipulation from Passive Human Demonstrations in Augmented Reality](https://arxiv.org/abs/2509.22914)
*Rohan Walia,Yusheng Wang,Ralf Römer,Masahiro Nishio,Angela P. Schoellig,Jun Ota*

Main category: cs.RO

TL;DR: 提出ARMimic框架，利用消费级XR头显和固定摄像头实现轻量、无机器人参与的模仿学习数据采集，显著降低演示时间并提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统机器人模仿学习的示教方法（如物理示教和遥操作）繁琐、依赖硬件且干扰工作流；现有XR观察方法仍需额外设备或复杂校准，限制了可扩展性。

Method: ARMimic结合第一人称手势追踪、增强现实机器人叠加和实时深度感知，通过统一的模仿学习 pipeline 将人类与虚拟机器人轨迹视为可互换，实现跨形态和环境的策略泛化。

Result: 在两个操作任务（包括长视野的碗堆叠）上验证，相比遥操作演示时间减少50%，相比基于遥操作数据的ACT基线任务成功率提升11%。

Conclusion: ARMimic实现了安全、无缝、真实场景下的机器人数据采集，为现实世界中可扩展的机器人学习提供了新可能。

Abstract: Imitation learning is a powerful paradigm for robot skill acquisition, yet
conventional demonstration methods--such as kinesthetic teaching and
teleoperation--are cumbersome, hardware-heavy, and disruptive to workflows.
Recently, passive observation using extended reality (XR) headsets has shown
promise for egocentric demonstration collection, yet current approaches require
additional hardware, complex calibration, or constrained recording conditions
that limit scalability and usability. We present ARMimic, a novel framework
that overcomes these limitations with a lightweight and hardware-minimal setup
for scalable, robot-free data collection using only a consumer XR headset and a
stationary workplace camera. ARMimic integrates egocentric hand tracking,
augmented reality (AR) robot overlays, and real-time depth sensing to ensure
collision-aware, kinematically feasible demonstrations. A unified imitation
learning pipeline is at the core of our method, treating both human and virtual
robot trajectories as interchangeable, which enables policies that generalize
across different embodiments and environments. We validate ARMimic on two
manipulation tasks, including challenging long-horizon bowl stacking. In our
experiments, ARMimic reduces demonstration time by 50% compared to
teleoperation and improves task success by 11% over ACT, a state-of-the-art
baseline trained on teleoperated data. Our results demonstrate that ARMimic
enables safe, seamless, and in-the-wild data collection, offering great
potential for scalable robot learning in diverse real-world settings.

</details>


### [952] [DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent Autonomous Racing Overtakes](https://arxiv.org/abs/2509.22937)
*Trent Weiss,Amar Kulkarni,Madhur Behl*

Main category: cs.RO

TL;DR: 本文提出了一种基于微分贝叶斯滤波框架扩展的轨迹合成方法，用于自动驾驶赛车中的超车动作生成，能够在复杂赛道上实现高效、无碰撞的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 自主赛车中生成超车动作具有挑战性，现有方法常依赖于对避障和动力学约束的过度简化假设，难以在复杂赛道上可靠执行。

Method: 提出一种基于扩展微分贝叶斯滤波（DBF-MA）的轨迹合成方法，将问题建模为复合贝塞尔曲线空间上的贝叶斯推断，无需导数计算，不依赖车辆形状的球形近似、约束线性化或避障的简化上界。

Result: 在闭环测试中，该方法在87%的测试场景中成功超越对手，性能优于现有的自主超车方法。

Conclusion: 所提出的方法在不依赖常见简化假设的情况下，显著提升了复杂赛道上自主超车的可靠性与成功率。

Abstract: A significant challenge in autonomous racing is to generate overtaking
maneuvers. Racing agents must execute these maneuvers on complex racetracks
with little room for error. Optimization techniques and graph-based methods
have been proposed, but these methods often rely on oversimplified assumptions
for collision-avoidance and dynamic constraints. In this work, we present an
approach to trajectory synthesis based on an extension of the Differential
Bayesian Filtering framework. Our approach for collision-free trajectory
synthesis frames the problem as one of Bayesian Inference over the space of
Composite Bezier Curves. Our method is derivative-free, does not require a
spherical approximation of the vehicle footprint, linearization of constraints,
or simplifying upper bounds on collision avoidance. We conduct a closed-loop
analysis of DBF-MA and find it successfully overtakes an opponent in 87% of
tested scenarios, outperforming existing methods in autonomous overtaking.

</details>


### [953] [Hierarchical Control Design for Space Robots with Application to In-Orbit Servicing Missions](https://arxiv.org/abs/2509.22955)
*Pietro Bruschi*

Main category: cs.RO

TL;DR: 本文提出了一种用于在轨服务和主动清除空间碎片的自主机器人捕获翻滚目标的分层控制框架，并开发了包含追逐器晃动动力学的仿真环境，仿真结果表明所提控制器相比现有方案具有更好的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 在轨服务和主动清除空间碎片需要先进的机器人技术来捕获和稳定不受控的目标，而现有研究较少考虑追逐器的晃动动力学影响。

Method: 提出一种结合内层基于Lyapunov的鲁棒控制回路与外层扩展逆运动学问题求解的分层控制框架，并构建包含晃动动力学的仿真环境进行验证。

Result: 仿真结果显示该控制器在处理多体动力学和不确定性方面表现出更高的鲁棒性和适应性，优于现有控制方案。

Conclusion: 所提出的分层控制框架有效提升了对翻滚目标的自主捕获性能，同时考虑晃动动力学的仿真更贴近真实空间环境，具有实际应用价值。

Abstract: In-Orbit Servicing and Active Debris Removal require advanced robotic
capabilities for capturing and detumbling uncooperative targets. This work
presents a hierarchical control framework for autonomous robotic capture of
tumbling objects in space. A simulation environment is developed, incorporating
sloshing dynamics of the chaser, a rarely studied effect in space robotics. The
proposed controller combines an inner Lyapunov-based robust control loop for
multi-body dynamics with an outer loop addressing an extended inverse
kinematics problem. Simulation results show improved robustness and
adaptability compared to existing control schemes.

</details>


### [954] [Robot Learning from Any Images](https://arxiv.org/abs/2509.22970)
*Siheng Zhao,Jiageng Mao,Wei Chow,Zeyu Shangguan,Tianheng Shi,Rong Xue,Yuxi Zheng,Yijia Weng,Yang You,Daniel Seita,Leonidas Guibas,Sergey Zakharov,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: RoLA是一个将任意野外图像转换为交互式、物理启用的机器人环境的框架，仅需单张图像即可快速生成大规模视觉运动机器人演示。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人数据生成方法通常需要额外的硬件或数字资产，限制了其广泛应用。RoLA旨在通过直接从单张图像生成物理真实的机器人环境，降低数据生成门槛，实现更广泛和高效的机器人学习。

Method: RoLA结合了一种新的单视角物理场景恢复方法和高效的视觉融合策略，能够在没有额外硬件或数字资产的情况下，从单张图像中恢复出物理合理的3D场景，并生成逼真的视觉-运动数据。

Result: RoLA能够从多种图像源（如相机拍摄、机器人数据集和互联网图像）中快速生成大量高质量的机器人演示数据，并在可扩展的数据生成、互联网图像学习以及单图实-虚-实系统等应用中展现出强大灵活性。

Conclusion: RoLA为机器人数据生成提供了一个高效、低成本且易于普及的解决方案，推动了基于真实世界图像的机器人学习发展。

Abstract: We introduce RoLA, a framework that transforms any in-the-wild image into an
interactive, physics-enabled robotic environment. Unlike previous methods, RoLA
operates directly on a single image without requiring additional hardware or
digital assets. Our framework democratizes robotic data generation by producing
massive visuomotor robotic demonstrations within minutes from a wide range of
image sources, including camera captures, robotic datasets, and Internet
images. At its core, our approach combines a novel method for single-view
physical scene recovery with an efficient visual blending strategy for
photorealistic data collection. We demonstrate RoLA's versatility across
applications like scalable robotic data generation and augmentation, robot
learning from Internet images, and single-image real-to-sim-to-real systems for
manipulators and humanoids. Video results are available at
https://sihengz02.github.io/RoLA .

</details>


### [955] [Safe Task Space Synchronization with Time-Delayed Information](https://arxiv.org/abs/2509.22976)
*Rounak Bhattacharya,Vrithik R. Guthikonda,Ashwin P. Dani*

Main category: cs.RO

TL;DR: 本文设计了一种自适应控制器，用于在存在通信时延的情况下，实现具有未知运动学和动力学的机器人与人类轨迹的同步，并通过Barrier Lyapunov函数保证安全性。


<details>
  <summary>Details</summary>
Motivation: 在人机协作任务中，通信延迟和系统不确定性（如未知运动学和动力学）会影响轨迹同步性能，因此需要设计鲁棒且安全的自适应控制方法。

Method: 采用Barrier Lyapunov函数约束机器人笛卡尔坐标以确保安全，结合基于ICL的自适应律估计未知运动学，梯度型自适应律估计未知动力学，并利用Barrier Lyapunov-Krasovskii泛函进行稳定性分析。

Result: 仿真结果表明，在存在时间延迟的人机同步场景中，所提出的控制器能有效实现轨迹同步，同时保持同步误差和参数估计误差半全局一致最终有界（SGUUB）。

Conclusion: 该自适应控制策略能够有效处理通信延迟、未知运动学与动力学，并满足安全约束，适用于实际人机协作场景。

Abstract: In this paper, an adaptive controller is designed for the synchronization of
the trajectory of a robot with unknown kinematics and dynamics to that of the
current human trajectory in the task space using the delayed human trajectory
information. The communication time delay may be a result of various factors
that arise in human-robot collaboration tasks, such as sensor processing or
fusion to estimate trajectory/intent, network delays, or computational
limitations. The developed adaptive controller uses Barrier Lyapunov Function
(BLF) to constrain the Cartesian coordinates of the robot to ensure safety, an
ICL-based adaptive law to account for the unknown kinematics, and a
gradient-based adaptive law to estimate unknown dynamics. Barrier
Lyapunov-Krasovskii (LK) functionals are used for the stability analysis to
show that the synchronization and parameter estimation errors remain
semi-globally uniformly ultimately bounded (SGUUB). The simulation results
based on a human-robot synchronization scenario with time delay are provided to
demonstrate the effectiveness of the designed synchronization controller with
safety constraints.

</details>


### [956] [UniPrototype: Humn-Robot Skill Learning with Uniform Prototypes](https://arxiv.org/abs/2509.23021)
*Xiao Hu,Qi Yin,Yangming Shi,Yang Ye*

Main category: cs.RO

TL;DR: 本文提出了一种名为UniPrototype的新框架，通过共享运动原语实现从人类到机器人领域的知识迁移，有效解决了机器人学习中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 由于机器人操作领域训练样本有限，而人类动作数据丰富，因此需要在两者之间建立桥梁以提升机器人学习效率。

Method: 提出一种具有软分配的组合式原型发现机制，允许多个原语共同激活以捕捉混合和层次化技能，并设计自适应原型选择策略，根据任务复杂度自动调整原型数量。

Result: 在仿真环境和真实机器人系统上的大量实验表明，该方法显著提高了机器人的学习效率和任务性能。

Conclusion: UniPrototype能够有效迁移人类操作知识至机器人，优于现有方法，具备良好的可扩展性和实用性。

Abstract: Data scarcity remains a fundamental challenge in robot learning. While human
demonstrations benefit from abundant motion capture data and vast internet
resources, robotic manipulation suffers from limited training examples. To
bridge this gap between human and robot manipulation capabilities, we propose
UniPrototype, a novel framework that enables effective knowledge transfer from
human to robot domains via shared motion primitives. ur approach makes three
key contributions: (1) We introduce a compositional prototype discovery
mechanism with soft assignments, enabling multiple primitives to co-activate
and thus capture blended and hierarchical skills; (2) We propose an adaptive
prototype selection strategy that automatically adjusts the number of
prototypes to match task complexity, ensuring scalable and efficient
representation; (3) We demonstrate the effectiveness of our method through
extensive experiments in both simulation environments and real-world robotic
systems. Our results show that UniPrototype successfully transfers human
manipulation knowledge to robots, significantly improving learning efficiency
and task performance compared to existing approaches.The code and dataset will
be released upon acceptance at an anonymous repository.

</details>


### [957] [RAISE: A Robot-Assisted Selective Disassembly and Sorting System for End-of-Life Phones](https://arxiv.org/abs/2509.23048)
*Chang Liu,Badrinath Balasubramaniam,Neal Yancey,Michael Severson,Adam Shine,Philip Bove,Beiwen Li,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 提出了一种低成本、易部署的自动化选择性拆解与分拣系统，用于废旧手机回收，包含自适应切割、基于视觉的机器人分拣和电池移除三个子系统，每小时可处理120部以上手机，平均拆解成功率达98.9%，并能实现经济盈利。


<details>
  <summary>Details</summary>
Motivation: 废旧手机因产量大、生命周期短而加剧电子垃圾问题，现有拆解依赖人工，劳动强度高且效率低。

Method: 设计并集成三个子系统：自适应切割系统、基于视觉的机器人分拣系统和电池移除系统，实现自动化、选择性拆解与分类。

Result: 系统每小时可处理超过120部手机，平均拆解成功率为98.9%，能够高效分离高价值部件，并使拆解过程由亏损转为盈利。

Conclusion: 该系统为废旧手机拆解提供了可靠、可扩展的自动化解决方案，兼具环境效益与经济效益。

Abstract: End-of-Life (EoL) phones significantly exacerbate global e-waste challenges
due to their high production volumes and short lifecycles. Disassembly is among
the most critical processes in EoL phone recycling. However, it relies heavily
on human labor due to product variability. Consequently, the manual process is
both labor-intensive and time-consuming. In this paper, we propose a low-cost,
easily deployable automated and selective disassembly and sorting system for
EoL phones, consisting of three subsystems: an adaptive cutting system, a
vision-based robotic sorting system, and a battery removal system. The system
can process over 120 phones per hour with an average disassembly success rate
of 98.9%, efficiently delivering selected high-value components to downstream
processing. It provides a reliable and scalable automated solution to the
pressing challenge of EoL phone disassembly. Additionally, the automated system
can enhance disassembly economics, converting a previously unprofitable process
into one that yields a net profit per unit weight of EoL phones.

</details>


### [958] [In-Hand Manipulation of Articulated Tools with Dexterous Robot Hands with Sim-to-Real Transfer](https://arxiv.org/abs/2509.23075)
*Soofiyan Atar,Daniel Huang,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 提出了一种结合仿真训练与硬件演示的传感器驱动精细化控制方法，用于机器人手部对铰接工具的灵巧操作，实现了从仿真到真实环境的鲁棒迁移。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习策略在处理具有复杂接触动力学和未建模关节现象（如摩擦、粘滑、回差）的铰接机构时表现脆弱，难以实现灵巧操作。

Method: 采用仿真训练的基础策略，并通过基于跨注意力机制融合本体感知、目标关节状态及全手触觉与力反馈的传感器驱动精细化模块，结合硬件示范进行学习，实现在线适应与稳定控制。

Result: 在多种真实铰接工具（如剪刀、钳子、微创手术工具和订书机）上验证了方法的有效性，表现出强鲁棒性、抗干扰能力和对未见工具的良好泛化性。

Conclusion: 该方法显著提升了机器人对铰接工具的操作能力，减少了对精确物理建模的依赖，推动了在高接触复杂环境中灵巧操作的实用化进展。

Abstract: Reinforcement learning (RL) and sim-to-real transfer have advanced robotic
manipulation of rigid objects. Yet, policies remain brittle when applied to
articulated mechanisms due to contact-rich dynamics and under-modeled joint
phenomena such as friction, stiction, backlash, and clearances. We address this
challenge through dexterous in-hand manipulation of articulated tools using a
robotic hand with reduced articulation and kinematic redundancy relative to the
human hand. Our controller augments a simulation-trained base policy with a
sensor-driven refinement learned from hardware demonstrations, conditioning on
proprioception and target articulation states while fusing whole-hand tactile
and force feedback with the policy's internal action intent via
cross-attention-based integration. This design enables online adaptation to
instance-specific articulation properties, stabilizes contact interactions,
regulates internal forces, and coordinates coupled-link motion under
perturbations. We validate our approach across a diversity of real-world
examples, including scissors, pliers, minimally invasive surgical tools, and
staplers. We achieve robust transfer from simulation to hardware, improved
disturbance resilience, and generalization to previously unseen articulated
tools, thereby reducing reliance on precise physical modeling in contact-rich
settings.

</details>


### [959] [Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning](https://arxiv.org/abs/2509.23107)
*Yi Wang,Zeyu Xue,Mujie Liu,Tongqin Zhang,Yan Hu,Zhou Zhao,Chenguang Yang,Zhenyu Lu*

Main category: cs.RO

TL;DR: 提出了一种名为ST-OVSG的时空开放词汇场景图表示方法，用于缓解远程操作中通信延迟导致的状态与意图不匹配问题，提升自然语言遥操作的鲁棒性和成功率。


<details>
  <summary>Details</summary>
Motivation: 在动态远程环境中，双向通信的传输延迟会导致操作员意图与实际环境状态之间的不一致，从而引发命令误解和执行错误。因此需要一种能够应对延迟并支持开放词汇感知的场景表示方法。

Method: 提出Spatio-Temporal Open-Vocabulary Scene Graph（ST-OVSG），利用LVLM构建开放词汇3D对象表示，并通过匈牙利匹配和时序匹配代价扩展至时间域，形成统一的时空场景图；嵌入延迟标签以支持对历史状态的查询，并设计任务导向的子图过滤策略以压缩输入。

Result: 在Replica基准上达到74%的节点准确率，优于ConceptGraph；在延迟鲁棒性实验中，使用ST-OVSG的LVLM规划器取得70.5%的规划成功率。

Conclusion: ST-OVSG有效提升了自然语言遥操作在通信延迟下的规划鲁棒性，无需微调即可泛化到新类别，且通过轻量级延迟标注和任务相关特征筛选增强了系统实用性。

Abstract: Teleoperation via natural-language reduces operator workload and enhances
safety in high-risk or remote settings. However, in dynamic remote scenes,
transmission latency during bidirectional communication creates gaps between
remote perceived states and operator intent, leading to command
misunderstanding and incorrect execution. To mitigate this, we introduce the
Spatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that
enriches open-vocabulary perception with temporal dynamics and lightweight
latency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D
object representations, and extends them into the temporal domain via Hungarian
assignment with our temporal matching cost, yielding a unified spatio-temporal
scene graph. A latency tag is embedded to enable LVLM planners to
retrospectively query past scene states, thereby resolving local-remote state
mismatches caused by transmission delays. To further reduce redundancy and
highlight task-relevant cues, we propose a task-oriented subgraph filtering
strategy that produces compact inputs for the planner. ST-OVSG generalizes to
novel categories and enhances planning robustness against transmission latency
without requiring fine-tuning. Experiments show that our method achieves 74
percent node accuracy on the Replica benchmark, outperforming ConceptGraph.
Notably, in the latency-robustness experiment, the LVLM planner assisted by
ST-OVSG achieved a planning success rate of 70.5 percent.

</details>


### [960] [Liaohe-CobotMagic-PnP: an Imitation Learning Dataset of Intelligent Robot for Industrial Applications](https://arxiv.org/abs/2509.23111)
*Chen Yizhe,Wang Qi,Hu Dongxiao,Jingzhe Fang,Liu Sichao,Zixin An,Hongliang Niu,Haoran Liu,Li Dong,Chuanfen Feng,Lan Dapeng,Liu Yu,Zhibo Pang*

Main category: cs.RO

TL;DR: 提出了一种用于复杂工业环境下机器人感知与控制的工业级多模态干扰数据集，融合视觉、力矩和关节状态等多维传感器数据，具有高时间同步精度和抗振动能力，提升了模型验证的鲁棒性和机器人操作稳定性。


<details>
  <summary>Details</summary>
Motivation: 动态环境干扰导致环境状态与机器人行为之间存在高度非线性与强耦合关系，现有机器人数据集在多模态传感器数据融合表征动态环境方面仍面临挑战。

Method: 构建了一个包含尺寸、颜色和光照变化等多维干扰特征的工业级多模态干扰数据集，采用高精度传感器通过ROS系统同步采集视觉、力矩和关节状态数据，并设计了几何相似度超过85%和标准化光照梯度的实验场景以增强现实代表性。

Result: 实验结果表明该数据集能够提升模型验证的鲁棒性，并改善机器人在动态、强干扰环境下的操作稳定性。数据已公开发布。

Conclusion: 该工业级多模态干扰数据集有效支持了复杂环境下机器人感知与控制的研究，具备良好的真实性和同步性，有助于推动智能制造中协作机器人技术的发展。

Abstract: In Industry 4.0 applications, dynamic environmental interference induces
highly nonlinear and strongly coupled interactions between the environmental
state and robotic behavior. Effectively representing dynamic environmental
states through multimodal sensor data fusion remains a critical challenge in
current robotic datasets. To address this, an industrial-grade multimodal
interference dataset is presented, designed for robotic perception and control
under complex conditions. The dataset integrates multi-dimensional interference
features including size, color, and lighting variations, and employs
high-precision sensors to synchronously collect visual, torque, and joint-state
measurements. Scenarios with geometric similarity exceeding 85\% and
standardized lighting gradients are included to ensure real-world
representativeness. Microsecond-level time-synchronization and
vibration-resistant data acquisition protocols, implemented via the Robot
Operating System (ROS), guarantee temporal and operational fidelity.
Experimental results demonstrate that the dataset enhances model validation
robustness and improves robotic operational stability in dynamic,
interference-rich environments. The dataset is publicly available
at:https://modelscope.cn/datasets/Liaoh_LAB/Liaohe-CobotMagic-PnP.

</details>


### [961] [FTACT: Force Torque aware Action Chunking Transformer for Pick-and-Reorient Bottle Task](https://arxiv.org/abs/2509.23112)
*Ryo Watanabe,Maxime Alvarez,Pablo Ferreiro,Pavel Savkin,Genki Sano*

Main category: cs.RO

TL;DR: 提出一种结合力和力矩感知的多模态模仿学习策略，用于提升零售环境中机器人对饮料瓶的抓取与重定向任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 在零售环境中，操纵机器人常因接触密集的边缘情况需要人工远程操作，尤其是仅靠视觉线索难以处理直立倒下的饮料瓶的精细操作。

Method: 将力和力矩感知融入Action Chunking Transformer模型，实现基于图像、关节状态及力/力矩信号的端到端学习，并在Telexistence Inc的Ghost单臂平台上部署。

Result: 实验表明，该方法在按压和放置阶段显著提高了任务成功率，尤其在视觉受限的情况下，力和力矩信号有效辅助了接触转换的检测与利用。

Conclusion: 结合现代模仿学习架构与轻量级力/力矩传感，为扩展零售场景中的机器人操作提供了可行路径。

Abstract: Manipulator robots are increasingly being deployed in retail environments,
yet contact rich edge cases still trigger costly human teleoperation. A
prominent example is upright lying beverage bottles, where purely visual cues
are often insufficient to resolve subtle contact events required for precise
manipulation. We present a multimodal Imitation Learning policy that augments
the Action Chunking Transformer with force and torque sensing, enabling
end-to-end learning over images, joint states, and forces and torques. Deployed
on Ghost, single-arm platform by Telexistence Inc, our approach improves
Pick-and-Reorient bottle task by detecting and exploiting contact transitions
during pressing and placement. Hardware experiments demonstrate greater task
success compared to baseline matching the observation space of ACT as an
ablation and experiments indicate that force and torque signals are beneficial
in the press and place phases where visual observability is limited, supporting
the use of interaction forces as a complementary modality for contact rich
skills. The results suggest a practical path to scaling retail manipulation by
combining modern imitation learning architectures with lightweight force and
torque sensing.

</details>


### [962] [EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation](https://arxiv.org/abs/2509.23118)
*Zeyi Li,Zhe Tang,Kyeong Soo Kim,Sihao Li,Jeremy S. Smith*

Main category: cs.RO

TL;DR: 提出了一种融合Wi-Fi RSSI指纹识别、LiDAR SLAM和IMU的室内定位与导航框架，通过扩展卡尔曼滤波实现多传感器数据融合，显著提高了定位精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi RSSI指纹定位精度不足，而LiDAR方案成本高且复杂，难以满足高精度室内定位需求。

Method: 结合DNN-based Wi-Fi RSSI指纹粗定位、基于Gmapping的LiDAR SLAM构建占据栅格地图，并利用IMU进行动态定位，通过扩展卡尔曼滤波（EKF）融合多传感器数据，抑制噪声与漂移误差。

Result: 实验证明该框架在多种路径下均保持稳定精度，二维平均误差为0.2449–0.3781米；相比之下，Wi-Fi RSSI误差最高达1.3404米，LiDAR/IMU因累积漂移误差为0.6233–2.8803米。

Conclusion: 所提出的多传感器融合框架有效克服了单一技术的局限性，在成本与性能之间实现了良好平衡，适用于高精度室内定位与导航应用。

Abstract: Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting
cannot meet the growing demand for accurate indoor localization and navigation
due to its lower accuracy, while solutions based on light detection and ranging
(LiDAR) can provide better localization performance but is limited by their
higher deployment cost and complexity. To address these issues, we propose a
novel indoor localization and navigation framework integrating Wi-Fi RSSI
fingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and
inertial measurement unit (IMU) navigation based on an extended Kalman filter
(EKF). Specifically, coarse localization by deep neural network (DNN)-based
Wi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a
Gmapping-based SLAM to generate an occupancy grid map and output high-frequency
attitude estimates, which is followed by EKF prediction-update integrating
sensor information while effectively suppressing Wi-Fi-induced noise and IMU
drift errors. Multi-group real-world experiments conducted on the IR building
at Xi'an Jiaotong-Liverpool University demonstrates that the proposed
multi-sensor fusion framework suppresses the instability caused by individual
approaches and thereby provides stable accuracy across all path configurations
with mean two-dimensional (2D) errors ranging from 0.2449 m to 0.3781 m. In
contrast, the mean 2D errors of Wi-Fi RSSI fingerprinting reach up to 1.3404 m
in areas with severe signal interference, and those of LiDAR/IMU localization
are between 0.6233 m and 2.8803 m due to cumulative drift.

</details>


### [963] [LAGEA: Language Guided Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.23155)
*Abdul Monaf Chowdhury,Akm Moshiur Rahman Mazumder,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.RO

TL;DR: 本文提出了LAGEA框架，利用自然语言作为反馈信号，帮助具身智能体通过视觉-语言模型的反思来诊断错误并改进决策，在Meta-World MT10任务中显著提升了操作成功率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人缺乏从自身错误中学习的系统性方法，而自然语言有望作为结构化的错误推理信号，提升智能体的自我纠错能力。

Method: 提出LAGEA框架，利用视觉语言模型生成 episodic、schema-constrained 的语言反思，将关键时刻与视觉状态对齐，并将其转化为时序锚定的塑形奖励，结合自适应失败感知系数指导强化学习。

Result: 在Meta-World MT10基准上，相比SOTA方法，随机目标和固定目标的成功率分别提升9.0%和5.3%，且收敛更快。

Conclusion: 结构化且时序接地的自然语言反馈是一种有效机制，可教会机器人自我反思错误并做出更优决策。

Abstract: Robotic manipulation benefits from foundation models that describe goals, but
today's agents still lack a principled way to learn from their own mistakes. We
ask whether natural language can serve as feedback, an error reasoning signal
that helps embodied agents diagnose what went wrong and correct course. We
introduce LAGEA (Language Guided Embodied Agents), a framework that turns
episodic, schema-constrained reflections from a vision language model (VLM)
into temporally grounded guidance for reinforcement learning. LAGEA summarizes
each attempt in concise language, localizes the decisive moments in the
trajectory, aligns feedback with visual state in a shared representation, and
converts goal progress and feedback agreement into bounded, step-wise shaping
rewardswhose influence is modulated by an adaptive, failure-aware coefficient.
This design yields dense signals early when exploration needs direction and
gracefully recedes as competence grows. On the Meta-World MT10 embodied
manipulation benchmark, LAGEA improves average success over the
state-of-the-art (SOTA) methods by 9.0% on random goals and 5.3% on fixed
goals, while converging faster. These results support our hypothesis: language,
when structured and grounded in time, is an effective mechanism for teaching
robots to self-reflect on mistakes and make better choices. Code will be
released soon.

</details>


### [964] [Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion](https://arxiv.org/abs/2509.23185)
*Ziyi Zhou,Qian Meng,Hadas Kress-Gazit,Ye Zhao*

Main category: cs.RO

TL;DR: 提出了一种用于四足机器人在动态未知地形上运动的集成规划框架，结合反应式合成与混合整数凸规划，实现鲁棒、自适应且实时可行的步态规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式实时选足或计算成本高的轨迹优化，难以兼顾鲁棒性、适应性与实时性。

Method: 结合反应式合成为基础生成符号级控制器，并利用混合整数凸规划（MICP）进行每一步的动态可行落脚点规划；引入符号修复机制减少对昂贵MICP求解的依赖，并通过在线实时重规划与延迟感知协调实现离线合成与在线执行的无缝衔接。

Result: 在仿真和真实硬件实验中验证了该框架在散乱踏石和钢筋等复杂危险地形下的有效性，能够识别缺失的运动技能并做出安全响应。

Conclusion: 该框架提升了四足机器人在动态不可预见地形上的适应性与安全性，实现了高效、可靠的实时运动规划。

Abstract: We present an integrated planning framework for quadrupedal locomotion over
dynamically changing, unforeseen terrains. Existing methods often depend on
heuristics for real-time foothold selection-limiting robustness and
adaptability-or rely on computationally intensive trajectory optimization
across complex terrains and long horizons. In contrast, our approach combines
reactive synthesis for generating correct-by-construction symbolic-level
controllers with mixed-integer convex programming (MICP) for dynamic and
physically feasible footstep planning during each symbolic transition. To
reduce the reliance on costly MICP solves and accommodate specifications that
may be violated due to physical infeasibility, we adopt a symbolic repair
mechanism that selectively generates only the required symbolic transitions.
During execution, real-time MICP replanning based on actual terrain data,
combined with runtime symbolic repair and delay-aware coordination, enables
seamless bridging between offline synthesis and online operation. Through
extensive simulation and hardware experiments, we validate the framework's
ability to identify missing locomotion skills and respond effectively in
safety-critical environments, including scattered stepping stones and rebar
scenarios.

</details>


### [965] [CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation](https://arxiv.org/abs/2509.23203)
*Kai Yang,Tianlin Zhang,Zhengbo Wang,Zedong Chu,Xiaolong Wu,Yang Cai,Mu Xu*

Main category: cs.RO

TL;DR: 本文提出CE-Nav，一种两阶段（IL-then-RL）框架，通过解耦通用几何推理与形态特定的动力学适应，实现跨多种机器人形态的局部导航策略泛化。


<details>
  <summary>Details</summary>
Motivation: 现有导航策略难以在不同机器人形态间泛化，常受限于高成本的实体特定数据、规划与控制的紧密耦合，以及确定性模型无法处理多模态决策的“灾难性平均”问题。

Method: 第一阶段，使用模仿学习训练一个与形态无关的通用专家模型VelFlow（条件标准化流模型），从经典规划器生成的大规模数据集中学习合法运动动作的完整分布；第二阶段，针对新机器人，冻结该专家模型作为先验，通过在线强化学习训练一个轻量级的动力学感知优化器，以最小环境交互快速适应目标机器人的动力学特性。

Result: 在四足、双足和四旋翼等多种机器人上实验表明，CE-Nav在显著降低适应成本的同时达到最先进性能，并在真实世界部署中验证了其有效性。

Conclusion: CE-Nav提供了一种高效且可扩展的解决方案，能够实现跨机器人形态的通用导航系统构建。

Abstract: Generalizing local navigation policies across diverse robot morphologies is a
critical challenge. Progress is often hindered by the need for costly and
embodiment-specific data, the tight coupling of planning and control, and the
"disastrous averaging" problem where deterministic models fail to capture
multi-modal decisions (e.g., turning left or right). We introduce CE-Nav, a
novel two-stage (IL-then-RL) framework that systematically decouples universal
geometric reasoning from embodiment-specific dynamic adaptation. First, we
train an embodiment-agnostic General Expert offline using imitation learning.
This expert, a conditional normalizing flow model named VelFlow, learns the
full distribution of kinematically-sound actions from a large-scale dataset
generated by a classical planner, completely avoiding real robot data and
resolving the multi-modality issue. Second, for a new robot, we freeze the
expert and use it as a guiding prior to train a lightweight, Dynamics-Aware
Refiner via online reinforcement learning. This refiner rapidly learns to
compensate for the target robot's specific dynamics and controller
imperfections with minimal environmental interaction. Extensive experiments on
quadrupeds, bipeds, and quadrotors show that CE-Nav achieves state-of-the-art
performance while drastically reducing adaptation cost. Successful real-world
deployments further validate our approach as an efficient and scalable solution
for building generalizable navigation systems.

</details>


### [966] [Simulated Annealing for Multi-Robot Ergodic Information Acquisition Using Graph-Based Discretization](https://arxiv.org/abs/2509.23214)
*Benjamin Wong,Aaron Weber,Mohamed M. Safwat,Santosh Devasia,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: 提出一种基于模拟退火的主动信息采集方法，通过逐步调整玻尔兹曼分布的冷度参数，实现从均匀采样到最优采样分布的平滑过渡，有效降低多机器人系统在未知噪声环境下的不确定性。


<details>
  <summary>Details</summary>
Motivation: 在多机器人主动信息采集任务中，各区域的观测噪声水平未知且初始估计不可靠，导致采样分布波动大、采集质量不一致，需设计一种能动态适应噪声估计变化的采样策略。

Method: 采用模拟退火思想，以估计的采样熵为能量，通过调节玻尔兹曼分布的冷度参数，生成目标采样分布；结合遍历覆盖机制分配机器人采样任务。

Result: 仿真结果表明，与均匀采样和直接遍历搜索相比，该方法在瞬态和渐近熵方面均有显著改善；并通过TurtleBot群系统实验证明了其物理可行性。

Conclusion: 所提方法能有效平衡多机器人在未知噪声环境中的采样过程，提升信息采集的一致性与效率，具备实际应用潜力。

Abstract: One of the goals of active information acquisition using multi-robot teams is
to keep the relative uncertainty in each region at the same level to maintain
identical acquisition quality (e.g., consistent target detection) in all the
regions. To achieve this goal, ergodic coverage can be used to assign the
number of samples according to the quality of observation, i.e., sampling noise
levels. However, the noise levels are unknown to the robots. Although this
noise can be estimated from samples, the estimates are unreliable at first and
can generate fluctuating values. The main contribution of this paper is to use
simulated annealing to generate the target sampling distribution, starting from
uniform and gradually shifting to an estimated optimal distribution, by varying
the coldness parameter of a Boltzmann distribution with the estimated sampling
entropy as energy. Simulation results show a substantial improvement of both
transient and asymptotic entropy compared to both uniform and direct-ergodic
searches. Finally, a demonstration is performed with a TurtleBot swarm system
to validate the physical applicability of the algorithm.

</details>


### [967] [GLUE: Global-Local Unified Encoding for Imitation Learning via Key-Patch Tracking](https://arxiv.org/abs/2509.23220)
*Ye Chen,Zichen Zhou,Jianyu Dou,Te Cui,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: 本文提出了一种名为GLUE的全局-局部统一编码框架，用于基于关键图像块跟踪的视觉模仿学习，通过文本引导机制选择和跟踪任务相关的关键局部特征，并融合全局与局部信息以提升在复杂分布外环境下的策略鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在复杂且存在遮挡的分布外（OOD）环境中，全局视觉表征容易受到干扰，导致模仿学习策略性能下降；需要一种能够增强视觉注意力对任务相关物体聚焦并减少协变量偏移的方法。

Method: 提出GLUE框架，采用文本引导机制选取并跟踪关键图像块作为局部表征，设计新颖的融合结构，使全局图像块特征查询局部图像块以提取关键信息，生成异质性低且细粒度的局部特征，从而实现全局与局部信息的统一编码。

Result: 实验表明，GLUE在模拟和真实世界多种任务中均表现出色，在模拟环境中比最强基线提升17.6%，真实场景中提升36.3%，真实世界泛化设置下提升58.3%。

Conclusion: GLUE通过融合全局上下文与任务相关的局部特征，有效对齐训练与测试分布，显著提升了模仿学习策略在复杂OOD环境中的鲁棒性和泛化能力。

Abstract: In recent years, visual representation learning has gained widespread
attention in robotic imitation learning. However, in complex
Out-of-Distribution(OOD) settings characterized by clutter and occlusion, the
attention of global visual representations can be diluted or interfered,
leading to degraded policy performance. The invariance of local representations
for task-relevant objects offers a solution. By efficiently utilizing these
local representations, training and testing data can be mapped to a more
similar feature space, thereby mitigating the covariate shift problem.
Accordingly, we propose GLUE, a global-local unified encoding framework for
imitation learning based on key-patch tracking. GLUE selects and tracks
key-patches as critical local representations by employing a text-guided
mechanism. It features a novel fusion framework where global patch features
query local patches to distill essential information, yielding fine-grained
local features with low heterogeneity relative to the global context. This
fused representation steers the robot's visual attention toward task-relevant
objects and preserves precise global context, which together align the training
and testing distributions into a similar and task-informative feature space,
ultimately enhancing the robustness of the imitation learning policy.
Experiments demonstrate that GLUE achieves strong performance across diverse
tasks in both simulation and real-world settings, outperforming the strongest
baseline by 17.6% in simulation, 36.3% in real-world environments, and 58.3% on
real-world generalization settings. The project website of GLUE is available at
https://GLUE666.github.io/.

</details>


### [968] [SAC-Loco: Safe and Adjustable Compliant Quadrupedal Locomotion](https://arxiv.org/abs/2509.23223)
*Aoqian Zhang,Zixuan Zhuang,Chunzheng Wang,Shuzhi Sam Ge,Fan Shi,Cheng Xiang*

Main category: cs.RO

TL;DR: 提出一种切换策略框架，实现四足机器人在外部扰动下的柔顺性和安全性控制。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人控制方法缺乏动物所具有的自适应柔顺性，在强扰动下容易失稳。

Method: 采用师生强化学习框架训练可调柔顺性的力控策略；基于捕获点概念设计安全策略；引入可恢复性网络预测失效风险并切换策略。

Result: 机器人能够在无显式力传感的情况下实现可调节柔顺，并在大扰动下通过策略切换保持稳定。

Conclusion: 该框架有效结合柔顺控制与安全控制，提升了四足机器人在强扰动下的鲁棒性和适应性。

Abstract: Quadruped robots are designed to achieve agile locomotion by mimicking legged
animals. However, existing control methods for quadrupeds often lack one of the
key capabilities observed in animals: adaptive and adjustable compliance in
response to external disturbances. Most locomotion controllers do not provide
tunable compliance and tend to fail under large perturbations. In this work, we
propose a switched policy framework for compliant and safe quadruped
locomotion. First, we train a force compliant policy with adjustable compliance
levels using a teacher student reinforcement learning framework, eliminating
the need for explicit force sensing. Next, we develop a safe policy based on
the capture point concept to stabilize the robot when the compliant policy
fails. Finally, we introduce a recoverability network that predicts the
likelihood of failure and switches between the compliant and safe policies.
Together, this framework enables quadruped robots to achieve both force
compliance and robust safety when subjected to severe external disturbances.

</details>


### [969] [Leave No Observation Behind: Real-time Correction for VLA Action Chunks](https://arxiv.org/abs/2509.23224)
*Kohei Sendai,Maxime Alvarez,Tatsuya Matsushima,Yutaka Matsuo,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 提出了一种轻量级实时动作块校正模块A2C2，可在不重新训练基础模型的情况下提升视觉-语言-动作（VLA）模型在延迟和长视野下的反应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型使用动作块预测以提高效率和时间一致性，但在推理延迟和长执行周期下会降低反应速度，因此需要一种能实时修正动作块的方法。

Method: 设计了一个异步动作块校正（A2C2）模块，每控制步运行一次，结合最新观测、VLA预测的动作、动作在块中的位置编码及基础策略特征，输出逐步行修正；该模块无需重新训练基础策略，可与现有异步执行方案（如RTC）结合。

Result: 在Kinetix任务集（12项任务）和LIBERO Spatial上，相比RTC，A2C2在增加延迟和执行周期下分别提升了23个百分点和7个百分点的成功率，并在零延迟下也表现出对长周期更好的鲁棒性；且校正模块开销极小。

Conclusion: A2C2是一种高效、即插即用的机制，能够在保持基础VLA模型能力的同时恢复闭环响应能力，适用于高容量动作块策略的实时控制部署。

Abstract: To improve efficiency and temporal coherence, Vision-Language-Action (VLA)
models often predict action chunks; however, this action chunking harms
reactivity under inference delay and long horizons. We introduce Asynchronous
Action Chunk Correction (A2C2), which is a lightweight real-time chunk
correction head that runs every control step and adds a time-aware correction
to any off-the-shelf VLA's action chunk. The module combines the latest
observation, the predicted action from VLA (base action), a positional feature
that encodes the index of the base action within the chunk, and some features
from the base policy, then outputs a per-step correction. This preserves the
base model's competence while restoring closed-loop responsiveness. The
approach requires no retraining of the base policy and is orthogonal to
asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic
Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent
success rate improvements across increasing delays and execution horizons (+23%
point and +7% point respectively, compared to RTC), and also improves
robustness for long horizons even with zero injected delay. Since the
correction head is small and fast, there is minimal overhead compared to the
inference of large VLA models. These results indicate that A2C2 is an
effective, plug-in mechanism for deploying high-capacity chunking policies in
real-time control.

</details>


### [970] [Online Dynamic Goal Recognition in Gym Environments](https://arxiv.org/abs/2509.23244)
*Shamir Matan,Elhadad Osher,Nageris Ben,Mirsky Reuth*

Main category: cs.RO

TL;DR: 本文介绍了两个开源框架gr-libs和gr-envs，旨在为目标识别（GR）算法的开发、评估和比较提供标准化、可扩展且可复现的平台。


<details>
  <summary>Details</summary>
Motivation: 由于目标识别领域在基准、领域和评估协议上存在不一致，导致研究分散，因此需要统一的工具来促进该领域的研究进展。

Method: 提出并实现了两个互补的开源框架：gr-libs包含基于MDP的目标识别基线、诊断工具和评估工具；gr-envs提供了一系列适用于动态和目标导向行为的环境，并确保与标准强化学习工具包兼容。

Result: 这两个框架共同支持Gym兼容环境下的目标识别算法开发与评估，已在GitHub和PyPI上开源发布。

Conclusion: gr-libs和gr-envs为推进目标识别研究提供了标准化、可扩展且可复现的平台，有助于统一和促进该领域的发展。

Abstract: Goal Recognition (GR) is the task of inferring an agent's intended goal from
partial observations of its behavior, typically in an online and one-shot
setting. Despite recent advances in model-free GR, particularly in applications
such as human-robot interaction, surveillance, and assistive systems, the field
remains fragmented due to inconsistencies in benchmarks, domains, and
evaluation protocols.
  To address this, we introduce gr-libs
(https://github.com/MatanShamir1/gr_libs) and gr-envs
(https://github.com/MatanShamir1/gr_envs), two complementary open-source
frameworks that support the development, evaluation, and comparison of GR
algorithms in Gym-compatible environments. gr-libs includes modular
implementations of MDP-based GR baselines, diagnostic tools, and evaluation
utilities. gr-envs provides a curated suite of environments adapted for dynamic
and goal-directed behavior, along with wrappers that ensure compatibility with
standard reinforcement learning toolkits. Together, these libraries offer a
standardized, extensible, and reproducible platform for advancing GR research.
Both packages are open-source and available on GitHub and PyPI.

</details>


### [971] [Preventing Robotic Jailbreaking via Multimodal Domain Adaptation](https://arxiv.org/abs/2509.23281)
*Francesco Marchiori,Rohan Sinha,Christopher Agia,Alexander Robey,George J. Pappas,Mauro Conti,Marco Pavone*

Main category: cs.RO

TL;DR: J-DAPT是一个轻量级的多模态越狱检测框架，通过注意力机制融合文本和视觉信息，并结合领域自适应技术，有效提升机器人应用中视觉语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型在机器人环境中易受越狱攻击，现有数据驱动防御方法在缺乏专用数据集的领域泛化能力有限。

Method: 提出J-DAPT框架，采用基于注意力的融合机制整合文本与视觉嵌入，并通过域适应将通用越狱数据集与特定领域数据对齐。

Result: 在自动驾驶、海洋机器人和四足机器人导航等场景中，J-DAPT将检测准确率提升至近100%，且开销极小。

Conclusion: J-DAPT为机器人应用中的视觉语言模型提供了一种高效、实用的安全防御方案。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) are
increasingly deployed in robotic environments but remain vulnerable to
jailbreaking attacks that bypass safety mechanisms and drive unsafe or
physically harmful behaviors in the real world. Data-driven defenses such as
jailbreak classifiers show promise, yet they struggle to generalize in domains
where specialized datasets are scarce, limiting their effectiveness in robotics
and other safety-critical contexts. To address this gap, we introduce J-DAPT, a
lightweight framework for multimodal jailbreak detection through
attention-based fusion and domain adaptation. J-DAPT integrates textual and
visual embeddings to capture both semantic intent and environmental grounding,
while aligning general-purpose jailbreak datasets with domain-specific
reference data. Evaluations across autonomous driving, maritime robotics, and
quadruped navigation show that J-DAPT boosts detection accuracy to nearly 100%
with minimal overhead. These results demonstrate that J-DAPT provides a
practical defense for securing VLMs in robotic applications. Additional
materials are made available at: https://j-dapt.github.io.

</details>


### [972] [A Novel Narrow Region Detector for Sampling-Based Planners' Efficiency: Match Based Passage Identifier](https://arxiv.org/abs/2509.23288)
*Yafes Enes Şahiner,Esat Yusuf Gündoğdu,Volkan Sezer*

Main category: cs.RO

TL;DR: 本文提出了一种新的采样器，用于在狭窄通道环境中提高自主系统路径规划的效率，通过占用栅格地图确定性地识别这些区域并增加采样密度。


<details>
  <summary>Details</summary>
Motivation: 解决现有概率路径规划方法在狭窄通道环境中表现不佳的问题。

Method: 利用占用栅格地图，设计一种能够确定性识别狭窄通道并在此类区域增加采样密度的新采样器。

Result: 在特定和随机仿真环境以及真实环境中进行基准测试，结果表明该算法在规划时间和里程碑数量上优于基线采样器。

Conclusion: 所提出的采样器显著提升了路径规划在复杂环境中的性能，具有良好的应用前景。

Abstract: Autonomous technology, which has become widespread today, appears in many
different configurations such as mobile robots, manipulators, and drones. One
of the most important tasks of these vehicles during autonomous operations is
path planning. In the literature, path planners are generally divided into two
categories: probabilistic and deterministic methods. In the analysis of
probabilistic methods, the common problem of almost all methods is observed in
narrow passage environments. In this paper, a novel sampler is proposed that
deterministically identifies narrow passage environments using occupancy grid
maps and accordingly increases the amount of sampling in these regions. The
codes of the algorithm is provided as open source. To evaluate the performance
of the algorithm, benchmark studies are conducted in three distinct categories:
specific and random simulation environments, and a real-world environment. As a
result, it is observed that our algorithm provides higher performance in
planning time and number of milestones compared to the baseline samplers.

</details>


### [973] [Distributed Multi-Robot Multi-Target Simultaneous Search and Tracking in an Unknown Non-convex Environment](https://arxiv.org/abs/2509.23308)
*Jun Chen,Jiaqing Ma,Philip Dames*

Main category: cs.RO

TL;DR: 提出一种集成前沿探索、Lloyd算法覆盖和传感器多目标跟踪的机器人运动规划框架，用于在未知非凸环境中同时优化环境探索与目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 在室内和地下等复杂非结构化环境中，现有方法难以同时高效完成环境探索与目标持续高精度跟踪，亟需统一框架解决多任务协同问题。

Method: 融合基于前沿的探索策略、基于Lloyd算法的全覆盖策略和基于传感器的多目标跟踪策略，通过综合控制实现探索与跟踪的平衡。

Result: MATLAB仿真结果表明，该方法在覆盖率、探索效率和目标跟踪精度方面优于传统方法。

Conclusion: 所提框架有效解决了未知非凸环境中多机器人系统在探索过程中兼顾全面覆盖与高精度目标跟踪的难题，具有在环境监测和救援任务中的应用潜力。

Abstract: In unknown non-convex environments, such as indoor and underground spaces,
deploying a fleet of robots to explore the surroundings while simultaneously
searching for and tracking targets of interest to maintain high-precision data
collection represents a fundamental challenge that urgently requires resolution
in applications such as environmental monitoring and rescue operations. Current
research has made significant progress in addressing environmental exploration,
information search, and target tracking problems, but has yet to establish a
framework for simultaneously optimizing these tasks in complex environments. In
this paper, we propose a novel motion planning algorithm framework that
integrates three control strategies: a frontier-based exploration strategy, a
guaranteed coverage strategy based on Lloyd's algorithm, and a sensor-based
multi-target tracking strategy. By incorporating these three strategies, the
proposed algorithm balances coverage search and high-precision active tracking
during exploration. Our approach is validated through a series of MATLAB
simulations, demonstrating validity and superiority over standard approaches.

</details>


### [974] [GUARD: Toward a Compromise between Traditional Control and Learning for Safe Robot Systems](https://arxiv.org/abs/2509.23312)
*Johannes A. Gaus,Junheon Yoon,Woo-Jeong Baek,Seungwon Choi,Suhan Park,Jaeheung Park*

Main category: cs.RO

TL;DR: 本文提出了GUARD框架，结合传统控制与不确定性感知的主动学习方法，通过概率核优化实现机器人风险感知决策和实时避障。


<details>
  <summary>Details</summary>
Motivation: 解决传统控制方法与学习算法在机器人安全性和效率之间权衡的挑战，提升机器人应用的安全性、效率和灵活性。

Method: 将反应式模型预测轮廓控制（RMPCC）与可实时进行不确定性源归因的迭代最近点（ICP）算法结合，采用基于主动学习的概率核优化技术实现不确定性感知。

Result: 实验研究表明GUARD具有高性能，能够有效处理机器人领域中‘安全性’概念的模糊性，具备良好的实时避障能力。

Conclusion: GUARD框架成功融合了传统控制与学习方法，在保证安全性的同时提升了灵活性和效率，未来有望进一步拓展其应用范围。

Abstract: This paper presents the framework \textbf{GUARD} (\textbf{G}uided robot
control via \textbf{U}ncertainty attribution and prob\textbf{A}bilistic kernel
optimization for \textbf{R}isk-aware \textbf{D}ecision making) that combines
traditional control with an uncertainty-aware perception technique using active
learning with real-time capability for safe robot collision avoidance. By doing
so, this manuscript addresses the central challenge in robotics of finding a
reasonable compromise between traditional methods and learning algorithms to
foster the development of safe, yet efficient and flexible applications. By
unifying a reactive model predictive countouring control (RMPCC) with an
Iterative Closest Point (ICP) algorithm that enables the attribution of
uncertainty sources online using active learning with real-time capability via
a probabilistic kernel optimization technique, \emph{GUARD} inherently handles
the existing ambiguity of the term \textit{safety} that exists in robotics
literature. Experimental studies indicate the high performance of \emph{GUARD},
thereby highlighting the relevance and need to broaden its applicability in
future.

</details>


### [975] [Space Robotics Bench: Robot Learning Beyond Earth](https://arxiv.org/abs/2509.23328)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 本文提出了一个名为Space Robotics Bench的开源仿真框架，用于支持太空机器人学习，通过模块化架构和大规模并行仿真环境，促进在复杂外星条件下自主系统的发展与评估。


<details>
  <summary>Details</summary>
Motivation: 由于技术验证成本高昂且数据稀缺，太空领域中的机器人学习发展受限，因此需要一个高效的仿真平台来推动研究进展。

Method: 设计了一个模块化的开源仿真框架，集成了程序化生成和大规模并行模拟，并提供了涵盖多种任务场景的基准测试套件，采用标准强化学习算法建立性能基线。

Result: 实验结果揭示了当前方法在泛化、端到端学习、自适应控制和仿真到现实迁移方面的局限性，并展示了该框架能够生成可在真实世界运行的策略。

Conclusion: Space Robotics Bench是一个有价值的工具，可用于开发、评估和部署面向深空探索的鲁棒自主系统。

Abstract: The growing ambition for space exploration demands robust autonomous systems
that can operate in unstructured environments under extreme extraterrestrial
conditions. The adoption of robot learning in this domain is severely hindered
by the prohibitive cost of technology demonstrations and the limited
availability of data. To bridge this gap, we introduce the Space Robotics
Bench, an open-source simulation framework for robot learning in space. It
offers a modular architecture that integrates on-demand procedural generation
with massively parallel simulation environments to support the creation of vast
and diverse training distributions for learning-based agents. To ground
research and enable direct comparison, the framework includes a comprehensive
suite of benchmark tasks that span a wide range of mission-relevant scenarios.
We establish performance baselines using standard reinforcement learning
algorithms and present a series of experimental case studies that investigate
key challenges in generalization, end-to-end learning, adaptive control, and
sim-to-real transfer. Our results reveal insights into the limitations of
current methods and demonstrate the utility of the framework in producing
policies capable of real-world operation. These contributions establish the
Space Robotics Bench as a valuable resource for developing, benchmarking, and
deploying the robust autonomous systems required for the final frontier.

</details>


### [976] [Robust Orientation Estimation with TRIAD-aided Manifold EKF](https://arxiv.org/abs/2509.23456)
*Arjun Sadananda,Ravi Banavar,Kavi Arya*

Main category: cs.RO

TL;DR: 本文提出将TRIAD算法的次优特性引入流形扩展卡尔曼滤波（Manifold EKF）中，以减轻磁力计读数在俯仰和横滚轴确定中的影响，并通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 磁力计易受校准和外部磁场干扰，影响姿态确定精度，因此需要改进现有滤波算法以提高鲁棒性。

Method: 将TRIAD算法的次优估计特性融合到Manifold EKF中，利用其在俯仰和横滚轴上的优势来抑制磁力计干扰。

Result: 实验结果表明，所提出的方法能有效降低磁力计扰动对姿态估计的影响，提升姿态确定的准确性。

Conclusion: 结合TRIAD特性的Manifold EKF在姿态确定中表现出更强的抗干扰能力，是一种有效的改进方案。

Abstract: The manifold extended Kalman filter (Manifold EKF) has found extensive
application for attitude determination. Magnetometers employed as sensors for
such attitude determination are easily prone to disturbances by their
sensitivity to calibration and external magnetic fields. The TRIAD (Tri-Axial
Attitude Determination) algorithm is well known as a sub-optimal attitude
estimator. In this article, we incorporate this sub-optimal feature of the
TRIAD in mitigating the influence of the magnetometer reading in the pitch and
roll axis determination in the Manifold EKF algorithm. We substantiate our
results with experiments.

</details>


### [977] [Multi-Modal Manipulation via Multi-Modal Policy Consensus](https://arxiv.org/abs/2509.23468)
*Haonan Chen,Jiaming Xu,Hongyu Chen,Kaiwen Hong,Binghao Huang,Chaoqi Liu,Jiayuan Mao,Yunzhu Li,Yilun Du,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 提出一种基于扩散模型和路由器网络的多模态策略分解方法，实现对视觉、触觉等感知模态的自适应融合，提升机器人操作中多模态推理的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的特征拼接方法在多模态融合中存在模态主导和架构僵化问题，难以有效处理接触丰富的任务中稀疏但关键的触觉信号，且无法灵活添加或缺失模态。

Method: 将策略分解为多个专用扩散模型，每个模型处理单一感知模态，并通过学习共识权重的路由器网络自适应地融合各模态输出，支持新模态的增量集成。

Result: 在RLBench仿真环境及遮挡抓取、手中勺子重定向、拼图插入等真实任务中显著优于特征拼接基线，表现出对物理扰动和传感器损坏的鲁棒性，并通过扰动分析验证了模态间自适应权重转移。

Conclusion: 该方法实现了更优的多模态融合，提升了机器人在复杂操作任务中的性能与适应性，具有良好的扩展性和实际应用潜力。

Abstract: Effectively integrating diverse sensory modalities is crucial for robotic
manipulation. However, the typical approach of feature concatenation is often
suboptimal: dominant modalities such as vision can overwhelm sparse but
critical signals like touch in contact-rich tasks, and monolithic architectures
cannot flexibly incorporate new or missing modalities without retraining. Our
method factorizes the policy into a set of diffusion models, each specialized
for a single representation (e.g., vision or touch), and employs a router
network that learns consensus weights to adaptively combine their
contributions, enabling incremental of new representations. We evaluate our
approach on simulated manipulation tasks in {RLBench}, as well as real-world
tasks such as occluded object picking, in-hand spoon reorientation, and puzzle
insertion, where it significantly outperforms feature-concatenation baselines
on scenarios requiring multimodal reasoning. Our policy further demonstrates
robustness to physical perturbations and sensor corruption. We further conduct
perturbation-based importance analysis, which reveals adaptive shifts between
modalities.

</details>


### [978] [Ask, Reason, Assist: Decentralized Robot Collaboration via Language and Logic](https://arxiv.org/abs/2509.23506)
*Dan BW Choe,Sundhar Vinodh Sangeetha,Steven Emanuel,Chih-Yuan Chiu,Samuel Coogan,Shreyas Kousik*

Main category: cs.RO

TL;DR: 提出一种去中心化的异构机器人协作框架，利用大语言模型（LLM）生成和理解自然语言求助请求，并结合信号时序逻辑（STL）与混合整数线性规划（MILP）实现高效的任务协调与帮助选择。


<details>
  <summary>Details</summary>
Motivation: 应对仓储等场景中异构机器人团队在未知冲突下的协作难题，提升系统自主协调能力，减少对集中式控制的依赖。

Method: 采用去中心化架构：当机器人检测到冲突时，使用LLM判断是否需要帮助并广播自然语言求助请求；其他机器人通过基于BNF语法的LLM-STL推理机制评估请求并返回协助方案，以MILP求解最优响应；请求者根据对系统总完成时间的影响选择最佳协助者。

Result: 实验表明，相比最近助手启发式策略，该方法能显著减少增加的完工时间（makespan），并通过多候选评估接近集中式‘Oracle’基线的性能，同时避免了高信息交换开销。

Conclusion: 所提框架有效支持异构机器人间的自然语言驱动协作，在保证语义准确性和任务可行性的前提下，实现了高效、去中心化的冲突解决与任务协调。

Abstract: Increased robot deployment, such as in warehousing, has revealed a need for
seamless collaboration among heterogeneous robot teams to resolve unforeseen
conflicts. To address this challenge, we propose a novel decentralized
framework that enables robots to request and provide help. The process begins
when a robot detects a conflict and uses a Large Language Model (LLM) to decide
whether external assistance is required. If so, it crafts and broadcasts a
natural language (NL) help request. Potential helper robots reason over the
request and respond with offers of assistance, including information about the
effect on their ongoing tasks. Helper reasoning is implemented via an LLM
grounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar,
ensuring syntactically valid NL-to-STL translations, which are then solved as a
Mixed Integer Linear Program (MILP). Finally, the requester robot selects a
helper by reasoning over the expected increase in system-level total task
completion time. We evaluated our framework through experiments comparing
different helper-selection strategies and found that considering multiple
offers allows the requester to minimize added makespan. Our approach
significantly outperforms heuristics such as selecting the nearest available
candidate helper robot, and achieves performance comparable to a centralized
"Oracle" baseline but without heavy information demands.

</details>


### [979] [Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso via Guided Reinforcement Learning](https://arxiv.org/abs/2509.23556)
*Curtis C. Johnson,Carlo Alessi,Egidio Falotico,Marc D. Killpack*

Main category: cs.RO

TL;DR: 本文提出了一种用于连续体软臂机器人全身操作的高速仿真框架，并实现了从仿真到真实环境的零样本策略迁移，在Baloo硬件平台上达到88%的成功率。


<details>
  <summary>Details</summary>
Motivation: 软体机器人在接触丰富的操作任务中具有优势，但由于其运动学和动力学的不确定性，给建模与控制带来挑战；现有方法难以稳定学习全身操作策略。

Method: 构建可在MuJoCo上单线程加速至350倍实时速度的仿真环境，结合强化学习与简单运动原语引导策略训练，实现零样本sim-to-real迁移。

Result: 在Baloo平台上成功实现六自由度全身操作，负载达10公斤，策略迁移成功率达88%；策略展现出再抓取、抗扰动等反应性行为，但也存在过度校正现象。

Conclusion: 这是首个在大型软体平台上实现无需微调的全身操作策略零样本迁移的工作，验证了仿真加速与运动原语引导对复杂软体操控任务的有效性。

Abstract: Whole-body manipulation is a powerful yet underexplored approach that enables
robots to interact with large, heavy, or awkward objects using more than just
their end-effectors. Soft robots, with their inherent passive compliance, are
particularly well-suited for such contact-rich manipulation tasks, but their
uncertainties in kinematics and dynamics pose significant challenges for
simulation and control. In this work, we address this challenge with a
simulation that can run up to 350x real time on a single thread in MuJoCo and
provide a detailed analysis of the critical tradeoffs between speed and
accuracy for this simulation. Using this framework, we demonstrate a successful
zero-shot sim-to-real transfer of a learned whole-body manipulation policy,
achieving an 88% success rate on the Baloo hardware platform. We show that
guiding RL with a simple motion primitive is critical to this success where
standard reward shaping methods struggled to produce a stable and successful
policy for whole-body manipulation. Furthermore, our analysis reveals that the
learned policy does not simply mimic the motion primitive. It exhibits
beneficial reactive behavior, such as re-grasping and perturbation recovery. We
analyze and contrast this learned policy against an open-loop baseline to show
that the policy can also exhibit aggressive over-corrections under
perturbation. To our knowledge, this is the first demonstration of forceful,
six-DoF whole-body manipulation using two continuum soft arms on a large-scale
platform (10 kg payloads), with zero-shot policy transfer.

</details>


### [980] [High Torque Density PCB Axial Flux Permanent Magnet Motor for Micro Robots](https://arxiv.org/abs/2509.23561)
*Jianren Wang,Jie Han,Abhinav Gupta,Deepak Pathak,Yang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于高密度互连（HDI）印刷电路板技术的微尺度轴向磁通永磁电机，通过48层PCB定子实现45%的铜填充率，解决了无齿轮直驱机器人对低速高扭矩、薄型化电机的需求。


<details>
  <summary>Details</summary>
Motivation: 传统绕线定子在小尺寸下铜填充率低，导致电阻增加，限制了连续扭矩输出，难以满足小型化准直驱机器人的需求。

Method: 采用HDI印刷电路板技术制造48层堆叠式PCB定子，结合电磁与热仿真分析优化电机设计，并制作原型进行实验验证。

Result: 实现了直径19mm、厚度5mm的微型AFPM电机，铜填充率达到创纪录的45%，显著降低电阻并提升连续扭矩输出。

Conclusion: 该设计为微型准直驱机器人提供了一种高效、高扭矩密度的电机解决方案，推动了小型化高性能驱动系统的发展。

Abstract: Quasi-direct-drive (QDD) actuation is transforming legged and manipulator
robots by eliminating high-ratio gearboxes, yet it demands motors that deliver
very high torque at low speed within a thin, disc-shaped joint envelope.
Axial-flux permanent-magnet (AFPM) machines meet these geometric and torque
requirements, but scaling them below a 20mm outer diameter is hampered by poor
copper fill in conventional wound stators, inflating resistance and throttling
continuous torque. This paper introduces a micro-scale AFPM motor that
overcomes these limitations through printed-circuit-board (PCB) windings
fabricated with advanced IC-substrate high-density interconnect (HDI)
technology. The resulting 48-layer stator-formed by stacking four 12-layer HDI
modules-achieves a record 45\% copper fill in a package only 5mm thick and 19mm
in diameter. We perform comprehensive electromagnetic and thermal analyses to
inform the motor design, then fabricate a prototype whose performance
characteristics are experimentally verified.

</details>


### [981] [RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation](https://arxiv.org/abs/2509.23563)
*Seungchan Kim,Omar Alama,Dmytro Kurdydyk,John Keller,Nikhil Keetha,Wenshan Wang,Yonatan Bisk,Sebastian Scherer*

Main category: cs.RO

TL;DR: RAVEN是一种基于3D记忆和行为树框架的空中语义导航方法，适用于非结构化户外环境，通过语义体素-射线地图实现长视距规划，并结合视觉语言模型提升目标稀疏场景下的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义导航方法在室内或结构化环境中表现良好，但在大范围、非结构化的户外环境中受限于短视行为或离线预计算，难以实现实时自适应导航。

Method: 提出RAVEN框架，采用空间一致的语义体素-射线图作为持久化记忆，结合短距离体素搜索与长距离射线搜索，并利用大型视觉语言模型提供辅助线索，由行为树协调不同行为模式以实现自适应切换。

Result: 在10个逼真户外仿真环境中完成100项语义任务，RAVEN相比基线方法性能提升85.25%，并在真实户外环境中通过无人机实验证明其可部署性。

Conclusion: RAVEN有效解决了户外长距离语义导航中的感知稀疏与适应性差问题，具备良好的仿真与实际应用表现，适用于复杂非结构化环境中的空中探索任务。

Abstract: Aerial outdoor semantic navigation requires robots to explore large,
unstructured environments to locate target objects. Recent advances in semantic
navigation have demonstrated open-set object-goal navigation in indoor
settings, but these methods remain limited by constrained spatial ranges and
structured layouts, making them unsuitable for long-range outdoor search. While
outdoor semantic navigation approaches exist, they either rely on reactive
policies based on current observations, which tend to produce short-sighted
behaviors, or precompute scene graphs offline for navigation, limiting
adaptability to online deployment. We present RAVEN, a 3D memory-based,
behavior tree framework for aerial semantic navigation in unstructured outdoor
environments. It (1) uses a spatially consistent semantic voxel-ray map as
persistent memory, enabling long-horizon planning and avoiding purely reactive
behaviors, (2) combines short-range voxel search and long-range ray search to
scale to large environments, (3) leverages a large vision-language model to
suggest auxiliary cues, mitigating sparsity of outdoor targets. These
components are coordinated by a behavior tree, which adaptively switches
behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor
simulation environments over 100 semantic tasks, encompassing single-object
search, multi-class, multi-instance navigation and sequential task changes.
Results show RAVEN outperforms baselines by 85.25% in simulation and
demonstrate its real-world applicability through deployment on an aerial robot
in outdoor field tests.

</details>


### [982] [GES-UniGrasp: A Two-Stage Dexterous Grasping Strategy With Geometry-Based Expert Selection](https://arxiv.org/abs/2509.23567)
*Fangting Xu,Jilin Zhu,Xiaoming Gu,Jianzhong Tang*

Main category: cs.RO

TL;DR: 提出ContactGrasp数据集和基于几何的专家选择框架（GES），实现自然、高成功率的灵巧抓取。


<details>
  <summary>Details</summary>
Motivation: 现有基于抓取先验的强化学习方法常导致不自然的行为，缺乏对腕部朝向和拇指-食指协调的显式建模。

Method: 构建包含773个物体的ContactGrasp数据集，引入任务相关的腕部姿态和拇指-食指捏合协调；采用几何聚类划分物体形状，设计两阶段的几何专家选择（GES）框架，为不同几何类型选择专用抓取专家。

Result: 在训练集和测试集上分别达到99.4%和96.3%的抓取成功率，表现出良好的泛化能力和自然的抓取姿态。

Conclusion: ContactGrasp数据集与GES框架有效提升了机器人灵巧抓取的自然性、适应性和跨类别泛化能力。

Abstract: Robust and human-like dexterous grasping of general objects is a critical
capability for advancing intelligent robotic manipulation in real-world
scenarios. However, existing reinforcement learning methods guided by grasp
priors often result in unnatural behaviors. In this work, we present
\textit{ContactGrasp}, a robotic dexterous pre-grasp and grasp dataset that
explicitly accounts for task-relevant wrist orientation and thumb-index
pinching coordination. The dataset covers 773 objects in 82 categories,
providing a rich foundation for training human-like grasp strategies. Building
upon this dataset, we perform geometry-based clustering to group objects by
shape, enabling a two-stage Geometry-based Expert Selection (GES) framework
that selects among specialized experts for grasping diverse object geometries,
thereby enhancing adaptability to diverse shapes and generalization across
categories. Our approach demonstrates natural grasp postures and achieves high
success rates of 99.4\% and 96.3\% on the train and test sets, respectively,
showcasing strong generalization and high-quality grasp execution.

</details>


### [983] [Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D Keypoints](https://arxiv.org/abs/2509.23575)
*Jianshu Hu,Lidi Wang,Shujia Li,Yunpeng Jiang,Xiao Li,Paul Weng,Yutong Ban*

Main category: cs.RO

TL;DR: 提出了一种名为CLAP的粗到细语言对齐操作策略，通过任务分解、VLM微调用于3D关键点预测和3D感知表示，在仿真和真实机器人实验中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的分层策略在面对新指令和环境变化时泛化能力不足，需要提升机器人3D操作任务中的样本效率和精确性。

Method: 引入Coarse-to-fine Language-Aligned manipulation Policy (CLAP)，结合任务分解、基于视觉语言模型（VLM）的3D关键点预测微调以及3D感知表征学习。

Result: 在GemBench基准上比现有方法平均成功率高出12%，且仅使用五分之一的训练轨迹；在真实场景中仅用10个演示即可泛化到新指令和环境中。

Conclusion: CLAP显著提升了机器人在3D操作任务中的泛化能力和样本效率，适用于复杂多变的实际应用场景。

Abstract: Hierarchical coarse-to-fine policy, where a coarse branch predicts a region
of interest to guide a fine-grained action predictor, has demonstrated
significant potential in robotic 3D manipulation tasks by especially enhancing
sample efficiency and enabling more precise manipulation. However, even
augmented with pre-trained models, these hierarchical policies still suffer
from generalization issues. To enhance generalization to novel instructions and
environment variations, we propose Coarse-to-fine Language-Aligned manipulation
Policy (CLAP), a framework that integrates three key components: 1) task
decomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware
representation. Through comprehensive experiments in simulation and on a real
robot, we demonstrate its superior generalization capability. Specifically, on
GemBench, a benchmark designed for evaluating generalization, our approach
achieves a 12\% higher average success rate than the SOTA method while using
only 1/5 of the training trajectories. In real-world experiments, our policy,
trained on only 10 demonstrations, successfully generalizes to novel
instructions and environments.

</details>


### [984] [Encoding Material Safety using Control Barrier Functions for Soft Actuator Control](https://arxiv.org/abs/2509.23623)
*Nicholas Pagliocca,Behrad Koohbor,Mitja Trkov*

Main category: cs.RO

TL;DR: 本文提出了基于应变能函数的材料安全形式化定义，并利用高阶控制屏障函数（HOCBF）和二次规划反馈控制来保证软机器人的材料安全性。


<details>
  <summary>Details</summary>
Motivation: 随着软机器人技术向实际应用发展，亟需明确定义安全性并识别其潜在风险，尤其是由于材料变形带来的模型不准确和材料失效问题。

Method: 通过引入基于应变能函数的材料安全正式定义，使用高阶控制屏障函数（HOCBF）结合二次规划反馈控制，在包含惯性和粘性效应的可压缩超弹性管状执行器上实现全状态反馈控制。

Result: 仿真结果表明，所提出的方法能够有效强制执行材料安全规范，确保系统运行在安全集合内。

Conclusion: 该工作为软机器人提供了一种可验证的安全控制框架，有助于应对由材料非线性和模型不确定性带来的安全挑战。

Abstract: Until recently, the concept of soft robot safety was an informal notion,
often attributed solely to the fact that soft robots are less likely to damage
their operating environment than rigid robots. As the field moves toward
feedback control for practical applications, it becomes increasingly important
to define what safety means and to characterize how soft robots can become
unsafe. The unifying theme of soft robotics is to achieve useful functionality
through deformation. Consequently, limitations in constitutive model accuracy
and risks of material failure are inherent to all soft robots and pose a key
challenge in designing provably safe controllers. This work introduces a formal
definition of material safety based on strain energy functions and provides a
controller that enforces it. We characterize safe and unsafe sets of an
incompressible hyperelastic material and demonstrate that safety can be
enforced using a high-order control barrier function (HOCBF) with quadratic
program-based feedback control. As a case study, we consider a pressurized
hyperelastic tube with inertial effects, first-order viscous effects, and
full-state feedback. Simulation results verify that the proposed methodology
can enforce the material safety specification.

</details>


### [985] [KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe Egocentric Legged Locomotion](https://arxiv.org/abs/2509.23650)
*Peizhuo Li,Hongyi Li,Yuxuan Ma,Linnan Chang,Xinrong Yang,Ruiqi Yu,Yifeng Zhang,Yuhong Cao,Qiuguo Zhu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出了一种名为KiVi的动觉-视觉空间融合框架，通过结合本体感觉与视觉感知，提升四足机器人在复杂、非结构化户外环境中的稳定行走能力。


<details>
  <summary>Details</summary>
Motivation: 视觉信息易受遮挡、反光和光照变化影响，导致运动不稳定，因此需要一种更鲁棒的感知融合方法来实现可靠的腿式运动。

Method: KiVi框架分离动觉（本体感觉）和视觉空间两个通路，以本体感觉作为稳定的主干，选择性地融合视觉信息用于地形感知和避障，并引入记忆增强注意力机制。

Result: 实验表明，该方法使四足机器人能在多种地形上稳定行走，在训练中未见的分布外视觉噪声和遮挡下仍保持鲁棒性，适用于非结构化户外环境。

Conclusion: KiVi通过模态平衡且可整合的设计，实现了对视觉线索的稳健解析与基于本体感觉的回退稳定性，显著提升了真实世界中腿部机器人运动的可靠性与适应性。

Abstract: Vision-based locomotion has shown great promise in enabling legged robots to
perceive and adapt to complex environments. However, visual information is
inherently fragile, being vulnerable to occlusions, reflections, and lighting
changes, which often cause instability in locomotion. Inspired by animal
sensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial
integration framework, where kinesthetics encodes proprioceptive sensing of
body motion and visuospatial reasoning captures visual perception of
surrounding terrain. Specifically, KiVi separates these pathways, leveraging
proprioception as a stable backbone while selectively incorporating vision for
terrain awareness and obstacle avoidance. This modality-balanced, yet
integrative design, combined with memory-enhanced attention, allows the robot
to robustly interpret visual cues while maintaining fallback stability through
proprioception. Extensive experiments show that our method enables quadruped
robots to stably traverse diverse terrains and operate reliably in unstructured
outdoor environments, remaining robust to out-of-distribution (OOD) visual
noise and occlusion unseen during training, thereby highlighting its
effectiveness and applicability to real-world legged locomotion.

</details>


### [986] [HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod Robot](https://arxiv.org/abs/2509.23651)
*Xinrong Yang,Peizhuo Li,Hongyi Li,Junkai Lu,Linnan Chang,Yuhong Cao,Yifeng Zhang,Ge Sun,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出了一种基于学习的分层全身操作框架HeLoM，用于六足机器人在真实环境中稳定推动不同大小和未知物理属性的重物。


<details>
  <summary>Details</summary>
Motivation: 机器人在现实环境中常需移动与自身重量相当的物体，传统抓取方式复杂且低效，而推动作为一种非抓取操作策略更具优势，但面临施力与稳定性双重挑战。

Method: 受多足昆虫协作策略启发，设计了分层框架HeLoM，包含高层规划器规划推进行为和目标姿态，底层控制器保证运动稳定性并生成动力学一致的关节动作，通过前肢协调推动与后肢辅助推进实现多肢体协同控制。

Result: 在仿真中训练的策略可直接部署于真实机器人，无需额外微调；实验表明机器人能稳定推动不同尺寸、未知物理特性的箱子至指定位置。

Conclusion: HeLoM框架有效解决了重物推动中的力控与稳定性问题，实现了从仿真到现实的无缝迁移，验证了多肢体协调在非抓取操作中的优势。

Abstract: Robots in real-world environments are often required to move/manipulate
objects comparable in weight to their own bodies. Compared to grasping and
carrying, pushing provides a more straightforward and efficient non-prehensile
manipulation strategy, avoiding complex grasp design while leveraging direct
contact to regulate an object's pose. Achieving effective pushing, however,
demands both sufficient manipulation forces and the ability to maintain
stability, which is particularly challenging when dealing with heavy or
irregular objects. To address these challenges, we propose HeLoM, a
learning-based hierarchical whole-body manipulation framework for a hexapod
robot that exploits coordinated multi-limb control. Inspired by the cooperative
strategies of multi-legged insects, our framework leverages redundant contact
points and high degrees of freedom to enable dynamic redistribution of contact
forces. HeLoM's high-level planner plans pushing behaviors and target object
poses, while its low-level controller maintains locomotion stability and
generates dynamically consistent joint actions. Our policies trained in
simulation are directly deployed on real robots without additional fine-tuning.
This design allows the robot to maintain balance while exerting continuous and
controllable pushing forces through coordinated foreleg interaction and
supportive hind-leg propulsion. We validate the effectiveness of HeLoM through
both simulation and real-world experiments. Results show that our framework can
stably push boxes of varying sizes and unknown physical properties to
designated goal poses in the real world.

</details>


### [987] [Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models](https://arxiv.org/abs/2509.23655)
*Rokas Bendikas,Daniel Dijkman,Markus Peschl,Sanjay Haresh,Pietro Mazzaglia*

Main category: cs.RO

TL;DR: Oat-VLA提出一种面向视觉-语言-动作模型的对象-智能体中心化分词方法，显著减少视觉token数量，提升训练效率并在仿真和真实场景中优于OpenVLA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM转VLA模型因视觉输入的分词方式导致计算成本过高，需提高效率。

Method: 引入对象中心化表征学习思想，设计针对场景物体和智能体自身视觉信息的新型分词机制（Oat-VLA）。

Result: Oat-VLA将视觉token大幅压缩至几个，训练收敛速度至少是OpenVLA的两倍，并在LIBERO任务集和真实世界抓取放置任务中表现更优。

Conclusion: Oat-VLA通过引入对象与智能体中心化的分词策略，有效降低计算开销，实现高效且高性能的VLA训练。

Abstract: Vision-Language-Action (VLA) models offer a pivotal approach to learning
robotic manipulation at scale by repurposing large pre-trained
Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs
for robotic domains comes with an unnecessarily high computational cost, which
we attribute to the tokenization scheme of visual inputs. In this work, we aim
to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric
Tokenization for VLAs. Building on the insights of object-centric
representation learning, our method introduces an inductive bias towards scene
objects and the agent's own visual information. As a result, we find that
Oat-VLA can drastically reduce the number of visual tokens to just a few tokens
without sacrificing performance. We reveal that Oat-VLA converges at least
twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in
diverse real-world pick and place tasks.

</details>


### [988] [Certifiably Optimal State Estimation and Robot Calibration Using Trace-Constrained SDP](https://arxiv.org/abs/2509.23656)
*Liangting Wu,Roberto Tron*

Main category: cs.RO

TL;DR: 本文提出了一种基于迹约束半定规划（SDP）的框架，用于解决机器人中的非凸问题，通过引入定制化变量和梯度优化方法，有效恢复秩为1的解，并在多个机器人任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多机器人中的非凸问题可通过SDP松弛获得全局最优解，但实际效果依赖于得到秩为1的矩阵，通常需额外收紧条件。本文旨在通过迹约束提升秩为1解的恢复能力。

Method: 引入具有固定迹的半正定矩阵变量来表示旋转、平移等机器人量，并设计梯度-based refinement方法将松弛解投影向低秩、低成本候选解，结合对偶问题进行最优性验证。

Result: 在PnP估计、手眼标定和双机器人系统标定等任务中展示了该方法的有效性，能够高效恢复精确的秩为1解。

Conclusion: 迹约束SDP框架能有效表达多种机器人任务，结合梯度优化可显著提升实用性能，且通过模块化“虚拟机器人”抽象便于推广使用。

Abstract: Many nonconvex problems in robotics can be relaxed into convex formulations
via semidefinite programming (SDP), which offers the advantage of global
optimality. The practical quality of these solutions, however, critically
depends on achieving rank-1 matrices, a condition that typically requires
additional tightening. In this work, we focus on trace-constrained SDPs, where
the decision variables are positive semidefinite (PSD) matrices with fixed
trace values. These additional constraints not only capture important
structural properties but also facilitate first-order methods for recovering
rank-1 solutions. We introduce customized fixed-trace variables and constraints
to represent common robotic quantities such as rotations and translations,
which can be exactly recovered when the corresponding variables are rank-1. To
further improve practical performance, we develop a gradient-based refinement
procedure that projects relaxed SDP solutions toward rank-1, low-cost
candidates, which can then be certified for global optimality via the dual
problem. We demonstrate that many robotics tasks can be expressed within this
trace-constrained SDP framework, and showcase its effectiveness through
simulations in perspective-n-point (PnP) estimation, hand-eye calibration, and
dual-robot system calibration. To support broader use, we also introduce a
modular ``virtual robot'' abstraction that simplifies modeling across different
problem settings.

</details>


### [989] [MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload Adaptation](https://arxiv.org/abs/2509.23705)
*Jun Chen,Mingjia Chen,Shinkyu Park*

Main category: cs.RO

TL;DR: 提出了一种新的多机器人动态覆盖路径规划（MDCPP）算法，通过高斯混合模型估计剩余工作量，并使用容量约束的Voronoi图分配区域，仿真结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人覆盖路径规划假设机器人以固定速度移动，导致任务分配不均和完成时间增加，难以适应实际应用中机器人需根据任务调整速度的需求。

Method: 提出MDCPP算法，利用高斯混合模型动态估算每个机器人的剩余工作量，并采用容量约束的Voronoi图分配覆盖区域，进一步开发了适用于通信范围受限网络的分布式实现。

Result: 仿真结果显示MDCPP在覆盖效率上优于现有的扫描算法，并量化了通信范围对覆盖性能的影响。

Conclusion: MDCPP能有效改善多机器人系统的负载均衡和覆盖效率，适用于二维环境中的完全覆盖任务。

Abstract: Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing
paths for multiple robots to effectively cover a large area of interest.
Conventional approaches to MCPP typically assume that robots move at fixed
velocities, which is often unrealistic in real-world applications where robots
must adapt their speeds based on the specific coverage tasks assigned to
them.Consequently, conventional approaches often lead to imbalanced workload
distribution among robots and increased completion time for coverage tasks. To
address this, we introduce a novel Multi-robot Dynamic Coverage Path Planning
(MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP
dynamically estimates each robot's remaining workload by approximating the
target distribution with Gaussian mixture models, and assigns coverage regions
using a capacity-constrained Voronoi diagram. We further develop a distributed
implementation of MDCPP for range-constrained robotic networks. Simulation
results validate the efficacy of MDCPP, showing qualitative improvements and
superior performance compared to an existing sweeping algorithm, and a
quantifiable impact of communication range on coverage efficiency.

</details>


### [990] [DA-MMP: Learning Coordinated and Accurate Throwing with Dynamics-Aware Motion Manifold Primitives](https://arxiv.org/abs/2509.23721)
*Chi Chu,Huazhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Dynamics-Aware Motion Manifold Primitives（DA-MMP）的动态操作运动生成框架，用于解决动态操控任务中规划与执行之间的动力学差距问题，并在现实世界的环投任务中实现了高成功率，甚至超越人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖手动设计的动作参数化，难以生成复杂任务所需的协调动作；同时运动规划受控不准、接触不确定性和空气动力效应影响，导致规划与实际执行轨迹偏差大。

Method: 扩展运动流形基元以支持变长轨迹，通过紧凑参数化从大规模规划动作数据集中学习高质量流形，并在潜在空间中使用少量真实试验数据训练条件流匹配模型，以生成考虑执行动力学的投掷轨迹。

Result: 在环投任务中生成了协调且平滑的运动轨迹，在真实环境中达到高成功率，超越人类专家表现，并能泛化到训练范围之外的新目标。

Conclusion: DA-MMP有效解决了动态操作中的动力学差距问题，通过结合大规模规划数据与少量真实数据，在现实任务中实现了高性能和良好泛化能力。

Abstract: Dynamic manipulation is a key capability for advancing robot performance,
enabling skills such as tossing. While recent learning-based approaches have
pushed the field forward, most methods still rely on manually designed action
parameterizations, limiting their ability to produce the highly coordinated
motions required in complex tasks. Motion planning can generate feasible
trajectories, but the dynamics gap-stemming from control inaccuracies, contact
uncertainties, and aerodynamic effects-often causes large deviations between
planned and executed trajectories. In this work, we propose Dynamics-Aware
Motion Manifold Primitives (DA-MMP), a motion generation framework for
goal-conditioned dynamic manipulation, and instantiate it on a challenging
real-world ring-tossing task. Our approach extends motion manifold primitives
to variable-length trajectories through a compact parametrization and learns a
high-quality manifold from a large-scale dataset of planned motions. Building
on this manifold, a conditional flow matching model is trained in the latent
space with a small set of real-world trials, enabling the generation of
throwing trajectories that account for execution dynamics. Experiments show
that our method can generate coordinated and smooth motion trajectories for the
ring-tossing task. In real-world evaluations, it achieves high success rates
and even surpasses the performance of trained human experts. Moreover, it
generalizes to novel targets beyond the training range, indicating that it
successfully learns the underlying trajectory-dynamics mapping.

</details>


### [991] [LocoFormer: Generalist Locomotion via Long-context Adaptation](https://arxiv.org/abs/2509.23745)
*Min Liu,Deepak Pathak,Ananye Agarwal*

Main category: cs.RO

TL;DR: LocoFormer是一种通用的、适用于多种身体结构的运动控制模型，能够在未知机器人形态和动力学条件下实现自适应控制。


<details>
  <summary>Details</summary>
Motivation: 传统运动控制器需要针对特定机器人进行手动调参，缺乏泛化能力，难以应对形态和动态变化。

Method: 采用大规模强化学习，结合程序化生成的机器人和强域随机化训练；并显著延长策略的上下文长度以跨越多个 episode，实现长期记忆与适应。

Result: LocoFormer 可成功部署于多种未见过的轮式和足式机器人，在体重变化、电机故障等干扰下仍保持鲁棒控制，并能在多次试验中从跌倒等失败中学习，实现跨 episode 的适应能力。

Conclusion: 该研究表明，通过大规模训练和长上下文建模，可以构建具有强泛化能力的通用运动控制器，为未来机器人技能的基础模型提供了可行路径。

Abstract: Modern locomotion controllers are manually tuned for specific embodiments. We
present LocoFormer, a generalist omni-bodied locomotion model that can control
previously unseen legged and wheeled robots, even without precise knowledge of
their kinematics. LocoFormer is able to adapt to changes in morphology and
dynamics at test time. We find that two key choices enable adaptation. First,
we train massive scale RL on procedurally generated robots with aggressive
domain randomization. Second, in contrast to previous policies that are myopic
with short context lengths, we extend context by orders of magnitude to span
episode boundaries. We deploy the same LocoFormer to varied robots and show
robust control even with large disturbances such as weight change and motor
failures. In extreme scenarios, we see emergent adaptation across episodes,
LocoFormer learns from falls in early episodes to improve control strategies in
later ones. We believe that this simple, yet general recipe can be used to
train foundation models for other robotic skills in the future. Videos at
generalist-locomotion.github.io.

</details>


### [992] [High-Precision Climbing Robot Localization Using Planar Array UWB/GPS/IMU/Barometer Integration](https://arxiv.org/abs/2509.23801)
*Shuning Zhang,Renjing Xu,Zhanchen Zhu,Xiangyu Chen,Yunheng Wang,Xu Jiang,Peibo Duan*

Main category: cs.RO

TL;DR: 提出了一种基于多传感器融合的攀爬机器人高精度定位方法，结合UWB、GPS、IMU和气压计，利用注意力机制和无迹卡尔曼滤波提升复杂高空环境下的定位精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为解决复杂高海拔环境中单一传感器定位精度不足和可靠性差的问题，特别是GPS遮挡和UWB非视距误差。

Method: 设计了基于注意力机制的融合算法（AMFA），融合平面阵列UWB、GPS、IMU和气压计数据；构建端到端神经网络模型处理UWB和气压计数据，并引入多模态注意力机制进行自适应融合，最后使用无迹卡尔曼滤波（UKF）优化轨迹。

Result: 实验证明该方法定位精度达到0.48米，最大误差低于1.50米，优于GPS/INS-EKF等基线算法，具有更强的鲁棒性。

Conclusion: 所提出的多传感器融合系统能有效提升攀爬机器人在复杂高空环境中的定位精度和稳定性，适用于存在传感器遮挡和干扰的实际场景。

Abstract: To address the need for high-precision localization of climbing robots in
complex high-altitude environments, this paper proposes a multi-sensor fusion
system that overcomes the limitations of single-sensor approaches. Firstly, the
localization scenarios and the problem model are analyzed. An integrated
architecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating
planar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and
barometer is designed to handle challenges such as GPS occlusion and UWB
Non-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference
models for UWB and barometer are developed, along with a multimodal attention
mechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied
to refine the trajectory, improving accuracy and robustness. Finally,
real-world experiments show that the method achieves 0.48 m localization
accuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such
as GPS/INS-EKF and demonstrating stronger robustness.

</details>


### [993] [Fostering Robots: A Governance-First Conceptual Framework for Domestic, Curriculum-Based Trajectory Collection](https://arxiv.org/abs/2509.23821)
*Federico Pablo-Marti,Carlos Mir Fernandez*

Main category: cs.RO

TL;DR: 提出了一种以课程驱动、治理优先的家用机器人部署框架，强调长期、有计划的交互轨迹，并通过可量化的指标和符合欧盟治理标准的评估协议来形式化轨迹质量。


<details>
  <summary>Details</summary>
Motivation: 为了确保家用机器人在实际应用中的安全性和可控性，需要一种系统性的方法来指导其逐步部署和长期交互管理。

Method: 提出了一个概念性且可实证检验的‘机器人培育’框架，采用课程驱动和治理优先的设计原则，形式化定义了交互轨迹的质量度量与评估协议。

Result: 建立了符合欧盟治理标准的低资源实证路线图，为未来试点研究提供了严谨的验证基础。

Conclusion: 该框架为家用机器人的安全、可控部署提供了一个可行的路径，支持通过小规模实验逐步推进实际应用。

Abstract: We propose a conceptual, empirically testable framework for Robot Fostering,
-a curriculum-driven, governance-first approach to domestic robot deployments,
emphasizing long-term, curated interaction trajectories. We formalize
trajectory quality with quantifiable metrics and evaluation protocols aligned
with EU-grade governance standards, delineating a low-resource empirical
roadmap to enable rigorous validation through future pilot studies.

</details>


### [994] [Control Your Robot: A Unified System for Robot Control and Policy Deployment](https://arxiv.org/abs/2509.23823)
*Tian Nian,Weijie Ke,Yao Mu,Tianxing Chen,Shaolong Zhu,Bingshan Hu*

Main category: cs.RO

TL;DR: Control Your Robot 是一个模块化、通用的框架，旨在统一跨平台机器人控制中的数据采集与策略部署，通过标准化流程和统一API解决硬件接口和控制范式差异导致的工具链碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于硬件接口、数据格式和控制范式的差异，跨平台机器人控制面临工具链碎片化和部署缓慢的问题，亟需一个统一的解决方案。

Method: 提出 Control Your Robot 框架，采用模块化设计、统一API和闭环架构，支持灵活的机器人注册、双模式控制（遥操作与轨迹回放）以及从多模态数据采集到推理的无缝集成。

Result: 在单臂和双臂系统上的实验表明，该框架能高效、低延迟地进行数据采集，并有效支持基于模仿学习和视觉-语言-动作模型的策略学习，训练出的策略能紧密匹配专家示范。

Conclusion: Control Your Robot 框架实现了跨平台的可扩展且可复现的机器人学习，显著降低了异构机器人系统的开发与部署成本。

Abstract: Cross-platform robot control remains difficult because hardware interfaces,
data formats, and control paradigms vary widely, which fragments toolchains and
slows deployment. To address this, we present Control Your Robot, a modular,
general-purpose framework that unifies data collection and policy deployment
across diverse platforms. The system reduces fragmentation through a
standardized workflow with modular design, unified APIs, and a closed-loop
architecture. It supports flexible robot registration, dual-mode control with
teleoperation and trajectory playback, and seamless integration from multimodal
data acquisition to inference. Experiments on single-arm and dual-arm systems
show efficient, low-latency data collection and effective support for policy
learning with imitation learning and vision-language-action models. Policies
trained on data gathered by Control Your Robot match expert demonstrations
closely, indicating that the framework enables scalable and reproducible robot
learning across platforms.

</details>


### [995] [DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation](https://arxiv.org/abs/2509.23829)
*Kefei Zhu,Fengshuo Bai,YuanHao Xiang,Yishuai Cai,Xinglin Chen,Ruochong Li,Xingtao Wang,Hao Dong,Yaodong Yang,Xiaopeng Fan,Yuanpei Chen*

Main category: cs.RO

TL;DR: 本文提出了DexFlyWheel，一种可扩展的数据生成框架，通过自我改进的循环持续提升数据多样性，用于机器人灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作数据收集方法依赖人工示教或工程设计，数据多样性不足，限制了策略的泛化与扩展。

Method: 采用闭环流程：从少量种子演示开始，结合模仿学习（IL）提取人类行为，残差强化学习（RL）增强泛化能力，通过仿真生成轨迹，并在多样化环境中进行数据增强，迭代构建自改进的数据飞轮。

Result: DexFlyWheel在四项挑战性任务中生成了超过2000个多样化演示；训练出的策略在挑战测试集上平均成功率达81.9%，并通过数字孪生成功迁移到真实世界，在双臂抓取任务中达到78.3%的成功率。

Conclusion: DexFlyWheel实现了高效、可扩展的灵巧操作数据生成，显著提升了策略的泛化能力和现实迁移性能。

Abstract: Dexterous manipulation is critical for advancing robot capabilities in
real-world applications, yet diverse and high-quality datasets remain scarce.
Existing data collection methods either rely on human teleoperation or require
significant human engineering, or generate data with limited diversity, which
restricts their scalability and generalization. In this paper, we introduce
DexFlyWheel, a scalable data generation framework that employs a self-improving
cycle to continuously enrich data diversity. Starting from efficient seed
demonstrations warmup, DexFlyWheel expands the dataset through iterative
cycles. Each cycle follows a closed-loop pipeline that integrates Imitation
Learning (IL), residual Reinforcement Learning (RL), rollout trajectory
collection, and data augmentation. Specifically, IL extracts human-like
behaviors from demonstrations, and residual RL enhances policy generalization.
The learned policy is then used to generate trajectories in simulation, which
are further augmented across diverse environments and spatial configurations
before being fed back into the next cycle. Over successive iterations, a
self-improving data flywheel effect emerges, producing datasets that cover
diverse scenarios and thereby scaling policy performance. Experimental results
demonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across
four challenging tasks. Policies trained on our dataset achieve an average
success rate of 81.9\% on the challenge test sets and successfully transfer to
the real world through digital twin, achieving a 78.3\% success rate on
dual-arm lift tasks.

</details>


### [996] [MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control](https://arxiv.org/abs/2509.23960)
*Manan Tayal,Aditya Singh,Shishir Kolathaya,Somil Bansal*

Main category: cs.RO

TL;DR: 本文提出了一种名为MAD-PINN的去中心化物理信息机器学习框架，用于解决大规模多智能体系统中的状态约束最优控制问题，在保证安全性的同时实现了良好的性能与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多智能体系统中难以同时保证严格的安全性、高性能和可扩展性，尤其是在复杂动态环境中缺乏安全保证或过于保守。

Method: 采用基于增广图的SC-OCP重构方法，结合物理信息神经网络（PINN）逼近解；通过在小规模系统上训练值函数并以去中心化方式部署，引入基于Hamilton-Jacobi可达性的邻居选择策略和滚动时域执行机制以提升安全性和效率。

Result: 实验表明，MAD-PINN在多智能体导航任务中实现了优于现有方法的安全-性能权衡，具备良好的可扩展性，并能有效应对动态交互环境。

Conclusion: MAD-PINN为大规模多智能体系统的安全与性能协同优化提供了一个可扩展且高效的解决方案，具有较强的实用性与推广潜力。

Abstract: Co-optimizing safety and performance in large-scale multi-agent systems
remains a fundamental challenge. Existing approaches based on multi-agent
reinforcement learning (MARL), safety filtering, or Model Predictive Control
(MPC) either lack strict safety guarantees, suffer from conservatism, or fail
to scale effectively. We propose MAD-PINN, a decentralized physics-informed
machine learning framework for solving the multi-agent state-constrained
optimal control problem (MASC-OCP). Our method leverages an epigraph-based
reformulation of SC-OCP to simultaneously capture performance and safety, and
approximates its solution via a physics-informed neural network. Scalability is
achieved by training the SC-OCP value function on reduced-agent systems and
deploying them in a decentralized fashion, where each agent relies only on
local observations of its neighbours for decision-making. To further enhance
safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based
neighbour selection strategy to prioritize safety-critical interactions, and a
receding-horizon policy execution scheme that adapts to dynamic interactions
while reducing computational burden. Experiments on multi-agent navigation
tasks demonstrate that MAD-PINN achieves superior safety-performance
trade-offs, maintains scalability as the number of agents grows, and
consistently outperforms state-of-the-art baselines.

</details>


### [997] [Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras](https://arxiv.org/abs/2509.24094)
*Vignesh Ramanathan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文提出了一种名为Flash的轻量级视觉位置识别（VPR）系统，首次实现了基于事件相机的亚毫秒级VPR，通过利用活跃像素位置生成高效二进制帧并使用位运算计算相似性，在多个数据集上显著提升了召回率，并降低了定位延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的VPR方法依赖于稠密表示且需要数十到数百毫秒的数据积累，难以实现快速定位，因此需要一种能在极短时间内完成高精度VPR的方法。

Method: Flash利用事件相机中活跃像素的位置作为判别特征，将其编码为高效的二进制帧，并通过快速的位运算计算帧间相似度，再根据查询与参考帧中的事件活动水平进行归一化以提升匹配准确性。

Result: 在QCR-Event-Dataset上亚毫秒VPR的Recall@1提升了11.33倍，在8公里的Brisbane-Event-VPR数据集上提升了5.92倍，同时显著降低了‘正确匹配时间’（TCM），减少了机器人失位运行的时间。

Conclusion: Flash是首个实现亚毫秒级VPR的事件相机系统，具有低延迟、高效率和高召回率的优势，为自主导航中的快速定位提供了新范式。

Abstract: Visual Place Recognition (VPR) enables systems to identify previously visited
locations within a map, a fundamental task for autonomous navigation. Prior
works have developed VPR solutions using event cameras, which asynchronously
measure per-pixel brightness changes with microsecond temporal resolution.
However, these approaches rely on dense representations of the inherently
sparse camera output and require tens to hundreds of milliseconds of event data
to predict a place. Here, we break this paradigm with Flash, a lightweight VPR
system that predicts places using sub-millisecond slices of event data. Our
method is based on the observation that active pixel locations provide strong
discriminative features for VPR. Flash encodes these active pixel locations
using efficient binary frames and computes similarities via fast bitwise
operations, which are then normalized based on the relative event activity in
the query and reference frames. Flash improves Recall@1 for sub-millisecond VPR
over existing baselines by 11.33x on the indoor QCR-Event-Dataset and 5.92x on
the 8 km Brisbane-Event-VPR dataset. Moreover, our approach reduces the
duration for which the robot must operate without awareness of its position, as
evidenced by a localization latency metric we term Time to Correct Match (TCM).
To the best of our knowledge, this is the first work to demonstrate
sub-millisecond VPR using event cameras.

</details>


### [998] [Ancestry Tree Clustering for Particle Filter Diversity Maintenance](https://arxiv.org/abs/2509.24124)
*Ilari Vallivaara,Bingnan Duan,Yinhuan Dong,Tughrul Arslan*

Main category: cs.RO

TL;DR: 提出一种基于祖先树拓扑结构的粒子滤波线性时间多样性维护方法，通过聚类和适应度共享有效防止多模态环境中的早熟收敛。


<details>
  <summary>Details</summary>
Motivation: 在多模态环境中，传统粒子滤波容易因粒子多样性丧失而导致早熟收敛，需一种不依赖空间度量的高效多样性维护机制。

Method: 根据粒子祖先树的拓扑结构进行聚类，将具有足够大子树的近亲粒子分组，并结合组内适应度共享及未聚类粒子保护策略。

Result: 在多模态机器人仿真和真实室内环境中验证，相比确定性重采样和粒子高斯混合等方法，该算法成功率高，对紧凑性影响小，且在不同领域和初始条件下表现出强鲁棒性。

Conclusion: 该方法能在线性时间内有效维持粒子多样性，适用于复杂多模态场景，提升了粒子滤波的稳定性和泛化能力。

Abstract: We propose a method for linear-time diversity maintenance in particle
filtering. It clusters particles based on ancestry tree topology: closely
related particles in sufficiently large subtrees are grouped together. The main
idea is that the tree structure implicitly encodes similarity without the need
for spatial or other domain-specific metrics. This approach, when combined with
intra-cluster fitness sharing and the protection of particles not included in a
cluster, effectively prevents premature convergence in multimodal environments
while maintaining estimate compactness. We validate our approach in a
multimodal robotics simulation and a real-world multimodal indoor environment.
We compare the performance to several diversity maintenance algorithms from the
literature, including Deterministic Resampling and Particle Gaussian Mixtures.
Our algorithm achieves high success rates with little to no negative effect on
compactness, showing particular robustness to different domains and challenging
initial conditions.

</details>


### [999] [BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes](https://arxiv.org/abs/2509.24126)
*Athanasios Bacharis,Konstantinos D. Polyzos,Georgios B. Giannakis,Nikolaos Papanikolopoulos*

Main category: cs.RO

TL;DR: 提出了一种基于贝叶斯优化的视图规划（VP）框架，用于在噪声环境下高效、准确地进行农业场景的3D重建，并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决主动视觉中3D重建面临的相机位置和图像噪声问题，以及在不同农业环境中缺乏泛化能力的挑战。

Method: 采用基于重建质量的优化模型，结合结构光运动（structure-from-motion）方法，利用贝叶斯优化在无解析表达和高成本评估下高效搜索最优相机布局。

Result: 在模拟和真实农业环境中验证了该方法能以较少图像实现高精度3D重建，并对未知相似环境具有良好泛化性。

Conclusion: 所提VP框架在噪声存在和跨环境应用中表现出高效性和鲁棒性，适用于实际农业机器人任务中的主动视觉3D重建。

Abstract: Active vision (AV) has been in the spotlight of robotics research due to its
emergence in numerous applications including agricultural tasks such as
precision crop monitoring and autonomous harvesting to list a few. A major AV
problem that gained popularity is the 3D reconstruction of targeted
environments using 2D images from diverse viewpoints. While collecting and
processing a large number of arbitrarily captured 2D images can be arduous in
many practical scenarios, a more efficient solution involves optimizing the
placement of available cameras in 3D space to capture fewer, yet more
informative, images that provide sufficient visual information for effective
reconstruction of the environment of interest. This process termed as view
planning (VP), can be markedly challenged (i) by noise emerging in the location
of the cameras and/or in the extracted images, and (ii) by the need to
generalize well in other unknown similar agricultural environments without need
for re-optimizing or re-training. To cope with these challenges, the present
work presents a novel VP framework that considers a reconstruction
quality-based optimization formulation that relies on the notion of
`structure-from-motion' to reconstruct the 3D structure of the sought
environment from the selected 2D images. With no analytic expression of the
optimization function and with costly function evaluations, a Bayesian
optimization approach is proposed to efficiently carry out the VP process using
only a few function evaluations, while accounting for different noise cases.
Numerical tests on both simulated and real agricultural settings signify the
benefits of the advocated VP approach in efficiently estimating the optimal
camera placement to accurately reconstruct 3D environments of interest, and
generalize well on similar unknown environments.

</details>


### [1000] [Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress](https://arxiv.org/abs/2509.24129)
*Priyanka Mandikal,Jiaheng Hu,Shivin Dass,Sagnik Majumder,Roberto Martín-Martín,Kristen Grauman*

Main category: cs.RO

TL;DR: 本文提出了SPARTA，首个用于物体状态变化操作任务的统一框架，通过空间渐进式分割图和视觉技能感知可操作与已变换区域，实现高效学习与控制。


<details>
  <summary>Details</summary>
Motivation: 传统机器人操作主要关注物体的运动学状态改变，而许多现实世界任务涉及物体物理和视觉状态的渐进式变化（如压扁、涂抹、切割），缺乏统一的处理框架。

Method: 提出SPARTA框架，利用空间渐进式物体变化分割图和视觉技能识别可操作与已变换区域，生成结构化策略输入和密集奖励信号，并结合强化学习与贪心控制两种策略变体。

Result: 在真实机器人上验证了三种复杂任务，涵盖10种不同真实物体，相比稀疏奖励和视觉目标条件基线方法，在训练时间和准确性上均有显著提升。

Conclusion: 基于进度感知的视觉表征为广泛的物体状态操作任务提供了通用且有效的基础。

Abstract: Most robot manipulation focuses on changing the kinematic state of objects:
picking, placing, opening, or rotating them. However, a wide range of
real-world manipulation tasks involve a different class of object state
change--such as mashing, spreading, or slicing--where the object's physical and
visual state evolve progressively without necessarily changing its position. We
present SPARTA, the first unified framework for the family of object state
change manipulation tasks. Our key insight is that these tasks share a common
structural pattern: they involve spatially-progressing, object-centric changes
that can be represented as regions transitioning from an actionable to a
transformed state. Building on this insight, SPARTA integrates spatially
progressing object change segmentation maps, a visual skill to perceive
actionable vs. transformed regions for specific object state change tasks, to
generate a) structured policy observations that strip away appearance
variability, and b) dense rewards that capture incremental progress over time.
These are leveraged in two SPARTA policy variants: reinforcement learning for
fine-grained control without demonstrations or simulation; and greedy control
for fast, lightweight deployment. We validate SPARTA on a real robot for three
challenging tasks across 10 diverse real-world objects, achieving significant
improvements in training time and accuracy over sparse rewards and visual
goal-conditioned baselines. Our results highlight progress-aware visual
representations as a versatile foundation for the broader family of object
state manipulation tasks. Project website:
https://vision.cs.utexas.edu/projects/sparta-robot

</details>


### [1001] [A Novel Model for 3D Motion Planning for a Generalized Dubins Vehicle with Pitch and Yaw Rate Constraints](https://arxiv.org/abs/2509.24143)
*Deepak Prakash Kumar,Swaroop Darbha,Satyanarayana Gupta Manyam,David Casbeer*

Main category: cs.RO

TL;DR: 提出了一种新的3D运动规划建模方法和快速算法，适用于固定翼无人机，通过考虑完整的车辆姿态和双控制输入，生成比现有方法更短且可行的路径。


<details>
  <summary>Details</summary>
Motivation: 现有方法在描述无人机三维运动时未能充分考虑完整的姿态（如滚转、俯仰和偏航角），且通常使用单一控制输入，无法准确反映真实动力学特性。因此需要一种更精确的建模方式。

Method: 采用附体坐标系描述完整姿态，引入旋转最小化框架，并利用两个独立的控制输入（俯仰率和偏航率）建模；通过拼接球面、柱面和平面上的最优Dubins路径来构建完整路径。

Result: 数值仿真表明，该方法平均在10秒内生成可行路径，在大多数情况下生成的路径比现有方法更短。

Conclusion: 所提方法在建模精度和路径质量上优于传统方法，适用于固定翼无人机的高效3D运动规划。

Abstract: In this paper, we propose a new modeling approach and a fast algorithm for 3D
motion planning, applicable for fixed-wing unmanned aerial vehicles. The goal
is to construct the shortest path connecting given initial and final
configurations subject to motion constraints. Our work differs from existing
literature in two ways. First, we consider full vehicle orientation using a
body-attached frame, which includes roll, pitch, and yaw angles. However,
existing work uses only pitch and/or heading angle, which is insufficient to
uniquely determine orientation. Second, we use two control inputs to represent
bounded pitch and yaw rates, reflecting control by two separate actuators. In
contrast, most previous methods rely on a single input, such as path curvature,
which is insufficient for accurately modeling the vehicle's kinematics in 3D.
We use a rotation minimizing frame to describe the vehicle's configuration and
its evolution, and construct paths by concatenating optimal Dubins paths on
spherical, cylindrical, or planar surfaces. Numerical simulations show our
approach generates feasible paths within 10 seconds on average and yields
shorter paths than existing methods in most cases.

</details>


### [1002] [Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation](https://arxiv.org/abs/2509.24160)
*Tomoyuki Kagaya,Subramanian Lakshmi,Yuxuan Lou,Thong Jing Yuan,Jayashree Karlekar,Sugiri Pranata,Natsuki Murakami,Akira Kinose,Yang You*

Main category: cs.RO

TL;DR: 提出Memory Transfer Planning (MTP) 框架，利用跨环境的成功代码示例作为程序性知识，提升大语言模型在机器人操作中的适应性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在机器人操作中难以适应新环境，通常需要特定训练或固定提示，缺乏可迁移性。

Method: MTP通过生成初始计划、从代码记忆中检索成功案例，并将检索到的代码上下文适配到目标环境进行重新规划，无需更新模型参数。

Result: 在RLBench、CALVIN和真实机器人上的实验表明，MTP相比固定提示生成、简单检索等方法显著提升了成功率和适应性，且仿真构建的记忆在实际硬件上也有效。

Conclusion: MTP是一种实用框架，能有效利用程序性知识实现跨多样化操作场景的鲁棒LLM规划，增强对新环境的适应能力，并缩小仿真与现实部署之间的差距。

Abstract: Large language models (LLMs) are increasingly explored in robot manipulation,
but many existing methods struggle to adapt to new environments. Many systems
require either environment-specific policy training or depend on fixed prompts
and single-shot code generation, leading to limited transferability and manual
re-tuning. We introduce Memory Transfer Planning (MTP), a framework that
leverages successful control-code examples from different environments as
procedural knowledge, using them as in-context guidance for LLM-driven
planning. Specifically, MTP (i) generates an initial plan and code using LLMs,
(ii) retrieves relevant successful examples from a code memory, and (iii)
contextually adapts the retrieved code to the target setting for re-planning
without updating model parameters. We evaluate MTP on RLBench, CALVIN, and a
physical robot, demonstrating effectiveness beyond simulation. Across these
settings, MTP consistently improved success rate and adaptability compared with
fixed-prompt code generation, naive retrieval, and memory-free re-planning.
Furthermore, in hardware experiments, leveraging a memory constructed in
simulation proved effective. MTP provides a practical approach that exploits
procedural knowledge to realize robust LLM-based planning across diverse
robotic manipulation scenarios, enhancing adaptability to novel environments
and bridging simulation and real-world deployment.

</details>


### [1003] [Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models](https://arxiv.org/abs/2509.24163)
*Wanming Yu,Adrian Röfer,Abhinav Valada,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本文提出使用多模态大语言模型（MLLM）作为高层规划器，解决长周期机器人堆叠任务中的物理属性推理问题，并通过构建包含重量、稳定性、尺寸和底面积等偏好的自定义数据集对模型进行微调，显著提升了堆叠任务的完成率。


<details>
  <summary>Details</summary>
Motivation: 预训练的大语言模型在处理需要理解物体物理特性的长周期操作任务时表现不佳，例如包含隐藏物体的容器堆叠任务，因此需要增强模型对重量、稳定性等隐含物理属性的推理能力。

Method: 采用多模态大语言模型作为高层规划器，输入每个待堆叠物体的多模态信息，并基于堆叠偏好（如重量、稳定性、尺寸、底面积）进行推理；构建自定义数据集用于微调模型，使其能同时考虑多种偏好而无需显式指令。

Result: 与仅通过提示调优的预训练LLM相比，使用自定义数据集微调后的MLLM在大规模仿真评估中显著提高了堆叠任务的完成率，并在真实人形机器人上实现了在线长周期堆叠任务的有效部署。

Conclusion: 通过多模态输入和基于堆叠偏好的微调策略，多模态大语言模型能够有效提升在复杂物理环境下的长周期机器人操作任务规划能力。

Abstract: Pretrained large language models (LLMs) can work as high-level robotic
planners by reasoning over abstract task descriptions and natural language
instructions, etc. However, they have shown a lack of knowledge and
effectiveness in planning long-horizon robotic manipulation tasks where the
physical properties of the objects are essential. An example is the stacking of
containers with hidden objects inside, which involves reasoning over hidden
physics properties such as weight and stability. To this end, this paper
proposes to use multimodal LLMs as high-level planners for such long-horizon
robotic stacking tasks. The LLM takes multimodal inputs for each object to
stack and infers the current best stacking sequence by reasoning over stacking
preferences. Furthermore, in order to enable the LLM to reason over multiple
preferences at the same time without giving explicit instructions, we propose
to create a custom dataset considering stacking preferences including weight,
stability, size, and footprint, to fine-tune the LLM. Compared to the
pretrained LLM with prompt tuning, we demonstrate the improved stacking
completion of the LLM fine-tuned with our custom dataset via large-scale
simulation evaluation. Furthermore, we showcase the effectiveness of the
proposed framework for the long-horizon stacking task on a real humanoid robot
in an online manner.

</details>


### [1004] [Very High Frequency Interpolation for Direct Torque Control](https://arxiv.org/abs/2509.24175)
*Rafael Kourdis,Maciej Stępień,Jérôme Manhes,Nicolas Mansard,Steve Tonneau,Philippe Souères,Thomas Flayols*

Main category: cs.RO

TL;DR: 提出一种在开源硬件上实现高达40 kHz的全身线性反馈控制的新方法，用于稳定力矩控制器并提升扭矩控制机器人的性能。


<details>
  <summary>Details</summary>
Motivation: 力矩控制虽能实现敏捷和鲁棒的机器人运动，但常因不稳定性和硬件限制而难以部署。

Method: 在开源硬件上实现高达40 kHz的全身线性反馈控制，并在实际执行中插值非线性方案（如逆动力学和学习到的力矩策略）。

Result: 结果表明，通过稳定力矩控制器，高频线性反馈可有效释放扭矩控制机器人的潜力。

Conclusion: 高频线性反馈结合稳定化技术是推动扭矩控制机器人实用化的有效途径。

Abstract: Torque control enables agile and robust robot motion, but deployment is often
hindered by instability and hardware limits. Here, we present a novel solution
to execute whole-body linear feedback at up to 40 kHz on open-source hardware.
We use this to interpolate non-linear schemes during real-world execution, such
as inverse dynamics and learned torque policies. Our results show that by
stabilizing torque controllers, high-frequency linear feedback could be an
effective route towards unlocking the potential of torque-controlled robotics.

</details>


### [1005] [ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning](https://arxiv.org/abs/2509.24219)
*Tomoyuki Kagaya,Subramanian Lakshmi,Anbang Ye,Thong Jing Yuan,Jayashree Karlekar,Sugiri Pranata,Natsuki Murakami,Akira Kinose,Yang You*

Main category: cs.RO

TL;DR: ViReSkill 是一种结合视觉接地重规划和技能记忆的框架，用于提升机器人在新任务中的适应能力，通过失败时重规划、成功时存储可复用技能，实现自主持续学习，在仿真和真实环境中均表现出优于传统基线的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和视觉语言模型在机器人动作规划中存在符号计划缺乏场景几何与物理 grounding 以及输出不稳定的问题，导致执行可靠性低。

Method: 提出 ViReSkill 框架，将视觉接地的重规划与技能记忆相结合：失败时根据当前场景生成新的动作序列，成功时将执行的计划存储为可复用技能，避免重复调用大模型。

Result: 在 LIBERO、RLBench 等仿真平台及真实机器人上验证，ViReSkill 在任务成功率上 consistently 超过传统基线，展现出强健的仿真到真实迁移能力。

Conclusion: ViReSkill 实现了基于经验积累的自主持续学习，有效提升了 LLM/VLM 驱动机器人系统的鲁棒性和泛化能力。

Abstract: Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL)
often adapt slowly to new tasks, whereas recent Large Language Models (LLMs)
and Vision-Language Models (VLMs) promise knowledge-rich planning from minimal
data. Deploying LLMs/VLMs for motion planning, however, faces two key
obstacles: (i) symbolic plans are rarely grounded in scene geometry and object
physics, and (ii) model outputs can vary for identical prompts, undermining
execution reliability. We propose ViReSkill, a framework that pairs
vision-grounded replanning with a skill memory for accumulation and reuse. When
a failure occurs, the replanner generates a new action sequence conditioned on
the current scene, tailored to the observed state. On success, the executed
plan is stored as a reusable skill and replayed in future encounters without
additional calls to LLMs/VLMs. This feedback loop enables autonomous continual
learning: each attempt immediately expands the skill set and stabilizes
subsequent executions. We evaluate ViReSkill on simulators such as LIBERO and
RLBench as well as on a physical robot. Across all settings, it consistently
outperforms conventional baselines in task success rate, demonstrating robust
sim-to-real generalization.

</details>


### [1006] [Towards Tighter Convex Relaxation of Mixed-integer Programs: Leveraging Logic Network Flow for Task and Motion Planning](https://arxiv.org/abs/2509.24235)
*Xuan Lin,Jiming Ren,Yandong Luo,Weijun Xie,Ye Zhao*

Main category: cs.RO

TL;DR: 提出了一种名为“Logic Network Flow”的优化任务与运动规划框架，将时序逻辑规范融入混合整数规划中，通过网络流模型上的傅里叶-莫茨金消元法实现更紧的凸松弛和更快的计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑树方法在处理时序逻辑任务规划时存在约束多、求解效率低的问题，需要更高效的建模方式以提升机器人规划性能。

Method: 受Graph-of-Convex-Sets启发，将时序谓词编码为网络流模型每条边上的多面体约束，并提出基于网络流的傅里叶-莫茨金消元法，在保持凸松弛紧致性的同时消除连续流变量。

Result: 在车辆路径规划、多机器人协调和动态系统时序逻辑控制等实验中实现了高达几个数量级的计算加速，并通过四足机器人硬件实验验证了动态环境中实时重规划的能力。

Conclusion: Logic Network Flow框架相比传统逻辑树方法具有更少的约束、更紧的凸松弛和显著更高的计算效率，适用于复杂时序逻辑任务下的实时机器人规划。

Abstract: This paper proposes an optimization-based task and motion planning framework,
named "Logic Network Flow", that integrates temporal logic specifications into
mixed-integer programs for efficient robot planning. Inspired by the
Graph-of-Convex-Sets formulation, temporal predicates are encoded as polyhedron
constraints on each edge of a network flow model, instead of as constraints
between nodes in traditional Logic Tree formulations. We further propose a
network-flow-based Fourier-Motzkin elimination procedure that removes
continuous flow variables while preserving convex relaxation tightness, leading
to provably tighter convex relaxations and fewer constraints than Logic Tree
formulations. For temporal logic motion planning with piecewise-affine dynamic
systems, comprehensive experiments across vehicle routing, multi-robot
coordination, and temporal logic control on dynamical systems using point mass
and linear inverted pendulum models demonstrate computational speedups of up to
several orders of magnitude. Hardware demonstrations with quadrupedal robots
validate real-time replanning capabilities under dynamically changing
environmental conditions. The project website is at
https://logicnetworkflow.github.io/.

</details>


### [1007] [PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization](https://arxiv.org/abs/2509.24236)
*Siyan Dong,Zijun Wang,Lulu Cai,Yi Ma,Yanchao Yang*

Main category: cs.RO

TL;DR: 提出了一种结合学习初始化与优化精修的RGB-D SLAM方法，能够在相机剧烈运动下实现鲁棒且精确的实时稠密场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D SLAM系统在相机大视角变化、快速运动或突然抖动时表现不佳：传统优化方法依赖良好初始化，而学习方法精度不足。因此需要兼顾鲁棒性与精度。

Method: 采用学习-based相机位姿回归网络预测连续RGB-D帧间的度量感知相对位姿，作为优化算法的可靠初值；随后使用随机化优化算法进一步对齐深度图像与场景几何。

Result: 在具有挑战性的动态运动基准上优于最佳竞争对手，同时在稳定运动序列上保持相当的精度，系统支持实时运行。

Conclusion: 结合学习初始化与优化精修的简单而原则性方法，可同时实现对不稳定运动的鲁棒性和稠密重建的高精度。

Abstract: Real-time dense scene reconstruction during unstable camera motions is
crucial for robotics, yet current RGB-D SLAM systems fail when cameras
experience large viewpoint changes, fast motions, or sudden shaking. Classical
optimization-based methods deliver high accuracy but fail with poor
initialization during large motions, while learning-based approaches provide
robustness but lack sufficient accuracy for dense reconstruction. We address
this challenge through a combination of learning-based initialization with
optimization-based refinement. Our method employs a camera pose regression
network to predict metric-aware relative poses from consecutive RGB-D frames,
which serve as reliable starting points for a randomized optimization algorithm
that further aligns depth images with the scene geometry. Extensive experiments
demonstrate promising results: our approach outperforms the best competitor on
challenging benchmarks, while maintaining comparable accuracy on stable motion
sequences. The system operates in real-time, showcasing that combining simple
and principled techniques can achieve both robustness for unstable motions and
accuracy for dense reconstruction. Project page:
https://github.com/siyandong/PROFusion.

</details>


### [1008] [SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions](https://arxiv.org/abs/2509.24243)
*Jeongyong Yang,Seunghwan Jang,Soojean Han*

Main category: cs.RO

TL;DR: SafeFlowMatcher是一种结合流匹配（FM）与控制屏障函数（CBF）的规划框架，通过预测-校正积分器实现高效且安全的路径生成，在保持实时性能的同时提供形式化安全保证。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流匹配的生成式规划器虽然高效，但在约束区域附近可能产生不完整路径，且缺乏形式化安全保证。

Method: 提出SafeFlowMatcher框架，采用两阶段的预测-校正积分器：先用FM生成候选路径，再通过CBF优化向量场以最小扰动修正路径，并证明系统的屏障证书。

Result: 在迷宫导航和运动生成任务中，SafeFlowMatcher相比扩散和FM基线方法生成更快、更平滑、更安全的路径，且避免了分布漂移和局部陷阱问题。

Conclusion: SafeFlowMatcher实现了效率与安全性的平衡，通过仅对执行路径施加安全约束，保障了系统前向不变性和有限时间收敛性，适用于高要求的实时规划场景。

Abstract: Generative planners based on flow matching (FM) can produce high-quality
paths in one or a few ODE steps, but their sampling dynamics offer no formal
safety guarantees and can yield incomplete paths near constraints. We present
SafeFlowMatcher, a planning framework that couples FM with control barrier
functions (CBFs) to achieve both real-time efficiency and certified safety.
SafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a
prediction phase integrates the learned FM once (or a few steps) to obtain a
candidate path without intervention; (ii) a correction phase refines this path
with a vanishing time-scaled vector field and a CBF-based quadratic program
that minimally perturbs the vector field. We prove a barrier certificate for
the resulting flow system, establishing forward invariance of a robust safe set
and finite-time convergence to the safe set. By enforcing safety only on the
executed path (rather than on all intermediate latent paths), SafeFlowMatcher
avoids distributional drift and mitigates local trap problems. Across maze
navigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother,
and safer paths than diffusion- and FM-based baselines. Extensive ablations
corroborate the contributions of the PC integrator and the barrier certificate.

</details>


### [1009] [Contextual Neural Moving Horizon Estimation for Robust Quadrotor Control in Varying Conditions](https://arxiv.org/abs/2509.24281)
*Kasra Torshizi,Chak Lam Shek,Khuzema Habib,Guangyao Shi,Pratap Tokekar,Troi Williams*

Main category: cs.RO

TL;DR: 本文提出了一种名为Contextual NeuroMHE的顺序决策策略，利用贝叶斯优化和高斯过程选择关键环境上下文进行数据采集，使神经网络动态调整估计器参数，在减少训练开销的同时提升了无人机在多变环境下的轨迹跟踪鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统自适应控制器依赖扰动估计来保证轨迹跟踪性能，但在多变环境中难以准确估计扰动且参数需大量调优，缺乏灵活性和鲁棒性。现有机器学习方法因需覆盖所有环境而数据效率低，本文旨在减少数据需求并提升跨环境的适应能力。

Method: 提出Contextual NeuroMHE方法，结合贝叶斯优化与高斯过程，主动选择最具信息量的环境上下文进行数据收集，并训练神经网络动态调整估计器参数，实现高效且泛化的自适应控制。

Result: 实验表明，该方法相比先前工作最大绝对位置误差降低了20.3%，仅用少量精心选择的上下文即可捕捉环境变化，显著提升控制精度与适应性。

Conclusion: Contextual NeuroMHE通过智能选择训练上下文，实现了高效、鲁棒且具有良好泛化能力的无人机控制，避免了在所有环境中进行 exhaustive 训练的需求，为自适应控制提供了可扩展的解决方案。

Abstract: Adaptive controllers on quadrotors typically rely on estimation of
disturbances to ensure robust trajectory tracking. Estimating disturbances
across diverse environmental contexts is challenging due to the inherent
variability and uncertainty in the real world. Such estimators require
extensive fine-tuning for a specific scenario, which makes them inflexible and
brittle to changing conditions. Machine-learning approaches, such as training a
neural network to tune the estimator's parameters, are promising. However,
collecting data across all possible environmental contexts is impossible. It is
also inefficient as the same estimator parameters could work for "nearby"
contexts. In this paper, we present a sequential decision making strategy that
decides which environmental contexts, using Bayesian Optimization with a
Gaussian Process, to collect data from in order to ensure robust performance
across a wide range of contexts. Our method, Contextual NeuroMHE, eliminates
the need for exhaustive training across all environments while maintaining
robust performance under different conditions. By enabling the neural network
to adapt its parameters dynamically, our method improves both efficiency and
generalization. Experimental results in various real-world settings demonstrate
that our approach outperforms the prior work by 20.3\% in terms of maximum
absolute position error and can capture the variations in the environment with
a few carefully chosen contexts.

</details>


### [1010] [Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning](https://arxiv.org/abs/2509.24313)
*Korbinian Moller,Roland Stroop,Mattia Piccinini,Alexander Langmann,Johannes Betz*

Main category: cs.RO

TL;DR: 提出一种结合强化学习和世界模型的混合采样框架，用于自动驾驶中的运动规划，显著减少采样数量和运行时间，同时保持规划质量。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市场景中，传统的均匀或启发式采样常产生大量不可行或不相关的轨迹，导致效率低下。

Method: 采用强化学习代理指导采样过程，结合基于可解码深度集编码器的世界模型，提升对交通参与者变化的适应性，并保留解析性和可验证的轨迹生成与评估。

Result: 在CommonRoad仿真环境中，最多减少99%的采样量，运行时间降低84%，且保持了较高的任务成功率和无碰撞率。

Conclusion: 该方法显著提升了自动驾驶在城市环境中的决策速度与可靠性，实现了更安全、响应更快的导航。

Abstract: Sampling-based motion planning is a well-established approach in autonomous
driving, valued for its modularity and analytical tractability. In complex
urban scenarios, however, uniform or heuristic sampling often produces many
infeasible or irrelevant trajectories. We address this limitation with a hybrid
framework that learns where to sample while keeping trajectory generation and
evaluation fully analytical and verifiable. A reinforcement learning (RL) agent
guides the sampling process toward regions of the action space likely to yield
feasible trajectories, while evaluation and final selection remains governed by
deterministic feasibility checks and cost functions. We couple the RL sampler
with a world model (WM) based on a decodable deep set encoder, enabling both
variable numbers of traffic participants and reconstructable latent
representations. The approach is evaluated in the CommonRoad simulation
environment, showing up to 99% fewer required samples and a runtime reduction
of up to 84% while maintaining planning quality in terms of success and
collision-free rates. These improvements lead to faster, more reliable
decision-making for autonomous vehicles in urban environments, achieving safer
and more responsive navigation under real-world constraints. Code and trained
artifacts are publicly available at:
https://github.com/TUM-AVS/Learning-to-Sample

</details>


### [1011] [SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm](https://arxiv.org/abs/2509.24321)
*Yao Wang,Zhirui Sun,Wenzheng Chi,Baozhi Jia,Wenjun Xu,Jiankun Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SONAR的跨模态聚合推理方法，用于解决视觉-语言导航任务中现有方法在语义线索较弱时表现不佳和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模块化方法依赖训练数据质量，泛化能力差；基于视觉-语言模型的方法在语义线索弱时表现不理想。因此需要一种兼具强泛化能力和环境适应性的导航方法。

Method: SONAR结合了基于语义地图的目标预测模块和基于视觉-语言模型的价值地图模块，并引入多尺度语义地图与置信度地图融合策略，以提高目标定位的准确性。

Result: 在Gazebo模拟器和Matterport3D（MP3D）数据集上进行评估，SONAR取得了38.4%的成功率和17.7%的SPL。

Conclusion: SONAR通过融合语义地图与视觉-语言模型，在不同语义线索强度的未知环境中实现了更鲁棒的导航，有效平衡了泛化能力与场景适应性。

Abstract: Understanding human instructions and accomplishing Vision-Language Navigation
tasks in unknown environments is essential for robots. However, existing
modular approaches heavily rely on the quality of training data and often
exhibit poor generalization. Vision-Language Model based methods, while
demonstrating strong generalization capabilities, tend to perform
unsatisfactorily when semantic cues are weak. To address these issues, this
paper proposes SONAR, an aggregated reasoning approach through a cross modal
paradigm. The proposed method integrates a semantic map based target prediction
module with a Vision-Language Model based value map module, enabling more
robust navigation in unknown environments with varying levels of semantic cues,
and effectively balancing generalization ability with scene adaptability. In
terms of target localization, we propose a strategy that integrates multi-scale
semantic maps with confidence maps, aiming to mitigate false detections of
target objects. We conducted an evaluation of the SONAR within the Gazebo
simulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the
experimental benchmark. Experimental results demonstrate that SONAR achieves a
success rate of 38.4% and an SPL of 17.7%.

</details>


### [1012] [AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation](https://arxiv.org/abs/2509.24387)
*Xin Ding,Jianyu Wei,Yifan Yang,Shiqi Jiang,Qianxi Zhang,Hao Wu,Fucheng Jia,Liang Mi,Yuxuan Yan,Weijun Wang,Yunxin Liu,Zhibo Chen,Ting Cao*

Main category: cs.RO

TL;DR: 本文提出了一种基于不确定性的自适应推理框架AdaNav，用于视觉语言导航（VLN），通过动态触发推理提升长视野下的导航性能。


<details>
  <summary>Details</summary>
Motivation: 固定步长的推理方式在视觉语言导航中常导致性能不佳和计算浪费，因此需要一种能根据任务难度动态调整推理时机的方法。

Method: 提出Uncertainty Adaptive Reasoning Block (UAR)，利用动作熵作为策略先验，并通过启发式到强化学习（Heuristics to RL）方法进行渐进优化，实现数据高效训练下的自适应推理。

Result: 仅用6K训练样本，AdaNav在R2R val-unseen上成功率提升20%，RxR-CE提升11.7%，在真实场景中提升11.4%，优于大规模训练的闭源模型。

Conclusion: AdaNav通过不确定性驱动的自适应推理机制，在极小数据下实现了显著性能提升，展现了其在资源受限场景下的有效性与实用性。

Abstract: Vision Language Navigation (VLN) requires agents to follow natural language
instructions by grounding them in sequential visual observations over long
horizons. Explicit reasoning could enhance temporal consistency and perception
action alignment, but reasoning at fixed steps often leads to suboptimal
performance and unnecessary computation. To address this, we propose AdaNav, an
uncertainty-based adaptive reasoning framework for VLN. At its core is the
Uncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that
dynamically triggers reasoning. We introduce Action Entropy as a policy prior
for UAR and progressively refine it through a Heuristics to RL training method,
enabling agents to learn difficulty aware reasoning policies under the strict
data limitations of embodied tasks. Results show that with only 6K training
samples, AdaNav achieves substantial gains over closed source models trained on
million scale data, improving success rate by 20% on R2R val-unseen, 11.7% on
RxR-CE, and 11.4% in real world scenes. The code is available at
https://github.com/xinding-sys/AdaNav.

</details>


### [1013] [DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability](https://arxiv.org/abs/2509.24413)
*Tianqiang Yan,Ziqiao Lin,Sicheng Wang,Tianwei Zhang,Zhenglong Sun*

Main category: cs.RO

TL;DR: 本文提出了指令反事实（DCFs）的概念，源于误导性的人类指令，并引入DynaMIC框架来生成机器人任务流程以识别DCFs并向人类主动反馈，从而提高任务执行的可靠性。


<details>
  <summary>Details</summary>
Motivation: 发现严格遵循包含误导信息的人类指令可能导致机器人执行错误和安全隐患，而这一问题在机器人研究中尚未引起足够关注。

Method: 提出DynaMIC框架，通过生成机器人任务流来识别由误导性指令引发的指令反事实（DCFs），并主动向人类提供反馈。

Result: 语义级实验和消融研究表明该框架能有效识别DCFs并提升机器人对潜在问题的敏感性。

Conclusion: DynaMIC框架有助于增强机器人系统在自然语言指令下的安全性和可靠性，为未来研究提供了新的方向。

Abstract: The emergence of large pre-trained models based on natural language has
breathed new life into robotics development. Extensive research has integrated
large models with robots, utilizing the powerful semantic understanding and
generation capabilities of large models to facilitate robot control through
natural language instructions gradually. However, we found that robots that
strictly adhere to human instructions, especially those containing misleading
information, may encounter errors during task execution, potentially leading to
safety hazards. This resembles the concept of counterfactuals in natural
language processing (NLP), which has not yet attracted much attention in
robotic research. In an effort to highlight this issue for future studies, this
paper introduced directive counterfactuals (DCFs) arising from misleading human
directives. We present DynaMIC, a framework for generating robot task flows to
identify DCFs and relay feedback to humans proactively. This capability can
help robots be sensitive to potential DCFs within a task, thus enhancing the
reliability of the execution process. We conducted semantic-level experiments
and ablation studies, showcasing the effectiveness of this framework.

</details>


### [1014] [PhysiAgent: An Embodied Agent Framework in Physical World](https://arxiv.org/abs/2509.24524)
*Zhihao Wang,Jianxiong Li,Jinliang Zheng,Wencong Zhang,Dongxiu Liu,Yinan Zheng,Haoyi Niu,Junzhi Yu,Xianyuan Zhan*

Main category: cs.RO

TL;DR: 本文提出了PhysiAgent，一种通过引入监控、记忆和自反思机制，结合现成工具箱，实现视觉-语言模型（VLMs）与视觉-语言-动作模型（VLAs）有效协作的具身智能体框架，在复杂真实机器人任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM与VLA结合方法多为刚性顺序结构，导致协作效率低、接地能力差，难以充分发挥VLAs的潜力。

Method: 提出PhysiAgent框架，集成监控、记忆和自反思机制，利用VLA的实时能力反馈动态调整VLM的提示，实现自主搭建并优化组件协作，并结合轻量级现成工具箱提升灵活性和实用性。

Result: 在复杂真实世界机器人任务中，PhysiAgent显著提升了任务解决性能，展现出有效的自我调节、工具协同和执行过程中的自适应演化能力。

Conclusion: PhysiAgent实现了VLMs与VLAs的有效整合，推动了具身智能体框架在现实环境中的实际应用，是该领域的开创性工作。

Abstract: Vision-Language-Action (VLA) models have achieved notable success but often
struggle with limited generalizations. To address this, integrating generalized
Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular
solution. However, current approaches often combine these models in rigid,
sequential structures: using VLMs primarily for high-level scene understanding
and task planning, and VLAs merely as executors of lower-level actions, leading
to ineffective collaboration and poor grounding challenges. In this paper, we
propose an embodied agent framework, PhysiAgent, tailored to operate
effectively in physical environments. By incorporating monitor, memory,
self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent
offers an autonomous scaffolding framework to prompt VLMs to organize different
components based on real-time proficiency feedback from VLAs to maximally
exploit VLAs' capabilities. Experimental results demonstrate significant
improvements in task-solving performance on complex real-world robotic tasks,
showcasing effective self-regulation of VLMs, coherent tool collaboration, and
adaptive evolution of the framework during execution. PhysiAgent makes
practical and pioneering efforts to integrate VLMs and VLAs, effectively
grounding embodied agent frameworks in real-world settings.

</details>


### [1015] [Game Theory to Study Cooperation in Human-Robot Mixed Groups: Exploring the Potential of the Public Good Game](https://arxiv.org/abs/2509.24530)
*Giulia Pusceddu,Sara Mongile,Francesco Rea,Alessandra Sciutti*

Main category: cs.RO

TL;DR: 本研究通过引入公共物品博弈（PGG）的改进版本，探讨机器人在人类-机器人混合群体中促进合作与信任的潜力，实验中三名人类与人形机器人iCub互动，测试不同机器人策略对人类合作意愿的影响。


<details>
  <summary>Details</summary>
Motivation: 探索机器人如何在群体情境中影响人类的合作行为和信任建立，推动社会机器人的发展。

Method: 采用修改版的公共物品博弈（PGG），让三名人类参与者与iCub机器人共同游戏，对比不同机器人策略（如始终合作、始终搭便车、以牙还牙）对人类投资行为的影响，进行了包含19名参与者的初步实验。

Result: 初步分析显示，尽管参与者认为机器人慷慨，但他们仍倾向于不将钱投入公共池。

Conclusion: 该研究为理解机器人在混合群体中促进信任与合作的作用提供了初步见解，具有推动社会机器人设计的潜力。

Abstract: In this study, we explore the potential of Game Theory as a means to
investigate cooperation and trust in human-robot mixed groups. Particularly, we
introduce the Public Good Game (PGG), a model highlighting the tension between
individual self-interest and collective well-being. In this work, we present a
modified version of the PGG, where three human participants engage in the game
with the humanoid robot iCub to assess whether various robot game strategies
(e.g., always cooperate, always free ride, and tit-for-tat) can influence the
participants' inclination to cooperate. We test our setup during a pilot study
with nineteen participants. A preliminary analysis indicates that participants
prefer not to invest their money in the common pool, despite they perceive the
robot as generous. By conducting this research, we seek to gain valuable
insights into the role that robots can play in promoting trust and cohesion
during human-robot interactions within group contexts. The results of this
study may hold considerable potential for developing social robots capable of
fostering trust and cooperation within mixed human-robot groups.

</details>


### [1016] [Unlocking the Potential of Soft Actor-Critic for Imitation Learning](https://arxiv.org/abs/2509.24539)
*Nayari Marie Lessa,Melya Boukheddimi,Frank Kirchner*

Main category: cs.RO

TL;DR: 本文提出了一种结合对抗运动先验（AMP）与非策略型Soft Actor-Critic（SAC）算法的新型模仿学习框架，旨在提升机器人运动生成的数据效率和鲁棒性，在四足机器人步态任务中表现优于传统的AMP+PPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的机器人运动生成方法多依赖于PPO等策略内算法，存在样本效率低、策略泛化能力差的问题，限制了复杂动态行为的高效学习与适应。

Method: 将对抗运动先验（AMP）与非策略型强化学习算法Soft Actor-Critic（SAC）相结合，利用回放缓冲区进行高效数据重用，并通过熵正则化促进探索，实现对动物运动模式的自然且鲁棒的模仿。

Result: 在多种参考运动和复杂地形下的四足机器人步态任务中，AMP+SAC相比AMP+PPO实现了更高的模仿奖励和更稳定的行为表现，同时提升了训练效率和跨任务适应能力。

Conclusion: 非策略型IL框架在机器人运动生成中具有显著优势，结合SAC的AMP方法能够有效提高数据利用率、策略鲁棒性和行为自然性，为未来生物启发式机器人控制提供了新方向。

Abstract: Learning-based methods have enabled robots to acquire bio-inspired movements
with increasing levels of naturalness and adaptability. Among these, Imitation
Learning (IL) has proven effective in transferring complex motion patterns from
animals to robotic systems. However, current state-of-the-art frameworks
predominantly rely on Proximal Policy Optimization (PPO), an on-policy
algorithm that prioritizes stability over sample efficiency and policy
generalization. This paper proposes a novel IL framework that combines
Adversarial Motion Priors (AMP) with the off-policy Soft Actor-Critic (SAC)
algorithm to overcome these limitations. This integration leverages
replay-driven learning and entropy-regularized exploration, enabling
naturalistic behavior and task execution, improving data efficiency and
robustness. We evaluate the proposed approach (AMP+SAC) on quadruped gaits
involving multiple reference motions and diverse terrains. Experimental results
demonstrate that the proposed framework not only maintains stable task
execution but also achieves higher imitation rewards compared to the widely
used AMP+PPO method. These findings highlight the potential of an off-policy IL
formulation for advancing motion generation in robotics.

</details>


### [1017] [U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation](https://arxiv.org/abs/2509.24579)
*Linzhi Wu,Aoran Mei,Xiyue Wang,Guo-Niu Zhu,Zhongxue Gan*

Main category: cs.RO

TL;DR: 本文提出了一种基于U形扩散Transformer的新型视觉运动控制策略U-DiT，结合了U-Net的多尺度特征融合和Transformer的全局上下文建模能力，在仿真和真实机器人任务中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于U-Net结构的扩散策略（DP-U）在全局上下文建模和细节保持方面存在局限，容易产生过平滑问题，限制了其在复杂机器人操作任务中的表现。

Method: 提出U-DiT Policy，采用U形Diffusion Transformer架构，保留U-Net的多尺度特征融合机制，同时引入Transformer以增强全局上下文建模能力，并使用AdaLN块进行条件控制，提升策略表达力。

Result: 在仿真任务中，U-DiT比基线方法平均提升10%，比使用AdaLN的Transformer扩散策略（DP-T）高6%；在真实世界任务中，相比DP-U平均提升22.5%，且在干扰物和光照变化下表现出更强的鲁棒性和泛化能力。

Conclusion: U-DiT Policy通过融合U-Net与Transformer的优势，有效提升了扩散策略的表示能力和实际操控性能，展现出作为扩散型机器人操作新基础架构的潜力。

Abstract: Diffusion-based methods have been acknowledged as a powerful paradigm for
end-to-end visuomotor control in robotics. Most existing approaches adopt a
Diffusion Policy in U-Net architecture (DP-U), which, while effective, suffers
from limited global context modeling and over-smoothing artifacts. To address
these issues, we propose U-DiT Policy, a novel U-shaped Diffusion Transformer
framework. U-DiT preserves the multi-scale feature fusion advantages of U-Net
while integrating the global context modeling capability of Transformers,
thereby enhancing representational power and policy expressiveness. We evaluate
U-DiT extensively across both simulation and real-world robotic manipulation
tasks. In simulation, U-DiT achieves an average performance gain of 10\% over
baseline methods and surpasses Transformer-based diffusion policies (DP-T) that
use AdaLN blocks by 6\% under comparable parameter budgets. On real-world
robotic tasks, U-DiT demonstrates superior generalization and robustness,
achieving an average improvement of 22.5\% over DP-U. In addition, robustness
and generalization experiments under distractor and lighting variations further
highlight the advantages of U-DiT. These results highlight the effectiveness
and practical potential of U-DiT Policy as a new foundation for diffusion-based
robotic manipulation.

</details>


### [1018] [PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control](https://arxiv.org/abs/2509.24591)
*Haozhuo Zhang,Michele Caprio,Jing Shao,Qiang Zhang,Jian Tang,Shanghang Zhang,Wei Pan*

Main category: cs.RO

TL;DR: PoseDiff是一种基于条件扩散模型的统一框架，用于机器人状态估计与控制，能从单张RGB图像中直接预测结构化机器人状态，并通过视频关键帧生成连续动作序列，在姿态估计和操作任务中均表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常依赖多阶段流水线或额外模态进行机器人状态估计与控制，限制了效率与可扩展性。因此，需要一种端到端、统一且高效的模型来整合感知与控制。

Method: PoseDiff采用条件扩散模型，从单张RGB图像中直接预测机器人状态（如3D关键点或关节角）；并通过在世界模型生成的稀疏视频关键帧上进行条件建模，结合重叠平均策略生成平滑的长时程动作序列。

Result: 在DREAM数据集上，PoseDiff实现了最先进的精度和实时性能；在Libero-Object操作任务中，即使在严格的离线设置下，其成功率也显著优于现有逆动力学模块。

Conclusion: PoseDiff为具身AI中的感知、规划与控制之间提供了一个可扩展、准确且高效的桥梁，验证了统一建模范式的潜力。

Abstract: We present PoseDiff, a conditional diffusion model that unifies robot state
estimation and control within a single framework. At its core, PoseDiff maps
raw visual observations into structured robot states-such as 3D keypoints or
joint angles-from a single RGB image, eliminating the need for multi-stage
pipelines or auxiliary modalities. Building upon this foundation, PoseDiff
extends naturally to video-to-action inverse dynamics: by conditioning on
sparse video keyframes generated by world models, it produces smooth and
continuous long-horizon action sequences through an overlap-averaging strategy.
This unified design enables scalable and efficient integration of perception
and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy
and real-time performance for pose estimation. On Libero-Object manipulation
tasks, it substantially improves success rates over existing inverse dynamics
modules, even under strict offline settings. Together, these results show that
PoseDiff provides a scalable, accurate, and efficient bridge between
perception, planning, and control in embodied AI. The video visualization
results can be found on the project page:
https://haozhuo-zhang.github.io/PoseDiff-project-page/.

</details>


### [1019] [CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations](https://arxiv.org/abs/2509.24661)
*Zhiyuan Wu,Rolandos Alexandros Potamias,Xuyang Zhang,Zhongqun Zhang,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: 提出CEDex方法，通过结合人类抓取运动学与机器人运动学，生成大规模跨形态灵巧抓取数据集，显著提升机器人抓取的泛化能力与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对人体运动学的理解或依赖繁琐的手动数据采集，难以适应多种机器人手部结构，限制了抓取的通用性和灵活性。

Method: 利用预训练的条件变分自编码器生成类人接触表示，并通过拓扑融合将人类手部结构映射到机器人组件，结合符号距离场与物理约束进行抓取优化。

Result: 构建了目前最大的跨形态抓取数据集（50万物体，2000万抓取），涵盖四种夹持器类型，实验表明CEDex在抓取质量与多样性上优于现有方法。

Conclusion: CEDex有效桥接了人类与机器人抓取运动学，支持高质量、多样化的跨形态抓取生成，为通用机器人操作提供了可靠的数据基础。

Abstract: Cross-embodiment dexterous grasp synthesis refers to adaptively generating
and optimizing grasps for various robotic hands with different morphologies.
This capability is crucial for achieving versatile robotic manipulation in
diverse environments and requires substantial amounts of reliable and diverse
grasp data for effective model training and robust generalization. However,
existing approaches either rely on physics-based optimization that lacks
human-like kinematic understanding or require extensive manual data collection
processes that are limited to anthropomorphic structures. In this paper, we
propose CEDex, a novel cross-embodiment dexterous grasp synthesis method at
scale that bridges human grasping kinematics and robot kinematics by aligning
robot kinematic models with generated human-like contact representations. Given
an object's point cloud and an arbitrary robotic hand model, CEDex first
generates human-like contact representations using a Conditional Variational
Auto-encoder pretrained on human contact data. It then performs kinematic human
contact alignment through topological merging to consolidate multiple human
hand parts into unified robot components, followed by a signed distance
field-based grasp optimization with physics-aware constraints. Using CEDex, we
construct the largest cross-embodiment grasp dataset to date, comprising 500K
objects across four gripper types with 20M total grasps. Extensive experiments
show that CEDex outperforms state-of-the-art approaches and our dataset
benefits cross-embodiment grasp learning with high-quality diverse grasps.

</details>


### [1020] [Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering](https://arxiv.org/abs/2509.24697)
*Evelyn D'Elia,Paolo Maria Viceconte,Lorenzo Rapetti,Diego Ferigo,Giulio Romualdi,Giuseppe L'Erario,Raffaello Camoriano,Daniele Pucci*

Main category: cs.RO

TL;DR: 提出一种结合物理先验和反馈控制的双管齐下学习策略，提升人形机器人模仿学习中生成轨迹的准确性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法受限于运动数据量，且未融入系统物理规律，易导致轨迹发散和接触滑动，影响实际稳定性。

Method: 在监督模仿学习中引入物理先验以提升轨迹可行性，并在推理时对生成状态应用比例-积分控制器以减少漂移。

Result: 在ergoCub人形机器人多种行走行为上验证了方法的有效性，物理信息损失函数促使足部接触速度为零，显著提升了轨迹精度和物理约束符合度。

Conclusion: 该方法兼容多种控制器，在真实机器人上表现出更优的轨迹生成性能，增强了模仿学习在实际应用中的稳定性和可靠性。

Abstract: Recent trends in humanoid robot control have successfully employed imitation
learning to enable the learned generation of smooth, human-like trajectories
from human data. While these approaches make more realistic motions possible,
they are limited by the amount of available motion data, and do not incorporate
prior knowledge about the physical laws governing the system and its
interactions with the environment. Thus they may violate such laws, leading to
divergent trajectories and sliding contacts which limit real-world stability.
We address such limitations via a two-pronged learning strategy which leverages
the known physics of the system and fundamental control principles. First, we
encode physics priors during supervised imitation learning to promote
trajectory feasibility. Second, we minimize drift at inference time by applying
a proportional-integral controller directly to the generated output state. We
validate our method on various locomotion behaviors for the ergoCub humanoid
robot, where a physics-informed loss encourages zero contact foot velocity. Our
experiments demonstrate that the proposed approach is compatible with multiple
controllers on a real robot and significantly improves the accuracy and
physical constraint conformity of generated trajectories.

</details>


### [1021] [LLM-Handover:Exploiting LLMs for Task-Oriented Robot-Human Handovers](https://arxiv.org/abs/2509.24706)
*Andreea Tulbure,Rene Zurbruegg,Timm Grigat,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型（LLM）与部件分割的上下文感知抓取选择框架LLM-Handover，用于优化机器人-人类任务导向型物体交接，显著提升了抓取成功率和交接自然性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略交接后人类的动作，依赖限制泛化能力的假设，因此需要一种能根据任务上下文智能选择抓取方式的方法。

Method: 提出LLM-Handover框架，利用大语言模型推理与RGB-D图像中的部件分割相结合，根据任务描述推断关键物体部件并优化抓取策略。同时构建了一个包含60个家用物品、带细粒度部件标注的新数据集用于评估。

Result: 在零样本设置下硬件实验抓取成功率达83%，优于现有方法；用户研究显示86%参与者更偏好该方法；且提升了部件分割性能和对交接后任务的适应能力。

Conclusion: LLM-Handover通过融合LLM推理与视觉感知，实现了更智能、自然的任务导向型人机交接，具有良好的泛化性和用户体验。

Abstract: Effective human-robot collaboration depends on task-oriented handovers, where
robots present objects in ways that support the partners intended use. However,
many existing approaches neglect the humans post-handover action, relying on
assumptions that limit generalizability. To address this gap, we propose
LLM-Handover, a novel framework that integrates large language model
(LLM)-based reasoning with part segmentation to enable context-aware grasp
selection and execution. Given an RGB-D image and a task description, our
system infers relevant object parts and selects grasps that optimize
post-handover usability. To support evaluation, we introduce a new dataset of
60 household objects spanning 12 categories, each annotated with detailed part
labels. We first demonstrate that our approach improves the performance of the
used state-of-the-art part segmentation method, in the context of robot-human
handovers. Next, we show that LLM-Handover achieves higher grasp success rates
and adapts better to post-handover task constraints. During hardware
experiments, we achieve a success rate of 83% in a zero-shot setting over
conventional and unconventional post-handover tasks. Finally, our user study
underlines that our method enables more intuitive, context-aware handovers,
with participants preferring it in 86% of cases.

</details>


### [1022] [APREBot: Active Perception System for Reflexive Evasion Robot](https://arxiv.org/abs/2509.24733)
*Zihao Xu,Kuankuan Sima,Junhao Deng,Zixuan Zhuang,Chunzheng Wang,Ce Hao,Jin Song Dong*

Main category: cs.RO

TL;DR: APREBot是一种结合LiDAR全向扫描与相机主动聚焦的主动感知系统，用于提升四足机器人在动态环境中敏捷避障的能力。


<details>
  <summary>Details</summary>
Motivation: 单传感器系统存在局限性，如LiDAR缺乏纹理信息，相机视野受限，难以满足四足机器人在动态环境中快速反应的感知需求。

Method: 提出APREBot框架，融合LiDAR的全向覆盖与相机的高分辨率细节，通过分层主动感知实现快速避障决策，并在仿真到实机的实验中进行验证。

Result: 实验表明，APREBot在安全性指标和运行效率上均显著优于现有基线方法，能够有效应对多种障碍物类型、轨迹和接近方向。

Conclusion: APREBot通过多传感器协同的主动感知策略，提升了四足机器人在复杂动态环境中的可靠性和自主性，具有在安全关键场景中的应用潜力。

Abstract: Reliable onboard perception is critical for quadruped robots navigating
dynamic environments, where obstacles can emerge from any direction under
strict reaction-time constraints. Single-sensor systems face inherent
limitations: LiDAR provides omnidirectional coverage but lacks rich texture
information, while cameras capture high-resolution detail but suffer from
restricted field of view. We introduce APREBot (Active Perception System for
Reflexive Evasion Robot), a novel framework that integrates reflexive evasion
with active hierarchical perception. APREBot strategically combines LiDAR-based
omnidirectional scanning with camera-based active focusing, achieving
comprehensive environmental awareness essential for agile obstacle avoidance in
quadruped robots. We validate APREBot through extensive sim-to-real experiments
on a quadruped platform, evaluating diverse obstacle types, trajectories, and
approach directions. Our results demonstrate substantial improvements over
state-of-the-art baselines in both safety metrics and operational efficiency,
highlighting APREBot's potential for dependable autonomy in safety-critical
scenarios. Videos are available at https://sites.google.com/view/aprebot/

</details>


### [1023] [SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework](https://arxiv.org/abs/2509.24763)
*Xiangyi Meng,Delun Li,Zihao Mao,Yi Yang,Wenjie Song*

Main category: cs.RO

TL;DR: 提出SSR-ZSON方法，结合视点生成策略与基于大语言模型的全局引导机制，提升零样本目标导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标导航方法因语义引导不足和空间记忆有限，导致探索效率低且易陷入局部区域。

Method: 基于TARE分层探索框架，提出SSR-ZSON方法：1）设计兼顾空间覆盖与语义密度的视点生成策略；2）引入基于大语言模型的全局语义引导机制，指导高价值区域探索。

Result: 在Matterport3D和Habitat-Matterport3D数据集上，相比最先进方法，成功率（SR）分别提升18.5%和11.2%，路径加权成功率（SPL）分别提高0.181和0.140，并在仿真与真实平台上实现高效实时导航。

Conclusion: SSR-ZSON通过融合空间-语义相对信息与大语言模型引导，显著提升了未知环境中的零样本目标导航效率与成功率。

Abstract: Zero-shot object navigation in unknown environments presents significant
challenges, mainly due to two key limitations: insufficient semantic guidance
leads to inefficient exploration, while limited spatial memory resulting from
environmental structure causes entrapment in local regions. To address these
issues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object
navigation method based on the TARE hierarchical exploration framework,
integrating a viewpoint generation strategy balancing spatial coverage and
semantic density with an LLM-based global guidance mechanism. The performance
improvement of the proposed method is due to two key innovations. First, the
viewpoint generation strategy prioritizes areas of high semantic density within
traversable sub-regions to maximize spatial coverage and minimize invalid
exploration. Second, coupled with an LLM-based global guidance mechanism, it
assesses semantic associations to direct navigation toward high-value spaces,
preventing local entrapment and ensuring efficient exploration. Deployed on
hybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves
real-time operation and superior performance. On Matterport3D and
Habitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\% and
11.2\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,
respectively, over state-of-the-art methods.

</details>


### [1024] [IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks](https://arxiv.org/abs/2509.24768)
*Eric Hannus,Miika Malin,Tran Nguyen Le,Ville Kyrki*

Main category: cs.RO

TL;DR: 提出IA-VLA框架，利用大视觉语言模型增强VLA的语义理解能力，尤其在处理视觉重复对象和复杂语言指令时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型（VLA）受限于实时控制需求，难以使用大型语言模型，导致语言理解能力不足，难以处理复杂的操作指令，尤其是在存在视觉上难以区分的重复对象时。

Method: 提出IA-VLA框架，利用大型视觉语言模型（LVLM）作为预处理阶段，对复杂语言指令进行语义解析并生成增强上下文，再输入到轻量级VLA中执行动作决策。通过在包含视觉重复对象的数据集上对比基线VLA与两种增强版本的表现来验证方法有效性。

Result: 实验表明，IA-VLA在处理需要语义外推的复杂语言指令时显著优于基线模型，尤其在涉及视觉重复对象的任务中性能提升明显。

Conclusion: 通过引入大型语言模型进行语义预处理，可有效提升轻量级VLA在复杂语义任务中的表现，为解决机器人操作中语言理解瓶颈提供了可行方案。

Abstract: Vision-language-action models (VLAs) have become an increasingly popular
approach for addressing robot manipulation problems in recent years. However,
such models need to output actions at a rate suitable for robot control, which
limits the size of the language model they can be based on, and consequently,
their language understanding capabilities. Manipulation tasks may require
complex language instructions, such as identifying target objects by their
relative positions, to specify human intention. Therefore, we introduce IA-VLA,
a framework that utilizes the extensive language understanding of a large
vision language model as a pre-processing stage to generate improved context to
augment the input of a VLA. We evaluate the framework on a set of semantically
complex tasks which have been underexplored in VLA literature, namely tasks
involving visual duplicates, i.e., visually indistinguishable objects. A
dataset of three types of scenes with duplicate objects is used to compare a
baseline VLA against two augmented variants. The experiments show that the VLA
benefits from the augmentation scheme, especially when faced with language
instructions that require the VLA to extrapolate from concepts it has seen in
the demonstrations. For the code, dataset, and videos, see
https://sites.google.com/view/ia-vla.

</details>


### [1025] [Fidelity-Aware Data Composition for Robust Robot Generalization](https://arxiv.org/abs/2509.24797)
*Zizhao Tong,Di Chen,Sicheng Hu,Hongwei Fan,Liliang Chen,Guanghui Ren,Hao Tang,Hao Dong,Ling Shao*

Main category: cs.RO

TL;DR: 本文提出了一种新的数据组合框架CIFT，通过保持信息保真度来提升通用机器人策略的分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模同质视觉数据训练的通用机器人策略容易因捷径学习而导致分布外泛化性能下降。

Method: 提出Coherent Information Fidelity Tuning (CIFT)框架，利用特征空间几何结构衡量信息保真度，并引入Multi-View Video Augmentation (MVAug)生成因果解耦的数据谱进行优化。

Result: 在π₀和Diffusion Policy等策略架构上应用CIFT后，分布外任务成功率提升超过54%。

Conclusion: 保真度感知的数据组合是实现鲁棒、通用机器人学习的关键因素，优于单纯的数据增强。

Abstract: Generalist robot policies trained on large-scale, visually homogeneous
datasets can be susceptible to shortcut learning, which impairs their
out-of-distribution (OOD) generalization. While generative data augmentation is
a common approach to introduce diversity, it presents a subtle challenge: data
composition. Naively mixing real and synthetic data can corrupt the learning
signal, as this process often prioritizes visual diversity at the expense of
information fidelity. This paper suggests that robust generalization depends on
principled, fidelity-aware data composition. We introduce Coherent Information
Fidelity Tuning (CIFT), a framework that treats data composition as an
optimization problem. CIFT uses a practical proxy for Information Fidelity
based on the feature-space geometry of a dataset. This enables the
identification of a phase transition, termed the Decoherence Point, where
training stability degrades. The framework includes a generative engine,
Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled
data spectrum for this tuning process. Applying CIFT to policy architectures
such as $\pi_0$ and Diffusion Policy improves OOD success rates by over 54\%.
These results indicate that fidelity-aware composition, beyond data synthesis
alone, is an important component for developing robust, general-purpose robots.

</details>


### [1026] [Towards Modular and Accessible AUV Systems](https://arxiv.org/abs/2509.24864)
*Mingxi Zhou,Farhang Naderi,Yuewei Fu,Tony Jacob,Lin Zhao,Manavi Panjnani,Chengzhi Yuan,William McConnell,Emir Cem Gezer*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Marine Vehicle Packages (MVP)的新型开源模块化框架，用于自主水下航行器（AUV）的研究与开发。该框架包含软硬件设计，支持可扩展的硬件系统和模块化软件架构，并集成了关节推进器和高级图形用户界面等新功能。通过仿真和实地实验验证了MVP的性能和兼容性。


<details>
  <summary>Details</summary>
Motivation: 为了提高AUV研究中的可定制性、易构建性和有效载荷能力，开发一个开放、模块化的软硬件框架。

Method: 提出并实现了一个模块化的软硬件系统架构（MVP），包括可扩展的硬件设计和模块化软件架构，并集成关节推进器与高级图形用户界面，通过仿真和实地实验进行验证。

Result: 仿真和实地实验结果表明，MVP框架具有良好的性能和系统兼容性，能够支持多种AUV配置并提升研究灵活性。

Conclusion: MVP为AUV研究提供了一个灵活、开放且易于定制的平台，有助于推动海洋机器人技术的发展和普及。

Abstract: This paper reports the development of a new open-access modular framework,
called Marine Vehicle Packages (MVP), for Autonomous Underwater Vehicles. The
framework consists of both software and hardware designs allowing easy
construction of AUV for research with increased customizability and sufficient
payload capacity. This paper will present the scalable hardware system design
and the modular software design architecture. New features, such as articulated
thruster integration and high-level Graphic User Interface will be discussed.
Both simulation and field experiments results are shown to highlight the
performance and compatibility of the MVP.

</details>


### [1027] [Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction](https://arxiv.org/abs/2509.24867)
*Mariadas Capsran Roshan,Edgar M Hidalgo,Mats Isaksson,Michelle Dunn,Jagannatha Charjee Pyaraka*

Main category: cs.RO

TL;DR: 提出一种基于机器人安装的2D LiDAR方法，用于自动估计超声探头的初始位姿，实现胸部表面3D重建，并在人体试验中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图检查依赖操作者技术水平，且在资源匮乏地区缺乏专业人员；远程操作机器人超声虽可缓解该问题，但检查时间较长。自动化设置探头初始位姿有助于减轻操作负担。

Method: 采用机器人安装的2D LiDAR进行胸部表面3D重建，通过平面特征完成LiDAR与机器人基座间的外参标定，并利用非刚性模板匹配确定探头的初始位姿。

Result: LiDAR标定均方根残差为1.8 mm，旋转不确定性低于0.2°；模拟人实验显示表面重建平均误差为2.78±0.21 mm；人体试验（N=5）中探头初始位置与临床定义位置偏差为20-30 mm，同一名受试者多次重复试验间差异小于4 mm。

Conclusion: 该方法首次实现了机器人搭载2D LiDAR对人体表面的3D重建，具有较高的重建精度和良好的重复性，有望用于自动化超声检查流程中探头的初始定位。

Abstract: Echocardiography is a key imaging modality for cardiac assessment but remains
highly operator-dependent, and access to trained sonographers is limited in
underserved settings. Teleoperated robotic echocardiography has been proposed
as a solution; however, clinical studies report longer examination times than
manual procedures, increasing diagnostic delays and operator workload.
Automating non-expert tasks, such as automatically moving the probe to an ideal
starting pose, offers a pathway to reduce this burden. Prior vision- and
depth-based approaches to estimate an initial probe pose are sensitive to
lighting, texture, and anatomical variability. We propose a robot-mounted 2D
LiDAR-based approach that reconstructs the chest surface in 3D and estimates
the initial probe pose automatically. To the best of our knowledge, this is the
first demonstration of robot-mounted 2D LiDAR used for 3D reconstruction of a
human body surface. Through plane-based extrinsic calibration, the
transformation between the LiDAR and robot base frames was estimated with an
overall root mean square (RMS) residual of 1.8 mm and rotational uncertainty
below 0.2{\deg}. The chest front surface, reconstructed from two linear LiDAR
sweeps, was aligned with non-rigid templates to identify an initial probe pose.
A mannequin-based study assessing reconstruction accuracy showed mean surface
errors of 2.78 +/- 0.21 mm. Human trials (N=5) evaluating the proposed approach
found probe initial points typically 20-30 mm from the clinically defined
initial point, while the variation across repeated trials on the same subject
was less than 4 mm.

</details>


### [1028] [JuggleRL: Mastering Ball Juggling with a Quadrotor via Deep Reinforcement Learning](https://arxiv.org/abs/2509.24892)
*Shilong Ji,Yinuo Chen,Chuqi Wang,Jiayu Chen,Ruize Zhang,Feng Gao,Wenhao Tang,Shu'ang Yu,Sirui Xiang,Xinlei Chen,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 本文提出了JuggleRL，首个基于强化学习的无人机空中击球系统，通过仿真训练与零样本部署实现稳定空中杂耍，显著优于传统模型方法。


<details>
  <summary>Details</summary>
Motivation: 空中机器人在不确定性下进行精确、富含接触的操作具有挑战性，而现有方法难以满足空中击球任务对时序精度和持续适应性的高要求。

Method: 提出JuggleRL，采用强化学习在大规模仿真中学习闭环策略，结合动力学标定减小仿真到现实差距，使用奖励塑形和领域随机化提升鲁棒性，并输出中层指令由底层控制器执行，配合轻量通信协议实现高频状态估计与实时控制。

Result: 在真实硬件上零样本部署成功，平均311次击球（最高462次），远超基于模型的基线方法（平均3.1次，最多14次）；且能泛化至未见场景，成功击打5g轻球，平均145.9次。

Conclusion: 强化学习可有效赋能 aerial robots 在动态交互任务中实现稳健、稳定的控制，验证了其在复杂接触任务中的潜力。

Abstract: Aerial robots interacting with objects must perform precise, contact-rich
maneuvers under uncertainty. In this paper, we study the problem of aerial ball
juggling using a quadrotor equipped with a racket, a task that demands accurate
timing, stable control, and continuous adaptation. We propose JuggleRL, the
first reinforcement learning-based system for aerial juggling. It learns
closed-loop policies in large-scale simulation using systematic calibration of
quadrotor and ball dynamics to reduce the sim-to-real gap. The training
incorporates reward shaping to encourage racket-centered hits and sustained
juggling, as well as domain randomization over ball position and coefficient of
restitution to enhance robustness and transferability. The learned policy
outputs mid-level commands executed by a low-level controller and is deployed
zero-shot on real hardware, where an enhanced perception module with a
lightweight communication protocol reduces delays in high-frequency state
estimation and ensures real-time control. Experiments show that JuggleRL
achieves an average of $311$ hits over $10$ consecutive trials in the real
world, with a maximum of $462$ hits observed, far exceeding a model-based
baseline that reaches at most $14$ hits with an average of $3.1$. Moreover, the
policy generalizes to unseen conditions, successfully juggling a lighter $5$ g
ball with an average of $145.9$ hits. This work demonstrates that reinforcement
learning can empower aerial robots with robust and stable control in dynamic
interaction tasks.

</details>


### [1029] [DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits](https://arxiv.org/abs/2509.24903)
*Lantao Li,Kang Yang,Rui Song,Chen Sun*

Main category: cs.RO

TL;DR: 本文提出了DRCP框架，通过跨模态协同感知和基于扩散的特征优化模块，提升动态环境中自动驾驶系统的感知鲁棒性和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法在实际应用中面临部分检测缺失和噪声累积等问题，影响检测精度，亟需提升复杂场景下的感知鲁棒性。

Method: 提出DRCP框架，包含Precise-Pyramid-Cross-Modality-Cross-Agent模块（基于注意力融合与自适应卷积）和Mask-Diffusion-Mask-Aggregation模块（轻量级扩散模型用于特征优化）。

Result: 该系统在移动平台上实现实时性能，并在挑战性条件下显著提升感知鲁棒性。

Conclusion: DRCP有效解决了协同感知中的部分检测和噪声问题，为自动驾驶在复杂环境中的部署提供了可行方案。

Abstract: Cooperative perception enabled by Vehicle-to-Everything communication has
shown great promise in enhancing situational awareness for autonomous vehicles
and other mobile robotic platforms. Despite recent advances in perception
backbones and multi-agent fusion, real-world deployments remain challenged by
hard detection cases, exemplified by partial detections and noise accumulation
which limit downstream detection accuracy. This work presents Diffusion on
Reinforced Cooperative Perception (DRCP), a real-time deployable framework
designed to address aforementioned issues in dynamic driving environments. DRCP
integrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,
a cross-modal cooperative perception module that leverages
camera-intrinsic-aware angular partitioning for attention-based fusion and
adaptive convolution to better exploit external features; and (2)
Mask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement
module that encourages robustness against feature perturbations and aligns
bird's-eye-view features closer to the task-optimal manifold. The proposed
system achieves real-time performance on mobile platforms while significantly
improving robustness under challenging conditions. Code will be released in
late 2025.

</details>


### [1030] [Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation](https://arxiv.org/abs/2509.24907)
*Thanh Long Nguyen,Duc Phu Nguyen,Thanh Thao Ton Nu,Quan Le,Thuan Hoang Tran,Manh Duong Phung*

Main category: cs.RO

TL;DR: 提出一种基于RGB-D相机的框架，用于识别多人交互场景中的兴趣点和参与区域，以实现社交感知导航。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统多关注避障，忽视了社会线索，导致人机交互不够自然。为了使机器人能够在共享环境中更安全、自然地导航，需要识别并理解人类群体的交互行为。

Method: 利用单目RGB-D相机获取彩色和深度图像，估计3D人体关键点和位置；通过主成分分析（PCA）确定主要交互方向；使用鞋带公式计算兴趣点和参与区域。

Result: 实验表明该方法能在不同人数和场景下有效识别群体交互，处理速度约为每帧4毫秒，可在单板计算机上实时运行，并已实现为ROS 2包便于集成。

Conclusion: 所提方法高效、准确，适用于社交感知导航系统，提升了机器人对人类交互行为的理解能力，有助于实现更自然的人机共存。

Abstract: {Recognizing human interactions is essential for social robots as it enables
them to navigate safely and naturally in shared environments. Conventional
robotic systems however often focus on obstacle avoidance, neglecting social
cues necessary for seamless human-robot interaction. To address this gap, we
propose a framework to recognize human group interactions for socially aware
navigation. Our method utilizes color and depth frames from a monocular RGB-D
camera to estimate 3D human keypoints and positions. Principal component
analysis (PCA) is then used to determine dominant interaction directions. The
shoelace formula is finally applied to compute interest points and engagement
areas. Extensive experiments have been conducted to evaluate the validity of
the proposed method. The results show that our method is capable of recognizing
group interactions across different scenarios with varying numbers of
individuals. It also achieves high-speed performance, processing each frame in
approximately 4 ms on a single-board computer used in robotic systems. The
method is implemented as a ROS 2 package making it simple to integrate into
existing navigation systems. Source code is available at
https://github.com/thanhlong103/social-interaction-detector

</details>


### [1031] [From Code to Action: Hierarchical Learning of Diffusion-VLM Policies](https://arxiv.org/abs/2509.24917)
*Markus Peschl,Pietro Mazzaglia,Daniel Dijkman*

Main category: cs.RO

TL;DR: 提出一种结合视觉-语言模型和扩散策略的分层框架，用于提升机器人操作中模仿学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在复杂、长视野任务中常受限于泛化能力和数据稀缺问题。

Method: 利用开源机器人API提供的子任务函数作为结构化监督信号，训练视觉-语言模型将任务分解为可执行子程序，并通过带记忆机制的扩散策略实现低层行为模仿。

Result: 该方法实现了可解释的策略分解，提升了相比扁平策略的泛化能力，并支持高层规划与低层控制的分离评估。

Conclusion: 所提出的分层架构通过代码生成VLM与扩散策略结合，有效解决了长时程任务中的模仿学习挑战。

Abstract: Imitation learning for robotic manipulation often suffers from limited
generalization and data scarcity, especially in complex, long-horizon tasks. In
this work, we introduce a hierarchical framework that leverages code-generating
vision-language models (VLMs) in combination with low-level diffusion policies
to effectively imitate and generalize robotic behavior. Our key insight is to
treat open-source robotic APIs not only as execution interfaces but also as
sources of structured supervision: the associated subtask functions - when
exposed - can serve as modular, semantically meaningful labels. We train a VLM
to decompose task descriptions into executable subroutines, which are then
grounded through a diffusion policy trained to imitate the corresponding robot
behavior. To handle the non-Markovian nature of both code execution and certain
real-world tasks, such as object swapping, our architecture incorporates a
memory mechanism that maintains subtask context across time. We find that this
design enables interpretable policy decomposition, improves generalization when
compared to flat policies and enables separate evaluation of high-level
planning and low-level control.

</details>


### [1032] [CineWild: Balancing Art and Robotics for Ethical Wildlife Documentary Filmmaking](https://arxiv.org/abs/2509.24921)
*Pablo Pueyo,Fernando Caballero,Ana Cristina Murillo,Eduardo Montijano*

Main category: cs.RO

TL;DR: 本文提出了CineWild，一个结合机器人技术、电影拍摄和伦理考量的自主无人机框架，用于在野生动物纪录片拍摄中平衡画面质量与动物福利。


<details>
  <summary>Details</summary>
Motivation: 无人机在野生动物纪录片拍摄中具有巨大潜力，但可能干扰动物行为，因此需要兼顾拍摄效果与动物保护的解决方案。

Method: 基于模型预测控制（MPC）设计CineWild系统，实现动态调整飞行路径和相机参数，包括自适应变焦、避开动物视野的路径规划以及低噪声平滑机动。

Result: 通过仿真研究验证了CineWild在保持安全距离的同时获取高质量影像的能力，并展示了其在伦理与技术之间的平衡有效性。

Conclusion: CineWild体现了工程、视觉叙事与环境伦理的跨学科融合，为负责任的野生动物拍摄提供了可行的技术框架。

Abstract: Drones, or unmanned aerial vehicles (UAVs), have become powerful tools across
domains-from industry to the arts. In documentary filmmaking, they offer
dynamic, otherwise unreachable perspectives, transforming how stories are told.
Wildlife documentaries especially benefit, yet drones also raise ethical
concerns: the risk of disturbing the animals they aim to capture. This paper
introduces CineWild, an autonomous UAV framework that combines robotics,
cinematography, and ethics. Built on model predictive control, CineWild
dynamically adjusts flight paths and camera settings to balance cinematic
quality with animal welfare. Key features include adaptive zoom for filming
from acoustic and visual safe distances, path-planning that avoids an animal's
field of view, and smooth, low-noise maneuvers. CineWild exemplifies
interdisciplinary innovation-bridging engineering, visual storytelling, and
environmental ethics. We validate the system through simulation studies and
will release the code upon acceptance.

</details>


### [1033] [Trajectory Prediction via Bayesian Intention Inference under Unknown Goals and Kinematics](https://arxiv.org/abs/2509.24928)
*Shunan Yin,Zehui Lu,Shaoshuai Mou*

Main category: cs.RO

TL;DR: 提出一种基于自适应贝叶斯的实时轨迹预测方法，通过联合估计目标意图和运动特性，实现对动态变化意图的鲁棒预测。


<details>
  <summary>Details</summary>
Motivation: 在未知且可能变化的意图与运动特性下，现有方法难以实现实时、鲁棒的轨迹预测。

Method: 采用自适应贝叶斯框架，将目标意图建模为马尔可夫潜在状态，同时估计意图及其遵循最短路径策略的参数；结合采样机制生成带不确定性量化概率预测。

Result: 在数值实验和硬件平台（四旋翼、四足机器人）上验证，该方法在500次蒙特卡洛试验中显著优于非自适应和部分自适应方法，运行频率约270Hz。

Conclusion: 所提方法无需训练或详细先验知识，具备高实时性和鲁棒性，适用于多种机器人系统中的意图变化场景。

Abstract: This work introduces an adaptive Bayesian algorithm for real-time trajectory
prediction via intention inference, where a target's intentions and motion
characteristics are unknown and subject to change. The method concurrently
estimates two critical variables: the target's current intention, modeled as a
Markovian latent state, and an intention parameter that describes the target's
adherence to a shortest-path policy. By integrating this joint update
technique, the algorithm maintains robustness against abrupt intention shifts
and unknown motion dynamics. A sampling-based trajectory prediction mechanism
then exploits these adaptive estimates to generate probabilistic forecasts with
quantified uncertainty. We validate the framework through numerical
experiments: Ablation studies of two cases, and a 500-trial Monte Carlo
analysis; Hardware demonstrations on quadrotor and quadrupedal platforms.
Experimental results demonstrate that the proposed approach significantly
outperforms non-adaptive and partially adaptive methods. The method operates in
real time around 270 Hz without requiring training or detailed prior knowledge
of target behavior, showcasing its applicability in various robotic systems.

</details>


### [1034] [World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training](https://arxiv.org/abs/2509.24948)
*Junjin Xiao,Yandan Yang,Xinyuan Chang,Ronghan Chen,Feng Xiong,Mu Xu,Wei-Shi Zheng,Qing Zhang*

Main category: cs.RO

TL;DR: 提出World-Env框架，利用基于世界模型的虚拟模拟器进行强化学习后训练，解决视觉-语言-动作模型在数据稀缺、不可重置环境中的性能下降和任务完成检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖大量演示数据，在数据稀缺场景下性能差；真实环境不可重置导致强化学习难以应用；缺乏任务完成检测机制导致冗余动作。

Method: 构建World-Env框架，包含基于视频的世界模拟器生成未来视觉观测，以及由视觉-语言模型引导的即时反射器提供奖励信号并预测动作终止。

Result: 在仅需每任务5个专家示范的情况下显著提升性能，在复杂机器人操作任务中验证了其在数据效率、安全性和执行效率上的优势。

Conclusion: World-Env为资源受限场景下的VLA模型后训练提供了安全、高效且可扩展的解决方案。

Abstract: Vision-Language-Action (VLA) models trained via imitation learning suffer
from significant performance degradation in data-scarce scenarios due to their
reliance on large-scale demonstration datasets. Although reinforcement learning
(RL)-based post-training has proven effective in addressing data scarcity, its
application to VLA models is hindered by the non-resettable nature of
real-world environments. This limitation is particularly critical in high-risk
domains such as industrial automation, where interactions often induce state
changes that are costly or infeasible to revert. Furthermore, existing VLA
approaches lack a reliable mechanism for detecting task completion, leading to
redundant actions that reduce overall task success rates. To address these
challenges, we propose World-Env, an RL-based post-training framework that
replaces physical interaction with a low-cost, world model-based virtual
simulator. World-Env consists of two key components: (1) a video-based world
simulator that generates temporally consistent future visual observations, and
(2) a vision-language model (VLM)-guided instant reflector that provides
continuous reward signals and predicts action termination. This simulated
environment enables VLA models to safely explore and generalize beyond their
initial imitation learning distribution. Our method achieves notable
performance gains with as few as five expert demonstrations per task.
Experiments on complex robotic manipulation tasks demonstrate that World-Env
effectively overcomes the data inefficiency, safety constraints, and
inefficient execution of conventional VLA models that rely on real-world
interaction, offering a practical and scalable solution for post-training in
resource-constrained settings.

</details>


### [1035] [MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation](https://arxiv.org/abs/2509.24956)
*Jan Ole von Hartz,Lukas Schweizer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: 提出多流生成策略（MSG），通过在推理时组合多个以对象为中心的策略，显著提高样本效率和泛化能力，仅需5次演示即可学习高质量策略，并实现零样本对象实例迁移。


<details>
  <summary>Details</summary>
Motivation: 生成式机器人策略（如Flow Matching）虽灵活但样本效率低，现有对象中心策略未能解决此问题。

Method: 设计一种模型无关、仅在推理时组合的多流生成策略框架（MSG），训练多个对象中心策略并在推理时融合。

Result: 在仿真和真实机器人上验证，相比单流方法减少95%演示需求，性能提升89%，并支持零样本对象实例迁移。

Conclusion: MSG显著提升了生成式策略的样本效率和泛化能力，具有广泛适用性和实用价值。

Abstract: Generative robot policies such as Flow Matching offer flexible, multi-modal
policy learning but are sample-inefficient. Although object-centric policies
improve sample efficiency, it does not resolve this limitation. In this work,
we propose Multi-Stream Generative Policy (MSG), an inference-time composition
framework that trains multiple object-centric policies and combines them at
inference to improve generalization and sample efficiency. MSG is
model-agnostic and inference-only, hence widely applicable to various
generative policies and training paradigms. We perform extensive experiments
both in simulation and on a real robot, demonstrating that our approach learns
high-quality generative policies from as few as five demonstrations, resulting
in a 95% reduction in demonstrations, and improves policy performance by 89
percent compared to single-stream approaches. Furthermore, we present
comprehensive ablation studies on various composition strategies and provide
practical recommendations for deployment. Finally, MSG enables zero-shot object
instance transfer. We make our code publicly available at
https://msg.cs.uni-freiburg.de.

</details>


### [1036] [Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks](https://arxiv.org/abs/2509.24972)
*Vijja Wichitwechkarn,Emlyn Williams,Charles Fox,Ruchi Choudhary*

Main category: cs.RO

TL;DR: 提出一种无需额外训练或手动标注即可处理长视野多步任务的一次性模仿学习方法，在多步和单步操作任务中分别达到82.5%和90%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的一次性模仿学习方法在处理长视野、多步任务时受限，需要额外模型训练或人工标注，难以扩展到复杂任务。

Method: 提出一种新方法，利用单个人类示范，结合预训练特征提取器，在不进行额外模型训练或手动标注的情况下实现多步任务的模仿学习。

Result: 在多步和单步操作任务中分别取得82.5%和90%的平均成功率，性能优于基线方法，并评估了不同预训练特征提取器的效率与性能。

Conclusion: 该方法有效提升了在一次性模仿学习中对长视野多步任务的处理能力，具有良好的泛化性和计算效率。

Abstract: Recent advances in one-shot imitation learning have enabled robots to acquire
new manipulation skills from a single human demonstration. While existing
methods achieve strong performance on single-step tasks, they remain limited in
their ability to handle long-horizon, multi-step tasks without additional model
training or manual annotation. We propose a method that can be applied to this
setting provided a single demonstration without additional model training or
manual annotation. We evaluated our method on multi-step and single-step
manipulation tasks where our method achieves an average success rate of 82.5%
and 90%, respectively. Our method matches and exceeds the performance of the
baselines in both these cases. We also compare the performance and
computational efficiency of alternative pre-trained feature extractors within
our framework.

</details>


### [1037] [Path Diffuser: Diffusion Model for Data-Driven Traffic Simulator](https://arxiv.org/abs/2509.24995)
*Da Saem Lee,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: 提出Path Diffuser（PD），一种无需历史轨迹信息、基于地图条件的两阶段扩散模型，用于生成多样且符合道路规则的交通场景代理姿态初始化和轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有学习型模拟器依赖历史轨迹和标注数据，难以生成新场景，且在无历史信息或分布外地图下生成轨迹不真实，限制了多样性与可扩展性。

Method: 采用两阶段扩散模型，结合Frenet坐标系下的运动基元先验，在无历史轨迹条件下根据地图生成代理初始位姿与轨迹，并建模多智能体交互。

Result: 在Argoverse2数据集上实验表明，PD在分布性、常识性及道路合规性指标上分别超越基线方法1.92倍、1.14倍和1.62倍，并在分布外地图上展现出良好泛化能力。

Conclusion: Path Diffuser能在无需历史轨迹输入的情况下生成多样化、现实且符合道路约束的交通场景，显著提升仿真系统的可扩展性与实用性。

Abstract: Simulating diverse and realistic traffic scenarios is critical for developing
and testing autonomous planning. Traditional rule-based planners lack diversity
and realism, while learning-based simulators often replay, forecast, or edit
scenarios using historical agent trajectories. However, they struggle to
generate new scenarios, limiting scalability and diversity due to their
reliance on fully annotated logs and historical data. Thus, a key challenge for
a learning-based simulator's performance is that it requires agents' past
trajectories and pose information in addition to map data, which might not be
available for all agents on the road.Without which, generated scenarios often
produce unrealistic trajectories that deviate from drivable areas, particularly
under out-of-distribution (OOD) map scenes (e.g., curved roads). To address
this, we propose Path Diffuser (PD): a two-stage, diffusion model for
generating agent pose initializations and their corresponding trajectories
conditioned on the map, free of any historical context of agents' trajectories.
Furthermore, PD incorporates a motion primitive-based prior, leveraging Frenet
frame candidate trajectories to enhance diversity while ensuring road-compliant
trajectory generation. We also explore various design choices for modeling
complex multi-agent interactions. We demonstrate the effectiveness of our
method through extensive experiments on the Argoverse2 Dataset and additionally
evaluate the generalizability of the approach on OOD map variants. Notably,
Path Diffuser outperforms the baseline methods by 1.92x on distribution
metrics, 1.14x on common-sense metrics, and 1.62x on road compliance from
adversarial benchmarks.

</details>


### [1038] [AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation](https://arxiv.org/abs/2509.25032)
*Ryosuke Takanami,Petr Khrapchenkov,Shu Morikuni,Jumpei Arima,Yuta Takaba,Shunsuke Maeda,Takuya Okubo,Genki Sano,Satoshi Sekioka,Aoi Kadoya,Motonari Kambara,Naoya Nishiura,Haruto Suzuki,Takanori Yoshimoto,Koya Sakamoto,Shinnosuke Ono,Hu Yang,Daichi Yashima,Aoi Horo,Tomohiro Motoda,Kensuke Chiyoma,Hiroshi Ito,Koki Fukuda,Akihito Goto,Kazumi Morinaga,Yuya Ikeda,Riko Kawada,Masaki Yoshikawa,Norio Kosuge,Yuki Noguchi,Kei Ota,Tatsuya Matsushima,Yusuke Iwasawa,Yutaka Matsuo,Tetsuya Ogata*

Main category: cs.RO

TL;DR: AIRoA MoMa是一个大规模真实世界多模态移动操作数据集，包含同步的视觉、力矩和机器人状态数据，以及用于分层学习和错误分析的双层标注，支持自然语言指令下的长视野、接触丰富的任务。


<details>
  <summary>Details</summary>
Motivation: 现有移动操作数据集缺乏同步的力-扭矩感知、分层标注和显式失败案例，限制了具身智能体在非结构化环境中的鲁棒性发展。

Method: 收集了25,469段使用HSR机器人执行的移动操作任务数据，同步记录RGB图像、关节状态、六轴腕部力-扭矩信号和内部机器人状态，并提出两层标注结构（子目标与基本动作）以支持分层学习和错误分析，数据已标准化为LeRobot v2.1格式。

Result: 构建了首个包含同步力-扭矩信号和分层任务标注的大规模真实世界移动操作数据集，涵盖约94小时的真实操作数据，可作为Vision-Language-Action模型的基准测试资源。

Conclusion: AIRoA MoMa数据集填补了移动操作领域在接触感知、长视野任务结构和错误分析方面的空白，为开发能在人类环境中可靠执行自然语言指令的通用机器人代理提供了重要基础。

Abstract: As robots transition from controlled settings to unstructured human
environments, building generalist agents that can reliably follow natural
language instructions remains a central challenge. Progress in robust mobile
manipulation requires large-scale multimodal datasets that capture contact-rich
and long-horizon tasks, yet existing resources lack synchronized force-torque
sensing, hierarchical annotations, and explicit failure cases. We address this
gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset
for mobile manipulation. It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis. The initial dataset comprises 25,469
episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is
fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile
manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa
provides a critical benchmark for advancing the next generation of
Vision-Language-Action models. The first version of our dataset is now
available at https://huggingface.co/datasets/airoa-org/airoa-moma .

</details>


### [1039] [AgriCruiser: An Open Source Agriculture Robot for Over-the-row Navigation](https://arxiv.org/abs/2509.25056)
*Kenny Truong,Yongkyu Lee,Jason Irie,Shivam Kumar Panda,Shahab Ahmad,Md. Mukhlesur Rahman,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 本文介绍了一种名为AgriCruiser的开源跨行农业机器人，具有低成本、易部署和可适应多种作物的特点，配备精确喷洒系统，在亚麻田中显著减少杂草并减少作物损伤，且可在多种地面上稳定行驶，整体材料成本约为5000-6000美元，并公开设计文件以促进模块化农业机器人的研究与应用。


<details>
  <summary>Details</summary>
Motivation: 开发一种低成本、可重构且适用于多种作物和行距的跨行农业机器人，以解决传统农业中杂草管理效率低、劳动力需求高和作物损伤大的问题。

Method: 设计并构建AgriCruiser机器人平台，采用商品化T型槽型材降低制造成本，实现可调节轮距和高离地间隙，并集成精准喷洒系统；在12块亚麻田中进行单次自动喷洒作业，与4块人工除草田对比评估杂草控制效果和作物损伤情况，同时在多种地面（混凝土、沥青、碎石、草地、湿/干土壤）上测试其移动性能。

Result: AgriCruiser在亚麻田中单次喷洒使总杂草数量减少24至42倍，相比人工除草造成更少作物损伤；机器人可在多种地形可靠行驶，转弯半径为0.71–0.79米，便于高效地头转向；整车材料成本约5000–6000美元，易于复制和定制。

Conclusion: 低成本、可重构的跨行农业机器人能够有效实现杂草管理，减少作物损伤和人力投入，同时为植物表型、传感及其他农业应用提供了通用平台，开放设计有助于推动模块化农业机器人的研究与普及。

Abstract: We present the AgriCruiser, an open-source over-the-row agricultural robot
developed for low-cost deployment and rapid adaptation across diverse crops and
row layouts. The chassis provides an adjustable track width of 1.42 m to 1.57
m, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact
pivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland
maneuvers. The platform is designed for the integration of the other
subsystems, and in this study, a precision spraying system was implemented to
assess its effectiveness in weed management. In twelve flax plots, a single
robotic spray pass reduced total weed populations (pigweed and Venice mallow)
by 24- to 42-fold compared to manual weeding in four flax plots, while also
causing less crop damage. Mobility experiments conducted on concrete, asphalt,
gravel, grass, and both wet and dry soil confirmed reliable traversal
consistent with torque sizing. The complete chassis can be constructed from
commodity T-slot extrusion with minimal machining, resulting in a bill of
materials costing approximately $5,000 - $6,000, which enables replication and
customization. The mentioned results demonstrate that low-cost, reconfigurable
over-the-row robots can achieve effective weed management with reduced crop
damage and labor requirements, while providing a versatile foundation for
phenotyping, sensing, and other agriculture applications. Design files and
implementation details are released to accelerate research and adoption of
modular agricultural robotics.

</details>


### [1040] [Crop Spirals: Re-thinking the field layout for future robotic agriculture](https://arxiv.org/abs/2509.25091)
*Lakshan Lavan,Lanojithan Thiyagarasa,Udara Muthugala,Rajitha de Silva*

Main category: cs.RO

TL;DR: 提出了一种机器人中心的方形螺旋布局，结合中央轨道线和专用导航栈，显著提升了农业机器人在路径长度和任务执行速度方面的性能。


<details>
  <summary>Details</summary>
Motivation: 传统线性作物布局不利于机器人导航，存在转弯困难、行程长和感知混淆等问题。

Method: 设计了带有中央轨道线的方形螺旋布局，并开发了结合DH-ResNet18航点回归、像素到里程计映射、A*规划和模型预测控制（MPC）的导航系统。

Result: 仿真结果显示，与线性布局相比，螺旋布局最多可缩短28%的路径，航点任务执行速度快约25%，多机器人协调效率也更高。

Conclusion: 重新设计田间几何结构能有效提升自主农业机器人的导航与作业效率。

Abstract: Conventional linear crop layouts, optimised for tractors, hinder robotic
navigation with tight turns, long travel distances, and perceptual aliasing. We
propose a robot-centric square spiral layout with a central tramline, enabling
simpler motion and more efficient coverage. To exploit this geometry, we
develop a navigation stack combining DH-ResNet18 waypoint regression,
pixel-to-odometry mapping, A* planning, and model predictive control (MPC). In
simulations, the spiral layout yields up to 28% shorter paths and about 25%
faster execution for waypoint-based tasks across 500 waypoints than linear
layouts, while full-field coverage performance is comparable to an optimised
linear U-turn strategy. Multi-robot studies demonstrate efficient coordination
on the spirals rule-constrained graph, with a greedy allocator achieving 33-37%
lower batch completion times than a Hungarian assignment under our setup. These
results highlight the potential of redesigning field geometry to better suit
autonomous agriculture.

</details>


### [1041] [Safe Planning in Unknown Environments using Conformalized Semantic Maps](https://arxiv.org/abs/2509.25124)
*David Smith Sundarsingh,Yifei Li,Tianji Tang,George J. Pappas,Nikolay Atanasov,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 本文提出了一种在感知不确定的未知环境中进行语义避障规划的新方法，首次实现了无需传感器模型知识即可满足用户指定任务完成率的规划器。


<details>
  <summary>Details</summary>
Motivation: 现有规划算法要么忽略感知不确定性，缺乏正确性保证，要么依赖已知的传感器模型和噪声特性，难以应用于实际未知环境。

Method: 利用共形预测以模型无关和分布无关的方式量化语义地图中的不确定性，并在运行时从感知数据构建语义地图，从而实现鲁棒路径规划。

Result: 通过大量实验验证了该方法的有效性，任务成功率始终优于基线方法，并符合理论预期的任务完成率。

Conclusion: 所提出的方法能够在不依赖传感器模型的情况下，为语义避障任务提供具有概率保证的规划解决方案。

Abstract: This paper addresses semantic planning problems in unknown environments under
perceptual uncertainty. The environment contains multiple unknown semantically
labeled regions or objects, and the robot must reach desired locations while
maintaining class-dependent distances from them. We aim to compute robot paths
that complete such semantic reach-avoid tasks with user-defined probability
despite uncertain perception. Existing planning algorithms either ignore
perceptual uncertainty - thus lacking correctness guarantees - or assume known
sensor models and noise characteristics. In contrast, we present the first
planner for semantic reach-avoid tasks that achieves user-specified mission
completion rates without requiring any knowledge of sensor models or noise.
This is enabled by quantifying uncertainty in semantic maps - constructed
on-the-fly from perceptual measurements - using conformal prediction in a
model- and distribution-free manner. We validate our approach and the
theoretical mission completion rates through extensive experiments, showing
that it consistently outperforms baselines in mission success rates.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1042] [Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives](https://arxiv.org/abs/2509.23026)
*Yue Wang*

Main category: cs.MA

TL;DR: 本文提出了多目标马尔可夫博弈（MOMG）框架，并引入帕累托-纳什均衡（PNE）作为求解概念，证明了其存在性并与线性标量化游戏的纳什均衡建立了等价关系；为应对计算挑战，设计了可分离探索与规划的两阶段在线学习算法，能高效刻画整个帕累托-纳什前沿。


<details>
  <summary>Details</summary>
Motivation: 在实际多智能体系统中，智能体常具有多样化目标，导致系统复杂性增加，传统单目标方法难以处理多目标间的策略权衡，因此需要新的建模框架与求解机制。

Method: 提出多目标马尔可夫博弈（MOMG）模型，定义帕累托-纳什均衡（PNE）为解概念，建立其与线性标量化单目标马尔可夫博弈纳什均衡的等价性，并设计基于两阶段、偏好无关的在线学习算法，实现探索与规划的解耦。

Result: 证明了PNE的存在性及其与标量化游戏纳什均衡的等价性，提出了可计算任意偏好下PNE的高效算法，无需额外采样即可刻画完整的帕累托-纳什前沿。

Conclusion: MOMG为多目标多智能体强化学习提供了理论基础，所提算法通过解耦探索与规划显著提升了求解效率，实现了对Pareto-Nash前沿的有效刻画。

Abstract: In practical multi-agent systems, agents often have diverse objectives, which
makes the system more complex, as each agent's performance across multiple
criteria depends on the joint actions of all agents, creating intricate
strategic trade-offs. To address this, we introduce the Multi-Objective Markov
Game (MOMG), a framework for multi-agent reinforcement learning with multiple
objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary
solution concept, where no agent can unilaterally improve one objective without
sacrificing performance on another. We prove existence of PNE, and establish an
equivalence between the PNE and the set of Nash Equilibria of MOMG's
corresponding linearly scalarized games, enabling solutions of MOMG by
transferring to a standard single-objective Markov game. However, we note that
computing a PNE is theoretically and computationally challenging, thus we
propose and study weaker but more tractable solution concepts. Building on
these foundations, we develop online learning algorithm that identify a single
solution to MOMGs. Furthermore, we propose a two-phase, preference-free
algorithm that decouples exploration from planning. Our algorithm enables
computation of a PNE for any given preference profile without collecting new
samples, providing an efficient methodological characterization of the entire
Pareto-Nash front.

</details>


### [1043] [Situational Awareness for Safe and Robust Multi-Agent Interactions Under Uncertainty](https://arxiv.org/abs/2509.23425)
*Benjamin Alcorn,Eman Hammad*

Main category: cs.MA

TL;DR: 本文研究了多智能体系统中如何预测非协作智能体的意图并在资源受限情况下优化决策，提出了一种基于观测和历史轨迹的估计算法，并通过强化学习和博弈论框架进行验证。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，准确预测其他智能体的行为并在此基础上做出最优决策是一个关键挑战，尤其是在缺乏直接观测的情况下。

Method: 设计一个自主智能体模型，该模型可在安全观测半径内感知环境、识别周围智能体状态、预测其未来行为，并结合历史轨迹使用估计算法进行决策；同时引入风险分析来管理预测不确定性。

Result: 所提出的估计算法能够在无直接观测时有效预测其他智能体的行为，且通过强化学习和博弈论两种学习框架验证了该方法的有效性。

Conclusion: 该方法能够提升多智能体系统中对非协作智能体行为的预测能力，并在资源约束下实现较优的决策性能。

Abstract: Multi-agent systems are prevalent in a wide range of domains including power
systems, vehicular networks, and robotics. Two important problems to solve in
these types of systems are how the intentions of non-coordinating agents can be
determined to predict future behavior and how the agents can achieve their
objectives under resource constraints without significantly sacrificing
performance. To study this, we develop a model where an autonomous agent
observes the environment within a safety radius of observation, determines the
state of a surrounding agent of interest (within the observation radius),
estimates future actions to be taken, and acts in an optimal way. In the
absence of observations, agents are able to utilize an estimation algorithm to
predict the future actions of other agents based on historical trajectory. The
use of the proposed estimation algorithm introduces uncertainty, which is
managed via risk analysis. The proposed approach in this study is validated
using two different learning-based decision making frameworks: reinforcement
learning and game theoretic algorithms.

</details>


### [1044] [PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features](https://arxiv.org/abs/2509.24046)
*Lingyao Li,Haolun Wu,Zhenkun Li,Jiabei Hu,Yu Wang,Xiaoshan Huang,Wenyue Hua,Wenqian Wang*

Main category: cs.MA

TL;DR: 提出PartnerMAS，一种分层多智能体框架，用于高维决策任务中的合作伙伴选择，通过规划、专业化评估和监督整合，在风险投资共同投资数据集上优于单智能体和基于辩论的方法。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体或辩论式系统在处理高维、异构特征的决策任务时存在可扩展性和一致性问题，难以有效支持如商业伙伴选择等复杂决策。

Method: 设计一个包含规划智能体、专门化智能体和监督智能体的三层多智能体系统，并构建一个包含真实联合投资信息的风险投资基准数据集进行评估。

Result: 在140个案例中，PartnerMAS比基线模型匹配率高出10-15%，智能体分析显示各角色分工明确且互补，监督智能体在结果整合中起关键作用。

Conclusion: 结构化的多智能体协作能比单纯扩大单个模型产生更稳健的决策结果，PartnerMAS为数据丰富场景下的高维决策提供了有效框架。

Abstract: High-dimensional decision-making tasks, such as business partner selection,
involve evaluating large candidate pools with heterogeneous numerical,
categorical, and textual features. While large language models (LLMs) offer
strong in-context reasoning capabilities, single-agent or debate-style systems
often struggle with scalability and consistency in such settings. We propose
PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation
into three layers: a Planner Agent that designs strategies, Specialized Agents
that perform role-specific assessments, and a Supervisor Agent that integrates
their outputs. To support systematic evaluation, we also introduce a curated
benchmark dataset of venture capital co-investments, featuring diverse firm
attributes and ground-truth syndicates. Across 140 cases, PartnerMAS
consistently outperforms single-agent and debate-based multi-agent baselines,
achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows
that planners are most responsive to domain-informed prompts, specialists
produce complementary feature coverage, and supervisors play an important role
in aggregation. Our findings demonstrate that structured collaboration among
LLM agents can generate more robust outcomes than scaling individual models,
highlighting PartnerMAS as a promising framework for high-dimensional
decision-making in data-rich domains.

</details>


### [1045] [CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems](https://arxiv.org/abs/2509.24088)
*Yifan Yu,Moyan Li,Shaoyuan Xu,Jinmiao Fu,Xinhai Hou,Fan Lai,Bryan Wang*

Main category: cs.MA

TL;DR: 本文提出了CORRECT，一种轻量级、无需训练的框架，利用在线缓存的错误模式来识别多智能体系统中的结构化错误，并通过大规模数据集CORRECT-Error验证其在多种应用中显著提升错误定位性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的微小错误可能传播并导致任务失败，且错误轨迹复杂、难以调试，现有方法难以高效识别和复用错误结构。

Method: 提出CORRECT框架，通过在线缓存提取和重用错误模式，结合LLM实现推理时的精准错误定位；构建包含2000多个标注轨迹的大规模数据集CORRECT-Error，基于真实分布注入错误并经人工验证。

Result: 在七个不同多智能体应用场景中，CORRECT相比现有方法将步骤级错误定位性能最高提升19.8%，且开销接近为零。

Conclusion: CORRECT通过结构化错误模式的缓存与迁移，实现了高效、自适应的错误识别，显著缩小了自动化与人类水平错误识别之间的差距。

Abstract: Multi-agent systems (MAS) are increasingly capable of tackling complex
real-world tasks, yet their reliance on inter-agent coordination, tool use, and
long-horizon reasoning makes error recognition particularly challenging. Minor
errors can propagate across agents, escalating into task failures while
producing long, intertwined execution trajectories that impose significant
costs for both human developers and automated systems to debug and analyze. Our
key insight is that, despite surface differences in failure trajectories (e.g.,
logs), MAS errors often recur with similar structural patterns. This paper
presents CORRECT, the first lightweight, training-free framework that leverages
an online cache of distilled error schemata to recognize and transfer knowledge
of failure structures across new requests. This cache-based reuse allows LLMs
to perform targeted error localization at inference time, avoiding the need for
expensive retraining while adapting to dynamic MAS deployments in subseconds.
To support rigorous study in this domain, we also introduce CORRECT-Error, a
large-scale dataset of over 2,000 annotated trajectories collected through a
novel error-injection pipeline guided by real-world distributions, and further
validated through human evaluation to ensure alignment with natural failure
patterns. Experiments across seven diverse MAS applications show that CORRECT
improves step-level error localization up to 19.8% over existing advances while
at near-zero overhead, substantially narrowing the gap between automated and
human-level error recognition.

</details>


### [1046] [MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems](https://arxiv.org/abs/2509.24323)
*Kun Wang,Guibin Zhang,ManKit Ye,Xinyu Deng,Dongxia Wang,Xiaobin Hu,Jinyang Guo,Yang Liu,Yufei Guo*

Main category: cs.MA

TL;DR: 本文提出了MAS$^2$，一种基于递归自我生成的多智能体系统范式，通过“生成-实施-修正”三智能体团队动态构建和自适应调整代理系统，在复杂任务中性能提升高达19.6%，且具有良好的跨模型泛化能力与成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有自动多智能体系统多遵循“一次性生成并部署”的固定范式，缺乏应对现实环境动态性和不确定性的灵活性与鲁棒性。

Method: 提出MAS$^2$范式，采用“生成-实施-修正”三代理架构，结合协作树优化方法训练元代理，实现多智能体系统的自主设计与实时调整。

Result: 在七个基准上评估显示，MAS$^2$在深度研究和代码生成等复杂场景下性能最高提升19.6%，跨骨干模型泛化性能提升达15.1%，同时保持较低的令牌开销，处于成本-性能帕累托前沿。

Conclusion: MAS$^2$通过递归自生成机制显著提升了多智能体系统的灵活性、适应性和性能，为LLM驱动的多智能体系统提供了可持续进化的新型范式。

Abstract: The past two years have witnessed the meteoric rise of Large Language Model
(LLM)-powered multi-agent systems (MAS), which harness collective intelligence
and exhibit a remarkable trajectory toward self-evolution. This paradigm has
rapidly progressed from manually engineered systems that require bespoke
configuration of prompts, tools, roles, and communication protocols toward
frameworks capable of automated orchestration. Yet, dominant automatic
multi-agent systems, whether generated by external modules or a single LLM
agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}''
paradigm, rendering the resulting systems brittle and ill-prepared for the
dynamism and uncertainty of real-world environments. To transcend this
limitation, we introduce MAS$^2$, a paradigm predicated on the principle of
recursive self-generation: a multi-agent system that autonomously architects
bespoke multi-agent systems for diverse problems. Technically, we devise a
``\textit{generator-implementer-rectifier}'' tri-agent team capable of
dynamically composing and adaptively rectifying a target agent system in
response to real-time task demands. Collaborative Tree Optimization is proposed
to train and specialize these meta-agents. Extensive evaluation across seven
benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$
over state-of-the-art MAS in complex scenarios such as deep research and code
generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,
effectively leveraging previously unseen LLMs to yield improvements of up to
$15.1\%$. Crucially, these gains are attained without incurring excessive token
costs, as MAS$^2$ consistently resides on the Pareto frontier of
cost-performance trade-offs. The source codes are available at
https://github.com/yeyeyeah2/MAS2.

</details>


### [1047] [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](https://arxiv.org/abs/2509.25034)
*Heming Fu,Guojun Xiong,Jian Li,Shan Lin*

Main category: cs.MA

TL;DR: MARLIN是一个受椋鸟群智能启发的去中心化水库管理框架，结合多智能体强化学习与大语言模型实时奖励塑形，在处理不确定性、计算效率和洪水响应速度方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化下极端天气事件加剧带来的水资源管理挑战，特别是级联不确定性和传统方法在计算复杂度与协调能力上的不足。

Method: 提出MARLIN框架，融合生物启发的对齐、分离、凝聚规则与多智能体强化学习，并利用大语言模型进行实时奖励塑形，实现局部决策与全局协调的统一。

Result: 在真实USGS数据上实验显示，MARLIN比现有方法提升不确定性处理能力23%，减少计算量35%，加快洪水响应速度68%，且具备超线性协调能力，系统复杂度仅增加5.4倍（从400到10,000节点）。

Conclusion: MARLIN展现出在智能、可扩展的水资源管理中预防灾害和保护社区的巨大潜力。

Abstract: As climate change intensifies extreme weather events, water disasters pose
growing threats to global communities, making adaptive reservoir management
critical for protecting vulnerable populations and ensuring water security.
Modern water resource management faces unprecedented challenges from cascading
uncertainties propagating through interconnected reservoir networks. These
uncertainties, rooted in physical water transfer losses and environmental
variability, make precise control difficult. For example, sending 10 tons
downstream may yield only 8-12 tons due to evaporation and seepage. Traditional
centralized optimization approaches suffer from exponential computational
complexity and cannot effectively handle such real-world uncertainties, while
existing multi-agent reinforcement learning (MARL) methods fail to achieve
effective coordination under uncertainty. To address these challenges, we
present MARLIN, a decentralized reservoir management framework inspired by
starling murmurations intelligence. Integrating bio-inspired alignment,
separation, and cohesion rules with MARL, MARLIN enables individual reservoirs
to make local decisions while achieving emergent global coordination. In
addition, a LLM provides real-time reward shaping signals, guiding agents to
adapt to environmental changes and human-defined preferences. Experiments on
real-world USGS data show that MARLIN improves uncertainty handling by 23\%,
cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting
super-linear coordination, with complexity scaling 5.4x from 400 to 10,000
nodes. These results demonstrate MARLIN's potential for disaster prevention and
protecting communities through intelligent, scalable water resource management.

</details>
