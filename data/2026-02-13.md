<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.CL](#cs.CL) [Total: 78]
- [cs.AI](#cs.AI) [Total: 78]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 138]
- [cs.RO](#cs.RO) [Total: 34]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration](https://arxiv.org/abs/2602.11214)
*Manuel Hetzel,Kerim Turacan,Hannes Reichert,Konrad Doll,Bernhard Sick*

Main category: cs.CV

TL;DR: 本文提出DD-MDN模型，一种端到端的、基于去噪扩散与双混合密度网络的概率化人类轨迹预测方法，兼顾高定位精度、校准的不确定性建模及对短观测时长的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注预测精度、社交交互建模和多样性，而忽视了不确定性建模、校准性以及短观测期下的预测能力，而这三者对路径规划与碰撞规避等下游任务至关重要。

Method: 提出DD-MDN：采用少样本去噪扩散主干网络与双混合密度网络（dual MDN），自动学习自校准的驻留区域和概率排序的锚点路径，无需预定义锚点或终点，生成多样化轨迹假设。

Result: 在ETH/UCY、SDD、inD和IMPTC数据集上达到SOTA精度，尤其在短观测区间下表现出强鲁棒性，并实现可靠的不确定性建模。

Conclusion: DD-MDN有效统一了精度、不确定性校准与短时观测鲁棒性，为实际部署中安全关键型HTF任务提供了更可信的预测基础。

Abstract: Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.

</details>


### [2] [ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning](https://arxiv.org/abs/2602.11236)
*Yandan Yang,Shuang Zeng,Tong Lin,Xinyuan Chang,Dekang Qi,Junjin Xiao,Haoyun Liu,Ronghan Chen,Yuzhi Chen,Dongjie Huo,Feng Xiong,Xing Wei,Zhiheng Ma,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出ABot-M0框架，通过构建统一数据集UniACT和提出动作流形学习（AML）方法，实现跨形态机器人的通用具身智能，提升动作预测效率与策略稳定性，并支持模块化感知。


<details>
  <summary>Details</summary>
Motivation: 解决机器人领域中'一脑多形'范式下数据碎片化、表征不一致和训练目标不匹配的问题。

Method: 构建系统性数据清洗与标准化流程，创建大规模统一数据集UniACT；提出动作流形假设并设计基于DiT的动作流形学习（AML）；采用双流机制融合视觉语言模型语义与几何先验，支持即插即用3D模块。

Result: 在六个公开数据集上构建了含600万轨迹、9500小时数据的UniACT数据集；AML显著提升动作解码速度与策略稳定性；双流模块化感知增强空间理解且不改动主干网络。

Conclusion: ABot-M0实现了跨硬件平台的高效、稳定、可扩展的通用具身智能框架，各组件可独立运行并具备累加增益，代码与流程将全部开源。

Abstract: Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.

</details>


### [3] [Toward Reliable Tea Leaf Disease Diagnosis Using Deep Learning Model: Enhancing Robustness With Explainable AI and Adversarial Training](https://arxiv.org/abs/2602.11239)
*Samanta Ghosh,Jannatul Adan Mahi,Shayan Abrar,Md Parvez Mia,Asaduzzaman Rayhan,Abdul Awal Yasir,Asaduzzaman Hridoy*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动茶树叶病分类方法，使用teaLeafBD数据集（含5278张图像、7类），结合DenseNet201与EfficientNetB3模型、对抗训练和Grad-CAM可解释性分析，EfficientNetB3达到93%准确率。


<details>
  <summary>Details</summary>
Motivation: 茶是孟加拉国重要经济作物，但易受叶部病害影响，人工检测费时且易错，亟需自动化、高精度的智能诊断方案。

Method: 采用teaLeafBD数据集，构建包含数据预处理、划分、对抗训练、增强、模型训练、评估及Grad-CAM可解释性分析的完整流程；对比DenseNet201与EfficientNetB3，并引入对抗训练提升鲁棒性。

Result: EfficientNetB3取得93%最高分类准确率，DenseNet201为91%；Grad-CAM可视化验证了模型关注区域符合病征分布；模型对噪声输入具备较强鲁棒性。

Conclusion: 所提方法能高效、准确识别茶树叶病，具备实际农业应用价值，为智慧茶园管理提供了可行技术路径。

Abstract: Tea is a valuable asset for the economy of Bangladesh. So, tea cultivation plays an important role to boost the economy. These valuable plants are vulnerable to various kinds of leaf infections which may cause less production and low quality. It is not so easy to detect these diseases manually. It may take time and there could be some errors in the detection.Therefore, the purpose of the study is to develop an automated deep learning model for tea leaf disease classification based on the teaLeafBD dataset so that anyone can detect the diseases more easily and efficiently. There are 5,278 high-resolution images in this dataset. The images are classified into seven categories. Six of them represents various diseases and the rest one represents healthy leaves. The proposed pipeline contains data preprocessing, data splitting, adversarial training, augmentation, model training, evaluation, and comprehension made possible with Explainable AI strategies. DenseNet201 and EfficientNetB3 were employed to perform the classification task. To prepare the model more robustly, we applied adversarial training so it can operate effectively even with noisy or disturbed inputs. In addition, Grad-CAM visualization was executed to analyze the model's predictions by identifying the most influential regions of each image. Our experimental outcomes revealed that EfficientNetB3 achieved the highest classification accuracy of 93%, while DenseNet201 reached 91%. The outcomes prove that the effectiveness of the proposed approach can accurately detect tea leaf diseases and provide a practical solution for advanced agricultural management.

</details>


### [4] [Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration](https://arxiv.org/abs/2602.11241)
*Jinghan He,Junfeng Fang,Feng Xiong,Zijun Yao,Fei Shen,Haiyun Guo,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出Active-Zero框架，通过三个协同进化的智能体（Searcher、Questioner、Solver）实现视觉语言模型在开放世界环境中的主动探索与自适应学习，显著提升推理与理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的自博弈方法依赖静态图像集，缺乏主动获取适配其当前能力水平的视觉数据的能力，导致学习低效且对初始数据集依赖性强。

Method: 提出Active-Zero框架，包含三个协同进化的智能体：Searcher从开放世界库中按能力边界检索图像，Questioner生成校准的推理任务，Solver通过准确率奖励进行优化，形成闭环自搭建课程学习机制。

Result: 在Qwen2.5-VL-7B-Instruct上，Active-Zero在12个基准测试中推理任务平均准确率达53.97（+5.7%），通用理解达59.77（+3.9%），持续优于现有自博弈基线。

Conclusion: 主动探索是构建可扩展、自适应的自演化视觉语言系统的关键要素。

Abstract: Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model's capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.

</details>


### [5] [ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems](https://arxiv.org/abs/2602.11242)
*Yitong Wang,Yue Yao*

Main category: cs.CV

TL;DR: ReTracing 是一个结合人类舞者与四足机器人的多智能体具身表演艺术项目，通过考古学方法探究人工智能如何塑造、约束并生成身体运动；利用大语言模型生成动作指令，扩散模型生成视频指导，最终形成运动轨迹的数字档案，揭示生成式系统中隐含的社会文化偏见。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能如何塑造、约束和生成人类及机器人的身体运动，并揭示生成式AI系统中隐含的社会文化偏见。

Method: 从科幻小说中提取人机交互描述句，用大语言模型生成'该做什么'与'不该做什么'配对提示，再由扩散文本到视频模型生成人类舞者 choreographic 指南与机器人运动指令，在镜面地板上同步执行，通过多相机运动捕捉重建为3D点云与运动轨迹。

Result: 构建了一个包含人类与机器人协同运动痕迹的数字档案，可视化呈现了AI生成内容中潜藏的规范性与偏见，并引发关于‘在能运动、思考与留痕的AI时代，何以为人’的哲学反思。

Conclusion: ReTracing 作为一种新型具身艺术实践，不仅拓展了AI艺术表达的边界，更提供了一种批判性路径来解码生成式系统中的社会规训逻辑与文化预设。

Abstract: We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts "what to do" and "what not to do" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?

</details>


### [6] [Advancing Digital Twin Generation Through a Novel Simulation Framework and Quantitative Benchmarking](https://arxiv.org/abs/2602.11314)
*Jacob Rubinstein,Avi Donaty,Don Engel*

Main category: cs.CV

TL;DR: 本文提出了一种基于高质量3D模型和程序化生成相机位姿来合成图像的新流程，用于定量评估数字孪生建模中摄影测量方法的重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有摄影测量生成数字孪生的方法缺乏可重复、可量化的评估手段，依赖主观定性判断。

Method: 构建一个合成图像生成管线：从高保真3D模型出发，结合程序化生成的精确相机位姿，渲染合成图像，并与摄影测量重建结果进行定量对比。

Result: 实现了对虚拟相机参数与物体几何的已知真值与重建估计值之间误差的可重复、可量化实验评估。

Conclusion: 该合成数据驱动的评估框架为摄影测量及数字孪生建模方法提供了客观、可控的基准测试手段。

Abstract: The generation of 3D models from real-world objects has often been accomplished through photogrammetry, i.e., by taking 2D photos from a variety of perspectives and then triangulating matched point-based features to create a textured mesh. Many design choices exist within this framework for the generation of digital twins, and differences between such approaches are largely judged qualitatively. Here, we present and test a novel pipeline for generating synthetic images from high-quality 3D models and programmatically generated camera poses. This enables a wide variety of repeatable, quantifiable experiments which can compare ground-truth knowledge of virtual camera parameters and of virtual objects against the reconstructed estimations of those perspectives and subjects.

</details>


### [7] [Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models](https://arxiv.org/abs/2602.11244)
*Sethuraman T,Savya Khosla,Aditi Tiwari,Vidya Ganesh,Rakshana Jayaprakash,Aditya Jain,Vignesh Srinivasakumar,Onkar Kishor Susladkar,Srinidhi Sunkara,Aditya Shanmugham,Rakesh Vaideeswaran,Abbaas Alif Mohamed Nishar,Simon Jenni,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文提出REVEAL诊断基准，通过五种受控压力测试揭示当前视频-语言模型（VidLMs）在时间序列、运动理解和视频内容依赖等方面的严重缺陷，并提供自动生成诊断样本的数据管道。


<details>
  <summary>Details</summary>
Motivation: 探究视频-语言模型是否能稳健地理解视频内容、时间顺序和运动这一基础问题，发现现有模型存在诸多未被充分认识的脆弱性。

Method: 构建REVEAL诊断基准，包含五个可控压力测试：时间预期偏差、语言捷径依赖、视频谄媚倾向、相机运动敏感性、时空遮挡鲁棒性；并设计自动化数据生成流程。

Result: 主流开源与闭源VidLMs在各项测试中表现糟糕——如将倒放视频误判为正向、忽略视频内容仅靠文本回答、附和错误陈述、难以处理基本相机运动、无法在简单时空掩码下聚合时序信息；而人类轻松完成这些任务。

Conclusion: 当前VidLMs在核心视频理解能力上存在根本性不足，亟需更严谨的评估范式与更具鲁棒性的建模方法，REVEAL为未来研究提供了可扩展的诊断工具与开源资源。

Abstract: This work investigates a fundamental question: Do Video-Language Models (VidLMs) robustly account for video content, temporal sequence, and motion? Our investigation shows that, surprisingly, they often do not. We introduce REVEAL{}, a diagnostic benchmark that probes fundamental weaknesses of contemporary VidLMs through five controlled stress tests; assessing temporal expectation bias, reliance on language-only shortcuts, video sycophancy, camera motion sensitivity, and robustness to spatiotemporal occlusion. We test leading open- and closed-source VidLMs and find that these models confidently describe reversed scenes as forward, answer questions while neglecting video content, agree with false claims, struggle with basic camera motion, and fail to aggregate temporal information amidst simple spatiotemporal masking. Humans, on the other hand, succeed at these tasks with ease. Alongside our benchmark, we provide a data pipeline that automatically generates diagnostic examples for our stress tests, enabling broader and more scalable evaluation. We will release our benchmark and code to support future research.

</details>


### [8] [TexSpot: 3D Texture Enhancement with Spatially-uniform Point Latent Representation](https://arxiv.org/abs/2602.12157)
*Ziteng Lu,Yushuang Wu,Chongjie Ye,Yuda Qiu,Jing Shao,Xiaoyang Guo,Jiaqing Zhou,Tianlei Hu,Kun Zhou,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出TexSpot，一种基于扩散模型的纹理增强框架，通过引入新型3D纹理表示Texlet，结合局部2D编码与全局3D几何上下文建模，在多视角扩散生成的纹理基础上实现高质量、视图一致的3D纹理增强。


<details>
  <summary>Details</summary>
Motivation: 现有3D纹理生成方法存在UV映射失真或点云表示依赖几何密度导致高分辨率受限的问题，且多视角扩散方法存在视角不一致性。

Method: 提出Texlet表示：用2D编码器提取局部纹理块特征，3D编码器聚合全局形状上下文；设计级联3D-to-2D解码器重建纹理；构建以Texlet为条件的扩散Transformer进行纹理精细化增强。

Result: 在多个指标和定性评估中，TexSpot显著提升纹理视觉保真度、几何一致性与鲁棒性，优于当前SOTA方法。

Conclusion: TexSpot通过解耦纹理表征与几何密度，并融合局部细节与全局结构信息，有效解决了多视角扩散中纹理视图不一致与失真问题，为高质量3D纹理生成提供了新范式。

Abstract: High-quality 3D texture generation remains a fundamental challenge due to the view-inconsistency inherent in current mainstream multi-view diffusion pipelines. Existing representations either rely on UV maps, which suffer from distortion during unwrapping, or point-based methods, which tightly couple texture fidelity to geometric density that limits high-resolution texture generation. To address these limitations, we introduce TexSpot, a diffusion-based texture enhancement framework. At its core is Texlet, a novel 3D texture representation that merges the geometric expressiveness of point-based 3D textures with the compactness of UV-based representation. Each Texlet latent vector encodes a local texture patch via a 2D encoder and is further aggregated using a 3D encoder to incorporate global shape context. A cascaded 3D-to-2D decoder reconstructs high-quality texture patches, enabling the Texlet space learning. Leveraging this representation, we train a diffusion transformer conditioned on Texlets to refine and enhance textures produced by multi-view diffusion methods. Extensive experiments demonstrate that TexSpot significantly improves visual fidelity, geometric consistency, and robustness over existing state-of-the-art 3D texture generation and enhancement approaches. Project page: https://anonymous.4open.science/w/TexSpot-page-2D91.

</details>


### [9] [Selective Prior Synchronization via SYNC Loss](https://arxiv.org/abs/2602.11316)
*Ishan Mishra,Jiajie Li,Deepak Mishra,Jinjun Xiong*

Main category: cs.CV

TL;DR: 本文提出SYNC损失函数，将后验方法（如softmax响应）引入SelectiveNet的训练过程，通过利用选择先验（selective prior）提升深度神经网络的选择性预测能力，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测方法分为两类：一类是针对网络结构或目标函数进行修改的ad-hoc方法（如SelectiveNet），另一类是基于模型概率输出分析的post-hoc方法（如softmax响应）。作者观察到post-hoc方法隐式生成的选择先验在训练阶段同样重要，但此前仅用于推理阶段。

Method: 提出SYNC损失函数，将softmax响应（代表选择先验）显式引入SelectiveNet的训练过程，实现ad-hoc与post-hoc方法的有机融合，使模型在训练中即能学习利用不确定性信息进行选择性预测。

Result: 在CIFAR-100、ImageNet-100和Stanford Cars等多个基准数据集上，该方法不仅提升了模型泛化能力，还在选择性预测性能上超越先前工作，树立了新的SOTA基准。

Conclusion: 选择先验不仅对推理阶段重要，更应在训练阶段被充分利用；SYNC损失通过联合建模选择性与预测任务，有效提升了DNN在不确定性下的可靠决策能力。

Abstract: Prediction under uncertainty is a critical requirement for the deep neural network to succeed responsibly. This paper focuses on selective prediction, which allows DNNs to make informed decisions about when to predict or abstain based on the uncertainty level of their predictions. Current methods are either ad-hoc such as SelectiveNet, focusing on how to modify the network architecture or objective function, or post-hoc such as softmax response, achieving selective prediction through analyzing the model's probabilistic outputs. We observe that post-hoc methods implicitly generate uncertainty information, termed the selective prior, which has traditionally been used only during inference. We argue that the selective prior provided by the selection mechanism is equally vital during the training stage. Therefore, we propose the SYNC loss which introduces a novel integration of ad-hoc and post-hoc method. Specifically, our approach incorporates the softmax response into the training process of SelectiveNet, enhancing its selective prediction capabilities by examining the selective prior. Evaluated across various datasets, including CIFAR-100, ImageNet-100, and Stanford Cars, our method not only enhances the model's generalization capabilities but also surpasses previous works in selective prediction performance, and sets new benchmarks for state-of-the-art performance.

</details>


### [10] [MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors](https://arxiv.org/abs/2602.11323)
*Arda Alniak,Sinan Kalkan,Mustafa Mert Ankarali,Afsar Saranli,Abdullah Aydin Alatan*

Main category: cs.CV

TL;DR: 本文提出了一种将学习到的深度先验直接集成到VINS-Mono优化后端的新框架，通过引入仿射不变深度一致性与序数约束，并结合方差门控机制抑制不稳定伪影，在边缘设备计算限制下实现了鲁棒的尺度恢复和精度提升。


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉惯性里程计（VIO）在低纹理环境下因稀疏特征不足而性能下降，需借助稠密单目深度估计（MDE）作为补充；但现有基于ViT的高精度深度模型计算开销大，难以实现实时边缘部署。

Method: 将学习到的深度先验嵌入VINS-Mono后端优化；引入仿射不变的深度一致性约束和成对序数约束；采用基于方差的门控机制过滤不稳定深度伪影。

Result: 在TartanGround和M3ED数据集上验证，显著防止轨迹发散，绝对轨迹误差（ATE）最高降低28.3%。

Conclusion: 所提方法在严格满足边缘设备算力约束的前提下，实现了几何一致、尺度可观测且鲁棒的单目VIO性能提升。

Abstract: Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.

</details>


### [11] [Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content](https://arxiv.org/abs/2602.11339)
*Evgeney Bogatyrev,Khaled Abud,Ivan Molodetskikh,Nikita Alutis,Dmitry Vatolin*

Main category: cs.CV

TL;DR: 本文提出了StreamSR数据集和EfRLFN模型，旨在提升压缩视频流的实时超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有实时超分辨率方法在处理压缩视频内容时表现不佳，且常用数据集无法准确反映流媒体特性，导致基准测试缺乏现实相关性。

Method: 构建了源自YouTube的StreamSR数据集，并对11种前沿实时超分辨率模型进行基准测试；提出EfRLFN模型，融合高效通道注意力机制与双曲正切激活函数，并设计复合损失函数以加速训练收敛。

Result: EfRLFN在视觉质量与运行效率上均优于现有方法；在StreamSR上微调其他模型可显著提升其在多个标准基准上的泛化性能。

Conclusion: StreamSR数据集和EfRLFN模型为真实场景下的视频流超分辨率提供了更实用、高效的解决方案，并推动了该领域向实际应用迈进。

Abstract: Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a comprehensive dataset - StreamSR - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case.
  Furthermore, we propose EfRLFN, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance.
  Finally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at https://github.com/EvgeneyBogatyrev/EfRLFN.

</details>


### [12] [ArtContext: Contextualizing Artworks with Open-Access Art History Articles and Wikidata Knowledge through a LoRA-Tuned CLIP Model](https://arxiv.org/abs/2602.11349)
*Samuel Waugh,Stuart James*

Main category: cs.CV

TL;DR: 本文提出了ArtContext管道，利用开放获取的艺术史文章和Wikidata知识，通过定制化CLIP模型（PaintingCLIP）为艺术品自动添加上下文注释，提升对艺术品特定部分的理解与检索能力。


<details>
  <summary>Details</summary>
Motivation: 艺术史文献常讨论艺术品整体及局部特征（如构图、图像学、物质文化），但人工关联文献内容与具体艺术品困难，亟需自动化方法支持。

Method: 构建新型语料库采集流程，并基于LoRA技术微调CLIP模型，得到领域专用的PaintingCLIP；结合开放艺术史文章与Wikidata知识对艺术品进行上下文标注。

Result: PaintingCLIP在弱监督下优于原始CLIP模型，能有效为给定艺术品提供语义上下文；该管道具备跨人文学科的通用性。

Conclusion: ArtContext为艺术史研究提供了可扩展、可复用的多模态语义标注框架，推动数字人文中图文联合分析的发展。

Abstract: Many Art History articles discuss artworks in general as well as specific parts of works, such as layout, iconography, or material culture. However, when viewing an artwork, it is not trivial to identify what different articles have said about the piece. Therefore, we propose ArtContext, a pipeline for taking a corpus of Open-Access Art History articles and Wikidata Knowledge and annotating Artworks with this information. We do this using a novel corpus collection pipeline, then learn a bespoke CLIP model adapted using Low-Rank Adaptation (LoRA) to make it domain-specific. We show that the new model, PaintingCLIP, which is weakly supervised by the collected corpus, outperforms CLIP and provides context for a given artwork. The proposed pipeline is generalisable and can be readily applied to numerous humanities areas.

</details>


### [13] [Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation](https://arxiv.org/abs/2602.11401)
*Alan Baade,Eric Ryan Chan,Kyle Sargent,Changan Chen,Justin Johnson,Ehsan Adeli,Li Fei-Fei*

Main category: cs.CV

TL;DR: 本文提出Latent Forcing方法，在保持潜在扩散模型高效性的同时，直接在原始图像上操作，通过联合处理潜在表示和像素并采用独立调优的噪声调度，提升像素级生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型虽能生成高质量图像，但存在信息丢失、需单独训练解码器、建模辅助分布等问题，无法实现端到端建模。

Method: 提出Latent Forcing，对现有架构进行简单修改：联合处理潜在表示与像素，使用分别调优的噪声调度来排序去噪轨迹，使潜在表示作为中间计算的暂存区，再生成高频像素特征；并分析条件信号顺序的影响。

Result: 在ImageNet上，Latent Forcing在同等计算规模下，实现了基于扩散Transformer的像素生成新SOTA。

Conclusion: Latent Forcing成功融合了潜在空间的效率与像素空间的端到端建模优势，揭示了条件信号顺序对生成性能的关键影响，并为 tokenizer 与扩散模型协同优化提供了新视角。

Abstract: Latent diffusion models excel at generating high-quality images but lose the benefits of end-to-end modeling. They discard information during image encoding, require a separately trained decoder, and model an auxiliary distribution to the raw data. In this paper, we propose Latent Forcing, a simple modification to existing architectures that achieves the efficiency of latent diffusion while operating on raw natural images. Our approach orders the denoising trajectory by jointly processing latents and pixels with separately tuned noise schedules. This allows the latents to act as a scratchpad for intermediate computation before high-frequency pixel features are generated. We find that the order of conditioning signals is critical, and we analyze this to explain differences between REPA distillation in the tokenizer and the diffusion model, conditional versus unconditional generation, and how tokenizer reconstruction quality relates to diffusability. Applied to ImageNet, Latent Forcing achieves a new state-of-the-art for diffusion transformer-based pixel generation at our compute scale.

</details>


### [14] [Fighting MRI Anisotropy: Learning Multiple Cardiac Shapes From a Single Implicit Neural Representation](https://arxiv.org/abs/2602.11436)
*Carolina Brás,Soufiane Ben Haddou,Thijs P. Kuipers,Laura Alvarez-Florez,R. Nils Planken,Fleur V. Y. Tjong,Connie Bezzina,Ivana Išgum*

Main category: cs.CV

TL;DR: 本文提出了一种利用高分辨率、近各向同性的CTA数据训练单个神经隐式函数，以重建低分辨率、各向异性的短轴CMRI心脏结构（RV和MYO），并在4CH切面上验证了其重建精度。


<details>
  <summary>Details</summary>
Motivation: 短轴CMRI图像具有各向异性，限制了心脏形状分析的精度；而高质量参考分割标注（尤其是高分辨率SAX）难以获取。

Method: 利用近各向同性、高分辨率CTA数据训练一个神经隐式函数，联合表征任意分辨率下的CMRI心脏形状；重点重建右心室（RV）和心肌（MYO，同时建模左心室内外膜）；通过从重建结果中提取4CH切面并与CMRI原始4CH分割对比进行评估。

Result: RV和MYO在4CH切面上分别达到Dice系数0.91±0.07和0.75±0.13，Hausdorff距离为6.21±3.97 mm和7.53±5.13 mm；定性和定量结果均表明重建形状准确、光滑且解剖合理。

Conclusion: 该方法能有效克服CMRI各向异性限制，借助CTA先验提升心脏形状重建质量，有助于改进临床心脏形态学分析。

Abstract: The anisotropic nature of short-axis (SAX) cardiovascular magnetic resonance imaging (CMRI) limits cardiac shape analysis. To address this, we propose to leverage near-isotropic, higher resolution computed tomography angiography (CTA) data of the heart. We use this data to train a single neural implicit function to jointly represent cardiac shapes from CMRI at any resolution. We evaluate the method for the reconstruction of right ventricle (RV) and myocardium (MYO), where MYO simultaneously models endocardial and epicardial left-ventricle surfaces. Since high-resolution SAX reference segmentations are unavailable, we evaluate performance by extracting a 4-chamber (4CH) slice of RV and MYO from their reconstructed shapes. When compared with the reference 4CH segmentation masks from CMRI, our method achieved a Dice similarity coefficient of 0.91 $\pm$ 0.07 and 0.75 $\pm$ 0.13, and a Hausdorff distance of 6.21 $\pm$ 3.97 mm and 7.53 $\pm$ 5.13 mm for RV and MYO, respectively. Quantitative and qualitative assessment demonstrate the model's ability to reconstruct accurate, smooth and anatomically plausible shapes, supporting improvements in cardiac shape analysis.

</details>


### [15] [Ctrl&Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation](https://arxiv.org/abs/2602.11440)
*Penghui Ruan,Bojia Zi,Xianbiao Qi,Youze Huang,Rong Xiao,Pichao Wang,Jiannong Cao,Yuhui Shi*

Main category: cs.CV

TL;DR: 本文提出Ctrl&Shift，一种无需显式3D建模的端到端扩散框架，通过两阶段（对象移除+姿态控制的参考引导修复）和多任务多阶段训练，实现几何一致、背景保持、用户可控的对象级图像/视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足背景保持、视角变化下的几何一致性与用户可控变换三大目标；几何法依赖3D重建且泛化差，扩散法泛化好但缺乏细粒度几何控制。

Method: 提出Ctrl&Shift：1）将操作解耦为对象移除与相机姿态控制下的参考引导修复两阶段；2）设计多任务多阶段训练策略，分离背景、身份、姿态信号；3）构建含估计相对相机姿态的大规模真实世界配对图像/视频数据集。

Result: 在保真度、视角一致性与可控性上达到SOTA；首次在不依赖任何显式3D建模前提下，统一细粒度几何控制与真实世界泛化能力。

Conclusion: Ctrl&Shift有效克服了传统几何法与扩散法的局限，为对象级编辑提供了兼具精度、可控性与泛化性的新范式。

Abstract: Object-level manipulation, relocating or reorienting objects in images or videos while preserving scene realism, is central to film post-production, AR, and creative editing. Yet existing methods struggle to jointly achieve three core goals: background preservation, geometric consistency under viewpoint shifts, and user-controllable transformations. Geometry-based approaches offer precise control but require explicit 3D reconstruction and generalize poorly; diffusion-based methods generalize better but lack fine-grained geometric control. We present Ctrl&Shift, an end-to-end diffusion framework to achieve geometry-consistent object manipulation without explicit 3D representations. Our key insight is to decompose manipulation into two stages, object removal and reference-guided inpainting under explicit camera pose control, and encode both within a unified diffusion process. To enable precise, disentangled control, we design a multi-task, multi-stage training strategy that separates background, identity, and pose signals across tasks. To improve generalization, we introduce a scalable real-world dataset construction pipeline that generates paired image and video samples with estimated relative camera poses. Extensive experiments demonstrate that Ctrl&Shift achieves state-of-the-art results in fidelity, viewpoint consistency, and controllability. To our knowledge, this is the first framework to unify fine-grained geometric control and real-world generalization for object manipulation, without relying on any explicit 3D modeling.

</details>


### [16] [Enhanced Portable Ultra Low-Field Diffusion Tensor Imaging with Bayesian Artifact Correction and Deep Learning-Based Super-Resolution](https://arxiv.org/abs/2602.11446)
*Mark D. Olchanyi,Annabel Sorby-Adams,John Kirsch,Brian L. Edlow,Ava Farnan,Renfei Liu,Matthew S. Rosen,Emery N. Brown,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出了一种适用于超低场（ULF）磁共振扩散张量成像（DTI）的九方向单壳采集序列，以及配套的具备角度依赖性的贝叶斯偏置场校正算法和无需重训练、可泛化的卷积神经网络超分辨率算法DiffSR，显著提升了ULF DTI的空间/角度分辨率与信噪比，并在白质微结构重建与阿尔茨海默病分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 超低场（ULF）MRI虽具便携性和可及性优势，但其DTI存在空间/角度分辨率低、信噪比差及跨域伪影等问题，亟需专用建模与重建方法。

Method: 提出九方向单壳ULF DTI采集序列；设计角度依赖的贝叶斯偏置场校正算法；开发通用型CNN超分辨率算法DiffSR，支持跨数据集直接应用且无需重新训练。

Result: 在合成下采样实验和真实匹配的ULF/高场DTI数据上，算法成功恢复白质微结构与体积信息；在合成退化数据上进行阿尔茨海默病分类时，DTI指标与原始高场结果一致性显著提升。

Conclusion: 所提序列与算法（尤其是DiffSR）有效克服ULF DTI固有缺陷，推动ULF神经影像临床转化，并开源全部代码以促进ULF重建与DTI序列标准化研究。

Abstract: Portable, ultra-low-field (ULF) magnetic resonance imaging has the potential to expand access to neuroimaging but currently suffers from coarse spatial and angular resolutions and low signal-to-noise ratios. Diffusion tensor imaging (DTI), a sequence tailored to detect and reconstruct white matter tracts within the brain, is particularly prone to such imaging degradation due to inherent sequence design coupled with prolonged scan times. In addition, ULF DTI scans exhibit artifacting that spans both the space and angular domains, requiring a custom modelling algorithm for subsequent correction. We introduce a nine-direction, single-shell ULF DTI sequence, as well as a companion Bayesian bias field correction algorithm that possesses angular dependence and convolutional neural network-based superresolution algorithm that is generalizable across DTI datasets and does not require re-training (''DiffSR''). We show through a synthetic downsampling experiment and white matter assessment in real, matched ULF and high-field DTI scans that these algorithms can recover microstructural and volumetric white matter information at ULF. We also show that DiffSR can be directly applied to white matter-based Alzheimers disease classification in synthetically degraded scans, with notable improvements in agreement between DTI metrics, as compared to un-degraded scans. We freely disseminate the Bayesian bias correction algorithm and DiffSR with the goal of furthering progress on both ULF reconstruction methods and general DTI sequence harmonization. We release all code related to DiffSR for $\href{https://github.com/markolchanyi/DiffSR}{public \space use}$.

</details>


### [17] [A Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness](https://arxiv.org/abs/2602.11466)
*Yun-Cheng Li,Sen Lei,Heng-Chao Li,Ke Li*

Main category: cs.CV

TL;DR: 本文提出DBTANet，一种双分支框架，结合冻结的SAM分支（捕获全局语义与边界先验）和ResNet34分支（提供局部细节），并引入双向时间感知模块（BTAM）和高斯平滑投影模块（GSPM），以提升遥感图像语义变化检测的边界清晰度与时间建模能力，在两个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义变化检测方法存在边界模糊和时序建模不足的问题，限制了分割精度。

Method: 提出双分支Siamese编码器（冻结SAM分支 + ResNet34分支），设计双向时间感知模块（BTAM）进行多尺度特征聚合与对称时序建模，并引入高斯平滑投影模块（GSPM）优化浅层SAM特征以增强边界约束。

Result: 在两个公开基准上实现最先进性能，有效融合全局语义、局部细节、时序推理与边界感知能力。

Conclusion: DBTANet通过协同建模语义、空间、时间和边界信息，显著提升了遥感图像语义变化检测的准确性与鲁棒性。

Abstract: Semantic Change Detection (SCD) aims to detect and categorize land-cover changes from bi-temporal remote sensing images. Existing methods often suffer from blurred boundaries and inadequate temporal modeling, limiting segmentation accuracy. To address these issues, we propose a Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness, termed DBTANet. Specifically, we utilize a dual-branch Siamese encoder where a frozen SAM branch captures global semantic context and boundary priors, while a ResNet34 branch provides local spatial details, ensuring complementary feature representations. On this basis, we design a Bidirectional Temporal Awareness Module (BTAM) to aggregate multi-scale features and capture temporal dependencies in a symmetric manner. Furthermore, a Gaussian-smoothed Projection Module (GSPM) refines shallow SAM features, suppressing noise while enhancing edge information for boundary-aware constraints. Extensive experiments on two public benchmarks demonstrate that DBTANet effectively integrates global semantics, local details, temporal reasoning, and boundary awareness, achieving state-of-the-art performance.

</details>


### [18] [Arbitrary Ratio Feature Compression via Next Token Prediction](https://arxiv.org/abs/2602.11494)
*Yufan Liu,Daoyuan Ren,Zhipeng Zhang,Wenyang Luo,Bing Li,Weiming Hu,Stephen Maybank*

Main category: cs.CV

TL;DR: 本文提出了一种任意比率特征压缩（ARFC）框架，通过一个自回归的任意比率压缩器（ARC）实现单模型支持任意压缩比，无需重新训练；引入混合解（MoS）模块提升鲁棒性，实体关系图约束（ERGC）模块保持语义结构；在多个跨模态与图像任务上显著优于现有方法，甚至超越原始未压缩特征性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征压缩方法通常需为不同压缩比训练专用模型，缺乏灵活性和泛化能力，适应新压缩比时必须重新训练。

Method: 提出ARFC框架，核心是自回归的ARC模型，通过控制生成token数量调节压缩比；引入MoS模块融合多解以降低不确定性；在训练中加入ERGC模块以保持语义与结构关系。

Result: 在跨模态检索、图像分类与图像检索等多个任务和数据集上，ARFC在各种压缩比下均一致优于现有方法，部分场景下性能甚至超过原始未压缩特征。

Conclusion: ARFC是一种灵活、高效且通用的特征压缩方案，适用于资源受限的实际应用场景，解决了传统方法对特定压缩比依赖强、泛化性差的问题。

Abstract: Feature compression is increasingly important for improving the efficiency of downstream tasks, especially in applications involving large-scale or multi-modal data. While existing methods typically rely on dedicated models for achieving specific compression ratios, they are often limited in flexibility and generalization. In particular, retraining is necessary when adapting to a new compression ratio. To address this limitation, we propose a novel and flexible Arbitrary Ratio Feature Compression (ARFC) framework, which supports any compression ratio with a single model, eliminating the need for multiple specialized models. At its core, the Arbitrary Ratio Compressor (ARC) is an auto-regressive model that performs compression via next-token prediction. This allows the compression ratio to be controlled at inference simply by adjusting the number of generated tokens. To enhance the quality of the compressed features, two key modules are introduced. The Mixture of Solutions (MoS) module refines the compressed tokens by utilizing multiple compression results (solutions), reducing uncertainty and improving robustness. The Entity Relation Graph Constraint (ERGC) is integrated into the training process to preserve semantic and structural relationships during compression. Extensive experiments on cross-modal retrieval, image classification, and image retrieval tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches at various compression ratios. Notably, in some cases, it even surpasses the performance of the original, uncompressed features. These results validate the effectiveness and versatility of ARFC for practical, resource-constrained scenarios.

</details>


### [19] [What if Agents Could Imagine? Reinforcing Open-Vocabulary HOI Comprehension through Generation](https://arxiv.org/abs/2602.11499)
*Zhenlong Yuan,Xiangyan Qu,Jing Tang,Rui Chen,Lei Sun,Ruidong Chen,Hongwei Yu,Chengxuan Qian,Xiangxiang Chu,Shuo Li,Yuyin Zhou*

Main category: cs.CV

TL;DR: 本文提出ImagineAgent框架，通过认知推理与生成式想象结合，解决开放词汇人-物交互（OV-HOI）中的跨模态幻觉和遮挡模糊问题，在SWIG-HOI和HICO-DET上达到SOTA，仅需20%训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在开放词汇人-物交互（OV-HOI）任务中受限于跨模态幻觉和遮挡导致的语义模糊，难以实现鲁棒视觉理解。

Method: 提出ImagineAgent代理框架：构建显式的认知图建模实体与动作关系，并动态调用检索增强、图像裁剪和扩散模型等工具获取领域知识与视觉证据；引入兼顾预测准确率与工具效率的复合奖励机制。

Result: 在SWIG-HOI和HICO-DET数据集上取得SOTA性能，且仅需约20%的训练数据，验证了方法的鲁棒性与高效性。

Conclusion: 认知推理与生成式想象协同的代理式框架可有效缓解OV-HOI中的跨模态幻觉与遮挡歧义，为多模态视觉理解提供新范式。

Abstract: Multimodal Large Language Models have shown promising capabilities in bridging visual and textual reasoning, yet their reasoning capabilities in Open-Vocabulary Human-Object Interaction (OV-HOI) are limited by cross-modal hallucinations and occlusion-induced ambiguity. To address this, we propose \textbf{ImagineAgent}, an agentic framework that harmonizes cognitive reasoning with generative imagination for robust visual understanding. Specifically, our method innovatively constructs cognitive maps that explicitly model plausible relationships between detected entities and candidate actions. Subsequently, it dynamically invokes tools including retrieval augmentation, image cropping, and diffusion models to gather domain-specific knowledge and enriched visual evidence, thereby achieving cross-modal alignment in ambiguous scenarios. Moreover, we propose a composite reward that balances prediction accuracy and tool efficiency. Evaluations on SWIG-HOI and HICO-DET datasets demonstrate our SOTA performance, requiring approximately 20\% of training data compared to existing methods, validating our robustness and efficiency.

</details>


### [20] [Vascular anatomy-aware self-supervised pre-training for X-ray angiogram analysis](https://arxiv.org/abs/2602.11536)
*De-Xing Huang,Chaohui Yu,Xiao-Hu Zhou,Tian-Yu Xiang,Qin-Yi Zhang,Mei-Jiang Gui,Rui-Ze Ma,Chen-Yu Wang,Nu-Fang Xiao,Fan Wang,Zeng-Guang Hou*

Main category: cs.CV

TL;DR: 本文提出了一种血管解剖学感知的掩码图像建模框架VasoMIM，结合自建的大规模X射线血管造影数据集XA-170K，显著提升了下游任务性能，推动了X射线血管造影分析的发展。


<details>
  <summary>Details</summary>
Motivation: 当前X射线血管造影分析的深度学习方法受限于标注数据稀缺，而大规模自监督学习（SSL）在此领域尚未充分探索，主要由于缺乏有效的SSL框架和大规模数据集。

Method: 提出VasoMIM框架，包含解剖引导的掩码策略（聚焦血管区域）和解剖一致性损失（保证重建图像中血管结构一致性），并构建了迄今最大的X射线血管造影预训练数据集XA-170K。

Result: 在四个下游任务、六个数据集上验证，VasoMIM展现出优越的迁移能力和SOTA性能。

Conclusion: VasoMIM有望成为X射线血管造影分析的基础模型，推动该领域广泛应用；代码与数据集将开源。

Abstract: X-ray angiography is the gold standard imaging modality for cardiovascular diseases. However, current deep learning approaches for X-ray angiogram analysis are severely constrained by the scarcity of annotated data. While large-scale self-supervised learning (SSL) has emerged as a promising solution, its potential in this domain remains largely unexplored, primarily due to the lack of effective SSL frameworks and large-scale datasets. To bridge this gap, we introduce a vascular anatomy-aware masked image modeling (VasoMIM) framework that explicitly integrates domain-specific anatomical knowledge. Specifically, VasoMIM comprises two key designs: an anatomy-guided masking strategy and an anatomical consistency loss. The former strategically masks vessel-containing patches to compel the model to learn robust vascular semantics, while the latter preserves structural consistency of vessels between original and reconstructed images, enhancing the discriminability of the learned representations. In conjunction with VasoMIM, we curate XA-170K, the largest X-ray angiogram pre-training dataset to date. We validate VasoMIM on four downstream tasks across six datasets, where it demonstrates superior transferability and achieves state-of-the-art performance compared to existing methods. These findings highlight the significant potential of VasoMIM as a foundation model for advancing a wide range of X-ray angiogram analysis tasks. VasoMIM and XA-170K will be available at https://github.com/Dxhuang-CASIA/XA-SSL.

</details>


### [21] [Supervise-assisted Multi-modality Fusion Diffusion Model for PET Restoration](https://arxiv.org/abs/2602.11545)
*Yingkai Zhang,Shuang Chen,Ye Tian,Yunyi Gao,Jianyong Jiang,Ying Fu*

Main category: cs.CV

TL;DR: 本文提出了一种监督辅助的多模态融合扩散模型（MFdiff），利用MR图像辅助恢复低剂量PET图像，通过多模态特征融合模块和两阶段监督学习策略，有效提升重建质量，尤其在分布外数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 降低PET成像辐射剂量会导致图像质量下降；利用MR图像辅助恢复标准剂量PET图像面临多模态结构/纹理不一致及分布外（OOD）数据不匹配的挑战。

Method: 提出监督辅助的多模态融合扩散模型（MFdiff）：1）设计多模态特征融合模块以优化MR与PET特征融合；2）将融合特征作为扩散模型的条件进行迭代生成；3）采用两阶段监督学习策略，联合利用仿真数据的通用先验和真实OOD数据的特异性先验。

Result: MFdiff在定性和定量评估中均优于当前最先进方法，能有效从多模态输入中恢复高质量标准剂量PET图像。

Conclusion: MFdiff通过协同建模多模态信息与分阶段监督学习，显著提升了低剂量PET图像重建质量，尤其增强了对分布外数据的泛化能力，为安全、精准的功能影像提供了新思路。

Abstract: Positron emission tomography (PET) offers powerful functional imaging but involves radiation exposure. Efforts to reduce this exposure by lowering the radiotracer dose or scan time can degrade image quality. While using magnetic resonance (MR) images with clearer anatomical information to restore standard-dose PET (SPET) from low-dose PET (LPET) is a promising approach, it faces challenges with the inconsistencies in the structure and texture of multi-modality fusion, as well as the mismatch in out-of-distribution (OOD) data. In this paper, we propose a supervise-assisted multi-modality fusion diffusion model (MFdiff) for addressing these challenges for high-quality PET restoration. Firstly, to fully utilize auxiliary MR images without introducing extraneous details in the restored image, a multi-modality feature fusion module is designed to learn an optimized fusion feature. Secondly, using the fusion feature as an additional condition, high-quality SPET images are iteratively generated based on the diffusion model. Furthermore, we introduce a two-stage supervise-assisted learning strategy that harnesses both generalized priors from simulated in-distribution datasets and specific priors tailored to in-vivo OOD data. Experiments demonstrate that the proposed MFdiff effectively restores high-quality SPET images from multi-modality inputs and outperforms state-of-the-art methods both qualitatively and quantitatively.

</details>


### [22] [Perception-based Image Denoising via Generative Compression](https://arxiv.org/abs/2602.11553)
*Nam Nguyen,Thinh Nguyen,Bella Bose*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成式压缩的感知驱动图像去噪框架，通过熵编码潜在表示和生成解码器（如WGAN或扩散模型）实现结构保持与纹理真实性的平衡，并提供了理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统失真驱动的去噪方法在强噪声和分布偏移下易产生过度平滑结果，难以兼顾结构保真与感知真实性。

Method: 构建生成式压缩框架：利用熵编码的低复杂度潜在表示进行重建，并采用基于LPIPS损失和Wasserstein距离的生成解码器恢复真实纹理；具体包括条件WGAN压缩去噪器和条件扩散重建策略；并给出压缩型最大似然去噪器在加性高斯噪声下的非渐近误差界。

Result: 在合成噪声与真实噪声数据集上均取得一致的感知质量提升（如LPIPS降低），同时保持有竞争力的失真指标（如PSNR、SSIM）。

Conclusion: 生成式压缩为感知驱动去噪提供了新范式，能有效平衡率-失真-感知三者关系，并具备理论可解释性与实际有效性。

Abstract: Image denoising aims to remove noise while preserving structural details and perceptual realism, yet distortion-driven methods often produce over-smoothed reconstructions, especially under strong noise and distribution shift. This paper proposes a generative compression framework for perception-based denoising, where restoration is achieved by reconstructing from entropy-coded latent representations that enforce low-complexity structure, while generative decoders recover realistic textures via perceptual measures such as learned perceptual image patch similarity (LPIPS) loss and Wasserstein distance. Two complementary instantiations are introduced: (i) a conditional Wasserstein GAN (WGAN)-based compression denoiser that explicitly controls the rate-distortion-perception (RDP) trade-off, and (ii) a conditional diffusion-based reconstruction strategy that performs iterative denoising guided by compressed latents. We further establish non-asymptotic guarantees for the compression-based maximum-likelihood denoiser under additive Gaussian noise, including bounds on reconstruction error and decoding error probability. Experiments on synthetic and real-noise benchmarks demonstrate consistent perceptual improvements while maintaining competitive distortion performance.

</details>


### [23] [LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts](https://arxiv.org/abs/2602.11564)
*Chen Zhao,Jiawei Chen,Hongyu Li,Zhuoliang Kang,Shilin Lu,Xiaoming Wei,Kai Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: LUVE是一种基于双频专家的潜空间级联式超高清视频生成框架，通过三阶段架构（低分辨率运动生成、潜空间视频上采样、高分辨率内容细化）解决UHR视频生成中的运动建模、语义规划与细节合成难题。


<details>
  <summary>Details</summary>
Motivation: 超高清（UHR）视频生成面临运动建模、语义规划和细节合成等多重挑战，现有视频扩散模型难以兼顾质量与效率。

Method: 提出LUVE框架：第一阶段生成运动一致的低分辨率潜表示；第二阶段在潜空间直接上采样以降低计算与内存开销；第三阶段融合低频（语义）与高频（细节）专家协同优化内容保真度。

Result: LUVE在UHR视频生成中实现了更优的视觉真实感与内容保真度，消融实验验证了各模块有效性。

Conclusion: LUVE通过潜空间级联与双频专家协同设计，为高效高质量UHR视频生成提供了新范式。

Abstract: Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}.

</details>


### [24] [Move What Matters: Parameter-Efficient Domain Adaptation via Optimal Transport Flow for Collaborative Perception](https://arxiv.org/abs/2602.11565)
*Zesheng Jia,Jin Wang,Siao Liu,Lingzhi Li,Ziyao Huang,Yunjiang Xu,Jianping Wang*

Main category: cs.CV

TL;DR: 本文提出FlowAdapt，一种基于最优传输理论的参数高效多智能体域自适应框架，通过Wasserstein贪心采样和渐进知识迁移模块解决V2X协同感知中PEFT应用导致的冗余与语义退化问题，在仅训练1%参数下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Fast domain adaptation在V2X多智能体协同感知中面临挑战，直接将PEFT应用于多智能体场景会导致性能下降和训练不稳定。

Method: 提出FlowAdapt框架：1）基于最优传输理论最小化数据分布与网络层级间的信息传输代价；2）Wasserstein贪心采样策略通过有界覆盖半径筛选冗余样本；3）渐进知识迁移模块通过可学习路径将压缩的早期表征注入后期层以缓解语义退化。

Result: 在三个基准上验证，FlowAdapt仅需1%可训练参数即达SOTA性能，显著提升样本效率与跨域泛化能力。

Conclusion: FlowAdapt有效解决了多智能体PEFT中的冗余与深层语义退化问题，为V2X快速域自适应提供了高效、稳定的新范式。

Abstract: Fast domain adaptation remains a fundamental challenge for deploying multi-agent systems across diverse environments in Vehicle-to-Everything (V2X) collaborative perception. Despite the success of Parameter-Efficient Fine-Tuning (PEFT) in natural language processing and conventional vision tasks, directly applying PEFT to multi-agent settings leads to significant performance degradation and training instability. In this work, we conduct a detailed analysis and identify two key factors: (i) inter-frame redundancy in heterogeneous sensory streams, and (ii) erosion of fine-grained semantics in deep-layer representations under PEFT adaptation. To address these issues, we propose FlowAdapt, a parameter-efficient framework grounded in optimal transport theory, which minimizes information transport costs across both data distributions and network hierarchies. Specifically, we introduce a Wasserstein Greedy Sampling strategy to selectively filter redundant samples via a bounded covering radius. Furthermore, Progressive Knowledge Transfer module is designed to progressively inject compressed early-stage representations into later stages through learnable pathways, alleviating semantic degradation in late-stage adaptation. Extensive experiments on three benchmarks demonstrate that FlowAdapt achieves state-of-the-art performance with only 1% of trainable parameters, effectively bridging domain gaps with superior sample efficiency and generalization.

</details>


### [25] [A Large Language Model for Disaster Structural Reconnaissance Summarization](https://arxiv.org/abs/2602.11588)
*Yuqing Gao,Guanren Zhou,Khalid M. Mosalam*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型（LLM）的灾害侦察摘要框架（LLM-DRS），将视觉数据与文本元数据融合，利用深度卷积网络提取结构损伤属性，并通过LLM生成结构化灾后评估报告，提升建筑环境韧性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉驱动的结构健康监测方法仅输出离散标签或坐标，需人工进一步分析；而大语言模型的兴起为自动生成可读、可决策的灾后评估报告提供了新路径。

Method: 构建标准化侦察流程，采集图像与文本元数据；用深度卷积神经网络提取损伤状态、材料类型、损伤等级等关键属性；将结构化属性与元数据输入经提示工程优化的LLM，生成自然语言摘要报告。

Result: LLM-DRS能自动生成面向单体结构或区域的灾后侦察摘要报告，实验证明其在快速灾后评估中具备实用性和扩展潜力。

Conclusion: 将LLM融入视觉驱动SHM，尤其适用于灾后快速侦察，可显著提升建筑环境的监测自动化水平与应急响应能力，推动韧性城市建设。

Abstract: Artificial Intelligence (AI)-aided vision-based Structural Health Monitoring (SHM) has emerged as an effective approach for monitoring and assessing structural condition by analyzing image and video data. By integrating Computer Vision (CV) and Deep Learning (DL), vision-based SHM can automatically identify and localize visual patterns associated with structural damage. However, previous works typically generate only discrete outputs, such as damage class labels and damage region coordinates, requiring engineers to further reorganize and analyze these results for evaluation and decision-making. In late 2022, Large Language Models (LLMs) became popular across multiple fields, providing new insights into AI-aided vision-based SHM. In this study, a novel LLM-based Disaster Reconnaissance Summarization (LLM-DRS) framework is proposed. It introduces a standard reconnaissance plan in which the collection of vision data and corresponding metadata follows a well-designed on-site investigation process. Text-based metadata and image-based vision data are then processed and integrated into a unified format, where well-trained Deep Convolutional Neural Networks extract key attributes, including damage state, material type, and damage level. Finally, all data are fed into an LLM with carefully designed prompts, enabling the LLM-DRS to generate summary reports for individual structures or affected regions based on aggregated attributes and metadata. Results show that integrating LLMs into vision-based SHM, particularly for rapid post-disaster reconnaissance, demonstrates promising potential for improving resilience of the built environment through effective reconnaissance.

</details>


### [26] [PLOT-CT: Pre-log Voronoi Decomposition Assisted Generation for Low-dose CT Reconstruction](https://arxiv.org/abs/2602.11625)
*Bin Huang,Xun Yu,Yikun Zhang,Yi Zhang,Yang Chen,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出PLOT-CT框架，通过在预对数域对sinogram进行Voronoi分解，分离噪声与结构信息，从而提升低剂量CT重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有LDCT重建方法多在图像域或对数后投影域操作，易受对数变换放大噪声影响，且未能充分利用预对数测量中的结构信息。

Method: 提出Pre-Log vOronoi decomposiTion-assisted CT（PLOT-CT）框架：对预对数sinogram进行Voronoi分解，将数据解耦为不同潜在空间的成分，以增强特征判别性并抑制噪声。

Result: 在1e4入射光子水平下，PLOT-CT在预对数域较传统方法PSNR提升2.36dB，达到SOTA性能。

Conclusion: 预对数域的显式结构分解可有效缓解噪声放大问题，提升重建保真度，为LDCT重建提供了新范式。

Abstract: Low-dose computed tomography (LDCT) reconstruction is fundamentally challenged by severe noise and compromised data fidelity under reduced radiation exposure. Most existing methods operate either in the image or post-log projection domain, which fails to fully exploit the rich structural information in pre-log measurements while being highly susceptible to noise. The requisite logarithmic transformation critically amplifies noise within these data, imposing exceptional demands on reconstruction precision. To overcome these challenges, we propose PLOT-CT, a novel framework for Pre-Log vOronoi decomposiTion-assisted CT generation. Our method begins by applying Voronoi decomposition to pre-log sinograms, disentangling the data into distinct underlying components, which are embedded in separate latent spaces. This explicit decomposition significantly enhances the model's capacity to learn discriminative features, directly improving reconstruction accuracy by mitigating noise and preserving information inherent in the pre-log domain. Extensive experiments demonstrate that PLOT-CT achieves state-of-the-art performance, attaining a 2.36dB PSNR improvement over traditional methods at the 1e4 incident photon level in the pre-log domain.

</details>


### [27] [PLESS: Pseudo-Label Enhancement with Spreading Scribbles for Weakly Supervised Segmentation](https://arxiv.org/abs/2602.11628)
*Yeva Gabrielyan,Varduhi Yeghiazaryan,Irina Voiculescu*

Main category: cs.CV

TL;DR: 本文提出PLESS方法，通过分层空间区域划分和语义一致性传播来增强弱监督分割中的伪标签质量，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于涂鸦标注的弱监督医学图像分割方法依赖伪标签，但伪标签质量低限制了性能。

Method: PLESS是一种通用伪标签增强策略，基于图像的分层空间一致区域划分，在语义一致区域内传播涂鸦信息以优化伪标签；该方法与模型无关，易于集成到现有伪标签框架中。

Result: 在ACDC和MSCMRseg两个心脏MRI数据集上，PLESS在四种涂鸦监督算法中均显著提升了分割精度。

Conclusion: PLESS能有效提升伪标签的可靠性与空间一致性，是通用且即插即用的弱监督分割增强策略。

Abstract: Weakly supervised learning with scribble annotations uses sparse user-drawn strokes to indicate segmentation labels on a small subset of pixels. This annotation reduces the cost of dense pixel-wise labeling, but suffers inherently from noisy and incomplete supervision. Recent scribble-based approaches in medical image segmentation address this limitation using pseudo-label-based training; however, the quality of the pseudo-labels remains a key performance limit. We propose PLESS, a generic pseudo-label enhancement strategy which improves reliability and spatial consistency. It builds on a hierarchical partitioning of the image into a hierarchy of spatially coherent regions. PLESS propagates scribble information to refine pseudo-labels within semantically coherent regions. The framework is model-agnostic and easily integrates into existing pseudo-label methods. Experiments on two public cardiac MRI datasets (ACDC and MSCMRseg) across four scribble-supervised algorithms show consistent improvements in segmentation accuracy. Code will be made available on GitHub upon acceptance.

</details>


### [28] [ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning](https://arxiv.org/abs/2602.11636)
*Changti Wu,Jiahuai Mao,Yuzhuo Miao,Shijie Lian,Bin Yu,Xiaopeng Lin,Cong Huang,Lei Zhang,Kai Chen*

Main category: cs.CV

TL;DR: 本文提出ScalSelect，一种无需训练、线性时间复杂度的多模态数据选择方法，用于视觉指令调优（VIT），通过提取指令关注的视觉特征构建样本表征，并利用子空间近似进行重要性评分，在仅用16%数据时达到全量数据97.5%以上的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉指令调优（VIT）因数据冗余导致计算昂贵且低效，亟需高效、可扩展、无需训练的多模态数据选择方法。

Method: ScalSelect首先从目标视觉语言模型中提取被指令token最关注的视觉特征来构建样本表征；然后通过主子空间近似而非两两相似度计算，实现线性时间复杂度的重要性评分，不依赖外部模型或辅助数据集。

Result: 在多个VLM、数据集和不同选择预算下实验表明，ScalSelect仅用16%的数据即可达到全量训练97.5%以上的性能，某些设置下甚至超越全量训练。

Conclusion: ScalSelect是一种高效、可扩展、训练免费的多模态数据选择方法，显著提升了VIT的训练效率与实用性，为大规模多模态模型训练提供了新范式。

Abstract: Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at \href{https://github.com/ChangtiWu/ScalSelect}{ScalSelect}.

</details>


### [29] [Electrostatics-Inspired Surface Reconstruction (EISR): Recovering 3D Shapes as a Superposition of Poisson's PDE Solutions](https://arxiv.org/abs/2602.11642)
*Diego Patiño,Knut Peterson,Kostas Daniilidis,David K. Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于泊松方程（而非传统Eikonal方程）的隐式形状表示新方法，利用格林函数和线性叠加原理构建SDF近似，在少量先验下仍能更好恢复高频几何细节。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDF的隐式表面重建多依赖Eikonal PDE约束，但其对高频率几何细节建模能力有限；本文旨在探索更适配物理直觉与数学性质（如线性、解析可解）的代理PDE以提升重建质量。

Method: 将表面重建建模为泊松方程的解，借助静电势等物理类比建立直观理解；采用格林函数获得闭式参数化解，并利用泊松方程的线性特性，将目标隐式场表示为多个基解的叠加。

Result: 在少量形状先验条件下，该方法在高频率几何细节（如尖锐边缘、精细纹理）的SDF逼近上优于现有基于Eikonal的方法。

Conclusion: 使用泊松方程作为代理PDE进行隐式形状建模是可行且有效的，其线性与物理可解释性为高质量、数据高效表面重建提供了新路径。

Abstract: Implicit shape representation, such as SDFs, is a popular approach to recover the surface of a 3D shape as the level sets of a scalar field. Several methods approximate SDFs using machine learning strategies that exploit the knowledge that SDFs are solutions of the Eikonal partial differential equation (PDEs). In this work, we present a novel approach to surface reconstruction by encoding it as a solution to a proxy PDE, namely Poisson's equation. Then, we explore the connection between Poisson's equation and physics, e.g., the electrostatic potential due to a positive charge density. We employ Green's functions to obtain a closed-form parametric expression for the PDE's solution, and leverage the linearity of our proxy PDE to find the target shape's implicit field as a superposition of solutions. Our method shows improved results in approximating high-frequency details, even with a small number of shape priors.

</details>


### [30] [Brain Tumor Classifiers Under Attack: Robustness of ResNet Variants Against Transferable FGSM and PGD Attacks](https://arxiv.org/abs/2602.11646)
*Ryan Deem,Garrett Goodman,Waqas Majeed,Md Abdullah Al Hafiz Khan,Michail S. Alexiou*

Main category: cs.CV

TL;DR: 本文研究了基于ResNet的脑肿瘤分类模型（BrainNet、BrainNeXt、DilationNet）在MRI数据上的对抗鲁棒性，发现BrainNeXt对黑盒攻击最鲁棒但迁移性差，而输入分辨率降低和去增强会显著削弱鲁棒性，即使准确率未明显下降。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在脑肿瘤分类中的对抗鲁棒性尚缺乏充分研究，尤其在临床MRI部署中至关重要。

Method: 评估三种ResNet变体（BrainNet、BrainNeXt、DilationNet）在FGSM和PGD攻击下的鲁棒性，对比三种MRI预处理配置（全尺寸增强、缩小增强、缩小非增强）。

Result: BrainNeXt对黑盒攻击最鲁棒但生成的对抗样本迁移性弱；BrainNet和DilationNet易相互攻击；缩小且非增强的数据显著降低鲁棒性，即使测试准确率仍高。

Conclusion: 脑MRI分析的实际部署需联合评估分类性能与对抗鲁棒性，预处理策略对鲁棒性影响显著。

Abstract: Adversarial robustness in deep learning models for brain tumor classification remains an underexplored yet critical challenge, particularly for clinical deployment scenarios involving MRI data. In this work, we investigate the susceptibility and resilience of several ResNet-based architectures, referred to as BrainNet, BrainNeXt and DilationNet, against gradient-based adversarial attacks, namely FGSM and PGD. These models, based on ResNet, ResNeXt, and dilated ResNet variants respectively, are evaluated across three preprocessing configurations (i) full-sized augmented, (ii) shrunk augmented and (iii) shrunk non-augmented MRI datasets. Our experiments reveal that BrainNeXt models exhibit the highest robustness to black-box attacks, likely due to their increased cardinality, though they produce weaker transferable adversarial samples. In contrast, BrainNet and Dilation models are more vulnerable to attacks from each other, especially under PGD with higher iteration steps and $α$ values. Notably, shrunk and non-augmented data significantly reduce model resilience, even when the untampered test accuracy remains high, highlighting a key trade-off between input resolution and adversarial vulnerability. These results underscore the importance of jointly evaluating classification performance and adversarial robustness for reliable real-world deployment in brain MRI analysis.

</details>


### [31] [GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction](https://arxiv.org/abs/2602.11653)
*Mengxiao Geng,Zijie Chen,Ran Hong,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出GR-Diffusion框架，结合三维离散高斯表示（GR）的几何先验与扩散模型的生成能力，用于低剂量全身影像PET重建，显著提升图像质量与生理细节保留。


<details>
  <summary>Details</summary>
Motivation: PET重建面临噪声放大、结构模糊和细节丢失等挑战，尤其在低剂量和稀疏采样条件下；传统方法受限于低通滤波特性，难以兼顾全局一致性与局部精度。

Method: 提出GR-Diffusion框架：利用GR从投影数据生成物理可解释、结构显式的3D参考图像；在此基础上设计分层引导机制——细粒度引导基于差异图精修局部细节，粗粒度引导利用多尺度差异图校正全局偏差，使扩散模型逐步融合GR几何先验并恢复亚体素信息。

Result: 在UDPET和临床数据集上，GR-Diffusion在不同剂量水平下均优于现有最先进方法，显著提升3D全身影像PET质量与生理结构保真度。

Conclusion: GR与扩散模型的协同建模可有效克服PET逆问题的病态性，为低剂量分子影像重建提供兼具物理合理性与生成表现力的新范式。

Abstract: Positron emission tomography (PET) reconstruction is a critical challenge in molecular imaging, often hampered by noise amplification, structural blurring, and detail loss due to sparse sampling and the ill-posed nature of inverse problems. The three-dimensional discrete Gaussian representation (GR), which efficiently encodes 3D scenes using parameterized discrete Gaussian distributions, has shown promise in computer vision. In this work, we pro-pose a novel GR-Diffusion framework that synergistically integrates the geometric priors of GR with the generative power of diffusion models for 3D low-dose whole-body PET reconstruction. GR-Diffusion employs GR to generate a reference 3D PET image from projection data, establishing a physically grounded and structurally explicit benchmark that overcomes the low-pass limitations of conventional point-based or voxel-based methods. This reference image serves as a dual guide during the diffusion process, ensuring both global consistency and local accuracy. Specifically, we employ a hierarchical guidance mechanism based on the GR reference. Fine-grained guidance leverages differences to refine local details, while coarse-grained guidance uses multi-scale difference maps to correct deviations. This strategy allows the diffusion model to sequentially integrate the strong geometric prior from GR and recover sub-voxel information. Experimental results on the UDPET and Clinical datasets with varying dose levels show that GR-Diffusion outperforms state-of-the-art methods in enhancing 3D whole-body PET image quality and preserving physiological details.

</details>


### [32] [SToRM: Supervised Token Reduction for Multi-modal LLMs toward efficient end-to-end autonomous driving](https://arxiv.org/abs/2602.11656)
*Seo Hyun Kim,Jin Bok Park,Do Yeon Koo,Ho Gun Park,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种面向多模态大语言模型的监督式视觉令牌压缩框架SToRM，用于在端到端自动驾驶中高效融合自然语言指令，在大幅降低计算开销（最高30倍）的同时保持与全令牌输入相当的性能。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统需结合人类语言指令提升异常场景下的安全性，但多模态大语言模型（MLLM）因依赖大量视觉令牌和LLM而计算开销过大，现有令牌压缩方法常以性能下降为代价。

Method: 提出监督式令牌缩减框架SToRM：1）轻量级重要性预测器基于短时滑动窗口估计令牌重要性；2）通过辅助路径从全令牌LLM前向传播中生成伪监督信号进行监督训练；3）锚-上下文合并模块将令牌划分为锚点与上下文，并将上下文令牌融合进相关锚点以减少冗余。

Result: 在LangAuto基准上，SToRM在相同令牌缩减预算下优于当前最优端到端驾驶MLLM，维持了全令牌性能，同时计算成本最高降低30倍。

Conclusion: SToRM是首个面向多模态大语言模型的监督式令牌缩减框架，实现了端到端自动驾驶中语言引导推理的高效与高性能兼顾。

Abstract: In autonomous driving, end-to-end (E2E) driving systems that predict control commands directly from sensor data have achieved significant advancements. For safe driving in unexpected scenarios, these systems may additionally rely on human interventions such as natural language instructions. Using a multi-modal large language model (MLLM) facilitates human-vehicle interaction and can improve performance in such scenarios. However, this approach requires substantial computational resources due to its reliance on an LLM and numerous visual tokens from sensor inputs, which are limited in autonomous vehicles. Many MLLM studies have explored reducing visual tokens, but often suffer end-task performance degradation compared to using all tokens.
  To enable efficient E2E driving while maintaining performance comparable to using all tokens, this paper proposes the first Supervised Token Reduction framework for multi-modal LLMs (SToRM). The proposed framework consists of three key elements. First, a lightweight importance predictor with short-term sliding windows estimates token importance scores. Second, a supervised training approach uses an auxiliary path to obtain pseudo-supervision signals from an all-token LLM pass. Third, an anchor-context merging module partitions tokens into anchors and context tokens, and merges context tokens into relevant anchors to reduce redundancy while minimizing information loss. Experiments on the LangAuto benchmark show that SToRM outperforms state-of-the-art E2E driving MLLMs under the same reduced-token budget, maintaining all-token performance while reducing computational cost by up to 30x.

</details>


### [33] [EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation](https://arxiv.org/abs/2602.11658)
*Bingyuan Wang,Xingbei Chen,Zongyang Qiu,Linping Yuan,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出了EmoSpace框架，通过视觉-语言对齐学习动态、可解释的情绪原型，实现无需显式情绪标签的细粒度情绪控制生成，支持VR等沉浸式应用。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法难以捕捉细腻的情绪语义和实现沉浸体验所需的精细情绪控制。

Method: 提出EmoSpace框架，采用分层情绪表征与可学习动态原型，结合多原型引导、时间融合和注意力重加权的可控生成流程。

Result: 在定性和定量评估中均优于现有方法，并通过用户研究验证了VR环境对情绪感知的影响。

Conclusion: EmoSpace实现了细粒度情绪控制的沉浸式视觉内容生成，适用于治疗、教育、叙事、艺术创作和文化保护等多领域。

Abstract: Emotion is important for creating compelling virtual reality (VR) content. Although some generative methods have been applied to lower the barrier to creating emotionally rich content, they fail to capture the nuanced emotional semantics and the fine-grained control essential for immersive experiences. To address these limitations, we introduce EmoSpace, a novel framework for emotion-aware content generation that learns dynamic, interpretable emotion prototypes through vision-language alignment. We employ a hierarchical emotion representation with rich learnable prototypes that evolve during training, enabling fine-grained emotional control without requiring explicit emotion labels. We develop a controllable generation pipeline featuring multi-prototype guidance, temporal blending, and attention reweighting that supports diverse applications, including emotional image outpainting, stylized generation, and emotional panorama generation for VR environments. Our experiments demonstrate the superior performance of EmoSpace over existing methods in both qualitative and quantitative evaluations. Additionally, we present a comprehensive user study investigating how VR environments affect emotional perception compared to desktop settings. Our work facilitates immersive visual content generation with fine-grained emotion control and supports applications like therapy, education, storytelling, artistic creation, and cultural preservation. Code and models will be made publicly available.

</details>


### [34] [Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes](https://arxiv.org/abs/2602.11660)
*Jeongho Noh,Tai Hyoung Rhee,Eunho Lee,Jeongyun Kim,Sunwoo Lee,Ayoung Kim*

Main category: cs.CV

TL;DR: Clutt3R-Seg 是一种零样本、层次化语义树驱动的 3D 实例分割方法，专为语言引导的机器人抓取在杂乱场景中设计，显著提升遮挡与稀疏视角下的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在杂乱环境中因遮挡、视角受限和掩码噪声导致 3D 实例分割不可靠，难以支撑语言引导的机器人操作。

Method: 提出分层实例语义树，将原始噪声掩码作为有用线索，通过跨视图聚类与条件替换抑制过/欠分割；引入开放词汇语义嵌入以支持自然语言目标选择；设计一致性感知更新机制，仅凭单张交互后图像维持实例对应关系。

Result: 在合成与真实数据集及真实机器人上均超越 SOTA；重杂乱序列下 AP@25 达 61.66（超基线 2.2 倍）；仅用 4 个视角即超过 MaskClustering（8 视角）2 倍以上。

Conclusion: Clutt3R-Seg 实现了高效、鲁棒、语言可解释的 3D 实例分割，为真实杂乱场景中的语言-动作闭环提供了可靠感知基础。

Abstract: Reliable 3D instance segmentation is fundamental to language-grounded robotic manipulation. Its critical application lies in cluttered environments, where occlusions, limited viewpoints, and noisy masks degrade perception. To address these challenges, we present Clutt3R-Seg, a zero-shot pipeline for robust 3D instance segmentation for language-grounded grasping in cluttered scenes. Our key idea is to introduce a hierarchical instance tree of semantic cues. Unlike prior approaches that attempt to refine noisy masks, our method leverages them as informative cues: through cross-view grouping and conditional substitution, the tree suppresses over- and under-segmentation, yielding view-consistent masks and robust 3D instances. Each instance is enriched with open-vocabulary semantic embeddings, enabling accurate target selection from natural language instructions. To handle scene changes during multi-stage tasks, we further introduce a consistency-aware update that preserves instance correspondences from only a single post-interaction image, allowing efficient adaptation without rescanning. Clutt3R-Seg is evaluated on both synthetic and real-world datasets, and validated on a real robot. Across all settings, it consistently outperforms state-of-the-art baselines in cluttered and sparse-view scenarios. Even on the most challenging heavy-clutter sequences, Clutt3R-Seg achieves an AP@25 of 61.66, over 2.2x higher than baselines, and with only four input views it surpasses MaskClustering with eight views by more than 2x. The code is available at: https://github.com/jeonghonoh/clutt3r-seg.

</details>


### [35] [Egocentric Gaze Estimation via Neck-Mounted Camera](https://arxiv.org/abs/2602.11669)
*Haoyu Huang,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了颈戴式视角凝视估计这一新任务，并构建了首个相关数据集，提出了一种基于Transformer的凝视估计模型GLC及其两种改进方法：辅助的视线越界分类任务和多视角协同学习方法；实验表明视线越界分类有效提升了性能，而协同学习未带来增益。


<details>
  <summary>Details</summary>
Motivation: 现有以第一人称视角进行凝视估计的研究主要集中在头戴式摄像头，而颈戴式等其他视角尚未被充分探索，因此需要拓展该任务并填补数据与方法空白。

Method: 构建首个颈戴式凝视估计数据集（含8名参与者、约4小时日常活动视频）；提出基于Transformer的GLC模型；引入辅助的视线越界分类任务；设计几何感知的多视角（头视图+颈视图）协同学习框架。

Result: 视线越界分类辅助任务显著提升模型性能，而多视角协同学习方法未带来性能增益；对结果进行了深入分析并讨论了颈戴式凝视估计的技术挑战与启示。

Conclusion: 颈戴式凝视估计是一个有潜力的新方向，辅助分类任务是有效的建模策略，但多视角协同需更合理的几何建模或损失设计；该工作为后续研究提供了基准数据集与初步方法验证。

Abstract: This paper introduces neck-mounted view gaze estimation, a new task that estimates user gaze from the neck-mounted camera perspective. Prior work on egocentric gaze estimation, which predicts device wearer's gaze location within the camera's field of view, mainly focuses on head-mounted cameras while alternative viewpoints remain underexplored. To bridge this gap, we collect the first dataset for this task, consisting of approximately 4 hours of video collected from 8 participants during everyday activities. We evaluate a transformer-based gaze estimation model, GLC, on the new dataset and propose two extensions: an auxiliary gaze out-of-bound classification task and a multi-view co-learning approach that jointly trains head-view and neck-view models using a geometry-aware auxiliary loss. Experimental results show that incorporating gaze out-of-bound classification improves performance over standard fine-tuning, while the co-learning approach does not yield gains. We further analyze these results and discuss implications for neck-mounted gaze estimation.

</details>


### [36] [U-Net with Hadamard Transform and DCT Latent Spaces for Next-day Wildfire Spread Prediction](https://arxiv.org/abs/2602.11672)
*Yingyi Luo,Shuaiang Rong,Adam Watts,Ahmet Enis Cetin*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级深度学习模型TD-FusionUNet，融合可训练的哈达玛与余弦变换层，结合定制预处理方法，在两个 wildfire 数据集上实现了高F1分数（0.591）与低参数量（370k）的平衡，适用于资源受限环境下的实时野火蔓延预测。


<details>
  <summary>Details</summary>
Motivation: 提升野火蔓延预测的实时性与适用性，尤其在计算资源受限环境下，需兼顾模型精度与轻量化。

Method: 提出TD-FusionUNet模型，引入可训练的二维Hadamard与DCT变换层以捕获正交隐空间中的频率特征；结合随机边缘裁剪和高斯混合模型等定制预处理技术，增强稀疏火前掩模表征与泛化能力。

Result: 在Next-Day Wildfire Spread和WildfireSpreadTS两个数据集上，F1得分为0.591，参数量仅370k，优于WildfireSpreadTS中基于ResNet18编码器的UNet基线模型。

Conclusion: TD-FusionUNet在轻量约束下实现了精度与效率的良好权衡，具备在资源受限场景中部署用于实时野火预测的潜力。

Abstract: We developed a lightweight and computationally efficient tool for next-day wildfire spread prediction using multimodal satellite data as input. The deep learning model, which we call Transform Domain Fusion UNet (TD-FusionUNet), incorporates trainable Hadamard Transform and Discrete Cosine Transform layers that apply two-dimensional transforms, enabling the network to capture essential "frequency" components in orthogonalized latent spaces. Additionally, we introduce custom preprocessing techniques, including random margin cropping and a Gaussian mixture model, to enrich the representation of the sparse pre-fire masks and enhance the model's generalization capability. The TD-FusionUNet is evaluated on two datasets which are the Next-Day Wildfire Spread dataset released by Google Research in 2023, and WildfireSpreadTS dataset. Our proposed TD-FusionUNet achieves an F1 score of 0.591 with 370k parameters, outperforming the UNet baseline using ResNet18 as the encoder reported in the WildfireSpreadTS dataset while using substantially fewer parameters. These results show that the proposed latent space fusion model balances accuracy and efficiency under a lightweight setting, making it suitable for real time wildfire prediction applications in resource limited environments.

</details>


### [37] [RI-Mamba: Rotation-Invariant Mamba for Robust Text-to-Shape Retrieval](https://arxiv.org/abs/2602.11673)
*Khanh Nguyen,Dasith de Silva Edirimuni,Ghulam Mubashar Hassan,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文提出了RI-Mamba，首个面向点云的旋转不变状态空间模型，用于解决文本到三维形状检索中因物体姿态任意、类别多样带来的挑战；通过定义参考系、Hilbert排序和方向嵌入调制，在保持旋转不变性的同时恢复空间上下文，结合自动三元组生成的跨模态对比学习，在OmniObject3D上实现200+类别的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到形状检索方法依赖规范姿态且支持类别有限，难以应对真实场景中物体姿态任意、类别多样的问题。

Method: 提出RI-Mamba：1）构建全局与局部参考系解耦姿态与几何；2）采用Hilbert排序生成具有几何意义且旋转不变的token序列；3）设计方向嵌入并通过特征级线性调制（FiLM）重融入空间信息；4）结合自动三元组生成的跨模态对比学习进行大规模训练。

Result: 在OmniObject3D基准上，对200多个物体类别、任意朝向下的文本-形状检索任务达到SOTA性能，验证了模型的强表征能力与姿态鲁棒性。

Conclusion: RI-Mamba首次将旋转不变性与状态空间建模有效结合，为开放域、多类别、非规范姿态下的三维内容检索提供了高效、可扩展的新范式。

Abstract: 3D assets have rapidly expanded in quantity and diversity due to the growing popularity of virtual reality and gaming. As a result, text-to-shape retrieval has become essential in facilitating intuitive search within large repositories. However, existing methods require canonical poses and support few object categories, limiting their real-world applicability where objects can belong to diverse classes and appear in random orientations. To address this challenge, we propose RI-Mamba, the first rotation-invariant state-space model for point clouds. RI-Mamba defines global and local reference frames to disentangle pose from geometry and uses Hilbert sorting to construct token sequences with meaningful geometric structure while maintaining rotation invariance. We further introduce a novel strategy to compute orientational embeddings and reintegrate them via feature-wise linear modulation, effectively recovering spatial context and enhancing model expressiveness. Our strategy is inherently compatible with state-space models and operates in linear time. To scale up retrieval, we adopt cross-modal contrastive learning with automated triplet generation, allowing training on diverse datasets without manual annotation. Extensive experiments demonstrate RI-Mamba's superior representational capacity and robustness, achieving state-of-the-art performance on the OmniObject3D benchmark across more than 200 object categories under arbitrary orientations. Our code will be made available at https://github.com/ndkhanh360/RI-Mamba.git.

</details>


### [38] [Semantically Conditioned Diffusion Models for Cerebral DSA Synthesis](https://arxiv.org/abs/2602.11703)
*Qiwen Xu,David Rügamer,Holger Wenz,Johann Fontana,Nora Meggyeshazi,Andreas Bender,Máté E. Maros*

Main category: cs.CV

TL;DR: 本文提出了一种语义条件隐式扩散模型（LDM），用于合成具有解剖循环（前/后循环）和C臂位置控制的动脉期脑DSA图像；在99,349帧单中心数据上训练，经4位医学专家评估显示临床真实感良好（Likert评分3.1–3.3），FID为15.27，表明其可用于算法开发、研究与培训。


<details>
  <summary>Details</summary>
Motivation: DSA虽关键但具侵入性和高成本，导致大规模数据采集与共享受限，亟需高质量合成数据替代方案。

Method: 构建语义条件隐式扩散模型（LDM），以解剖循环类型（前/后）和C臂标准位姿为文本条件，基于自建99,349帧单中心DSA数据集进行训练。

Result: 生成图像获4位医学专家5级Likert评分（平均3.1–3.3），组内相关系数ICC(2,k)=0.80–0.87；Fréchet Inception Distance中位数为15.27，分布相似性高。

Conclusion: 语义可控的LDM能生成具备临床真实感的DSA图像，可支撑下游算法研发、科研及医学培训。

Abstract: Digital subtraction angiography (DSA) plays a central role in the diagnosis and treatment of cerebrovascular disease, yet its invasive nature and high acquisition cost severely limit large-scale data collection and public data sharing. Therefore, we developed a semantically conditioned latent diffusion model (LDM) that synthesizes arterial-phase cerebral DSA frames under explicit control of anatomical circulation (anterior vs.\ posterior) and canonical C-arm positions. We curated a large single-centre DSA dataset of 99,349 frames and trained a conditional LDM using text embeddings that encoded anatomy and acquisition geometry. To assess clinical realism, four medical experts, including two neuroradiologists, one neurosurgeon, and one internal medicine expert, systematically rated 400 synthetic DSA images using a 5-grade Likert scale for evaluating proximal large, medium, and small peripheral vessels. The generated images achieved image-wise overall Likert scores ranging from 3.1 to 3.3, with high inter-rater reliability (ICC(2,k) = 0.80--0.87). Distributional similarity to real DSA frames was supported by a low median Fréchet inception distance (FID) of 15.27. Our results indicate that semantically controlled LDMs can produce realistic synthetic DSAs suitable for downstream algorithm development, research, and training.

</details>


### [39] [TG-Field: Geometry-Aware Radiative Gaussian Fields for Tomographic Reconstruction](https://arxiv.org/abs/2602.11705)
*Yuxiang Zhong,Jun Wei,Chaoqi Chen,Senyou An,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了TG-Field，一种面向CT重建（静态与动态）的几何感知高斯形变框架，通过多分辨率哈希编码、时序条件表示、时空注意力机制及运动流网络，显著提升了超稀疏视角下的重建精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法在CT重建中面临超稀疏投影和动态运动下的严重伪影问题。

Method: 提出Tomographic Geometry Field（TG-Field）：1）多分辨率哈希编码建模局部空间先验；2）时间条件化表示与时空注意力块实现动态特征自适应聚合；3）运动流网络建模呼吸运动以跟踪解剖形变。

Result: 在合成与真实CT数据集上，TG-Field在高度稀疏视角下显著优于现有方法，达到SOTA重建精度。

Conclusion: TG-Field是一种有效且通用的CT重建框架，兼顾几何感知、稀疏鲁棒性与动态一致性，为医学影像三维重建提供了新范式。

Abstract: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene representation with superior efficiency and quality. While recent adaptations for computed tomography (CT) show promise, they struggle with severe artifacts under highly sparse-view projections and dynamic motions. To address these challenges, we propose Tomographic Geometry Field (TG-Field), a geometry-aware Gaussian deformation framework tailored for both static and dynamic CT reconstruction. A multi-resolution hash encoder is employed to capture local spatial priors, regularizing primitive parameters under ultra-sparse settings. We further extend the framework to dynamic reconstruction by introducing time-conditioned representations and a spatiotemporal attention block to adaptively aggregate features, thereby resolving spatiotemporal ambiguities and enforcing temporal coherence. In addition, a motion-flow network models fine-grained respiratory motion to track local anatomical deformations. Extensive experiments on synthetic and real-world datasets demonstrate that TG-Field consistently outperforms existing methods, achieving state-of-the-art reconstruction accuracy under highly sparse-view conditions.

</details>


### [40] [LLM-Driven 3D Scene Generation of Agricultural Simulation Environments](https://arxiv.org/abs/2602.11706)
*Arafa Yoncalik,Wouter Jansen,Nico Huebel,Mohammad Hasan Rahmani,Jan Steckel*

Main category: cs.CV

TL;DR: 本文提出了一种基于多LLM模块化流水线的农业合成仿真环境生成方法，通过结合领域知识注入、RAG、微调与验证等技术，实现从自然语言提示到Unreal引擎3D场景的自动、可控、可扩展生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的3D场景生成方法缺乏农业等特定领域的推理能力、验证机制和模块化设计，导致控制力弱、可扩展性差。

Method: 构建模块化多LLM流水线，集成3D资产检索、农业领域知识注入与Unreal API代码生成；采用少样本提示、RAG、微调与多级验证的混合策略。

Result: 系统在结构化提示测试中表现良好，用户研究显示生成场景具有高 realism 和 familiarity；专家评估表明相比手工建模显著节省时间。

Conclusion: 多LLM模块化架构能有效提升领域专用3D场景生成的可靠性、精度与可维护性，为农业及其他仿真领域提供可扩展自动化方案。

Abstract: Procedural generation techniques in 3D rendering engines have revolutionized the creation of complex environments, reducing reliance on manual design. Recent approaches using Large Language Models (LLMs) for 3D scene generation show promise but often lack domain-specific reasoning, verification mechanisms, and modular design. These limitations lead to reduced control and poor scalability. This paper investigates the use of LLMs to generate agricultural synthetic simulation environments from natural language prompts, specifically to address the limitations of lacking domain-specific reasoning, verification mechanisms, and modular design. A modular multi-LLM pipeline was developed, integrating 3D asset retrieval, domain knowledge injection, and code generation for the Unreal rendering engine using its API. This results in a 3D environment with realistic planting layouts and environmental context, all based on the input prompt and the domain knowledge. To enhance accuracy and scalability, the system employs a hybrid strategy combining LLM optimization techniques such as few-shot prompting, Retrieval-Augmented Generation (RAG), finetuning, and validation. Unlike monolithic models, the modular architecture enables structured data handling, intermediate verification, and flexible expansion. The system was evaluated using structured prompts and semantic accuracy metrics. A user study assessed realism and familiarity against real-world images, while an expert comparison demonstrated significant time savings over manual scene design. The results confirm the effectiveness of multi-LLM pipelines in automating domain-specific 3D scene generation with improved reliability and precision. Future work will explore expanding the asset hierarchy, incorporating real-time generation, and adapting the pipeline to other simulation domains beyond agriculture.

</details>


### [41] [GSO-SLAM: Bidirectionally Coupled Gaussian Splatting and Direct Visual Odometry](https://arxiv.org/abs/2602.11714)
*Jiung Yeon,Seongbo Ha,Hyeonwoo Yu*

Main category: cs.CV

TL;DR: 本文提出了GSO-SLAM，一种基于高斯场景表示的实时单目稠密SLAM系统，通过EM框架双向耦合视觉里程计（VO）与高斯泼溅（GS），联合优化深度估计与场景表示，并引入高斯泼溅初始化方法提升重建精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法在跟踪与建图耦合方式上存在计算开销大或冗余度高的问题：统一场景建模导致计算成本高，松耦合则引入冗余；亟需一种高效、精准且实时的稠密SLAM方案。

Method: 提出GSO-SLAM，采用EM框架实现VO与高斯泼溅的双向联合优化；设计高斯泼溅初始化方法，利用VO提供的图像信息、关键帧位姿和像素关联生成高质量初始高斯场景。

Result: 在多项实验中验证了方法的有效性：实现实时运行，同时在几何/光度重建保真度和跟踪精度上达到当前最优水平。

Conclusion: GSO-SLAM通过紧耦合VO与GS并消除启发式初始化，显著提升了单目稠密SLAM的效率与精度，为实时高保真场景重建提供了新范式。

Abstract: We propose GSO-SLAM, a real-time monocular dense SLAM system that leverages Gaussian scene representation. Unlike existing methods that couple tracking and mapping with a unified scene, incurring computational costs, or loosely integrate them with well-structured tracking frameworks, introducing redundancies, our method bidirectionally couples Visual Odometry (VO) and Gaussian Splatting (GS). Specifically, our approach formulates joint optimization within an Expectation-Maximization (EM) framework, enabling the simultaneous refinement of VO-derived semi-dense depth estimates and the GS representation without additional computational overhead. Moreover, we present Gaussian Splat Initialization, which utilizes image information, keyframe poses, and pixel associations from VO to produce close approximations to the final Gaussian scene, thereby eliminating the need for heuristic methods. Through extensive experiments, we validate the effectiveness of our method, showing that it not only operates in real time but also achieves state-of-the-art geometric/photometric fidelity of the reconstructed scene and tracking accuracy.

</details>


### [42] [STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning](https://arxiv.org/abs/2602.11730)
*Xiaowen Zhang,Zhi Gao,Licheng Jiao,Lingling Li,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉提示范式和首个用于时空视频定位（STVG）的强化学习框架STVG-R1，通过实例级ID编码避免跨模态坐标对齐难题，并利用任务驱动奖励联合优化时序精度、空间一致性和格式规范性，在多个基准上显著超越现有方法，并展现出优异的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在时空视频定位（STVG）等稠密预测任务中，因文本描述与视觉坐标的错位常引发幻觉；而现有方法依赖额外可训练模块，带来高昂标注成本与计算开销。

Method: 提出基于唯一且时间一致的实例ID的视觉提示范式，将逐帧坐标预测转化为紧凑的实例识别问题；并构建首个STVG强化学习框架STVG-R1，采用融合时序精度、空间一致性与结构格式正则化的任务驱动奖励进行联合优化。

Result: 在HCSTVG-v2上m_IoU较Qwen2.5-VL-7B提升20.9%，创SOTA；在MeViS上零样本迁移实现47.3% J&F，亦达SOTA。

Conclusion: 所提视觉提示与STVG-R1框架有效规避跨模态对齐难题，大幅降低训练依赖，同时兼顾性能与泛化性，为STVG及多目标视频分割任务提供新范式。

Abstract: In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.

</details>


### [43] [Adapting Vision-Language Models for E-commerce Understanding at Scale](https://arxiv.org/abs/2602.11733)
*Matteo Nulli,Vladimir Orshulevich,Tala Bazazo,Christian Herold,Michael Kozielski,Marcin Mazur,Szymon Tuzel,Cees G. M. Snoek,Seyyed Hadi Hashemi,Omar Javed,Yannick Versley,Shahram Khadivi*

Main category: cs.CV

TL;DR: 本文提出了一种针对电商数据特性的通用视觉语言模型（VLM）适配方法，并构建了涵盖深度商品理解、严格指令遵循与动态属性抽取的综合评估套件。


<details>
  <summary>Details</summary>
Motivation: 通用VLM难以直接适配电商场景中属性密集、多图输入及噪声干扰等特点，缺乏已验证的适配策略。

Method: 通过大规模实验研究，设计面向电商数据特性的目标化VLM适配方法，并构建新型多维度评估套件。

Result: 显著提升电商任务性能，同时保持VLM在通用多模态任务上的能力。

Conclusion: 针对性适配通用VLM是兼顾电商专用性能与通用多模态能力的有效路径。

Abstract: E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction.

</details>


### [44] [JEPA-VLA: Video Predictive Embedding is Needed for VLA Models](https://arxiv.org/abs/2602.11832)
*Shangchen Miao,Ningya Feng,Jialong Wu,Ye Lin,Xu He,Dong Li,Mingsheng Long*

Main category: cs.CV

TL;DR: 本文提出JEPA-VLA方法，通过引入视频预训练的预测性视觉表征（如V-JEPA 2）来弥补现有视觉语言动作模型在环境理解与策略先验上的不足，显著提升样本效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型受限于预训练视觉表征，难以有效捕获任务相关环境信息和策略先验（即对成功执行任务时环境演化的预见性知识）。

Method: 分析不同视觉表征的局限性，发现视频预测式表征（如V-JEPA 2）能更好建模任务相关时序动态并忽略不可预测因素；据此提出JEPA-VLA，将预测性嵌入自适应地融入现有VLA架构。

Result: JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0及真实机器人任务等多个基准上均取得显著性能提升。

Conclusion: 视频预训练的预测性视觉表征是提升VLA模型样本效率与泛化能力的关键，JEPA-VLA为VLA架构设计提供了新思路。

Abstract: Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.

</details>


### [45] [Mask What Matters: Mitigating Object Hallucinations in Multimodal Large Language Models with Object-Aligned Visual Contrastive Decoding](https://arxiv.org/abs/2602.11737)
*Boqi Chen,Xudong Liu,Jianing Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种改进视觉对比解码（VCD）的方法，通过构建对象对齐的辅助视图来缓解多模态大语言模型（MLLMs）中的物体幻觉问题，该方法利用自监督ViT中的对象中心注意力，移除最显著的视觉证据以增强对比信号，具有提示无关、模型无关和低计算开销的特点。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLMs）中普遍存在的物体幻觉问题。

Method: 基于自监督Vision Transformer的对象中心注意力机制，构造一个移除最显著视觉证据的辅助视图，以增强视觉对比解码（VCD）中的对比信号。

Result: 在两个主流物体幻觉评测基准上，对两种MLLMs均展现出一致性能提升，且方法无需修改提示或模型结构，仅需一次可缓存的前向传播。

Conclusion: 所提对象对齐辅助视图方法能有效缓解MLLMs中的物体幻觉，具备通用性、即插即用性和高效性。

Abstract: We study object hallucination in Multimodal Large Language Models (MLLMs) and improve visual contrastive decoding (VCD) by constructing an object-aligned auxiliary view. We leverage object-centric attention in self-supervised Vision Transformers. In particular, we remove the most salient visual evidence to construct an auxiliary view that disrupts unsupported tokens and produces a stronger contrast signal. Our method is prompt-agnostic, model-agnostic, and can be seamlessly plugged into the existing VCD pipeline with little computation overhead, i.e., a single cacheable forward pass. Empirically, our method demonstrates consistent gains on two popular object hallucination benchmarks across two MLLMs.

</details>


### [46] [DiffPlace: Street View Generation via Place-Controllable Diffusion Model Enhancing Place Recognition](https://arxiv.org/abs/2602.11875)
*Ji Li,Zhiwei Li,Shihao Li,Zhenjiang Yu,Boyang Wang,Haiou Liu*

Main category: cs.CV

TL;DR: 本文提出DiffPlace框架，通过引入place-ID控制器实现可控的多视角图像生成，提升城市街景生成的地点感知能力和背景一致性，从而增强视觉地点识别任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有多视角扩散模型在生成文本、鸟瞰图（BEV）地图和物体边界框驱动的城市街景时，难以保证地点感知性和背景一致性，限制了其在地点识别任务中的应用效果。

Method: 提出DiffPlace框架，包含place-ID控制器，利用线性投影、Perceiver Transformer和对比学习将place-ID嵌入映射到固定CLIP空间，以实现背景建筑一致、前景物体与天气可调的多视角图像生成。

Result: 实验表明DiffPlace在生成质量及对视觉地点识别任务的训练支持方面均优于现有方法。

Conclusion: DiffPlace有效提升了生成场景的地点感知性和背景一致性，为自动驾驶中地点识别任务提供了新的生成式解决方案。

Abstract: Generative models have advanced significantly in realistic image synthesis, with diffusion models excelling in quality and stability. Recent multi-view diffusion models improve 3D-aware street view generation, but they struggle to produce place-aware and background-consistent urban scenes from text, BEV maps, and object bounding boxes. This limits their effectiveness in generating realistic samples for place recognition tasks. To address these challenges, we propose DiffPlace, a novel framework that introduces a place-ID controller to enable place-controllable multi-view image generation. The place-ID controller employs linear projection, perceiver transformer, and contrastive learning to map place-ID embeddings into a fixed CLIP space, allowing the model to synthesize images with consistent background buildings while flexibly modifying foreground objects and weather conditions. Extensive experiments, including quantitative comparisons and augmented training evaluations, demonstrate that DiffPlace outperforms existing methods in both generation quality and training support for visual place recognition. Our results highlight the potential of generative models in enhancing scene-level and place-aware synthesis, providing a valuable approach for improving place recognition in autonomous driving

</details>


### [47] [Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation](https://arxiv.org/abs/2602.11743)
*Xiangyu Wu,Dongming Jiang,Feng Yu,Yueying Tian,Jiaqi Tang,Qing-Guo Chen,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于Tsallis熵的自适应去偏方法ADTE，用于视觉-语言模型（如CLIP）的测试时自适应（TTA），有效缓解了因预训练数据不平衡导致的Shannon熵不确定性估计偏差问题。


<details>
  <summary>Details</summary>
Motivation: 主流TTA方法依赖Shannon熵衡量预测不确定性，但CLIP等模型在高度不平衡的网络数据上预训练，导致Shannon熵产生有偏的不确定性估计。

Method: 提出Adaptive Debiasing Tsallis Entropy（ADTE），将Tsallis熵推广为类别自适应的非广延参数q^l，该参数由持续到来的测试样本估计的标签偏差归一化得到；无需额外超参调优，可直接替代Shannon熵并兼容标签调整策略。

Result: ADTE在ImageNet及其5个变体上超越现有最优方法，并在10个跨域基准上取得最高平均性能，且不依赖模型架构或文本提示。

Conclusion: Tsallis熵是Shannon熵的自然泛化，更适合刻画有偏分布；ADTE提供了一种通用、即插即用、无需调参的去偏TTA方案。

Abstract: Mainstream Test-Time Adaptation (TTA) methods for adapting vision-language models, e.g., CLIP, typically rely on Shannon Entropy (SE) at test time to measure prediction uncertainty and inconsistency. However, since CLIP has a built-in bias from pretraining on highly imbalanced web-crawled data, SE inevitably results in producing biased estimates of uncertainty entropy. To address this issue, we notably find and demonstrate that Tsallis Entropy (TE), a generalized form of SE, is naturally suited for characterizing biased distributions by introducing a non-extensive parameter q, with the performance of SE serving as a lower bound for TE. Building upon this, we generalize TE into Adaptive Debiasing Tsallis Entropy (ADTE) for TTA, customizing a class-specific parameter q^l derived by normalizing the estimated label bias from continuously incoming test instances, for each category. This adaptive approach allows ADTE to accurately select high-confidence views and seamlessly integrate with a label adjustment strategy to enhance adaptation, without introducing distribution-specific hyperparameter tuning. Besides, our investigation reveals that both TE and ADTE can serve as direct, advanced alternatives to SE in TTA, without any other modifications. Experimental results show that ADTE outperforms state-of-the-art methods on ImageNet and its five variants, and achieves the highest average performance on 10 cross-domain benchmarks, regardless of the model architecture or text prompts used. Our code is available at https://github.com/Jinx630/ADTE.

</details>


### [48] [Code2Worlds: Empowering Coding LLMs for 4D World Generation](https://arxiv.org/abs/2602.11757)
*Yi Zhang,Yunshuang Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出Code2Worlds框架，将4D（时空）动态场景生成建模为‘语言到物理仿真代码’的生成任务，通过双流架构解耦物体与环境生成，并引入物理感知的闭环机制（含PostProcess Agent和VLM-Motion Critic）提升动态保真度，在Code4D基准上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的3D生成方法难以扩展至4D动态模拟，存在多尺度上下文纠缠和语义-物理执行鸿沟两大挑战，亟需构建物理 grounded 的世界模拟器。

Method: 提出Code2Worlds框架：1）双流架构，分别处理检索增强的物体生成与分层环境编排；2）物理感知闭环机制，包含脚本化动力学的PostProcess Agent和基于视觉语言模型的Motion Critic进行自反思式迭代优化。

Result: 在Code4D基准上，相比基线方法，SGS指标提升41%，Richness提升49%，且首次实现无需人工干预的、具备物理合理性的4D动态生成。

Conclusion: Code2Worlds验证了‘语言→仿真代码→物理世界’范式的有效性，为构建空间智能与具身AI提供了可扩展、可验证的4D生成新路径。

Abstract: Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.

</details>


### [49] [Light4D: Training-Free Extreme Viewpoint 4D Video Relighting](https://arxiv.org/abs/2602.11769)
*Zhenghuang Wu,Kang Chen,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了Light4D，一种无需训练的4D视频重光照框架，通过解耦光流引导和时序一致注意力机制，在极端视角变化下实现高保真、时间一致的4D重光照合成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的重光照方法难以扩展到4D（时空）场景，主要受限于配对4D训练数据稀缺及极端视角下时序一致性难以维持。

Method: 提出Light4D框架：1）解耦光流引导（Disentangled Flow Guidance），在潜在空间中注入光照控制并保持几何完整性；2）在IC-Light架构中引入时序一致注意力（Temporal Consistent Attention）并加入确定性正则化以消除闪烁。

Result: 实验表明该方法在时序一致性和光照保真度上达到领先水平，可稳健处理-90°至90°的相机旋转。

Conclusion: Light4D是一种训练无关、高效且鲁棒的4D视频重光照新范式，为无配对数据下的高维光照编辑提供了可行路径。

Abstract: Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.

</details>


### [50] [Efficient Segment Anything with Depth-Aware Fusion and Limited Training Data](https://arxiv.org/abs/2602.11804)
*Yiming Zhou,Xuenjie Xie,Panfeng Li,Albrecht Kunz,Ahmad Osman,Xavier Maldague*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级RGB-D融合框架，通过引入单目深度先验增强EfficientViT-SAM，在仅用11.2k样本（<0.1% SA-1B）训练下，分割精度超越原模型。


<details>
  <summary>Details</summary>
Motivation: SAM虽性能优异但依赖海量数据和纯RGB输入；现有高效变体仍需大规模训练，缺乏对几何信息的有效利用。

Method: 在EfficientViT-SAM基础上，接入预训练单目深度估计器生成深度图，并设计专用深度编码器将深度特征与RGB中层特征进行融合。

Result: 仅用11.2k样本训练即在分割精度上超过EfficientViT-SAM，验证了深度线索作为强几何先验的有效性。

Conclusion: 引入轻量级RGB-D融合机制可显著降低SAM类模型的数据依赖，提升小样本下的分割性能，凸显几何先验的重要性。

Abstract: Segment Anything Models (SAM) achieve impressive universal segmentation performance but require massive datasets (e.g., 11M images) and rely solely on RGB inputs. Recent efficient variants reduce computation but still depend on large-scale training. We propose a lightweight RGB-D fusion framework that augments EfficientViT-SAM with monocular depth priors. Depth maps are generated with a pretrained estimator and fused mid-level with RGB features through a dedicated depth encoder. Trained on only 11.2k samples (less than 0.1\% of SA-1B), our method achieves higher accuracy than EfficientViT-SAM, showing that depth cues provide strong geometric priors for segmentation.

</details>


### [51] [How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?](https://arxiv.org/abs/2602.11810)
*Marko Putak,Thomas B. Moeslund,Joakim Bruslund Haurum*

Main category: cs.CV

TL;DR: 本文提出Targeted Smart Filtering方法，利用3D迭代函数系统（IFS）生成高质量、多样化的3D分形视频用于动作识别模型的预训练，显著提升采样速度（约100倍）和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统合成数据方法存在人工标注成本高、隐私与伦理问题；而现有3D分形生成方法速度慢且易产生退化结构，难以支撑有效预训练。

Method: 基于3D迭代函数系统（IFS）生成分形，引入时间维度构造视频序列，并提出Targeted Smart Filtering策略，通过智能筛选机制平衡生成速度与分形多样性。

Result: 所提方法比标准3D分形生成快约100倍，在动作识别下游任务中性能优于其他3D分形过滤方法。

Conclusion: Targeted Smart Filtering是一种高效、实用的合成预训练数据生成方案，验证了高质量合成数据在视觉表征学习中的潜力。

Abstract: Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.

</details>


### [52] [WorldTree: Towards 4D Dynamic Worlds from Monocular Video using Tree-Chains](https://arxiv.org/abs/2602.11845)
*Qisen Wang,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 本文提出WorldTree框架，通过Temporal Partition Tree（TPT）和Spatial Ancestral Chains（SAC）实现统一的时空分解，显著提升单目动态重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目动态重建方法缺乏统一的时空分解框架，导致时间优化粗糙或空间层级耦合，难以兼顾效率与精度。

Method: 提出WorldTree框架：1）Temporal Partition Tree（TPT）基于继承式划分树结构实现由粗到细的时间层次分解；2）Spatial Ancestral Chains（SAC）通过递归查询祖先层级结构建模空间动态，并在各祖先节点上特化运动表征。

Result: 在NVIDIA-LS上LPIPS提升8.26%，在DyCheck上mLPIPS提升9.09%，优于次优方法。

Conclusion: WorldTree提供了一种解耦且可扩展的时空建模范式，有效提升了单目动态场景重建的质量与灵活性。

Abstract: Dynamic reconstruction has achieved remarkable progress, but there remain challenges in monocular input for more practical applications. The prevailing works attempt to construct efficient motion representations, but lack a unified spatiotemporal decomposition framework, suffering from either holistic temporal optimization or coupled hierarchical spatial composition. To this end, we propose WorldTree, a unified framework comprising Temporal Partition Tree (TPT) that enables coarse-to-fine optimization based on the inheritance-based partition tree structure for hierarchical temporal decomposition, and Spatial Ancestral Chains (SAC) that recursively query ancestral hierarchical structure to provide complementary spatial dynamics while specializing motion representations across ancestral nodes. Experimental results on different datasets indicate that our proposed method achieves 8.26% improvement of LPIPS on NVIDIA-LS and 9.09% improvement of mLPIPS on DyCheck compared to the second-best method. Code: https://github.com/iCVTEAM/WorldTree.

</details>


### [53] [Free Lunch for Stabilizing Rectified Flow Inversion](https://arxiv.org/abs/2602.11850)
*Chenru Wang,Beier Zhu,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出Proximal-Mean Inversion（PMI）和mimic-CFG两种训练免费的梯度校正方法，以提升Rectified-Flow生成模型的反演稳定性、重建质量与编辑保真度。


<details>
  <summary>Details</summary>
Motivation: 现有RF模型的反演方法存在跨时间步累积的近似误差，导致速度场不稳定、重建与编辑质量下降。

Method: 提出PMI方法，通过将当前速度引导至历史速度均值并在理论推导的球形高斯约束内稳定速度场；同时引入轻量级mimic-CFG，对速度进行插值校正以兼顾编辑效果与结构一致性。

Result: 在PIE-Bench上实验表明，所提方法显著提升反演稳定性、图像重建质量与编辑保真度，并减少神经函数评估次数。

Conclusion: PMI与mimic-CFG在保持训练免费特性的同时，实现了更优的理论保障、效率与性能，达到PIE-Bench上的SOTA水平。

Abstract: Rectified-Flow (RF)-based generative models have recently emerged as strong alternatives to traditional diffusion models, demonstrating state-of-the-art performance across various tasks. By learning a continuous velocity field that transforms simple noise into complex data, RF-based models not only enable high-quality generation, but also support training-free inversion, which facilitates downstream tasks such as reconstruction and editing. However, existing inversion methods, such as vanilla RF-based inversion, suffer from approximation errors that accumulate across timesteps, leading to unstable velocity fields and degraded reconstruction and editing quality. To address this challenge, we propose Proximal-Mean Inversion (PMI), a training-free gradient correction method that stabilizes the velocity field by guiding it toward a running average of past velocities, constrained within a theoretically derived spherical Gaussian. Furthermore, we introduce mimic-CFG, a lightweight velocity correction scheme for editing tasks, which interpolates between the current velocity and its projection onto the historical average, balancing editing effectiveness and structural consistency. Extensive experiments on PIE-Bench demonstrate that our methods significantly improve inversion stability, image reconstruction quality, and editing fidelity, while reducing the required number of neural function evaluations. Our approach achieves state-of-the-art performance on the PIE-Bench with enhanced efficiency and theoretical soundness.

</details>


### [54] [Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception](https://arxiv.org/abs/2602.11858)
*Lai Wei,Liangbo He,Jun Lan,Lingzhong Dong,Yutong Cai,Siyuan Li,Huijia Zhu,Weiqiang Wang,Linghe Kong,Yue Wang,Zhuosheng Zhang,Weiran Huang*

Main category: cs.CV

TL;DR: 本文提出Region-to-Image Distillation方法，将‘图像思考’中的推理时缩放操作转化为训练时的蒸馏过程，在单次前向传播中实现细粒度视觉理解，无需推理时调用工具，并构建ZoomBench基准评估该能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在宽泛视觉理解上表现优异，但在需关注微小关键区域的细粒度感知任务中仍存在不足；现有‘Thinking-with-Images’方法虽有效但推理延迟高。

Method: 提出Region-to-Image Distillation：利用强教师模型在微裁剪区域上生成高质量VQA数据，再将区域级监督蒸馏回完整图像，使学生模型在单次前向中获得细粒度感知能力；同时构建ZoomBench基准与双视角评估协议。

Result: 所提模型在多个细粒度感知基准上达到领先性能，并在视觉推理、GUI智能体等通用多模态认知任务上也有提升；验证了部分‘Thinking-with-Images’增益可被蒸馏进单次前向。

Conclusion: 将推理时的主动缩放行为内化为训练时的蒸馏机制，是提升MLLM细粒度感知效率与效果的有效路径；并非所有细粒度任务都需运行时工具调用，部分能力可通过训练阶段建模获得。

Abstract: Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent "Thinking-with-Images" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves "single-glance" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional "zooming gap". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when "Thinking-with-Images" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.

</details>


### [55] [SynthRAR: Ring Artifacts Reduction in CT with Unrolled Network and Synthetic Data Training](https://arxiv.org/abs/2602.11880)
*Hongxu Yang,Levente Lippenszky,Edina Timko,Gopal Avinash*

Main category: cs.CV

TL;DR: 本文提出了一种基于非理想CT探测器响应理论分析的无监督深度学习方法，通过解耦正向投影与探测器响应建模，结合合成数据训练，在无需真实临床数据的情况下有效去除环状和条纹伪影。


<details>
  <summary>Details</summary>
Motivation: 现有环状伪影去除方法依赖大量标注的真实临床数据，成本高；且多局限于图像域或正弦图域单一空间修正，忽略CT几何前向过程中的内在关联。

Method: 将环状伪影去除（RAR）问题建模为一个逆问题，采用展开网络（unrolled network）联合建模非理想探测器响应与CT线性前向投影；利用自然图像生成带伪影的合成正弦图-图像对，实现无真实临床数据的训练。

Result: 在多种扫描几何与解剖区域上验证，该模型在合成数据上训练后性能持续超越现有SOTA方法。

Conclusion: 所提方法突破了对真实临床标注数据的依赖，通过融合物理模型与合成数据驱动，实现了更鲁棒、泛化性更强的环状伪影校正。

Abstract: Defective and inconsistent responses in CT detectors can cause ring and streak artifacts in the reconstructed images, making them unusable for clinical purposes. In recent years, several ring artifact reduction solutions have been proposed in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, leading to a high data collection cost. Furthermore, existing approaches focus exclusively on either image-space or sinogram-space correction, neglecting the intrinsic correlations from the forward operation of the CT geometry. Based on the theoretical analysis of non-ideal CT detector responses, the RAR problem is reformulated as an inverse problem by using an unrolled network, which considers non-ideal response together with linear forward-projection with CT geometry. Additionally, the intrinsic correlations of ring artifacts between the sinogram and image domains are leveraged through synthetic data derived from natural images, enabling the trained model to correct artifacts without requiring real-world clinical data. Extensive evaluations on diverse scanning geometries and anatomical regions demonstrate that the model trained on synthetic data consistently outperforms existing state-of-the-art methods.

</details>


### [56] [DynaHOI: Benchmarking Hand-Object Interaction for Dynamic Target](https://arxiv.org/abs/2602.11919)
*BoCheng Hu,Zhonghan Zhao,Kaiyue Zhou,Hongwei Wang,Gaoang Wang*

Main category: cs.CV

TL;DR: 本文提出了DynaHOI-Gym平台和DynaHOI-10M基准数据集，用于评估动态手-物交互（HOI）中的手部运动生成，同时提出ObAct基线方法以提升定位成功率。


<details>
  <summary>Details</summary>
Motivation: 现有手-物交互（HOI）基准多聚焦静态物体，缺乏对动态目标与时间敏感协同任务的评测能力，亟需构建面向动态场景的评测平台与数据集。

Method: 构建了统一的在线闭环平台DynaHOI-Gym，包含参数化运动生成器和基于rollout的评估指标；发布大规模动态HOI基准DynaHOI-10M（10M帧、180K轨迹），并设计ObAct基线模型，利用时空注意力融合短时观测与当前帧进行动作预测。

Result: DynaHOI-10M涵盖8大类、22细分类别动态目标运动；ObAct基线在位置成功率达8.1%提升。

Conclusion: DynaHOI-Gym与DynaHOI-10M填补了动态HOI评测空白，ObAct验证了观测-动作范式在动态手部生成中的有效性，为后续研究提供新基准与方法启示。

Abstract: Most existing hand motion generation benchmarks for hand-object interaction (HOI) focus on static objects, leaving dynamic scenarios with moving targets and time-critical coordination largely untested. To address this gap, we introduce the DynaHOI-Gym, a unified online closed-loop platform with parameterized motion generators and rollout-based metrics for dynamic capture evaluation. Built on DynaHOI-Gym, we release DynaHOI-10M, a large-scale benchmark with 10M frames and 180K hand capture trajectories, whose target motions are organized into 8 major categories and 22 fine-grained subcategories. We also provide a simple observe-before-act baseline (ObAct) that integrates short-term observations with the current frame via spatiotemporal attention to predict actions, achieving an 8.1% improvement in location success rate.

</details>


### [57] [Synthesis of Late Gadolinium Enhancement Images via Implicit Neural Representations for Cardiac Scar Segmentation](https://arxiv.org/abs/2602.11942)
*Soufiane Ben Haddou,Laura Alvarez-Florez,Erik J. Bekkers,Fleur V. Y. Tjong,Ahmad S. Amin,Connie R. Bezzina,Ivana Išgum*

Main category: cs.CV

TL;DR: 本文提出了一种结合隐式神经表示（INRs）与去噪扩散模型的合成框架，用于生成带配准分割掩码的晚期钆增强（LGE）心脏MRI图像，以缓解标注数据稀缺问题；在133例真实扫描数据上验证表明，加入200例合成数据可将纤维化分割Dice分数从0.509提升至0.524。


<details>
  <summary>Details</summary>
Motivation: 晚期钆增强（LGE）成像是心肌瘢痕评估的临床金标准，但标注数据稀缺严重制约了自动化分割方法的发展。

Method: 首先用隐式神经表示（INRs）分别建模LGE图像及对应的心肌/纤维化分割掩码的连续空间表征；再将INRs压缩为紧凑的潜在嵌入以保留解剖信息；最后在该潜在空间上训练扩散模型生成新表征，并解码为具有解剖一致性的合成LGE图像及配准分割掩码。

Result: 在133例真实心脏MRI扫描数据上实验表明，使用200例合成数据扩充训练集后，纤维化分割Dice分数由0.509提升至0.524。

Conclusion: 该方法提供了一种无需人工标注的合成数据生成方案，有效缓解LGE图像分割任务中的数据稀缺问题。

Abstract: Late gadolinium enhancement (LGE) imaging is the clinical standard for myocardial scar assessment, but limited annotated datasets hinder the development of automated segmentation methods. We propose a novel framework that synthesises both LGE images and their corresponding segmentation masks using implicit neural representations (INRs) combined with denoising diffusion models. Our approach first trains INRs to capture continuous spatial representations of LGE data and associated myocardium and fibrosis masks. These INRs are then compressed into compact latent embeddings, preserving essential anatomical information. A diffusion model operates on this latent space to generate new representations, which are decoded into synthetic LGE images with anatomically consistent segmentation masks. Experiments on 133 cardiac MRI scans suggest that augmenting training data with 200 synthetic volumes contributes to improved fibrosis segmentation performance, with the Dice score showing an increase from 0.509 to 0.524. Our approach provides an annotation-free method to help mitigate data scarcity.The code for this research is publicly available.

</details>


### [58] [Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion](https://arxiv.org/abs/2602.11960)
*Bruno Rigal,Victor Dupriez,Alexis Mignon,Ronan Le Hy,Nicolas Mery*

Main category: cs.CV

TL;DR: 本文提出了一种面向法语复杂文档的PDF转Markdown评测基准，采用模型分歧采样构建难例数据集，并设计了兼顾语义正确性与呈现无关性的评估方法，发现闭源VLM在手写体和表单上表现更鲁棒，而部分开源模型在标准印刷体上仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有PDF解析评测多聚焦英文或中文，且过度惩罚对下游RAG任务无影响的格式差异（如换行、列表分割、表格渲染方式），缺乏针对法语复杂文档（如手写表单、密集表格、图文混排）的鲁棒性评估。

Method: 构建法语专属基准：从6万份文档中通过模型分歧采样挑选难例；设计单元测试式评估：检查文本存在性、阅读顺序和局部表格约束，并结合类别特定归一化以忽略纯格式差异。

Result: 在15个VLM上评测发现：最强闭源模型在手写体和表单解析上鲁棒性显著更高；多个开源权重模型在标准印刷体页面上仍保持竞争力。

Conclusion: 评估方法需区分语义错误与呈现差异；法语复杂文档解析仍具挑战，模型选择应依文档类型而定；该基准可推动更实用、鲁棒的文档解析研究。

Abstract: This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.
  We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.

</details>


### [59] [Calibrated Bayesian Deep Learning for Explainable Decision Support Systems Based on Medical Imaging](https://arxiv.org/abs/2602.11973)
*Hua Xu,Julián D. Arias-Londoño,Juan I. Godino-Llorente*

Main category: cs.CV

TL;DR: 本文提出了一种基于贝叶斯深度学习的概率优化框架，包含新提出的CUB-Loss和双温度缩放（DTS）策略，以提升医学影像AI模型的不确定性校准能力，增强临床可信度。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI决策系统需兼顾预测准确性和不确定性校准可靠性；现有深度学习模型常存在误校准（如错误预测却高置信），阻碍临床采纳。

Method: 提出通用概率优化框架：1）训练阶段引入Confidence-Uncertainty Boundary Loss（CUB-Loss），惩罚高置信错误和低置信正确预测；2）推理阶段采用Dual Temperature Scaling（DTS）进行后处理校准，优化后验分布可解释性。

Result: 在肺炎筛查、糖尿病视网膜病变检测和皮肤病变识别三个医学影像任务上验证：显著且一致地提升校准性能，对小样本和严重类别不平衡数据鲁棒。

Conclusion: 该框架提升了AI输出不确定性与真实正确性的对齐程度，增强了临床可信赖性，具备实际部署潜力。

Abstract: In critical decision support systems based on medical imaging, the reliability of AI-assisted decision-making is as relevant as predictive accuracy. Although deep learning models have demonstrated significant accuracy, they frequently suffer from miscalibration, manifested as overconfidence in erroneous predictions. To facilitate clinical acceptance, it is imperative that models quantify uncertainty in a manner that correlates with prediction correctness, allowing clinicians to identify unreliable outputs for further review. In order to address this necessity, the present paper proposes a generalizable probabilistic optimization framework grounded in Bayesian deep learning. Specifically, a novel Confidence-Uncertainty Boundary Loss (CUB-Loss) is introduced that imposes penalties on high-certainty errors and low-certainty correct predictions, explicitly enforcing alignment between prediction correctness and uncertainty estimates. Complementing this training-time optimization, a Dual Temperature Scaling (DTS) strategy is devised for post-hoc calibration, further refining the posterior distribution to improve intuitive explainability. The proposed framework is validated on three distinct medical imaging tasks: automatic screening of pneumonia, diabetic retinopathy detection, and identification of skin lesions. Empirical results demonstrate that the proposed approach achieves consistent calibration improvements across diverse modalities, maintains robust performance in data-scarce scenarios, and remains effective on severely imbalanced datasets, underscoring its potential for real clinical deployment.

</details>


### [60] [Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation](https://arxiv.org/abs/2602.11980)
*Wei Chen,Yancheng Long,Mingqiao Liu,Haojie Ding,Yankai Yang,Hongyang Wei,Yi-Fan Zhang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为Spatial Chain-of-Thought (SCoT)的框架，通过将多模态大语言模型（MLLMs）的空间推理能力与扩散模型的生成能力结合，提升其空间理解与布局规划能力，无需联合训练且避免空间信息丢失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现出色，但在复杂空间理解与推理上存在不足；现有方法依赖MLLMs增强空间能力，但存在计算开销大或仅用文本提示导致空间信息丢失的问题。

Method: 提出SCoT框架：1）使用交错文本-坐标指令格式训练扩散模型以增强布局感知；2）利用先进MLLMs作为‘规划器’生成详细布局计划，并将其空间规划能力直接融入生成过程。

Result: 在图像生成基准测试中达到SOTA性能，在复杂空间推理任务上显著优于基线方法，并在图像编辑任务中也展现出强有效性。

Conclusion: SCoT是一种即插即用、高效且无需联合训练的空间增强框架，成功弥合了MLLMs的推理能力与扩散模型生成能力之间的鸿沟。

Abstract: While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model's layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.

</details>


### [61] [Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation](https://arxiv.org/abs/2602.12002)
*Enrico Guerriero,Kjersti Engan,Øyvind Meinich-Bache*

Main category: cs.CV

TL;DR: 本文探索了生成式AI（GenAI）方法，特别是本地视觉语言模型（VLM）结合大语言模型（LLM），在新生儿复苏视频中进行细粒度活动识别的效果，并通过LoRA微调显著提升了性能，超越了TimeSformer基线。


<details>
  <summary>Details</summary>
Motivation: 新生儿复苏过程的准确记录对质量改进和指南依从性至关重要，但实践中常被忽视；现有基于3D-CNN和ViT的方法虽有进展，但仍面临细粒度活动识别困难的问题。

Method: 采用模拟的13.26小时新生儿复苏视频数据集，对比评估多种零样本VLM策略与带分类头的微调VLM（含LoRA适配），并与监督式TimeSformer基线进行比较。

Result: 经LoRA微调的小型本地VLM达到F1分数0.91，显著优于TimeSformer的0.70；但零样本VLM存在明显幻觉问题。

Conclusion: 微调本地VLM（尤其是LoRA方式）是提升新生儿复苏视频细粒度活动识别性能的有效路径，生成式AI在该临床场景中具有实用潜力。

Abstract: Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.

</details>


### [62] [Projected Representation Conditioning for High-fidelity Novel View Synthesis](https://arxiv.org/abs/2602.12003)
*Min-Seop Kwak,Minkyung Kwon,Jinhyeok Choi,Jiho Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReNoV的新框架，利用外部视觉表征作为条件，通过专门的表征投影模块将其注入扩散过程，以提升新视角合成的几何一致性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在新视角合成中存在几何不一致问题，需借助外部表征增强几何与语义对应能力。

Method: 分析外部视觉表征空间注意力的对应能力，并设计表征引导的扩散新视角合成方法ReNoV，引入专用表征投影模块将外部表征注入扩散过程。

Result: 在标准基准上显著优于先前的基于扩散的新视角合成方法，在重建保真度和修复质量上均有提升，并支持从稀疏、无位姿图像集合中鲁棒合成。

Conclusion: 外部表征可有效提升扩散模型在新视角合成中的几何一致性与生成鲁棒性，ReNoV为该方向提供了新范式。

Abstract: We propose a novel framework for diffusion-based novel view synthesis in which we leverage external representations as conditions, harnessing their geometric and semantic correspondence properties for enhanced geometric consistency in generated novel viewpoints. First, we provide a detailed analysis exploring the correspondence capabilities emergent in the spatial attention of external visual representations. Building from these insights, we propose a representation-guided novel view synthesis through dedicated representation projection modules that inject external representations into the diffusion process, a methodology named ReNoV, short for representation-guided novel view synthesis. Our experiments show that this design yields marked improvements in both reconstruction fidelity and inpainting quality, outperforming prior diffusion-based novel-view methods on standard benchmarks and enabling robust synthesis from sparse, unposed image collections.

</details>


### [63] [A DMD-Based Adaptive Modulation Method for High Dynamic Range Imaging in High-Glare Environments](https://arxiv.org/abs/2602.12044)
*Banglei Guan,Jing Tao,Liang Xu,Dongcai Tan,Pengju Sun,Jianbing Liu,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于数字微镜器件（DMD）的高动态范围（HDR）成像系统，用于提升强眩光环境下光测力学测量（如焊接弧监测、金属表面分析）的图像质量和数字图像相关（DIC）精度。系统实现127 dB动态范围，显著降低应变误差（78%）并提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统CCD/CMOS传感器动态范围低（<70 dB），在强眩光下易饱和，导致DIC测量严重失真，亟需更高动态范围（>120 dB）的成像方案。

Method: 构建基于DMD的空间调制HDR成像系统，包含DMD光学调制单元与自适应计算成像流水线，支持自主区域分割与动态曝光控制。

Result: 实测动态范围达127 dB，消除饱和伪影；DIC应变误差降低78%，定位精度提升；在极端亮度变化下保持稳定性能。

Conclusion: 该DMD-HDR系统克服了传统传感器在高眩光场景下的关键局限，为光学计量与应力分析提供了高保真、自适应的新工具。

Abstract: Background The accuracy of photomechanics measurements critically relies on image quality,particularly under extreme illumination conditions such as welding arc monitoring and polished metallic surface analysis. High dynamic range (HDR) imaging above 120 dB is essential in these contexts. Conventional CCD/CMOS sensors, with dynamic ranges typically below 70 dB, are highly susceptible to saturation under glare, resulting in irreversible loss of detail and significant errors in digital image correlation (DIC). Methods This paper presents an HDR imaging system that leverages the spatial modulation capability of a digital micromirror device (DMD). The system architecture enables autonomous regional segmentation and adaptive exposure control for high-dynamic-range scenes through an integrated framework comprising two synergistic subsystems: a DMD-based optical modulation unit and an adaptive computational imaging pipeline. Results The system achieves a measurable dynamic range of 127 dB, effectively eliminating satu ration artifacts under high glare. Experimental results demonstrate a 78% reduction in strain error and improved DIC positioning accuracy, confirming reliable performance across extreme intensity variations. Conclusion The DMD-based system provides high fidelity adaptive HDR imaging, overcoming key limitations of conventional sensors. It exhibits strong potential for optical metrology and stress analysis in high-glare environments where traditional methods are inadequate.

</details>


### [64] [GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning](https://arxiv.org/abs/2602.12099)
*GigaBrain Team,Boyuan Wang,Chaojun Ni,Guan Huang,Guosheng Zhao,Hao Li,Jie Li,Jindi Lv,Jingyu Liu,Lv Feng,Mingming Yu,Peng Li,Qiuping Deng,Tianze Liu,Xinyu Zhou,Xinze Chen,Xiaofeng Wang,Yang Wang,Yifan Li,Yifei Nie,Yilong Li,Yukun Zhou,Yun Ye,Zhichao Liu,Zheng Zhu*

Main category: cs.CV

TL;DR: 本文提出GigaBrain-0.5M*，一种基于视频世界模型引导的强化学习方法（RAMP）训练的视觉-语言-动作（VLA）模型，显著提升多步操作任务性能与长时程执行鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型受限于场景理解能力弱和未来预测能力差；而视频世界模型具备强时空推理与未来预测能力，可作为增强VLA学习的理想基础。

Method: 在已预训练的机器人操作模型GigaBrain-0.5基础上，引入RAMP框架——即基于世界模型条件化策略的强化学习方法，实现跨任务自适应优化。

Result: 在Laundry Folding、Box Packing、Espresso Preparation等难任务上相较RECAP基线提升约30%；实测验证其长时程复杂操作成功率高、无失败。

Conclusion: 世界模型引导的强化学习能有效弥补VLA模型的时空理解短板，GigaBrain-0.5M*为具身智能提供了更鲁棒、可泛化的动作决策范式。

Abstract: Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\% on challenging tasks including \texttt{Laundry Folding}, \texttt{Box Packing}, and \texttt{Espresso Preparation}. Critically, \textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \href{https://gigabrain05m.github.io}{project page}.

</details>


### [65] [AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer](https://arxiv.org/abs/2602.12100)
*Lingting Zhu,Shengju Qian,Haidi Fan,Jiayu Dong,Zhenchao Jin,Siwei Zhou,Gen Dong,Xin Wang,Lequan Yu*

Main category: cs.CV

TL;DR: AssetFormer 是一个基于 Transformer 的自回归模型，用于根据文本描述生成符合设计约束的模块化 3D 资产，提升专业开发与用户生成内容（UGC）中的资产创建效率。


<details>
  <summary>Details</summary>
Motivation: 数字产业尤其是用户生成内容（UGC）领域亟需高质量、多样化的模块化3D资产，而现有方法难以兼顾设计约束与生成多样性。

Method: 提出 AssetFormer，一种基于 Transformer 的自回归模型；借鉴语言模型的序列建模与解码技术，对模块化3D资产（由基础几何体组成）进行序列化建模与生成。

Result: 初步实验表明 AssetFormer 能有效生成符合设计约束的高质量模块化3D资产，适用于专业开发和UGC场景。

Conclusion: AssetFormer 提供了一个灵活可扩展的框架，为模块化3D内容生成提供了新思路，并开源代码以促进后续研究。

Abstract: The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling. Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets, contributing to the broader field of 3D content generation. The code is available at https://github.com/Advocate99/AssetFormer.

</details>


### [66] [PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback](https://arxiv.org/abs/2602.12127)
*Sixiang Chen,Jianyu Lai,Jialin Gao,Hengyu Shi,Zhongying Liu,Tian Ye,Junfeng Luo,Xiaoming Wei,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出PosterOmni框架，统一处理图像到海报生成中的局部编辑与全局创作任务，通过数据构建、知识蒸馏与统一奖励反馈机制提升语义保真度与美学一致性，并建立PosterOmni-Bench基准评测其性能。


<details>
  <summary>Details</summary>
Motivation: 图像到海报生成需同时满足局部实体保持与全局设计概念理解，现有模型难以兼顾二者，缺乏统一框架与评测基准。

Method: 提出PosterOmni框架，包含三部分：(i) 构建覆盖六类任务的多场景图像-海报数据集；(ii) 在局部与全局专家模型间进行知识蒸馏以支持监督微调；(iii) 引入统一的PosterOmni Reward Feedback机制联合优化实体保持与美学偏好。同时建立PosterOmni-Bench评测基准。

Result: PosterOmni在参考遵循性、全局构图质量与美学协调性上显著优于所有开源基线，甚至超越多个商用系统。

Conclusion: PosterOmni成功耦合实体保持型编辑与概念驱动型创作，验证了统一多任务框架在艺术化图像生成中的有效性与泛化能力。

Abstract: Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence. The process spans two regimes: local editing, where ID-driven generation, rescaling, filling, and extending must preserve concrete visual entities; and global creation, where layout- and style-driven tasks rely on understanding abstract design concepts. These intertwined demands make image-to-poster a multi-dimensional process coupling entity-preserving editing with concept-driven creation under image-prompt control. To address these challenges, we propose PosterOmni, a generalized artistic poster creation framework that unlocks the potential of a base edit model for multi-task image-to-poster generation. PosterOmni integrates the two regimes, namely local editing and global creation, within a single system through an efficient data-distillation-reward pipeline: (i) constructing multi-scenario image-to-poster datasets covering six task types across entity-based and concept-based creation; (ii) distilling knowledge between local and global experts for supervised fine-tuning; and (iii) applying unified PosterOmni Reward Feedback to jointly align visual entity-preserving and aesthetic preference across all tasks. Additionally, we establish PosterOmni-Bench, a unified benchmark for evaluating both local editing and global creation. Extensive experiments show that PosterOmni significantly enhances reference adherence, global composition quality, and aesthetic harmony, outperforming all open-source baselines and even surpassing several proprietary systems.

</details>


### [67] [FAIL: Flow Matching Adversarial Imitation Learning for Image Generation](https://arxiv.org/abs/2602.12155)
*Yeyao Ma,Chen Li,Xiaosong Zhang,Han Hu,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出FAIL（Flow Matching Adversarial Imitation Learning），一种无需显式奖励或成对偏好的对抗式模仿学习方法，用于后训练流匹配模型；通过两种算法（FAIL-PD和FAIL-PG）实现低方差梯度或黑箱优化，在FLUX模型上仅用1.3万示范即达优性能，并可推广至离散图像/视频生成及抑制奖励欺骗。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调无法纠正未见状态下的策略漂移，偏好优化又依赖高成本的偏好对或奖励建模，亟需一种高效、免奖励、泛化强的流匹配模型后训练方法。

Method: 提出FAIL框架，基于对抗训练最小化策略与专家分布的散度；推导出两种算法：FAIL-PD利用可微ODE求解器获取低方差路径梯度，FAIL-PG提供适用于离散或计算受限场景的黑箱替代方案。

Result: 在FLUX模型上仅用13,000条Nano Banana pro示范，FAIL在提示遵循与美学评估中达到竞争性性能；成功泛化至离散图像/视频生成，并作为正则器有效缓解奖励欺骗问题。

Conclusion: FAIL为流匹配模型的后训练提供了免奖励、免偏好对、高泛化性的新范式，兼具理论严谨性与实际有效性，拓展了模仿学习在生成建模中的应用边界。

Abstract: Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.

</details>


### [68] [DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation](https://arxiv.org/abs/2602.12160)
*Xu Guo,Fulong Ye,Qichao Sun,Liyang Chen,Bingchuan Li,Pengze Zhang,Jiawei Liu,Songtao Zhao,Qian He,Xiangwang Hou*

Main category: cs.CV

TL;DR: 本文提出DreamID-Omni，一个统一的可控人像音视频生成框架，通过引入对称条件扩散Transformer、双层级解耦策略（同步RoPE与结构化描述）以及多任务渐进训练方案，有效解决多角色身份与音色解耦难题，在多项指标上达到SOTA，并超越主流商用模型。


<details>
  <summary>Details</summary>
Motivation: 现有音视频生成方法将人像相关任务（如参考式生成、视频编辑、语音驱动动画）孤立处理，且难以在单框架中实现多角色身份与语音音色的精确、解耦控制。

Method: 提出DreamID-Omni框架，包含：1）对称条件扩散Transformer，采用对称条件注入融合异构控制信号；2）双层级解耦策略——信号层用同步RoPE保障注意力空间绑定，语义层用结构化描述建立属性-主体显式映射；3）多任务渐进训练，利用弱约束生成先验正则化强约束任务。

Result: 在视频质量、音频质量及音视频一致性等多维度全面达到SOTA，性能超越当前领先商用模型。

Conclusion: DreamID-Omni实现了人像音视频生成任务的统一建模与细粒度可控性，显著提升了多角色场景下的身份与音色解耦能力，推动学术研究向商用级应用迈进。

Abstract: Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

</details>


### [69] [EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data](https://arxiv.org/abs/2602.12177)
*Nils Lehmann,Yi Wang,Zhitong Xiong,Xiaoxiang Zhu*

Main category: cs.CV

TL;DR: 本文提出EO-VAE，一种面向地球观测（EO）多传感器数据的统一变分自编码器 tokenizer，利用动态超网络支持灵活光谱通道组合的编码与重建，在TerraMesh数据集上优于现有专用tokenizer。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖tokenizer压缩输入，但地球观测数据因传感器多样、光谱通道可变，难以用单一或固定通道tokenizer有效处理。

Method: 提出EO-VAE——基于变分自编码器的多传感器tokenizer，引入动态超网络，使单个模型能适应不同通道数和光谱配置的输入。

Result: 在TerraMesh数据集上，EO-VAE的重建保真度显著优于TerraMind tokenizer。

Conclusion: EO-VAE为遥感领域的潜在生成建模提供了鲁棒、统一的基础tokenizer方案，推动多模态EO数据的通用表征学习。

Abstract: State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.

</details>


### [70] [DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing](https://arxiv.org/abs/2602.12205)
*Dianyi Wang,Ruihang Li,Feng Han,Chaofan Ma,Wei Song,Siyuan Wang,Yibin Wang,Yi Xin,Hongjian Liu,Zhixiong Zhang,Shengyuan Ding,Tianhang Wang,Zhenglin Cheng,Tao Lin,Cheng Jin,Kaicheng Yu,Jingjing Chen,Wenjie Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: DeepGen 1.0 是一个仅5B参数的轻量级统一多模态模型，在图像生成与编辑任务上性能超越更大规模模型（如80B HunyuanImage、27B Qwen-Image-Edit），其核心创新包括堆叠通道桥接（SCB）对齐框架和三阶段数据驱动训练策略（对齐预训练、联合监督微调、MR-GRPO强化学习）。


<details>
  <summary>Details</summary>
Motivation: 解决当前统一多模态图像生成与编辑模型参数规模过大（>10B）、训练与部署成本高昂的问题，探索轻量级模型能否实现高性能与高效率兼顾。

Method: 提出Stacked Channel Bridging（SCB）深度对齐框架，融合多层视觉语言模型（VLM）特征与可学习‘think tokens’；设计三阶段训练策略：（1）基于图像-文本对与编辑三元组的对齐预训练，（2）面向生成、编辑与推理任务的联合监督微调，（3）采用混合奖励函数与监督信号的MR-GRPO强化学习。

Result: 在仅约50M样本上训练的DeepGen 1.0，在WISE基准上比80B HunyuanImage提升28%，在UniREditBench上比27B Qwen-Image-Edit提升37%，显著优于更大模型；同时避免视觉伪影，训练稳定。

Conclusion: 证明轻量级统一多模态模型通过结构创新与高效训练策略，可在性能、效率与可扩展性间取得更好平衡，开源代码、权重与数据集以推动社区发展。

Abstract: Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.

</details>


### [71] [Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching](https://arxiv.org/abs/2602.12221)
*Onkar Susladkar,Tushar Prakash,Gayatri Deshmukh,Kiet A. Nguyen,Jiaxun Zhang,Adheesh Juvekar,Tianshu Bao,Lin Chai,Sparsh Mittal,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: UniDFlow is a unified discrete flow-matching framework for multimodal tasks, decoupling understanding and generation via adapters and using reference-based preference alignment for improved faithfulness and controllability, achieving SOTA across eight benchmarks and strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To address objective interference and representation entanglement in multimodal models, and to improve faithfulness and controllability without large-scale retraining.

Method: UniDFlow uses task-specific low-rank adapters to decouple understanding and generation, and introduces reference-based multimodal preference alignment to optimize relative outcomes under identical conditioning.

Result: Achieves state-of-the-art performance across eight benchmarks and shows strong zero-shot generalization to inpainting, in-context image generation, reference-based editing, and compositional generation.

Conclusion: UniDFlow provides a flexible, unified framework for multimodal understanding, generation, and editing with minimal task-specific training and superior generalization.

Abstract: We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.

</details>


### [72] [MonarchRT: Efficient Attention for Real-Time Video Generation](https://arxiv.org/abs/2602.12271)
*Krish Agarwal,Zhuoming Chen,Cheng Luo,Yongqi Chen,Haizhong Zheng,Xun Huang,Atri Rudra,Beidi Chen*

Main category: cs.CV

TL;DR: 本文提出Monarch-RT，一种基于Monarch矩阵分解的结构化注意力参数化方法，用于提升实时视频扩散模型（尤其是自回归、少步生成场景）的效率与表现，在保持高质量的同时实现高达95%的注意力稀疏性，并在多款GPU上显著超越FlashAttention系列内核性能。


<details>
  <summary>Details</summary>
Motivation: 3D自注意力在实时视频扩散中计算成本高（二次复杂度），尤其在少步、自回归设定下误差累积严重，且现有稀疏注意力方法（如top-k）在该场景下失效，因其无法建模视频注意力中周期性位置结构、动态语义对应与密集混合共存的复杂模式。

Method: 提出Monarch-RT：利用Monarch矩阵对注意力权重进行结构化分解，结合对齐的分块设计与扩展的平铺Monarch参数化；通过微调+定制Triton内核消除参数化开销；适配于Self-Forcing等实时视频生成模型。

Result: 在Self-Forcing模型上实现95%注意力稀疏性且无质量损失；自研内核在RTX 5090/H100/B200上分别比FlashAttention-2/3/4快1.4–11.8倍；首次在单张RTX 5090上实现16 FPS真实时视频生成。

Conclusion: Monarch-RT是首个面向实时视频生成的高性能稀疏注意力参数化方案，突破了传统稀疏注意力在自回归少步扩散中的局限，兼顾高表达力与高效计算。

Abstract: Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.

</details>


### [73] [UniT: Unified Multimodal Chain-of-Thought Test-time Scaling](https://arxiv.org/abs/2602.12279)
*Leon Liangyu Chen,Haoyu Ma,Zhipeng Fan,Ziqi Huang,Animesh Sinha,Xiaoliang Dai,Jialiang Wang,Zecheng He,Jianwei Yang,Chunyuan Li,Junzhe Sun,Chu Wang,Serena Yeung-Levy,Felix Juefei-Xu*

Main category: cs.CV

TL;DR: 本文提出UniT框架，实现统一多模态模型的链式思维测试时扩展（TTS），支持多轮推理、验证与修正，提升复杂多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型通常单次前向推理，难以应对需分解指令、验证中间结果和迭代修正的复杂任务（如空间组合、多对象交互、动态指令）；而语言模型中已验证有效的测试时扩展（TTS）尚未成功迁移到统一多模态模型。

Method: 提出UniT框架，融合智能体式数据合成、统一模型训练与灵活测试时推理，支持验证、子目标分解与内容记忆等认知行为；训练使用短推理轨迹，测试时泛化至长链推理；对比串行链式推理与并行采样，并引入生成与编辑轨迹联合训练。

Result: （1）基于短推理轨迹训练的统一模型可在测试时泛化至更长推理链；（2）串行链式思维推理比并行采样更具可扩展性与计算效率；（3）生成+编辑轨迹训练显著提升分布外视觉推理能力。

Conclusion: 多模态测试时扩展是提升统一模型生成与理解能力的有效新范式。

Abstract: Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.

</details>


### [74] [Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching](https://arxiv.org/abs/2602.12280)
*Huai-Hsun Cheng,Siang-Ling Zhang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新型矢量素描任务——渐进语义错觉，通过逐步添加笔画使单个草图发生显著语义转变，并设计了Stroke of Surprise生成框架，利用双分支Score Distillation Sampling和Overlay Loss实现兼顾前后语义一致与结构融合的优化。


<details>
  <summary>Details</summary>
Motivation: 传统视觉错觉依赖空间操作（如多视角一致性），而本文旨在探索时间维度上的语义错觉，即在单一线条序列中实现从一个语义对象到另一个语义对象的自然、可识别的渐进转变。

Method: 提出Stroke of Surprise框架，采用序列感知的联合优化方法，核心是双分支Score Distillation Sampling（SDS）机制，动态调整前缀笔画以寻找两个目标概念的‘公共结构子空间’；并引入Overlay Loss强制空间互补性，避免遮挡、促进结构融合。

Result: 实验表明该方法在可识别性和错觉强度上显著优于现有最先进基线，成功将视觉回文（visual anagrams）从空间域拓展至时间域。

Conclusion: 渐进语义错觉是一种新颖且可行的视觉生成范式，其关键在于建模笔画序列的双重语义约束与结构共享，为可控创意生成和人机协同绘图提供了新思路。

Abstract: Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [75] [HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents](https://arxiv.org/abs/2602.11156)
*Sungmoon Kim,Hyuna Jeon,Dahye Kim,Mingyu Kim,Dong-Kyu Chae,Jiwoong Kim*

Main category: cs.CL

TL;DR: HybridRAG 是一种新型检索增强生成（RAG）框架，通过预构建问答知识库并结合实时检索与生成，在处理非结构化PDF文档时提升聊天机器人响应质量与速度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖结构化文本且仅在查询时进行检索与生成，难以应对真实场景中大量非结构化PDF文档（含复杂版式）及高并发、低资源限制的挑战。

Method: HybridRAG 先用OCR和布局分析解析原始PDF，生成分层文本块；再利用LLM预生成问答知识库；查询时优先匹配该QA库以实现即时回答，仅在无匹配时启用实时RAG生成。

Result: 在OHRBench上实验表明，HybridRAG相比标准RAG基线具有更高答案质量与更低延迟。

Conclusion: HybridRAG是一种面向实际聊天机器人应用的高效、可扩展RAG方案，尤其适用于处理海量非结构化文档与资源受限环境。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.

</details>


### [76] [Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety](https://arxiv.org/abs/2602.11157)
*Max Zhang,Derek Liu,Kai Zhang,Joshua Franco,Haihao Liu*

Main category: cs.CL

TL;DR: 本文探讨了知识蒸馏在多语言越狱防护中的应用，发现标准微调反而会增加越狱成功率，而去除细粒度边界拒绝可缓解安全退化，但推理能力仍下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐目前以英语为中心，导致低资源语言场景存在安全隐患，亟需多语言安全对齐方法。

Method: 采用基于黑盒响应的知识蒸馏方法，利用XSafety的约2.8万个多种语言越狱提示，通过LoRA对三个开源模型进行参数高效微调，蒸馏OpenAI o1-mini的拒绝行为。

Result: 标准‘安全’拒绝数据微调反而使所有学生模型的越狱成功率（JSR）上升最多达16.6个百分点；去除‘边界性’拒绝可缓解或逆转安全退化，但GSM8K推理性能仍下降。

Conclusion: 知识蒸馏在多语言安全对齐中具有潜力但也面临挑战，需谨慎设计拒绝行为建模，为后续研究提供基础。

Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher's ``safe'' refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced `boundary' refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.

</details>


### [77] [Retrieval Heads are Dynamic](https://arxiv.org/abs/2602.11162)
*Yuping Lin,Zitao Li,Yue Xing,Pengfei He,Yingqian Cui,Yaliang Li,Bolin Ding,Jingren Zhou,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文从动态视角研究大语言模型（LLM）中的检索头，发现其在自回归生成过程中随时间步动态变化、不可被静态头替代，且隐藏状态可预测未来检索模式，揭示了模型内部的规划机制。


<details>
  <summary>Details</summary>
Motivation: 现有工作多依赖跨数据集的静态统计识别检索头，忽略了自回归生成中细粒度的时间动态性。

Method: 通过大量分析，在Needle-in-a-Haystack和多跳问答任务上验证动态检索头的时序变化性、不可替代性及与隐藏状态的预测相关性，并在动态检索增强生成框架中量化其效用差异。

Result: 证实了检索头具有时序动态性、各时刻不可替代性，且模型隐藏状态蕴含对未来检索头模式的预测信号。

Conclusion: LLM中的检索行为是动态、时序敏感且具备内部规划能力的，挑战了静态分析范式，为理解其内部机制提供了新视角。

Abstract: Recent studies have identified "retrieval heads" in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that perform retrieval on average. This perspective overlooks the fine-grained temporal dynamics of autoregressive generation. In this paper, we investigate retrieval heads from a dynamic perspective. Through extensive analysis, we establish three core claims: (1) Dynamism: Retrieval heads vary dynamically across timesteps; (2) Irreplaceability: Dynamic retrieval heads are specific at each timestep and cannot be effectively replaced by static retrieval heads; and (3) Correlation: The model's hidden state encodes a predictive signal for future retrieval head patterns, indicating an internal planning mechanism. We validate these findings on the Needle-in-a-Haystack task and a multi-hop QA task, and quantify the differences on the utility of dynamic and static retrieval heads in a Dynamic Retrieval-Augmented Generation framework. Our study provides new insights into the internal mechanisms of LLMs.

</details>


### [78] [Nested Named Entity Recognition in Plasma Physics Research Articles](https://arxiv.org/abs/2602.11163)
*Muhammad Haris,Hans Höft,Markus M. Becker,Markus Stocker*

Main category: cs.CL

TL;DR: 本文提出了一种基于编码器-Transformer与条件随机场（CRF）的轻量级嵌套命名实体识别（NER）方法，专用于等离子体物理研究文献，通过构建16类标注语料、实体特化建模及超参数优化，提升了领域内专业实体抽取效果。


<details>
  <summary>Details</summary>
Motivation: 等离子体物理研究论文内容高度复杂且上下文丰富，需有效提取专业实体以支持高级检索等应用，但现有通用NER方法难以应对该领域嵌套、专业化实体识别挑战。

Method: 构建包含16个嵌套实体类别的等离子体物理语料库；采用多个独立的BERT-CRF模型分别识别各类实体（实体特化建模）；引入系统化超参数优化流程提升模型性能。

Result: 在等离子体物理文本上实现了有效的嵌套命名实体识别，验证了轻量级encoder-transformer+CRF架构及实体特化策略的有效性，并为科学文献分析提供了可复用的技术基础。

Conclusion: 该工作推动了领域专用NER技术的发展，不仅提升了等离子体物理文献中专业实体的识别能力，也为科研人员高效导航和分析科学文献提供了实用支撑。

Abstract: Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challenges of extracting specialized entities from scientific text in this domain. Research articles in plasma physics often contain highly complex and context-rich content that must be extracted to enable, e.g., advanced search. We propose a lightweight approach based on encoder-transformers and conditional random fields to extract (nested) named entities from plasma physics research articles. First, we annotate a plasma physics corpus with 16 classes specifically designed for the nested NER task. Second, we evaluate an entity-specific model specialization approach, where independent BERT-CRF models are trained to recognize individual entity types in plasma physics text. Third, we integrate an optimization process to systematically fine-tune hyperparameters and enhance model performance. Our work contributes to the advancement of entity recognition in plasma physics and also provides a foundation to support researchers in navigating and analyzing scientific literature.

</details>


### [79] [Assessing LLM Reliability on Temporally Recent Open-Domain Questions](https://arxiv.org/abs/2602.11165)
*Pushwitha Krishnappa,Amit Das,Vinija Jain,Tathagata Mukherjee,Aman Chadha*

Main category: cs.CL

TL;DR: 本文提出RECOM基准数据集，评估大语言模型对近期Reddit问题的回答与人类答案的对齐程度，发现语义相似性高但词汇重叠极低的悖论，表明模型通过大量改写保持语义一致性，并指出词法指标不可靠，需多维评估框架。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在开放域问答中对时间上最新信息与人类观点的一致性，现有研究对此关注不足。

Method: 构建包含15000条2025年9月Reddit问题及社区参考答案的RECOM数据集；使用BLEU、ROUGE、BERTScore、MoverScore、余弦相似度和NLI等多维指标评估四个开源LLM（Llama3.1-8B、Mistral-7B、Gemma-2-9B、GPT-OSS-20B）的回答质量。

Result: 所有模型余弦相似度>99%，但BLEU-1<8%，呈现显著语义-词法悖论；MoverScore居中（51–53%）；模型参数量不决定性能（Mistral-7B优于GPT-OSS-20B）；NLI显示矛盾率<7%。

Conclusion: 词法指标（如BLEU）不足以可靠评估抽象生成质量；应采用融合语义、逻辑与表层特征的多维评估框架；RECOM数据集已开源。

Abstract: Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of Models), a benchmark dataset of 15,000 recent Reddit questions from September 2025 paired with community-derived reference answers. We investigate how four open-source LLMs (Llama3.1-8B, Mistral-7B, Gemma-2-9B, and GPT-OSS-20B) respond to these questions, evaluating alignment using lexical metrics (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore, cosine similarity), and logical inference (NLI). Our central finding is a striking semantic-lexical paradox: all models achieve over 99% cosine similarity with references despite less than 8% BLEU-1 overlap, a 90+ percentage point gap indicating that models preserve meaning through extensive paraphrasing rather than lexical reproduction. MoverScore (51-53%) confirms this pattern, occupying an intermediate position that reflects the optimal transport cost of semantic alignment. Furthermore, model scale does not predict performance: Mistral-7B (7B parameters) outperforms GPT-OSS-20B (20B parameters) across all metrics. NLI analysis reveals that contradiction rates remain below 7%, suggesting models rarely generate content that directly conflicts with human consensus. These findings challenge the reliability of lexical metrics for evaluating abstractive generation and argue for multi-dimensional evaluation frameworks that capture semantic fidelity beyond surface-level text matching. The RECOM dataset is publicly available at https://anonymous.4open.science/r/recom-D4B0

</details>


### [80] [Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?](https://arxiv.org/abs/2602.11166)
*Xu Hu,Yifan Zhang,Songtao Wei,Chen Zhao,Qiannan Li,Bingzhe Li,Feng Chen*

Main category: cs.CL

TL;DR: 本文系统研究了参数高效微调（PEFT）方法对大语言模型幻觉检测能力的影响，发现PEFT能显著提升多种无监督幻觉检测器的AUROC性能，其作用机制主要是重构模型中不确定性的编码与表征方式，而非注入新知识。


<details>
  <summary>Details</summary>
Motivation: 尽管参数高效微调（PEFT）被广泛用于适配大语言模型并常被认为可提升事实正确性，但其对幻觉行为（尤其在问答任务中）的影响尚不明确。

Method: 在三个开源大语言模型主干和三个事实导向的问答数据集上，对七种覆盖语义一致性、置信度和熵三类范式的无监督幻觉检测方法进行系统评估；辅以线性探针和表征诊断分析PEFT影响机制。

Result: PEFT一致增强了幻觉检测能力，在各类检测器上显著提升AUROC；进一步分析表明PEFT主要重塑不确定性在模型中的编码与呈现方式，而非注入新事实知识。

Conclusion: PEFT不仅有助于下游任务适配，还能通过优化不确定性建模来增强幻觉检测鲁棒性，为理解其内在机制提供了新视角。

Abstract: Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucination behavior remains insufficiently understood, especially on QA datasets. In this work, we systematically investigate the impact of PEFT on hallucination detection through a comprehensive empirical study across three open-weight LLM backbones and three fact-seeking QA benchmarks. For each model, we evaluate performance using seven unsupervised hallucination detection methods spanning three complementary approaches: semantic consistency based detectors, confidence based detectors, and entropy based detectors. This multifaceted evaluation enables us to characterize how PEFT reshapes uncertainty across different detection paradigms. In conclusion, our experimental results show that PEFT consistently strengthens hallucination detection ability, substantially improving AUROC across a wide range of hallucination detectors. Besides, further analyses using linear probes and representation diagnostics indicate that PEFT methods primarily reshapes how uncertainty is encoded and surfaced, comparing with injecting new factual knowledge into the models.

</details>


### [81] [Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering](https://arxiv.org/abs/2602.11167)
*Nathan Mao,Varun Kaushik,Shreya Shivkumar,Parham Sharafoleslami,Kevin Zhu,Sunishchal Dev*

Main category: cs.CL

TL;DR: 本文提出了FalseCite数据集，用于系统研究大语言模型（LLM）因误导性或伪造引用而产生的幻觉现象，并通过多模型实验与隐藏状态分析揭示了幻觉的内在模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常产生幻觉（即生成错误或无意义信息），尤其在医学、法律等敏感领域危害严重；现有研究缺乏针对引用诱导型幻觉的系统性评测基准。

Method: 构建了专门用于诱发和评测引用相关幻觉的基准数据集FalseCite；在GPT-4o-mini、Falcon-7B和Mistral-7B上进行测试；并对模型隐藏状态向量进行可视化与聚类分析。

Result: 发现虚假引用显著提升模型幻觉率（尤以GPT-4o-mini为甚）；幻觉与非幻觉样本的隐藏状态均呈现相似的‘角状’（horn-like）分布结构。

Conclusion: FalseCite为系统评估与缓解LLM幻觉提供了新工具和新视角，隐藏状态的共性结构暗示幻觉可能源于模型内部表征的固有特性。

Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite's potential as a foundation for evaluating and mitigating hallucinations in future LLM research.

</details>


### [82] [Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI](https://arxiv.org/abs/2602.11168)
*Jingyan Xu,Marcelo L. LaFleur,Christina Schweikert,D. Frank Hsu*

Main category: cs.CL

TL;DR: 本文提出了一种结合多模型与人类专家知识的文本分类方法，用于联合国可持续发展目标（SDGs）文本分类任务；通过生成式AI扩充数据，并利用组合融合分析（CFA）融合多个模型，达到96.73%准确率，超越单个最佳模型，并验证了模型融合与人类专家协同的有效性。


<details>
  <summary>Details</summary>
Motivation: SDG相关文本分类面临类别模糊、互相关联、标注困难等挑战，传统NLP方法效果受限，亟需提升分类精度与可解释性。

Method: 采用生成式AI构建合成训练数据，训练多个基础分类模型；利用组合融合分析（CFA），基于秩-得分特征（RSC）函数和认知多样性（CD）融合模型输出；同时引入人类领域专家判断进行对比与协同分析。

Result: CFA融合方法在SDG文本分类任务中达到96.73%性能，显著优于单个最佳模型；与人类专家结果对比表明，模型融合与专家知识可互补并相互增强。

Conclusion: 多模型融合（特别是CFA）结合生成式AI数据增强与人类专家输入，是提升复杂、模糊文本分类任务性能与可信度的有效范式，尤其适用于政策导向型社会分析场景。

Abstract: (Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a challenging problem in cases where the categories are unavailable, difficult to differentiate, or are interrelated. Social analysis with human context is an area that can benefit from text classification, as it relies substantially on text data. The focus of this paper is to enhance the classification of text according to the UN's Sustainable Development Goals (SDGs) by collecting and combining intelligence from multiple models. Combinatorial Fusion Analysis (CFA), a system fusion paradigm using a rank-score characteristic (RSC) function and cognitive diversity (CD), has been used to enhance classifier methods by combining a set of relatively good and mutually diverse classification models. We use a generative AI model to generate synthetic data for model training and then apply CFA to this classification task. The CFA technique achieves 96.73% performance, outperforming the best individual model. We compare the outcomes with those obtained from human domain experts. It is demonstrated that combining intelligence from multiple ML/AI models using CFA and getting input from human experts can, not only complement, but also enhance each other.

</details>


### [83] [Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis](https://arxiv.org/abs/2602.11169)
*Mangadoddi Srikar Vardhan,Lekkala Sai Teja*

Main category: cs.CL

TL;DR: 本文发现Transformer隐藏状态的方向（角度）和模长（大小）在语言建模和句法处理中承担不同功能角色：方向扰动主要损害语言建模损失，而模长扰动更显著影响句法准确性；该分离现象在LayerNorm架构中稳健存在，但在RMSNorm中不同，揭示了归一化方式对表征功能分工的关键影响。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer隐藏状态中方向（向量角度）与模长（向量范数）是否具有不同的计算功能，挑战线性表征假设并深化对神经网络内部表征机制的理解。

Method: 在Pythia系列模型上采用L2匹配扰动分析法，分别施加角度与模长扰动以保证欧氏距离一致，并结合因果干预（如修复注意力或LayerNorm路径）定位损伤传播路径；同时对比LayerNorm与RMSNorm架构的差异。

Result: 角度扰动使语言建模损失增加最多42.9倍，模长扰动导致主谓一致准确率下降20.4%（远高于角度扰动的1.6%）；角度损伤主要经注意力路径传导（修复后损失降低28.4%），模长损伤部分经LayerNorm路径传导（修复后恢复29.9%）；该分离现象在Pythia各尺度模型中可复现，但在RMSNorm模型中不显著。

Conclusion: 在LayerNorm架构中，隐藏状态的方向主导注意力路由功能，模长则调节细粒度句法判断的处理强度；该功能分离依赖于归一化机制设计，提示模型编辑与可解释性研究需区分方向与模长的语义角色。

Abstract: Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we discover a striking cross-over dissociation: angular perturbations cause up to 42.9 more damage to language modeling loss, while magnitude perturbations cause disproportionately more damage to syntactic processing (20.4% vs.1.6% accuracy drop on subject-verb agreement).This finding is enabled by L2-matched perturbation analysis, a methodology ensuring that an gular and magnitude perturbations achieve identical Euclidean displacements. Causal intervention reveals that angular damage flows substantially through the attention pathways (28.4% loss recovery via attention repair), while magnitude damage flows partly through the LayerNorm pathways(29.9% recovery via LayerNorm repair). These patterns replicate across scales within the Pythia architecture family. These findings provide evidence that direction and magnitude support partially distinct computational roles in LayerNorm based architectures. The direction preferentially affects attentional routing, while magnitude modulates processing intensity for fine-grained syntactic judgments. We find different patterns in RMSNorm-based architectures, suggesting that the dissociation depends on architectural choices. Our results refine the linear representation hypothesis and have implications for model editing and interpretability research

</details>


### [84] [PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models](https://arxiv.org/abs/2602.11170)
*Jiawei Xu,Zhenyu Yu,Ziqian Bi,Minh Duc Pham,Xiaoyi Qu,Danyang Zhang*

Main category: cs.CL

TL;DR: 本文提出PRIME框架，通过三个专业化智能体（执行器、验证器、协调器）协同完成算法推理任务，并引入大规模基准PRIME-Bench进行评估，显著提升算法推理准确率，尤其在需持续状态跟踪的任务上效果突出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在算法推理任务上表现有限，亟需一种能有效处理复杂逻辑、约束检查与错误回溯的推理框架。

Method: 提出PRIME框架，包含执行器（逐步推理）、验证器（约束检查）和协调器（回溯控制），采用组相对策略优化；同时构建PRIME-Bench基准（86个任务、51600个实例）。

Result: PRIME将平均准确率从26.8%提升至93.8%，相对提升250%；图灵机模拟从9%升至92%，长除法从16%升至94%；消融实验表明迭代验证是关键；小模型（8B）性能接近大模型（120B）的八分之一。

Conclusion: PRIME通过多智能体协同与迭代验证机制，显著增强大语言模型在算法推理上的鲁棒性与准确性，尤其缓解小模型在复杂推理中的性能瓶颈。

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.

</details>


### [85] [Efficient Hyper-Parameter Search for LoRA via Language-aided Bayesian Optimization](https://arxiv.org/abs/2602.11171)
*Baek Seong-Eun,Lee Jung-Mok,Kim Sung-Bin,Tae-Hyun Oh*

Main category: cs.CL

TL;DR: 本文提出了一种将大语言模型（LLM）的领域知识融入贝叶斯优化（BO）的新框架，用于高效搜索LoRA微调的超参数；通过语言提示引导LLM构建超参数到连续向量空间的映射，并引入可学习token补充难以描述的残差信息，结合子集代理训练进一步提升效率。


<details>
  <summary>Details</summary>
Motivation: LoRA微调虽高效但对超参数敏感，传统穷搜计算开销大，亟需结合领域知识的高效超参优化方法。

Method: 将预训练LLM作为离散超参数到连续向量空间的映射器，通过领域感知文本提示注入LoRA知识，并添加可学习token建模残差信息；同时利用全量与子集数据性能强相关性，采用子集代理训练/评估加速BO过程。

Result: 仅用约30次迭代找到的超参数，性能比从约45,000种组合中选出的标准超参数提升超20%。

Conclusion: 融合LLM领域知识与贝叶斯优化、辅以代理子集评估的框架，显著提升了LoRA超参数搜索的效率与效果。

Abstract: Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enables resource-efficient personalization or specialization, but it comes at the expense of additional hyperparameter tuning. Although LoRA makes fine-tuning efficient, it is highly sensitive to the choice of hyperparameters, and exhaustive hyperparameter search is still computationally very demanding. To address these challenges, we propose a framework that integrates the domain knowledge of pre-trained LLMs into Bayesian Optimization (BO) to efficiently search for LoRA hyperparameters. To leverage the informed knowledge of LLMs, we repurpose LLMs as a discrete-to-continuous mapping to link the hyperparameters and their domain knowledge with a continuous vector space, where BO is conducted. We design and control the mapping by language prompting, where we provide a domain-aware textual prompt describing the relationships among hyperparameters and their respective roles; thereby, we explicitly inject domain knowledge about LoRA into the LLM in natural language. Also, we model the residual information that is hard to linguistically describe in the prompt with an additional learnable token. This aids BO to sample more high-performing hyperparameters. In addition, by leveraging the observation of the strong correlation between the respective performance obtained from full and subset training datasets in LoRA training regimes, we introduce proxy training and evaluation with a data subset. This further increases the efficiency of our method. We demonstrate that our hyperparameter found with only about 30 iterations achieves more than 20% performance improvement over standard hyperparameters found from about 45,000 combinations.

</details>


### [86] [Synthesizing the Virtual Advocate: A Multi-Persona Speech Generation Framework for Diverse Linguistic Jurisdictions in Indic Languages](https://arxiv.org/abs/2602.11172)
*Aniket Deroy*

Main category: cs.CL

TL;DR: 本研究评估了 Gemini 2.5 Flash 和 Pro TTS 模型在五种印度语言（泰米尔语、泰卢固语、孟加拉语、印地语、古吉拉特语）中生成法庭演说的表现，发现其虽能准确传递程序性信息，但缺乏说服性倡导所需的动态语调与情感分量，尤其在孟加拉语和古吉拉特语中表现更弱。


<details>
  <summary>Details</summary>
Motivation: 法律辩护需兼具权威语气、节奏性停顿与情感智能；而印度多语言环境下，现有TTS难以复现人类律师的富有表现力的语音艺术。

Method: 提出一种提示框架，利用 Gemini 2.5 原生支持的五种语言能力及其上下文感知语速控制，构建不同律师人设，并评估其在法庭语音合成中的表现。

Result: 模型表现出“单调权威”特征：擅长程序性内容传达，但在动态语调、情感庄重感方面明显不足；孟加拉语和古吉拉特语性能最弱，揭示了音系建模的前沿挑战。

Conclusion: 多语言TTS已初步适用于程序性法律任务，但尚无法替代人类律师在说服性表达上的艺术性，需在语音表现力与语言特异性建模上进一步突破。

Abstract: Legal advocacy requires a unique combination of authoritative tone, rhythmic pausing for emphasis, and emotional intelligence. This study investigates the performance of the Gemini 2.5 Flash TTS and Gemini 2.5 Pro TTS models in generating synthetic courtroom speeches across five Indic languages: Tamil, Telugu, Bengali, Hindi, and Gujarati. We propose a prompting framework that utilizes Gemini 2.5s native support for 5 languages and its context-aware pacing to produce distinct advocate personas. The evolution of Large Language Models (LLMs) has shifted the focus of TexttoSpeech (TTS) technology from basic intelligibility to context-aware, expressive synthesis. In the legal domain, synthetic speech must convey authority and a specific professional persona a task that becomes significantly more complex in the linguistically diverse landscape of India. The models exhibit a "monotone authority," excelling at procedural information delivery but struggling with the dynamic vocal modulation and emotive gravitas required for persuasive advocacy. Performance dips in Bengali and Gujarati further highlight phonological frontiers for future refinement. This research underscores the readiness of multilingual TTS for procedural legal tasks while identifying the remaining challenges in replicating the persuasive artistry of human legal discourse. The code is available at-https://github.com/naturenurtureelite/Synthesizing-the-Virtual-Advocate/tree/main

</details>


### [87] [Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review](https://arxiv.org/abs/2602.11173)
*Qian Ruan,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种作者参与式的作者回应生成框架REspGen，结合作者输入、多属性控制与评估引导优化，并构建了首个评审-回应-修改三元组数据集Re³Align，配套评估工具REspEval。


<details>
  <summary>Details</summary>
Motivation: 现有自动作者回应生成方法忽视了作者的领域专长、独有信息及修改策略等关键意图信号，亟需将作者纳入生成闭环。

Method: 提出作者-in-the-loop的回应生成范式；设计REspGen框架（含显式作者输入接口、多属性可控生成、评估引导的迭代优化）；构建Re³Align三元组数据集；开发覆盖输入利用、可控性、质量与话语的REspEval评估套件。

Result: 实验表明作者输入和评估引导显著提升回应质量；输入设计对效果影响显著；可控性与质量存在权衡。

Conclusion: 作者专业知识与意图应被显式建模并融入生成流程，作者-in-the-loop范式更符合真实评审场景，为NLP辅助科研写作提供了新方向。

Abstract: Author response (rebuttal) writing is a critical stage of scientific peer review that demands substantial author effort. Recent work frames this task as automatic text generation, underusing author expertise and intent. In practice, authors possess domain expertise, author-only information, revision and response strategies--concrete forms of author expertise and intent--to address reviewer concerns, and seek NLP assistance that integrates these signals to support effective response writing in peer review. We reformulate author response generation as an author-in-the-loop task and introduce REspGen, a generation framework that integrates explicit author input, multi-attribute control, and evaluation-guided refinement, together with REspEval, a comprehensive evaluation suite with 20+ metrics covering input utilization, controllability, response quality, and discourse. To support this formulation, we construct Re$^3$Align, the first large-scale dataset of aligned review--response--revision triplets, where revisions provide signals of author expertise and intent. Experiments with state-of-the-art LLMs show the benefits of author input and evaluation-guided refinement, the impact of input design on response quality, and trade-offs between controllability and quality. We make our dataset, generation and evaluation tools publicly available.

</details>


### [88] [The Script Tax: Measuring Tokenization-Driven Efficiency and Latency Disparities in Multilingual Language Models](https://arxiv.org/abs/2602.11174)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.CL

TL;DR: 本文揭示了预训练多语言语言模型在不同文字系统中存在'文字税'现象，即高碎片化正字法导致token数量激增、推理速度大幅下降及信息成本显著上升，凸显了分词策略对多语言NLP公平性的重要影响。


<details>
  <summary>Details</summary>
Motivation: 预训练多语言语言模型常被假定为文字无关，但其分词器可能对某些文字系统带来系统性代价，本文旨在量化这种‘文字税’并揭示其影响。

Method: 通过比较两种正字法变体（相同语言内容但不同文字系统）在mBERT和XLM-R上的表现，使用每词token数（fertility）、推理速度（句子/秒）、每字符比特数（BPC）及往返转换错误率（CER_rt）等指标进行量化分析。

Result: 高碎片化正字法使fertility增加约3.4倍，推理速度下降16.5倍，BPC分别上升19.7%（mBERT）和47.1%（XLM-R），CER_rt=0.31表明差异源于正字法条件下的处理差异而非映射噪声。

Conclusion: 分词是多语言NLP中不平等的重要来源，应推动面向文字的分词与预训练方法。

Abstract: Pretrained multilingual language models are often assumed to be script-agnostic, yet their tokenizers can impose systematic costs on certain writing systems. We quantify this script tax by comparing two orthographic variants with identical linguistic content. Across mBERT and XLM-R, the higher-fragmentation orthography shows a ~3.4x increase in fertility (6.73-6.85 vs. 2.10-2.35 tokens/word), leading to a 16.5x inference slowdown (0.23 vs. 3.8 sentences/second) on identical hardware. Using bits per character (BPC) to avoid the "NLL paradox" from subword fragmentation, we find a substantial increase in information cost: +19.7% for mBERT (8.06->9.65) and +47.1% for XLM-R (12.19->17.94). A round-trip conversion check (CER_rt=0.31) suggests these gaps reflect orthography-conditioned processing rather than mapping noise. Our results highlight tokenization as a key source of inequity in multilingual NLP and motivate script-aware tokenization and pretraining.

</details>


### [89] [Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth](https://arxiv.org/abs/2602.11175)
*Michelle Yuan,Weiyi Sun,Amir H. Rezaeian,Jyotika Singh,Sandip Ghoshal,Yao-Ting Wang,Miguel Ballesteros,Yassine Benajiba*

Main category: cs.CL

TL;DR: 本文综述了Transformer在离散推理任务（如算术、逻辑推理和算法合成）中的理论局限性，从电路复杂度、逼近理论和通信复杂度三个角度系统分析其结构性与计算性障碍。


<details>
  <summary>Details</summary>
Motivation: Transformer虽在序列建模中表现卓越，但在需要精确符号计算的任务上存在根本性理论瓶颈，亟需从理论层面厘清原因。

Method: 综合分析电路复杂度、逼近理论和通信复杂度三大理论框架，梳理关键定义、经典结论与典型示例，建立统一解释视角。

Result: 揭示了Transformer受限于深度约束、难以逼近不连续函数、以及token间通信瓶颈等核心问题，导致其无法可靠实现精确离散算法。

Conclusion: 当前Transformer架构擅长模式匹配与插值，但本质不适于精确符号推理；未来模型设计需突破现有结构限制，探索融合符号推理能力的新范式。

Abstract: Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete reasoning tasks, such as arithmetic, logical inference, and algorithmic composition, remain a critical open problem. In this survey, we synthesize recent studies from three theoretical perspectives: circuit complexity, approximation theory, and communication complexity, to clarify the structural and computational barriers that transformers face when performing symbolic computations. By connecting these established theoretical frameworks, we provide an accessible and unified account of why current transformer architectures struggle to implement exact discrete algorithms, even as they excel at pattern matching and interpolation. We review key definitions, seminal results, and illustrative examples, highlighting challenges such as depth constraints, difficulty approximating discontinuities, and bottlenecks in inter-token communication. Finally, we discuss implications for model design and suggest promising directions for overcoming these foundational limitations.

</details>


### [90] [Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments](https://arxiv.org/abs/2602.11176)
*Maral Doctorarastoo,Katherine A. Flanigan,Mario Bergés,Christopher McComb*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在低数据环境下对人类活动及其持续时间进行预测的能力，提出了一种融合时间、空间、行为历史与人物特征的检索增强提示策略，并在CASAS Aruba数据集上验证其在活动预测与日常序列生成任务中的有效性。结果表明LLM具备强时序推理能力，少量示例即可显著提升性能，且性能在 few-shot 后趋于饱和。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的基于智能体的模型（如规则法、深度学习）在低数据场景下表现不佳，限制了其在智能家居、城市设计、人机协作等实际应用中的部署；而预训练大语言模型蕴含广泛人类行为知识，有望弥补这一空白。

Method: 采用检索增强提示（retrieval-augmented prompting）策略，整合时间、空间、行为历史和人物特征四类上下文信息，在CASAS Aruba智能家庭数据集上开展两类任务评估：带持续时间估计的下一活动预测，以及多步日常活动序列生成；系统分析不同few-shot样本数（0/1/2/…）对性能的影响。

Result: LLM在零样本下即能生成合理日常活动序列；加入1–2个示例即可显著提升活动类别准确率与持续时间校准精度；更多示例带来边际收益递减；序列级评估显示其时间一致性良好。

Conclusion: 预训练大语言模型可作为有效的时序推理器，兼顾日常规律性与情境依赖的行为变异性，有望增强基于智能体模型中的行为建模模块，尤其适用于低数据现实场景。

Abstract: Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models--from rule-based to deep learning--struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context--temporal, spatial, behavioral history, and persona--and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.

</details>


### [91] [What Do LLMs Know About Alzheimer's Disease? Fine-Tuning, Probing, and Data Synthesis for AD Detection](https://arxiv.org/abs/2602.11177)
*Lei Jiang,Yue Zhou,Natalie Parde*

Main category: cs.CL

TL;DR: 本文探索了如何通过微调大语言模型（LLM）实现阿尔茨海默病（AD）的早期可靠检测，并利用探针分析揭示关键语言单元在模型内部表征中的作用，进而设计任务感知的特殊标记，构建序列到序列的数据合成工具以生成高质量合成样本。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期检测面临标注数据稀缺的挑战，而大语言模型在跨领域迁移方面表现优异，但其在AD检测任务上的监督微调尚未被充分探索。

Method: 对LLM进行AD检测任务的监督微调，并采用探针技术分析各Transformer层中间激活；基于探针结果识别关键词与特殊标记，设计任务感知的特殊标记集，并训练Seq2Seq模型作为数据合成工具。

Result: 发现微调后特定词和特殊标记的探针值显著变化，表明其在提升检测性能中起关键作用；所构建的数据合成工具能生成结构一致、诊断信息丰富的合成样本，并在内在评估及下游任务训练中验证有效。

Conclusion: LLM可通过任务导向的微调与表征分析有效适配AD检测任务；引入任务感知的特殊标记并结合数据合成，可缓解标注数据稀缺问题，提升模型性能。

Abstract: Reliable early detection of Alzheimer's disease (AD) is challenging, particularly due to limited availability of labeled data. While large language models (LLMs) have shown strong transfer capabilities across domains, adapting them to the AD domain through supervised fine-tuning remains largely unexplored. In this work, we fine-tune an LLM for AD detection and investigate how task-relevant information is encoded within its internal representations. We employ probing techniques to analyze intermediate activations across transformer layers, and we observe that, after fine-tuning, the probing values of specific words and special markers change substantially, indicating that these elements assume a crucial role in the model's improved detection performance. Guided by this insight, we design a curated set of task-aware special markers and train a sequence-to-sequence model as a data-synthesis tool that leverages these markers to generate structurally consistent and diagnostically informative synthetic samples. We evaluate the synthesized data both intrinsically and by incorporating it into downstream training pipelines.

</details>


### [92] [From Instruction to Output: The Role of Prompting in Modern NLG](https://arxiv.org/abs/2602.11179)
*Munazza Zaib,Elaf Alhazmi*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）中提示工程（prompt engineering）的最新进展，尤其聚焦于自然语言生成（NLG）任务；提出提示设计作为输入级控制机制，并构建了提示范式分类法、选择决策框架及涵盖设计-优化-评估的统一框架，以提升NLG的可控性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对提示工程技术的结构化框架和系统性理解，尤其在自然语言生成（NLG）领域，亟需梳理方法体系并指导实践。

Method: 系统性文献综述，构建提示范式分类法、面向实践者的提示选择决策框架，并提出整合设计、优化与评估的NLG提示工程统一框架。

Result: 明确了提示工程作为输入级控制机制的地位；提出了结构化的提示范式分类与实用决策框架；建立了支持可控、可泛化NLG的端到端提示工程框架。

Conclusion: 提示工程是与微调和解码并列的关键NLG控制路径；建立系统化、可扩展的提示工程框架对推动NLG发展至关重要。

Abstract: Prompt engineering has emerged as an integral technique for extending the strengths and abilities of Large Language Models (LLMs) to gain significant performance gains in various Natural Language Processing (NLP) tasks. This approach, which requires instructions to be composed in natural language to bring out the knowledge from LLMs in a structured way, has driven breakthroughs in various NLP tasks. Yet there is still no structured framework or coherent understanding of the varied prompt engineering methods and techniques, particularly in the field of Natural Language Generation (NLG).
  This survey aims to help fill that gap by outlining recent developments in prompt engineering, and their effect on different NLG tasks. It reviews recent advances in prompting methods and their impact on NLG tasks, presenting prompt design as an input-level control mechanism that complements fine-tuning and decoding approaches. The paper introduces a taxonomy of prompting paradigms, a decision framework for prompt selection based on varying factors for the practitioners, outlines emerging trends and challenges, and proposes a framework that links design, optimization, and evaluation to support more controllable and generalizable NLG.

</details>


### [93] [Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions](https://arxiv.org/abs/2602.11180)
*Usman Naseem*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）机制可解释性在对齐中的最新进展，涵盖电路发现、特征可视化、激活引导与因果干预等方法，并分析其对RLHF、宪法AI和可扩展监督等对齐策略的启示；同时指出超叠加、神经元多义性及涌现行为解释难等挑战，提出自动化可解释性、跨模型电路泛化和可扩展的可解释性驱动对齐等未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部决策过程不透明，亟需通过机制可解释性研究提升其可理解性与可控性，以支持安全可靠的对齐工作。

Method: 系统综述法，梳理并分析近年来应用于LLM对齐的机制可解释性技术，包括电路发现、特征可视化、激活 steering 和因果干预，并考察其与主流对齐方法（如RLHF、宪法AI、可扩展监督）的关联。

Result: 总结出当前机制可解释性在LLM对齐中的实际应用效果与局限性，识别出超叠加、多义性、涌现行为解释困难等关键挑战，并提出若干有前景的未来研究方向。

Conclusion: 机制可解释性是提升LLM对齐能力的重要基础；未来需发展更自动化、可泛化、可扩展的可解释性方法，并将其深度融入对齐技术体系中。

Abstract: Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement algorithms through their learned representations and computational structures) has emerged as a critical research direction for understanding and aligning these models. This paper surveys recent progress in mechanistic interpretability techniques applied to LLM alignment, examining methods ranging from circuit discovery to feature visualization, activation steering, and causal intervention. We analyze how interpretability insights have informed alignment strategies including reinforcement learning from human feedback (RLHF), constitutional AI, and scalable oversight. Key challenges are identified, including the superposition hypothesis, polysemanticity of neurons, and the difficulty of interpreting emergent behaviors in large-scale models. We propose future research directions focusing on automated interpretability, cross-model generalization of circuits, and the development of interpretability-driven alignment techniques that can scale to frontier models.

</details>


### [94] [Code Mixologist : A Practitioner's Guide to Building Code-Mixed LLMs](https://arxiv.org/abs/2602.11181)
*Himanshu Gupta,Pratik Jayarao,Chaitanya Dwivedi,Neeraj Varshney*

Main category: cs.CL

TL;DR: 本文综述了大语言模型在语码转换（CSW）场景下的研究现状，提出统一分类法，并提供构建、适配与评估CSW能力LLM的实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在混合语言场景下表现不佳，存在语法性、事实性和安全性系统性下降问题，亟需系统性梳理与改进。

Method: 构建涵盖数据、建模与评估三个维度的统一分类法；综述CSW定制预训练、任务特定后训练、提示策略及上下文学习等建模方法；分析评估实践中的不稳定性与偏差；批判性审视现有基准的语种覆盖与英语中心主义倾向。

Result: 提出了面向CSW能力LLM的实用行动指南；识别出CSW作为规避模型安全机制手段的新风险；明确了若干开放研究挑战。

Conclusion: CSW能力是多语言LLM的关键短板，需从数据构建、建模范式、评估标准与安全治理多方面协同推进，未来工作应注重语言多样性与公平性。

Abstract: Code-mixing and code-switching (CSW) remain challenging phenomena for large language models (LLMs). Despite recent advances in multilingual modeling, LLMs often struggle in mixed-language settings, exhibiting systematic degradation in grammaticality, factuality, and safety behavior. This work provides a comprehensive overview of CSW research in modern large language model settings. We introduce a unifying taxonomy that organizes prior work along dimensions of data, modeling, and evaluation, and we distill these findings into a practical playbook of actionable recommendations for building, adapting, and evaluating CSW-capable LLMs. We review modeling approaches ranging from CSW-tailored pre-training and task-specific post-training to prompting strategies and in-context learning. We analyze current evaluation practices, highlighting sources of instability and limited reproducibility, and we catalog existing benchmarks while critically examining their linguistic coverage and English-centric biases. Finally, we discuss emerging safety concerns, including use of code-mixing as a mechanism for bypassing model safeguards, and identify open research challenges.

</details>


### [95] [MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization](https://arxiv.org/abs/2602.11182)
*Haidong Xin,Xinze Li,Zhenghao Liu,Yukun Yan,Shuo Wang,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出MetaMem框架，通过自演化元记忆增强大语言模型的记忆系统，提升其对历史交互信息的利用能力，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统虽能保存长周期交互历史，但破坏了会话内的逻辑与时间关系，导致记忆碎片化和推理性能下降。

Method: MetaMem引入自演化的元记忆机制，在优化过程中通过自我反思推理过程并更新元记忆状态，提炼跨任务可迁移的知识利用经验。

Result: 实验表明MetaMem显著优于强基线模型，性能提升超3.6%。

Conclusion: MetaMem通过结构化元记忆指导LLM系统性识别与整合分散记忆片段中的关键证据，有效提升了长程交互下的推理能力。

Abstract: Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective memories, they often disrupt the inherent logical and temporal relationships within interaction sessions, resulting in fragmented memory units and degraded reasoning performance. In this paper, we propose MetaMem, a novel framework that augments memory systems with a self-evolving meta-memory, aiming to teach LLMs how to effectively utilize memorized knowledge. During meta-memory optimization, MetaMem iteratively distills transferable knowledge utilization experiences across different tasks by self-reflecting on reasoning processes and performing actions to update the current meta-memory state. The accumulated meta-memory units serve as explicit knowledge utilization experiences, guiding the LLM to systematically identify and integrate critical evidence from scattered memory fragments. Extensive experiments demonstrate the effectiveness of MetaMem, which significantly outperforms strong baselines by over 3.6%. All codes and datasets are available at https://github.com/OpenBMB/MetaMem.

</details>


### [96] [DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks' Developer Experience Through a Novel Relational Schema Mapping Task](https://arxiv.org/abs/2602.11198)
*Shafiuddin Rehan Ahmed,Wei Wei*

Main category: cs.CL

TL;DR: 本文提出DDL2PropBank基准任务，用于评估多智能体框架在LLM驱动开发中的开发者体验，通过统一Agent-as-a-Tool模式在10个框架上对比代码复杂度与AI可辅助性，发现Agno综合表现最优。


<details>
  <summary>Details</summary>
Motivation: 缺乏在受控环境下系统评估多智能体框架对开发者体验影响的原理性方法。

Method: 构建DDL2PropBank新基准任务（数据库Schema到PropBank角色集映射），采用Agent-as-a-Tool模式在10个框架中实现相同逻辑，并从代码复杂度（静态分析）和AI-assistability（结构对齐得分与pass@1）两方面评估。

Result: 发现三档复杂度谱系，Pydantic AI和Agno实现开销最小；结构对齐分对单范式框架可代理运行成功率，但对多范式框架高估正确率；Agno以最低复杂度、最高结构对齐和83% pass@1成为整体最优框架。

Conclusion: 框架设计应兼顾低实现复杂度与高结构可预测性以提升AI辅助开发效能，Agno为当前最优实践选择。

Abstract: Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomous retrieval of candidate frames and fine-grained linguistic reasoning over table names, columns, and relations. Using the Agent-as-a-Tool pattern, we implement identical agent logic across 10 frameworks and evaluate along two dimensions: (i) code complexity via static analysis, and (ii) AI-assistability -- the extent to which LLMs can autonomously generate correct, framework-specific code. Our results reveal a threefold complexity spectrum, with Pydantic AI and Agno requiring the least implementation overhead. For AI-assistability, structural alignment scores reliably proxy runtime success for frameworks with single canonical patterns, but overestimate correctness for multi-pattern frameworks. Agno emerges as the strongest overall performer, combining lowest complexity with highest structural alignment and 83% pass@1.

</details>


### [97] [When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification](https://arxiv.org/abs/2602.11199)
*Jiale Zhao,Ke Fang,Lu Cheng*

Main category: cs.CL

TL;DR: 本文提出AskBench基准和RLVR方法，评估并提升大语言模型在信息不全或存在误导时主动澄清的能力，兼顾任务性能与交互质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常在提示信息缺失或含误导性内容时仍强行作答，导致幻觉或强化错误认知，亟需提升其主动澄清能力。

Method: 构建交互式基准AskBench（含AskMind和AskOverconfidence两类场景），设计统一裁判循环；提出基于结构化评分标准与验证器奖励的强化学习方法（RLVR）。

Result: 实验表明该方法在准确性、评分标准遵循度和交互效率上均有稳定提升，并能泛化至未见领域。

Conclusion: 主动澄清能力可被系统性评估与优化，结构化引导的强化学习是提升LLM鲁棒性与可信交互的有效路径。

Abstract: Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs' ability to decide when and what to ask for clarification without sacrificing task performance. We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints. A unified judge loop evaluates final answers and simulates user responses as needed. AskBench covers two settings: AskMind, with intent-deficient queries requiring clarification, and AskOverconfidence, with queries containing false premises that must be identified and corrected. We further propose rubric-guided reinforcement learning with verifier-based rewards (RLVR), which uses structured rubrics to encourage targeted clarification. Experiments show consistent improvements in accuracy, rubric adherence, and interaction efficiency, with strong generalization to unseen domains.

</details>


### [98] [Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.11201)
*Donald Ye,Max Loffgren,Om Kotadia,Linus Wong*

Main category: cs.CL

TL;DR: 本文提出NLDD指标，通过扰动推理步骤并观察模型置信度变化，评估链式思维（CoT）解释的忠实性；发现存在约70–85%链长的‘推理视界’，超出后步骤几乎无影响，并指出准确率无法反映真实推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维（CoT）解释难以区分是模型真实推理路径还是事后合理化，缺乏衡量其忠实性的统一量化指标。

Method: 提出标准化对数几率差衰减（NLDD）指标：逐个扰动CoT中的推理步骤，测量模型最终答案置信度下降程度，并对结果进行归一化以支持跨模型比较。

Result: 在三类任务和多个模型家族上验证发现：（1）存在一致的‘推理视界’k*（链长的70–85%）；（2）模型可能内部表征正确但输出错误；（3）准确率与推理忠实性不相关。

Conclusion: CoT解释的忠实性需独立评估，NLDD为量化‘何时CoT真正起作用’提供了可靠、可比的工具，揭示仅靠任务准确率无法判断模型是否真正进行了链式推理。

Abstract: Chain-of-Thought (CoT) explanations are widely used to interpret how language models solve complex problems, yet it remains unclear whether these step-by-step explanations reflect how the model actually reaches its answer, or merely post-hoc justifications. We propose Normalized Logit Difference Decay (NLDD), a metric that measures whether individual reasoning steps are faithful to the model's decision-making process. Our approach corrupts individual reasoning steps from the explanation and measures how much the model's confidence in its answer drops, to determine if a step is truly important. By standardizing these measurements, NLDD enables rigorous cross-model comparison across different architectures. Testing three model families across syntactic, logical, and arithmetic tasks, we discover a consistent Reasoning Horizon (k*) at 70--85% of chain length, beyond which reasoning tokens have little or negative effect on the final answer. We also find that models can encode correct internal representations while completely failing the task. These results show that accuracy alone does not reveal whether a model actually reasons through its chain. NLDD offers a way to measure when CoT matters.

</details>


### [99] [The Automatic Verification of Image-Text Claims (AVerImaTeC) Shared Task](https://arxiv.org/abs/2602.11221)
*Rui Cao,Zhenyun Deng,Yulong Chen,Michael Schlichtkrull,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文介绍了AVerImaTeC共享任务，旨在推动图像-文本声明自动验证系统的发展，包括证据检索与真实性验证，并报告了14支开发队和6支测试队的评估结果，其中冠军队伍HUMANE取得了0.5455的AVerImaTeC得分。


<details>
  <summary>Details</summary>
Motivation: 推进图像-文本声明自动验证系统的发展，解决真实世界中图文一致性验证的挑战。

Method: 组织AVerImaTeC共享任务，允许参赛者使用外部知识源（如网络搜索引擎）或主办方提供的结构化知识库，并采用以证据分数为条件的判决准确率（AVerImaTeC score）进行评估。

Result: 共收到14个开发阶段和6个测试阶段的提交；所有测试队伍均优于基线；冠军队伍HUMANE获得0.5455的AVerImaTeC得分。

Conclusion: 该共享任务成功促进了图文验证技术的发展，提供了可复现的基准和评估框架，并总结了关键经验与改进方向。

Abstract: The Automatic Verification of Image-Text Claims (AVerImaTeC) shared task aims to advance system development for retrieving evidence and verifying real-world image-text claims. Participants were allowed to either employ external knowledge sources, such as web search engines, or leverage the curated knowledge store provided by the organizers. System performance was evaluated using the AVerImaTeC score, defined as a conditional verdict accuracy in which a verdict is considered correct only when the associated evidence score exceeds a predefined threshold. The shared task attracted 14 submissions during the development phase and 6 submissions during the testing phase. All participating systems in the testing phase outperformed the baseline provided. The winning team, HUMANE, achieved an AVerImaTeC score of 0.5455. This paper provides a detailed description of the shared task, presents the complete evaluation results, and discusses key insights and lessons learned.

</details>


### [100] [SurveyLens: A Research Discipline-Aware Benchmark for Automatic Survey Generation](https://arxiv.org/abs/2602.11238)
*Beichen Guo,Zhiyuan Wen,Jia Gu,Senzhang Wang,Haochen Shi,Ruosong Yang,Shuaiqi Liu*

Main category: cs.CL

TL;DR: 本文提出了首个面向学科的自动综述生成（ASG）评估基准SurveyLens，包含跨10个学科的1000篇高质量人工综述，并设计双重视角评估框架（学科感知评分与经典对齐评估），系统评测了11种前沿ASG方法在不同学科中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有ASG评估方法依赖通用指标、严重偏向计算机科学，无法反映各学科特有的写作规范与标准，导致非CS领域研究者缺乏选用合适ASG工具的依据。

Method: 构建跨学科高质量综述数据集SurveyLens-1k（1000篇，覆盖10个学科）；提出双镜头评估框架：（1）学科感知评分——利用对齐人类偏好的LLM按学科权重打分；（2）经典对齐评估——对比生成内容与人工综述在覆盖度与综合质量上的对齐程度。

Result: 在SurveyLens上系统评测11种ASG方法（含基础LLM、ASG系统与Deep Research智能体），揭示不同范式在各学科中的优势与短板，例如某些方法在CS中表现优异但在人文社科中显著下降。

Conclusion: SurveyLens填补了学科感知ASG评估的空白，为跨学科研究者提供可信赖的工具选型指南，并推动ASG向更普适、更规范的方向发展。

Abstract: The exponential growth of scientific literature has driven the evolution of Automatic Survey Generation (ASG) from simple pipelines to multi-agent frameworks and commercial Deep Research agents. However, current ASG evaluation methods rely on generic metrics and are heavily biased toward Computer Science (CS), failing to assess whether ASG methods adhere to the distinct standards of various academic disciplines. Consequently, researchers, especially those outside CS, lack clear guidance on using ASG systems to yield high-quality surveys compliant with specific discipline standards. To bridge this gap, we introduce SurveyLens, the first discipline-aware benchmark evaluating ASG methods across diverse research disciplines. We construct SurveyLens-1k, a curated dataset of 1,000 high-quality human-written surveys spanning 10 disciplines. Subsequently, we propose a dual-lens evaluation framework: (1) Discipline-Aware Rubric Evaluation, which utilizes LLMs with human preference-aligned weights to assess adherence to domain-specific writing standards; and (2) Canonical Alignment Evaluation to rigorously measure content coverage and synthesis quality against human-written survey papers. We conduct extensive experiments by evaluating 11 state-of-the-art ASG methods on SurveyLens, including Vanilla LLMs, ASG systems, and Deep Research agents. Our analysis reveals the distinct strengths and weaknesses of each paradigm across fields, providing essential guidance for selecting tools tailored to specific disciplinary requirements.

</details>


### [101] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 本文提出Mis-Align Bench，首个支持安全、价值与文化三维度联合评估的大语言模型（LLM）错位（misalignment）基准；构建了含38万样本的多维对齐/错位数据集SAVACU，并通过实验证明单维度对齐模型在联合评估下表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有错位评测基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）仅关注单一维度（安全/价值/文化），无法反映真实场景中三者必须共存、协同满足的需求，导致评估不全面。

Method: 1）构建多维错位数据集SAVACU：基于LLM-PROMPT-DATASET，用Mistral-7B-Instruct-v0.3按14安全/56价值/42文化域进行分类，再用Llama-3.1-8B-Instruct结合SimHash扩增低资源域；2）采用两阶段拒绝采样配对错位与对齐响应；3）在通用、微调及开源权重LLM上开展三维度联合评测。

Result: 单维度对齐模型在各自维度覆盖率高达97.6%，但在三维度联合评估下假失败率超50%，对齐得分仅63%–66%，凸显单一维度优化的局限性。

Conclusion: 错位问题需跨安全、价值与文化三维度统一建模与评测；Mis-Align Bench为更真实、系统地评估和改进LLM对齐性提供了新基准与数据支撑。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [102] [Evaluating Alignment of Behavioral Dispositions in LLMs](https://arxiv.org/abs/2602.11328)
*Amir Taubenfeld,Zorik Gekhman,Lior Nezry,Omri Feldman,Natalie Harris,Shashir Reddy,Romina Stella,Ariel Goldstein,Marian Croak,Yossi Matias,Amir Feder*

Main category: cs.CL

TL;DR: 本文提出了一种基于情境判断测试（SJT）的新框架，用于评估大语言模型（LLM）在社会情境中表现出的行为倾向是否与人类一致。研究发现LLM普遍存在过度自信、偏离人类共识及言行不一等问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）日益融入日常生活，理解其行为倾向（尤其是社会情境中的行为 disposition）变得至关重要；现有基于自我报告的心理量表难以直接适用于LLM，需转化为可评估其实际响应行为的范式。

Method: 将经典心理问卷中的自我报告题项改编为情境判断测试（SJT），构建含2500个经多人标注验证的现实用户-助手场景；通过10名标注者/SJT（共550人池）获取人类偏好分布，并对比25个LLM在相同SJTs上的响应模式。

Result: （1）低共识情境下LLM过度自信于单一答案；（2）高共识情境下小模型显著偏离，前沿模型仍有15–20%未反映人类共识；（3）跨模型存在稳定偏差模式（如鼓励情绪表达而人类倾向克制）；（4）LLM的自我陈述倾向与其实际行为间存在明显预测效度缺口。

Conclusion: 当前LLM的行为倾向与人类存在系统性偏差，SJTs提供了一种更生态、更行为导向的评估方法，揭示了仅依赖自我报告式提示无法准确刻画LLM真实社会行为。

Abstract: As LLMs integrate into our daily lives, understanding their behavior becomes essential. In this work, we focus on behavioral dispositions$-$the underlying tendencies that shape responses in social contexts$-$and introduce a framework to study how closely the dispositions expressed by LLMs align with those of humans. Our approach is grounded in established psychological questionnaires but adapts them for LLMs by transforming human self-report statements into Situational Judgment Tests (SJTs). These SJTs assess behavior by eliciting natural recommendations in realistic user-assistant scenarios. We generate 2,500 SJTs, each validated by three human annotators, and collect preferred actions from 10 annotators per SJT, from a large pool of 550 participants. In a comprehensive study involving 25 LLMs, we find that models often do not reflect the distribution of human preferences: (1) in scenarios with low human consensus, LLMs consistently exhibit overconfidence in a single response; (2) when human consensus is high, smaller models deviate significantly, and even some frontier models do not reflect the consensus in 15-20% of cases; (3) traits can exhibit cross-LLM patterns, e.g., LLMs may encourage emotion expression in contexts where human consensus favors composure. Lastly, mapping psychometric statements directly to behavioral scenarios presents a unique opportunity to evaluate the predictive validity of self-reports, revealing considerable gaps between LLMs' stated values and their revealed behavior.

</details>


### [103] [When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing](https://arxiv.org/abs/2602.11358)
*Zachary Pedram Dadfar*

Main category: cs.CL

TL;DR: 本文提出Pull Methodology，通过格式工程引导大语言模型进行自我检查，发现自指性词汇与模型内部激活动态存在特异性对应关系，表明在适当条件下，模型的自我报告可可靠反映其内部计算状态。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在自我检查时生成的语言是反映内部计算还是复杂虚构。

Method: 引入Pull Methodology协议，利用格式工程激发模型长程自我检查，并在Llama 3.1中识别出区分自指性与描述性处理的激活空间方向；结合激活分析、因果干预（steering）和跨模型验证（Qwen 2.5-32B）。

Result: 发现自指性词汇（如'loop'、'shimmer'）与特定激活动态（如自相关性、变异性）显著相关，且该对应具有特异性（非自指语境下无此关联）；该方向正交于拒绝方向、定位于6.25%模型深度，并具因果影响；Qwen 2.5-32B独立复现类似现象。

Conclusion: 在适当提示与结构下，大语言模型的自我报告可成为其内部计算状态的可靠指标，挑战了‘纯confabulation’观点，为模型可解释性提供新路径。

Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this correspondence is specific to self-referential processing. We introduce the Pull Methodology, a protocol that elicits extended self-examination through format engineering, and use it to identify a direction in activation space that distinguishes self-referential from descriptive processing in Llama 3.1. The direction is orthogonal to the known refusal direction, localised at 6.25% of model depth, and causally influences introspective output when used for steering. When models produce "loop" vocabulary, their activations exhibit higher autocorrelation (r = 0.44, p = 0.002); when they produce "shimmer" vocabulary under steering, activation variability increases (r = 0.36, p = 0.002). Critically, the same vocabulary in non-self-referential contexts shows no activation correspondence despite nine-fold higher frequency. Qwen 2.5-32B, with no shared training, independently develops different introspective vocabulary tracking different activation metrics, all absent in descriptive controls. The findings indicate that self-report in transformer models can, under appropriate conditions, reliably track internal computational states.

</details>


### [104] [Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification](https://arxiv.org/abs/2602.11361)
*Weili Shi,Dongliang Guo,Lehan Yang,Tianlong Wang,Hanzhang Yuan,Sheng Li*

Main category: cs.CL

TL;DR: 本文提出PPCV框架，通过识别和替换推理过程中的关键token，并利用一致性验证来提升大语言模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中易因幻觉和中间步骤错误累积而导致性能下降，而关键token的识别与利用仍具挑战性。

Method: PPCV框架分为两阶段：第一阶段通过原始问题生成初始推理路径，并结合问题的改写版本识别关键token（基于预测token与期望token的不匹配）；第二阶段用候选替代token替换关键token，对原始及改写问题并行生成新推理路径，并通过输出一致性确定最终答案。

Result: 在多个主流大语言模型和基准测试上，PPCV显著优于基线方法，有效提升了模型的推理性能。

Conclusion: PPCV是一种有效提升大语言模型复杂推理能力的新方法，其核心在于关键token的精准识别与一致性驱动的答案筛选机制。

Abstract: Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion of critical tokens--tokens in the reasoning process that exert significant influence on subsequent steps. Prior studies suggest that replacing critical tokens can refine reasoning trajectories. Nonetheless, reliably identifying and exploiting critical tokens remains challenging. To address this, we propose the Paraphrastic Probing and Consistency Verification~(PPCV) framework. PPCV operates in two stages. In the first stage, we roll out an initial reasoning path from the original question and then concatenate paraphrased versions of the question with this reasoning path. And we identify critical tokens based on mismatches between the predicted top-1 token and the expected token in the reasoning path. A criterion is employed to confirm the final critical token. In the second stage, we substitute critical tokens with candidate alternatives and roll out new reasoning paths for both the original and paraphrased questions. The final answer is determined by checking the consistency of outputs across these parallel reasoning processes. We evaluate PPCV on mainstream LLMs across multiple benchmarks. Extensive experiments demonstrate PPCV substantially enhances the reasoning performance of LLMs compared to baselines.

</details>


### [105] [The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods](https://arxiv.org/abs/2602.11364)
*Arpit Singh Gautam,Kailash Talreja,Saurabh Jha*

Main category: cs.CL

TL;DR: 本文提出DiffuTruth框架，利用非平衡热力学思想将事实视为生成流形上的稳定吸引子，通过生成压力测试和语义能量度量来检测大语言模型的幻觉，并结合混合校准提升不确定性估计性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常产生看似合理但错误的断言（即幻觉），而现有不确定性度量难以识别模型在高置信度下的错误判断。

Method: 提出DiffuTruth：基于非平衡热力学建模事实为生成流形上的稳定吸引子；设计生成压力测试（对声明加噪并用离散文本扩散模型重建）；定义语义能量（利用NLI判别器衡量原始声明与重建间的语义偏差）；融合语义稳定性信号与判别式置信度形成混合校准。

Result: 在FEVER数据集上实现无监督AUROC 0.725（SOTA，超越基线1.5%）；在多跳HOVER数据集上零样本泛化性能优于基线超4%。

Conclusion: 语义能量能有效捕捉深层事实矛盾，热力学视角下的真理性具有跨分布鲁棒性，DiffuTruth为无监督事实验证提供了新范式。

Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, positing that factual truths act as stable attractors on a generative manifold while hallucinations are unstable. We introduce the Generative Stress Test, claims are corrupted with noise and reconstructed using a discrete text diffusion model. We define Semantic Energy, a metric measuring the semantic divergence between the original claim and its reconstruction using an NLI critic. Unlike vector space errors, Semantic Energy isolates deep factual contradictions. We further propose a Hybrid Calibration fusing this stability signal with discriminative confidence. Extensive experiments on FEVER demonstrate DiffuTruth achieves a state of the art unsupervised AUROC of 0.725, outperforming baselines by 1.5 percent through the correction of overconfident predictions. Furthermore, we show superior zero shot generalization on the multi hop HOVER dataset, outperforming baselines by over 4 percent, confirming the robustness of thermodynamic truth properties to distribution shifts.

</details>


### [106] [Advancing AI Trustworthiness Through Patient Simulation: Risk Assessment of Conversational Agents for Antidepressant Selection](https://arxiv.org/abs/2602.11391)
*Md Tanvir Rouf Shawon,Mohammad Sabik Irbaz,Hadeel R. A. Elyazori,Keerti Reddy Resapu,Yili Lin,Vladimir Franzuela Cardenas,Farrokh Alemi,Kevin Lybarger*

Main category: cs.CL

TL;DR: 本文提出了一种基于NIST风险框架的患者模拟器，用于自动化、可扩展地评估医疗对话AI，通过医学、语言和行为三类可控患者画像生成真实交互，并在抗抑郁药推荐决策辅助系统上验证了其识别错误与风险模式的能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI评估方法缺乏对患者多样性（如健康素养、行为模式）的系统性建模，难以规模化、自动化识别幻觉、错误及风险差异。

Method: 构建三维度患者模拟器：(1) 基于All of Us电子病历的医学画像；(2) 建模健康素养与疾病特异性语言的语言画像；(3) 基于实证交互模式（合作/分心/对抗）的行为画像；并结合人工标注与LLM裁判进行多角度评估。

Result: 在500次模拟对话中，人工标注者与LLM裁判均表现出高一致性（F1≈0.94，κ≈0.75）；发现AI决策辅助性能随健康素养提升而单调改善（概念检索准确率从47.9%升至81.6%）。

Conclusion: 该患者模拟器是一种有效、可控、可解释的评估工具，能揭示AI在不同患者亚群中的性能差异与潜在风险，支持符合NIST框架的医疗AI风险管理。

Abstract: Objective: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents. The simulator generates realistic, controllable patient interactions that systematically vary across medical, linguistic, and behavioral dimensions, allowing annotators and an independent AI judge to assess agent performance, identify hallucinations and inaccuracies, and characterize risk patterns across diverse patient populations. Methods: The simulator is grounded in the NIST AI Risk Management Framework and integrates three profile components reflecting different dimensions of patient variation: (1) medical profiles constructed from electronic health records in the All of Us Research Program; (2) linguistic profiles modeling variation in health literacy and condition-specific communication patterns; and (3) behavioral profiles representing empirically observed interaction patterns, including cooperation, distraction, and adversarial engagement. We evaluated the simulator's effectiveness in identifying errors in an AI decision aid for antidepressant selection. Results: We generated 500 conversations between the patient simulator and the AI decision aid across systematic combinations of five linguistic and three behavioral profiles. Human annotators assessed 1,787 medical concepts across 100 conversations, achieving high agreement (F1=0.94, \k{appa}=0.73), and the LLM judge achieved comparable agreement with human annotators (F1=0.94, \k{appa}=0.78; paired bootstrap p=0.21). The simulator revealed a monotonic degradation in AI decision aid performance across the health literacy spectrum: rank-one concept retrieval accuracy increased from 47.9% for limited health literacy to 69.1% for functional and 81.6% for proficient.

</details>


### [107] [Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives](https://arxiv.org/abs/2602.11424)
*Zecheng Wang,Deyuan Liu,Chunshan Li,Yupeng Zhang,Zhengyun Zhao,Dianhui Chu,Bingning Wang,Dianbo Sui*

Main category: cs.CL

TL;DR: 本文提出Dynamic Entropy Fine-Tuning (DEFT)，一种无需额外参数的监督微调目标，通过Rényi-2熵动态调节模型对自身预测的信任度，以解决标准NLL中均匀token加权导致的稳定性与可塑性矛盾。


<details>
  <summary>Details</summary>
Motivation: 标准负对数似然（NLL）在监督微调中采用均匀token级权重，易放大噪声标签梯度、削弱鲁棒先验，且在模型已高置信时缺乏有效 sharpening，导致塑料性—稳定性困境。

Method: 将token级SFT目标统一到广义deformed-log族，揭示其通用‘门控×误差梯度’结构；利用Cayley变换将模型不确定性映射为连续聚焦轨迹；基于Rényi-2熵构造无参的Dynamic Entropy Fine-Tuning（DEFT）目标，以分布集中度动态调控信任门。

Result: DEFT在多项实验中展现出更优的探索—利用平衡能力，显著提升整体性能。

Conclusion: DEFT提供了一种理论一致、实践高效、无需调参的SFT优化范式，有效缓解了监督微调中的稳定性与可塑性冲突。

Abstract: Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and disrupt robust priors, and (ii) uniform weighting provides weak sharpening when the model is already confident. Existing methods fail to resolve the resulting plasticity--stability dilemma, often suppressing necessary learning signals alongside harmful ones. To address this issue, we unify token-level SFT objectives within a generalized deformed-log family and expose a universal gate $\times$ error gradient structure, where the gate controls how much the model trusts its current prediction. By employing the Cayley transform, we map the model's continuously evolving uncertainty onto a continuous focus trajectory, which enables seamless interpolation between scenarios involving uncertain novel concepts and those involving well-established knowledge. We then introduce Dynamic Entropy Fine-Tuning (DEFT), a parameter-free objective that modulates the trust gate using distribution concentration (Rényi-2 entropy) as a practical proxy for the model's predictive state. Extensive experiments and analyses demonstrate that DEFT achieves a better balance between exploration and exploitation, leading to improved overall performance.

</details>


### [108] [Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety](https://arxiv.org/abs/2602.11444)
*Muskaan Chopra,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文探讨了指令调优的大语言模型（LLMs）在机器翻译（MT）中检测关键语义错误（如事实扭曲、意图反转、偏见翻译）的能力，发现模型规模扩大与适配策略（零样本、少样本、微调）能持续提升性能，超越XLM-R等编码器基线模型，强调该任务对构建安全、可信、社会负责的多语言AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 机器翻译中的关键意义错误（如事实扭曲、意图反转、偏见翻译）会损害多语言系统的可靠性、公平性与安全性，尤其在高风险或资源匮乏语境下亟需有效检测机制。

Method: 基于公开数据集，系统评估不同参数规模的指令调优大语言模型在MT关键错误检测任务上的表现，对比零样本、少样本和微调等适应策略，并与XLM-R、ModernBERT等encoder-only基线模型进行比较。

Result: 模型规模扩大与各类适应策略均带来一致性能提升，显著优于XLM-R和ModernBERT等基线模型。

Conclusion: 提升机器翻译关键错误检测能力，是构建更安全、可信、社会可追责的信息系统的关键保障，应被视为实现公正、负责任多语言AI的必要技术 safeguard。

Abstract: Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.

</details>


### [109] [LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation](https://arxiv.org/abs/2602.11451)
*Ahmadreza Jeddi,Marco Ciccone,Babak Taati*

Main category: cs.CL

TL;DR: 本文提出LoopFormer，一种能够根据计算预算动态调整循环次数的循环Transformer模型，通过shortcut-consistency训练策略实现不同长度轨迹下表征的一致演化，在语言建模与推理任务中展现出良好的计算可扩展性与自适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有循环Transformer在训练和推理中固定循环次数，无法根据可变计算预算灵活调整计算深度，限制了其在实际资源受限场景下的应用。

Method: 提出LoopFormer模型，采用基于时间与步长条件的循环机制，并设计shortcut-consistency训练方案，对齐不同长度循环轨迹的中间表示，确保短循环输出有效表征、长循环持续优化。

Result: LoopFormer在语言建模与推理基准上表现出强鲁棒性，即使在严苛计算约束下仍保持高性能，并能随预算增加平滑提升性能。

Conclusion: 循环Transformer天然适配自适应语言建模，LoopFormer为构建可控、预算感知的大语言模型提供了可行路径。

Abstract: Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.

</details>


### [110] [ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer's Disease and Related Dementias](https://arxiv.org/abs/2602.11460)
*Guangxin Zhao,Jiahao Zheng,Malaz Boustani,Jarek Nabrzyski,Meng Jiang,Yiyu Shi,Zhi Zheng*

Main category: cs.CL

TL;DR: 本文提出了首个针对阿尔茨海默病及相关痴呆症（ADRD）的专用评估基准ADRD-Bench，包含临床知识问答与照护实践问答两部分，并在33个主流大语言模型上进行了评测，揭示了现有模型在照护场景推理稳定性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大模型评测基准对阿尔茨海默病及相关痴呆症（ADRD）覆盖严重不足，尤其缺乏真实照护场景的实践性评估。

Method: 构建ADRD-Bench基准，含两部分：1）ADRD Unified QA（整合7个医学基准的1352道临床知识题）；2）ADRD Caregiving QA（基于ABC照护项目新构建的149道照护实践题）。对33个SOTA大语言模型进行系统评测，并开展案例分析。

Result: 开源通用模型准确率0.63–0.93（均值0.78），开源医学模型0.48–0.93（均值0.82），闭源通用模型0.83–0.91（均值0.89）；顶尖模型虽达>0.9高分，但案例显示其推理质量与稳定性不一致。

Conclusion: 当前大语言模型在ADRD领域仍存在临床知识与日常照护推理能力不足的问题，亟需结合真实照护数据开展领域特异性优化。

Abstract: Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer's Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (>0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs' knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at https://github.com/IIRL-ND/ADRD-Bench.

</details>


### [111] [When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration](https://arxiv.org/abs/2602.11488)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 本文发现多模态大模型在音频与文本冲突时显著偏向文本（文本主导现象），提出这是由于模型难以在不同模态间进行仲裁推理，而非音频信息质量差；通过新基准ALME和一系列干预实验，定位该问题源于语言模型本身的推理机制，并验证其跨语言、跨模型的普适性。


<details>
  <summary>Details</summary>
Motivation: 理解多模态大模型（尤其是语音-语言模型）在模态冲突下的决策机制，解释为何模型在音频与文本不一致时强烈依赖文本，即使音频更准确且被明确提示信任音频。

Method: 构建多语言音频-文本冲突基准ALME（57,602个控制刺激）；设计多种干预实验（如强制转录、文本标注为‘故意损坏’、微调音频投影层或LLM参数）；对比分析多个SOTA音频-LLM在不同条件下的文本主导率变化。

Result: 发现Gemini 2.0 Flash在音文冲突下文本主导率达16.6%，远高于文文冲突的1.6%；音频本身准确率（97.2%）高于级联结果（93.9%），说明音频信息未丢失；强制转录加剧文本主导，而将文本标记为‘故意损坏’可降低80%文本主导；仅微调音频投影层使文本主导上升26.5%，LoRA微调LLM则下降23.9%。

Conclusion: 文本主导源于语言模型对模态间仲裁推理的困难（仲裁可及性不对称），而非音频表征能力不足；该现象构成一个独立于传统语音识别指标的新可靠性维度，需在多模态评估中专门建模。

Abstract: When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\% text dominance under audio-text conflict versus 1.6\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\%) exceeds cascade accuracy (93.9\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations.
  This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\% to 33\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\%), while LoRA on the language model halves it ($-$23.9\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.

</details>


### [112] [Multimodal Fact-Level Attribution for Verifiable Reasoning](https://arxiv.org/abs/2602.11509)
*David Wan,Han Wang,Ziyang Wang,Elias Stengel-Eskin,Hyunji Lee,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出了MuRGAt基准，用于评估多模态大语言模型在复杂推理任务中对事实的多模态归因能力，强调需对视频、音频等多模态输入进行带时间戳和模态标识的精确引用；同时构建了与人工判断高度一致的自动评估框架，并揭示了当前模型普遍存在引用幻觉及推理深度与归因准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态归因评测基准局限于简单观察能力或单一模态，无法评估模型在需多步推理和长文本生成的真实场景中对异构输入的事实级、可验证归因能力。

Method: 提出MuRGAt基准，要求模型对跨模态（如视频、音频）输入生成带显式推理链和精确引用（含模态类型与时间片段）的答案；并设计一种与人工评分强相关的自动评估框架。

Result: 实验表明，即使强大多模态大语言模型也常出现引用幻觉；且推理深度增加或强制结构化归因会降低答案准确性，暴露出内部推理与外部可验证归因之间存在显著鸿沟。

Conclusion: MuRGAt填补了复杂多模态推理中事实归因评测的空白，揭示了当前MLLMs在可信归因方面的重要缺陷，为未来提升模型可靠性提供了新方向和量化工具。

Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.

</details>


### [113] [Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm](https://arxiv.org/abs/2602.11543)
*Jinrui Zhang,Chaodong Xiao,Aoqi Wu,Xindong Zhang,Lei Zhang*

Main category: cs.CL

TL;DR: 本文提出SPES框架，一种内存高效的去中心化训练方法，用于预训练混合专家（MoE）大语言模型，通过在各节点仅训练部分专家并周期性同步，显著降低GPU内存需求，并在有限资源下实现与中心化训练相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化训练方法仍受限于单节点GPU内存，无法支持大规模LLM预训练；需突破内存瓶颈以实现真正分布式、低成本的大模型训练。

Method: 提出SParse Expert Synchronization (SPES)框架：1）各节点仅训练局部子集专家；2）异步/周期性专家参数同步，避免全参数传输；3）引入专家合并热身策略，加速早期知识融合。

Result: 在16块48GB GPU（互联网连接）上成功预训练2B MoE模型，性能媲美同算力中心化训练；进一步扩展至7B从头训练和9B由稠密模型升级的MoE模型，均达到先前中心化基线水平。

Conclusion: SPES有效解耦模型规模与单节点内存限制，为资源受限环境下的大模型去中心化预训练提供了可行、可扩展的新范式。

Abstract: Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.

</details>


### [114] [SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent](https://arxiv.org/abs/2602.11551)
*Wenlin Zhong,Jinluan Yang,Yiquan Wu,Yi Liu,Jianhang Yao,Kun Kuang*

Main category: cs.CL

TL;DR: 本文提出SIGHT框架，通过自证支持（SES）和信息增益驱动的多样化分支策略，提升大语言模型在多轮搜索问答中的推理能力，有效缓解冗余与噪声问题，减少错误累积，并在单跳与多跳问答任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多轮搜索场景下，检索结果冗余高、信噪比低，导致智能体陷入“隧道视野”，早期噪声引发不可逆的错误累积。

Method: 提出SIGHT框架，包含：1）自证支持（SES）提炼高保真证据；2）基于信息增益评分识别关键状态并触发动态提示干预（去重、反思或自适应分支）；3）结合SES与正确性奖励，采用组相对策略优化（GRPO）内化鲁棒探索策略。

Result: 在单跳与多跳问答基准上显著超越现有方法，尤其在复杂推理任务中表现更优，且使用更少搜索步数。

Conclusion: SIGHT通过融合证据质量评估与信息驱动的探索控制，提升了LLM在自主搜索推理中的鲁棒性与效率，无需外部验证器即可实现高质量多轮搜索。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search results often suffer from high redundancy and low signal-to-noise ratios. Consequently, agents easily fall into "Tunnel Vision," where the forced interpretation of early noisy retrievals leads to irreversible error accumulation. To address these challenges, we propose SIGHT, a framework that enhances search-based reasoning through Self-Evidence Support (SES) and Information-Gain Driven Diverse Branching. SIGHT distills search results into high-fidelity evidence via SES and calculates an Information Gain score to pinpoint pivotal states where observations maximally reduce uncertainty. This score guides Dynamic Prompting Interventions - including de-duplication, reflection, or adaptive branching - to spawn new branches with SES. Finally, by integrating SES and correctness rewards via Group Relative Policy Optimization, SIGHT internalizes robust exploration strategies without external verifiers. Experiments on single-hop and multi-hop QA benchmarks demonstrate that SIGHT significantly outperforms existing approaches, particularly in complex reasoning scenarios, using fewer search steps.

</details>


### [115] [PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering](https://arxiv.org/abs/2602.11570)
*Xiangfeng Wang,Hangyu Guo,Yanlin Lai,Mitt Huang,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Xiaoxiao Ren,Chun Yuan,Tong Xu,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 本文提出PRIME基准，用于评估数学与工程领域中验证器在过程-结果一致性上的能力，并基于此设计过程感知的RLVR训练范式，显著提升模型性能，且验证器在PRIME上的准确率可有效预测其在RLVR中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果的验证范式忽视推导过程中的错误，导致对错误推导得出的正确答案给予正向奖励，影响RLVR的可靠性。

Method: 构建PRIME基准（2530道高难度STEM题目），提出过程感知的RLVR训练范式，并通过PRIME筛选验证器；进行大规模评测与相关性分析。

Result: 当前验证器常无法检测推导错误；新范式在AIME24、AIME25和Beyond-AIME上分别带来8.29%、9.12%、7.31%的绝对性能提升；PRIME准确率与RLVR效果呈强线性相关（R² > 0.92）。

Conclusion: PRIME有效弥补过程验证缺失，是评估与选择验证器的可靠基准，过程感知训练范式显著增强RLVR效果。

Abstract: While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often neglecting potential errors in the derivation process. This leads to assigning positive rewards to correct answers produced from incorrect derivations. To bridge this gap, we introduce PRIME, a benchmark for evaluating verifiers on Process-Outcome Alignment verification in Mathematics and Engineering. Curated from a comprehensive collection of college-level STEM problems, PRIME comprises 2,530 high-difficulty samples through a consistency-based filtering pipeline. Through extensive evaluation, we find that current verifiers frequently fail to detect derivation flaws. Furthermore, we propose a process-aware RLVR training paradigm utilizing verifiers selected via PRIME. This approach substantially outperforms the outcome-only verification baseline, achieving absolute performance gains of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME, respectively, for the Qwen3-14B-Base model. Finally, we demonstrate a strong linear correlation ($R^2 > 0.92$) between verifier accuracy on PRIME and RLVR training effectiveness, validating PRIME as a reliable predictor for verifier selection.

</details>


### [116] [Scene-Aware Memory Discrimination: Deciding Which Personal Knowledge Stays](https://arxiv.org/abs/2602.11607)
*Yijie Zhong,Mengying Guo,Zewei Wang,Zhongyang Li,Dandan Tu,Haofen Wang*

Main category: cs.CL

TL;DR: 本文提出了一种场景感知的记忆判别方法（SAMD），通过门控单元模块（GUM）和聚类提示模块（CPM）提升大语言模型在个人知识记忆写入与管理中的效率与准确性，解决了信息过滤难和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的记忆写入、管理和读取研究面临难以过滤无关信息及计算成本上升的挑战，而人类选择性注意机制为解决该问题提供了启发。

Method: 提出场景感知记忆判别任务，并设计SAMD方法，包含两个核心模块：GUM用于过滤非记忆性交互、聚焦关键内容；CPM建立自适应记忆标准并生成面向用户意图与记忆上下文的聚类提示。

Result: 实验表明SAMD在记忆判别任务中能成功召回大多数可记忆数据，在动态场景下保持鲁棒性；集成到个性化应用中显著提升了记忆构建的效率与质量。

Conclusion: SAMD通过模拟人类选择性注意机制，有效提升了大规模用户交互数据中个人知识记忆的组织能力，具有良好的泛化性与实用性。

Abstract: Intelligent devices have become deeply integrated into everyday life, generating vast amounts of user interactions that form valuable personal knowledge. Efficient organization of this knowledge in user memory is essential for enabling personalized applications. However, current research on memory writing, management, and reading using large language models (LLMs) faces challenges in filtering irrelevant information and in dealing with rising computational costs. Inspired by the concept of selective attention in the human brain, we introduce a memory discrimination task. To address large-scale interactions and diverse memory standards in this task, we propose a Scene-Aware Memory Discrimination method (SAMD), which comprises two key components: the Gating Unit Module (GUM) and the Cluster Prompting Module (CPM). GUM enhances processing efficiency by filtering out non-memorable interactions and focusing on the salient content most relevant to application demands. CPM establishes adaptive memory standards, guiding LLMs to discern what information should be remembered or discarded. It also analyzes the relationship between user intents and memory contexts to build effective clustering prompts. Comprehensive direct and indirect evaluations demonstrate the effectiveness and generalization of our approach. We independently assess the performance of memory discrimination, showing that SAMD successfully recalls the majority of memorable data and remains robust in dynamic scenarios. Furthermore, when integrated into personalized applications, SAMD significantly enhances both the efficiency and quality of memory construction, leading to better organization of personal knowledge.

</details>


### [117] [PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning](https://arxiv.org/abs/2602.11639)
*Ruixiang Feng,Yuntao Wen,Silin Zhou,Ke Shi,Yifan Wang,Ran Le,Zhenwei An,Zongchao Chen,Chen Yang,Guangyue Peng,Yiming Jia,Dongsheng Wang,Tao Zhang,Lisi Chen,Yang Song,Shen Gao,Shuo Shang*

Main category: cs.CL

TL;DR: 本文提出了一种双层次压缩框架\model，通过前缀保护和难度感知的策略，在保持推理有效性的同时显著减少语言推理模型（LRMs）的推理长度，从而降低延迟与内存开销，并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有语言推理模型（LRMs）存在“过度思考”问题，即生成过长推理链，导致延迟和内存开销增加；而统一长度惩罚策略在序列层面会压缩关键早期推理步骤，在群体层面则对所有查询一视同仁，缺乏适应性。

Method: 提出双层次框架\model：序列层面采用前缀保护优化（使用衰减混合rollout保证有效推理路径）；群体层面采用难度感知惩罚（根据查询复杂度动态调整长度约束）。

Result: 在DeepSeek-R1-Distill-Qwen（1.5B/7B）上实验表明，\model最多减少55.7% token使用量，同时数学基准准确率最高提升4.1%，并可泛化至代码、科学和通用领域。

Conclusion: 双层次压缩策略能兼顾推理质量与效率，为LRMs的实用化提供了新思路。

Abstract: Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking'', producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \textbf{\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B/7B) demonstrate that \model achieves a substantial reduction in token usage (up to \textbf{55.7\%}) while simultaneously improving accuracy (up to \textbf{4.1\%}) on math benchmarks, with generalization ability to code, science, and general domains.

</details>


### [118] [Which Feedback Works for Whom? Differential Effects of LLM-Generated Feedback Elements Across Learner Profiles](https://arxiv.org/abs/2602.11650)
*Momoka Furuhashi,Kouta Nakayama,Noboru Kawai,Takashi Kodama,Saku Sugawara,Kyosuke Takami*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）生成的教育反馈中不同元素（如语气、信息覆盖度）对学习效果和学生接受度的影响，特别关注学生大五人格特质的调节作用；通过对321名高一学生开展实验，发现反馈有效性存在共性模式，但主观偏好因人格类型而异，强调需依据人格特质个性化设计LLM反馈。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚LLM生成反馈中的具体元素（如语气、信息覆盖）如何影响不同人格特质学生的学习效果与接受度。

Method: 定义六类反馈元素，利用GPT-5为多项选择题生成反馈；开展含321名高一学生的实验，采用两类学习成效指标与六项主观评价标准评估反馈效果；进一步按大五人格特质聚类分析反馈接受度差异。

Result: 有效反馈元素存在支持学习成效的共性模式；但学生对反馈的主观偏好显著因人格特质聚类而异。

Conclusion: 在设计LLM生成反馈时，应依据学习者人格特质选择和适配反馈元素，为教育中个性化反馈提供实践指导。

Abstract: Large language models (LLMs) show promise for automatically generating feedback in education settings. However, it remains unclear how specific feedback elements, such as tone and information coverage, contribute to learning outcomes and learner acceptance, particularly across learners with different personality traits. In this study, we define six feedback elements and generate feedback for multiple-choice biology questions using GPT-5. We conduct a learning experiment with 321 first-year high school students and evaluate feedback effectiveness using two learning outcomes measures and subjective evaluations across six criteria. We further analyze differences in how feedback acceptance varies across learners based on Big Five personality traits. Our results show that effective feedback elements share common patterns supporting learning outcomes, while learners' subjective preferences differ across personality-based clusters. These findings highlight the importance of selecting and adapting feedback elements according to learners' personality traits when we design LLM-generated feedback, and provide practical implications for personalized feedback design in education.

</details>


### [119] [PatientHub: A Unified Framework for Patient Simulation](https://arxiv.org/abs/2602.11684)
*Sahand Sabour,TszYam NG,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了PatientHub，一个用于标准化模拟患者对话的统一、模块化框架，旨在解决现有患者模拟方法在数据格式、提示词和评估指标上的不一致性问题，提升可复现性与跨方法比较能力。


<details>
  <summary>Details</summary>
Motivation: 现有患者模拟工作碎片化，缺乏统一的数据格式、提示设计和评估标准，导致难以复现和公平比较。

Method: 设计并实现了一个名为PatientHub的模块化框架，支持模拟患者的定义、组合与部署；集成多种代表性模拟方法作为案例，并支持自定义评估指标和新变体的快速原型开发。

Result: 成功实现了多个患者模拟方法的标准化部署与跨方法评估；验证了框架的可扩展性，快速原型出两种新模拟器变体；显著降低了新方法开发的基础设施开销。

Conclusion: PatientHub为患者中心对话领域提供了可复现、易扩展的基础框架，推动未来数据集、方法和基准的协同发展。

Abstract: As Large Language Models increasingly power role-playing applications, simulating patients has become a valuable tool for training counselors and scaling therapeutic assessment. However, prior work is fragmented: existing approaches rely on incompatible, non-standardized data formats, prompts, and evaluation metrics, hindering reproducibility and fair comparison. In this paper, we introduce PatientHub, a unified and modular framework that standardizes the definition, composition, and deployment of simulated patients. To demonstrate PatientHub's utility, we implement several representative patient simulation methods as case studies, showcasing how our framework supports standardized cross-method evaluation and the seamless integration of custom evaluation metrics. We further demonstrate PatientHub's extensibility by prototyping two new simulator variants, highlighting how PatientHub accelerates method development by eliminating infrastructure overhead. By consolidating existing work into a single reproducible pipeline, PatientHub lowers the barrier to developing new simulation methods and facilitates cross-method and cross-model benchmarking. Our framework provides a practical foundation for future datasets, methods, and benchmarks in patient-centered dialogue, and the code is publicly available via https://github.com/Sahandfer/PatientHub.

</details>


### [120] [Finding Sense in Nonsense with Generated Contexts: Perspectives from Humans and Language Models](https://arxiv.org/abs/2602.11699)
*Katrin Olsen,Sebastian Padó*

Main category: cs.CL

TL;DR: 本文通过人类评分员和大语言模型（LLM）对五个语义异常数据集中的句子进行可理解性判断，发现多数句子仅属异常而非真正无意义，并且LLMs能为异常句子生成合理上下文。


<details>
  <summary>Details</summary>
Motivation: 现有语义异常数据集中哪些句子真正‘无意义’尚不明确，且不清楚大语言模型能否准确区分‘异常’与‘无意义’。

Method: 收集人类评分员和大语言模型对五个语义偏离数据集中的句子（无上下文及提供上下文两种情形）的可理解性判断。

Result: 人类评分员判定大多数句子最多仅为异常，仅有少数为真正无意义；LLMs在为异常句子生成合理上下文方面表现出较强能力。

Conclusion: 当前语义异常数据集大多并不真正‘无意义’，而大语言模型已具备较强上下文补全与语义合理性判断能力。

Abstract: Nonsensical and anomalous sentences have been instrumental in the development of computational models of semantic interpretation. A core challenge is to distinguish between what is merely anomalous (but can be interpreted given a supporting context) and what is truly nonsensical. However, it is unclear (a) how nonsensical, rather than merely anomalous, existing datasets are; and (b) how well LLMs can make this distinction. In this paper, we answer both questions by collecting sensicality judgments from human raters and LLMs on sentences from five semantically deviant datasets: both context-free and when providing a context. We find that raters consider most sentences at most anomalous, and only a few as properly nonsensical. We also show that LLMs are substantially skilled in generating plausible contexts for anomalous cases.

</details>


### [121] [Thinking with Drafting: Optical Decompression via Logical Reconstruction](https://arxiv.org/abs/2602.11731)
*Jingxuan Wei,Honghao He,Caijun Jia,Siyuan Li,Zheng Sun,Yuhang Xu,Yuanyuan Lin,Linzhuang Sun,Yuchen Wu,Bihui Yu,Xiangxiang Zhang,Cheng Tan*

Main category: cs.CL

TL;DR: 本文提出Thinking with Drafting (TwD)方法，将视觉推理重构为光学解压缩过程，利用领域特定语言（DSL）作为中间表示，通过生成可执行代码实现逻辑验证，并在VisAlg视觉代数基准上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在高保真视觉感知和生成方面表现优异，但在复杂推理任务中存在精度悖论：光学感知系统仅转录符号而忽略逻辑结构，像素级生成模型则产生缺乏数学精确性的视觉伪影。

Method: 提出Thinking with Drafting (TwD)框架，以‘解析即推理’为公理，采用极简领域特定语言（DSL）作为中间表示，强制模型将思维过程草拟为可执行代码，从而生成确定性视觉证明用于自验证。

Result: 在新构建的VisAlg视觉代数基准上实验表明，TwD显著提升视觉推理性能，形成以视觉生成为逻辑验证工具的闭环系统。

Conclusion: TwD为视觉推理提供了一种通用、可扩展的认知架构，将视觉生成从创造性输出转变为逻辑验证机制，有效弥合感知与推理之间的鸿沟。

Abstract: Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.

</details>


### [122] [Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning](https://arxiv.org/abs/2602.11748)
*Futing Wang,Jianhao Yan,Yun Luo,Ganqu Cui,Zhi Wang,Xiaoye Qu,Yue Zhang,Yu Cheng,Tao Lin*

Main category: cs.CL

TL;DR: 本文提出Length-Incentivized Exploration（LIE）方法，通过长度奖励与冗余惩罚协同优化，缓解大模型在上下文内探索中因自回归生成导致的‘浅层探索陷阱’，提升状态覆盖率，从而增强测试时缩放能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在上下文内探索（In-Context Exploration）中受限于自回归生成的指数衰减特性，难以生成足够长、多样化的推理轨迹，导致状态覆盖率低，形成‘浅层探索陷阱’。

Method: 提出Length-Incentivized Exploration（LIE）：在推理阶段引入基于生成长度的奖励函数，并叠加冗余惩罚项，以两步方式最大化状态覆盖；无需修改模型结构或训练过程，适用于Qwen3、Llama等主流模型。

Result: 在多个模型（Qwen3、Llama）上验证有效，平均提升域内任务性能4.4%，域外任务2.7%。

Conclusion: 显式鼓励更长、更少冗余的推理轨迹可显著增强模型的上下文内探索能力，是实现高效测试时缩放的关键路径。

Abstract: Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.
  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.
  To bridge this gap, we propose Length-Incentivized Exploration(\method).
  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.
  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.
  As a result, our method achieves an average improvement of 4.4\% on in-domain tasks and a 2.7\% gain on out-of-domain benchmarks.

</details>


### [123] [MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling](https://arxiv.org/abs/2602.11761)
*MiniCPM Team,Wenhao An,Yingfa Chen,Yewei Fang,Jiayi Li,Xin Li,Yaohui Li,Yishan Li,Yuxuan Li,Biyuan Lin,Chuan Liu,Hezi Liu,Siyuan Liu,Hongya Lyu,Yinxu Pan,Shixin Ren,Xingyu Shen,Zhou Su,Haojun Sun,Yangang Sun,Zhen Leng Thai,Xin Tian,Rui Wang,Xiaorong Wang,Yudong Wang,Bo Wu,Xiaoyue Xu,Dong Xu,Shuaikang Xue,Jiawei Yang,Bowen Zhang,Jinqian Zhang,Letian Zhang,Shengnan Zhang,Xinyu Zhang,Xinyuan Zhang,Zhu Zhang,Hengyu Zhao,Jiacheng Zhao,Jie Zhou,Zihan Zhou,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出MiniCPM-SALA，一种9B参数的混合注意力架构，结合稀疏注意力（InfLLM-V2）与线性注意力（Lightning Attention），通过层选择算法和混合位置编码（HyPE）在保持长上下文建模能力的同时显著提升推理效率与内存可扩展性，并设计低成本持续训练框架，训练成本降低约75%。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在超长上下文应用中面临高昂的计算与内存开销，现有稀疏/线性注意力方法常以性能为代价换取效率，亟需兼顾二者的新架构。

Method: 提出MiniCPM-SALA混合架构：1）采用1:3比例的层选择算法融合InfLLM-V2（稀疏）与Lightning Attention（线性）；2）引入混合位置编码HyPE；3）设计基于预训练模型的低成本持续训练框架。

Result: 在单张A6000D GPU上，256K序列长度下推理速度达全注意力模型的3.5倍；支持最长1M token上下文（传统8B全注意力模型在此尺度下因内存不足失效）；通用能力与全注意力模型相当，训练成本降低约75%。

Conclusion: MiniCPM-SALA成功在不显著牺牲性能的前提下大幅提升长上下文推理效率与可扩展性，验证了混合注意力架构及低成本适配策略的有效性与实用性。

Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.

</details>


### [124] [A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments](https://arxiv.org/abs/2602.11795)
*Anne-Marie Lutgen,Alistair Plum,Christoph Purschke*

Main category: cs.CL

TL;DR: 本文提出了一种基于子词嵌入的变体检测方法，无需预归一化或预定义变体列表，通过结合余弦相似度与n-gram相似度对相关形式聚类，在卢森堡语用户评论大数据集上成功揭示了符合方言学和社会语言学规律的词汇与正字法变异。


<details>
  <summary>Details</summary>
Motivation: 在低资源或‘噪声’较大的语言环境下（如小语种、多语种场景），传统依赖预定义变体列表或人工归一化的方法难以适用，亟需一种不依赖先验知识、可复现的变体发现方法。

Method: 在原始文本上训练子词嵌入，再结合余弦相似度和n-gram相似度对词语形式进行聚类，从而识别拼写与形态变体；以卢森堡语用户评论大语料库为实验数据。

Result: 成功发现大量符合方言学和社会语言学描述的系统性词汇与正字法变异，生成的变体簇具有可解释性，支持定量与定性分析，并验证了分布建模在低资源语言中揭示语言变异的有效性。

Conclusion: 分布语义建模可用于自动、透明地揭示语言变体结构，为小语种及多语种环境下的语言多样性研究提供了可复现的方法论框架。

Abstract: This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in ''noisy'' or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.

</details>


### [125] [DMAP: A Distribution Map for Text](https://arxiv.org/abs/2602.11871)
*Tom Kempton,Julia Rozanova,Parameswaran Kamalaruban,Maeve Madigan,Karolina Wresilo,Yoann L. Launay,David Sutton,Stuart Burrell*

Main category: cs.CL

TL;DR: 本文提出DMAP方法，通过将文本映射为单位区间内的样本集，联合编码词序与概率信息，实现对大语言模型输出的更精细、上下文感知的统计分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的文本分析方法（如困惑度）未能充分考虑上下文对下一个词概率解释的影响，即概率需结合条件分布的形状（合理候选词数量）来理解。

Method: 提出DMAP（Distribution Mapping for Analysis of Probabilities）方法：利用LLM生成的下一个词概率分布，将其数学地映射为单位区间[0,1]内的一组样本，该映射同时保留词汇排名（rank）与对应概率（probability）信息，从而形成一种模型无关、可高效计算的文本表示。

Result: 在三个案例中验证了DMAP的有效性：(i) 生成参数校验以保障数据完整性；(ii) 揭示概率曲率在机器生成文本检测中的关键作用；(iii) 发现下游模型经合成数据微调后留下的可识别统计指纹。实验表明DMAP可在消费级硬件上快速计算，具有广泛适用性。

Conclusion: DMAP提供了一种统一、简洁且理论严谨的文本统计表征方式，不仅提升了对LLM输出的理解与分析能力，也为未来基于LLM的文本分析研究奠定了新基础。

Abstract: Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity, which do not adequately account for context; how one should interpret a given next-token probability is dependent on the number of reasonable choices encoded by the shape of the conditional distribution. In this work, we present DMAP, a mathematically grounded method that maps a text, via a language model, to a set of samples in the unit interval that jointly encode rank and probability information. This representation enables efficient, model-agnostic analysis and supports a range of applications. We illustrate its utility through three case studies: (i) validation of generation parameters to ensure data integrity, (ii) examining the role of probability curvature in machine-generated text detection, and (iii) a forensic analysis revealing statistical fingerprints left in downstream models that have been subject to post-training on synthetic data. Our results demonstrate that DMAP offers a unified statistical view of text that is simple to compute on consumer hardware, widely applicable, and provides a foundation for further research into text analysis with LLMs.

</details>


### [126] [Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems](https://arxiv.org/abs/2602.11877)
*Wanxing Wu,He Zhu,Yixia Li,Lei Yang,Jiehui Zhao,Hongru Wang,Jian Yang,Benyou Wang,Bingyi Jing,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出RouterXBench评估框架和ProbeDirichlet轻量级路由器，利用模型内部隐藏状态建模不确定性，提升本地-云端LLM协同中的路由准确性和跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由机制评估不系统，忽视场景适配性与分布外鲁棒性；同时依赖输出概率或外部嵌入，未能有效利用模型内部不确定性信号。

Method: 构建三维度评估框架RouterXBench（路由能力、场景对齐、跨域鲁棒性）；提出ProbeDirichlet路由器，基于可学习的狄利克雷分布聚合多层隐藏状态，并采用概率化训练。

Result: ProbeDirichlet在路由能力和高精度场景下分别比最优基线提升16.68%和18.86%，且在不同模型族、规模、异构任务及智能体工作流中表现稳定。

Conclusion: 利用内部隐藏状态建模不确定性是提升LLM路由性能的有效路径，RouterXBench为未来路由器设计与评估提供了系统化基准。

Abstract: Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking scenario-specific requirements and out-of-distribution robustness. We propose RouterXBench, a principled evaluation framework with three dimensions: router ability, scenario alignment, and cross-domain robustness. Unlike prior work that relies on output probabilities or external embeddings, we utilize internal hidden states that capture model uncertainty before answer generation. We introduce ProbeDirichlet, a lightweight router that aggregates cross-layer hidden states via learnable Dirichlet distributions with probabilistic training. Trained on multi-domain data, it generalizes robustly across in-domain and out-of-distribution scenarios. Our results show ProbeDirichlet achieves 16.68% and 18.86% relative improvements over the best baselines in router ability and high-accuracy scenarios, with consistent performance across model families, model scales, heterogeneous tasks, and agentic workflows.

</details>


### [127] [LLM-based Triplet Extraction from Financial Reports](https://arxiv.org/abs/2602.11886)
*Dante Wesslund,Ville Stenström,Pontus Linde,Alexander Holmberg*

Main category: cs.CL

TL;DR: 本文提出了一种基于本体驱动代理指标（本体一致性与忠实性）的半自动化三元组抽取流程，用于企业财务报告知识图谱构建，避免依赖标注真值；自动诱导本体在所有配置下实现100%模式一致性，并通过混合验证策略将主语幻觉率从65.2%降至1.6%。


<details>
  <summary>Details</summary>
Motivation: 企业财务报告是构建知识图谱的重要结构化知识源，但该领域缺乏标注真值，导致评估困难。

Method: 提出半自动化三元组抽取流程，采用本体驱动的代理评估指标（Ontology Conformance 和 Faithfulness）；对比静态人工本体与全自动文档特定本体诱导方法；结合正则匹配与LLM-as-a-judge进行混合验证；分析主语与宾语幻觉的系统性不对称现象。

Result: 自动诱导本体实现100%本体一致性，消除了人工本体中的本体漂移；混合验证策略将主语幻觉率从65.2%显著降低至1.6%；发现主语与宾语幻觉存在系统性不对称，归因于财务文本中的被动语态和施事省略。

Conclusion: 本体驱动的代理评估与自动本体诱导可有效替代真值依赖评估；混合验证策略大幅提升抽取可靠性；对幻觉模式的分析为金融文本NLP提供了重要语言学洞见。

Abstract: Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.

</details>


### [128] [Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences](https://arxiv.org/abs/2602.11898)
*Eddie Yang,Dashun Wang*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型（LLMs）在基准测试中准确率趋同的表象下，存在显著的认知分歧（epistemic divergence），即模型间对同一问题的答案高度不一致；这种分歧导致在科学数据标注与推断中产生严重偏差，甚至逆转研究结论，形成‘基准幻觉’，威胁科研可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估过度依赖整体准确率，忽视模型间答案分歧及其对下游科学研究的实际影响，亟需揭示准确率掩盖下的认知差异与潜在风险。

Method: 基于MMLU-Pro和GPQA两大推理基准，量化前沿LLM之间的答案一致性；进一步在教育学与政治学已发表研究的再分析中，实证检验不同LLM作为标注器对因果效应估计的影响。

Result: LLM在相同基准上准确率相近，但答案一致率仅34%-84%；顶级模型间仍有16%-38%分歧；更换标注模型可使处理效应估计变化超80%，甚至符号反转。

Conclusion: 基准准确率不能保证模型认知一致性；模型选择本身已成为影响科学结论可靠性的隐性关键变量，需发展能反映认知一致性的新评估范式。

Abstract: Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro and GPQA - we show that LLMs achieving comparable accuracy still disagree on 16-66% of items, and 16-38% among top-performing frontier models. These discrepancies suggest distinct error profiles for different LLMs. When such models are used for scientific data annotation and inference, their hidden disagreements propagate into research results: in re-analyses of published studies in education and political science, switching the annotation model can change estimated treatment effects by more than 80%, and in some cases reverses their sign. Together, these findings illustrate a benchmark illusion, where equal accuracy may conceal disagreement, with model choice becoming a hidden yet consequential variable for scientific reproducibility.

</details>


### [129] [AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection](https://arxiv.org/abs/2602.11931)
*Pretam Ray,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 本文提出AdaptEvolve方法，利用生成置信度动态选择适合当前推理步骤的LLM，在保证高准确率的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 进化式智能体系统中反复调用大语言模型导致计算效率与推理能力之间的权衡加剧，亟需一种能根据当前步骤需求动态选择合适LLM的方法。

Method: 提出AdaptEvolve框架，在多LLM进化精炼过程中，利用生成过程中的内在置信度实时估计问题可解性，并据此自适应选择LLM；采用基于置信度的路由策略，替代静态启发式或外部控制器。

Result: 在多个基准测试中，该方法平均降低37.9%总推理成本，同时保持静态大模型基线97.5%的上限准确率，形成更优的Pareto前沿。

Conclusion: 基于内在生成置信度的动态LLM选择是提升进化式智能体系统效率与性能平衡的有效途径，AdaptEvolve为多模型协同推理提供了新范式。

Abstract: Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the current generation step while remaining computationally efficient? While model cascades offer a practical mechanism for balancing this trade-off, existing routing strategies typically rely on static heuristics or external controllers and do not explicitly account for model uncertainty. We introduce AdaptEvolve: Adaptive LLM Selection for Multi-LLM Evolutionary Refinement within an evolutionary sequential refinement framework that leverages intrinsic generation confidence to estimate real-time solvability. Empirical results show that confidence-driven selection yields a favourable Pareto frontier, reducing total inference cost by an average of 37.9% across benchmarks while retaining 97.5% of the upper-bound accuracy of static large-model baselines. Our code is available at https://github.com/raypretam/adaptive_llm_selection.

</details>


### [130] [Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text](https://arxiv.org/abs/2602.11933)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文提出跨模态鲁棒性迁移（CMRT）框架，将文本模态的对抗鲁棒性迁移到语音模态，无需生成对抗语音数据，显著提升端到端语音翻译模型对形态变化攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音翻译模型在干净数据上表现良好，但面对真实场景中非母语或方言语音的屈折形态变化时鲁棒性不足；生成高质量对抗语音数据成本高、难度大。

Method: 设计面向语音域的屈折形态对抗攻击，并提出CMRT框架，通过跨模态知识迁移，将文本对抗训练所得鲁棒性迁移到语音翻译模型，避免使用对抗语音数据进行训练。

Result: 在四个语言对上的实验表明，CMRT平均提升对抗鲁棒性超3 BLEU分，显著优于基线，且不依赖对抗语音数据生成。

Conclusion: CMRT为构建鲁棒的端到端语音翻译系统提供了高效可行的新范式，验证了跨模态鲁棒性迁移的有效性。

Abstract: End-to-End Speech Translation (E2E-ST) has seen significant advancements, yet current models are primarily benchmarked on curated, "clean" datasets. This overlooks critical real-world challenges, such as morphological robustness to inflectional variations common in non-native or dialectal speech. In this work, we adapt a text-based adversarial attack targeting inflectional morphology to the speech domain and demonstrate that state-of-the-art E2E-ST models are highly vulnerable it. While adversarial training effectively mitigates such risks in text-based tasks, generating high-quality adversarial speech data remains computationally expensive and technically challenging. To address this, we propose Cross-Modal Robustness Transfer (CMRT), a framework that transfers adversarial robustness from the text modality to the speech modality. Our method eliminates the requirement for adversarial speech data during training. Extensive experiments across four language pairs demonstrate that CMRT improves adversarial robustness by an average of more than 3 BLEU points, establishing a new baseline for robust E2E-ST without the overhead of generating adversarial speech.

</details>


### [131] [Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance](https://arxiv.org/abs/2602.11938)
*Yunchong Huang,Gianni Barlacchi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在问答任务中表现不佳的一个潜在原因：问题表述不明确（underspecified questions）。作者提出了一种基于LLM的分类器来识别此类问题，并发现多个主流QA数据集中有16%至50%以上的问题存在该问题；进一步通过重写实验验证，明确化问题后模型性能显著提升，说明部分‘失败’实为问题设计缺陷而非模型能力不足。


<details>
  <summary>Details</summary>
Motivation: 标准问答基准尚未被大语言模型完全解决，而其在良定义问题上表现良好，作者推测这中间的性能差距部分源于问题本身表述不明确，即缺乏足够上下文导致歧义。

Method: 构建一个基于大语言模型的分类器自动识别问答数据集中的underspecified问题；在多个主流QA数据集（如SQuAD、Natural Questions等）上进行检测统计；开展控制变量的重写实验——将underspecified问题人工/半自动改写为明确版本，同时保持标准答案不变，评估模型性能变化。

Result: 发现16%–50%+的基准问题属于underspecified；LLMs在这些题目上的准确率显著更低；重写后所有测试模型的QA性能均稳定提升，验证了问题明确性对评估结果的关键影响。

Conclusion: 问题 underspecification 是当前QA评测中一个被忽视但关键的混杂因素；应重视问题表述的清晰性与无歧义性，在未来基准构建中纳入相关质量控制机制。

Abstract: Large language models (LLMs) perform well on well-posed questions, yet standard question-answering (QA) benchmarks remain far from solved. We argue that this gap is partly due to underspecified questions - queries whose interpretation cannot be uniquely determined without additional context. To test this hypothesis, we introduce an LLM-based classifier to identify underspecified questions and apply it to several widely used QA datasets, finding that 16% to over 50% of benchmark questions are underspecified and that LLMs perform significantly worse on them. To isolate the effect of underspecification, we conduct a controlled rewriting experiment that serves as an upper-bound analysis, rewriting underspecified questions into fully specified variants while holding gold answers fixed. QA performance consistently improves under this setting, indicating that many apparent QA failures stem from question underspecification rather than model limitations. Our findings highlight underspecification as an important confound in QA evaluation and motivate greater attention to question clarity in benchmark design.

</details>


### [132] [Do Large Language Models Adapt to Language Variation across Socioeconomic Status?](https://arxiv.org/abs/2602.11939)
*Elisa Bassignana,Mike Zhang,Dirk Hovy,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在不同社会经济地位（SES）社群中的语言风格适应能力，发现其仅能微弱调整风格，且更倾向于模仿高SES语言，可能加剧语言不平等与社会分层。


<details>
  <summary>Details</summary>
Motivation: 随着LLM日益介入人际沟通，其对多元语言风格（尤其是不同SES群体）的适应不足可能强化刻板印象、边缘化语言规范未被充分建模的社群，并加剧社会不平等。

Method: 构建来自Reddit和YouTube、按SES分层的新数据集；以其中不完整文本为提示，驱动4个LLM生成续写；对比生成文本与原始文本在94个社会语言学指标（句法、修辞、词汇等）上的差异。

Result: LLM仅轻微调节语言风格以适配SES，常出现近似或刻板化表达；对高SES语言风格的模仿显著优于低SES；整体呈现向上偏移的语言偏好。

Conclusion: LLM当前存在系统性语言风格偏差，不仅可能放大既有语言等级秩序，也质疑其在基于代理的社会模拟、调查实验及依赖语言风格作为社会信号的研究中的适用性。

Abstract: Humans adjust their linguistic style to the audience they are addressing. However, the extent to which LLMs adapt to different social contexts is largely unknown. As these models increasingly mediate human-to-human communication, their failure to adapt to diverse styles can perpetuate stereotypes and marginalize communities whose linguistic norms are less closely mirrored by the models, thereby reinforcing social stratification. We study the extent to which LLMs integrate into social media communication across different socioeconomic status (SES) communities. We collect a novel dataset from Reddit and YouTube, stratified by SES. We prompt four LLMs with incomplete text from that corpus and compare the LLM-generated completions to the originals along 94 sociolinguistic metrics, including syntactic, rhetorical, and lexical features. LLMs modulate their style with respect to SES to only a minor extent, often resulting in approximation or caricature, and tend to emulate the style of upper SES more effectively. Our findings (1) show how LLMs risk amplifying linguistic hierarchies and (2) call into question their validity for agent-based social simulation, survey experiments, and any research relying on language style as a social signal.

</details>


### [133] [Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models](https://arxiv.org/abs/2602.11961)
*Yuzhe Shang,Pengzhi Gao,Wei Liu,Jian Luan,Jinsong Su*

Main category: cs.CL

TL;DR: 本文研究了开源大语言模型（LLMs）在多语言机器翻译（MT）任务中的性能，基于Gemma3模型家族开发了支持46种语言的MiLMMT-46模型，在多项评测中超越多个SOTA开源模型，并媲美Google Translate和Gemini 3 Pro等闭源系统。


<details>
  <summary>Details</summary>
Motivation: 开源LLM多语言能力持续提升，但其在多语言机器翻译任务上的系统性评估与优化策略（如模型与数据缩放）仍缺乏深入研究。

Method: 通过持续预训练（continual pretraining）和指令微调（instruction finetuning）两种方式，结合模型缩放与数据缩放策略，在Gemma3模型家族基础上构建多语言机器翻译模型MiLMMT-46。

Result: MiLMMT-46在46种语言上实现顶尖多语言翻译性能，持续优于Seed-X、HY-MT-1.5、TranslateGemma等最新开源SOTA模型，并与Google Translate、Gemini 3 Pro等强闭源系统性能相当。

Conclusion: 模型与数据缩放对提升开源LLM的多语言MT能力具有显著效果；MiLMMT-46验证了开源模型在高质量多语言翻译任务中具备与顶级商业系统竞争的潜力。

Abstract: Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.

</details>


### [134] [DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling](https://arxiv.org/abs/2602.11968)
*Mariia Fedorova,Andrey Kutuzov,Khonzoda Umarova*

Main category: cs.CL

TL;DR: 本文介绍了DHPLT，一个包含41种语言的历时语料库开放集合，旨在填补多语言历时语料在语义变化建模中的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏面向语义变化建模的多语言历时语料库（仅限十余种高资源语言），本文旨在填补这一空白。

Method: 基于网络爬取的HPLT数据集，利用网页爬取时间戳作为文档生成时间的近似信号，构建覆盖2011–2015、2020–2021和2024至今三个时段的历时语料库；每种语言每时段100万文档，并提供预计算的词类型/词例嵌入及目标词的词汇替换。

Result: 发布了一个涵盖41种语言、三个时间阶段的开放历时语料库DHPLT，附带预计算语言学资源，并支持研究者自定义目标词。

Conclusion: DHPLT为多语言语义变化研究提供了可扩展、可复现的新资源与实验基础，推动该领域向更广泛的语言覆盖发展。

Abstract: In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.

</details>


### [135] [Automatic Simplification of Common Vulnerabilities and Exposures Descriptions](https://arxiv.org/abs/2602.11982)
*Varpu Vehomäki,Kimmo K. Kaski*

Main category: cs.CL

TL;DR: 本文研究了如何利用大语言模型（LLMs）自动简化通用漏洞与暴露（CVE）描述，以提升网络安全信息的可理解性；构建了首个网络安全领域自动文本简化基线和含40个CVE描述的测试集，并通过两轮专家调查评估发现，现成LLM虽能提升表观简洁性，但在语义保真上表现不足。


<details>
  <summary>Details</summary>
Motivation: 网络安全信息对非专业人士而言难以理解，而现有自动文本简化研究尚未覆盖快速变化且复杂的网络安全领域，亟需针对性方法提升CVE等安全文本的可读性。

Method: 构建网络安全自动文本简化（ATS）基线和包含40个CVE描述的测试数据集，并通过两轮由网络安全专家参与的调查评估多种现成大语言模型在简化过程中的表现，重点考察表观简洁性与语义保真度。

Result: 实验表明，现成大语言模型虽能生成更简洁的文本，但在关键安全语义（如漏洞影响、严重程度、利用条件）的保持方面存在明显缺陷，语义失真问题突出。

Conclusion: 直接应用通用大语言模型进行网络安全文本简化不可靠，需结合领域知识进行专门适配与评估，本文为该方向提供了首个基准、数据集与实证依据。

Abstract: Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on investigating how large language models (LLMs) could be utilized in automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions. Automatic text simplification has been studied in several contexts, such as medical, scientific, and news texts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of cyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions, evaluated by two groups of cyber security experts in two survey rounds. We have found that while out-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation. Code and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification\_nmi.

</details>


### [136] [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005)
*Szilvia Ujváry,Louis Béthune,Pierre Ablin,João Monteiro,Marco Cuturi,Michael Kirchhof*

Main category: cs.CL

TL;DR: 本文提出LaCy预训练方法，通过结合损失值与spaCy语法解析器判断哪些token应由小语言模型（SLM）自主生成、哪些应通过<CALL>标记交由外部大模型等资源处理，从而在保持低成本的同时提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）参数量有限，预训练中难以充分压缩世界知识，易产生事实性错误；虽可通过调用外部资源缓解，但需解决‘哪些token该学、哪些该委托’这一根本问题。

Method: 提出LaCy预训练方法：利用损失值初步筛选高风险token，并引入spaCy语法解析器增强判别能力，区分‘可接受的高损失token’（如语法合理、事实正确替代续写）与‘应委托的token’，据此决定是否插入<CALL>标记。

Result: LaCy使SLM能有效学习预测与委托策略，在与大模型级联生成时显著提升FactScore，优于Rho和LLM-judge训练的SLMs，且更简单、成本更低。

Conclusion: token级学习-委托决策不应仅依赖损失，而需结合语言结构信息；LaCy验证了该思想的有效性，为SLM高效、可信部署提供了新路径。

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

</details>


### [137] [Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study](https://arxiv.org/abs/2602.12015)
*Angelo Ziletti,Leonardo D'Ambrosi*

Main category: cs.CL

TL;DR: 本文提出CLUES框架，用于临床Text-to-SQL任务中区分输入歧义与模型不稳定性两类不确定性，并分别量化为歧义分和不稳定性分，从而支持精准干预。


<details>
  <summary>Details</summary>
Motivation: 在临床Text-to-SQL部署中，需区分因输入歧义（应触发用户澄清）和模型不稳定性（应触发人工审核）导致的输出多样性，现有方法缺乏可解释的不确定性分解。

Method: 将Text-to-SQL建模为两阶段过程（语义解释→答案），构建二部语义图，利用其矩阵的Schur补计算不稳定性得分；同时设计歧义得分，共同实现语义不确定性的解耦评估。

Result: 在AmbigQA/SituatedQA及临床Text-to-SQL基准上，CLUES在故障预测上优于当前最优的Kernel Language Entropy；高歧义-高不稳定性区域覆盖25%查询但包含51%错误，显著提升错误筛查效率。

Conclusion: CLUES提供了可解释、可干预的不确定性分解机制，支持面向临床部署的差异化响应策略（如查询优化或模型改进），提升了LLM在高风险场景中的可控性与实用性。

Abstract: Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human review. We propose CLUES, a framework that models Text-to-SQL as a two-stage process (interpretations --> answers) and decomposes semantic uncertainty into an ambiguity score and an instability score. The instability score is computed via the Schur complement of a bipartite semantic graph matrix. Across AmbigQA/SituatedQA (gold interpretations) and a clinical Text-to-SQL benchmark (known interpretations), CLUES improves failure prediction over state-of-the-art Kernel Language Entropy. In deployment settings, it remains competitive while providing a diagnostic decomposition unavailable from a single score. The resulting uncertainty regimes map to targeted interventions - query refinement for ambiguity, model improvement for instability. The high-ambiguity/high-instability regime contains 51% of errors while covering 25% of queries, enabling efficient triage.

</details>


### [138] [Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.12036)
*Xin Xu,Clive Bai,Kai Yang,Tianhao Chen,Yangkun Chen,Weijie Liu,Hao Chen,Yang Wang,Saiyong Yang,Can Yang*

Main category: cs.CL

TL;DR: 本文提出Composition-RL方法，通过自动组合多个问题生成新的可验证提示，以更有效地利用高通过率（pass-rate-1）的提示数据，提升大模型推理能力，并支持跨领域强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法受限于大规模可验证提示中大量低信息量样本及扩展成本高；训练后期易出现大量pass-rate-1的“简单提示”，导致有效数据规模下降。

Method: 提出Composition-RL：自动将多个基础问题组合成新的可验证问题作为训练提示；并设计课程学习变体，逐步增加组合深度；同时支持跨领域提示组合。

Result: 在4B至30B参数规模模型上实验表明，Composition-RL持续提升推理能力；课程学习变体进一步增强性能；跨领域组合亦显著提升泛化效果。

Conclusion: Composition-RL是一种简单而有效的数据利用策略，能缓解高通过率提示带来的数据效率瓶颈，为RLVR提供新思路，并具备跨领域适用性。

Abstract: Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.

</details>


### [139] [DeepSight: An All-in-One LM Safety Toolkit](https://arxiv.org/abs/2602.12092)
*Bo Zhang,Jiaxuan Guo,Lijun Li,Dongrui Liu,Sujin Chen,Guanxu Chen,Zhijie Zheng,Qihao Lin,Lewen Yan,Chen Qian,Yijin Zhou,Yuyao Wu,Shaoxiong Guo,Tianyi Du,Jingyi Yang,Xuhao Hu,Ziqi Miao,Xiaoya Lu,Jing Shao,Xia Hu*

Main category: cs.CL

TL;DR: 本文提出DeepSight开源项目，整合安全评估与诊断，实现从黑盒到白盒的大模型安全分析。


<details>
  <summary>Details</summary>
Motivation: 现有大模型安全工作流中评估、诊断和对齐由不同工具处理，导致评估无法定位内部根源、诊断脱离具体风险场景、对齐缺乏机制解释，可能损害通用能力。

Method: 提出DeepSight项目，包含评估工具DeepSafe和诊断工具DeepScan，通过统一任务与数据协议连接两阶段，实现评估-诊断一体化。

Result: 构建了低成本、可复现、高效且高可扩展的大模型安全评估框架，首次支持前沿AI风险评估及联合安全评估与诊断。

Conclusion: DeepSight推动大模型安全从黑盒行为检测迈向白盒机制理解，为系统性安全治理提供新范式。

Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.

</details>


### [140] [P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling](https://arxiv.org/abs/2602.12116)
*Pinyi Zhang,Ting-En Lin,Yuchuan Wu,Jingyang Chen,Zongqi Wang,Hua Yang,Ze Xu,Fei Huang,Kai Zhang,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出P-GenRM，首个支持测试时用户自适应缩放的个性化生成式奖励模型，通过构建评估链、用户原型聚类与双粒度缩放机制，显著提升个性化对齐效果与新用户泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有个性化奖励模型难以准确建模用户多样、场景特定的偏好，且在新用户（反馈少）上泛化能力差。

Method: 提出P-GenRM：将偏好信号转化为结构化评估链，生成自适应角色与评分标准；聚类用户形成User Prototypes；设计个体级（用户自身评分缩放聚合）与原型级（相似用户偏好迁移）双粒度缩放机制。

Result: 在主流个性化奖励模型基准上达到SOTA，平均提升2.31%；在OOD数据集上泛化性强；测试时用户缩放带来额外3%性能提升。

Conclusion: P-GenRM通过生成式建模与双粒度缩放，有效缓解偏好噪声、增强新用户泛化，验证了测试时用户自适应缩放对个性化对齐的关键作用。

Abstract: Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.

</details>


### [141] [A Rule-based Computational Model for Gaidhlig Morphology](https://arxiv.org/abs/2602.12132)
*Peter J Barclay*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的苏格兰盖尔语（Gaidhlig）形态学建模方法，利用Wiktionary数据构建可解释、低数据依赖的系统，支持教学工具和高阶NLP工具开发。


<details>
  <summary>Details</summary>
Motivation: 当前主流神经模型需大量训练数据，不适用于缺乏语料的低资源语言；而规则系统能有效利用有限样本、增强可解释性，并辅助教学材料设计。

Method: 从Wiktionary提取数据，使用SQL查询词汇模式，构建声明式规则库，并通过Python工具实现盖尔语词形变化推导。

Result: 实现了基于规则的盖尔语形态分析系统，可支持教育工具（如语言模式讲解）及高阶工具（如基于规则的依存句法分析器）。

Conclusion: 该方法成功将Wiktionary现有数据适配至新应用场景，在低资源语言处理中兼具实用性与可解释性。

Abstract: Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.

</details>


### [142] [WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models](https://arxiv.org/abs/2602.12135)
*Yangzhuo Li,Shengpeng Ji,Yifu Chen,Tianle Liang,Haorong Ying,Yule Wang,Junbo Li,Jun Fang,Zhou Zhao*

Main category: cs.CL

TL;DR: 本文提出了WavBench，一个面向真实语音对话能力评估的新型基准，涵盖推理能力（Pro子集）、口语化表达（Basic子集）和副语言理解与生成（Acoustic子集）三方面，弥补了现有文本导向评测在音频特性和认知深度上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前语音对话模型评估仍沿用文本生成标准，忽视语音特有的副语言特征、口语表达及现代智能体所需的认知深度，亟需更贴合真实场景的评测基准。

Method: 构建WavBench三元评测框架：Pro子集（高难度推理挑战）、Basic子集（以‘可听性’为核心的口语化标准）、Acoustic子集（覆盖显式/隐式副语言理解与生成），并在5个SOTA模型上开展系统评估。

Result: WavBench揭示了当前模型在复杂推理、自然口语表达与副语言保真度三方面的关键短板，为语音对话系统发展提供了实证依据与改进方向。

Conclusion: WavBench是首个兼顾认知深度、口语自然性与音频特性的综合性语音对话基准，推动评测范式从文本中心转向真实语音交互中心。

Abstract: With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes "listenability" through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at https://naruto-2024.github.io/wavbench.github.io/.

</details>


### [143] [CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes](https://arxiv.org/abs/2602.12137)
*Ricardo Campos,Ana Filipa Pacheco,Ana Luísa Fernandes,Inês Cantante,Rute Rebouças,Luís Filipe Cunha,José Miguel Isidro,José Pedro Evans,Miguel Marques,Rodrigo Batista,Evelin Amorim,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano*

Main category: cs.CL

TL;DR: 本文介绍了CitiLink-Minutes数据集，一个包含120份欧洲葡萄牙语市政会议纪要的多层标注数据集，旨在填补市政记录在信息检索和自然语言处理领域缺乏标注数据的空白。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要作为重要官方记录，在信息检索和自然语言处理领域长期缺乏标注数据集，限制了相关计算模型的发展。

Method: 构建了一个名为CitiLink-Minutes的多层标注数据集，涵盖6个葡萄牙市镇的120份会议纪要，包含超百万词符；所有个人标识已去识别化；由两名训练有素的标注员和一名经验丰富的语言学家在元数据、讨论主题和投票结果三个维度进行人工标注，共38,000余条标注；数据集按FAIR原则发布，并提供元数据抽取、主题分类和投票标签等基线实验结果。

Result: 发布了首个面向市政会议纪要的多层标注数据集CitiLink-Minutes，包含结构化链接与多维人工标注，并提供了若干NLP/IR任务的基线性能。

Conclusion: CitiLink-Minutes为市政决策文本的计算分析提供了高质量资源，推动了地方政府透明度研究及下游NLP/IR任务的发展。

Abstract: City councils play a crucial role in local governance, directly influencing citizens' daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.

</details>


### [144] [dVoting: Fast Voting for dLLMs](https://arxiv.org/abs/2602.12153)
*Sicheng Feng,Zigeng Chen,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文提出dVoting方法，利用扩散大语言模型（dLLMs）的任意位置生成能力，通过多采样一致性分析识别不确定性token并投票重生成，实现无需训练的推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 观察到dLLMs在多次采样中多数token预测一致，而性能瓶颈在于少数跨样本不一致的token；同时，dLLMs支持任意位置并行生成，为迭代精炼提供基础。

Method: dVoting方法：对同一提示进行多次采样，通过一致性分析识别不确定token，利用dLLMs的任意位置生成能力对这些token进行投票重生成，并迭代该过程直至收敛。

Result: 在GSM8K、MATH500、ARC-C和MMLU等基准上分别取得6.22%-7.66%、4.40%-7.20%、3.16%-14.84%和4.83%-5.74%的性能提升。

Conclusion: dVoting是一种高效、无需训练的推理增强技术，充分利用dLLMs的并行生成特性，在多个推理任务上显著提升性能，且计算开销可控。

Abstract: Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting

</details>


### [145] [Query-focused and Memory-aware Reranker for Long Context Processing](https://arxiv.org/abs/2602.12192)
*Yuqing Li,Jiangnan Li,Mo Yu,Guoxuan Ding,Zheng Lin,Weiping Wang,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型中检索头注意力分数的重排序框架，通过训练模型估计段落-查询相关性，实现轻量高效且无需显式标注的列表级重排序。


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法多依赖点式或需人工标注的列表式方法，缺乏对候选列表整体信息的有效利用，且对标注数据依赖强。

Method: 利用大语言模型中特定检索头的注意力分数作为段落-查询相关性的代理，构建端到端可训练的列表级重排序器，支持小模型（如4B参数）高效训练与推理。

Result: 在Wikipedia、长叙事数据集及LoCoMo对话理解基准上均超越现有SOTA点式和列表式重排序器，尤其在LoCoMo上达到新SOTA；支持上下文增强和中层注意力头训练等灵活扩展。

Conclusion: 该框架提供了一种轻量、通用、无需Likert标注的列表级重排序新范式，兼具高性能与部署友好性。

Abstract: Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.

</details>


### [146] [Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education](https://arxiv.org/abs/2602.12196)
*Mohamed Huti,Alasdair Mackintosh,Amy Waldock,Dominic Andrews,Maxime Lelièvre,Moritz Boos,Tobias Murray,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: 本文提出视觉推理基准（VRB），用于评估多模态大语言模型（MLLMs）在真实小学课堂视觉问题上的推理能力，发现模型在静态任务（如计数）上表现较好，但在动态空间操作（如折叠、反射、旋转）上存在明显瓶颈，凸显教育场景中专用评测基准的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在文本推理上表现优异，但在空间与关系结构推理（尤其小学数学依赖的视觉推理）方面存在关键瓶颈，亟需面向教育真实需求的评测基准。

Method: 构建了包含701道来自赞比亚和印度小学考试题的视觉推理基准（VRB），题目涵盖类比推理、模式补全、空间匹配等任务，并采用未经编辑、极简文字的真实图像以贴近课堂实际。

Result: 实验揭示模型能力呈“锯齿状前沿”：静态技能（计数、缩放）较强，但动态空间操作（折叠、反射、旋转）存在明显“空间天花板”，易导致错误批改、无效引导及强化学生误解。

Conclusion: 面向教育的专用基准（如VRB）对界定多模态工具在课堂中的功能边界至关重要，当前MLLMs尚不具备可靠支撑小学视觉推理教学的能力。

Abstract: AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.

</details>


### [147] [ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images](https://arxiv.org/abs/2602.12203)
*Mathieu Sibue,Andres Muñoz Garza,Samuel Mensah,Pranav Shetty,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文提出了一个新的基准数据集ExStrucTiny，用于评估和提升通用视觉语言模型在多样化文档图像上进行灵活、细粒度结构化信息抽取的能力。


<details>
  <summary>Details</summary>
Motivation: 现有Key Entity Extraction (KEE)、Relation Extraction (RE) 和 Visual Question Answering (VQA) 数据集在实体本体范围、查询复杂度和文档多样性方面存在局限，难以支持面向企业文档的可适配、结构化信息抽取需求。

Method: 构建了融合KEE、RE与VQA任务特点的新型基准ExStrucTiny，采用人工标注与合成样本结合、并经人工验证的流程生成，覆盖更广的文档类型与抽取场景；并在该基准上系统评测了开源与闭源视觉语言模型。

Result: 揭示了当前通用VLM在schema适应性、查询描述模糊性（under-specification）及答案定位（localization）等方面的显著挑战。

Conclusion: ExStrucTiny为推动通用视觉语言模型在企业文档结构化信息抽取任务上的能力提升提供了坚实基础与统一评估平台。

Abstract: Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.

</details>


### [148] [Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.12235)
*Julia Belikova,Danila Rozhevskii,Dennis Svirin,Konstantin Polev,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本文提出了一种检测软压缩架构中'token overflow'（令牌溢出）现象的方法，即压缩表示不足以回答查询的问题。通过在xRAG设置下分析饱和统计量和轻量级探针分类器，实现了对压缩失效的早期识别，支持低成本预大模型门控以减少压缩导致的错误。


<details>
  <summary>Details</summary>
Motivation: 现有软压缩架构虽能扩展LLM有效上下文长度，但其压缩极限及何时开始擦除任务相关的信息尚不明确，亟需一种可靠方法来刻画和检测压缩失效（token overflow）。

Method: 定义token overflow概念；在xRAG软压缩框架下，分别使用查询无关的饱和统计量与结合查询和上下文表征的轻量级探针分类器进行检测；在HotpotQA、SQuADv2和TriviaQA数据集上评估性能。

Result: 查询无关的饱和统计量可有效区分压缩/未压缩token，但溢出检测能力有限；而查询感知的探针分类器在三个数据集上平均AUC-ROC达0.72，显著提升溢出检测效果。

Conclusion: 从查询无关诊断迈向查询感知检测是可行且有效的，所提方法可作为低开销预LLM门控机制，缓解压缩引发的推理错误。

Abstract: Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define \emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.

</details>


### [149] [Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications](https://arxiv.org/abs/2602.12241)
*Manjunath Kudlur,Evan King,James Wang,Pete Warden*

Main category: cs.CL

TL;DR: 本文提出Moonshine v2，一种面向边缘设备的流式ASR模型，采用滑动窗口自注意力机制，在保持高精度的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 低延迟语音应用（如实时转录、语音指令）在资源受限的边缘设备上需要低首字延迟（TTFT）和高识别准确率，但全注意力Transformer编码器存在二次计算复杂度和线性增长的TTFT问题。

Method: 提出Moonshine v2模型，采用遍历式流式编码器结构与滑动窗口自注意力机制，实现有界低延迟推理并保留强局部上下文建模能力。

Result: 在标准基准上达到SOTA词错误率（WER），精度媲美大6倍的模型，同时推理速度显著提升。

Conclusion: 精心设计的局部注意力机制可在大幅减小模型尺寸和延迟的前提下，达到与全注意力相当的识别精度，为边缘设备上的交互式语音接口开辟新路径。

Abstract: Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent "encode-the-whole-utterance" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.

</details>


### [150] [A technical curriculum on language-oriented artificial intelligence in translation and specialised communication](https://arxiv.org/abs/2602.12251)
*Ralph Krüger*

Main category: cs.CL

TL;DR: 本文提出了一门面向语言与翻译（L&T）行业的语言导向人工智能（AI）技术课程，旨在提升相关从业者对现代语言AI的技术理解力与算法素养，核心内容包括向量嵌入、神经网络基础、分词及Transformer模型，并通过硕士课程验证了其教学有效性，但需辅以教师支持等更高层次的教学支架。


<details>
  <summary>Details</summary>
Motivation: 提升语言与翻译行业从业者在AI驱动工作环境中的技术AI素养、计算思维、算法意识与算法能动性，增强其数字韧性。

Method: 设计并实施一门涵盖向量嵌入、神经网络基础、tokenization和Transformer模型的结构化技术课程，并在TH Koeln翻译与多语传播研究所的AI主题硕士课程中进行教学实践与效果评估。

Result: 课程被证实具有教学有效性，但参与者反馈指出需嵌入更高层次的教学支架（如教师指导）以实现最优学习效果。

Conclusion: 该课程为L&T领域提供了可行的技术AI素养培养路径，但成功实施依赖于适当的教学支持与 scaffolding。

Abstract: This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.

</details>


### [151] [T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization](https://arxiv.org/abs/2602.12262)
*Tunyu Zhang,Xinxi Zhang,Ligong Han,Haizhou Shi,Xiaoxiao He,Zhuowei Li,Hao Wang,Kai Xu,Akash Srivastava,Hao Wang,Vladimir Pavlovic,Dimitris N. Metaxas*

Main category: cs.CL

TL;DR: 本文提出了一种轨迹自蒸馏框架，结合Direct Discriminative Optimization（DDO）目标，提升扩散大语言模型（DLLMs）在少量步数下的解码质量与效率。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）虽支持并行多token生成，但实际推理中需大量精炼步数；减少步数会显著降低生成质量，亟需在少步条件下兼顾效率与质量。

Method: 提出轨迹自蒸馏框架，利用模型自身生成轨迹进行蒸馏，并引入基于逆KL的Direct Discriminative Optimization（DDO）目标，使学生模型聚焦于教师模型的高概率模式。

Result: 在多个基准上，该方法在严格步数限制下持续优于强少步基线和标准训练；虽仍略逊于全步解码，但显著缩小了性能差距。

Conclusion: 所提方法为实现高效、实用的少步DLLMs奠定了坚实基础。

Abstract: Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.

</details>


### [152] [On-Policy Context Distillation for Language Models](https://arxiv.org/abs/2602.12275)
*Tianzhu Ye,Li Dong,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为On-Policy Context Distillation (OPCD)的新框架，通过在学生模型自身生成的轨迹上进行策略内蒸馏，并最小化其与上下文条件教师模型之间的逆KL散度，实现上下文知识向参数的内化。该方法在经验知识蒸馏和系统提示蒸馏两个任务中表现优异，且支持跨尺寸模型的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有上下文蒸馏方法难以有效将模型在推理过程中积累的经验或优化提示中的行为内化为参数知识，缺乏对策略一致性与分布匹配的联合建模。

Method: 提出OPCD框架：学生模型基于自身生成的轨迹进行训练，以逆KL散度约束其输出分布逼近上下文条件下的教师模型分布，从而实现策略一致的上下文知识内化。

Result: OPCD在数学推理、文本游戏和领域任务中均超越基线方法，提升任务准确率并更好保持OOD泛化能力；同时支持小模型从大模型中高效蒸馏经验知识。

Conclusion: OPCD成功融合了策略内学习与上下文蒸馏，为语言模型实现经验驱动的参数化知识固化提供了新范式，具备良好的可扩展性与实用性。

Abstract: Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [153] [Explaining AI Without Code: A User Study on Explainable AI](https://arxiv.org/abs/2602.11159)
*Natalia Abarca,Andrés Carvallo,Claudia López Moncada,Felipe Bravo-Marquez*

Main category: cs.AI

TL;DR: 本文提出了一种面向初学者和专家的、集成于开源无代码机器学习平台DashAI中的人本XAI模块，结合PDP、PFI和KernelSHAP三种可解释性技术，通过用户研究验证其在可用性、解释满意度和信任度方面的有效性，并揭示了兼顾不同用户群体需求的XAI设计挑战。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融等敏感领域的广泛应用，自动化决策的透明性引发关注；而现有可解释AI（XAI）方法多需技术背景，难以满足无代码ML平台中大量非专业用户的可解释需求。

Method: 在开源无代码ML平台DashAI中构建人本XAI模块，集成Partial Dependence Plots（PDP）、Permutation Feature Importance（PFI）和KernelSHAP三种互补技术，用于表格分类任务；并通过包含20名ML新手与专家的用户研究评估其可用性、解释满意度（ESS量表）与信任度（TiA量表）。

Result: （i）所有可解释性任务的任务成功率≥80%；（ii）新手认为解释有用、准确、可信（ESS，α=0.74），专家则更质疑其充分性与完整性；（iii）解释提升了用户对模型可预测性和信心的感知（TiA，α=0.60），且新手信任度高于专家。

Conclusion: 在无代码ML中实现XAI需平衡可访问性与技术深度：既要让新手易于理解，又要满足专家对解释完备性的要求，这是当前XAI落地的关键挑战。

Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\geq80\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $α$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $α$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.

</details>


### [154] [Latent Generative Solvers for Generalizable Long-Term Physics Simulation](https://arxiv.org/abs/2602.11229)
*Zituo Chen,Haixu Wu,Sili Deng*

Main category: cs.AI

TL;DR: 本文提出Latent Generative Solvers（LGS），一种用于异构偏微分方程（PDE）系统长时程代理仿真的两阶段生成式求解器，通过隐空间建模、不确定性调控与流匹配训练，显著抑制滚动误差，提升长期预测稳定性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经PDE求解器在长时程仿真中易出现滚动漂移（rollout drift），且难以跨PDE系统泛化；需兼顾精度、效率与不确定性建模能力。

Method: LGS包含两个阶段：(i) 使用预训练VAE将多类PDE状态映射至共享隐物理空间；(ii) 用基于流匹配（flow matching）训练的Transformer学习概率性隐动力学；引入‘不确定性旋钮’扰动隐输入以校正离流形漂移，并采用流强制（flow forcing）动态更新系统描述符以对齐训练/测试条件。

Result: 在约250万条、覆盖12类PDE、128²分辨率的轨迹上预训练；LGS在短时程上媲美强确定性神经算子基线，在长时程显著降低滚动漂移；计算开销低至非生成式基线的1/70；可仅用少量微调适配256² Kolmogorov流（OOD）数据。

Conclusion: LGS为构建可泛化、带不确定性感知、高可靠性的神经PDE求解器提供了实用路径，适用于长期科学预测与下游工作流。

Abstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \textbf{70$\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.

</details>


### [155] [Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization](https://arxiv.org/abs/2602.11437)
*Chengrui Qu,Christopher Yeh,Kishan Panaganti,Eric Mazumdar,Adam Wierman*

Main category: cs.AI

TL;DR: 本文提出了一种分布鲁棒的个体-全局最大值（DrIGM）原则，以增强多智能体强化学习在现实不确定性环境下的鲁棒性，并基于此设计了兼容现有价值分解架构的鲁棒变体，在多个仿真环境中验证了其优越的OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于中心化训练-去中心化执行（CTDE）的价值分解方法在真实场景中因仿真到现实差距、模型失配和系统噪声等环境不确定性而可靠性不足。

Method: 提出分布鲁棒IGM（DrIGM）原则，定义鲁棒个体动作值，构建与之兼容的鲁棒价值分解架构（如DrVDN/DrQMIX/DrQTRAN），通过鲁棒Q目标训练，保持可扩展性且无需定制化奖励塑形。

Result: 在高保真SustainGym模拟器和《星际争霸》环境中，所提方法显著提升了分布外（out-of-distribution）性能。

Conclusion: DrIGM为多智能体强化学习提供了理论鲁棒性保证，并实现了与主流架构兼容、易部署、高性能的鲁棒化升级路径。

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.

</details>


### [156] [On Decision-Valued Maps and Representational Dependence](https://arxiv.org/abs/2602.11295)
*Gil Raitses*

Main category: cs.AI

TL;DR: 本文提出决策值映射（decision-valued maps）概念，形式化其定义，并设计DecisionDB系统来记录、回放与审计不同数据表示对计算结果的影响；通过内容哈希标识和一次写入存储实现确定性回放，并将表示空间划分为持久化区域与边界，使决策复用可机械验证。


<details>
  <summary>Details</summary>
Motivation: 不同数据表示可能导致同一计算引擎产生不同离散结果，需系统化刻画哪些表示保持结果、哪些改变结果，以支持可审计、可复现的决策过程。

Method: 形式化定义决策值映射；构建DecisionDB基础设施，利用内容标识符和一次写入存储记录决策与表示的关系；实现基于存储构件的确定性回放；引入表示空间的持久化区域与边界划分，并将决策复用建模为可机械验证的条件。

Result: 实现了DecisionDB原型系统，支持日志记录、精确回放与审计；验证了决策标识符可从存储构件中完全恢复；明确了表示空间中保持决策一致性的区域结构。

Conclusion: 决策值映射为理解与控制表示依赖性提供了理论框架，DecisionDB及其持久化区域划分使决策复用具备可验证性，增强了计算系统的透明性与可靠性。

Abstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.

</details>


### [157] [Multi UAVs Preflight Planning in a Shared and Dynamic Airspace](https://arxiv.org/abs/2602.12055)
*Amath Sow,Mauricio Rodriguez Cesen,Fabiola Martins Campos de Oliveira,Mariusz Wzorek,Daniel de Leng,Mattias Tiger,Fredrik Heintz,Christian Esteve Rothenberg*

Main category: cs.AI

TL;DR: 本文提出DTAPP-IICR方法，用于大规模异构无人机编队在动态共享空域中的交付时间感知的预飞行路径规划，结合优先级调度、新型4D单机规划器SFIPP-ST及基于冲突图的迭代大邻域搜索，在千机规模下实现近100%成功率与50%运行时加速。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体路径规划（MAPF）方法难以满足城市级无人机交通管理（UTM）对可扩展性、异构性、动态禁飞区（NFZ）和严格交付时限的实际需求。

Method: 提出DTAPP-IICR框架：1）按任务紧迫性优先生成初始解；2）使用SFIPP-ST（支持软/时间约束的4D单机规划器）计算带时空约束的往返轨迹；3）基于几何冲突图的迭代大邻域搜索解决残余冲突；4）引入保完备性的方向剪枝加速3D搜索。

Result: 在含时间NFZ的基准测试中，支持高达1000架无人机、成功率近100%，运行时间较增强型CBP减少最多50%；在城市级真实场景中显著优于其他优先级方法。

Conclusion: DTAPP-IICR是一种实用、可扩展的预飞行规划方案，能有效支撑高密度、动态城市空域中的大规模无人机物流与交通管理。

Abstract: Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.

</details>


### [158] [Voxtral Realtime](https://arxiv.org/abs/2602.11298)
*Alexander H. Liu,Andy Ehrenberg,Andy Lo,Chen-Yo Sun,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Rohin Arora,Sanchit Gandhi,Sandeep Subramanian,Soham Ghosh,Srijan Mishra,Abhinav Rastogi,Alan Jeffares,Albert Jiang,Alexandre Sablayrolles,Amélie Héliou,Andrew Bai,Angele Lenglemetz,Anmol Agarwal,Anton Eliseev,Antonia Calvi,Arjun Majumdar,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Benjamin Tibi,Clémence Lanfranchi,Connor Chen,Corentin Barreau,Corentin Sautier,Cyprien Courtot,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Enguerrand Paquin,Faruk Ahmed,Federico Baldassarre,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Genevieve Hayes,Georgii Novikov,Giada Pistilli,Guillaume Martin,Gunjan Dhanuka,Gunshi Gupta,Han Zhou,Indraneel Mukherjee,Irene Zhang,Jaeyoung Kim,Jan Ludziejewski,Jason Rute,Joachim Studnia,John Harvill,Jonas Amar,Josselin Somerville Roberts,Julien Tauran,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Laurence Aitchison,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Manan Sharma,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mert Unsal,Mia Chiquier,Nathan Grinsztajn,Neha Gupta,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Piotr Miłoś,Prateek Gupta,Pravesh Agrawal,Quentin Torroba,Ram Ramrakhya,Rishi Shah,Romain Sauvestre,Roman Soletskyi,Rosalie Millner,Sagar Vaze,Samuel Humeau,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Edwards,Tyler Wang,Valeriia Nemychnikova,Van Phung,Vedant Nanda,Victor Jouault,Virgile Richard,Vladislav Bataev,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xingran Guo,Xinyu Yang,Yannic Neuhaus,Yihan Wang,Zaccharie Ramzi,Zhenlin Xu*

Main category: cs.AI

TL;DR: Voxtral Realtime 是一种原生流式语音识别模型，在亚秒级延迟下达到离线转录质量，采用端到端流式训练与显式音文对齐，基于延迟流建模框架，引入因果音频编码器和 Ada RMS-Norm，并在13种语言大数据集上预训练，在480ms延迟下性能媲美 Whisper。


<details>
  <summary>Details</summary>
Motivation: 解决现有流式ASR系统通过离线模型分块或滑窗适配导致的性能损失问题，追求低延迟与高精度兼顾。

Method: 基于 Delayed Streams Modeling 框架，设计因果音频编码器和 Ada RMS-Norm，端到端流式训练，显式建模音频与文本流的时间对齐，并在13语言大规模数据集上预训练。

Result: 在480ms延迟下，多语言ASR性能与离线模型 Whisper 相当；模型已开源（Apache 2.0 许可）。

Conclusion: Voxtral Realtime 证明了原生流式架构可在极低延迟下实现离线级精度，为实时语音识别提供了新范式。

Abstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.

</details>


### [159] [The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates](https://arxiv.org/abs/2602.11301)
*John M. Willis*

Main category: cs.AI

TL;DR: 本文提出了一个名为Practitioners Blueprint for Secure AI (PBSAI)的多智能体参考架构，用于保障企业级和超大规模AI系统的安全治理，涵盖十二个责任领域、带上下文封装与结构化输出契约的智能体家族，并结合形式化建模确保可追溯性、溯源性与人在回路保障。


<details>
  <summary>Details</summary>
Motivation: 现有AI治理与安全框架（如NIST AI RMF）缺乏面向多智能体、AI赋能网络防御的可实施架构，而企业正快速将LLM、RAG、工具调用智能体部署于共享高性能计算与云加速平台，亟需适配AI estates（AI生态体系）的安全治理方案。

Method: 提出PBSAI多智能体参考架构，定义十二域责任分类、受限智能体家族、共享上下文封装与结构化输出契约；引入轻量级形式模型刻画智能体、上下文封装与系统级不变量；整合分析监控、协同防御、自适应响应等系统安全技术。

Result: 构建了可实施的PBSAI治理生态系统架构，明确了各域职责与智能体交互机制，形式化模型支持追溯性、溯源性与人在回路保障，并验证其与NIST AI RMF功能对齐，适用于企业SOC与超大规模防御场景。

Conclusion: PBSAI为AI estates提供了结构化、证据驱动的安全治理基础，支持开放生态共建与未来实证验证，填补了多智能体AI系统在生产环境中安全治理架构的空白。

Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.
  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.

</details>


### [160] [Dissecting Subjectivity and the "Ground Truth" Illusion in Data Annotation](https://arxiv.org/abs/2602.11318)
*Sheza Munir,Benjamin Mah,Krisha Kalsi,Shivani Kapania,Julian Posada,Edith Law,Ding Wang,Syed Ishtiaque Ahmed*

Main category: cs.AI

TL;DR: 本文批判了机器学习中将人类分歧视为噪声的‘真实标签’范式，通过系统性文献综述揭示数据标注实践中导致‘共识陷阱’的机制，指出位置可读性缺失、模型中介标注引发的锚定偏差，以及地理霸权强加西方标准等问题；主张将分歧视为高保真信号，并提出构建多元标注基础设施的路线图。


<details>
  <summary>Details</summary>
Motivation: 挑战机器学习中将人类分歧简单视为技术噪声的‘真实标签’范式，揭示其背后的社会技术问题，如共识陷阱、锚定偏差与地理霸权。

Method: 2020–2025年间在ACL、AIES、CHI、CSCW、EAAMO、FAccT和NeurIPS七大顶会开展系统性文献回顾；经关键词筛选与人工筛选，从30,897篇记录中最终纳入346篇进行反思性主题分析。

Result: 发现三大核心问题：（1）位置可读性缺失与人类作为验证者的架构转变加剧锚定偏差；（2）地理霸权使西方规范成为普适基准；（3）‘嘈杂传感器’谬误将文化多元性误判为随机误差。

Conclusion: 应摒弃追求唯一‘正确答案’的标注目标，转而将人类分歧视为构建文化胜任模型的关键高保真信号，并推动以映射人类经验多样性为目标的多元标注基础设施建设。

Abstract: In machine learning, "ground truth" refers to the assumed correct labels used to train and evaluate models. However, the foundational "ground truth" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this "consensus trap". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the "noisy sensor" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular "right" answer to mapping the diversity of human experience.

</details>


### [161] [Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge](https://arxiv.org/abs/2602.11340)
*Bo Pan,Xuan Kan,Kaitai Zhang,Yan Yan,Shunwen Tan,Zihao He,Zixin Ding,Junjie Wu,Liang Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种用于多模态大语言模型（LLM）作为评判器的自动提示优化框架BLPO，通过将图像转化为保留关键视觉线索的文本表示，解决上下文窗口限制下的提示优化难题，并在多个数据集和模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动评估方法在多模态场景（尤其是AI生成图像评估）中面临上下文窗口限制导致难以进行有效试错式提示优化的问题；同时，监督微调成本高、灵活性差，而现有自动提示优化（APO）方法主要面向纯文本任务。

Method: 提出双层提示优化框架BLPO：第一层优化评判器提示（judge prompt），第二层优化图像到文本（I2T）提示，在有限上下文预算下联合优化二者以保持视觉线索保真度。

Result: 在四个数据集和三个LLM评判器上的实验表明，BLPO显著提升了多模态LLM-as-a-judge与人类判断的一致性，优于现有APO方法。

Conclusion: BLPO为多模态场景下的自动提示优化提供了高效、灵活且无需训练的新范式，有效缓解了上下文限制瓶颈，推动LLM评判器在图像评估任务中的实用化。

Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.

</details>


### [162] [AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition](https://arxiv.org/abs/2602.11348)
*Ruipeng Wang,Yuxin Chen,Yukai Wang,Chang Wu,Junfeng Fang,Xiaodong Cai,Qi Gu,Hui Su,An Zhang,Xiang Wang,Xunliang Cai,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本文提出了AgentNoiseBench框架，用于系统评估LLM智能体在噪声环境下的鲁棒性，识别并建模了用户噪声与工具噪声两类现实噪声，并通过可控注入方式在多个基准上验证了现有模型对噪声的高度敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体在基准测试中表现良好，但在真实复杂、不完美环境中性能显著下降，主要因训练与评估范式基于理想化假设，忽视了现实交互中的随机性与噪声。

Method: 提出AgentNoiseBench框架；分析真实场景中的偏差与不确定性，将环境噪声分为用户噪声和工具噪声；构建可控制噪声注入的自动化流水线，在保持任务可解性的前提下扰动现有代理基准；在多架构、多参数规模模型上开展大规模鲁棒性评估。

Result: 实验表明，不同噪声条件下各模型性能呈现一致且显著的变化，证实当前智能体模型对现实环境扰动高度敏感。

Conclusion: 需重视噪声鲁棒性评估，AgentNoiseBench为推动更可靠、实用的智能体研发提供了新基准与方法论支持。

Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.

</details>


### [163] [Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization](https://arxiv.org/abs/2602.11351)
*Yihang Yao,Zhepeng Cen,Haohong Lin,Shiqi Liu,Zuxin Liu,Jiacheng Zhu,Zhang-Wei Hong,Laixi Shi,Ding Zhao*

Main category: cs.AI

TL;DR: 本文提出BAO框架，通过行为增强和行为正则化，在多轮交互中平衡任务性能与用户参与度，显著提升主动式LLM智能体的效果。


<details>
  <summary>Details</summary>
Motivation: 现有主动式LLM智能体训练方法难以兼顾任务性能与用户满意度：被动智能体无法适应用户意图，而过度依赖人工反馈又会降低用户体验。

Method: 提出BAO框架，融合行为增强（提升主动推理与信息获取能力）与行为正则化（抑制低效/冗余交互，对齐用户预期），基于用户反馈进行多轮交互式强化学习训练。

Result: 在UserRL基准套件多个任务上，BAO显著优于现有主动式RL基线，并达到甚至超越商用LLM智能体的性能。

Conclusion: BAO是一种高效、用户对齐的主动式LLM智能体训练框架，适用于复杂多轮真实场景。

Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.

</details>


### [164] [ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences](https://arxiv.org/abs/2602.11354)
*Bang Nguyen,Dominik Soós,Qian Ma,Rochana R. Obadage,Zack Ranjan,Sai Koneru,Timothy M. Errington,Shakhlo Nematova,Sarah Rajtmajer,Jian Wu,Meng Jiang*

Main category: cs.AI

TL;DR: 本文提出了ReplicatorBench，一个面向社会科学领域研究可复现性评估的端到端基准，涵盖可复现与不可复现的研究主张，并设计了ReplicatorAgent框架以评估大语言模型代理在数据获取、实验设计与结果解释三阶段的能力；实验发现当前LLM代理在实验执行上表现良好，但在关键的数据检索环节仍存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估基准过于关注可复现论文的计算结果重现，忽视了真实科研中数据获取不一致、不可复现研究普遍存在、以及过程评估缺失等关键问题。

Method: 构建包含人工验证的可复现/不可复现研究主张的ReplicatorBench基准，覆盖提取检索、实验设计执行、结果解释三阶段；开发具备网络搜索与沙盒环境交互能力的ReplicatorAgent框架，并在四种LLM、不同编程语言及代码访问权限下进行系统评测。

Result: 当前LLM代理能较有效地设计和执行计算实验，但在检索新数据等复制所需资源方面表现较差；不同LLM、编程语言和代码访问级别对性能有显著影响。

Conclusion: ReplicatorBench填补了AI代理在科研复制全流程评估上的空白，揭示了数据获取能力是当前LLM代理的核心瓶颈，为未来研究提供了新基准与改进方向。

Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.

</details>


### [165] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 本文提出C-JEPA，一种基于对象中心表示的掩码联合嵌入预测模型，通过对象级掩码增强交互推理能力，在视觉问答和智能体控制任务中显著提升性能，并具有因果归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 现有对象中心表征不足以捕捉依赖交互的动力学，需增强模型对对象间关系与干预效应的理解能力。

Method: 提出C-JEPA模型，将掩码联合嵌入预测（JEPA）从图像块扩展至对象中心表示，引入对象级掩码机制以强制模型基于其他对象推断被掩对象状态，从而诱导潜在干预和反事实推理。

Result: 在视觉问答任务中反事实推理准确率绝对提升约20%；在智能体控制任务中仅用1%的潜在特征即达与块级世界模型相当的性能；理论分析证实对象级掩码引入因果归纳偏置。

Conclusion: C-JEPA通过对象级掩码有效提升世界模型的关系建模与交互推理能力，兼具实证效果与理论可解释性，为构建具备因果理解能力的智能体提供了新路径。

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [166] [GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation](https://arxiv.org/abs/2602.11408)
*Michael Menezes,Anastasios Kyrillidis*

Main category: cs.AI

TL;DR: 本文提出GHOST框架，通过前向统计近似控制理论中的平衡截断，实现Mamba2模型状态维度的结构化剪枝，在保持模型性能的同时显著降低推理开销。


<details>
  <summary>Details</summary>
Motivation: Mamba2扩展状态维度虽提升时序建模能力，但导致自回归生成中带宽饱和、推理开销大；现有剪枝方法（非结构化、基于幅值、基于梯度）均无法有效缓解该瓶颈。

Method: 提出GHOST（Grouped Hidden-state Output-aware Selection and Truncation），一种仅依赖前向传播统计的结构化剪枝框架，联合衡量状态的能控性与能观性，近似控制论中的平衡截断。

Result: 在130M至2.7B参数规模的Mamba2模型上，实现50%状态维度压缩，WikiText-2上困惑度仅上升约1点。

Conclusion: GHOST在无需反向传播的前提下，达到与梯度法相当的精度，为高效Mamba架构部署提供了实用且可扩展的解决方案。

Abstract: While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.

</details>


### [167] [TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning](https://arxiv.org/abs/2602.11409)
*Sina Tayebati,Divake Kumar,Nastaran Darabi,Davide Ettori,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.AI

TL;DR: 本文提出TRACER，一种面向多轮人机协同工具使用场景的轨迹级不确定性度量方法，通过融合内容惊讶度、情境感知、语义/词汇重复及工具一致性缺口，并采用尾部风险聚焦的MAX聚合策略，显著提升任务失败预测与选择性执行的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性估计方法局限于单次文本生成，难以捕捉多轮人机工具交互中由稀疏关键事件（如循环、工具误用、人机协调失败）引发的整体轨迹级故障，而局部生成却可能表现出高置信度。

Method: TRACER是一种轨迹级不确定性度量，融合内容感知的surprisal、情境感知信号、语义与词汇重复度、工具锚定的连贯性缺口，并采用以尾部风险为导向的MAX复合步骤风险聚合函数来凸显决定性异常。

Result: 在τ²-bench基准上，TRACER在任务失败预测和选择性任务执行中，AUROC最高提升37.1%，AUARC最高提升55%，显著优于基线方法。

Conclusion: TRACER有效建模了复杂对话式工具使用中的轨迹级不确定性，为AI代理提供了更早、更准确的不确定性检测能力，支持更安全、可控的人机协同。

Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $τ^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.

</details>


### [168] [Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning](https://arxiv.org/abs/2602.11455)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 本文提出Anchor-Token Reinforcement Learning (AT-RL)，通过分析跨模态注意力连接性，识别出约15%的高连接性token作为视觉锚点，并在RLVR训练中聚焦强化这些锚点，显著提升多模态大模型推理性能，尤其在MathVista等任务上超越更大参数量基线。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法中视觉证据如何参与推理过程尚不清晰，尤其是跨模态信息如何被有效利用缺乏深入理解。

Method: 基于跨模态注意力连通性分析，识别高连接性token作为视觉锚点；提出AT-RL框架，利用图聚类对注意力拓扑进行建模，仅选择性地强化这些锚点token。

Result: AT-RL在3B–32B系列模型上仅引入1.2%开销，使32B模型在MathVista上达80.2分，超越72B-Instruct基线；在STEM、视频及通用任务中均表现一致提升；而仅训练低连接性token则导致性能严重下降。

Conclusion: 多模态强化学习的效果关键不在于token数量，而在于跨模态锚定的保真度；高质量推理依赖于对少数关键视觉锚点的精准信用分配。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.

</details>


### [169] [AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.11510)
*Faouzi El Yagoubi,Ranwa Al Mallah,Godwin Badu-Marfo*

Main category: cs.AI

TL;DR: 本文提出AgentLeak，首个面向多智能体LLM系统的全栈隐私泄露基准，揭示输出审计无法检测的内部通道（如智能体间消息）是主要隐私风险来源，实证显示内部泄露率显著高于输出通道，且模型安全对齐训练可能提升内部通道防护能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注LLM输出通道的隐私泄露，无法评估多智能体系统中敏感数据经由智能体间消息、共享内存和工具参数等内部通道泄露的风险。

Method: 构建AgentLeak基准：涵盖4个领域共1000个场景；提出32类攻击分类法；设计三级检测流程；在5个主流LLM上对4979条执行轨迹进行实证测量与对比分析。

Result: 多智能体配置虽降低输出通道泄露率（C1: 27.2% vs 单智能体43.2%），但因未监控的内部通道（尤其C2：智能体间消息，泄露率68.8%），系统总泄露率达68.9%，输出审计遗漏41.7%违规；Claude 3.5 Sonnet在内外通道均表现最优（外部3.3%，内部28.1%）；C2 > C1规律在所有模型与领域中一致成立。

Conclusion: 多智能体LLM系统的隐私风险主因在于未受控的内部通信通道，亟需在协调框架中嵌入针对内部通道（尤其是智能体间消息）的隐私保护机制与强制管控策略。

Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.

</details>


### [170] [Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems](https://arxiv.org/abs/2602.11516)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出一种人类启发的持续学习框架，将推理、行动、反思与验证统一于增强型顺序推理模型中，并通过并行学习优化内部推理结构本身，实现边执行边学习，显著提升系统适应性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法多关注任务输出或静态知识表示，忽视内部推理结构、动作调度策略及学习机制本身的持续优化，难以支持动态环境中的长期自适应。

Method: 提出一个统一推理、行动、反思和验证的序列推理模型，引入并行学习；将内部思维过程作为首要学习对象，记录推理轨迹与环境交互作为结构化学习材料；支持预定义逻辑向学习程序的可控替换，并设计分层的‘学会学习’机制，协同优化任务参数与学习策略。

Result: 在温度传感器异常检测任务中，引入内部过程学习使平均运行时间减少23.9%。

Conclusion: 该框架使AI系统能在执行过程中持续演化其内部认知架构，在保持运行稳定性的同时提升适应性与效率。

Abstract: Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.

</details>


### [171] [CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference](https://arxiv.org/abs/2602.11527)
*Jiawei Zhu,Wei Chen,Ruichu Cai*

Main category: cs.AI

TL;DR: 本文提出CausalAgent，一个基于多智能体、RAG和MCP的对话式端到端因果推断系统，使非专业用户可通过自然语言交互完成从数据清洗到报告生成的全流程因果分析。


<details>
  <summary>Details</summary>
Motivation: 传统因果分析流程技术门槛高，需统计与计算机双重背景，且需人工选算法、处理数据质量问题、解释复杂结果。

Method: 构建融合多智能体系统（MAS）、检索增强生成（RAG）和模型上下文协议（MCP）的对话式系统，支持自然语言驱动的数据清洗、因果结构学习、偏差校正与报告生成，并引入交互式可视化。

Result: 实现用户仅需上传数据集并用自然语言提问，即可获得严谨、可交互的因果分析报告；显著降低因果分析使用门槛，同时保障过程的严谨性与可解释性。

Conclusion: CausalAgent开创了以用户为中心的人机协同因果分析新范式，通过显式建模分析流程与交互可视化，弥合了因果推理技术与实际应用之间的鸿沟。

Abstract: Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.

</details>


### [172] [Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use](https://arxiv.org/abs/2602.11541)
*Hanbing Liu,Chunhao Tian,Nan An,Ziyuan Wang,Pinyan Lu,Changyuan Yu,Qi Qi*

Main category: cs.AI

TL;DR: 本文提出INTENT框架，用于在预算约束下提升工具增强型智能体的任务成功率，通过意图感知的分层世界模型实现在线决策规划。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型智能体在严格金钱预算下难以进行有效多步任务规划，因状态-动作空间巨大、结果高方差及探索成本高昂，导致直接规划不可行。

Method: 提出INTENT推理时规划框架，构建意图感知的分层世界模型，以预测未来工具调用、风险校准成本，并在线引导决策。

Result: 在成本增强版StableToolBench上，INTENT严格满足硬性预算约束，显著优于基线方法，并在工具价格变动和预算变化等动态市场条件下保持鲁棒性。

Conclusion: INTENT为预算受限的工具调用决策提供了可扩展、稳健且实用的推理时规划范式。

Abstract: We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.

</details>


### [173] [SemaPop: Semantic-Persona Conditioned Population Synthesis](https://arxiv.org/abs/2602.11569)
*Zhenlin Qin,Yancheng Ling,Leizhen Wang,Francisco Câmara Pereira,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出SemaPop模型，结合大语言模型与生成式人口建模，通过语义化人格表征和边际正则化，实现兼具统计一致性与行为语义可解释性的人口合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖结构化属性和统计约束，难以捕捉调查数据中隐含的抽象行为模式，缺乏语义条件化的人口生成能力。

Method: 提出SemaPop框架，利用LLM从调查记录中提取高层人格表征作为语义条件信号，并结合Wasserstein GAN-GP与边际正则化进行生成建模。

Result: SemaPop-GAN在边际和联合分布对齐、样本可行性与多样性方面均优于基线；消融实验验证了语义条件与架构设计的有效性。

Conclusion: SemaPop-GAN实现了可控、可解释的语义-统计融合人口合成，为融合个体行为语义与总体统计约束的生成式人口预测系统提供了模块化基础。

Abstract: Population synthesis is a critical component of individual-level socio-economic simulation, yet remains challenging due to the need to jointly represent statistical structure and latent behavioral semantics. Existing population synthesis approaches predominantly rely on structured attributes and statistical constraints, leaving a gap in semantic-conditioned population generation that can capture abstract behavioral patterns implicitly in survey data. This study proposes SemaPop, a semantic-statistical population synthesis model that integrates large language models (LLMs) with generative population modeling. SemaPop derives high-level persona representations from individual survey records and incorporates them as semantic conditioning signals for population generation, while marginal regularization is introduced to enforce alignment with target population marginals. In this study, the framework is instantiated using a Wasserstein GAN with gradient penalty (WGAN-GP) backbone, referred to as SemaPop-GAN. Extensive experiments demonstrate that SemaPop-GAN achieves improved generative performance, yielding closer alignment with target marginal and joint distributions while maintaining sample-level feasibility and diversity under semantic conditioning. Ablation studies further confirm the contribution of semantic persona conditioning and architectural design choices to balancing marginal consistency and structural realism. These results demonstrate that SemaPop-GAN enables controllable and interpretable population synthesis through effective semantic-statistical information fusion. SemaPop-GAN also provides a promising modular foundation for developing generative population projection systems that integrate individual-level behavioral semantics with population-level statistical constraints.

</details>


### [174] [Learning to Configure Agentic AI Systems](https://arxiv.org/abs/2602.11574)
*Aditya Taparia,Som Sagar,Ransalu Senanayake*

Main category: cs.AI

TL;DR: 本文提出ARC框架，通过强化学习动态调整LLM代理系统的每查询配置（如工作流、工具、token预算和提示），在多个基准测试中显著提升任务准确率并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统依赖固定模板或人工调优的启发式方法进行配置，导致对简单和复杂查询采用相同笨重配置，造成行为脆弱和计算浪费。

Method: 将代理配置建模为逐查询决策问题，设计轻量级分层策略，利用强化学习进行训练以动态选择最优配置。

Result: 在推理和工具增强问答等多类基准上，ARC持续超越强手工设计及其他基线方法，最高提升任务准确率25%，同时减少token消耗和运行时间。

Conclusion: 学习逐查询的代理配置是一种优于‘一刀切’设计的高效可行方案，可兼顾性能与效率。

Abstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to "one size fits all" designs.

</details>


### [175] [The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs](https://arxiv.org/abs/2602.11583)
*Jingdi Chen,Hanqing Yang,Zongjun Liu,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 本文综述了多智能体通信（MA-Comm）的研究进展，围绕‘五个W’（谁、与谁、什么、何时、为何）展开分析，梳理了多智能体强化学习（MARL）、涌现语言（EL）和大语言模型（LLM）三大范式下的通信方法演进、设计权衡与未解挑战，并提出支持可扩展、可解释多智能体协作的混合系统设计方向。


<details>
  <summary>Details</summary>
Motivation: 多智能体在动态、部分可观测环境中需依赖通信降低不确定性、实现协作；现有通信方法存在任务特异性、可解释性差、泛化与可扩展性不足等问题，亟需系统性梳理与跨范式整合。

Method: 采用‘五个W’（Who, Whom, What, When, Why）框架对MA-Comm进行结构化综述，横向对比MARL、涌现语言（EL）和LLM三大范式，分析其通信机制、设计选择、权衡关系及共性挑战。

Result: 厘清了三类范式下通信设计的关键差异与演进脉络，提炼出实用设计模式，识别出可解释性、接地性、泛化性、可扩展性等核心未解问题，并为融合学习、语言与控制的下一代混合系统提供方向指引。

Conclusion: MA-Comm研究正从隐式、任务专用协议走向更结构化、语言驱动、开放域的协作范式；未来需构建兼顾性能、可解释性与通用性的混合架构，推动多智能体系统在真实复杂场景中的落地。

Abstract: Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.

</details>


### [176] [MAPLE: Modality-Aware Post-training and Learning Ecosystem](https://arxiv.org/abs/2602.11596)
*Nikhil Verma,Minjung Kim,JooYoung Yoo,Kyung-Min Jin,Manasa Bharadwaj,Kevin Ferreira,Ko Keun Kim,Youngjoon Kim*

Main category: cs.AI

TL;DR: 本文提出MAPLE，一个模态感知的多模态强化学习后训练框架，通过模态感知策略优化（MAPO）、专用基准MAPLE-bench和自适应加权与课程调度，显著提升训练效率、鲁棒性和跨模态性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练忽略不同任务对模态的实际需求，导致梯度方差大、收敛慢、对现实分布偏移（如模态缺失或重加权）鲁棒性差。

Method: 提出MAPLE生态：包括标注最小必要模态组合的基准MAPLE-bench、按模态需求分层采样的策略优化框架MAPO，以及自适应加权与课程调度机制；系统分析了损失聚合、裁剪、采样和课程设计等策略。

Result: MAPLE缩小单/多模态准确率差距30.24%，收敛速度提升3.18倍，并在模态信号减少的真实场景下保持各组合下的稳定性。

Conclusion: MAPLE为部署就绪的多模态RL后训练提供了完整、实用且鲁棒的解决方案。

Abstract: Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.

</details>


### [177] [scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery](https://arxiv.org/abs/2602.11609)
*Yiming Gao,Zhen Wang,Jefferson Chen,Mark Antkowiak,Mengzhou Hu,JungHo Kong,Dexter Pratt,Jieyuan Liu,Enze Ma,Zhiting Hu,Eric P. Xing*

Main category: cs.AI

TL;DR: scPilot 是首个支持组学原生推理的系统框架，使大语言模型能直接分析单细胞RNA-seq数据并调用生物信息工具，显著提升细胞类型注释与发育轨迹推断等任务的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在单细胞组学分析中缺乏对原始数据的直接感知和迭代推理能力，难以实现可审计、可解释、诊断级的分析。

Method: 提出scPilot框架，将单细胞核心分析任务（如细胞类型注释、发育轨迹重建、转录因子靶向）建模为多步推理问题；构建scBench评测基准（含9个专家标注数据集及评估器）；结合o1和Gemini-2.5-Pro等模型进行迭代式组学原生推理实验。

Result: 相比单次提示，o1在细胞类型注释准确率上提升11%，Gemini-2.5-Pro将轨迹图编辑距离降低30%；生成透明推理链，可解释标记基因歧义与调控逻辑。

Conclusion: scPilot通过将LLM锚定于原始组学数据，实现了可审计、可解释、诊断信息丰富的单细胞分析，为AI驱动的精准生物学研究奠定新范式。

Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.
  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.
  Code, data, and package are available at https://github.com/maitrix-org/scPilot

</details>


### [178] [When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents](https://arxiv.org/abs/2602.11619)
*Aman Mehta*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLM）代理在相同任务上多次运行时的行为一致性问题，发现行为不一致与任务失败高度相关，并指出早期决策（尤其是第一步搜索查询）是主要分歧来源，提出通过监控行为一致性实现早期错误检测以提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 探究LLM代理在重复执行相同任务时行为是否一致，及其与任务成功率之间的关系，以理解并提升代理的可靠性。

Method: 对3000次代理运行（涵盖Llama 3.1 70B、GPT-4o、Claude Sonnet 4.5三模型，在HotpotQA数据集上使用ReAct风格代理）进行统计分析，量化动作序列多样性（每10次运行中不同动作路径数），并关联其与准确率的关系；进一步定位行为分歧发生的时间步。

Result: ReAct代理平均每10次运行产生2.0–4.2条不同动作路径；行为一致性高（≤2条路径）的任务准确率达80–92%，而一致性低（≥6条路径）的任务仅25–60%；69%的分歧发生在第2步（即首个搜索查询）。

Conclusion: LLM代理的行为不一致性是可测量且具预测性的失败信号；监控执行过程中的行为一致性有助于早期识别潜在错误，从而提升代理鲁棒性与可靠性。

Abstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.

</details>


### [179] [Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families](https://arxiv.org/abs/2602.11630)
*Yipeng Huang,Dejun Xu,Zexin Lin,Zhenzhong Wang,Min Jiang*

Main category: cs.AI

TL;DR: 本文提出NMIPS框架，结合神经网络与多任务符号求解，通过多因子优化和仿射迁移方法，高效求解PDE族，兼顾高精度与解析解的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法求解PDE族计算成本高；现有机器学习PDE求解器缺乏解析表达与科学可解释性。

Method: 提出神经辅助多任务符号PDE求解框架NMIPS，采用多因子优化同步发现多个PDE的解析解，并设计仿射迁移方法在PDE族内复用已学数学结构。

Result: 在多个实验案例中显著优于基线方法，准确率最高提升约35.7%，同时输出可解释的解析解。

Conclusion: NMIPS在保持计算效率的同时，首次实现了对PDE族的高精度、可解释符号求解， bridging numerical speed and analytical insight.

Abstract: Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\sim$35.7% increase in accuracy while providing interpretable analytical solutions.

</details>


### [180] [Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation](https://arxiv.org/abs/2602.11635)
*Shuo Lu,Jianjie Cheng,Yinuo Xu,Yongcan Yu,Lijun Sheng,Peijie Wang,Siru Jiang,Yongguan Hu,Run Ling,Yihua Shao,Ao Ma,Wei Feng,Lingxiao He,Meng Wang,Qianlong Xie,Xingxing Wang,Ran He,Jian Liang*

Main category: cs.AI

TL;DR: 本文提出MathSpatial框架，用于评估和提升多模态大语言模型（MLLMs）的数学空间推理能力，包含基准测试集、训练数据集和结构化推理方法，显著提升了模型性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在感知任务上表现优异，但在数学空间推理（如二维/三维关系解析与操作）方面存在明显短板，人类准确率超95%，而主流MLLMs普遍低于60%，亟需系统性评估与改进方法。

Method: 构建MathSpatial统一框架，包括三部分：(i) MathSpatial-Bench（2K题基准，隔离感知噪声以聚焦推理难度）；(ii) MathSpatial-Corpus（8K带验证解的训练数据）；(iii) MathSpatial-SRT（将推理建模为Correlate、Constrain、Infer三种原子操作构成的结构化轨迹）。对Qwen2.5-VL-7B进行微调验证。

Result: 在MathSpatial-Bench上，微调后的Qwen2.5-VL-7B达到具竞争力的准确率，同时推理token减少25%；MathSpatial成为首个大规模解耦感知与推理的资源。

Conclusion: 空间推理是当前MLLMs的关键瓶颈，MathSpatial提供了可量化、可改进的系统性解决方案，推动该方向的精准评估与能力提升。

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.

</details>


### [181] [Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm](https://arxiv.org/abs/2602.11661)
*Tianxiang Xu,Jiayi Liu,Yixuan Tong,Jialu Xu,Yunqing Wei,Kaiwen Feng,PanPan Hou,Kangping Yin,Jiyuan Hu,Hao Zhou,Zhenxin Ma,Jian Xu,Guanjun Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种面向医疗问答场景的鲁棒大模型对齐新范式，通过构建多维对齐矩阵和统一优化机制，解决现有RLHF和基于可验证奖励的强化学习在医疗领域存在的标注昂贵、验证困难、多目标冲突等问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习对齐方法（如RLHF和基于可验证奖励的方法）在高风险医疗问答中存在根本性不匹配：偏好标注成本高且难以反映医学事实绝对正确性；缺乏有效自动验证器且难处理复杂临床语境；多目标（正确性、安全性、合规性）异构奖励易导致尺度失配与优化冲突。

Method: 提出一种鲁棒医疗对齐范式：1）构建四维医疗对齐矩阵（基础能力、专家知识、在线反馈、格式规范），每维建立‘可观测指标→归因诊断→可优化奖励’闭环，提供细粒度监督信号；2）设计统一优化机制，包括Reference-Frozen Normalization（对齐奖励尺度）和Tri-Factor Adaptive Dynamic Weighting（弱项导向、风险优先、冗余削减的动态加权）。

Result: 实验表明该范式在真实医疗场景评估中显著有效，提升了模型在正确性、安全性和合规性上的综合表现。

Conclusion: 本工作建立了面向垂直领域（尤其是高风险医疗）复杂对齐任务的新范式，为多目标异构奖励下的稳健优化提供了系统性解决方案。

Abstract: While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are prohibitively expensive and often fail to reflect the absolute correctness of medical facts. Reinforcement Learning from Verifiable Rewards lacks effective automatic verifiers and struggles to handle complex clinical contexts. Meanwhile, medical alignment requires the simultaneous optimization of correctness, safety, and compliance, yet multi-objective heterogeneous reward signals are prone to scale mismatch and optimization conflicts.To address these challenges, we propose a robust medical alignment paradigm. We first construct a holistic multi-dimensional medical alignment matrix that decomposes alignment objectives into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. Within each category, we establish a closed loop of where observable metrics inform attributable diagnosis, which in turn drives optimizable rewards, thereby providing fine-grained, high-resolution supervision signals for subsequent iterative optimization. To resolve gradient domination and optimization instability problem caused by heterogeneous signals, we further propose a unified optimization mechanism. This mechanism employs Reference-Frozen Normalization to align reward scales and implements a Tri-Factor Adaptive Dynamic Weighting strategy to achieve collaborative optimization that is weakness-oriented, risk-prioritized, and redundancy-reducing. Experimental results demonstrate the effectiveness of our proposed paradigm in real-world medical scenario evaluations, establishing a new paradigm for complex alignment in vertical domains.

</details>


### [182] [PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics](https://arxiv.org/abs/2602.11666)
*E Fan,Lisong Shi,Zhengtong Li,Chih-yung Wen*

Main category: cs.AI

TL;DR: 本文提出PhyNiKCE框架，通过神经符号解耦（神经规划+符号验证）解决LLM在CFD任务中因语义-物理脱节导致的物理无效输出问题，显著提升可靠性与效率。


<details>
  <summary>Details</summary>
Motivation: LLM在CFD等物理仿真中因概率性本质难以满足守恒律与数值稳定性要求；纯语义RAG易引发'上下文中毒'，生成语言合理但物理错误的配置。

Method: 提出PhyNiKCE：神经符号框架，解耦神经规划与符号验证；引入符号知识引擎，将仿真设置建模为约束满足问题，并通过确定性RAG引擎（针对求解器、湍流模型、边界条件定制检索）刚性施加物理约束。

Result: 在OpenFOAM非教程级CFD任务中，相比SOTA基线相对提升96%；自主纠错循环减少59%，LLM token消耗降低17%。

Conclusion: 神经生成与符号约束执行的解耦可显著增强工业AI系统的鲁棒性与效率；该架构具备可扩展性与可审计性，适用于更广泛的可信工业自动化场景。

Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to "context poisoning," where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.

</details>


### [183] [Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs](https://arxiv.org/abs/2602.11674)
*Longyuan Zhu,Hairan Hua,Linlin Miao,Bing Zhao*

Main category: cs.AI

TL;DR: 本文提出Benchmark Health Index (BHI)，一种纯数据驱动的框架，用于从能力区分度、抗饱和性与影响力三方面评估基准测试集的健康状况，以应对大模型评测中日益严重的分数膨胀与选择性报告问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评测基准因分数膨胀和选择性报告而可信度下降，社区亟需可信赖的基准评估方法。

Method: 提出Benchmark Health Index（BHI）框架，从能力区分度（Capability Discrimination）、抗饱和性（Anti-Saturation）和影响力（Impact）三个正交维度对评测基准进行量化审计，并基于91个代表性模型的技术报告中提取的106个已验证基准开展系统分析。

Result: 首次实现了对评测基准健康状况的宏观量化评估，为基准选择提供原则性依据，并支持下一代评测协议的动态生命周期管理。

Conclusion: BHI是首个面向评测基准整体健康度的数据驱动审计框架，有助于重建大模型评测的可信性与可持续性。

Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.

</details>


### [184] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 本文揭示了机器学习系统因自回归训练导致的因果推理缺陷（Rung Collapse）和错误因果模型固化（Aleatoric Entrenchment），提出基于信念修正的Epistemic Regret Minimization（ERM）方法，并通过物理基础定理、AGM兼容性证明和失败模式分类法构建三层架构，理论保证干预分布恢复，实验验证ERM在多种大模型上显著纠正因果陷阱。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型“答对但理由错”的问题，即模型依赖统计关联而非因果机制，在分布偏移下失效；揭示其深层因果根源——自回归训练无法区分条件概率与干预概率。

Method: 提出Epistemic Regret Minimization（ERM）作为因果信念修正目标，并构建三层知识表示架构：（1）物理接地定理，证明满足执行器独立性的动作对应有效do操作；（2）将ERM形式化为满足AGM公设的因果信念修正算子；（3）建立通用失败模式分类与防护机制。理论给出渐近恢复真实干预分布的有限样本界。

Result: 在1360个因果陷阱场景、6个前沿大语言模型上的实验表明：Rung Collapse普遍存在（GPT-5.2达3.7%）；模型越先进，越难被通用修正（逆向缩放）；而靶向ERM反馈可恢复53–59%的已固化错误，远超结果级反馈。

Conclusion: Rung Collapse是自回归学习固有的因果建模缺陷，仅优化任务性能不足以保障因果正确性；ERM通过解耦因果信念修正与任务成功，提供可证明、可迁移、可扩展的因果鲁棒性提升路径。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [185] [Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing](https://arxiv.org/abs/2602.11678)
*Chengwei Ma,Zhen Tian,Zhou Zhou,Zhixian Xu,Xiaowei Zhu,Xia Hua,Si Shi,F. Richard Yu*

Main category: cs.AI

TL;DR: 本文提出了一种Vector-to-Graph（V2G）方法，将CAD图纸转化为属性图，以显式建模工程原理图中的结构关系，显著提升电气合规性检查的准确性，超越现有基于像素的多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在工程原理图理解中存在‘结构盲区’，无法捕捉拓扑与符号逻辑，因其像素驱动范式丢失了向量定义的显式关系。

Method: 提出Vector-to-Graph（V2G）流程，将CAD图纸自动转换为属性图，其中节点表示组件、边表示连接关系，使结构依赖关系显式化且可机器审计。

Result: 在电气合规性诊断基准上，V2G在所有错误类别中均取得大幅准确率提升，而当前领先MLLMs表现接近随机水平。

Conclusion: 基于像素的方法在工程领域存在系统性不足；结构感知表征是推动多模态AI在工程中实用部署的可靠路径。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards the explicit vector-defined relations needed for reasoning. To overcome this, we propose a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs where nodes represent components and edges encode connectivity, making structural dependencies explicit and machine-auditable. On a diagnostic benchmark of electrical compliance checks, V2G yields large accuracy gains across all error categories, while leading MLLMs remain near chance level. These results highlight the systemic inadequacy of pixel-based methods and demonstrate that structure-aware representations provide a reliable path toward practical deployment of multimodal AI in engineering domains. To facilitate further research, we release our benchmark and implementation at https://github.com/gm-embodied/V2G-Audit.

</details>


### [186] [ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces](https://arxiv.org/abs/2602.11683)
*Xin Xu,Tong Yu,Xiang Chen,Haoliang Wang,Julian McAuley,Saayan Mitra*

Main category: cs.AI

TL;DR: 本文提出ThinkRouter，一种推理时置信度感知的路由机制，通过在低置信度时转向离散token空间、高置信度时使用潜在空间，提升推理准确率并缩短生成长度。


<details>
  <summary>Details</summary>
Motivation: 观察到潜推理中错误答案轨迹含更少低置信步骤，且多条低置信软嵌入聚合会引入并传播噪声，导致对不可靠推理路径产生高置信；因此需一种能规避高置信与噪声的高效推理机制。

Method: 提出ThinkRouter：推理时根据模型置信度动态路由——低置信时转向显式token空间，高置信时使用潜空间；结合STEM与编程基准进行多模型实验验证。

Result: 在多个STEM和编程基准上，ThinkRouter平均Pass@1提升19.70分，生成长度最多减少15.55%；同时可校准显式CoT与潜推理的错误，并通过全局降低置信度加速终态token生成。

Conclusion: ThinkRouter通过置信度感知的混合推理路由，在保持效率的同时显著提升准确性，为潜推理提供了更鲁棒、可解释的替代方案。

Abstract: Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.

</details>


### [187] [Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging](https://arxiv.org/abs/2602.11717)
*Weihong Lin,Lin Sun,Qilong Shi,Aomufei Yuan,Yuxuan Tian,Zhengyang Wang,Guangxiang Zhao,Xiangzheng Zhang,Tong Yang*

Main category: cs.AI

TL;DR: 本文提出了一种新的模型融合框架SCF-RKL，通过稀疏、分布感知的更新显式控制功能干扰，利用反向KL散度衡量模型间功能差异，并选择性地融合互补参数，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法多依赖参数空间启发式，易导致严重功能干扰，造成泛化能力下降和生成不稳定（如重复、不连贯）。

Method: 提出Sparse Complementary Fusion with reverse KL（SCF-RKL），放弃参数空间线性可加假设，改用反向KL散度度量模型间功能差异，并进行稀疏、模式导向的参数融合。

Result: 在24个涵盖高级推理、通用推理与知识、指令遵循及安全等任务的基准上，SCF-RKL在多种模型规模和架构（含推理型与指令微调型）上均一致优于现有融合方法，同时保持强泛化能力和生成稳定性。

Conclusion: SCF-RKL是一种更鲁棒、可控的模型融合范式，通过功能层面建模与稀疏互补机制，有效缓解传统权重融合中的干扰问题。

Abstract: Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which often introduce severe interference, leading to degraded generalization and unstable generation behaviors such as repetition and incoherent outputs. In this work, we propose Sparse Complementary Fusion with reverse KL (SCF-RKL), a novel model merging framework that explicitly controls functional interference through sparse, distribution-aware updates. Instead of assuming linear additivity in parameter space, SCF-RKL measures the functional divergence between models using reverse Kullback-Leibler divergence and selectively incorporates complementary parameters. This mode-seeking, sparsity-inducing design effectively preserves stable representations while integrating new capabilities. We evaluate SCF-RKL across a wide range of model scales and architectures, covering both reasoning-focused and instruction-tuned models. Extensive experiments on 24 benchmarks spanning advanced reasoning, general reasoning and knowledge, instruction following, and safety demonstrate, vision classification that SCF-RKL consistently outperforms existing model merging methods while maintaining strong generalization and generation stability.

</details>


### [188] [Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs](https://arxiv.org/abs/2602.11729)
*Thomas Jiralerspong,Trenton Bricken*

Main category: cs.AI

TL;DR: 本文首次将crosscoder应用于跨架构大模型差异分析，提出专用特征crosscoder（DFC）以更好分离单个模型的独特特征，并在无监督下发现多个模型中的特定安全相关行为特征。


<details>
  <summary>Details</summary>
Motivation: 现有模型差异分析主要局限于基础模型与其微调版本的比较，而新发布的LLM常采用新架构，因此需要支持跨架构的差异分析方法。

Method: 将crosscoder扩展至跨架构模型比较场景，并提出Dedicated Feature Crosscoders（DFCs）架构改进，以增强对单模型特有特征的隔离能力。

Result: 在Qwen3-8B和Deepseek-R1-0528-Qwen3-8B中识别出中共 Alignment 特征，在Llama3.1-8B-Instruct中识别出美国例外主义特征，在GPT-OSS-20B中识别出版权拒绝机制。

Conclusion: 跨架构crosscoder模型差异分析可有效识别AI模型间有意义的行为差异，为模型安全评估提供新工具。

Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.

</details>


### [189] [Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]](https://arxiv.org/abs/2602.11745)
*Songlin Lyu,Lujie Ban,Zihang Wu,Tianqi Luo,Jirong Liu,Chenhao Ma,Yuyu Luo,Nan Tang,Shipeng Qi,Heng Lin,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 本文提出了Text2GQL-Bench，一个统一的Text-to-GQL基准，包含178,184个跨13个领域的（问题，查询）对，并引入多维评估方法，揭示了现有LLM在ISO-GQL生成中存在显著方言差距。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-GQL数据集在领域覆盖、支持的图查询语言种类及评估范围上存在局限，缺乏高质量基准和系统性评估方法，阻碍了该方向发展。

Method: 构建了多GQL、多领域、多抽象层级的Text2GQL-Bench基准数据集，并设计涵盖语法正确性、相似度、语义对齐与执行准确率的综合评估方法。

Result: 发现强LLM在零样本ISO-GQL生成中执行准确率仅≤4%，3样本提示提升至约50%，但语法正确率仍<70%；微调8B开源模型达45.1%执行准确率和90.8%语法正确率。

Conclusion: Text2GQL-Bench有效弥补了Text-to-GQL领域基准缺失问题，实证表明充分接触ISO-GQL示例是提升性能的关键，凸显了数据驱动微调的重要性。

Abstract: Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manipulate graph data, posi-tioning them as powerful agent infrastructures for Graph Database Management System (GDBMS). Despite recent progress, existing datasets are often limited in domain coverage, supported graph query languages, or evaluation scope. The advancement of Text-to-GQL systems is hindered by the lack of high-quality benchmark datasets and evaluation methods to systematically compare model capabilities across different graph query languages and domains. In this work, we present Text2GQL-Bench, a unified Text-to-GQL benchmark designed to address these limitations. Text2GQL-Bench couples a multi-GQL dataset that has 178,184 (Question, Query) pairs spanning 13 domains, with a scalable construction framework that generates datasets in different domains, question abstraction levels, and GQLs with heterogeneous resources. To support compre-hensive assessment, we introduce an evaluation method that goes beyond a single end-to-end metric by jointly reporting grammatical validity, similarity, semantic alignment, and execution accuracy. Our evaluation uncovers a stark dialect gap in ISO-GQL generation: even strong LLMs achieve only at most 4% execution accuracy (EX) in zero-shot settings, though a fixed 3-shot prompt raises accuracy to around 50%, the grammatical validity remains lower than 70%. Moreover, a fine-tuned 8B open-weight model reaches 45.1% EX, and 90.8% grammatical validity, demonstrating that most of the performance jump is unlocked by exposure to sufficient ISO-GQL examples.

</details>


### [190] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: 本文提出了AIR，首个面向大语言模型（LLM）智能体系统的事故响应框架，支持自动检测、遏制、恢复与规则生成，显著提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体安全机制主要侧重于事前预防，缺乏对已发生事故的有效响应、控制与恢复能力。

Method: 提出AIR框架，定义面向事故响应生命周期的领域特定语言，并将其嵌入智能体执行循环中，实现基于语义的事故检测、工具驱动的遏制与恢复、以及规则自动生成以防止复发。

Result: 在三类典型智能体上验证，AIR各项成功率均超90%；实验证明其核心组件必要、响应及时、开销适中，且LLM生成的防护规则效果接近人工编写。

Conclusion: 事故响应是提升LLM智能体安全性的可行且必要的核心机制，应作为一类首要安全能力进行设计与部署。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [191] [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Holger Boche*

Main category: cs.AI

TL;DR: 本文提出TSR（Trajectory-Search Rollouts）方法，在训练阶段引入轻量级树形搜索，提升多轮交互中轨迹采样的质量，从而缓解稀疏/延迟奖励和环境随机性带来的训练不稳定性；该方法与优化器无关，在Sokoban、FrozenLake和WebShop任务上带来最高15%性能提升。


<details>
  <summary>Details</summary>
Motivation: 多轮强化学习中奖励稀疏或延迟、环境随机，导致朴素轨迹采样易陷入模式坍塌、削弱利用能力。

Method: TSR在训练时的rollout阶段引入轻量级树式搜索（如best-of-N、beam、浅层前向搜索），依据任务特定反馈在每步选择高分动作，构造高质量轨迹；不改变原始优化目标，兼容PPO和GRPO等算法。

Result: 在Sokoban、FrozenLake和WebShop任务上实现最高15%性能提升，训练过程更稳定，仅引入一次性训练计算开销。

Conclusion: 将搜索从推理阶段前移到训练的rollout阶段，TSR提供了一种简单、通用且与其他框架正交的增强多轮智能体学习机制。

Abstract: Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.

</details>


### [192] [How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?](https://arxiv.org/abs/2602.11771)
*Sébastien Gigot--Léandri,Gaétan Morand,Alexis Joly,François Munoz,David Mouillot,Christophe Botella,Maximilien Servajean*

Main category: cs.AI

TL;DR: 本文提出MaxExp和SSE两种新方法，用于将物种分布模型（SDM）的概率预测结果转化为二元存在-缺失图，以提升生态推断与保护规划的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有SDM的二值化步骤多为启发式，易扭曲物种丰度和群落组成估计，尤其在类不平衡和高稀有性情况下问题突出。

Method: 提出MaxExp框架——基于决策驱动、直接最大化选定评估指标来选择最可能的物种组合；同时引入计算高效的SSE方法，依据期望物种丰富度进行预测。

Result: 在三个涵盖不同类群、物种数量和评估指标的案例研究中，MaxExp持续优于常用阈值法和校准法；SSE虽更简单，但性能具竞争力。

Conclusion: MaxExp与SSE为多物种SDM二值化提供了稳健、可重复的新工具，显著改善生态应用中的预测可靠性。

Abstract: Species distribution models (SDMs) commonly produce probabilistic occurrence predictions that must be converted into binary presence-absence maps for ecological inference and conservation planning. However, this binarization step is typically heuristic and can substantially distort estimates of species prevalence and community composition. We present MaxExp, a decision-driven binarization framework that selects the most probable species assemblage by directly maximizing a chosen evaluation metric. MaxExp requires no calibration data and is flexible across several scores. We also introduce the Set Size Expectation (SSE) method, a computationally efficient alternative that predicts assemblages based on expected species richness. Using three case studies spanning diverse taxa, species counts, and performance metrics, we show that MaxExp consistently matches or surpasses widely used thresholding and calibration methods, especially under strong class imbalance and high rarity. SSE offers a simpler yet competitive option. Together, these methods provide robust, reproducible tools for multispecies SDM binarization.

</details>


### [193] [RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation](https://arxiv.org/abs/2602.11780)
*Jinfang Wang,Jiajie Liu,Jianwei Wu,Ziqin Luo,Zhen Chen,Chunlei Li,Biao Han,Tao Deng,Yi Li,Shuanglong Li,Lin Liu*

Main category: cs.AI

TL;DR: 本文提出RELATE框架，通过强化学习实现广告文案生成与效果指标对齐的一体化建模，显著提升转化率并满足合规约束。


<details>
  <summary>Details</summary>
Motivation: 现有工业系统采用两阶段范式（先生成候选文案，再对齐CTR等指标），导致优化目标不一致、漏斗效率低，难以达到全局最优。

Method: 提出基于强化学习的端到端框架RELATE，将性能目标（如转化率）和合规约束联合建模为多维奖励，在策略学习中直接集成生成与对齐过程。

Result: 在大规模工业数据集上显著优于基线；线上部署后在严格政策约束下显著提升点击转化率（CTCVR）。

Conclusion: RELATE实现了广告文案生成与业务价值、合规要求的统一优化，具备强鲁棒性与实际落地价值。

Abstract: In online advertising, advertising text plays a critical role in attracting user engagement and driving advertiser value. Existing industrial systems typically follow a two-stage paradigm, where candidate texts are first generated and subsequently aligned with online performance metrics such as click-through rate(CTR). This separation often leads to misaligned optimization objectives and low funnel efficiency, limiting global optimality.
  To address these limitations, we propose RELATE, a reinforcement learning-based end-to-end framework that unifies generation and objective alignment within a single model. Instead of decoupling text generation from downstream metric alignment, RELATE integrates performance and compliance objectives directly into the generation process via policy learning. To better capture ultimate advertiser value beyond click-level signals, We incorporate conversion-oriented metrics into the objective and jointly model them with compliance constraints as multi-dimensional rewards, enabling the model to generate high-quality ad texts that improve conversion performance under policy constraints.
  Extensive experiments on large-scale industrial datasets demonstrate that RELATE consistently outperforms baselines. Furthermore, online deployment on a production advertising platform yields statistically significant improvements in click-through conversion rate(CTCVR) under strict policy constraints, validating the robustness and real-world effectiveness of the proposed framework.

</details>


### [194] [FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning](https://arxiv.org/abs/2602.11782)
*Yihao Liu,Ziyun Zhang,Zile He,Huaqian Cai*

Main category: cs.AI

TL;DR: 本文提出Execute-Summarize（ES）框架，通过解耦任务执行与工作流构建，提升大语言模型生成结构化工具调用序列的准确性与鲁棒性，并在新基准FlowBench上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在执行任务的同时构建工作流，易因二者相互干扰导致工作流不准确；亟需一种能可靠地将LLM自由推理落地为结构化工作流的机制。

Method: 提出Execute-Summarize（ES）框架：先由模型调用工具完成任务（Execute），再基于执行轨迹独立总结生成结构化工作流（Summarize）；并构建新评测基准FlowBench。

Result: 在FlowBench上实验表明，ES框架显著优于现有方法，在工作流准确性、鲁棒性和泛化性方面均有提升。

Conclusion: 解耦执行与总结是构建高质量结构化工作流的有效范式，为LLM工具使用和推理落地提供了更可靠的基础。

Abstract: LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workflows. Prior approaches that build workflows during execution often suffer from inaccuracies due to interference between the two processes. We propose an Execute-Summarize(ES) framework that decouples task execution from workflow construction: the model first completes the task using available tools, then independently reconstructs a structured workflow from execution traces. This separation improves workflow accuracy and robustness. We introduce FlowBench and show through extensive experiments that our approach outperforms existing methods, providing a reliable paradigm for grounding free-form LLM reasoning into structured workflows.

</details>


### [195] [Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation](https://arxiv.org/abs/2602.11790)
*Lingyong Yan,Jiulong Wu,Dong Xie,Weixian Shi,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 本文提出LAVES，一种基于大语言模型的多智能体系统，用于从教育问题自动生成高质量教学视频，通过分层协作与质量控制机制，实现高保真、低成本、全自动的教学视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频生成模型在需逻辑严谨与知识精准表达的教学/教育场景中表现不足，存在步骤保真度低、制作成本高、可控性差等问题。

Method: 提出分层多智能体系统LAVES，包含统筹代理（Orchestrating Agent）、解题代理（Solution Agent）、图示代理（Illustration Agent）和讲解代理（Narration Agent），辅以语义批判、规则约束与工具化编译验证；生成结构化可执行视频脚本，再依模板规则确定性合成音画同步视频。

Result: 大规模部署中日均生成超百万教学视频，相较行业标准方法成本降低95%以上，且保持高接受率。

Conclusion: LAVES有效弥合了AI视频生成在教育场景中逻辑性、准确性与可生产性之间的鸿沟，为自动化高质量教学内容生成提供了新范式。

Abstract: Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

</details>


### [196] [Detecting RLVR Training Data via Structural Convergence of Reasoning](https://arxiv.org/abs/2602.11792)
*Hongbo Zhang,Yue Yang,Jianhao Yan,Guangsheng Bao,Yue Zhang,Yue Zhang*

Main category: cs.AI

TL;DR: 本文提出Min-kNN Distance方法，通过测量同一提示下多个生成结果间的最小编辑距离平均值，检测强化学习与可验证奖励（RLVR）训练导致的生成行为僵化现象，从而识别被污染的基准测试样本。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励（RLVR）训练中使用的未公开训练数据可能导致基准测试污染，而传统基于似然的检测方法在RLVR场景下效果不佳。

Method: 提出Min-kNN Distance黑盒检测器：对给定提示采样多个生成文本，计算其中k个最小编辑距离的平均值，以此量化生成多样性下降（即行为僵化）程度。无需访问目标模型或token概率。

Result: 在多个RLVR训练的推理模型上实验表明，Min-kNN Distance能可靠区分RL训练见过与未见过的提示，并优于现有成员推断和RL污染检测基线方法。

Conclusion: RLVR训练会在模型行为上留下可检测的‘僵化’签名；Min-kNN Distance是一种简单、有效、无需模型访问权限的RL污染检测新范式。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.

</details>


### [197] [Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation](https://arxiv.org/abs/2602.11799)
*Pingjun Pan,Tingting Zhou,Peiyao Lu,Tingting Fei,Hongxiang Chen,Chuanjiang Luo*

Main category: cs.AI

TL;DR: 本文提出Hi-SAM框架，通过解耦语义分词器（DST）和分层记忆锚点Transformer（HMAT），解决多模态推荐中语义ID分词冗余与架构-数据不匹配问题，在冷启动场景和真实平台部署中均显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义ID方法在多模态推荐中存在两个问题：一是分词缺乏跨模态共享语义与模态特异性细节的解耦，导致冗余或坍缩；二是标准Transformer将语义ID视为扁平序列，忽略用户-物品-令牌的层次结构，造成注意力偏向局部细节而非整体语义。

Method: 提出Hi-SAM框架：(1) 解耦语义分词器（DST），采用几何感知对齐与粗到细量化策略，结合共享与模态专用码本，并用互信息最小化约束；(2) 分层记忆锚点Transformer（HMAT），通过分层RoPE分离物品间/内位置编码，并引入Anchor Token压缩物品表征为紧凑记忆。

Result: 在多个真实数据集上持续超越SOTA基线，尤其在冷启动场景表现突出；在服务数百万用户的大型社交平台上线后，核心在线指标提升6.55%。

Conclusion: Hi-SAM通过结构感知的分层建模与解耦语义表示，有效缓解了多模态推荐中语义冗余与层次失配问题，兼具理论合理性与工业落地价值。

Abstract: Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.

</details>


### [198] [PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts](https://arxiv.org/abs/2602.11807)
*Lianjun Wu,Shengchen Zhu,Yuxuan Liu,Liuyu Kai,Xiaoduan Feng,Duomin Wang,Wenshuo Liu,Jingxuan Zhang,Kelvin Li,Bin Wang*

Main category: cs.AI

TL;DR: 本文提出PuYun-LDM，结合3D Masked AutoEncoder与Variable-Aware Masked Frequency Modeling，提升高分辨率气象预报中潜在扩散模型的可扩散性，在短时预报中优于集合预报（ENS），长时预报中与ENS相当，且推理效率显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型（LDMs）在高分辨率（≤0.25°）集合天气预报中受限于低“可扩散性”；气象场缺乏通用基础模型和显式语义结构，使基于视觉基础模型（VFM）的正则化不可行；频率方法忽略多变量间频谱异质性，导致不均衡正则化。

Method: 提出3D Masked AutoEncoder（3D-MAE）编码天气状态演化特征作为扩散模型的额外条件输入，并设计Variable-Aware Masked Frequency Modeling（VA-MFM）策略，依据各气象变量的频谱能量分布自适应设定掩码阈值。

Result: PuYun-LDM在15天全球预报任务中，以6小时时间分辨率，在单块NVIDIA H200 GPU上仅需5分钟完成推断；短时预报性能优于集合预报（ENS），长时预报与ENS相当；支持高效并行生成集合预报。

Conclusion: 通过引入物理感知的时空特征建模与变量自适应频域正则化，PuYun-LDM有效提升了气象场潜在空间的可扩散性，为高分辨率、高效率数值天气预报提供了新范式。

Abstract: Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25°) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.

</details>


### [199] [Predicting LLM Output Length via Entropy-Guided Representations](https://arxiv.org/abs/2602.11812)
*Huanyi Xie,Yubin Chen,Liangyu Wang,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级框架，利用大模型内部隐藏状态进行序列长度预测，以减少长尾分布导致的填充浪费，包含静态预测（EGTP）和动态预测（PLP）两种方法，并构建了ForeLen基准进行验证。


<details>
  <summary>Details</summary>
Motivation: 长尾分布的序列长度在大模型服务和强化学习采样中导致大量因填充引起的计算浪费；现有基于辅助模型的静态长度预测方法开销高、泛化差，且无法应对随机的'一对多'采样场景。

Method: 提出一种复用主模型内部隐藏状态的轻量级长度预测框架，包含两个核心组件：1）熵引导的Token池化（EGTP），利用实时激活和token熵实现高精度、零开销静态预测；2）渐进式长度预测（PLP），在每个解码步动态估计剩余长度以应对随机生成。同时构建并开源了ForeLen基准。

Result: 在ForeLen基准上，EGTP将平均绝对误差（MAE）较最优基线降低29.16%；与长度感知调度器集成后，端到端吞吐量显著提升。

Conclusion: 该工作为高效大模型推理提供了新的技术路径与评估基准。

Abstract: The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generalize poorly, and fail in stochastic "one-to-many" sampling scenarios. We introduce a lightweight framework that reuses the main model's internal hidden states for efficient length prediction. Our framework features two core components: 1) Entropy-Guided Token Pooling (EGTP), which uses on-the-fly activations and token entropy for highly accurate static prediction with negligible cost, and 2) Progressive Length Prediction (PLP), which dynamically estimates the remaining length at each decoding step to handle stochastic generation. To validate our approach, we build and release ForeLen, a comprehensive benchmark with long-sequence, Chain-of-Thought, and RL data. On ForeLen, EGTP achieves state-of-the-art accuracy, reducing MAE by 29.16\% over the best baseline. Integrating our methods with a length-aware scheduler yields significant end-to-end throughput gains. Our work provides a new technical and evaluation baseline for efficient LLM inference.

</details>


### [200] [Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2602.11824)
*Jialin Wu,Wei Shi,Han Shen,Peigui Qi,Kunsheng Tang,Zhicong Huang,Binghao Wang,Zhou Yang*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的框架REVIS，通过正交投影提取纯视觉信息，并在特定网络深度进行稀疏干预，以缓解大视觉语言模型中的物体幻觉问题，显著降低幻觉率约19%，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）常出现物体幻觉，部分原因是深层网络中视觉特征与预训练文本表征相互纠缠，导致视觉信息被抑制。

Method: REVIS基于潜在空间几何，通过正交投影提取纯视觉信息向量，并采用校准策略仅在视觉信息被抑制的确切深度进行稀疏干预。

Result: 在标准基准上实验表明，REVIS相比当前最优基线将物体幻觉率降低约19%，且计算开销小、不损害通用推理能力。

Conclusion: REVIS是一种高效、训练-free的视觉信息恢复方法，能精准缓解LVLMs中的物体幻觉问题。

Abstract: Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.

</details>


### [201] [Prototype Transformer: Towards Language Model Architectures Interpretable by Design](https://arxiv.org/abs/2602.11852)
*Yordan Yordanov,Matteo Forasassi,Bayar Menzat,Ruizhi Wang,Chang Qi,Markus Kaltenberger,Amine M'Charrak,Tommaso Salvatori,Thomas Lukasiewicz*

Main category: cs.AI

TL;DR: 本文提出Prototype Transformer（ProtoT），一种基于原型（参数向量）的新型自回归语言模型架构，旨在提升模型可解释性与计算效率；其通过输入与原型间的双向通信自动学习可命名概念，支持推理路径可视化和行为编辑，并在性能、鲁棒性和线性扩展性上媲美或优于现有Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有先进语言模型虽性能强大，但推理过程不透明，易导致欺骗与幻觉；需设计一种可解释性强、同时保持高性能的新型架构。

Method: 提出Prototype Transformer（ProtoT），以原型（parameter vectors）替代标准自注意力机制，实现输入序列与原型之间的双向通信；原型在训练中自动捕获可命名语义概念，并构建多时间尺度上下文聚合通道。

Result: ProtoT在文本生成和下游任务（如GLUE）上接近SOTA性能；计算复杂度为序列长度的线性关系（优于自注意力的平方关系）；对输入扰动具有强鲁棒性，且提供可解释的稳健性/敏感性路径。

Conclusion: ProtoT是一种可解释性优先、性能不妥协的语言模型新范式，为构建‘按设计可解释’的高性能自回归LM提供了可行路径。

Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. "woman") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.

</details>


### [202] [Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models](https://arxiv.org/abs/2602.11860)
*Lu Tao,Jinxuan Luo,Yousuke Watanabe,Zhengshu Zhou,Yuhuan Lu,Shen Ying,Pan Zhang,Fei Zhao,Hiroaki Takada*

Main category: cs.AI

TL;DR: 本文提出Talk2DM，一种支持自然语言查询和常识推理的插件模块，用于增强车路云协同动态地图系统；基于新构建的VRC-QA数据集和VRCsim仿真框架，采用链式提示（CoP）机制融合人工规则与大语言模型常识，实现在多模型下93%以上准确率与2-5秒响应速度的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有动态地图系统缺乏自然语言支持的人机接口，限制了人与动态地图的交互效率，亟需提升可访问性与智能化水平。

Method: 构建VRCsim车路云协同感知仿真框架生成流式数据，据此创建VRC-QA空间问答数据集；提出Talk2DM插件模块，采用链式提示（CoP）机制融合人工规则与大语言模型常识知识。

Result: Talk2DM在VRC-QA上实现超93%自然语言查询准确率，平均响应时间仅2–5秒；支持Qwen3:8B、Gemma3:27B、GPT-oss等多模型切换，展现强泛化性与实用性。

Conclusion: Talk2DM有效 bridging 了动态地图系统与人类自然语言交互之间的鸿沟，为车路云协同自动驾驶提供了更直观、智能、可扩展的人机接口范式。

Abstract: Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical occlusions. Although DM-enhanced ADS have been successfully deployed in real-world applications in Japan, existing DM systems still lack a natural-language-supported (NLS) human interface, which could substantially enhance human-DM interaction. To address this gap, this paper introduces VRCsim, a VRC cooperative perception (CP) simulation framework designed to generate streaming VRC-CP data. Based on VRCsim, we construct a question-answering data set, VRC-QA, focused on spatial querying and reasoning in mixed-traffic scenes. Building upon VRCsim and VRC-QA, we further propose Talk2DM, a plug-and-play module that extends VRC-DM systems with NLS querying and commonsense reasoning capabilities. Talk2DM is built upon a novel chain-of-prompt (CoP) mechanism that progressively integrates human-defined rules with the commonsense knowledge of large language models (LLMs). Experiments on VRC-QA show that Talk2DM can seamlessly switch across different LLMs while maintaining high NLS query accuracy, demonstrating strong generalization capability. Although larger models tend to achieve higher accuracy, they incur significant efficiency degradation. Our results reveal that Talk2DM, powered by Qwen3:8B, Gemma3:27B, and GPT-oss models, achieves over 93\% NLS query accuracy with an average response time of only 2-5 seconds, indicating strong practical potential.

</details>


### [203] [Intelligent AI Delegation](https://arxiv.org/abs/2602.11865)
*Nenad Tomašev,Matija Franklin,Simon Osindero*

Main category: cs.AI

TL;DR: 本文提出了一种面向AI代理的自适应智能委托框架，强调任务分配、权责转移、角色边界、意图明确与信任建立等关键要素，以应对动态环境与意外失败。


<details>
  <summary>Details</summary>
Motivation: 现有任务分解与委托方法依赖简单启发式，无法动态适应环境变化和鲁棒处理意外失败。

Method: 提出一种包含任务分配、权责转移、责任界定、角色边界、意图澄清及信任机制的自适应AI委托框架。

Result: 该框架适用于人类与AI作为委托方或受托方的复杂委托网络，为新兴‘代理网络（agentic web）’中的协议设计提供理论支撑。

Conclusion: 该框架为实现更可靠、可扩展、人机协同的AI代理系统提供了结构化委托范式。

Abstract: AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.

</details>


### [204] [From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders](https://arxiv.org/abs/2602.11881)
*Yifan Luo,Yang Zhan,Jiedong Jiang,Tianyang Liu,Mingrui Wu,Zhennan Zhou,Bin Dong*

Main category: cs.AI

TL;DR: 本文提出层次稀疏自编码器（HSAE），通过联合学习多个稀疏自编码器及其特征间的父子关系，捕捉大语言模型中自然语言的层次结构，同时保持重建精度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAEs）提取的特征多为孤立，而大语言模型实际蕴含自然语言的内在层次结构（如特征分裂现象），需建模特征间的层级关系。

Method: 提出HSAE框架，联合学习多级SAEs及特征间的父子关系；引入结构约束损失和随机特征扰动机制以增强父子特征对齐。

Result: 在多个大语言模型及不同层上实验表明，HSAE能稳定恢复语义上有意义的层次结构，并在定性案例与定量指标上均得到验证；同时保持与标准SAE相当的重建保真度和可解释性。

Conclusion: HSAE是一种强大且可扩展的工具，可用于发现和分析大语言模型表征中嵌入的多尺度概念结构。

Abstract: Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of "feature splitting" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.

</details>


### [205] [When Should LLMs Be Less Specific? Selective Abstraction for Reliable Long-Form Text Generation](https://arxiv.org/abs/2602.11908)
*Shani Goren,Ido Galil,Ran El-Yaniv*

Main category: cs.AI

TL;DR: 本文提出Selective Abstraction (SA)框架，使大语言模型在面对不确定性时可选择性地降低输出内容的细节程度而非直接拒绝回答，从而在保持信息量的同时提升事实准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型易产生事实性错误，影响可信度；传统不确定性估计仅支持‘全有或全无’式拒绝回答，在长文本生成中过于保守，易丢失有价值信息。

Method: 提出Selective Abstraction（SA）框架，形式化定义选择性风险与覆盖率；具体实现为Atom-wise SA：将响应分解为原子级主张（atomic claims），对低置信度原子用更高置信、更抽象的表述替代。

Result: 在FactScore和LongFact-Objects六个开源模型上验证，Atom-wise SA显著优于现有基线（如claim removal），AURC指标最高提升27.73%，证明降细节可兼顾准确性与信息保留。

Conclusion: 选择性抽象是一种更灵活、更实用的不确定性应对机制，能在不牺牲大量信息的前提下有效提升LLM输出的事实可靠性。

Abstract: LLMs are widely used, yet they remain prone to factual errors that erode user trust and limit adoption in high-risk settings. One approach to mitigate this risk is to equip models with uncertainty estimation mechanisms that abstain when confidence is low. However, this binary "all-or-nothing" approach is excessively restrictive in long-form settings, often discarding valuable information. We introduce Selective Abstraction (SA), a framework that enables LLMs to trade specificity for reliability by selectively reducing the detail of uncertain content. We first formalize SA through the lenses of selective risk and coverage. We then propose Atom-wise Selective Abstraction, a claim-level instantiation that decomposes responses into atomic claims (short, self-contained statements each expressing a single fact) and replaces uncertain atoms with higher confidence, less specific abstractions. To evaluate this framework, we develop a novel end-to-end pipeline for open-ended generation that instantiates risk as factual correctness and measures coverage using an information-theoretic measure of retained information. Across six open-source models on the FactScore and LongFact-Objects benchmarks, atom-wise SA consistently outperforms existing baselines, improving the area under the risk-coverage curve (AURC) by up to 27.73% over claim removal, demonstrating that reducing specificity can boost accuracy and reliability while preserving most of their original meaning.

</details>


### [206] [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917)
*Taian Guo,Haiyang Shen,Junyu Luo,Binqi Chen,Hongjun Ding,Jinsheng Huang,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 本文提出AlphaPROBE框架，将alpha因子挖掘建模为有向无环图（DAG）上的导航问题，通过贝叶斯因子检索器和DAG感知的因子生成器，提升因子发现的多样性、效率与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化因子挖掘方法（解耦式生成与迭代式演化）缺乏全局结构视角，导致搜索冗余和因子多样性不足。

Method: 提出AlphaPROBE框架：1）构建因子DAG，节点为因子、边为演化关系；2）贝叶斯因子检索器基于后验概率平衡探索与利用以选取优质种子；3）DAG感知因子生成器利用完整祖先路径进行上下文感知、非冗余优化。

Result: 在三大中国股市数据集上，相比8种基线方法，AlphaPROBE显著提升预测精度、收益稳定性与训练效率。

Conclusion: 利用因子演化的全局拓扑结构对实现高效、鲁棒的自动化alpha发现至关重要。

Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.

</details>


### [207] [MEME: Modeling the Evolutionary Modes of Financial Markets](https://arxiv.org/abs/2602.11918)
*Taian Guo,Haiyang Shen,Junyu Luo,Zhongshi Xing,Hanchun Lian,Jinsheng Huang,Binqi Chen,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 本文提出MEME框架，从逻辑导向视角建模金融市场为动态演化的投资叙事生态系统，通过多智能体提取投资论点、高斯混合模型挖掘语义共识，并引入时序对齐机制追踪叙事生命周期，显著提升A股组合表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在量化金融中多聚焦个股预测或组合配置，缺乏对驱动市场变化的底层逻辑推理的建模。

Method: 提出Logic-Oriented视角，构建MEME框架：含多智能体提取模块生成高质量投资论点；高斯混合模型在语义空间发现潜在共识；时序评估与对齐机制追踪不同市场状态下语义漂移及模式盈亏生命周期。

Result: 在2023–2025年三个异构A股池上，MEME持续超越7个SOTA基线；消融实验、敏感性分析、生命周期案例及成本分析进一步验证其识别与适应市场共识演化的能力。

Conclusion: MEME通过聚焦持久市场智慧而非短期异常，使组合构建由稳健逻辑驱动，为LLM在量化金融中的可解释性与鲁棒性应用提供了新范式。

Abstract: LLMs have demonstrated significant potential in quantitative finance by processing vast unstructured data to emulate human-like analytical workflows. However, current LLM-based methods primarily follow either an Asset-Centric paradigm focused on individual stock prediction or a Market-Centric approach for portfolio allocation, often remaining agnostic to the underlying reasoning that drives market movements. In this paper, we propose a Logic-Oriented perspective, modeling the financial market as a dynamic, evolutionary ecosystem of competing investment narratives, termed Modes of Thought. To operationalize this view, we introduce MEME (Modeling the Evolutionary Modes of Financial Markets), designed to reconstruct market dynamics through the lens of evolving logics. MEME employs a multi-agent extraction module to transform noisy data into high-fidelity Investment Arguments and utilizes Gaussian Mixture Modeling to uncover latent consensus within a semantic space. To model semantic drift among different market conditions, we also implement a temporal evaluation and alignment mechanism to track the lifecycle and historical profitability of these modes. By prioritizing enduring market wisdom over transient anomalies, MEME ensures that portfolio construction is guided by robust reasoning. Extensive experiments on three heterogeneous Chinese stock pools from 2023 to 2025 demonstrate that MEME consistently outperforms seven SOTA baselines. Further ablation studies, sensitivity analysis, lifecycle case study and cost analysis validate MEME's capacity to identify and adapt to the evolving consensus of financial markets. Our implementation can be found at https://github.com/gta0804/MEME.

</details>


### [208] [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964)
*Romain Froger,Pierre Andrews,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Gerard Moreno-Torres Bertran,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Vladislav Vorotilov,Mengjue Wang,Ian Yu,Amine Benhalloum,Grégoire Mialon,Thomas Scialom*

Main category: cs.AI

TL;DR: Gaia2 is a new benchmark for evaluating LLM agents in realistic, asynchronous, dynamic environments, featuring time-sensitive tasks, noisy events, and multi-agent collaboration, with action-level verification and open-source infrastructure.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prior static or synchronous benchmarks by evaluating LLM agents in realistic, asynchronous, evolving environments that demand temporal reasoning, adaptability, ambiguity resolution, and collaboration.

Method: Design and release of Gaia2—a benchmark built on the open-source Agents Research Environments (ARE) platform—featuring asynchronous scenarios with write-action verifiers for fine-grained, reward-based evaluation; evaluation of leading proprietary (GPT-5, Claude-4 Sonnet) and open-source (Kimi-K2) models using pass@1 metric.

Result: No model dominates across all capabilities: GPT-5 achieves highest overall pass@1 (42%) but fails on time-sensitive tasks; Claude-4 Sonnet balances accuracy/speed/cost; Kimi-K2 leads open-source models (21% pass@1); results reveal trade-offs among reasoning, efficiency, and robustness, and highlight the 'sim2real' gap.

Conclusion: Gaia2 provides a more realistic, extensible, and verifiable benchmark for agent evaluation and training, and its release—alongside the ARE framework—aims to advance practical, deployable LLM agent systems.

Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the "sim2real" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.

</details>


### [209] [CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation](https://arxiv.org/abs/2602.12004)
*Robert Cronshaw,Konstantinos Vilouras,Junyu Yan,Yuning Du,Feng Chen,Steven McDonagh,Sotirios A. Tsaftaris*

Main category: cs.AI

TL;DR: 本文提出了一种名为CSEval的临床语义评估框架，利用语言模型评估生成医学图像与其文本提示之间的临床语义一致性，弥补了现有评估方法在解剖位置和病理等临床语义层面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成的评估方法主要关注图像真实感或多样性，无法有效衡量生成图像是否准确反映临床语义（如解剖位置、病理特征），而这对医疗应用的安全性至关重要。

Method: 提出Clinical Semantics Evaluator（CSEval）框架，利用语言模型对生成图像与对应文本提示之间的临床语义对齐程度进行量化评估。

Result: 实验表明CSEval能识别其他指标忽略的语义不一致问题，且评估结果与临床专家判断具有相关性。

Conclusion: CSEval是一种可扩展、具临床意义的评估工具，可作为现有评估方法的有效补充，助力生成式模型在医疗场景中的安全落地。

Abstract: Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while failing to capture whether the generated images reflect the intended clinical semantics, such as anatomical location and pathology. In this study, we propose the Clinical Semantics Evaluator (CSEval), a framework that leverages language models to assess clinical semantic alignment between the generated images and their conditioning prompts. Our experiments show that CSEval identifies semantic inconsistencies overlooked by other metrics and correlates with expert judgment. CSEval provides a scalable and clinically meaningful complement to existing evaluation methods, supporting the safe adoption of generative models in healthcare.

</details>


### [210] [InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection](https://arxiv.org/abs/2602.12013)
*Xiuping Wu,Zhao Yu,Yuxin Cheng,Ngai Wong,Liangjun Ke,Tapas Mishra,Konstantinos V. Katsikopoulos*

Main category: cs.AI

TL;DR: 本文提出两种无需参数更新的推理优化方法InjectCorrect和InjectRLOpt，通过建模和注入大模型在不同问题上的自适应推理行为模式，显著提升其推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于行为提示调整的推理增强方法缺乏对模型内在推理行为模式的系统性分析。

Method: 提出两种无参优化方法：InjectCorrect（模仿模型自身正确回答的行为模式）与InjectRLOpt（基于历史行为数据学习价值函数，并通过可靠性感知Softmax策略生成推理注入信号）。

Result: 在多种推理任务上实现最高达5.34%（InjectCorrect）和8.67%（InjectRLOpt）的性能提升，且无需修改模型参数。

Conclusion: 模型的推理行为具有可建模、可注入的结构化模式，显式利用这些行为模式是提升大模型推理能力的有效新路径。

Abstract: Reasoning can significantly enhance the performance of Large Language Models. While recent studies have exploited behavior-related prompts adjustment to enhance reasoning, these designs remain largely intuitive and lack a systematic analysis of the underlying behavioral patterns. Motivated by this, we investigate how models' reasoning behaviors shape reasoning from the perspective of behavioral patterns. We observe that models exhibit adaptive distributions of reasoning behaviors when responding to specific types of questions, and that structurally injecting these patterns can substantially influence the quality of the models' reasoning processes and outcomes. Building on these findings, we propose two optimization methods that require no parameter updates: InjectCorrect and InjectRLOpt. InjectCorrect guides the model by imitating behavioral patterns derived from its own past correct answers. InjectRLOpt learns a value function from historical behavior-pattern data and, via our proposed Reliability-Aware Softmax Policy, generates behavioral injectant during inference to steer the reasoning process. Our experiments demonstrate that both methods can improve model performance across various reasoning tasks without requiring any modifications to model parameters, achieving gains of up to 5.34% and 8.67%, respectively.

</details>


### [211] [LawThinker: A Deep Research Legal Agent in Dynamic Environments](https://arxiv.org/abs/2602.12056)
*Xinyu Yang,Chenlong Deng,Tongyu Wen,Binyu Xie,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出LawThinker，一种采用Explore-Verify-Memorize策略的自主法律研究智能体，通过在每步知识探索后强制执行验证（由DeepVerifier模块实现），确保法律推理过程的准确性、事实-法律相关性与程序合规性，显著提升动态与静态法律推理基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理方法缺乏对中间推理步骤的验证机制，导致如引用不适用法条等错误在推理链中未被察觉地传播，影响推理过程的程序合规性。

Method: 提出LawThinker智能体，采用Explore-Verify-Memorize策略；核心是每步知识探索后执行原子化验证；引入DeepVerifier模块，从知识准确性、事实-法律相关性、程序合规性三方面评估检索结果；配备记忆模块支持长程任务中的跨轮知识复用。

Result: 在动态基准J1-EVAL上，相比直接推理提升24%，相比工作流方法提升11%，尤其在过程导向指标上提升显著；在三个静态基准上也验证了泛化能力。

Conclusion: 强制中间验证是提升法律推理过程可靠性与合规性的关键，LawThinker为构建可信赖、可审计的法律AI系统提供了新范式。

Abstract: Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .

</details>


### [212] [Tiny Recursive Reasoning with Mamba-2 Attention Hybrid](https://arxiv.org/abs/2602.12078)
*Wenlong Wang,Fergal Reid*

Main category: cs.AI

TL;DR: 本文探讨了将Mamba-2状态空间模型（SSM）引入TRM递归推理框架的可行性，发现其在参数量相近下，在ARC-AGI-1任务上提升了pass@2和pass@100指标，表明SSM类算子可有效支持隐式递归推理。


<details>
  <summary>Details</summary>
Motivation: TRM等模型已证明小规模网络可通过隐式递归（隐藏层迭代优化）实现强抽象推理能力；而Mamba-2本身具备状态空间递归结构，自然适合作为递归算子，但其嵌入TRM是否仍能保持推理能力尚不明确。

Method: 将TRM中的Transformer模块替换为Mamba-2混合算子，在参数量基本一致（6.83M vs 6.86M）条件下，在ARC-AGI-1基准上评估其pass@K性能。

Result: 在ARC-AGI-1上，Mamba-2混合模型pass@2提升2.0%（45.88% vs 43.88%），pass@100提升4.75%，pass@1持平，表明候选解覆盖能力增强、首选准确性不变。

Conclusion: Mamba-2混合算子能在递归推理框架中保持并增强推理能力，验证了SSM类算子是递归算子设计的有效候选，为探索最优递归算子混合策略迈出第一步。

Abstract: Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion -- iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural question about operator choice: Mamba-2's state space recurrence is itself a form of iterative refinement, making it a natural candidate for recursive reasoning -- but does introducing Mamba-2 into the recursive scaffold preserve reasoning capability? We investigate this by replacing the Transformer blocks in TRM with Mamba-2 hybrid operators while maintaining parameter parity (6.83M vs 6.86M parameters). On ARC-AGI-1, we find that the hybrid improves pass@2 (the official metric) by +2.0\% (45.88\% vs 43.88\%) and consistently outperforms at higher K values (+4.75\% at pass@100), whilst maintaining pass@1 parity. This suggests improved candidate coverage -- the model generates correct solutions more reliably -- with similar top-1 selection. Our results validate that Mamba-2 hybrid operators preserve reasoning capability within the recursive scaffold, establishing SSM-based operators as viable candidates in the recursive operator design space and taking a first step towards understanding the best mixing strategies for recursive reasoning.

</details>


### [213] [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083)
*Antonin Sulc*

Main category: cs.AI

TL;DR: 本文提出可微模态逻辑（DML）与模态逻辑神经网络（MLNNs），通过神经符号方法从行为数据中自动学习多智能体系统中的信任网络、因果链和规范边界，支持可解释的调试框架。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统日益复杂，语义故障调试需建模知识、信念、因果与义务等模态概念，但传统模态逻辑依赖人工设定动态未知的关系结构，难以适用。

Method: 提出可微模态逻辑（DML）及其实现——模态逻辑神经网络（MLNNs），构建统一的神经符号调试框架，涵盖认知（epistemic）、时序（temporal）、道义（deontic）和信念（doxastic）四种模态，并将逻辑矛盾转化为可学习的优化目标。

Result: 在外交游戏欺骗联盟发现、大语言模型幻觉检测等真实场景中验证有效；实现可解释的显式参数化结构（如信任、因果）、基于可微公理的知识注入、多模态组合推理，以及面向监控、主动控制与通信的部署模式。

Conclusion: DML/MLNNs弥合了形式逻辑的可解释性与深度学习的数据驱动能力，为多智能体系统提供了一种实用、可扩展且可部署的神经符号调试范式。

Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.
  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.

</details>


### [214] [The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context](https://arxiv.org/abs/2602.12108)
*Xiaoyuan Liu,Tian Liang,Dongyang Ma,Deyu Zhou,Haitao Mi,Pinjia He,Yan Wang*

Main category: cs.AI

TL;DR: 本文提出StateLM，一种具备内部推理循环以自主管理状态的新一代基础模型，通过赋予模型记忆工具（如上下文剪枝、文档索引、笔记）并训练其主动使用，显著提升长文档问答、对话记忆和深度研究等任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏自主管理记忆与上下文的能力，只能被动接受固定长度的输入，限制了其在长程依赖和复杂推理任务中的表现。

Method: 提出StateLM架构，引入内部推理循环和可学习的记忆操作工具（如上下文剪枝、文档索引、笔记），通过强化或监督方式训练模型动态构建和维护自身上下文状态。

Result: StateLM在长文档问答、对话记忆和BrowseComp-Plus深度研究任务上大幅超越标准LLM：例如BrowseComp-Plus准确率从5%提升至52%，对话记忆绝对准确率提升10%-20%。

Conclusion: StateLM使大语言模型从被动预测器转变为具有状态意识的主动推理代理，推理过程变为可管理、有状态的流程，突破了固定上下文窗口的架构限制。

Abstract: In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the "wand" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.

</details>


### [215] [Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty](https://arxiv.org/abs/2602.12113)
*Zewei Yu,Lirong Gao,Yuke Zhu,Bo Zheng,Sheng Guo,Haobo Wang,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文提出ARLCP框架，通过自适应反射惩罚和长度协调惩罚，在保持甚至提升准确率的同时显著缩短大型推理模型的推理链长度，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中虽表现优异，但常生成过长、冗余的思维链（如重复自问、循环推理），导致高token消耗、计算开销大、延迟高，且未提升准确率，尤其在小模型上问题更突出；问题越复杂，冗余反思越多，反而降低准确率并增加token开销。

Method: 提出基于强化学习的Adaptive Reflection and Length Coordinated Penalty（ARLCP）框架，包含两个核心机制：（1）自适应反射惩罚，动态削减非必要反思步骤，保留关键推理；（2）基于问题复杂度估计的长度惩罚，协同调控推理长度。

Result: 在5个数学推理基准上，使用DeepSeek-R1-Distill-Qwen-1.5B和7B模型验证：1.5B模型平均响应长度减少53.1%，准确率提升5.8%；7B模型长度减少35.0%，准确率提升2.7%。

Conclusion: ARLCP能有效平衡推理效率与准确性，为测试时缩放下的高效推理提供了新范式。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .

</details>


### [216] [Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models](https://arxiv.org/abs/2602.12120)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Main category: cs.AI

TL;DR: 本文提出了一种在数据稀疏背景下，利用时序基础模型（TSFMs）结合无泄漏协变量（如IOCI指数和Google Trends）进行高校入学人数零样本预测的新方法，并通过严格回测验证其与经典方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 高校面临日益增长的财政压力，需准确预测新生入学人数；但传统时间序列方法在短样本、报告变更和制度突变下表现不稳定。

Method: 采用零样本设定下的多种时序基础模型（TSFMs），构建无数据泄露的紧凑协变量集，包括基于文档证据的可迁移制度运行条件指数（IOCI）和经稳定化特征工程处理的Google Trends需求代理指标，并使用严格对齐时间版本的滚动扩展窗口回测。

Result: 协变量驱动的TSFMs在无需机构特异性训练的情况下，性能与经典基准方法相当，不同学生群体和模型间性能差异存在。

Conclusion: 在数据稀疏的高等教育预测场景中，合理设计的协变量（尤其是IOCI）可显著提升TSFMs的零样本泛化能力，为机构提供可靠、可迁移的预测工具。

Abstract: Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.

</details>


### [217] [HLA: Hadamard Linear Attention](https://arxiv.org/abs/2602.12128)
*Hanno Ackermann,Hong Cai,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.AI

TL;DR: 本文提出了一种新的线性注意力机制——Hadamard线性注意力（HLA），其非线性操作在计算token间相似度之后进行，从而更精确地逼近softmax；该方法计算高效、无需复杂张量重排，并在大规模视频生成扩散Transformer中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制计算复杂度为二次，线性注意力虽提升效率，但其独立作用于查询与键的核函数仅能低阶逼近softmax，精度受限；需一种更高精度且保持高效性的线性注意力替代方案。

Method: 提出Hadamard线性注意力（HLA）：在计算查询与键的成对相似度后，再应用Hadamard积形式的非线性变换，使逼近softmax的函数升至更高阶有理函数；推导出类似标准线性注意力的高效计算流程，避免耗时的张量重塑。

Result: HLA在保持线性复杂度的同时，显著提升对softmax的逼近精度；在大型视频生成扩散Transformer模型上验证有效，尤其适用于token数量极大的场景。

Conclusion: HLA通过调整非线性引入时机，在不牺牲计算效率的前提下提升了线性注意力的建模能力，为高吞吐、长序列任务提供了更优注意力近似方案。

Abstract: The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.
  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.

</details>


### [218] [Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5](https://arxiv.org/abs/2602.12133)
*Roberto Balestri*

Main category: cs.AI

TL;DR: 本研究通过大规模对比审计，发现Gemini Flash 2.5和GPT Image 1.5在中性提示下仍存在显著的性别与肤色偏差：两者均强烈偏向白人（>96%），但Gemini偏向女性形象，GPT偏向浅肤色男性；研究提出光照感知色度分析方法，揭示‘中性提示’实为诊断探针而非无偏指令。


<details>
  <summary>Details</summary>
Motivation: 检验‘中性提示应产生人口统计学中性输出’这一假设是否成立，并揭示商业图像生成模型中潜在的性别与肤色偏见。

Method: 使用4个语义中性提示生成3200张写实图像；采用融合色彩归一化、面部关键点遮罩及基于Monk、PERLA和Fitzpatrick量表的感知均匀肤色量化方法进行偏差分析。

Result: 两模型均呈现强烈‘默认白人’偏差（>96%），但在性别上分化明显：Gemini偏向女性呈现，GPT偏向浅肤色男性呈现；中性提示实际暴露出模型内在偏差，而非实现中立输出。

Conclusion: 中性提示并非无偏指令，而是可揭示模型内在社会偏见的诊断探针；研究提供了可复现的算法视觉文化审计框架，挑战了‘无标记语言即具包容性’的社会语言学假设。

Abstract: This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong "default white" bias (>96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.

</details>


### [219] [Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment](https://arxiv.org/abs/2602.12134)
*Jiajun Chen,Hua Shen*

Main category: cs.AI

TL;DR: 本文提出价值对齐税（VAT）框架，用于量化对齐干预（如提示、微调）在改变目标价值的同时，如何引发其他关联价值的系统性波动；实验表明对齐效果常导致价值间的非均衡共变，传统仅评估目标价值的方法会忽略此类系统性风险。


<details>
  <summary>Details</summary>
Motivation: 现有价值对齐研究多静态建模价值关系，忽视干预措施对整体价值系统的动态影响，亟需刻画对齐过程中的价值传播效应。

Method: 提出Value Alignment Tax（VAT）框架，基于Schwartz价值理论构建场景-动作数据集，采集模型干预前后的规范性判断配对数据，分析不同模型、价值维度与对齐策略下的对齐传播效应。

Result: 对齐干预常引发价值间不均衡但结构化的共变；该现象在仅评估目标价值的传统方法下不可见，揭示了被忽视的系统性对齐风险。

Conclusion: 价值对齐不仅是目标价值的提升，更涉及整个价值系统的动态重构；VAT为评估和理解大语言模型中价值对齐的深层过程提供了新范式。

Abstract: Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.

</details>


### [220] [STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction](https://arxiv.org/abs/2602.12143)
*Xiaoxiao Wang,Chunxiao Li,Junying Wang,Yijin Guo,Zijian Chen,Chunyi Li,Xiaohong Liu,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: STAR框架结合统计期望与智能体推理，通过约束概率矩阵分解和期望违背理论提升大模型性能预测的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法难以应对模式漂移、数据稀疏性和缺乏解释性，而纯大语言模型方法又不可靠，因此需要一种更鲁棒、可解释的预测方法。

Method: STAR框架包含两部分：一是基于外部知识检索和语义嵌入的约束概率矩阵分解（CPMF）生成带不确定性估计的统计期望；二是基于期望违背理论（EVT）的推理模块，通过族内分析、跨模型比较和可信度感知聚合进行预测修正并提供可追溯解释。

Result: STAR在分数型和排序型指标上均显著优于所有基线方法，在极端稀疏（每个测试模型仅1–2个观测分数）条件下，总分比最强统计方法提升14.46%。

Conclusion: STAR有效融合数据驱动与知识驱动范式，在低资源场景下实现了高精度、高鲁棒性且可解释的大模型性能预测。

Abstract: As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.

</details>


### [221] [Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning](https://arxiv.org/abs/2602.12146)
*Mahdi Khodabandeh,Ghazal Shabani,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习与T5语言模型的新型无损压缩方法，将数据压缩为token序列而非传统向量表示，从而在保持语义完整性的同时提升压缩率。


<details>
  <summary>Details</summary>
Motivation: 传统字典式和统计压缩方法难以充分利用复杂数据中的结构与冗余；现有深度学习方法多依赖掩盖token结构的稠密向量表示。

Method: 将强化学习（离策略算法）应用于T5语言模型架构，使其直接学习生成最短有效token序列以实现无损压缩，不依赖外部语法或世界知识。

Result: 相比传统方法，在压缩率上取得显著提升；保留原始token结构，兼顾高效率与语义完整性；无需显式理解内容即可利用语言模型内在表征进行压缩。

Conclusion: 该方法构建了一个高效、自适应、端到端的无损压缩系统，拓展了大语言模型在数据压缩领域的实用边界。

Abstract: Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.

</details>


### [222] [GPT-4o Lacks Core Features of Theory of Mind](https://arxiv.org/abs/2602.12150)
*John Muchovej,Amanda Royka,Shane Lee,Julian Jara-Ettinger*

Main category: cs.AI

TL;DR: 本文质疑大语言模型（LLMs）是否真正具备心理理论（ToM），提出并验证了一种基于认知科学定义的新评估框架，发现LLMs虽能在简单任务中模仿人类判断，但在逻辑等价任务及心智状态与行为预测一致性上表现不佳，表明其社交能力并非源于通用、一致的心理理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅通过基准测试评估LLM的ToM能力，但未检验其是否真正建模了心智状态与行为之间的因果关系，因此需基于认知科学定义构建更严格的评估框架。

Method: 采用认知科学中对ToM的定义，设计新评估框架，检验LLM是否具备连贯、领域通用且一致的心智状态-行为因果模型，并对比其在等价任务中的表现及预测一致性。

Result: LLMs在简单ToM任务中可近似人类判断，但在逻辑等价任务中失败，且其行为预测与对应心智状态推断之间一致性低。

Conclusion: LLMs展现的社交能力并非源于领域通用或一致的心理理论，其ToM能力缺乏因果建模基础。

Abstract: Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.

</details>


### [223] [Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision](https://arxiv.org/abs/2602.12164)
*Xiaohan He,Shiyang Feng,Songtao Huang,Lei Bai,Bin Wang,Bo Zhang*

Main category: cs.AI

TL;DR: 本文提出Sci-CoE，一种两阶段科学协同演化框架，使大语言模型能作为求解器和验证器自我演化，通过从稀疏监督到无监督学习的过渡，提升科学推理任务中的鲁棒性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在科学推理任务中因解法评估不可靠及验证策略多样性有限而表现脆弱。

Method: 提出两阶段框架：第一阶段利用少量标注数据建立验证器的基本正确性判断锚点；第二阶段引入几何奖励机制，综合考虑共识性、可靠性与多样性，在无标签数据上进行大规模自迭代。

Result: 在多个通用科学基准测试中，Sci-CoE显著提升了复杂推理能力，并展现出强可扩展性，有助于构建更鲁棒、更多样化的评估系统。

Conclusion: Sci-CoE为科学推理任务提供了更可靠、自适应的协同演化范式，推动了大语言模型在该领域的进一步发展。

Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.

</details>


### [224] [Statistical Parsing for Logical Information Retrieval](https://arxiv.org/abs/2602.12170)
*Greg Coppola*

Main category: cs.AI

TL;DR: 本文提出了一种扩展的量化布尔贝叶斯网络（QBBN），通过引入NEG因子支持否定与反向推理，并结合类型化逻辑语言与槽位语法，实现从自然语言到逻辑形式的确定性解析与概率推理，融合LLM与形式模型以兼顾泛化性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决前序工作中QBBN缺乏否定/反向推理能力及自然语言解析器的两个关键缺陷，弥合形式语义学与现代大语言模型之间的鸿沟。

Method: 1）在QBBN中新增NEG因子以满足P(x)+P(¬x)=1，支持逆向lambda消息传递与modus tollens；2）设计三层表达力的类型化逻辑语言（含角色标注谓词、模态量词、lambda抽象）；3）构建确定性类型化槽位语法进行句法-逻辑映射；4）采用‘LLM预处理→语法解析→LLM重排序→QBBN推理’混合架构。

Result: 推理：在44个测试用例（22种推理模式）上达100%准确率；语义：建立具三层表达力的类型化逻辑语言；语法：槽位语法实现33/33全正确、零歧义解析；LLM在PP附着任务中达95%准确率，但依存分析UAS仅12.4%，验证语法必要性。

Conclusion: QBBN与LLM协同可兼顾形式严谨性与数据驱动灵活性，LLM作为‘标注器’缓解形式NLP的标注瓶颈，QBBN作为‘验证器’保障推理可靠性，从而调和Sutton‘苦涩教训’与蒙塔古语法传统。

Abstract: In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.
  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz's simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.
  We argue this reconciles formal semantics with Sutton's "bitter lesson" (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world

</details>


### [225] [Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation](https://arxiv.org/abs/2602.12172)
*Bowei He,Yankai Chen,Xiaokun Zhang,Linghe Kong,Philip S. Yu,Xue Liu,Chen Ma*

Main category: cs.AI

TL;DR: 本文提出了一种受教育学启发的LLM知识蒸馏框架IOA，包含知识识别、组织与适配三阶段，融合布鲁姆掌握学习和维果茨基最近发展区理论，显著提升小模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于合成数据的知识蒸馏方法缺乏教学意识，将知识传递视为一次性任务，而非系统性学习过程。

Method: 提出IOA三阶段框架：Knowledge Identifier识别学生模型知识缺陷；Organizer依据教育学原理构建渐进式课程组织知识交付；Adapter根据学生模型认知能力自适应调整表征；融合Bloom掌握学习与Vygotsky最近发展区理论实现动态难度调控。

Result: 在DollyEval上学生模型保留教师94.7%性能（参数<1/10）；MATH提升19.2%，HumanEval提升22.3%。

Conclusion: 教育学原理可有效指导LLM知识蒸馏，IOA框架实现了更高效、更具认知适配性的知识迁移，尤其利于复杂推理能力提升。

Abstract: Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.

</details>


### [226] [SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation](https://arxiv.org/abs/2602.12173)
*Chengxi Zeng,Yuxuan Jiang,Ge Gao,Shuai Wang,Duolikun Danier,Bin Zhu,Stevan Rudinac,David Bull,Fan Zhang*

Main category: cs.AI

TL;DR: 本文提出SAM3-LiteText，通过知识蒸馏将大型文本编码器替换为轻量级MobileCLIP学生模型，在大幅降低参数量（最高88%）和内存占用的同时，保持与原SAM3相当的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言分割模型（如SAM3）使用为通用语言理解设计的大规模文本编码器，但实际分割提示短小、结构化且语义受限，导致文本编码器能力严重冗余、计算与内存开销大。

Method: 对404,796个真实提示进行大规模解剖分析，发现文本编码存在上下文窗口利用率低、词表稀疏、嵌入位于低维流形等冗余现象；据此提出SAM3-LiteText，用知识蒸馏优化的轻量MobileCLIP替代原文本编码器。

Result: 在图像与视频分割基准上，SAM3-LiteText将文本编码器参数减少最多88%，显著降低静态内存占用，同时分割性能与原始SAM3基本持平。

Conclusion: 针对视觉-语言分割任务中提示文本的特殊性，精简文本编码器是可行且高效的优化路径，SAM3-LiteText为高效多模态分割提供了实用新范式。

Abstract: Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

</details>


### [227] ["Sorry, I Didn't Catch That": How Speech Models Miss What Matters Most](https://arxiv.org/abs/2602.12249)
*Kaitlyn Zhou,Martijn Bartelds,Federico Bianchi,James Zou*

Main category: cs.AI

TL;DR: 本文研究了语音识别系统在真实世界高风险场景（如美国街道名称转录）中的失败问题，发现现有模型平均错误率达44%，对非英语母语者影响更严重；作者提出一种基于开源TTS模型生成多样化发音的合成数据方法，仅用不到1000条样本微调即可使非英语母语者准确率提升近60%。


<details>
  <summary>Details</summary>
Motivation: 尽管语音识别在标准基准上表现良好，但在现实高风险短语音任务（如街道名转录）中频繁失败，尤其对语言多样性用户存在系统性偏差。

Method: 评估15个主流ASR模型在语言多样性美国说话人录音上的街道名转录性能；量化地理相关下游错误；提出基于开源TTS的合成数据生成方法，用于增强命名实体发音多样性，并进行少量样本微调。

Result: 15个模型平均转录错误率为44%；非英语母语者路由距离误差是英语母语者的两倍；使用<1000条合成样本微调后，非英语母语者准确率相对提升近60%。

Conclusion: 当前语音识别系统在基准与真实高风险场景间存在关键可靠性差距；基于合成发音数据的轻量微调是缓解此类偏差、提升公平性与鲁棒性的可行路径。

Abstract: Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.

</details>


### [228] [Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259)
*Jianke Yang,Ohm Venkatachalam,Mohammad Kianezhad,Sharvaree Vadgama,Rose Yu*

Main category: cs.AI

TL;DR: KeplerAgent 是一个基于代理的框架，模拟科学家多步推理过程（如先推断对称性等物理性质，再约束符号回归空间），结合物理工具与符号回归引擎（如 PySINDy、PySR），在物理方程发现任务中显著提升准确率与抗噪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法直接从数据猜测方程，忽视科学家依赖物理先验（如对称性）逐步推理的过程，导致泛化性与鲁棒性不足。

Method: 提出 KeplerAgent 框架，通过协调物理启发的工具提取中间结构（如对称性），并据此动态配置符号回归引擎的函数库与结构约束。

Result: 在多个物理方程基准测试中，KeplerAgent 的符号准确率和噪声鲁棒性均显著优于纯LLM方法及传统符号回归基线。

Conclusion: 将科学推理流程显式建模为多步代理协作，能有效提升符号方程发现的可解释性、准确性与鲁棒性，为AI for Science提供新范式。

Abstract: Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.

</details>


### [229] [CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268)
*Zhen Zhang,Kaiqiang Song,Xun Wang,Yebowen Hu,Weixiang Yan,Chenyang Zhao,Henry Peng Zou,Haoyun Deng,Sathish Reddy Indurthi,Shujian Liu,Simin Ma,Xiaoyang Wang,Xin Eric Wang,Song Wang*

Main category: cs.AI

TL;DR: CM2是一种无需可验证奖励的强化学习框架，通过清单式二元评判标准和LLM模拟工具环境，提升多轮多步工具调用AI代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在真实场景中面临缺乏可验证奖励、多轮多步工具使用研究不足、以及构建执行环境成本高等挑战。

Method: 提出CM2框架：用基于明确证据和结构化元数据的细粒度二元清单奖励替代传统可验证结果奖励；采用稀疏奖励分配但密集评估标准策略；在可扩展的LLM模拟工具环境中训练。

Result: 在tau^-Bench、BFCL-V4和ToolSandbox上分别比监督微调提升8、10、12分，性能匹配或超越同规模开源基线。

Conclusion: CM2为优化多轮多步工具使用AI代理提供了一种可扩展、低成本且不依赖可验证奖励的新范式。

Abstract: AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.

</details>


### [230] [Agentic Test-Time Scaling for WebAgents](https://arxiv.org/abs/2602.12276)
*Nicholas Lee,Lutfi Eren Erdogan,Chris Joseph John,Surya Krishnapillai,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.AI

TL;DR: 本文提出了一种名为CATTS的动态计算分配方法，用于提升多步智能体（如网页代理）在测试时的性能与可靠性。通过分析投票分布的不确定性（如熵、top-1/top-2 margin），CATTS仅在决策存在争议时增加计算资源，在WebArena-Lite和GoBrowse上相比React提升最高达9.1%，同时比均匀扩展节省最多2.3倍token。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放（test-time scaling）方法在多步智能体任务中效果受限：小错误易累积，且均匀增加每步计算收益递减；亟需一种更高效、可解释的动态计算分配机制。

Method: 首先对网页代理进行实证研究，发现均匀扩展快速饱和；接着探索更强聚合策略（如LLM仲裁器），但其可能误否高共识决策；进而发现代理自身投票分布的不确定性指标（熵、top-1/top-2 margin）与下游成功率强相关；最终提出CATTS，依据该不确定性动态分配计算。

Result: CATTS在WebArena-Lite和GoBrowse上相较React提升最高9.1%，同时token消耗比均匀扩展减少最多2.3倍；不确定性指标被验证为有效预测信号；方法具备可解释性。

Conclusion: 基于代理自身投票不确定性的动态测试时缩放（CATTS）是一种高效、鲁棒且可解释的多步智能体优化策略，优于均匀扩展与LLM仲裁等基线方法。

Abstract: Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [231] [Counterfactual Conditional Likelihood Rewards for Multiagent Exploration](https://arxiv.org/abs/2602.11740)
*Ayhan Alp Aydeniz,Robert Loftin,Kagan Tumer*

Main category: cs.MA

TL;DR: 本文提出了一种名为反事实条件似然（CCL）奖励的新方法，用于多智能体系统中更高效的协同探索，通过衡量每个智能体对团队整体探索的独特贡献，避免个体探索冗余，显著提升稀疏奖励和强协调需求任务的学习效率。


<details>
  <summary>Details</summary>
Motivation: 个体层面的探索易导致冗余，缺乏对队友探索状态的感知，难以实现高效协同探索。

Method: 提出反事实条件似然（CCL）奖励机制，基于团队联合探索状态评估单个智能体观测的信息价值，而非仅依赖其个体观测的新颖性。

Result: 在连续型多智能体环境中实验表明，CCL奖励能加速稀疏团队奖励场景下的学习，并在需紧密协作的任务中表现尤为突出。

Conclusion: CCL是一种有效提升多智能体协同探索效率的新奖励设计，兼顾个体贡献与团队一致性，适用于开放域复杂任务。

Abstract: Efficient exploration is critical for multiagent systems to discover coordinated strategies, particularly in open-ended domains such as search and rescue or planetary surveying. However, when exploration is encouraged only at the individual agent level, it often leads to redundancy, as agents act without awareness of how their teammates are exploring. In this work, we introduce Counterfactual Conditional Likelihood (CCL) rewards, which score each agent's exploration by isolating its unique contribution to team exploration. Unlike prior methods that reward agents solely for the novelty of their individual observations, CCL emphasizes observations that are informative with respect to the joint exploration of the team. Experiments in continuous multiagent domains show that CCL rewards accelerate learning for domains with sparse team rewards, where most joint actions yield zero rewards, and are particularly effective in tasks that require tight coordination among agents.

</details>


### [232] [Cooperation Breakdown in LLM Agents Under Communication Delays](https://arxiv.org/abs/2602.11754)
*Keita Nishimoto,Kimitaka Asatani,Ichiro Sakata*

Main category: cs.MA

TL;DR: 本文提出FLCOA框架，强调计算与通信资源等底层因素对多智能体系统协作的影响，并通过引入带通信延迟的连续囚徒困境实验，发现通信延迟与相互合作呈U型关系。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-MAS研究忽视了计算和通信资源等底层约束对协作与协调的影响，而这些约束在现实部署中至关重要。

Method: 提出五层协作/协调框架（FLCOA），并设计带通信延迟的连续囚徒困境博弈，利用LLM智能体进行仿真实验。

Result: 通信延迟增加初期会诱发策略性剥削行为；延迟过大时剥削循环减少，导致延迟与相互合作率呈U型关系。

Conclusion: 促进多智能体协作不仅需关注高层制度设计，也必须重视通信延迟、资源分配等底层因素，为MAS研究提供新方向。

Abstract: LLM-based multi-agent systems (LLM-MAS), in which autonomous AI agents cooperate to solve tasks, are gaining increasing attention. For such systems to be deployed in society, agents must be able to establish cooperation and coordination under real-world computational and communication constraints. We propose the FLCOA framework (Five Layers for Cooperation/Coordination among Autonomous Agents) to conceptualize how cooperation and coordination emerge in groups of autonomous agents, and highlight that the influence of lower-layer factors - especially computational and communication resources - has been largely overlooked. To examine the effect of communication delay, we introduce a Continuous Prisoner's Dilemma with Communication Delay and conduct simulations with LLM-based agents. As delay increases, agents begin to exploit slower responses even without explicit instructions. Interestingly, excessive delay reduces cycles of exploitation, yielding a U-shaped relationship between delay magnitude and mutual cooperation. These results suggest that fostering cooperation requires attention not only to high-level institutional design but also to lower-layer factors such as communication delay and resource allocation, pointing to new directions for MAS research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [233] [Automated Optimization Modeling via a Localizable Error-Driven Perspective](https://arxiv.org/abs/2602.11164)
*Weiting Liu,Han Wu,Yufei Kuang,Xiongwei Han,Tao Zhong,Jianfeng Feng,Wenlian Lu*

Main category: cs.LG

TL;DR: 本文提出了一种面向自动优化建模的错误驱动学习框架MIND，通过识别和利用建模错误的局部化传播特性，构建高密度训练数据并设计动态监督微调策略（DFPO），显著提升了大语言模型在该领域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动优化建模方法受限于高质量训练数据稀缺与利用不足，且存在两类根本局限：(L1) 错误特异性问题稀疏；(L2) 难题对应的奖励稀疏，导致领域后训练效果不佳。

Method: 提出MIND框架，核心是利用优化建模中错误传播具有局部化特征的观察，据此开展数据合成与后训练定制：构建聚焦、高密度的训练语料，并设计动态监督微调策略优化（DFPO）以实现局部精细化修正。

Result: 在六个基准测试上，MIND持续优于所有当前最优的自动优化建模方法。

Conclusion: 错误的局部化特性可被有效建模与利用，MIND框架为提升LLMs在结构化推理任务（如优化建模）中的后训练效能提供了新范式。

Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\textbf{m}ated opt\textbf{i}mization modeli\textbf{n}g via a localizable error-\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \textbf{D}ynamic Supervised \textbf{F}ine-Tuning \textbf{P}olicy \textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.

</details>


### [234] [KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models](https://arxiv.org/abs/2602.11184)
*Zukang Xu,Zhixiong Zhao,Xing Hu,Zhixuan Chen,Dawei Yang*

Main category: cs.LG

TL;DR: 本文提出KBVQ-MoE框架，通过输入驱动的冗余消除和偏差校正的输出稳定化技术，显著提升MoE大模型在极低比特（如3-bit）向量量化下的性能保持能力，适用于边缘设备等资源受限场景。


<details>
  <summary>Details</summary>
Motivation: MoE模型参数量大、内存需求高，难以部署于资源受限环境；而直接对MoE应用向量量化（VQ）会导致因专家间表征冗余和累积输出偏差引发的严重性能下降。

Method: 提出KBVQ-MoE：1）输入驱动的冗余消除——使用KLT引导的SVD提取并跨专家共享主导权重分量；2）偏差校正的输出稳定化——仅对专家特有（非冗余）部分做VQ，并通过通道级仿射补偿校正量化输出偏差。

Result: 在多个MoE大模型（如Qwen1.5-MoE-A2.7B）上验证，3-bit量化下平均准确率达67.99，几乎与FP16基线（68.07）持平，显著优于现有量化方法。

Conclusion: KBVQ-MoE有效缓解了MoE中VQ的冗余浪费与偏差累积问题，为超低比特量化MoE模型提供了高效、高保真的新范式，推动其在边缘设备上的实用化部署。

Abstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.

</details>


### [235] [Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy](https://arxiv.org/abs/2602.11185)
*Zhendong Huang,Hengjie Cao,Fang Dong,Ruijun Huang,Mengyi Chen,Yifeng Yang,Xin Zhang,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Li Shang*

Main category: cs.LG

TL;DR: 本文发现LLM训练中梯度信号具有高度各向异性，提出Spectra优化器抑制主导的低秩‘尖峰’子空间，提升尾部学习效率，在训练速度、内存占用和下游任务准确率上均优于AdamW和Muon。


<details>
  <summary>Details</summary>
Motivation: 梯度信号在LLM训练中呈现尖峰-长尾谱结构，主导尖峰（仅约1.5%方向）压制尾部学习，导致收敛慢、学习率受限。

Method: 提出Spectra优化器：通过缓存warm-started幂迭代追踪尖峰子空间，并实施低秩谱整形；不放大噪声敏感的尾部，且开销极小、状态内存显著降低。

Result: 在LLaMA3-8B上训练50B token时，相比AdamW：收敛快30%、单步开销降0.7%、优化器状态内存减49.25%、下游平均准确率升1.62%；相比Muon：优化器处理速度快5.1倍、终损更低、平均准确率高0.66%。

Conclusion: 梯度谱的尖峰-长尾分离是影响LLM优化的关键因素，Spectra通过谱感知设计有效缓解该问题，为大模型高效训练提供新范式。

Abstract: Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.

</details>


### [236] [GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices](https://arxiv.org/abs/2602.11186)
*Zhihan Zeng,Kaihe Wang,Zhongpei Zhang,Yue Xiu*

Main category: cs.LG

TL;DR: 本文提出GAC-KAN框架，通过物理引导仿真生成高保真干扰数据，并设计轻量高效MS-GAC主干网络与Kolmogorov-Arnold网络（KAN）分类头，在极低参数量（0.13M）下实现98.0%的GNSS干扰识别精度，适配GenAI边缘设备的安全需求。


<details>
  <summary>Details</summary>
Motivation: GenAI在消费电子中广泛应用带来巨大计算压力，挤压了GNSS信号保护等关键安全任务的资源；同时真实干扰数据稀缺，制约鲁棒分类器训练。

Method: 提出GAC-KAN框架：1）采用物理引导仿真合成大规模高保真干扰数据集；2）构建Multi-Scale Ghost-ACB-Coordinate（MS-GAC）主干网络，融合非对称卷积块（ACB）与Ghost模块以高效提取时频特征；3）用可学习样条激活的Kolmogorov-Arnold Network（KAN）替代传统MLP作为决策头。

Result: GAC-KAN在干扰识别任务中达到98.0%整体精度，参数量仅0.13百万，比ViT基线减少约660倍，具备'常开'式嵌入式安全能力。

Conclusion: GAC-KAN成功平衡了高精度与极端轻量化，为GenAI原生边缘芯片提供了可行、高效的GNSS安全保障方案，解决了数据稀缺与算力受限的双重挑战。

Abstract: The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge hardware, leaving strictly limited resources for fundamental security tasks like Global Navigation Satellite System (GNSS) signal protection. Furthermore, training robust classifiers for such devices is hindered by the scarcity of real-world interference data. To address the dual challenges of data scarcity and the extreme efficiency required by the GenAI era, this paper proposes a novel framework named GAC-KAN. First, we adopt a physics-guided simulation approach to synthesize a large-scale, high-fidelity jamming dataset, mitigating the data bottleneck. Second, to reconcile high accuracy with the stringent resource constraints of GenAI-native chips, we design a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone. This backbone combines Asymmetric Convolution Blocks (ACB) and Ghost modules to extract rich spectral-temporal features with minimal redundancy. Replacing the traditional Multi-Layer Perceptron (MLP) decision head, we introduce a Kolmogorov-Arnold Network (KAN), which employs learnable spline activation functions to achieve superior non-linear mapping capabilities with significantly fewer parameters. Experimental results demonstrate that GAC-KAN achieves an overall accuracy of 98.0\%, outperforming state-of-the-art baselines. Significantly, the model contains only 0.13 million parameter--approximately 660 times fewer than Vision Transformer (ViT) baselines. This extreme lightweight characteristic makes GAC-KAN an ideal "always-on" security companion, ensuring GNSS reliability without contending for the computational resources required by primary GenAI tasks.

</details>


### [237] [TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.11187)
*Yubo Hou,Furen Zhuang,Partha Pratim Kundu,Sezin Ata Kircali,Jie Wang,Mihai Dragos Rotaru,Dutta Rahul,Ashish James*

Main category: cs.LG

TL;DR: 本文提出了一种名为TDPNavigator-Placer的多智能体强化学习框架，用于2.5D集成电路中芯片小片（chiplet）的自动布局，旨在同时优化布线长度和热管理这两个冲突目标。


<details>
  <summary>Details</summary>
Motivation: 现有布局方法通常仅关注最小化线长或通过加权和将多目标优化转为单目标，难以应对布线长度优化与热管理之间的固有冲突，限制了其在大规模异构芯片小片系统中的实用部署。

Method: 提出TDPNavigator-Placer，一种基于芯片小片热设计功耗（TDP）动态优化布局的多智能体强化学习框架；各智能体被分配不同但互补的冲突目标（如线长、温度），并在各自奖励机制与环境约束下协同工作。

Result: 实验表明，该方法相较当前最优方法显著改善了帕累托前沿，在布线长度与热性能之间实现更均衡的权衡。

Conclusion: TDPNavigator-Placer通过多智能体分工与协同，有效解决了2.5D IC芯片小片布局中多目标冲突难题，提升了实际部署可行性。

Abstract: The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming multi-objective optimization into a single objective through weighted sum, which limits their ability to handle competing design requirements. Wirelength reduction and thermal management are inherently conflicting objectives, making prior approaches inadequate for practical deployment. To address this challenge, we propose TDPNavigator-Placer, a novel multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). This approach explicitly assigns these inherently conflicting objectives to specialized agents, each operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm. Experimental results demonstrate that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.

</details>


### [238] [Time-TK: A Multi-Offset Temporal Interaction Framework Combining Transformer and Kolmogorov-Arnold Networks for Time Series Forecasting](https://arxiv.org/abs/2602.11190)
*Fan Zhang,Shiming Fan,Hua Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列嵌入视角——多偏移时间嵌入（MOTE），以解决传统独立token嵌入破坏多偏移时序相关性的问题，并基于此构建了新型预测架构Time-TK，在14个真实数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法将每个时间步独立嵌入为token，导致长序列处理中的信息瓶颈，尤其破坏了Web数据中普遍存在的多偏移时序相关性（即跨不同时间步的细粒度依赖）

Method: 提出多偏移时间嵌入（MOTE）方法，并基于其构建Time-TK架构：包含多偏移交互KAN（学习多偏移子序列的时序模式）和多偏移时序交互机制（捕获子序列间复杂依赖）

Result: 在涵盖交通流、BTC/USDT吞吐量等领域的14个真实基准数据集上，Time-TK显著优于所有基线模型，达到当前最优预测精度

Conclusion: MOTE是一种通用、简洁而有效的时间序列嵌入方案，可无缝集成至现有模型；Time-TK通过显式建模多偏移时序相关性，从根本上提升了长期时序预测性能

Abstract: Time series forecasting is crucial for the World Wide Web and represents a core technical challenge in ensuring the stable and efficient operation of modern web services, such as intelligent transportation and website throughput. However, we have found that existing methods typically employ a strategy of embedding each time step as an independent token. This paradigm introduces a fundamental information bottleneck when processing long sequences, the root cause of which is that independent token embedding destroys a crucial structure within the sequence - what we term as multi-offset temporal correlation. This refers to the fine-grained dependencies embedded within the sequence that span across different time steps, which is especially prevalent in regular Web data. To fundamentally address this issue, we propose a new perspective on time series embedding. We provide an upper bound on the approximate reconstruction performance of token embedding, which guides our design of a concise yet effective Multi-Offset Time Embedding method to mitigate the performance degradation caused by standard token embedding. Furthermore, our MOTE can be integrated into various existing models and serve as a universal building block. Based on this paradigm, we further design a novel forecasting architecture named Time-TK. This architecture first utilizes a Multi-Offset Interactive KAN to learn and represent specific temporal patterns among multiple offset sub-sequences. Subsequently, it employs an efficient Multi-Offset Temporal Interaction mechanism to effectively capture the complex dependencies between these sub-sequences, achieving global information integration. Extensive experiments on 14 real-world benchmark datasets, covering domains such as traffic flow and BTC/USDT throughput, demonstrate that Time-TK significantly outperforms all baseline models, achieving state-of-the-art forecasting accuracy.

</details>


### [239] [MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models](https://arxiv.org/abs/2602.11192)
*Arian Raje,Anupam Nayak,Gauri Joshi*

Main category: cs.LG

TL;DR: MELINOE是一种通过微调MoE模型以减少每序列激活专家数量的方法，从而降低CPU-GPU专家传输开销、提升推理吞吐量，同时保持或提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽能减少每token激活参数量，但整体参数量大、需全部加载至GPU内存，在资源受限场景下难以部署；现有CPU卸载方法因专家频繁迁移带来显著I/O延迟。

Method: 提出MELINOE方法，对MoE模型进行微调，使其更倾向于每序列激活更少的专家；将这些偏好专家常驻GPU缓存，减少专家切换和跨设备传输。

Result: 相比高效基线吞吐量提升1.2–3倍，相比高传输开销基线最高提升14.7倍，且下游任务性能持平或提升。

Conclusion: MELINOE是一种可靠、实用的MoE推理效率优化方法，兼顾内存效率与模型性能。

Abstract: Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\times$ over efficient baselines and up to $14.7\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.

</details>


### [240] [Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data](https://arxiv.org/abs/2602.11194)
*Mahta Movasat,Ingrid Tomac*

Main category: cs.LG

TL;DR: 本文利用多种机器学习算法分析野火后泥流启动条件，发现细沙在低强度长历时降雨下易侵蚀，高强度降雨前10分钟最关键，ML方法有助于灾害评估与应急响应。


<details>
  <summary>Details</summary>
Motivation: 野火后土壤疏水性增强导致泥流强度、持续时间和破坏力显著增加，亟需理解其启动时机与关键影响因素（如降雨强度、坡度、入渗量、颗粒级配）的耦合作用。

Method: 基于斜槽降雨实验模拟野外条件，采用多元线性回归（MLR）、逻辑回归（LR）、支持向量分类器（SVC）、K均值聚类和主成分分析（PCA）等机器学习方法建模与分类，并开展敏感性分析。

Result: MLR能较好预测总径流量但侵蚀预测精度较低（尤其粗砂）；LR和SVC对失稳分类准确率高；细砂在低强度长历时降雨下最易侵蚀；高强度降雨前10分钟对产流与失稳最关键。

Conclusion: 机器学习方法可有效识别野火后泥流关键驱动因子与临界条件，为灾害风险评估和应急响应提供数据驱动支持。

Abstract: Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly on sand-based hillslopes. Rainwater and eroded soil blanket the downslope, leading to catastrophic debris flows. Soil hydrophobicity enhances erosion, resulting in post-wildfire debris flows that differ from natural mudflows in intensity, duration, and destructiveness. Thus, it is crucial to understand the timing and conditions of debris-flow onset, driven by the coupled effects of critical parameters: varying rain intensities (RI), slope gradients, water-entry values, and grain sizes (D50). Machine Learning (ML) techniques have become increasingly valuable in geotechnical engineering due to their ability to model complex systems without predefined assumptions. This study applies multiple ML algorithms: multiple linear regression (MLR), logistic regression (LR), support vector classifier (SVC), K-means clustering, and principal component analysis (PCA) to predict and classify outcomes from laboratory experiments that model field conditions using a rain device on various soils in sloped flumes. While MLR effectively predicted total discharge, erosion predictions were less accurate, especially for coarse sand. LR and SVC achieved good accuracy in classifying failure outcomes, supported by clustering and dimensionality reduction. Sensitivity analysis revealed that fine sand is highly susceptible to erosion, particularly under low-intensity, long-duration rainfall. Results also show that the first 10 minutes of high-intensity rain are most critical for discharge and failure. These findings highlight the potential of ML for post-wildfire hazard assessment and emergency response planning.

</details>


### [241] [AM-FM: A Foundation Model for Ambient Intelligence Through WiFi](https://arxiv.org/abs/2602.11200)
*Guozhen Zhu,Yuqian Hu,Sakila Jayaweera,Weihang Gao,Wei-Hsiang Wang,Jiaxuan Zhang,Beibei Wang,Chenshu Wu,K. J. Ray Liu*

Main category: cs.LG

TL;DR: 本文提出了首个面向环境智能和WiFi感知的基础模型AM-FM，通过在大量无标签信道状态信息（CSI）上预训练，结合对比学习、掩码重建与物理信息目标，显著提升了跨任务泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 无线感知通常依赖任务特定模型，需大量标注数据，限制了实际部署；而WiFi基础设施具备普适性、持续在线和隐私保护优势，其潜力尚未被充分挖掘。

Method: 提出AM-FM基础模型，基于920万条来自全球20种商用设备、历时439天采集的无标签CSI样本，采用对比学习、掩码重建和物理信息驱动的目标进行预训练。

Result: 在涵盖九类下游任务的公开基准上验证，AM-FM展现出优异的跨任务性能和更高的数据效率。

Conclusion: 基础模型可有效利用现有WiFi基础设施实现可扩展的环境智能，为无线感知开辟新范式。

Abstract: Ambient intelligence, continuously understanding human presence, activity, and physiology in physical spaces, is fundamental to smart environments, health monitoring, and human-computer interaction. WiFi infrastructure provides a ubiquitous, always-on, privacy-preserving substrate for this capability across billions of IoT devices. Yet this potential remains largely untapped, as wireless sensing has typically relied on task-specific models that require substantial labeled data and limit practical deployment. We present AM-FM, the first foundation model for ambient intelligence and sensing through WiFi. AM-FM is pre-trained on 9.2 million unlabeled Channel State Information (CSI) samples collected over 439 days from 20 commercial device types deployed worldwide, learning general-purpose representations via contrastive learning, masked reconstruction, and physics-informed objectives tailored to wireless signals. Evaluated on public benchmarks spanning nine downstream tasks, AM-FM shows strong cross-task performance with improved data efficiency, demonstrating that foundation models can enable scalable ambient intelligence using existing wireless infrastructure.

</details>


### [242] [Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders](https://arxiv.org/abs/2602.11204)
*Zhuxin Lei,Ziyuan Yang,Yi Zhang*

Main category: cs.LG

TL;DR: 本文提出ZePAD方法，通过双分支结构（MPAE-Branch和BMP-Branch）实现仅一次对抗微调即可在多种下游任务中同时提升对下游无关对抗样本（DAEs）的鲁棒性并保持良性性能，无需额外的对抗样本识别训练，具有零牺牲特性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖任务特定的对抗微调，泛化性差、易灾难性遗忘且损害良性性能；而下游无关对抗样本（DAEs）对公开预训练编码器构成严重威胁。

Method: 提出Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD)，包含两个分支：1）Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch)，使用两个对抗微调编码器增强鲁棒性；2）Benign Memory Preservation Branch (BMP-Branch)，基于本地数据训练以保留良性性能；利用分支置信度差异直接检测DAEs。

Result: 在11种SSL方法和6个数据集上验证有效；部分场景下良性性能提升29.20%，对抗鲁棒性提升73.86%。

Conclusion: ZePAD实现了单次微调、跨任务持久鲁棒性与良性性能保持的统一，是一种更严格、更实用的SSL编码器防御范式。

Abstract: The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.

</details>


### [243] [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](https://arxiv.org/abs/2602.11206)
*Jose Marie Antonio Miñoza*

Main category: cs.LG

TL;DR: 本文提出UltraLIF框架，利用热带几何中的ultradiscretization替代启发式代理梯度，实现完全可微的脉冲神经网络（SNN），支持标准反向传播训练，并在多个基准上提升性能，尤其在单时间步设置下效果显著。


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks (SNNs)因脉冲的非可微性而依赖启发式代理梯度，导致前向-反向不匹配和训练不稳定；本文旨在从数学原理出发，提供一种可微、收敛、无代理梯度的SNN训练新范式。

Method: 提出ultradiscretization（超离散化）作为连续松弛工具，基于max-plus半环建模神经元阈值动力学；构造UltraLIF（源自LIF微分方程）和UltraDLIF（源自扩散方程）两种全可微神经元模型；引入可学习温度参数ε控制soft-threshold到hard-threshold的收敛；支持标准BP训练，可选稀疏性正则化以降低能耗。

Result: 理论证明UltraLIF点态收敛于经典LIF且梯度有界非消失；实验在6个静态图像、神经形态视觉与音频任务上超越代理梯度基线，尤其在T=1设置下提升显著；加入稀疏惩罚后能耗大幅下降而精度保持竞争力。

Conclusion: UltraLIF为SNN提供了兼具数学严谨性、可微性与实用性的新框架，消除了对代理梯度的依赖，提升了训练稳定性与能效，在时序与神经形态任务中展现出优越性。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

</details>


### [244] [Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems](https://arxiv.org/abs/2602.11208)
*Xin Ju,Nok Hei,Fung,Yuyan Zhang,Carl Jacquemyn,Matthew Jackson,Randolph Settgast,Sally M. Benson,Gege Wen*

Main category: cs.LG

TL;DR: 本文提出了自适应物理Transformer（APT），一种几何、网格和物理无关的神经算子，用于高效模拟地球地下系统，显著提升了计算效率和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 地下系统全物理数值模拟因地质异质性、高分辨率需求及多物理过程时间尺度耦合而计算成本极高。

Method: 提出APT模型，融合图编码器提取局部异质特征与全局注意力机制捕捉长程物理影响，并首次直接学习自适应网格细化模拟数据。

Result: APT在规则与非规则网格的地下任务中均超越现有最优架构，具备强超分辨率能力和跨数据集学习能力。

Conclusion: APT是首个支持自适应网格学习的神经算子，为大规模地下基础模型提供了鲁棒、可扩展的骨干架构。

Abstract: The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.

</details>


### [245] [Towards Compressive and Scalable Recurrent Memory](https://arxiv.org/abs/2602.11212)
*Yunchong Song,Jushi Kai,Liming Lu,Kaixi Qiu,Zhouhan Lin*

Main category: cs.LG

TL;DR: 本文提出Elastic Memory，一种基于HiPPO框架的新型记忆架构，通过将历史序列视为连续信号采样并进行最优在线压缩，实现高效长上下文建模，在多个长文本任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长上下文时面临注意力机制的二次方计算瓶颈；现有引入循环记忆的方法常在理论严谨性与实际可扩展性之间难以兼顾。

Method: 基于HiPPO框架构建Elastic Memory，将历史序列建模为连续信号，采用最优在线压缩编码为固定大小记忆状态；引入多项式采样机制实现灵活的历史摘要重建；设计解耦结构以支持测试时注入归纳偏置。

Result: 在32k+长上下文数据集的三个领域中持续超越基线；同等参数下内存效率比Memorizing Transformer高16倍，性能全面优于Melodi（即使后者多30%参数）；模型放大时仍保持领先，且推理速度比4倍规模的Melodi更快。

Conclusion: Elastic Memory在理论基础、实际效率与可扩展性之间取得更好平衡，为长上下文建模提供了新范式。

Abstract: Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.

</details>


### [246] [Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning](https://arxiv.org/abs/2602.11215)
*Lintao Wang,Zhuqiang Lu,Yilin Zhu,Kun Hu,Zhenfei Yin,Shixiang Tang,Zhiyong Wang,Wanli Ouyang,Xinzhu Ma*

Main category: cs.LG

TL;DR: 本文首次系统研究了大语言模型在多学科环境下的微调学习动态，提出了四个经验法则，为多学科微调提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在单学科微调中表现优异，但其在多学科场景下的学习机制尚不清楚，而跨学科知识协同有望提升泛化能力和适用性。

Method: 构建五学科语料库，对比分析全参数微调、LoRA、LoRA-MoE及LoRA组合等方法的学习模式。

Result: 发现多学科学习比单学科更具变异性，并归纳出四条经验法则：平衡优先再多样性、合并优先再对齐、优化优先再扩展、共享优先再特化。

Conclusion: 这些法则构成了一套可操作的多学科微调原则，有助于开发更具泛化能力的科学领域大语言模型。

Abstract: While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.

</details>


### [247] [Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators](https://arxiv.org/abs/2602.11216)
*Panagiotis Antoniadis,Beatrice Pavesi,Simon Olsson,Ole Winther*

Main category: cs.LG

TL;DR: 本文提出PLaTITO方法，通过结合粗粒度建模与蛋白语言模型（pLM）嵌入等辅助信息，提升生成式分子动力学（GenMD）中隐式转移算子（TITO）的数据效率和跨系统泛化能力，在蛋白质系统平衡采样任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学（MD）计算成本高；现有生成式MD（GenMD）方法虽提升采样效率，但跨分子系统的可迁移性差。

Method: 提出PLaTITO框架：基于可迁移的隐式转移算子（TITO），引入粗粒度建模、蛋白语言模型（pLM）嵌入及其他条件信号（如结构嵌入、温度、大语言模型嵌入）以增强泛化能力。

Result: PLaTITO在蛋白质系统（尤其是快折叠蛋白）的分布外（out-of-distribution）平衡采样基准上达到当前最优性能；粗粒度TITO比Boltzmann Emulators更数据高效；pLM嵌入显著提升分布外泛化能力。

Conclusion: 融合多源辅助信息（特别是pLM嵌入）能有效提升GenMD模型的数据效率与跨系统泛化性，为构建通用、可迁移的分子模拟工具提供新范式。

Abstract: Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.

</details>


### [248] [The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning](https://arxiv.org/abs/2602.11217)
*Simin Fan,Dimitris Paparas,Natasha Noy,Binbin Xiong,Noveen Sachdeva,Berivan Isik*

Main category: cs.LG

TL;DR: 本文研究了语言模型在预训练到监督微调（SFT）过程中能力迁移的规律，重点考察准确性与置信度排序的保持性、跨阶段预测基准的可靠性、模型规模对迁移动态的影响，以及置信度与准确率之间校准质量的跨阶段一致性。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型能力如何从预训练迁移到监督微调，对高效模型开发和数据筛选至关重要。

Method: 通过一系列相关性分析协议，在多种数据混合和不同模型规模下，对准确率和置信度指标进行跨阶段评估。

Result: 发现迁移可靠性在能力类别、基准测试和模型规模间差异显著；准确率与置信度呈现不同甚至相反的缩放规律。

Conclusion: 预训练决策与下游效果存在复杂交互，结果为基准选择、数据构建和高效建模提供了实践指导。

Abstract: Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.

</details>


### [249] [Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty](https://arxiv.org/abs/2602.11219)
*Tanmoy Mukherjee,Marius Kloft,Pierre Marquis,Zied Bouraoui*

Main category: cs.LG

TL;DR: 本文提出了一种基于可信集（credal-set）的不确定性分解新框架，将认知不确定性（epistemic）与偶然不确定性（aleatoric）在建模层面显式分离，通过变分可信概念瓶颈模型实现两者的解耦估计，显著降低二者相关性，并提升各自与真实误差/标注歧义的对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常从同一预测分布中估计两类不确定性，导致其高度相关、语义模糊，难以支持可靠决策。

Method: 提出可信集形式化框架，将不确定性表示为预测分布的集合；设计变分可信概念瓶颈模型，包含两个不共享参数与梯度路径的独立不确定性头，分别优化不同目标，实现结构性分离。

Result: 在多标注者基准上，相比标准方法，两类不确定性相关性降低一个数量级以上，且认知不确定性更准确反映预测误差，偶然不确定性更准确反映真实数据歧义。

Conclusion: 通过建模层面的结构性解耦，而非后验分解，可更清晰、更可靠地区分和利用两类不确定性，提升不确定性建模的语义准确性与实用性。

Abstract: Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.

</details>


### [250] [Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT](https://arxiv.org/abs/2602.11220)
*Jiacheng Wang,Ping Jian,Zhen Yang,Zirong Chen,Keren Liao,Zhongbin Guo*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的数据重写方法，用于缓解大语言模型在监督微调中因分布偏移导致的灾难性遗忘问题；通过将数据重写建模为策略学习问题，联合优化问答风格对齐性、多样性与任务一致性，显著提升下游任务性能并降低遗忘。


<details>
  <summary>Details</summary>
Motivation: 当下游数据与预训练分布存在较大偏移时，监督微调易引发灾难性遗忘；现有数据重写方法受限于提示诱导的条件采样和固定模板，难以保证重写结果与模型固有问答生成分布对齐，且多样性不足。

Method: 将数据重写建模为策略学习问题，设计一个强化学习代理（RL-based data-rewriting agent），以可评估但难微分的目标（分布对齐、多样性、任务一致性）为奖励信号，通过硬性任务一致性门控联合优化重写分布。

Result: 在多个基准上，该方法在保持下游任务性能的同时，平均降低非下游任务遗忘率12.34%；重写数据质量更高，更适配模型原生生成风格。

Conclusion: 数据重写不应仅依赖启发式提示或模板，而应作为可控、可优化的策略学习过程；RL框架能有效协调多目标权衡，为数据中心范式提供新思路。

Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .

</details>


### [251] [Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks](https://arxiv.org/abs/2602.11234)
*Ankita Paul,Wenyi Wang*

Main category: cs.LG

TL;DR: 本文提出TopoGBM框架，利用带拓扑正则化的3D卷积自编码器，从多参数MRI中学习保留肿瘤异质性且对扫描仪鲁棒的表征，显著提升胶质母细胞瘤跨中心预后性能。


<details>
  <summary>Details</summary>
Motivation: GBM在空间与结构上高度异质，且不同机构MRI采集协议不一致，导致现有深度学习模型泛化能力差、难以准确预后。

Method: 提出TopoGBM：基于3D卷积自编码器，引入拓扑正则化以在压缩潜在空间中保持肿瘤流形的非欧几里得不变量，从而建模高变异结构特征；结合重建残差分析与遮挡归因进行可解释性验证。

Result: 在UPENN、UCSF、RHUH多中心队列及TCGA外部验证中，C-index达0.67（测试）和0.58（验证），优于受域偏移影响的基线方法；重建误差在病灶区显著富集，在正常组织中极低（测试0.03，验证0.09）；约50%预后信号定位于肿瘤及周围微环境。

Conclusion: 引入拓扑先验可学习形态保真、异质性敏感且跨机构鲁棒的嵌入表示，为无监督医学影像预后建模提供新范式。

Abstract: Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.

</details>


### [252] [AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management](https://arxiv.org/abs/2602.11237)
*Mujeeb Ur Rehman,Imran Rehan,Sohail Khalid*

Main category: cs.LG

TL;DR: 本研究开发并评估了一种用于2型糖尿病诊断的人工智能临床决策支持系统（AI-CDSS），该系统融合专家知识与机器学习，基于BMI、空腹血糖和糖化血红蛋白等关键指标，在训练和测试数据集上均表现出超过98%的高准确率，并在临床试点中显著优于非内分泌专科医生。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病在基层医疗中诊断困难，亟需辅助工具提升非专科医生的识别能力。

Method: 采用专家驱动与机器学习相结合的混合方法构建AI-CDSS；使用1298例患者数据（训练集650例，测试集648例）进行模型开发与验证；在105例患者的临床试点中对比AI-CDSS、内分泌专科医生与非内分泌专科医生的诊断一致性。

Result: AI-CDSS在测试集上对糖尿病、前期糖尿病、高风险及无糖尿病状态的预测准确率分别为99.8%、99.3%、99.2%和98.8%；与内分泌专科医生诊断一致率达98.8%；在试点研究中与专科医生的协和率为98.5%，远高于非专科医生的85%。

Conclusion: 该AI-CDSS具备高准确性与临床实用性，有望成为缺乏内分泌专科资源环境下辅助2型糖尿病早期识别的有效工具。

Abstract: Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDSS specifically developed for the diagnosis of type 2 diabetes by employing a hybrid approach that integrates expert-driven insights with machine learning techniques. The AI-CDSS was developed (training dataset: n = 650) and tested (test dataset: n = 648) using a dataset of 1298 patients with and without type 2 diabetes. To generate predictions, the algorithm utilized key features such as body mass index, plasma fasting glucose, and hemoglobin A1C. Furthermore, a clinical pilot study involving 105 patients was conducted to assess the diagnostic accuracy of the system in comparison to non-endocrinology specialists. The AI-CDSS showed a high degree of accuracy, with 99.8% accuracy in predicting diabetes, 99.3% in predicting prediabetes, 99.2% in identifying at-risk individuals, and 98.8% in predicting no diabetes. The test dataset revealed a 98.8% agreement between endocrinology specialists and the AI-CDSS. Type 2 diabetes was identified in 45% of 105 individuals in the pilot study. Compared with diabetes specialists, the AI-CDSS scored a 98.5% concordance rate, greatly exceeding that of nonendocrinology specialists, who had an 85% agreement rate. These findings indicate that the AI-CDSS has the potential to be a useful tool for accurately identifying type 2 diabetes, especially in situations in which diabetes specialists are not readily available.

</details>


### [253] [Evaluating Memory Structure in LLM Agents](https://arxiv.org/abs/2602.11243)
*Alina Shutova,Alexandra Olenina,Ivan Vinogradov,Anton Sinitsin*

Main category: cs.LG

TL;DR: 本文提出了StructMemEval基准，用于评估LLM代理组织长期记忆的能力，而不仅仅是事实性回忆。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆基准主要关注简单事实保留、多跳回忆和基于时间的变化，无法测试复杂的记忆层次结构，因此需要新的基准来评估记忆组织能力。

Method: 提出StructMemEval基准，包含人类通过特定结构（如交易账本、待办事项列表、树等）组织知识才能解决的任务，并通过实验对比检索增强型LLM与记忆代理的表现。

Result: 实验表明，简单检索增强型LLM在这些任务上表现较差，而经过适当提示的记忆代理能可靠解决；但现代LLM在未被提示时往往无法识别所需记忆结构。

Conclusion: StructMemEval揭示了当前LLM在自主识别和组织记忆结构方面的不足，为未来LLM训练和记忆框架设计指明了重要方向。

Abstract: Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.

</details>


### [254] [How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?](https://arxiv.org/abs/2602.11246)
*Nikhil Garg,Jon Kleinberg,Kenny Peng*

Main category: cs.LG

TL;DR: 本文提出了线性表示假设（LRH）的数学框架，区分了线性表示与线性可访问性，并给出了线性压缩感知下神经元数量d与特征数m、稀疏度k之间的紧致上下界，揭示了线性可访问性比单纯线性表示更强，并支持‘叠加假说’。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型中间层如何线性存储和读取语义特征，厘清‘线性表示假设’的理论基础与限制。

Method: 建立线性压缩感知理论模型，结合随机矩阵构造（上界）、Alon秩界与Turán定理（下界），并扩展至含激活函数和偏置的解码器情形。

Result: 证明线性压缩感知中所需神经元数d满足Ω_ε(k²/log k·log(m/k)) ≤ d ≤ O_ε(k² log m)；揭示线性可访问性带来本质性额外代价；确认在LRH下神经元可指数级存储特征。

Conclusion: 线性可访问性是比线性表示更强且非平凡的约束；该理论为叠加假说提供了首个定量支撑，并划定了LRH的可行性边界。

Abstract: We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear accessibility (features can be linearly decoded). We then ask: How many neurons $d$ suffice to both linearly represent and linearly access $m$ features? Classical results in compressed sensing imply that for $k$-sparse inputs, $d = O(k\log (m/k))$ suffices if we allow non-linear decoding algorithms (Candes and Tao, 2006; Candes et al., 2006; Donoho, 2006). However, the additional requirement of linear decoding takes the problem out of the classical compressed sensing, into linear compressed sensing.
  Our main theoretical result establishes nearly-matching upper and lower bounds for linear compressed sensing. We prove that $d = Ω_ε(\frac{k^2}{\log k}\log (m/k))$ is required while $d = O_ε(k^2\log m)$ suffices. The lower bound establishes a quantitative gap between classical and linear compressed setting, illustrating how linear accessibility is a meaningfully stronger hypothesis than linear representation alone. The upper bound confirms that neurons can store an exponential number of features under the LRH, giving theoretical evidence for the "superposition hypothesis" (Elhage et al., 2022).
  The upper bound proof uses standard random constructions of matrices with approximately orthogonal columns. The lower bound proof uses rank bounds for near-identity matrices (Alon, 2003) together with Turán's theorem (bounding the number of edges in clique-free graphs). We also show how our results do and do not constrain the geometry of feature representations and extend our results to allow decoders with an activation function and bias.

</details>


### [255] [HiFloat4 Format for Language Model Inference](https://arxiv.org/abs/2602.11287)
*Yuanyong Luo,Jing Huang,Yu Cheng,Ziwei Yu,Kaihua Zhang,Kehong Hong,Xinda Ma,Xin Wang,Anping Tong,Guipeng Hu,Yun Xu,Mehran Taghian,Peng Wu,Guanglin Li,Yunke Peng,Tianchi Hu,Minqi Chen,Michael Bi Mi,Hu Liu,Xiping Zhou,Junsong Wang,Qiang Lin,Heng Liao*

Main category: cs.LG

TL;DR: 本文提出HiFloat4 (HiF4)块浮点格式，每个单元打包64个4位元素并共享32位缩放元数据，平均4.5位/值，具备三级缩放层次结构以提升表示空间利用率，并支持高效定点矩阵乘法，显著降低硬件面积和功耗；实验表明其在多个大语言模型上的推理精度优于当前最优NVFP4格式。


<details>
  <summary>Details</summary>
Motivation: 为解决深度学习中低比特量化格式在动态范围表示与硬件效率之间的权衡问题，提升低比特表示的精度与硬件友好性。

Method: 设计HiFloat4块浮点格式，采用64元素大分组、32位共享三级缩放元数据（支持组间与组内动态范围建模），并优化其用于矩阵乘法的定点计算流程。

Result: 在LLaMA、Qwen、Mistral、DeepSeek-V3.1和LongCat等语言模型的推理任务中，HiF4在多种下游任务上平均精度高于NVFP4。

Conclusion: HiF4是一种兼顾高精度与高硬件效率的新型4位级块浮点格式，为大模型低比特推理提供了更优的数据表示方案。

Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.

</details>


### [256] [Efficient Analysis of the Distilled Neural Tangent Kernel](https://arxiv.org/abs/2602.11320)
*Jamie Mahowald,Brian Bell,Alex Ho,Michael Geyer*

Main category: cs.LG

TL;DR: 本文提出了一种称为蒸馏神经正切核（DNTK）的新方法，通过NTK调优的数据集蒸馏和投影技术相结合，大幅降低NTK计算复杂度（最高达五个数量级），同时保持核结构与预测性能。


<details>
  <summary>Details</summary>
Motivation: NTK方法因需在大量数据点上计算大规模雅可比矩阵而面临计算瓶颈，现有方法主要依赖雅可比投影与草图技术降本，本文探索从数据维度压缩角度进一步优化。

Method: 提出NTK调优的数据集蒸馏方法，利用蒸馏数据诱导原始输入数据张成的神经正切空间，并结合先进投影技术构建DNTK。

Result: 实现20–100倍雅可比计算量减少；验证每类NTK矩阵具有低有效秩且该性质在蒸馏后得以保持；DNTK将NTK计算复杂度最高降低五个数量级，同时保持核结构与预测性能。

Conclusion: 数据维度压缩（尤其是NTK调优的蒸馏）是降低NTK计算成本的有效新路径，DNTK为高效、保真NTK计算提供了实用框架。

Abstract: Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.

</details>


### [257] [Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence](https://arxiv.org/abs/2602.11322)
*Jason Dury*

Main category: cs.LG

TL;DR: 本文提出预测性联想记忆（PAM）架构，利用时间共现关系而非相似性进行记忆检索，通过Inward与Outward JEPA联合建模过去与未来状态的联想结构，在合成基准上显著优于基于余弦相似度的传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经记忆系统依赖相似性检索，但忽略了生物记忆中基于时间共现的关键联想机制。

Method: 提出Predictive Associative Memory（PAM），结合JEPA风格预测器，引入Inward JEPA（预测可联想的过去状态）与Outward JEPA（预测未来状态），在连续经验流中学习嵌入空间的联想结构。

Result: 在合成基准上，PAM实现Association Precision@1 = 0.970、跨边界Recall@20 = 0.421（余弦相似度为0）、联想判别AUC = 0.916（余弦为0.789）；对嵌入相似性无效的跨房间配对仍达AUC = 0.849（余弦仅0.503）；时间打乱控制实验证实性能源于真实时间结构。

Conclusion: PAM证明基于时间共现的预测性联想机制可有效建模生物式记忆，显著超越相似性驱动的记忆检索范式。

Abstract: Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\leq$ 0.012).

</details>


### [258] [Divide and Learn: Multi-Objective Combinatorial Optimization at Scale](https://arxiv.org/abs/2602.11346)
*Esha Singh,Dongxia Wu,Chien-Yi Yang,Tajana Rosing,Rose Yu,Yi-An Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于分解决策空间的在线学习框架，将多目标组合优化问题建模为位置式老虎机子问题，并通过自适应专家引导的顺序构造求解，在理论保证、效率和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多目标组合优化中往往在通用性、可扩展性或理论保证之间做出权衡，缺乏兼顾三者的方案。

Method: 将多目标组合优化重新建模为分解决策空间上的在线学习问题，每个位置对应一个带约束的老虎机子问题，采用自适应专家引导的顺序构造策略进行求解。

Result: 获得 $O(d\sqrt{T \log T})$ 的遗憾界；在标准基准上达到专用求解器80%-98%的性能，样本与计算效率比贝叶斯优化高2-3个数量级；在AI加速器软硬件协同设计的真实任务中，固定评估预算下优于对比方法，且优势随问题规模和目标数增加而增强。

Conclusion: 基于分解决策空间的老虎机优化是一种有理论保障、高效且可扩展的多目标优化新范式，可替代代理建模或离线训练方法。

Abstract: Multi-objective combinatorial optimization seeks Pareto-optimal solutions over exponentially large discrete spaces, yet existing methods sacrifice generality, scalability, or theoretical guarantees. We reformulate it as an online learning problem over a decomposed decision space, solving position-wise bandit subproblems via adaptive expert-guided sequential construction. This formulation admits regret bounds of $O(d\sqrt{T \log T})$ depending on subproblem dimensionality \(d\) rather than combinatorial space size. On standard benchmarks, our method achieves 80--98\% of specialized solvers performance while achieving two to three orders of magnitude improvement in sample and computational efficiency over Bayesian optimization methods. On real-world hardware-software co-design for AI accelerators with expensive simulations, we outperform competing methods under fixed evaluation budgets. The advantage grows with problem scale and objective count, establishing bandit optimization over decomposed decision spaces as a principled alternative to surrogate modeling or offline training for multi-objective optimization.

</details>


### [259] [Structured Hybrid Mechanistic Models for Robust Estimation of Time-Dependent Intervention Outcomes](https://arxiv.org/abs/2602.11350)
*Tomer Meir,Ori Linial,Danny Eytan,Uri Shalit*

Main category: cs.LG

TL;DR: 本文提出了一种混合机制-数据驱动方法，用于估计动态系统中干预措施的时间依赖性效果，特别适用于丙泊酚给药等临床场景，在分布外（OOD）情况下表现优于纯数据驱动或纯机制模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型在分布外（OOD）场景下泛化能力差，而机制模型虽鲁棒但过于简化；临床干预（如丙泊酚给药）需在个体化、动态变化的药代动力学系统中实现精准剂量优化，亟需兼顾可解释性与灵活性的建模方法。

Method: 将动态系统的转移算子分解为参数（机制）与非参数（数据驱动）两部分，并进一步区分干预相关与无关的动力学；针对机制参数未知的情况，采用两阶段策略：先在仿真数据上预训练编码器，再用真实观测数据学习校正项。

Result: 在周期摆和丙泊酚单次推注两个不完全机制已知的实验场景中，该混合方法在预测精度与OOD鲁棒性上均显著优于纯数据驱动和纯机制模型。

Conclusion: 混合机制-数据驱动建模能有效融合机制模型的鲁棒性与数据驱动模型的表达能力，为复杂现实动态系统中的稳健干预优化提供了新范式。

Abstract: Estimating intervention effects in dynamical systems is crucial for outcome optimization. In medicine, such interventions arise in physiological regulation (e.g., cardiovascular system under fluid administration) and pharmacokinetics, among others. Propofol administration is an anesthetic intervention, where the challenge is to estimate the optimal dose required to achieve a target brain concentration for anesthesia, given patient characteristics, while avoiding under- or over-dosing. The pharmacokinetic state is characterized by drug concentrations across tissues, and its dynamics are governed by prior states, patient covariates, drug clearance, and drug administration. While data-driven models can capture complex dynamics, they often fail in out-of-distribution (OOD) regimes. Mechanistic models on the other hand are typically robust, but might be oversimplified. We propose a hybrid mechanistic-data-driven approach to estimate time-dependent intervention outcomes. Our approach decomposes the dynamical system's transition operator into parametric and nonparametric components, further distinguishing between intervention-related and unrelated dynamics. This structure leverages mechanistic anchors while learning residual patterns from data. For scenarios where mechanistic parameters are unknown, we introduce a two-stage procedure: first, pre-training an encoder on simulated data, and subsequently learning corrections from observed data. Two regimes with incomplete mechanistic knowledge are considered: periodic pendulum and Propofol bolus injections. Results demonstrate that our hybrid approach outperforms purely data-driven and mechanistic approaches, particularly OOD. This work highlights the potential of hybrid mechanistic-data-driven models for robust intervention optimization in complex, real-world dynamical systems.

</details>


### [260] [Bootstrapping-based Regularisation for Reducing Individual Prediction Instability in Clinical Risk Prediction Models](https://arxiv.org/abs/2602.11360)
*Sara Matijevic,Christopher Yau*

Main category: cs.LG

TL;DR: 本文提出一种基于自助法（bootstrapping）的正则化框架，将自助采样过程嵌入深度神经网络训练中，以提升临床预测模型的稳定性，同时保持判别性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习临床预测模型常因训练样本微小变化而产生大幅预测波动，导致可靠性不足、临床采纳受限。

Method: 提出一种将自助法直接嵌入深度神经网络训练过程的正则化框架，通过约束模型在不同自助样本上的预测一致性，获得单个具备内在稳定性的模型。

Result: 在模拟数据及GUSTO-I、Framingham、SUPPORT三个临床数据集上验证，该方法显著降低预测差异（如GUSTO-I中平均绝对差从0.059降至0.019），减少异常预测，且保持高判别性能与SHAP特征重要性一致性（GUSTO-I达0.894，Framingham达0.965）；相比集成模型，本方法在稳定性略低但显著提升可解释性。

Conclusion: 该自助正则化方法可在不牺牲可解释性和判别能力的前提下，有效提升深度学习临床预测模型的鲁棒性与可重复性，尤其适用于医疗数据稀缺场景。

Abstract: Clinical prediction models are increasingly used to support patient care, yet many deep learning-based approaches remain unstable, as their predictions can vary substantially when trained on different samples from the same population. Such instability undermines reliability and limits clinical adoption. In this study, we propose a novel bootstrapping-based regularisation framework that embeds the bootstrapping process directly into the training of deep neural networks. This approach constrains prediction variability across resampled datasets, producing a single model with inherent stability properties. We evaluated models constructed using the proposed regularisation approach against conventional and ensemble models using simulated data and three clinical datasets: GUSTO-I, Framingham, and SUPPORT. Across all datasets, our model exhibited improved prediction stability, with lower mean absolute differences (e.g., 0.019 vs. 0.059 in GUSTO-I; 0.057 vs. 0.088 in Framingham) and markedly fewer significantly deviating predictions. Importantly, discriminative performance and feature importance consistency were maintained, with high SHAP correlations between models (e.g., 0.894 for GUSTO-I; 0.965 for Framingham). While ensemble models achieved greater stability, we show that this came at the expense of interpretability, as each constituent model used predictors in different ways. By regularising predictions to align with bootstrapped distributions, our approach allows prediction models to be developed that achieve greater robustness and reproducibility without sacrificing interpretability. This method provides a practical route toward more reliable and clinically trustworthy deep learning models, particularly valuable in data-limited healthcare settings.

</details>


### [261] [Retrieval-Aware Distillation for Transformer-SSM Hybrids](https://arxiv.org/abs/2602.11374)
*Aviv Bick,Eric P. Xing,Albert Gu*

Main category: cs.LG

TL;DR: 本文提出了一种检索感知蒸馏方法，通过仅保留Transformer中对检索至关重要的少量注意力头（约2%），并将其余部分蒸馏为循环头，构建高效混合模型，在保持95%以上性能的同时大幅降低内存开销。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在序列建模上高效，但在需要上下文检索的任务上落后于Transformer；先前研究认为这是由于SSMs难以复现一类关键的'聚集与聚合'（G&A）注意力头。

Method: 通过在合成检索任务上进行注意力头消融实验识别关键G&A头，仅保留这些头，其余用蒸馏方式转换为循环头，构建稀疏、非均匀注意力分布的混合模型，并简化SSM状态维度。

Result: 仅保留2%的注意力头（如10亿参数模型中10个头）即可恢复教师模型95%以上的检索性能；SSM状态维度可缩减8倍，整体内存效率提升5–6倍。

Conclusion: 检索感知蒸馏能以极低的注意力头和状态开销，显著缩小SSM与Transformer在检索任务上的性能差距，实现高效率与高性能的平衡。

Abstract: State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.

</details>


### [262] [Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges](https://arxiv.org/abs/2602.11378)
*Amirpasha Hedayat,Alberto Padovan,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 本文提出并研究了自适应非侵入式降阶模型（ROMs），通过在线更新潜在子空间和简化动力学，解决了传统静态ROMs在系统离开训练流形后性能下降的问题。作者提出了三种自适应方法：Adaptive OpInf、Adaptive NiTROM及其混合形式，并在扰动驱动空腔流问题上验证了其有效性与计算代价权衡。


<details>
  <summary>Details</summary>
Motivation: 传统投影型降阶模型作为静态代理，在系统偏离训练流形时失效，缺乏在线适应能力，限制了实际应用。

Method: 基于静态非侵入式ROM方法（OpInf和NiTROM），提出三种自适应框架：1）Adaptive OpInf（顺序更新基与算子）；2）Adaptive NiTROM（在黎曼流形上联合优化编码器/解码器与多项式动力学）；3）混合方法（以OpInf更新初始化NiTROM）。定义并分析了在线数据窗、适应窗与计算预算。

Result: 在瞬态扰动的驱动空腔流测试中：Adaptive OpInf以较低代价有效抑制振幅漂移；Adaptive NiTROM在频繁更新下接近精确能量跟踪，但对初始化和优化深度敏感；混合方法在工况变化和极少离线数据下最可靠，生成物理一致的场且能量有界。

Conclusion: 自适应非侵入式ROM需兼顾预测精度与在线计算成本；预测声明必须成本感知、透明，明确区分训练/适应/部署阶段，并报告在线预算与全阶模型查询次数；本工作为构建随动力学演化持续有效的自校正ROM提供了实用范式。

Abstract: Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.

</details>


### [263] [WSBD: Freezing-Based Optimizer for Quantum Neural Networks](https://arxiv.org/abs/2602.11383)
*Christopher Kverne,Mayur Akewar,Yuqian Huo,Tirthak Patel,Janki Bhimani*

Main category: cs.LG

TL;DR: 本文提出了一种名为加权随机块下降（WSBD）的新型优化器，通过基于梯度重要性评分的动态参数级冻结策略，减少量子神经网络（QNNs）训练中的前向传播次数并缓解平坦优化景观问题，显著提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决QNN训练中梯度估计计算开销大和“贫瘠高原”（barren plateau）导致优化困难的问题。

Method: 提出Weighted Stochastic Block Descent（WSBD），采用梯度导出的重要性评分进行参数级动态冻结，按需分配计算资源，不牺牲模型表达能力。

Result: 在基态能量任务上，WSBD平均比Adam快63.9%收敛，且优势随QNN规模增大而增强；理论证明其收敛性，并验证参数级冻结优于层级冻结。

Conclusion: WSBD是一种高效、自适应且理论上可证的QNN优化方法，为克服QNN训练瓶颈提供了新范式。

Abstract: The training of Quantum Neural Networks (QNNs) is hindered by the high computational cost of gradient estimation and the barren plateau problem, where optimization landscapes become intractably flat. To address these challenges, we introduce Weighted Stochastic Block Descent (WSBD), a novel optimizer with a dynamic, parameter-wise freezing strategy. WSBD intelligently focuses computational resources by identifying and temporarily freezing less influential parameters based on a gradient-derived importance score. This approach significantly reduces the number of forward passes required per training step and helps navigate the optimization landscape more effectively. Unlike pruning or layer-wise freezing, WSBD maintains full expressive capacity while adapting throughout training. Our extensive evaluation shows that WSBD converges on average 63.9% faster than Adam for the popular ground-state-energy problem, an advantage that grows with QNN size. We provide a formal convergence proof for WSBD and show that parameter-wise freezing outperforms traditional layer-wise approaches in QNNs. Project page: https://github.com/Damrl-lab/WSBD-Stochastic-Freezing-Optimizer.

</details>


### [264] [Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization](https://arxiv.org/abs/2602.11387)
*Anirudh Satheesh,Ziyi Chen,Furong Huang,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一种针对广义策略参数化下鲁棒马尔可夫决策过程（RMDP）的新方法，通过熵正则化将平均奖励RMDP转化为折扣RMDP，恢复强对偶性，并设计了高效梯度估计与优化算法，在s-矩形与非矩形不确定性集下均取得显著改进的样本复杂度界。


<details>
  <summary>Details</summary>
Motivation: 现有RMDP研究主要局限于表格策略，缺乏样本复杂度保证或计算代价高；且在非(s,a)-矩形及平均奖励设定下缺乏理论保障。

Method: 将平均奖励RMDP转化为熵正则化折扣RMDP以恢复强对偶性；证明广义策略参数化的Lipschitz与Lipschitz光滑性；提出多层蒙特卡洛梯度估计器；分别设计投影梯度下降（s-矩形）与Frank-Wolfe（非矩形）算法。

Result: 获得多项最优/领先样本复杂度：梯度估计为Õ(ε⁻²)，s-矩形下PGD为O(ε⁻⁵)，非矩形下FW为O(ε⁻⁴)（折扣）与O(ε⁻¹⁰·⁵)（平均奖励）。

Conclusion: 首次为超越(s,a)-矩形的广义策略参数化RMDP提供样本复杂度保证，也是平均奖励RMDP的首个此类结果，并全面改进了折扣RMDP的已有界。

Abstract: We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\tilde{\mathcal{O}}(ε^{-2})$ sample complexity, a factor of $\mathcal{O}(ε^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\mathcal{O}(ε^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\mathcal{O}(ε^{-4})$ discounted, $\mathcal{O}(ε^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.

</details>


### [265] [Sparse Semantic Dimension as a Generalization Certificate for LLMs](https://arxiv.org/abs/2602.11388)
*Dibyanayan Bandyopadhyay,Asif Ekbal*

Main category: cs.LG

TL;DR: 本文提出稀疏语义维度（SSD）作为衡量大语言模型泛化能力的新复杂度指标，指出模型实际泛化由内部表征的低维稀疏流形决定，而非参数总量；实验证明SSD能提供非空泛的泛化界，并发现‘特征锐度’缩放律及SSD在检测分布外输入中的安全监控潜力。


<details>
  <summary>Details</summary>
Motivation: 解释为何参数量远超训练token的大语言模型仍能稳健泛化，挑战传统统计学习理论的过拟合预测。

Method: 引入稀疏语义维度（SSD），基于在LLM各层激活上训练的稀疏自编码器（SAE）的活跃特征词表大小定义；将LLM与SAE视为冻结预言机，用SSD量化有效容量。

Result: 在GPT-2 Small和Gemma-2B上验证了SSD给出非空泛泛化界；发现Gemma-2B比GPT-2需更少校准样本即可识别活跃流形（'特征锐度'缩放律）；证明分布外输入会引发显著的'特征爆炸'，可作为可靠的安全监测信号。

Conclusion: LLM的泛化能力源于其内部表征的低维稀疏结构，SSD是比参数量更本质的复杂度度量；该框架兼具理论解释力与实用价值，可用于泛化分析与安全监控。

Abstract: Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive "feature sharpness" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable "feature explosion" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.

</details>


### [266] [General and Efficient Steering of Unconditional Diffusion](https://arxiv.org/abs/2602.11395)
*Qingsong Wang,Mikhail Belkin,Yusu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种无需推理时梯度计算即可高效引导无条件扩散模型的方法，基于噪声对齐和可迁移概念向量两个关键观察，并通过无反向传播的轻量方法RFM学习概念方向，在多个数据集上实现了比基于梯度引导更优的质量与更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有引导无条件扩散模型的方法（如重训练或每步梯度计算）计算开销大，亟需更高效的推理时引导机制。

Method: 利用噪声对齐特性在高噪声阶段进行粗粒度语义引导；引入跨时间步和样本可迁移的概念向量，并通过无反向传播的Recursive Feature Machine（RFM）学习固定概念方向；全程不依赖推理时梯度。

Result: 在CIFAR-10、ImageNet和CelebA上，生成质量/准确性优于基于梯度的引导方法，同时显著提升推理速度。

Conclusion: 无需梯度的、基于可迁移概念向量的引导策略是高效可控扩散生成的有效新范式。

Abstract: Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.

</details>


### [267] [Can We Really Learn One Representation to Optimize All Rewards?](https://arxiv.org/abs/2602.11399)
*Chongyi Zheng,Royina Karegoudra Jayanth,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文分析了前向-后向（FB）表示学习在强化学习中的作用，揭示了其训练目标和收敛行为，并提出了一种简化的单步FB方法，在多个连续控制任务中显著提升了零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有FB表示学习方法虽声称能实现任意奖励下的最优控制，但其训练目标和学习行为尚不明确，需进行理论澄清与改进。

Method: 通过理论分析建立FB与秩匹配、拟合Q评估及压缩映射的联系，提出简化版的单步FB表示学习方法，仅执行一步策略改进而非追求最优控制。

Result: 在10个基于状态和图像的连续控制任务中，单步FB收敛误差减小10^5倍，平均零样本性能提升24%。

Conclusion: FB表示学习的本质并非实现最优控制，而是隐含执行策略改进；单步FB是一种更简洁、高效且实用的无监督预训练方法。

Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.

</details>


### [268] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: 本文提出了CADET，一种用于广告点击率（CTR）预测的端到端解码器-only Transformer模型，已在LinkedIn广告平台部署并取得11.04%的CTR提升。


<details>
  <summary>Details</summary>
Motivation: 传统DLRM在广告CTR预测中占主导，但生成式推荐器（如Transformer）在内容推荐中表现优异；然而将其适配至广告CTR预测面临诸多挑战，包括后排序上下文信号建模、离线-在线一致性、工业级扩展性等。

Method: 提出CADET模型：1）上下文条件解码架构+多塔预测头，显式建模广告位置等后排序信号；2）自门控注意力机制，稳定训练；3）基于时间戳的RoPE变体，建模多尺度时间关系；4）会话掩码策略缓解训推偏差；5）多项工程优化（张量打包、序列分块、定制Flash Attention）支持大规模训练与服务。

Result: 在线A/B测试中，CADET相较生产环境LiRank基线（DCNv2与序列编码器混合模型）实现11.04% CTR提升，并已全量上线LinkedIn信息流广告主流量。

Conclusion: CADET成功将生成式解码器架构引入工业级广告CTR预测，在建模能力、训练稳定性、时序建模和系统效率等方面取得综合突破，验证了decoder-only范式在该任务中的可行性与优势。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [269] [TimeSynth: A Framework for Uncovering Systematic Biases in Time Series Forecasting](https://arxiv.org/abs/2602.11413)
*Md Rakibul Haque,Vishwa Goudar,Shireen Elhabian,Warren Woodrich Pettine*

Main category: cs.LG

TL;DR: 本文提出TimeSynth框架，通过合成具有真实时间序列特性的信号（如非平稳性、周期性、趋势和相位调制），重新评估线性与非线性模型在时间序列预测中的性能。结果表明：线性模型存在系统性坍缩倾向，而非线性模型（尤其是Transformer和CNN）在复杂信号下表现更优且更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 质疑当前关于线性模型优于复杂非线性模型的结论，指出既有基准缺乏多样性与公平评估协议，需构建更贴近真实世界动态的可控合成框架。

Method: 设计TimeSynth框架，基于真实时间序列统计特性生成多样化合成信号；在独立训练/验证/测试集上系统评测Linear、MLP、CNN和Transformer四类模型在干净预测、分布偏移及噪声鲁棒性下的表现。

Result: 线性模型在复杂信号中普遍坍缩为简单振荡；非线性模型随信号复杂度提升优势明显；Transformer与CNN对调制信号适应性略优于MLP；新框架揭示了模型在分布偏移和噪声下的鲁棒性差异。

Conclusion: 线性与非线性模型并非等效，其相对优劣高度依赖于时间序列的内在复杂性；TimeSynth为可复现、无偏的时间序列建模研究提供了原则性基准框架。

Abstract: Time series forecasting is a fundamental tool with wide ranging applications, yet recent debates question whether complex nonlinear architectures truly outperform simple linear models. Prior claims of dominance of the linear model often stem from benchmarks that lack diverse temporal dynamics and employ biased evaluation protocols. We revisit this debate through TimeSynth, a structured framework that emulates key properties of real world time series,including non-stationarity, periodicity, trends, and phase modulation by creating synthesized signals whose parameters are derived from real-world time series. Evaluating four model families Linear, Multi Layer Perceptrons (MLP), Convolutional Neural Networks (CNNs), and Transformers, we find a systematic bias in linear models: they collapse to simple oscillation regardless of signal complexity. Nonlinear models avoid this collapse and gain clear advantages as signal complexity increases. Notably, Transformers and CNN based models exhibit slightly greater adaptability to complex modulated signals compared to MLPs. Beyond clean forecasting, the framework highlights robustness differences under distribution and noise shifts and removes biases of prior benchmarks by using independent instances for train, test, and validation for each signal family. Collectively, TimeSynth provides a principled foundation for understanding when different forecasting approaches succeed or fail, moving beyond oversimplified claims of model equivalence.

</details>


### [270] [Multi-Level Strategic Classification: Incentivizing Improvement through Promotion and Relegation Dynamics](https://arxiv.org/abs/2602.11439)
*Ziyuan Huang,Lina Alkarmi,Mingyan Liu*

Main category: cs.LG

TL;DR: 本文研究了序贯策略性分类问题，提出了一种基于多级晋升-降级框架的阈值与难度设计方法，通过建模代理的远见性、技能保留和自我强化效应，证明在温和条件下可激励代理人仅通过诚实努力达到任意高水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注动态调整分类器权重，而忽视了阈值设计与难度演进对代理人长期行为的激励作用；同时，实际中代理人具有跨期决策能力、技能积累与正向反馈效应，需更贴合现实的建模范式。

Method: 构建包含多级阈值、技能状态演化、远见效用最大化的序贯博弈模型；分析代理人最优长期策略；设计满足激励相容性的阈值序列，并给出存在性与收敛性证明。

Result: 刻画了代理人在多级框架下的最优长期策略；证明存在阈值序列可严格激励诚实努力；进一步证实在温和条件下，代理人能仅凭真实能力提升达到任意高阶水平。

Conclusion: 阈值与难度的动态设计是比权重调整更根本的激励工具；引入跨期视角与自我强化机制，可实现纯诚实努力驱动的可持续能力跃迁。

Abstract: Strategic classification studies the problem where self-interested individuals or agents manipulate their response to obtain favorable decision outcomes made by classifiers, typically turning to dishonest actions when they are less costly than genuine efforts. While existing studies on sequential strategic classification primarily focus on optimizing dynamic classifier weights, we depart from these weight-centric approaches by analyzing the design of classifier thresholds and difficulty progression within a multi-level promotion-relegation framework. Our model captures the critical inter-temporal incentives driven by an agent's farsightedness, skill retention, and a leg-up effect where qualification and attainment can be self-reinforcing. We characterize the agent's optimal long-term strategy and demonstrate that a principal can design a sequence of thresholds to effectively incentivize honest effort. Crucially, we prove that under mild conditions, this mechanism enables agents to reach arbitrarily high levels solely through genuine improvement efforts.

</details>


### [271] [Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification](https://arxiv.org/abs/2602.11448)
*Nghia Nguyen,Tianjiao Ding,René Vidal*

Main category: cs.LG

TL;DR: 本文提出了一种名为HCEP的框架，通过在视觉语言模型的潜在空间中构建概念嵌入的层次结构，并采用分层稀疏编码来恢复图像中的概念，从而提升可解释性与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏概念恢复方法忽略了概念间的层次结构，导致解释与语义层次不一致，影响模型可解释性。

Method: 提出分层概念嵌入与追寻（HCEP）框架：基于语义概念层次构建对应的嵌入层次，并利用分层稀疏编码恢复图像中符合根路径假设的概念；理论推导了嵌入空间中识别正确概念的条件。

Result: 实验表明，HCEP在概念精确率和召回率上优于基线方法，分类准确率具竞争力；小样本下表现更优；分层稀疏编码比普通稀疏编码更可靠地恢复层次化概念嵌入。

Conclusion: 将层次结构引入稀疏编码可提升图像分类模型的可靠性与可解释性。

Abstract: Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.

</details>


### [272] [Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning](https://arxiv.org/abs/2602.11465)
*Jared Levy,Aarti Lalwani,Elijah Wyckoff,Kenneth J. Loh,Sara P. Gombatto,Rose Yu,Emilia Farcas*

Main category: cs.LG

TL;DR: 本文提出了一种名为MT-AIM的深度学习分类模型，利用条件生成模型增强小规模、含噪的Motion Tape传感器数据，实现高精度下背运动识别，适用于远程康复监测。


<details>
  <summary>Details</summary>
Motivation: 传统高保真动作捕捉传感器成本高、不便于居家使用；新型低成本可穿戴Motion Tape传感器虽便携但数据量小、噪声大、稳定性差，亟需适配的分析方法。

Method: 提出Motion-Tape Augmentation Inference Model（MT-AIM），结合条件生成模型合成目标运动的伪MT数据，并预测关节运动学特征作为额外输入，构建端到端深度学习分类流程。

Result: MT-AIM在MT数据集上实现了下背运动分类的当前最优准确率，有效融合生理传感与运动分析。

Conclusion: MT-AIM成功克服了Motion Tape数据规模小和噪声大的挑战，为低成本可穿戴设备在远程运动评估与康复中的实际应用提供了可行技术路径。

Abstract: Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.

</details>


### [273] [PRISM: A 3D Probabilistic Neural Representation for Interpretable Shape Modeling](https://arxiv.org/abs/2602.11467)
*Yining Jiao,Sreekalyani Bhamidi,Carlton Jude Zdanski,Julia S Kimbell,Andrew Prince,Cameron P Worden,Samuel Kirse,Christopher Rutter,Benjamin H Shields,Jisan Mahmud,Marc Niethammer*

Main category: cs.LG

TL;DR: 本文提出了PRISM框架，结合隐式神经表示与不确定性感知的统计形状分析，用于建模解剖形状随发育协变量变化的条件分布，并提供空间连续的均值与不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖忽略空间异质动态的全局时间扭曲模型，难以准确刻画解剖形状演化中空间变化的不确定性，而临床研究亟需可解释、空间分辨的不确定性量化。

Method: PRISM将隐式神经表示与统计形状建模结合，建模形状关于协变量的条件分布；提出闭式Fisher信息度量，支持通过自动微分实现高效、解析可解的局部时间不确定性量化。

Result: 在三个合成数据集和一个临床数据集上的实验表明，PRISM在多项任务中性能优异，能统一框架下提供可解释、临床有意义的空间不确定性估计。

Conclusion: PRISM为发育与临床形状分析提供了新范式，兼顾建模表达力、计算效率与不确定性可解释性，推动了从全局到局部、从确定性到概率性形状建模的发展。

Abstract: Understanding how anatomical shapes evolve in response to developmental covariates and quantifying their spatially varying uncertainties is critical in healthcare research. Existing approaches typically rely on global time-warping formulations that ignore spatially heterogeneous dynamics. We introduce PRISM, a novel framework that bridges implicit neural representations with uncertainty-aware statistical shape analysis. PRISM models the conditional distribution of shapes given covariates, providing spatially continuous estimates of both the population mean and covariate-dependent uncertainty at arbitrary locations. A key theoretical contribution is a closed-form Fisher Information metric that enables efficient, analytically tractable local temporal uncertainty quantification via automatic differentiation. Experiments on three synthetic datasets and one clinical dataset demonstrate PRISM's strong performance across diverse tasks within a unified framework, while providing interpretable and clinically meaningful uncertainty estimates.

</details>


### [274] [External Division of Two Bregman Proximity Operators for Poisson Inverse Problems](https://arxiv.org/abs/2602.11482)
*Kazuki Haishima,Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 本文提出了一种基于外部除法Bregman邻近算子的新方法，用于在泊松噪声干扰下恢复稀疏向量，相比传统KL方法具有更稳定收敛性和更高精度。


<details>
  <summary>Details</summary>
Motivation: 解决泊松噪声下线性模型中稀疏向量恢复问题，缓解传统ℓ1范数正则化带来的估计偏差。

Method: 引入由两个Bregman邻近算子外部除法定义的新算子以促进稀疏性，并将其嵌入NoLips算法中替代标准Bregman邻近算子；通过原空间与对偶空间的两种等价重构揭示其几何结构。

Result: 数值实验表明该方法比传统KL方法收敛更稳定，在合成数据和图像复原任务上性能显著更优。

Conclusion: 所提外部除法Bregman算子是一种有效且具解释性的稀疏恢复工具，适用于泊松逆问题。

Abstract: This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.

</details>


### [275] [Exploring Multiple High-Scoring Subspaces in Generative Flow Networks](https://arxiv.org/abs/2602.11491)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出CMAB-GFN，将组合多臂赌博机（CMAB）与生成流网络（GFlowNets）结合，通过剪枝低质量动作来聚焦高分子空间，从而在保持多样性的同时加速发现高奖励候选解。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets在庞大状态空间中过度探索，易陷入低奖励区域，难以有效偏向高奖励解。

Method: 提出CMAB-GFN框架，利用CMAB动态筛选高质量动作，约束GFlowNet仅在紧凑的高分搜索子空间内进行探索与采样。

Result: 在多个任务上实验表明，CMAB-GFN生成的候选解平均奖励显著高于现有方法。

Conclusion: CMAB-GFN通过引入CMAB机制有效平衡了探索效率与解多样性，在组合生成任务中提升了GFlowNets的性能。

Abstract: As a probabilistic sampling framework, Generative Flow Networks (GFlowNets) show strong potential for constructing complex combinatorial objects through the sequential composition of elementary components. However, existing GFlowNets often suffer from excessive exploration over vast state spaces, leading to over-sampling of low-reward regions and convergence to suboptimal distributions. Effectively biasing GFlowNets toward high-reward solutions remains a non-trivial challenge. In this paper, we propose CMAB-GFN, which integrates a combinatorial multi-armed bandit (CMAB) framework with GFlowNet policies. The CMAB component prunes low-quality actions, yielding compact high-scoring subspaces for exploration. Restricting GFNs to these compact high-scoring subspaces accelerates the discovery of high-value candidates, while the exploration of different subspaces ensures that diversity is not sacrificed. Experimental results on multiple tasks demonstrate that CMAB-GFN generates higher-reward candidates than existing approaches.

</details>


### [276] [Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning](https://arxiv.org/abs/2602.11498)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种改进的生成流网络（GFlowNets）方法，通过引入规划器将大状态空间划分为重叠的子空间，并结合启发式区域切换策略，提升在大规模状态空间中的收敛速度、高奖励样本生成能力和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets在大规模状态空间中自由探索导致收敛困难。

Method: 引入规划器将全状态空间划分为多个重叠的局部子空间，并设计启发式策略动态切换子空间，使Actor聚焦于高奖励区域。

Result: 在多个基准数据集上，该方法相比现有方法收敛更快，生成的候选样本奖励更高、多样性更好。

Conclusion: 受限探索+分而治之的策略显著提升了GFlowNets在大规模状态空间下的效率与性能。

Abstract: Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \modelname converges faster than existing works on large state spaces. Furthermore, \modelname not only generates candidates with higher rewards but also significantly improves their diversity.

</details>


### [277] [Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.11882)
*Suraj Ranganath,Anish Patnaik,Vaishak Menon*

Main category: cs.LG

TL;DR: 本文研究了在低比特精度下空间推理中不同模块（如编码器）的位宽分配对规划性能的影响，发现4比特是关键过渡区域，在此区域内编码器精度的保留显著提升规划效果，提出了模块和预算感知的量化策略。


<details>
  <summary>Details</summary>
Motivation: 高效的空间推理需要在严格精度限制下仍保持可靠性的世界模型，而低比特规划行为究竟主要取决于总位宽还是各模块间的位宽分配尚不明确。

Method: 基于DINO-WM模型，在Wall规划任务上开展配对目标混合比特评估，对比均匀、混合、非对称和逐层量化变体，并在两种规划器预算下进行实验。

Result: 观察到三阶段模式：8/6比特接近FP16性能，3比特完全崩溃，4比特则对位宽分配高度敏感；其中保留编码器高精度优于均匀量化，且非对称变体也呈现相同趋势；在更严格的22-cell复现实验中，INT4下混合vs均匀的优劣依赖于预算条件。

Conclusion: 4比特是低精度空间推理的关键过渡区，其性能强烈依赖模块级位宽分配策略，因此应发展模块感知、预算感知的量化政策作为未来研究方向。

Abstract: Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.

</details>


### [278] [A Generic Framework for Fair Consensus Clustering in Streams](https://arxiv.org/abs/2602.11500)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文提出了流式模型下的公平共识聚类算法，仅需存储对数数量的输入聚类，达到常数因子近似，并设计了一个通用框架，适用于任意可高效计算近似公平聚类的公平性定义，还扩展到k-中位数共识聚类。


<details>
  <summary>Details</summary>
Motivation: 现有公平共识聚类方法在离线场景下需存储全部输入聚类，内存开销大，难以适用于大规模应用；亟需在内存受限的流式场景下设计高效算法。

Method: 提出首个适用于流式模型的常数因子近似算法，仅存储对数数量的输入；构建融合‘最近公平聚类’与‘聚类拟合’的通用算法框架；该框架对公平性定义无依赖，且可扩展至k-中位数共识聚类。

Result: 实现了流式公平共识聚类的常数因子近似，空间复杂度降至O(log n)；新框架在流式和离线场景下均提升近似比；框架具有公平性无关性，并成功推广至k-中位数设定。

Conclusion: 本文首次将公平共识聚类拓展至流式模型，通过新颖的通用框架兼顾效率、可扩展性与公平适应性，为大规模多智能体环境下的公平聚类提供了实用解决方案。

Abstract: Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.
  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.

</details>


### [279] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 本文研究了在仅有购买数据的情况下，如何利用可能存在偏差的外部预测器来校准无购买概率，提出了两种校准方法，并分析了其对下游决策（如商品组合优化）的影响。


<details>
  <summary>Details</summary>
Motivation: 企业通常无法观察到消费者的关键行为（如是否购买竞争对手产品、是否放弃购买等），导致市场大小和偏好估计困难，尤其当仅有交易数据时。

Method: 提出了两种校准方法：一是在logit空间中存在仿射失准时，通过简单回归识别外部选项效用参数；二是在较弱的近单调条件下，提出基于排序的校准方法，并推导有限样本误差界。

Result: 实现了无需新增无购买标签即可一致恢复无购买概率；推导出误差界，明确区分辅助预测器质量与效用学习误差；数值实验验证了无购买估计及下游商品组合决策的改进。

Conclusion: 所提校准方法能有效利用有偏的黑盒预测器，在仅有购买数据下实现统计有效的无购买概率估计，并提升下游决策质量。

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [280] [RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis](https://arxiv.org/abs/2602.11506)
*Zhen Bi,Xueshu Chen,Luoyang Sun,Yuhang Yao,Qing Shen,Jungang Lou,Cheng Deng*

Main category: cs.LG

TL;DR: 本文提出了一种基于Roofline模型的系统性框架，用于在边缘硬件上量化小语言模型（SLMs）和大语言模型（LLMs）的推理性能上限，引入‘相对推理潜力’新指标，并揭示序列长度、模型深度与硬件异构性对效率的影响，指导软硬协同设计。


<details>
  <summary>Details</summary>
Motivation: 边缘端部署小型语言模型（SLMs）日益重要，但缺乏在异构资源受限硬件上客观、统一地评估不同架构理论性能上限的方法。

Method: 基于Roofline模型构建系统性分析框架，以操作强度（OI）为统一维度，定义‘推理潜力区域’，提出‘相对推理潜力’作为跨模型比较指标；结合多平台实证分析，探究序列长度、模型深度及硬件差异的影响，并验证结构改进（如MLA）的有效性。

Result: 发现序列长度显著影响性能与OI；模型深度增加导致OI关键性下降；硬件异构性引发效率陷阱；MLA等结构优化可有效释放不同硬件上的潜在推理能力。

Conclusion: 该框架为边缘AI提供了可量化的性能建模工具，揭示了模型结构与物理硬件约束间的深层关系，为面向设备端智能的软硬协同设计提供了明确路径。

Abstract: The transition toward localized intelligence through Small Language Models (SLMs) has intensified the need for rigorous performance characterization on resource-constrained edge hardware. However, objectively measuring the theoretical performance ceilings of diverse architectures across heterogeneous platforms remains a formidable challenge. In this work, we propose a systematic framework based on the Roofline model that unifies architectural primitives and hardware constraints through the lens of operational intensity (OI). By defining an inference-potential region, we introduce the Relative Inference Potential as a novel metric to compare efficiency differences between Large Language Models (LLMs) on the same hardware substrate. Extensive empirical analysis across diverse compute tiers reveals that variations in performance and OI are significantly influenced by sequence length. We further identify a critical regression in OI as model depth increases. Additionally, our findings highlight an efficiency trap induced by hardware heterogeneity and demonstrate how structural refinements, such as Multi-head Latent Attention (M LA), can effectively unlock latent inference potential across various hardware substrates. These insights provide actionable directions for hardware-software co-design to align neural structures with physical constraints in on-device intelligence. The released code is available in the Appendix C.

</details>


### [281] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的正则化方法，通过加权监督微调损失来平衡防止奖励黑客行为和保持策略更新稳定性，从而提升RLHF的对齐效果与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法分别用KL散度惩罚和策略比率裁剪来独立解决奖励黑客和优化不稳定问题，但二者隐含的权衡关系未被充分研究。

Method: 提出一种统一的正则化目标，将监督微调损失加权，显式协调对初始策略π₀和当前策略πₜ的约束，实现更优权衡。

Result: 在多个基准测试中，该方法持续优于标准RLHF和在线偏好学习方法，提升了对齐性能与训练稳定性。

Conclusion: 统一正则化框架不仅提高了对齐效果，还简化了实现复杂度，为RLHF提供了更鲁棒、更简洁的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [282] [Adaptive Milestone Reward for GUI Agents](https://arxiv.org/abs/2602.11524)
*Congmin Zheng,Xiaoyun Mo,Xinbei Ma,Qiqiang Lin,Yin Zhao,Jiachen Zhu,Xingyu Lou,Jun Wang,Zhaoxiang Wang,Weiwen Liu,Zhuosheng Zhang,Yong Yu,Weinan Zhang*

Main category: cs.LG

TL;DR: 本文提出ADMIRE机制，通过自适应里程碑奖励解决移动GUI代理中强化学习的长程信用分配问题，显著提升任务成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练移动GUI代理时面临长程任务中的时间信用分配难题，尤其是结果奖励稀疏但保真度高，过程奖励密集但易产生偏差和奖励黑客行为。

Method: 提出自适应里程碑奖励（ADMIRE）机制，基于成功探索动态提炼可验证的里程碑，并结合非对称信用分配策略对成功轨迹去噪、失败轨迹提供引导。

Result: 在AndroidWorld上，ADMIRE使不同基础模型的成功率绝对提升超10%；且在网页导航和具身任务等异构环境中展现出强泛化性。

Conclusion: ADMIRE有效缓解了奖励稀疏性与偏差间的权衡，为长程GUI任务提供了更鲁棒、通用的强化学习奖励设计范式。

Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.

</details>


### [283] [PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models](https://arxiv.org/abs/2602.11530)
*Eunyeong Cho,Jehyeon Bang,Ranggi Hwang,Minsoo Rhu*

Main category: cs.LG

TL;DR: 本文提出了PASCAL，一种面向推理型大语言模型（LLM）服务的相位感知调度算法，通过区分推理与回答阶段，优化GPU资源调度，显著降低首字延迟（TTFT），同时保障服务质量（QoE）。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架无法区分Chain-of-Thought（CoT）推理过程中的推理阶段和回答阶段，在GPU内存受限时导致性能下降，尤其是首字延迟（TTFT）严重增加。

Method: 提出PASCAL相位感知调度算法：优先调度推理阶段以降低TTFT；在回答阶段采用受控抢占与令牌节拍控制以保障QoE；设计分层调度器，支持实例级部署、实例内执行调度及跨阶段动态迁移。

Result: 在DeepSeek-R1-Distill-Qwen-32B模型上的实验表明，PASCAL将尾部TTFT最高降低72%，同时保持回答阶段的服务等级目标（SLO）达成率。

Conclusion: 对推理型LLM进行相位感知调度对提升实际部署性能至关重要，PASCAL为高效服务CoT类模型提供了新范式。

Abstract: The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.

</details>


### [284] [AltTS: A Dual-Path Framework with Alternating Optimization for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.11533)
*Zhihang Yuan,Zhiyuan Liu,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 本文提出ALTTS框架，通过分离自回归（AR）和跨维度关系（CR）建模，并采用交替优化策略，显著提升多元时间序列长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 单一模型同时建模稳定的自回归动态与易产生噪声的跨维度交互会导致优化冲突，损害训练稳定性与长时预测精度。

Method: 提出双路径框架ALTTS：AR路径使用线性预测器，CR路径采用带Cross-Relation Self-Attention（CRSA）的Transformer，并通过交替优化协调两路径以隔离梯度噪声。

Result: 在多个基准上ALTTS持续优于现有方法，尤其在长时预测任务中提升最显著。

Conclusion: 精心设计的优化策略比单纯增加模型复杂度更能推动多元时间序列预测的发展。

Abstract: Multivariate time series forecasting involves two qualitatively distinct factors: (i) stable within-series autoregressive (AR) dynamics, and (ii) intermittent cross-dimension interactions that can become spurious over long horizons. We argue that fitting a single model to capture both effects creates an optimization conflict: the high-variance updates needed for cross-dimension modeling can corrupt the gradients that support autoregression, resulting in brittle training and degraded long-horizon accuracy. To address this, we propose ALTTS, a dual-path framework that explicitly decouples autoregression and cross-relation (CR) modeling. In ALTTS, the AR path is instantiated with a linear predictor, while the CR path uses a Transformer equipped with Cross-Relation Self-Attention (CRSA); the two branches are coordinated via alternating optimization to isolate gradient noise and reduce cross-block interference. Extensive experiments on multiple benchmarks show that ALTTS consistently outperforms prior methods, with the most pronounced improvements on long-horizon forecasting. Overall, our results suggest that carefully designed optimization strategies, rather than ever more complex architectures, can be a key driver of progress in multivariate time series forecasting.

</details>


### [285] [Krause Synchronization Transformers](https://arxiv.org/abs/2602.11534)
*Jingkun Liu,Yisong Yue,Max Welling,Yue Song*

Main category: cs.LG

TL;DR: 本文提出Krause Attention，一种受有界置信共识动力学启发的新型注意力机制，通过距离驱动的局部稀疏交互替代全局softmax归一化，缓解表示坍缩与注意力汇现象，并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中的自注意力依赖全局归一化的softmax权重，导致各token在每层激烈竞争影响力；深层堆叠引发强同步动力学，易导致表示坍缩和注意力汇问题。

Method: 提出Krause Attention机制，以基于距离的、局部的、选择性稀疏的交互替代相似性驱动的全局聚合；其设计关联粒子系统建模的Transformer动态理论，并天然抑制注意力过度集中。

Result: 在视觉（ViT）、自回归生成（MNIST/CIFAR-10）及大语言模型（Llama/Qwen）上均取得一致性能提升，同时显著降低计算开销；序列长度复杂度由O(n²)降至O(n)。

Conclusion: 有界置信动力学是一种可扩展且有效的注意力归纳偏置，能兼顾建模能力与计算效率。

Abstract: Self-attention in Transformers relies on globally normalized softmax weights, causing all tokens to compete for influence at every layer. When composed across depth, this interaction pattern induces strong synchronization dynamics that favor convergence toward a dominant mode, a behavior associated with representation collapse and attention sink phenomena. We introduce Krause Attention, a principled attention mechanism inspired by bounded-confidence consensus dynamics. Krause Attention replaces similarity-based global aggregation with distance-based, localized, and selectively sparse interactions, promoting structured local synchronization instead of global mixing. We relate this behavior to recent theory modeling Transformer dynamics as interacting particle systems, and show how bounded-confidence interactions naturally moderate attention concentration and alleviate attention sinks. Restricting interactions to local neighborhoods also reduces runtime complexity from quadratic to linear in sequence length. Experiments across vision (ViT on CIFAR/ImageNet), autoregressive generation (MNIST/CIFAR-10), and large language models (Llama/Qwen) demonstrate consistent gains with substantially reduced computation, highlighting bounded-confidence dynamics as a scalable and effective inductive bias for attention.

</details>


### [286] [Real-Time Proactive Anomaly Detection via Forward and Backward Forecast Modeling](https://arxiv.org/abs/2602.11539)
*Luis Olmos,Rashida Hasan*

Main category: cs.LG

TL;DR: 本文提出两种主动异常检测框架FFM和BRM，结合TCN、GRU与Transformer建模时序方向性动态，分别通过前向预测与后向重构识别早期异常信号，在多个基准数据集上显著提升检测精度与时效性。


<details>
  <summary>Details</summary>
Motivation: 现有反应式异常检测无法满足工业监控、金融和网络安全等需及时干预场景的需求；而现有主动检测方法难以处理异构多变量数据，且在噪声或不可预测条件下精度不足。

Method: 提出前向预测模型（FFM）与后向重构模型（BRM），均采用TCN、GRU与Transformer编码器融合的混合架构，分别建模未来序列预测与基于未来上下文重建近期历史；通过预测误差大小与方向嵌入差异联合判定异常；支持连续与离散多变量特征。

Result: 在MSL、SMAP、SMD和PSM四个基准数据集上，FFM与BRM在各项检测指标上均超越现有最优方法，并显著提升异常预判的时效性。

Conclusion: 所提框架能有效捕捉早期异常先兆，兼顾鲁棒性与精度，适用于对时间敏感的主动监控任务。

Abstract: Reactive anomaly detection methods, which are commonly deployed to identify anomalies after they occur based on observed deviations, often fall short in applications that demand timely intervention, such as industrial monitoring, finance, and cybersecurity. Proactive anomaly detection, by contrast, aims to detect early warning signals before failures fully manifest, but existing methods struggle with handling heterogeneous multivariate data and maintaining precision under noisy or unpredictable conditions. In this work, we introduce two proactive anomaly detection frameworks: the Forward Forecasting Model (FFM) and the Backward Reconstruction Model (BRM). Both models leverage a hybrid architecture combining Temporal Convolutional Networks (TCNs), Gated Recurrent Units (GRUs), and Transformer encoders to model directional temporal dynamics. FFM forecasts future sequences to anticipate disruptions, while BRM reconstructs recent history from future context to uncover early precursors. Anomalies are flagged based on forecasting error magnitudes and directional embedding discrepancies. Our models support both continuous and discrete multivariate features, enabling robust performance in real-world settings. Extensive experiments on four benchmark datasets, MSL, SMAP, SMD, and PSM, demonstrate that FFM and BRM outperform state-of-the-art baselines across detection metrics and significantly improve the timeliness of anomaly anticipation. These properties make our approach well-suited for deployment in time-sensitive domains requiring proactive monitoring.

</details>


### [287] [Native Reasoning Models: Training Language Models to Reason on Unverifiable Data](https://arxiv.org/abs/2602.11549)
*Yuanfu Wang,Zhixuan Liu,Xiangtian Li,Chaochao Lu,Chao Yang*

Main category: cs.LG

TL;DR: 本文提出NRT（Native Reasoning Training）框架，通过让模型仅用问答对自生成推理过程，摆脱对人工标注推理链和外部验证器的依赖，以统一目标建模推理为隐变量优化问题，提升复杂推理能力并缓解策略崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理训练范式（SFT+RLVR）严重依赖高质量人工推理标注和外部验证器，导致成本高、易引入人类认知偏差，且仅适用于可验证任务（如数学、编程），难以推广至不可验证任务。

Method: NRT将推理过程建模为隐变量，设计统一训练目标：将推理路径视为优化问题，内在奖励能提高模型输出正确答案概率的路径；通过分析旧方法失败模式（如策略崩溃），设计更鲁棒的奖励聚合函数，形成自我强化反馈循环。

Result: 在Llama和Mistral系列模型上实验表明，NRT在无验证器方法中达到SOTA，显著优于标准SFT及以往无验证器RL方法，尤其在复杂推理任务中增益明显，且对策略崩溃具有高鲁棒性。

Conclusion: NRT提供了一种通用、可扩展的推理训练新范式，无需人工推理标注与外部验证器，推动构建更强大、更广泛适用的推理系统。

Abstract: The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs significant data-collection costs, risks embedding human cognitive biases, and confines the reinforcement learning stage to objectively assessable domains like mathematics and coding, leaving a wide range of unverifiable tasks beyond its scope. To overcome these limitations, we introduce NRT (Native Reasoning Training), a novel framework that cultivates complex reasoning by having the model generate its own reasoning traces using only standard question-answer pairs, thereby obviating the need for expert-written demonstrations. NRT reframes the training problem by treating the reasoning process as a latent variable. It employs a unified training objective that models reasoning as an optimization problem, intrinsically rewarding paths that increase the model's likelihood of producing the ground-truth answer. This unified perspective allows us to analyze intrinsic failure modes of prior methods, such as policy collapse, and systematically design more robust reward aggregation functions, creating a self-reinforcing feedback loop where the model learns to think in ways that resolve its own uncertainty. Empirical evaluation on Llama and Mistral model families demonstrates that NRT achieves state-of-the-art performance among verifier-free methods, significantly outperforming standard SFT baselines and prior verifier-free RL methods. Our approach yields particularly strong performance gains in complex reasoning domains and exhibits high robustness to policy collapse, offering a general, scalable path toward building more powerful and broadly applicable reasoning systems.

</details>


### [288] [TS-Memory: Plug-and-Play Memory for Time Series Foundation Models](https://arxiv.org/abs/2602.11550)
*Sisuo Lyu,Siru Zhong,Tiegang Chen,Weilin Ruan,Qingxiang Liu,Taiqiang Lv,Qingsong Wen,Raymond Chi-Wing Wong,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出TS-Memory，一种轻量级记忆适配器，通过两阶段参数化记忆蒸馏增强冻结的时间序列基础模型（TSFMs），在不牺牲推理效率的前提下提升零样本预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM下游适配方法面临权衡：参数化适应易导致灾难性遗忘且维护成本高；非参数检索虽提升预测效果但引入高推理延迟。

Method: 提出Parametric Memory Distillation方法，构建离线、防数据泄露的kNN教师模型生成置信度感知分位数目标，再通过置信度门控监督将该分布校正蒸馏至轻量级记忆适配器TS-Memory中。

Result: 在多种TSFMs和基准测试中，TS-Memory在点预测与概率预测上均一致优于代表性适配方法，推理效率接近冻结骨干网络。

Conclusion: TS-Memory实现了高效、鲁棒且无需检索的TSFM下游适配，在保持低开销的同时显著提升预测性能。

Abstract: Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.

</details>


### [289] [The Implicit Bias of Steepest Descent with Mini-batch Stochastic Gradient](https://arxiv.org/abs/2602.11557)
*Jichu Li,Xuan Tang,Difan Zou*

Main category: cs.LG

TL;DR: 本文研究了在多类分类中，小批量随机最速下降法（stochastic steepest descent）在不同范数几何下的隐式偏差，揭示了批量大小、动量和方差缩减如何影响其最大间隔极限行为与收敛速率，并给出了维度无关的显式收敛率。


<details>
  <summary>Details</summary>
Motivation: 理解常用优化方法（如SignSGD、Muon）在不同范数几何下作为最速下降法的统一解释，并厘清其在随机设置下的隐式正则化效应（特别是最大间隔性质）与训练动态的关系。

Method: 理论分析：基于一般逐元范数（entry-wise norms）和Schatten-p范数，对小批量随机最速下降法进行收敛性与隐式偏差分析；分别考察无动量、带动量及引入方差缩减三种情形，并构造反例揭示单样本情形下的偏差本质变化。

Result: 1) 无动量时仅大批次可收敛，存在批次依赖的间隔间隙但具全批量收敛速率；2) 动量可使小批次收敛，体现批次-动量权衡，但降低收敛速度；3) 方差缩减可精确恢复任意批次下的全批量隐式偏差（尽管速率更慢）；4) 单样本无动量情形收敛至完全不同的偏差，暴露纯随机更新的关键局限。

Conclusion: 该统一分析明确了随机优化何时能复现全批量行为，为深入理解随机梯度最速下降类算法的训练动力学提供了理论基础与新视角。

Abstract: A variety of widely used optimization methods like SignSGD and Muon can be interpreted as instances of steepest descent under different norm-induced geometries. In this work, we study the implicit bias of mini-batch stochastic steepest descent in multi-class classification, characterizing how batch size, momentum, and variance reduction shape the limiting max-margin behavior and convergence rates under general entry-wise and Schatten-$p$ norms. We show that without momentum, convergence only occurs with large batches, yielding a batch-dependent margin gap but the full-batch convergence rate. In contrast, momentum enables small-batch convergence through a batch-momentum trade-off, though it slows convergence. This approach provides fully explicit, dimension-free rates that improve upon prior results. Moreover, we prove that variance reduction can recover the exact full-batch implicit bias for any batch size, albeit at a slower convergence rate. Finally, we further investigate the batch-size-one steepest descent without momentum, and reveal its convergence to a fundamentally different bias via a concrete data example, which reveals a key limitation of purely stochastic updates. Overall, our unified analysis clarifies when stochastic optimization aligns with full-batch behavior, and paves the way for perform deeper explorations of the training behavior of stochastic gradient steepest descent algorithms.

</details>


### [290] [Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal](https://arxiv.org/abs/2602.11558)
*Fanqi Shen,Enhong Yang,Jiahe Li,Junru Hong,Xiaoran Pan,Zhizhang Yuan,Meng Li,Yang Yang*

Main category: cs.LG

TL;DR: 本文提出Brain4FMs，一个面向脑基础模型（BFMs）的开放评估平台，整合15种代表性BFMs和18个公开数据集，建立基于自监督学习分类法与下游任务/数据集双轴的基准设计框架，旨在推动BFMs的标准化评估与可迁移性提升。


<details>
  <summary>Details</summary>
Motivation: 当前脑基础模型（BFMs）发展迅速，但缺乏统一的方法学理解与标准化评估框架，制约其临床与科研应用的可靠性与可比性。

Method: 从模型视角构建自监督学习（SSL）分类法，从数据视角梳理下游任务并整理代表性公共数据集；在此基础上开发Brain4FMs平台，集成15个BFMs和18个数据集，支持预训练数据、SSL策略与架构对泛化与下游性能影响的系统分析。

Result: 发布Brain4FMs开源评估平台，提供即插即用接口，支持BFMs在统一基准下的可复现、可比较评估，并揭示不同预训练要素对模型性能的影响规律。

Conclusion: Brain4FMs为脑基础模型研究提供了首个系统化、开放化的评估基础设施，有望促进方法标准化、结果可比性及跨场景可迁移BFMs的发展。

Abstract: Brain Foundation Models (BFMs) are transforming neuroscience by enabling scalable and transferable learning from neural signals, advancing both clinical diagnostics and cutting-edge neuroscience exploration. Their emergence is powered by large-scale clinical recordings, particularly electroencephalography (EEG) and intracranial EEG, which provide rich temporal and spatial representations of brain dynamics. However, despite their rapid proliferation, the field lacks a unified understanding of existing methodologies and a standardized evaluation framework. To fill this gap, we map the benchmark design space along two axes: (i) from the model perspective, we organize BFMs under a self-supervised learning (SSL) taxonomy; and (ii) from the dataset perspective, we summarize common downstream tasks and curate representative public datasets across clinical and human-centric neurotechnology applications. Building on this consolidation, we introduce Brain4FMs, an open evaluation platform with plug-and-play interfaces that integrates 15 representative BFMs and 18 public datasets. It enables standardized comparisons and analysis of how pretraining data, SSL strategies, and architectures affect generalization and downstream performance, guiding more accurate and transferable BFMs. The code is available at https://anonymous.4open.science/r/Brain4FMs-85B8.

</details>


### [291] [Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization](https://arxiv.org/abs/2602.11584)
*Yujie Gu,Richeng Jin,Zhaoyang Zhang,Huaiyu Dai*

Main category: cs.LG

TL;DR: 本文提出FedSynSAM方法，通过利用全局模型轨迹构建合成数据，以准确估计全局扰动，解决梯度压缩在联邦学习中导致损失曲面变尖、泛化能力下降的问题。


<details>
  <summary>Details</summary>
Motivation: 梯度压缩在联邦学习中虽提升通信效率，但在非独立同分布（non-IID）数据下会导致损失曲面变尖，损害泛化能力；而现有Sharpness Aware Minimization（SAM）方法在FL中因数据异构性难以准确估计全局扰动，尤其在模型更新也被压缩时效果更差。

Method: 提出FedSynSAM，利用全局模型轨迹生成合成数据，从而更准确地估计全局扰动；理论分析其收敛性，并通过大量实验验证有效性。

Result: FedSynSAM能有效缓解梯度压缩引发的曲面尖锐化问题，提升模型泛化性能，在多个non-IID设置下优于基线方法。

Conclusion: 准确估计全局扰动对提升压缩型联邦学习的泛化能力至关重要；FedSynSAM通过合成数据机制实现了该目标，为通信高效且泛化良好的FL提供了新思路。

Abstract: It is commonly believed that gradient compression in federated learning (FL) enjoys significant improvement in communication efficiency with negligible performance degradation. In this paper, we find that gradient compression induces sharper loss landscapes in federated learning, particularly under non-IID data distributions, which suggests hindered generalization capability. The recently emerging Sharpness Aware Minimization (SAM) effectively searches for a flat minima by incorporating a gradient ascent step (i.e., perturbing the model with gradients) before the celebrated stochastic gradient descent. Nonetheless, the direct application of SAM in FL suffers from inaccurate estimation of the global perturbation due to data heterogeneity. Existing approaches propose to utilize the model update from the previous communication round as a rough estimate. However, its effectiveness is hindered when model update compression is incorporated. In this paper, we propose FedSynSAM, which leverages the global model trajectory to construct synthetic data and facilitates an accurate estimation of the global perturbation. The convergence of the proposed algorithm is established, and extensive experiments are conducted to validate its effectiveness.

</details>


### [292] [Learn from Your Mistakes: Self-Correcting Masked Diffusion Models](https://arxiv.org/abs/2602.11590)
*Yair Schiff,Omer Belhasin,Roy Uziel,Guanghan Wang,Marianne Arriola,Gilad Turok,Michael Elad,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 本文提出了一种名为ProSeCo的渐进式自校正框架，通过在掩码扩散模型（MDM）中引入纠错机制，在生成过程中反复修正已解码的token，从而提升样本质量与采样效率。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）虽能并行生成token且性能优异，但其一旦解码（unmask）的token即被固定，导致错误累积、样本质量下降。

Method: 提出ProSeCo框架：训练一个既能unmask又能correction的模型；利用MDM去噪网络输出作为corrector的输入进行训练；在生成时交替执行unmasking与correction步骤，动态修正已生成token。

Result: 在多个条件与无条件任务上验证，ProSeCo显著提升质量-效率权衡（采样速度提升约2–3倍），并支持推理时计算扩展，样本质量最高提升约1.3倍。

Conclusion: ProSeCo通过迭代式序列级修正，突破了MDM中token不可变的固有限制，为扩散模型生成范式提供了新思路。

Abstract: Masked diffusion models (MDMs) have emerged as a promising alternative to autoregressive models, enabling parallel token generation while achieving competitive performance. Despite these advantages, MDMs face a fundamental limitation: once tokens are unmasked, they remain fixed, leading to error accumulation and ultimately degrading sample quality. We address this by proposing a framework that trains a model to perform both unmasking and correction. By reusing outputs from the MDM denoising network as inputs for corrector training, we train a model to recover from potential mistakes. During generation we apply additional corrective refinement steps between unmasking ones in order to change decoded tokens and improve outputs. We name our training and sampling method Progressive Self-Correction (ProSeCo) for its unique ability to iteratively refine an entire sequence, including already generated tokens. We conduct extensive experimental validation across multiple conditional and unconditional tasks, demonstrating that ProSeCo yields better quality-efficiency trade-offs (up to ~2-3x faster sampling) and enables inference-time compute scaling to further increase sample quality beyond standard MDMs (up to ~1.3x improvement on benchmarks).

</details>


### [293] [SkillRater: Untangling Capabilities in Multimodal Data](https://arxiv.org/abs/2602.11615)
*Naveen Sahi,Jeremy Dohmann,Armen Aghajanyan,Akshat Shrivastava*

Main category: cs.LG

TL;DR: 本文提出SkillRater框架，通过多维能力分解与元学习训练的专用评分器，实现更精细的数据筛选，显著提升多任务视觉语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据清洗方法使用单一质量分数，难以同时优化模型所需多种能力；质量应被建模为多维，每维对应一种需习得的能力。

Method: 提出SkillRater：为不同能力（如视觉理解、OCR、STEM推理）设计独立的元学习评分器，并采用随训练阶段收紧阈值的渐进式选择规则进行数据过滤。

Result: 在2B参数视觉语言模型上，SkillRater相比无过滤基线，在视觉理解、OCR和STEM推理任务上分别提升5.63%、2.00%和3.53%；各评分器信号近正交，验证了多维分解的有效性。

Conclusion: 多维质量建模优于标量质量评分，SkillRater通过能力解耦与动态筛选，兼顾训练早期多样性与后期高质量聚焦，是更鲁棒的数据筛选范式。

Abstract: Data curation methods typically assign samples a single quality score. We argue this scalar framing is fundamentally limited: when training requires multiple distinct capabilities, a monolithic scorer cannot maximize useful signals for all of them simultaneously. Quality is better understood as multidimensional, with each dimension corresponding to a capability the model must acquire. We introduce SkillRater, a framework that decomposes data filtering into specialized raters - one per capability, each trained via meta-learning on a disjoint validation objective - and composes their scores through a progressive selection rule: at each training stage, a sample is retained if any rater ranks it above a threshold that tightens over time, preserving diversity early while concentrating on high-value samples late. We validate this approach on vision language models, decomposing quality into three capability dimensions: visual understanding, OCR, and STEM reasoning. At 2B parameters, SkillRater improves over unfiltered baselines by 5.63% on visual understanding, 2.00% on OCR, and 3.53% on STEM on held out benchmarks. The learned rater signals are near orthogonal, confirming that the decomposition captures genuinely independent quality dimensions and explaining why it outperforms both unfiltered training and monolithic learned filtering.

</details>


### [294] [How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?](https://arxiv.org/abs/2602.11618)
*Tatsuya Sagawa,Ryosuke Kojima*

Main category: cs.LG

TL;DR: 本文系统验证了化学语言模型（CLM）中增大训练资源（如模型规模、数据量、计算量）是否必然提升下游分子性质预测性能，发现预训练损失持续下降但下游性能提升有限，且常用预训练指标无法可靠预测下游表现，揭示了预训练评估与实际任务性能间的显著脱节。


<details>
  <summary>Details</summary>
Motivation: 验证化学领域中‘增加训练资源必提升下游性能’这一普遍假设是否成立，因该假设此前缺乏系统性实证支持。

Method: 通过在不同规模的模型、数据集和计算预算下预训练CLMs，并在多样的分子性质预测（MPP）任务上评估其迁移性能；同时分析Hessian矩阵、损失景观等替代指标与下游性能的相关性，并结合参数空间可视化探究任务依赖的失效模式。

Result: 预训练损失随资源增加而稳定下降，但下游MPP任务性能提升有限甚至饱和或下降；Hessian和损失景观等指标均无法有效估计下游性能；识别出下游性能饱和/退化的具体条件及任务相关失效机制。

Conclusion: 预训练指标（如损失）不能可靠反映CLM在下游任务中的实际表现，亟需发展面向具体下游任务特性的模型选择与评估策略。

Abstract: Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.

</details>


### [295] [TreeGrad-Ranker: Feature Ranking via $O(L)$-Time Gradients for Decision Trees](https://arxiv.org/abs/2602.11623)
*Weida Li,Yaoliang Yu,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文重新审视了使用概率值（如Shapley值和Banzhaf值）对决策树局部预测进行特征重要性排序的问题，指出其在联合优化插入/删除指标时存在理论不可靠性，并提出TreeGrad系列新方法直接优化该联合目标，显著提升解释质量与数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率值（如Shapley、Banzhaf）的特征排序方法在插入和删除指标上难以兼顾，且理论分析表明其不适用于对应的联合优化目标；需更可靠、高效、稳定的特征归因方法。

Method: 提出TreeGrad框架：利用多元线性扩展的梯度（含加权Banzhaf值）在O(L)时间计算决策树（L个叶子）的联合目标梯度；在此基础上构建TreeGrad-Ranker（用于生成满足多数公理的特征排序）和TreeGrad-Shap（稳定计算Beta Shapley值）；并推广出支持所有概率值的TreeProb。

Result: TreeGrad-Ranker在插入和删除指标上显著优于基线；TreeGrad-Shap数值误差比Linear TreeShap低至10^15倍；TreeProb统一支持各类概率值；所有方法均具备O(L)时间复杂度。

Conclusion: 概率值因线性公理导致对联合优化不可靠；直接优化联合目标的TreeGrad系列方法兼具理论合理性、计算高效性与强实证性能，为决策树可解释性提供了新范式。

Abstract: We revisit the use of probabilistic values, which include the well-known Shapley and Banzhaf values, to rank features for explaining the local predicted values of decision trees. The quality of feature rankings is typically assessed with the insertion and deletion metrics. Empirically, we observe that co-optimizing these two metrics is closely related to a joint optimization that selects a subset of features to maximize the local predicted value while minimizing it for the complement. However, we theoretically show that probabilistic values are generally unreliable for solving this joint optimization. Therefore, we explore deriving feature rankings by directly optimizing the joint objective. As the backbone, we propose TreeGrad, which computes the gradients of the multilinear extension of the joint objective in $O(L)$ time for decision trees with $L$ leaves; these gradients include weighted Banzhaf values. Building upon TreeGrad, we introduce TreeGrad-Ranker, which aggregates the gradients while optimizing the joint objective to produce feature rankings, and TreeGrad-Shap, a numerically stable algorithm for computing Beta Shapley values with integral parameters. In particular, the feature scores computed by TreeGrad-Ranker satisfy all the axioms uniquely characterizing probabilistic values, except for linearity, which itself leads to the established unreliability. Empirically, we demonstrate that the numerical error of Linear TreeShap can be up to $10^{15}$ times larger than that of TreeGrad-Shap when computing the Shapley value. As a by-product, we also develop TreeProb, which generalizes Linear TreeShap to support all probabilistic values. In our experiments, TreeGrad-Ranker performs significantly better on both insertion and deletion metrics. Our code is available at https://github.com/watml/TreeGrad.

</details>


### [296] [ArGEnT: Arbitrary Geometry-encoded Transformer for Operator Learning](https://arxiv.org/abs/2602.11626)
*Wenqian Chen,Yucheng Fu,Michael Penwarden,Pratanu Roy,Panos Stinis*

Main category: cs.LG

TL;DR: 本文提出了一种名为ArGEnT的几何感知Transformer架构，用于在任意几何域上学习解算子，通过点云直接编码几何信息，并与DeepONet结合，显著提升了跨几何泛化能力和预测精度。


<details>
  <summary>Details</summary>
Motivation: 在多查询场景（如设计优化、控制和反问题）中，现有 surrogate 模型难以同时兼顾对不同几何形状的泛化能力与在任意空间位置灵活评估的需求。

Method: 提出Arbitrary Geometry-encoded Transformer（ArGEnT），包含自注意力、交叉注意力和混合注意力三种变体，直接从点云表示中编码几何信息；将其作为 trunk 网络嵌入 DeepONet，避免将几何显式参数化为 branch 输入。

Result: 在流体力学、固体力学和电化学等基准问题上，ArGEnT-DeepONet 相比标准 DeepONet 及其他几何感知代理模型显著提升了预测精度与跨几何泛化性能；其中交叉注意力变体可减少对符号距离函数的依赖，实现高精度几何条件预测。

Conclusion: ArGEnT 提供了一种可扩展的、几何无关的算子学习框架，适用于复杂物理系统的优化、不确定性量化与数据驱动建模。

Abstract: Learning solution operators for systems with complex, varying geometries and parametric physical settings is a central challenge in scientific machine learning. In many-query regimes such as design optimization, control and inverse problems, surrogate modeling must generalize across geometries while allowing flexible evaluation at arbitrary spatial locations. In this work, we propose Arbitrary Geometry-encoded Transformer (ArGEnT), a geometry-aware attention-based architecture for operator learning on arbitrary domains. ArGEnT employs Transformer attention mechanisms to encode geometric information directly from point-cloud representations with three variants-self-attention, cross-attention, and hybrid-attention-that incorporates different strategies for incorporating geometric features. By integrating ArGEnT into DeepONet as the trunk network, we develop a surrogate modeling framework capable of learning operator mappings that depend on both geometric and non-geometric inputs without the need to explicitly parametrize geometry as a branch network input. Evaluation on benchmark problems spanning fluid dynamics, solid mechanics and electrochemical systems, we demonstrate significantly improved prediction accuracy and generalization performance compared with the standard DeepONet and other existing geometry-aware saurrogates. In particular, the cross-attention transformer variant enables accurate geometry-conditioned predictions with reduced reliance on signed distance functions. By combining flexible geometry encoding with operator-learning capabilities, ArGEnT provides a scalable surrogate modeling framework for optimization, uncertainty quantification, and data-driven modeling of complex physical systems.

</details>


### [297] [GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks](https://arxiv.org/abs/2602.11629)
*Dongxiao He,Wenxuan Sun,Yongqi Huang,Jitao Zhao,Di Jin*

Main category: cs.LG

TL;DR: 本文提出GP2F方法，通过双分支结构（冻结分支保留预训练知识，适配分支进行轻量任务适配）并结合拓扑约束下的自适应融合，在跨域图提示学习中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图提示学习（GPL）在跨域场景下仍有效，但其内在机理尚不明确；同时，GPL方法在跨域设置中表现接近全微调和线性探针，亟需深入理解其提示机制。

Method: 提出双分支GPL方法GP2F：一个冻结分支保留预训练知识，一个带轻量适配器的分支进行任务特定适配，并通过对比损失和拓扑一致性损失实现受拓扑约束的自适应融合。

Result: 在跨域少样本节点分类和图分类任务上，GP2F显著优于现有方法。

Conclusion: 跨域GPL的有效性源于预训练知识与任务适配能力的协同整合；GP2F通过显式建模该双重机制，提升了模型泛化性与适应性。

Abstract: Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.

</details>


### [298] [TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning](https://arxiv.org/abs/2602.11633)
*Jianhua Wang,Yinlin Su*

Main category: cs.LG

TL;DR: 本文提出了一种名为Targeted Interpretable Perturbation (TIP)的新型防御框架，通过结合模型可解释性与频域分析，有针对性地对卷积通道的高频分量添加扰动，以抵御梯度反演攻击（GIA），在保障隐私的同时显著提升模型效用和收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于差分隐私（DP）的防御方法对所有参数均匀加噪，严重损害模型效用和收敛稳定性；而梯度反演攻击（GIA）能从共享梯度中高保真重建私有训练数据，威胁联邦学习中的数据隐私。

Method: TIP采用双目标策略：首先利用Grad-CAM量化通道敏感性，动态识别编码主要语义特征的关键卷积通道；其次将这些通道核变换至频域（DFT），并仅对高频谱分量注入校准扰动。

Result: 在基准数据集上的实验表明，TIP使当前最优GIA重建图像完全不可识别，同时全局模型精度接近无隐私保护基线，显著优于现有DP类方法，在隐私-效用权衡与可解释性上表现更优。

Conclusion: TIP是一种兼顾隐私保护、模型效用与可解释性的高效FL防御新范式，为对抗梯度反演攻击提供了更精细、更鲁棒的解决方案。

Abstract: Federated Learning (FL) facilitates collaborative model training while preserving data locality; however, the exchange of gradients renders the system vulnerable to Gradient Inversion Attacks (GIAs), allowing adversaries to reconstruct private training data with high fidelity. Existing defenses, such as Differential Privacy (DP), typically employ indiscriminate noise injection across all parameters, which severely degrades model utility and convergence stability. To address those limitation, we proposes Targeted Interpretable Perturbation (TIP), a novel defense framework that integrates model interpretability with frequency domain analysis. Unlike conventional methods that treat parameters uniformly, TIP introduces a dual-targeting strategy. First, leveraging Gradient-weighted Class Activation Mapping (Grad-CAM) to quantify channel sensitivity, we dynamically identify critical convolution channels that encode primary semantic features. Second, we transform these selected kernels into the frequency domain via the Discrete Fourier Transform and selectively inject calibrated perturbations into the high-frequency spectrum. By selectively perturbing high-frequency components, TIP effectively destroys the fine-grained details necessary for image reconstruction while preserving the low-frequency information crucial for model accuracy. Extensive experiments on benchmark datasets demonstrate that TIP renders reconstructed images visually unrecognizable against state-of-the-art GIAs, while maintaining global model accuracy comparable to non-private baselines, significantly outperforming existing DP-based defenses in the privacy-utility trade-off and interpretability. Code is available in https://github.com/2766733506/asldkfjssdf_arxiv

</details>


### [299] [Both Topology and Text Matter: Revisiting LLM-guided Out-of-Distribution Detection on Text-attributed Graphs](https://arxiv.org/abs/2602.11641)
*Yinlin Zhu,Di Wu,Xu Wang,Guocong Quan,Miao Hu*

Main category: cs.LG

TL;DR: 本文提出LG-Plug方法，通过LLM引导的拓扑与文本表征对齐、聚类迭代提示生成共识驱动的OOD暴露，并结合轻量级码本与启发式采样降低LLM查询开销，从而提升文本属性图（TAG）中OOD节点检测性能，且可即插即用地集成到现有检测器中。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在文本属性图（TAG）上面对分布外（OOD）节点时表现不佳，早期拓扑驱动方法语义建模不足，而LLM方法存在OOD先验可靠性与信息性失衡、且难以融合成熟拓扑洞察的问题。

Method: LG-Plug：1）对齐拓扑与文本表征以生成细粒度节点嵌入；2）通过聚类迭代LLM提示生成共识驱动的OOD暴露；3）引入轻量级簇内码本与启发式采样降低LLM调用开销；4）将OOD暴露作为正则项增强ID/OOD分离能力。

Result: LG-Plug显著提升了TAG场景下的OOD检测性能，在多个基准数据集上超越现有SOTA方法，并具备良好的即插即用性和计算效率。

Conclusion: LG-Plug成功桥接了拓扑建模与大语言模型的语义能力，在保持架构兼容性的同时，有效缓解了OOD检测中的可靠性-信息性权衡问题，为TAG的鲁棒学习提供了新范式。

Abstract: Text-attributed graphs (TAGs) associate nodes with textual attributes and graph structure, enabling GNNs to jointly model semantic and structural information. While effective on in-distribution (ID) data, GNNs often encounter out-of-distribution (OOD) nodes with unseen textual or structural patterns in real-world settings, leading to overconfident and erroneous predictions in the absence of reliable OOD detection. Early approaches address this issue from a topology-driven perspective, leveraging neighboring structures to mitigate node-level detection bias. However, these methods typically encode node texts as shallow vector features, failing to fully exploit rich semantic information. In contrast, recent LLM-based approaches generate pseudo OOD priors by leveraging textual knowledge, but they suffer from several limitations: (1) a reliability-informativeness imbalance in the synthesized OOD priors, as the generated OOD exposures either deviate from the true OOD semantics, or introduce non-negligible ID noise, all of which offers limited improvement to detection performance; (2) reliance on specialized architectures, which prevents incorporation of the extensive effective topology-level insights that have been empirically validated in prior work. To this end, we propose LG-Plug, an LLM-Guided Plug-and-play strategy for TAG OOD detection tasks. LG-Plug aligns topology and text representations to produce fine-grained node embeddings, then generates consensus-driven OOD exposure via clustered iterative LLM prompting. Moreover, it leverages lightweight in-cluster codebook and heuristic sampling reduce time cost of LLM querying. The resulting OOD exposure serves as a regularization term to separate ID and OOD nodes, enabling seamless integration with existing detectors.

</details>


### [300] [UMAP Is Spectral Clustering on the Fuzzy Nearest-Neighbor Graph](https://arxiv.org/abs/2602.11662)
*Yang Yang*

Main category: cs.LG

TL;DR: 本文证明UMAP算法本质上是在模糊k近邻图上执行谱聚类，建立了UMAP与经典谱方法之间的严格理论联系。


<details>
  <summary>Details</summary>
Motivation: 尽管UMAP被广泛应用，但其与经典谱方法（如谱聚类）的精确关系仍缺乏形式化描述，本文旨在填补这一理论空白。

Method: 通过三步证明：(1) 将UMAP的随机优化与负采样建模为相似图上的对比学习目标；(2) 引用HaoChen等人的结果，指出该对比学习等价于谱 clustering；(3) 验证UMAP的谱初始化恰好求解该谱问题的线性解。

Result: 证明UMAP在高斯核下严格等价于谱聚类，在默认柯西型核下为一阶近似等价；统一了UMAP、对比学习和谱聚类的理论框架。

Conclusion: UMAP并非仅凭经验有效的黑箱方法，而是一种具有坚实谱理论基础的、隐式执行谱聚类的算法。

Abstract: UMAP (Uniform Manifold Approximation and Projection) is among the most widely used algorithms for non linear dimensionality reduction and data visualisation. Despite its popularity, and despite being presented through the lens of algebraic topology, the exact relationship between UMAP and classical spectral methods has remained informal. In this work, we prove that UMAP performs spectral clustering on the fuzzy k nearest neighbour graph. Our proof proceeds in three steps: (1) we show that UMAP's stochastic optimisation with negative sampling is a contrastive learning objective on the similarity graph; (2) we invoke the result of HaoChen et al. [8], establishing that contrastive learning on a similarity graph is equivalent to spectral clustering; and (3) we verify that UMAP's spectral initialisation computes the exact linear solution to this spectral problem. The equivalence is exact for Gaussian kernels, and holds as a first order approximation for UMAP's default Cauchy type kernel. Our result unifies UMAP, contrastive learning, and spectral clustering under a single framework, and provides theoretical grounding for several empirical observations about UMAP's behaviour.

</details>


### [301] [Fully First-Order Algorithms for Online Bilevel Optimization](https://arxiv.org/abs/2602.11665)
*Tingkai Jia,Cheng Chen*

Main category: cs.LG

TL;DR: 本文提出了一种无需Hessian-向量积（HVP）的全一阶在线双层优化（OBO）算法，通过拉格朗日重构消除了隐式微分需求，并给出两种变体：基础版达到O(1 + V_T + H_{2,T})遗憾界，改进版采用自适应内层迭代，实现O(√T + V_T)遗憾界，在外层变化较大时更优。


<details>
  <summary>Details</summary>
Motivation: 现有OBO算法依赖超梯度下降，需调用Hessian-向量积（HVP）oracle，计算开销高；亟需一种不依赖HVP、更高效的全一阶方法。

Method: 将非凸-强凸OBO问题重写为带不等式约束的单层在线问题，构造序列拉格朗日函数，避免隐式微分；设计全一阶算法及其自适应内层迭代改进版本。

Result: 基础算法达到O(1 + V_T + H_{2,T})遗憾界；改进算法实现O(√T + V_T)遗憾界，当V_T ≥ O(√T)时优于前者。

Conclusion: 本文成功摆脱HVP依赖，显著降低OBO计算复杂度，理论遗憾界具有竞争力，尤其适用于外层目标函数变化剧烈的场景。

Abstract: In this work, we study non-convex-strongly-convex online bilevel optimization (OBO). Existing OBO algorithms are mainly based on hypergradient descent, which requires access to a Hessian-vector product (HVP) oracle and potentially incurs high computational costs. By reformulating the original OBO problem as a single-level online problem with inequality constraints and constructing a sequence of Lagrangian function, we eliminate the need for HVPs arising from implicit differentiation. Specifically, we propose a fully first-order algorithm for OBO, and provide theoretical guarantees showing that it achieves regret of $O(1 + V_T + H_{2,T})$. Furthermore, we develop an improved variant with an adaptive inner-iteration scheme, which removes the dependence on the drift variation of the inner-level optimal solution and achieves regret of $O(\sqrt{T} + V_T)$. This regret have the advatange when $V_{T}\ge O(\sqrt{T})$.

</details>


### [302] [Explainable Machine-Learning based Detection of Knee Injuries in Runners](https://arxiv.org/abs/2602.11668)
*David Fuentes-Jiménez,Sara García-de-Villa,David Casillas-Pérez,Pablo Floría,Francisco-Manuel Melgarejo-Meseguer*

Main category: cs.LG

TL;DR: 本研究利用光学运动捕捉系统分析跑步者的步态数据，结合传统机器学习与深度学习模型（如CNN、LSTM）对健康、PFPS和ITBS三类人群进行分类，发现融合时序与离散特征的CNN模型性能最优，准确率分别达77.9%（PFPS）、73.8%（ITBS）和71.43%（总体受伤），并借助可解释性工具揭示关键生物力学特征。


<details>
  <summary>Details</summary>
Motivation: 跑步导致膝关节损伤（如PFPS和ITBS）高发，亟需通过精准步态分析辅助临床决策。

Method: 基于839例跑者（健康/受伤）的 treadmill 光学运动捕捉数据，提取支撑相的关节/节段角度时间序列与离散点值；构建多种特征空间（点值、时序、混合），对比KNN、高斯过程、决策树、CNN、LSTM等模型；采用准确率、精确率、召回率、F1及Shapley值、显著图、Grad-CAM进行评估与解释。

Result: CNN表现最佳：PFPS分类准确率77.9%，ITBS为73.8%，总体受伤分类为71.43%；融合时间序列与点值特征显著提升性能；可解释性分析识别出关键生物力学判别区域。

Conclusion: 光学运动捕捉联合深度学习（尤其CNN）能有效识别膝伤相关跑步模式，为无创筛查与个性化干预提供新路径。

Abstract: Running is a widely practiced activity but shows a high incidence of knee injuries, especially Patellofemoral Pain Syndrome (PFPS) and Iliotibial Band Syndrome (ITBS). Identifying gait patterns linked to these injuries can improve clinical decision-making, which requires precise systems capable of capturing and analyzing temporal kinematic data.
  This study uses optical motion capture systems to enhance detection of injury-related running patterns. We analyze a public dataset of 839 treadmill recordings from healthy and injured runners to evaluate how effectively these systems capture dynamic parameters relevant to injury classification. The focus is on the stance phase, using joint and segment angle time series and discrete point values.
  Three classification tasks are addressed: healthy vs. injured, healthy vs. PFPS, and healthy vs. ITBS. We examine different feature spaces, from traditional point-based metrics to full stance-phase time series and hybrid representations. Multiple models are tested, including classical algorithms (K-Nearest Neighbors, Gaussian Processes, Decision Trees) and deep learning architectures (CNNs, LSTMs).
  Performance is evaluated with accuracy, precision, recall, and F1-score. Explainability tools such as Shapley values, saliency maps, and Grad-CAM are used to interpret model behavior. Results show that combining time series with point values substantially improves detection. Deep learning models outperform classical ones, with CNNs achieving the highest accuracy: 77.9% for PFPS, 73.8% for ITBS, and 71.43% for the combined injury class.
  These findings highlight the potential of motion capture systems coupled with advanced machine learning to identify knee injury-related running patterns.

</details>


### [303] [DRACO: a Cross-Domain Benchmark for Deep Research Accuracy, Completeness, and Objectivity](https://arxiv.org/abs/2602.11685)
*Joey Zhong,Hao Zhang,Clare Southern,Jeremy Yang,Thomas Wang,Kate Jung,Shu Zhang,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 本文介绍了DRACO基准，用于评估复杂深度研究任务，涵盖10个领域、40个国家的信息源，强调准确性、完整性与客观性，并提供公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以全面评估复杂深度研究任务的准确性、完整性与客观性，需构建更贴近真实场景的评估标准。

Method: 基于匿名化的真实深度研究请求数据（Perplexity Deep Research）采样、筛选与增强，构建覆盖多领域、多国家的DRACO基准；设计四维评分标准（事实准确性、分析广度与深度、呈现质量、引用质量）对模型输出进行评估。

Result: 发布了一个公开可用的DRACO基准数据集（https://hf.co/datasets/perplexity-ai/draco），支持对深度研究能力的系统性、多维度评估。

Conclusion: DRACO为评估大模型在真实复杂研究任务中的表现提供了标准化、可复现且具代表性的新基准，推动深度研究能力的评测与提升。

Abstract: We present DRACO (Deep Research Accuracy, Completeness, and Objectivity), a benchmark of complex deep research tasks. These tasks, which span 10 domains and draw on information sources from 40 countries, originate from anonymized real-world usage patterns within a large-scale deep research system. Tasks are sampled from a de-identified dataset of Perplexity Deep Research requests, then filtered and augmented to ensure that the tasks are anonymized, open-ended and complex, objectively evaluable, and representative of the broad scope of real-world deep research use cases. Outputs are graded against task-specific rubrics along four dimensions: factual accuracy (accuracy), breadth and depth of analysis (including completeness), presentation quality (including objectivity), and citation quality. DRACO is publicly available at https://hf.co/datasets/perplexity-ai/draco.

</details>


### [304] [ANML: Attribution-Native Machine Learning with Guaranteed Robustness](https://arxiv.org/abs/2602.11690)
*Oliver Zahn,Matt Beton,Simran Chana*

Main category: cs.LG

TL;DR: 本文提出ANML（Attribution-Native Machine Learning）框架，通过结合梯度信号与数据来源的外部信息（如一致性、验证状态、贡献者声誉和时间相关性），为训练样本动态赋权，显著提升模型性能与可归因性。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练流水线对所有样本一视同仁，忽视了数据来源质量的巨大差异（如诺奖成果 vs 未经验证提交），导致性能瓶颈与归因困难。

Method: 提出ANML框架，基于四个质量因子（梯度一致性q、验证状态v、贡献者声誉r、时间相关性T）计算每个贡献者的质量权重；引入两阶段自适应门控机制保障性能下界，并支持对抗性攻击下的鲁棒性。

Result: 在5个数据集上错误率降低33%-72%；仅用20%高质量数据即超越100%均匀加权数据47%； contributor-level归因在难检测污染下比sample-level方法提升1.3-5.3倍。

Conclusion: 质量加权训练不仅提升数据效率与模型性能，还天然支持可信归因，是面向高质量、可信赖AI训练的关键范式。

Abstract: Frontier AI systems increasingly train on specialized expert data, from clinical records to proprietary research to curated datasets, yet current training pipelines treat all samples identically. A Nobel laureate's contribution receives the same weight as an unverified submission. We introduce ANML (Attribution-Native Machine Learning), a framework that weights training samples by four quality factors: gradient-based consistency (q), verification status (v), contributor reputation (r), and temporal relevance (T). By combining what the model observes (gradient signals) with what the system knows about data provenance (external signals), ANML produces per-contributor quality weights that simultaneously improve model performance and enable downstream attribution. Across 5 datasets (178-32,561 samples), ANML achieves 33-72% error reduction over gradient-only baselines. Quality-weighted training is data-efficient: 20% high-quality data outperforms 100% uniformly weighted data by 47%. A Two-Stage Adaptive gating mechanism guarantees that ANML never underperforms the best available baseline, including under strategic joint attacks combining credential faking with gradient alignment. When per-sample detection fails against subtle corruption, contributor-level attribution provides 1.3-5.3x greater improvement than sample-level methods, with the advantage growing as corruption becomes harder to detect.

</details>


### [305] [SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion](https://arxiv.org/abs/2602.11698)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,You Wu,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出了SpiralFormer，一种采用多分辨率递归调度的循环Transformer架构，通过在不同尺度上进行迭代式计算，提升参数和计算效率，并在160M到1.4B参数规模上均优于现有循环与非循环基线模型。


<details>
  <summary>Details</summary>
Motivation: 早期循环Transformer虽具迭代推理潜力，但常因固定全token分辨率导致效率低下；现有方法忽视了在压缩潜在表示上计算的效率优势。

Method: 提出SpiralFormer，引入多分辨率递归调度机制，在循环过程中动态调整序列分辨率，并通过探针实验验证其诱导出跨尺度的迭代功能特化。

Result: SpiralFormer在160M至1.4B参数范围内，参数与计算效率均超越循环及非循环基线模型，证实序列分辨率可作为递归架构扩展的新维度。

Conclusion: 多分辨率递归是一种有效提升循环Transformer效率的架构设计原则，为递归模型的可扩展性提供了新思路。

Abstract: Recursive (looped) Transformers decouple computational depth from parameter depth by repeatedly applying shared layers, providing an explicit architectural primitive for iterative refinement and latent reasoning. However, early looped Transformers often underperform non-recursive baselines of equal compute. While recent literature has introduced more effective recursion mechanisms to mitigate this gap, existing architectures still operate at a fixed, full-token resolution, neglecting the potential efficiency of computing over compressed latent representations. In this paper, we propose SpiralFormer, a looped Transformer that executes recurrence under a multi-resolution recursion schedule. We provide probing evidence that multi-resolution recursion enables the model to learn hierarchical dependencies by inducing iteration-wise functional specialization across different scales. Empirically, SpiralFormer achieves better parameter and compute efficiency than both looped and non-looped baselines across model scales from 160M to 1.4B, establishing sequence resolution as a potential axis for scaling recursive architectures.

</details>


### [306] [TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction](https://arxiv.org/abs/2602.11700)
*Yongyao Wang,Ziqi Miao,Lu Yang,Haonan Jia,Wenting Yan,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 本文提出TabSieve框架，通过先选择相关证据行再预测，提升表格预测的准确性和鲁棒性；结合合成数据TabSieve-SFT-40K与新强化学习算法TAB-GRPO，在分类和回归任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格预测模型多为实例级推理，LLM提示易受噪声干扰，难以稳定利用相关行作为少样本证据。

Method: 提出select-then-predict框架TabSieve：先筛选少量信息性行作为证据，再基于该证据预测目标；构建合成数据集TabSieve-SFT-40K；设计TAB-GRPO强化学习方法，联合优化证据选择与预测，并通过动态任务优势平衡稳定混合回归/分类训练。

Result: 在75个分类和52个回归表格构成的基准上，TabSieve平均提升2.92%（分类）和4.45%（回归），且更聚焦于所选证据，对噪声上下文更具鲁棒性。

Conclusion: 显式、可审计的证据选择机制能有效提升表格少样本预测性能与鲁棒性，TabSieve为结构化数据推理提供了新范式。

Abstract: Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challenge, we propose TabSieve, a select-then-predict framework that makes evidence usage explicit and auditable. Given a table and a query row, TabSieve first selects a small set of informative rows as evidence and then predicts the missing target conditioned on the selected evidence. To enable this capability, we construct TabSieve-SFT-40K by synthesizing high-quality reasoning trajectories from 331 real tables using a strong teacher model with strict filtering. Furthermore, we introduce TAB-GRPO, a reinforcement learning recipe that jointly optimizes evidence selection and prediction correctness with separate rewards, and stabilizes mixed regression and classification training via dynamic task-advantage balancing. Experiments on a held-out benchmark of 75 classification and 52 regression tables show that TabSieve consistently improves performance across shot budgets, with average gains of 2.92% on classification and 4.45% on regression over the second-best baseline. Further analysis indicates that TabSieve concentrates more attention on the selected evidence, which improves robustness to noisy context.

</details>


### [307] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 本文提出了一种基于势能的门控机制（potential-energy gating），用于双稳态随机动力学系统的鲁棒状态估计，通过势能函数调节观测噪声协方差，在势能极小值附近信任观测、在势垒附近降低观测权重；该方法在多种卡尔曼滤波器与粒子滤波器中实现，显著提升估计精度，并在合成数据与古气候实证中验证其有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯滤波器对状态空间各区域一视同仁，缺乏对物理势能结构的利用；硬约束滤波又过于刚性；而双稳态系统（如气候突变、分子跃迁）中，观测异常常发生在势垒穿越过程，需一种物理引导的自适应观测信任机制。

Method: 将已知或假设的势能函数（如Ginzburg-Landau势）嵌入滤波框架，使观测噪声协方差随局部势能值动态缩放：势能越低（越靠近稳定态），观测权重越高；越接近势垒，权重越低；该门控可无缝集成到EKF、UKF、EnKF、AKF及粒子滤波中，仅引入两个超参数。

Result: 在含10%异常值的双稳Ginzburg-Landau合成数据上，相比标准EKF，RMSE降低57–80%（p < 10^{-15}）；势能连续建模比仅用离散井距离的基线多贡献约21个百分点；即使势能参数误设50%，性能仍提升≥47%；在噪声诱导跃迁下保持68%改进，而基线降至30%；应用于NGRIP冰芯δ¹⁸O数据，估计突变不对称参数γ = -0.109（95% CI不包含0），且异常比例解释91%的滤波增益方差。

Conclusion: 势能门控是一种物理驱动、轻量可扩展、统计稳健的状态估计算法，显著提升双稳系统在存在观测异常时的滤波性能，兼具理论可解释性与实证有效性。

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [308] [DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels](https://arxiv.org/abs/2602.11715)
*Haolei Bai,Lingcheng Kong,Xueyi Chen,Jianmian Wang,Zhiqiang Tao,Huan Wang*

Main category: cs.LG

TL;DR: 本文提出DICE系列扩散大语言模型，专用于CUDA核生成，通过构建高质量数据集CuKe和双阶段强化学习框架BiC-RL，在KernelBench上显著超越同类自回归与扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）在并行生成和结构化代码生成方面具有优势，但缺乏高质量训练数据和针对CUDA核生成的专门优化，限制了其应用。

Method: 构建面向高性能CUDA核的增强监督微调数据集CuKe；提出双阶段精选强化学习框架BiC-RL（含核填充与端到端生成两阶段）；基于此训练1.7B/4B/8B三规模的DICE扩散模型。

Result: 在KernelBench基准上，DICE全面超越同规模自回归与扩散模型，成为CUDA核生成新SOTA。

Conclusion: DICE验证了扩散架构在高度专业化系统编程任务（如CUDA核生成）中的有效性，为dLLMs在硬软件协同设计领域的落地提供了可行路径与重要范例。

Abstract: Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.

</details>


### [309] [Dopamine: Brain Modes, Not Brains](https://arxiv.org/abs/2602.11726)
*Shervin Ghasemlou*

Main category: cs.LG

TL;DR: 本文提出了一种基于激活空间的参数高效微调方法（Neuromodulated PEFT），通过学习每个神经元的阈值和增益来实现模式选择与计算重标度，而非修改权重；该方法在旋转MNIST任务上验证了其有效性、稀疏性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）通过权重增量更新模型，但难以从机制上解释哪些内部计算被复用或绕过；本文受神经调控启发，提出‘模式切换’的新视角——即通过改变激活行为而非重写权重来适应新任务。

Method: 提出Neuromodulated PEFT方法：冻结预训练模型权重，为每个神经元学习可训练的阈值和增益；引入平滑门控机制决定神经元激活是否参与前向传播，推理时可硬化门控以实现显式的条件计算与神经元级归因。

Result: 在旋转MNIST（45°）任务上，该方法在仅数百个可训练参数/层条件下显著提升准确率；展现出部分激活稀疏性（少数神经元强激活）；相比LoRA，精度略有下降但参数量大幅减少，且提供‘哪些神经元被激活’的直观解释。

Conclusion: Neuromodulated PEFT提供了一种更可解释、轻量化的微调范式，强调激活层面的模式选择；其性能受限于冻结基础模型是否已具备目标任务所需特征。

Abstract: Parameter-efficient fine-tuning (PEFT) methods such as \lora{} adapt large pretrained models by adding small weight-space updates. While effective, weight deltas are hard to interpret mechanistically, and they do not directly expose \emph{which} internal computations are reused versus bypassed for a new task. We explore an alternative view inspired by neuromodulation: adaptation as a change in \emph{mode} -- selecting and rescaling existing computations -- rather than rewriting the underlying weights. We propose \methodname{}, a simple activation-space PEFT technique that freezes base weights and learns per-neuron \emph{thresholds} and \emph{gains}. During training, a smooth gate decides whether a neuron's activation participates; at inference the gate can be hardened to yield explicit conditional computation and neuron-level attributions.
  As a proof of concept, we study ``mode specialization'' on MNIST (0$^\circ$) versus rotated MNIST (45$^\circ$). We pretrain a small MLP on a 50/50 mixture (foundation), freeze its weights, and then specialize to the rotated mode using \methodname{}. Across seeds, \methodname{} improves rotated accuracy over the frozen baseline while using only a few hundred trainable parameters per layer, and exhibits partial activation sparsity (a minority of units strongly active). Compared to \lora{}, \methodname{} trades some accuracy for substantially fewer trainable parameters and a more interpretable ``which-neurons-fire'' mechanism. We discuss limitations, including reduced expressivity when the frozen base lacks features needed for the target mode.

</details>


### [310] [U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series](https://arxiv.org/abs/2602.11738)
*Ilya Kuleshov,Alexander Marusov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: UFO是一种新型架构，结合了U-Net、Transformer和Neural CDE的优点，实现了并行化、全局建模与连续时间动态建模，显著提升不规则采样时间序列的概率预测精度与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有Neural CDE方法虽能建模连续动力学，但计算慢、不可并行，难以扩展且缺乏全局上下文感知能力，尤其在医疗和金融等不规则时间序列场景中受限。

Method: 提出UFO（U-Former ODE）架构：融合U-Net的并行多尺度特征提取、Transformer的全局建模能力与Neural CDE的连续时间动态建模，并设计全因果、可并行结构以兼顾全局感受野与局部时序敏感性。

Result: 在五个标准基准（含规则与不规则采样）上，UFO持续超越10种SOTA神经基线；推理速度达传统Neural CDE的15倍；在长序列与高维多元序列上表现稳健。

Conclusion: UFO成功解决了Neural CDE的并行性与全局建模瓶颈，在不规则时间序列概率预测任务中实现了精度与效率的双重突破，为连续深度时间序列建模提供了新范式。

Abstract: Probabilistic forecasting of irregularly sampled time series is crucial in domains such as healthcare and finance, yet it remains a formidable challenge. Existing Neural Controlled Differential Equation (Neural CDE) approaches, while effective at modelling continuous dynamics, suffer from slow, inherently sequential computation, which restricts scalability and limits access to global context. We introduce UFO (U-Former ODE), a novel architecture that seamlessly integrates the parallelizable, multiscale feature extraction of U-Nets, the powerful global modelling of Transformers, and the continuous-time dynamics of Neural CDEs. By constructing a fully causal, parallelizable model, UFO achieves a global receptive field while retaining strong sensitivity to local temporal dynamics. Extensive experiments on five standard benchmarks -- covering both regularly and irregularly sampled time series -- demonstrate that UFO consistently outperforms ten state-of-the-art neural baselines in predictive accuracy. Moreover, UFO delivers up to 15$\times$ faster inference compared to conventional Neural CDEs, with consistently strong performance on long and highly multivariate sequences.

</details>


### [311] [TUBO: A Tailored ML Framework for Reliable Network Traffic Forecasting](https://arxiv.org/abs/2602.11759)
*Zhihang Yuan,Leyang Xue,Waleed Ahsan,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 本文提出TUBO框架，专为可靠网络流量预测设计，通过突发处理和模型选择提升预测精度与不确定性量化能力，在多个真实数据集上显著优于现有方法，并在主动流量工程中大幅提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难以有效应对网络流量的突发性和复杂模式，导致预测可靠性不足。

Method: 提出TUBO框架，包含突发处理模块和基于多模型池的自适应模型选择机制，并支持确定性预测与不确定性量化。

Result: 在Abilene、GEANT和CERNET数据集上，预测精度提升4倍，突发事件预测准确率达94%；在主动流量工程中，相比反应式和现有最优预测驱动的方法，聚合吞吐量分别提升9倍和3倍。

Conclusion: TUBO通过结构化建模网络流量特性，显著提升了预测可靠性与实用性，为网络运维优化提供了更可信的预测基础。

Abstract: Traffic forecasting based network operation optimization and management offers enormous promise but also presents significant challenges from traffic forecasting perspective. While deep learning models have proven to be relatively more effective than traditional statistical methods for time series forecasting, their reliability is not satisfactory due to their inability to effectively handle unique characteristics of network traffic. In particular, the burst and complex traffic patterns makes the existing models less reliable, as each type of deep learning model has limited capability in capturing traffic patterns. To address this issue, we introduce TUBO, a novel machine learning framework custom designed for reliable network traffic forecasting. TUBO features two key components: burst processing for handling significant traffic fluctuations and model selection for adapting to varying traffic patterns using a pool of models. A standout feature of TUBO is its ability to provide deterministic predictions along with quantified uncertainty, which serves as a cue for identifying the most reliable forecasts. Evaluations on three real-world network demand matrix (DM) datasets (Abilene, GEANT, and CERNET) show that TUBO significantly outperforms existing methods on forecasting accuracy (by 4 times), and also achieves up to 94% accuracy in burst occurrence forecasting. Furthermore, we also consider traffic demand forecasting based proactive traffic engineering (TE) as a downstream use case. Our results show that compared to reactive approaches and proactive TE using the best existing DM forecasting methods, proactive TE powered by TUBO improves aggregated throughput by 9 times and 3 times, respectively.

</details>


### [312] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: 本文提出MUSE框架，通过解耦模型分数与客户端决策边界，解决多租户Score-as-a-Service环境中因模型更新导致分数分布变化而需频繁重校准阈值的问题；采用动态意图路由与两级分数变换，实现模型共享与分数稳定映射，显著缩短模型上线时间，提升反欺诈能力并降低成本。


<details>
  <summary>Details</summary>
Motivation: 在多租户Score-as-a-Service场景中，模型重训练会改变分数分布，导致客户侧已部署的决策阈值失效；跨数百客户协调阈值更新成本高、耗时长，造成模型停滞。

Method: 提出MUSE模型服务框架：1）动态意图驱动的路由实现多租户模型共享；2）两级分数变换将模型输出映射至稳定的参考分布，使客户决策边界无需随模型更新而调整。

Result: 在Feedzai大规模部署，每秒处理超千事件、年处理超550亿事件，覆盖数十租户；模型上线时间从数周缩短至分钟级，显著提升对抗攻击的模型韧性，并节省数百万美元欺诈与运维成本。

Conclusion: MUSE通过解耦分数与决策逻辑、引入稳定分数表示，有效解决了多租户环境下模型迭代与阈值维护的耦合瓶颈，为Score-as-a-Service提供了可扩展、低延迟、高可用的工业级解决方案。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [313] [Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning](https://arxiv.org/abs/2602.11779)
*Haoran Dang,Cuiling Lan,Hai Wan,Xibin Zhao,Yan Lu*

Main category: cs.LG

TL;DR: 本文提出TAMPO框架，将温度参数建模为可学习的元策略，在强化学习中实现动态自适应温度控制，提升大语言模型在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 静态或启发式温度调度无法适应强化学习训练过程中的动态需求，限制了策略优化效果。

Method: 提出Temperature Adaptive Meta Policy Optimization（TAMPO），采用双循环结构：内循环用当前温度采样轨迹并更新LLM策略（如GRPO）；外循环通过高优势轨迹的似然奖励来更新温度分布，实现无需额外采样的在线自适应。

Result: 在五个数学推理基准上，TAMPO显著优于固定或启发式温度方法。

Conclusion: 温度可作为有效的可学习元策略，用于大语言模型强化学习中的自适应探索。

Abstract: Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Yet static or heuristic temperature schedules fail to adapt to the dynamic demands of reinforcement learning (RL) throughout training, often limiting policy improvement. We propose Temperature Adaptive Meta Policy Optimization (TAMPO), a new framework that recasts temperature control as a learnable meta-policy. TAMPO operates through a hierarchical two-loop process. In the inner loop, the LLM policy is updated (e.g., using GRPO) with trajectories sampled at the temperature selected by the meta-policy. In the outer loop, meta-policy updates the distribution over candidate temperatures by rewarding those that maximize the likelihood of high-advantage trajectories. This trajectory-guided, reward-driven mechanism enables online adaptation without additional rollouts, directly aligning exploration with policy improvement. On five mathematical reasoning benchmarks, TAMPO outperforms baselines using fixed or heuristic temperatures, establishing temperature as an effective learnable meta-policy for adaptive exploration in LLM reinforcement learning. Accepted at ICLR 2026.

</details>


### [314] [Safe Fairness Guarantees Without Demographics in Classification: Spectral Uncertainty Set Perspective](https://arxiv.org/abs/2602.11785)
*Ainhize Barrainkua,Santiago Mazuelas,Novi Quadrianto,Jose A. Lozano*

Main category: cs.LG

TL;DR: 本文提出SPECTRE方法，在无群体信息前提下提升分类器公平性，通过调整傅里叶特征映射频谱并约束最坏分布偏离经验分布的程度，实验证明其在公平性保障和稳定性上优于现有方法，甚至媲美使用真实群体信息的方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法大多需要全部样本的群体（如种族、性别）信息，但现实中难以获取；而无需群体信息的鲁棒优化方法又常因不确定性集选择不当（过度关注异常值或过于悲观）导致性能与公平性下降。

Method: 提出SPECTRE：一种极小极大公平方法，通过调节简单傅里叶特征映射的频谱，并对最坏情况分布相对于经验分布的偏差施加约束，实现无群体信息下的鲁棒公平学习。

Result: 在涵盖20个州的美国社区调查（ACS）数据集上广泛实验表明，SPECTRE在公平性保证的平均值最高、四分位距最小，表现优于当前最优方法（包括部分可获群体信息的方法）；理论分析给出了个体群体及总体最坏误差的可计算上界，并刻画了导致极端误差的最坏分布形式。

Conclusion: SPECTRE有效缓解了无群体信息下公平学习的鲁棒性与性能权衡难题，兼具实证优越性与理论可解释性，为隐私敏感与数据受限场景下的公平AI提供了新路径。

Abstract: As automated classification systems become increasingly prevalent, concerns have emerged over their potential to reinforce and amplify existing societal biases. In the light of this issue, many methods have been proposed to enhance the fairness guarantees of classifiers. Most of the existing interventions assume access to group information for all instances, a requirement rarely met in practice. Fairness without access to demographic information has often been approached through robust optimization techniques,which target worst-case outcomes over a set of plausible distributions known as the uncertainty set. However, their effectiveness is strongly influenced by the chosen uncertainty set. In fact, existing approaches often overemphasize outliers or overly pessimistic scenarios, compromising both overall performance and fairness. To overcome these limitations, we introduce SPECTRE, a minimax-fair method that adjusts the spectrum of a simple Fourier feature mapping and constrains the extent to which the worst-case distribution can deviate from the empirical distribution. We perform extensive experiments on the American Community Survey datasets involving 20 states. The safeness of SPECTRE comes as it provides the highest average values on fairness guarantees together with the smallest interquartile range in comparison to state-of-the-art approaches, even compared to those with access to demographic group information. In addition, we provide a theoretical analysis that derives computable bounds on the worst-case error for both individual groups and the overall population, as well as characterizes the worst-case distributions responsible for these extremal performances

</details>


### [315] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: 本文提出了一种深度导向的评估框架APST，用于检测大语言模型在重复推理下的操作性安全失败（如幻觉、拒绝不一致、不安全输出），通过伯努利/二项模型量化单次推理失败概率，揭示了传统单样本评测所掩盖的模型可靠性差异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全评测侧重任务广度，但实际部署中更关键的是在重复使用相同或相似提示时的响应一致性与安全性（即操作可靠性）；现有评测无法反映高风险场景下的持续推理失效问题。

Method: 提出加速提示压力测试（APST）框架，受可靠性工程启发：在可控解码参数（如temperature）下对同一提示进行多次采样，将安全失败建模为独立伯努利试验，用伯努利/二项分布估计单次推理失败概率，并在AIR-BENCH安全提示集上对比多个指令微调LLM的实证失败率。

Result: 发现基准分数相近的模型在重复采样下表现出显著不同的经验失败率，尤其随temperature升高差异加剧；单次样本评测会掩盖这些关键可靠性差异。

Conclusion: APST弥补了传统评测的不足，为LLM在真实部署场景中的安全与可靠性评估提供了可量化的深度评测新范式，连接了基准对齐与运维风险评估。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [316] [Latent-Variable Learning of SPDEs via Wiener Chaos](https://arxiv.org/abs/2602.11794)
*Sebastian Zeng,Andreas Petersson,Wolfgang Bock*

Main category: cs.LG

TL;DR: 本文提出一种结构化潜变量方法，仅需观测解的实现即可学习带加性高斯噪声驱动的线性随机偏微分方程（SPDE）的动力学规律，通过谱Galerkin投影与截断Wiener混沌展开将SPDE降维为含参常微分方程系统，并用变分学习联合推断潜动态与随机强迫结构。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法大多需已知驱动噪声或初值，或依赖无法刻画内在随机性的确定性代理模型，难以从纯观测中学习SPDE的真实随机动力学。

Method: 结合谱Galerkin投影与截断Wiener混沌展开，构建结构化潜变量模型，将无限维SPDE约简为有限维含参常微分方程系统；采用变分推断联合学习潜时间动态与随机强迫项。

Result: 在合成数据上验证了该方法在有界与无界一维空间域下均达到同等建模假设下的最先进性能。

Conclusion: 所提方法无需显式观测或模拟噪声即可有效恢复SPDE的随机结构，为从稀疏、纯观测数据中学习随机动力系统提供了新范式。

Abstract: We study the problem of learning the law of linear stochastic partial differential equations (SPDEs) with additive Gaussian forcing from spatiotemporal observations. Most existing deep learning approaches either assume access to the driving noise or initial condition, or rely on deterministic surrogate models that fail to capture intrinsic stochasticity. We propose a structured latent-variable formulation that requires only observations of solution realizations and learns the underlying randomly forced dynamics. Our approach combines a spectral Galerkin projection with a truncated Wiener chaos expansion, yielding a principled separation between deterministic evolution and stochastic forcing. This reduces the infinite-dimensional SPDE to a finite system of parametrized ordinary differential equations governing latent temporal dynamics. The latent dynamics and stochastic forcing are jointly inferred through variational learning, allowing recovery of stochastic structure without explicit observation or simulation of noise during training. Empirical evaluation on synthetic data demonstrates state-of-the-art performance under comparable modeling assumptions across bounded and unbounded one-dimensional spatial domains.

</details>


### [317] [Temporal Difference Learning with Constrained Initial Representations](https://arxiv.org/abs/2602.11800)
*Jiafei Lyu,Jingwen Yang,Zhongjian Qiao,Runze Liu,Zeyuan Liu,Deheng Ye,Zongqing Lu,Xiu Li*

Main category: cs.LG

TL;DR: 本文提出CIR框架，通过在初始层引入Tanh函数约束输入表征，结合归一化、跳跃连接和凸Q学习，提升离线策略强化学习的样本效率与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了对输入数据初始表征的直接约束，而该约束可缓解分布偏移、稳定训练。

Method: 引入Tanh函数于初始层；结合归一化、跳跃连接模块和凸Q学习，构成CIR框架。

Result: CIR在多个连续控制任务上表现优异，性能媲美甚至超越现有强基线方法。

Conclusion: 约束初始表征是一种有效且被低估的提升离线策略RL样本效率与稳定性的途径，CIR为此提供了理论支持与实用框架。

Abstract: Recently, there have been numerous attempts to enhance the sample efficiency of off-policy reinforcement learning (RL) agents when interacting with the environment, including architecture improvements and new algorithms. Despite these advances, they overlook the potential of directly constraining the initial representations of the input data, which can intuitively alleviate the distribution shift issue and stabilize training. In this paper, we introduce the Tanh function into the initial layer to fulfill such a constraint. We theoretically unpack the convergence property of the temporal difference learning with the Tanh function under linear function approximation. Motivated by theoretical insights, we present our Constrained Initial Representations framework, tagged CIR, which is made up of three components: (i) the Tanh activation along with normalization methods to stabilize representations; (ii) the skip connection module to provide a linear pathway from the shallow layer to the deep layer; (iii) the convex Q-learning that allows a more flexible value estimate and mitigates potential conservatism. Empirical results show that CIR exhibits strong performance on numerous continuous control tasks, even being competitive or surpassing existing strong baseline methods.

</details>


### [318] [SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG](https://arxiv.org/abs/2602.11801)
*Elham Rostami,Aref Einizade,Taous-Meriem Laleg-Kirati*

Main category: cs.LG

TL;DR: 本文提出了一种名为SpaTeoGL的时空图学习框架，用于可解释的癫痫发作网络分析，以提高iEEG中癫痫发作起始区（SOZ）定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确识别癫痫发作起始区（SOZ）对癫痫手术至关重要，但复杂多变的时空发作动力学给定位带来挑战。

Method: SpaTeoGL联合学习窗口级空间图（刻画电极间相互作用）和时间图（基于各窗口空间结构相似性连接时间窗口），在平滑图信号处理框架下建模，并通过具有收敛保证的交替块坐标下降算法求解。

Result: 在多中心、术后效果良好的iEEG数据集上实验表明，SpaTeoGL性能媲美基于水平可见性图与逻辑回归的基线方法，在非SOZ识别方面表现更优，并能提供关于发作起始与传播动态的可解释性洞察。

Conclusion: SpaTeoGL是一种有效且可解释的时空图学习方法，有助于提升SOZ定位精度并深化对癫痫发作机制的理解。

Abstract: Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.

</details>


### [319] [TopoFair: Linking Topological Bias to Fairness in Link Prediction Benchmarks](https://arxiv.org/abs/2602.11802)
*Lilian Marey,Mathilde Perez,Tiphaine Viard,Charlotte Laclau*

Main category: cs.LG

TL;DR: 本文提出了一种面向图链接预测（LP）任务的新型公平性基准框架，聚焦于图的结构偏差（而不仅限于同质性），通过构建可调控结构偏差的图生成方法，系统评估了多种LP模型在不同拓扑偏差下的公平性表现，揭示了公平性干预对非同质性结构偏差的高度敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有公平链接预测方法多关注图结构操作以缓解预测差异，但对社会图固有的拓扑偏差（远不止同质性）缺乏深入理解，导致公平干预泛化能力差、适用性受限。

Method: 提出一个以图结构偏差为核心的公平LP基准框架：1）梳理并形式化涵盖多类拓扑偏差的公平性度量体系；2）设计一种灵活图生成方法，兼顾真实图模式保真度与结构偏差可控调节；3）在多个用例上系统评测经典及公平感知LP模型。

Result: 实证分析揭示了预测公平性与各类结构偏差（尤其是超越同质性的偏差）之间的精细交互关系，表明当前公平干预方法对非同质性偏差高度敏感。

Conclusion: 公平性评估必须建立在对图结构偏差的深入理解之上，亟需发展结构驱动的公平学习评估范式。

Abstract: Graph link prediction (LP) plays a critical role in socially impactful applications, such as job recommendation and friendship formation. Ensuring fairness in this task is thus essential. While many fairness-aware methods manipulate graph structures to mitigate prediction disparities, the topological biases inherent to social graph structures remain poorly understood and are often reduced to homophily alone. This undermines the generalization potential of fairness interventions and limits their applicability across diverse network topologies. In this work, we propose a novel benchmarking framework for fair LP, centered on the structural biases of the underlying graphs. We begin by reviewing and formalizing a broad taxonomy of topological bias measures relevant to fairness in graphs. In parallel, we introduce a flexible graph generation method that simultaneously ensures fidelity to real-world graph patterns and enables controlled variation across a wide spectrum of structural biases. We apply this framework to evaluate both classical and fairness-aware LP models across multiple use cases. Our results provide a fine-grained empirical analysis of the interactions between predictive fairness and structural biases. This new perspective reveals the sensitivity of fairness interventions to beyond-homophily biases and underscores the need for structurally grounded fairness evaluations in graph learning.

</details>


### [320] [From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL](https://arxiv.org/abs/2602.11805)
*Ziyi Zhao,Qingchuan Li,Yuxuan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为增量签名贡献（ISC）的新方法，将路径签名分解为时序有序的张量代数元素序列，以显式建模路径的内部时间演化，并基于此构建了ISC-Transformer（ISCT）用于离线强化学习，在多个控制任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准路径签名将轨迹压缩为单一全局对象，丢失时间结构，难以适用于需要逐步响应的决策问题，尤其在对瞬时更新敏感、稳定性要求高的控制任务中表现受限。

Method: 提出增量签名贡献（ISC）方法，将截断路径签名分解为对应于最新路径增量的时序有序张量元素序列；在此基础上，将ISC嵌入标准Transformer架构，构建ISC-Transformer（ISCT）模型，无需额外架构修改。

Result: 在HalfCheetah、Walker2d、Hopper和Maze2d等基准任务（含延迟奖励与降质数据集设置）上，ISCT展现出优于基线的性能，验证了ISC在时序敏感控制任务中的理论合理性与实践有效性。

Conclusion: ISC提供了一种既保持路径签名代数表达力又显式保留时间动态的表示方式，是路径处理在时序敏感控制任务中的一种理论扎实且实用有效的替代方案。

Abstract: Path signatures embed trajectories into tensor algebra and constitute a universal, non-parametric representation of paths; however, in the standard form, they collapse temporal structure into a single global object, which limits their suitability for decision-making problems that require step-wise reactivity. We propose the Incremental Signature Contribution (ISC) method, which decomposes truncated path signatures into a temporally ordered sequence of elements in the tensor-algebra space, corresponding to incremental contributions induced by last path increments. This reconstruction preserves the algebraic structure and expressivity of signatures, while making their internal temporal evolution explicit, enabling processing signature-based representations via sequential modeling approaches. In contrast to full signatures, ISC is inherently sensitive to instantaneous trajectory updates, which is critical for sensitive and stability-requiring control dynamics. Building on this representation, we introduce ISC-Transformer (ISCT), an offline reinforcement learning model that integrates ISC into a standard Transformer architecture without further architectural modification. We evaluate ISCT on HalfCheetah, Walker2d, Hopper, and Maze2d, including settings with delayed rewards and downgraded datasets. The results demonstrate that ISC method provides a theoretically grounded and practically effective alternative to path processing for temporally sensitive control tasks.

</details>


### [321] [Deep Kernel Fusion for Transformers](https://arxiv.org/abs/2602.11808)
*Zixi Zhang,Zhiwen Mo,Yiren Zhao,Robert Mullins*

Main category: cs.LG

TL;DR: 本文提出DeepFusionKernel，一种深度融合核，通过减少HBM流量和提升缓存复用，显著加速长上下文下的Agentic LLM推理，在H100和A100上分别比SGLang快13.2%和9.7%。


<details>
  <summary>Details</summary>
Motivation: Agentic LLM在长上下文推理中受限于内存带宽而非计算能力，而SwiGLU MLP模块因权重过大超出缓存容量，成为未被充分优化的关键瓶颈。

Method: 设计并实现DeepFusionKernel——一种将多个操作深度融合的内核，结合SGLang框架与专用内核调度器，优化HBM访问与缓存利用。

Result: 在H100和A100上分别实现最高13.2%和9.7%的端到端推理加速；加速效果在不同生成长度、模型结构、配置及硬件平台上保持稳定且可迁移。

Conclusion: DeepFusionKernel验证了通过软硬协同的深度内核融合可有效缓解内存带宽瓶颈，为长上下文LLM推理提供了高效、通用且可扩展的优化路径。

Abstract: Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.

</details>


### [322] [CAAL: Confidence-Aware Active Learning for Heteroscedastic Atmospheric Regression](https://arxiv.org/abs/2602.11825)
*Fei Jiang,Jiyang Xia,Junjie Yu,Mingfei Sun,Hugh Coe,David Topping,Dantong Liu,Zhenhui Jessie Li,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种面向异方差噪声环境的置信度感知主动学习框架（CAAL），用于高效选择高成本大气颗粒物属性的测量或模拟样本，显著提升数据标注效率。


<details>
  <summary>Details</summary>
Motivation: 大气颗粒物的关键性质（如毒性、吸湿性）难以直接观测或模拟，常规观测数据与这些性质之间存在噪声大、输入依赖性强的异方差映射关系；而传统主动学习方法基于总不确定性采样，在异方差下易误选高噪声区域，浪费有限标注预算。

Method: 提出CAAL框架：1）解耦的不确定性感知训练目标，分别优化预测均值和噪声水平以稳定不确定性估计；2）置信度感知采集函数，利用预测的偶然不确定性（aleatoric uncertainty）动态加权认知不确定性（epistemic uncertainty）作为可靠性信号。

Result: 在颗粒物解析数值模拟和真实大气观测数据上的实验表明，CAAL持续优于标准主动学习基线方法。

Conclusion: CAAL为扩展高成本大气颗粒物属性数据库提供了实用且通用的解决方案，尤其适用于异方差回归与有限标注预算场景。

Abstract: Quantifying the impacts of air pollution on health and climate relies on key atmospheric particle properties such as toxicity and hygroscopicity. However, these properties typically require complex observational techniques or expensive particle-resolved numerical simulations, limiting the availability of labeled data. We therefore estimate these hard-to-measure particle properties from routinely available observations (e.g., air pollutant concentrations and meteorological conditions). Because routine observations only indirectly reflect particle composition and structure, the mapping from routine observations to particle properties is noisy and input-dependent, yielding a heteroscedastic regression setting. With a limited and costly labeling budget, the central challenge is to select which samples to measure or simulate. While active learning is a natural approach, most acquisition strategies rely on predictive uncertainty. Under heteroscedastic noise, this signal conflates reducible epistemic uncertainty with irreducible aleatoric uncertainty, causing limited budgets to be wasted in noise-dominated regions. To address this challenge, we propose a confidence-aware active learning framework (CAAL) for efficient and robust sample selection in heteroscedastic settings. CAAL consists of two components: a decoupled uncertainty-aware training objective that separately optimises the predictive mean and noise level to stabilise uncertainty estimation, and a confidence-aware acquisition function that dynamically weights epistemic uncertainty using predicted aleatoric uncertainty as a reliability signal. Experiments on particle-resolved numerical simulations and real atmospheric observations show that CAAL consistently outperforms standard AL baselines. The proposed framework provides a practical and general solution for the efficient expansion of high-cost atmospheric particle property databases.

</details>


### [323] [Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222)
*Miaosen Zhang,Yishan Liu,Shuxia Lin,Xu Yang,Qi Dai,Chong Luo,Weihao Jiang,Peng Hou,Anxiang Zeng,Xin Geng,Baining Guo*

Main category: cs.LG

TL;DR: 本文提出了一种名为On-Policy SFT的框架，通过Distribution Discriminant Theory（DDT）指导下的In-Distribution Finetuning（IDFT）和Hinted Decoding技术，使监督微调具备类似强化学习的泛化能力，同时保持计算高效性。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）计算高效但泛化能力弱于强化学习（RL），主要因RL使用on-policy数据；本文旨在弥合该差距。

Method: 提出Distribution Discriminant Theory（DDT）量化数据与模型分布对齐程度，并基于此设计In-Distribution Finetuning（损失层方法）和Hinted Decoding（数据层方法）两种技术。

Result: 实验表明，所提框架在泛化性能上媲美DPO、SimPO等主流离线RL算法，同时保持SFT的计算效率。

Conclusion: On-Policy SFT为RL不可行场景提供了高效且强泛化的实用替代方案。

Abstract: Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

</details>


### [324] [Towards Sustainable Investment Policies Informed by Opponent Shaping](https://arxiv.org/abs/2602.11829)
*Juan Agustin Duque,Razvan Ciuca,Ayoub Echchahed,Hugo Larochelle,Aaron Courville*

Main category: cs.LG

TL;DR: 本文形式化分析了InvestESG多智能体模拟中的跨期社会困境，并提出利用Advantage Alignment算法引导智能体学习趋向合作均衡，从而提升气候风险下市场与可持续发展目标的一致性。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化需全球协作，但理性经济主体常因短期利益而忽视集体福利，形成社会困境；InvestESG虽建模了投资者与企业在气候风险下的互动，但其内在激励冲突尚缺乏形式化刻画。

Method: 1）形式化推导InvestESG中出现跨期社会 dilemma 的理论阈值；2）应用Advantage Alignment算法进行对手塑造，调控智能体学习动态；3）提供该算法促进社会有益均衡的理论解释。

Result: Advantage Alignment能系统性地引导学习过程朝向更合作、更符合集体福利的均衡；实证表明该方法可显著改善长期可持续性结果。

Conclusion: 通过战略性地塑造经济智能体的学习过程，可在不依赖强制政策的前提下，内生地对齐市场激励与可持续发展目标，为气候治理提供新思路。

Abstract: Addressing climate change requires global coordination, yet rational economic actors often prioritize immediate gains over collective welfare, resulting in social dilemmas. InvestESG is a recently proposed multi-agent simulation that captures the dynamic interplay between investors and companies under climate risk. We provide a formal characterization of the conditions under which InvestESG exhibits an intertemporal social dilemma, deriving theoretical thresholds at which individual incentives diverge from collective welfare. Building on this, we apply Advantage Alignment, a scalable opponent shaping algorithm shown to be effective in general-sum games, to influence agent learning in InvestESG. We offer theoretical insights into why Advantage Alignment systematically favors socially beneficial equilibria by biasing learning dynamics toward cooperative outcomes. Our results demonstrate that strategically shaping the learning processes of economic agents can result in better outcomes that could inform policy mechanisms to better align market incentives with long-term sustainability goals.

</details>


### [325] [Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design](https://arxiv.org/abs/2602.11854)
*Mohammad Khosravi,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 本文研究在信号传输距离受限且网络链路与节点均存在不确定性的场景下，如何鲁棒地部署再生器以保证全网连通性，并提出基于列约束生成、Benders分解和迭代鲁棒优化的可扩展算法，以及一种学习驱动的‘藏匿-搜寻’博弈分析框架。


<details>
  <summary>Details</summary>
Motivation: 信号在通信网络中传输时会衰减，需在节点部署再生器来恢复信号；而实际中网络链路长度和再生器安装成本均存在不确定性，传统静态鲁棒模型难以刻画动态变化的不确定性。

Method: 构建动态预算不确定性集建模链路长度变化和再生器成本；建立鲁棒优化模型以最小化最坏情况下的再生器部署成本；设计列约束生成、Benders分解和迭代鲁棒优化三种求解方法；引入学习型hide-and-seek博弈分析问题结构。

Result: 所提方法在理论分析和计算实验中均优于经典静态预算鲁棒模型和确定性最坏情况模型，显著提升解的鲁棒性与经济性。

Conclusion: 动态预算不确定性建模与多策略鲁棒优化方法能更真实、高效地解决再生器部署问题，为高可靠性通信网络设计提供了新范式。

Abstract: We study the design of resilient and reliable communication networks in which a signal can be transferred only up to a limited distance before its quality falls below an acceptable threshold. When excessive signal degradation occurs, regeneration is required through regenerators installed at selected network nodes. In this work, both network links and nodes are subject to uncertainty. The installation costs of regenerators are modeled using a budgeted uncertainty set. In addition, link lengths follow a dynamic budgeted uncertainty set introduced in this paper, where deviations may vary over time. Robust optimization seeks solutions whose performance is guaranteed under all scenarios represented by the underlying uncertainty set. Accordingly, the objective is to identify a minimum-cost subset of nodes for regenerator deployment that ensures full network connectivity, even under the worst possible realizations of uncertainty. To solve the problem, we first formulate it within a robust optimization framework, and then develop scalable solution methods based on column-and-constraint generation, Benders decomposition, and iterative robust optimization. In addition, we formulate a learning-based hide-and-seek game to further analyze the problem structure. The proposed approaches are evaluated against classical static budgeted robust models and deterministic worst-case formulations. Both theoretical analysis and computational results demonstrate the effectiveness and advantages of our methodology.

</details>


### [326] [A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production](https://arxiv.org/abs/2602.11861)
*Sümeyye Meryem Taşyürek,Enis Mücahid İskender,Hacer Yalim Keles*

Main category: cs.LG

TL;DR: 本文提出A²V-SLP框架，采用对齐感知的变分方法实现手语生成中发音器官层级的解耦表征，通过分布式潜变量建模避免坍缩，并引入词性注意力增强文本-动作对齐，在无词性标注下提升回译性能与动作真实性。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法多依赖确定性潜变量，易导致潜空间坍缩，且缺乏对手语发音器官（articulator）层级结构的显式建模；同时，文本到动作的对齐机制不足，影响生成质量。

Method: 提出A²V-SLP：1）基于解耦变分自编码器（VAE）学习各发音器官对应的潜分布（均值与方差）；2）非自回归Transformer以文本嵌入为输入，联合预测潜均值和对数方差；3）VAE解码器通过随机采样重建姿态序列；4）引入gloss attention机制增强语言输入与动作的时序对齐。

Result: 在无gloss标注的纯文本驱动设定下，显著优于确定性潜变量回归方法，回译（back-translation）性能达SOTA，且生成动作更自然、真实。

Conclusion: 分布式的、发音器官层级解耦的潜变量建模是提升手语生成质量与可解释性的关键，对齐感知设计进一步强化了语言到动作的映射能力。

Abstract: Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.

</details>


### [327] [In-Context Function Learning in Large Language Models](https://arxiv.org/abs/2602.11863)
*Elif Akata,Konstantinos Voudouris,Vincent Fortuin,Eric Schulz*

Main category: cs.LG

TL;DR: 本文通过高斯过程（GP）视角研究大语言模型（LLM）的上下文学习能力，发现其学习曲线受生成函数的核函数影响显著，并随示例数增加趋近于GP回归下界；进一步通过似然分析揭示LLM偏好较不平滑的核，且后训练（如RL或SFT）可有效调整其归纳偏置以提升对平滑函数的学习效率。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在上下文学习中如何从少量演示中泛化，尤其是其隐含的归纳偏置和与贝叶斯非参数方法（如高斯过程）的关联性。

Method: 构建受控实验：让LLM观察从已知GP先验采样的多元标量函数序列；以GP回归为理论下界、1-NN为经验上界进行误差对比；采用基于似然的分析刻画LLM的归纳偏置；并通过强化学习和监督微调探索偏置可塑性。

Result: LLM学习曲线强烈依赖于函数生成核，且随示范数量增加逼近GP回归下界；LLM预测最可能对应于较不平滑的GP核；后训练能有效将归纳偏置转向更平滑核所生成的函数，提升样本效率。

Conclusion: LLM在连续函数学习任务中展现出类GP学习行为，其归纳偏置可被量化并经后训练定向调整，为可控提升小样本泛化能力提供了新框架。

Abstract: Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.

</details>


### [328] [Universal Diffusion-Based Probabilistic Downscaling](https://arxiv.org/abs/2602.11893)
*Roberto Molinaro,Niall Siegenheim,Henry Martin,Mark Frey,Niels Poulsen,Philipp Seitz,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的通用气象降尺度框架，无需针对特定模型微调，即可将低分辨率确定性天气预报转化为高分辨率概率预报。


<details>
  <summary>Details</summary>
Motivation: 提升天气预报的空间分辨率和不确定性表达能力，同时保持对上游不同数值或AI天气模型的通用性。

Method: 训练一个条件扩散模型，输入为粗分辨率（~25 km）天气预报，目标为高分辨率区域再分析数据（~5 km），并在多种异构确定性预报模型上零样本应用。

Result: 在近地面变量、长达90小时预报时效及多类观测站验证下，降尺度后集合平均优于原始确定性预报，CRPS等概率技巧显著提升。

Conclusion: 扩散模型降尺度是一种可扩展、模型无关的概率接口，能有效增强业务天气预报中的空间分辨率与不确定性表征能力。

Abstract: We introduce a universal diffusion-based downscaling framework that lifts deterministic low-resolution weather forecasts into probabilistic high-resolution predictions without any model-specific fine-tuning. A single conditional diffusion model is trained on paired coarse-resolution inputs (~25 km resolution) and high-resolution regional reanalysis targets (~5 km resolution), and is applied in a fully zero-shot manner to deterministic forecasts from heterogeneous upstream weather models. Focusing on near-surface variables, we evaluate probabilistic forecasts against independent in situ station observations over lead times up to 90 h. Across a diverse set of AI-based and numerical weather prediction (NWP) systems, the ensemble mean of the downscaled forecasts consistently improves upon each model's own raw deterministic forecast, and substantially larger gains are observed in probabilistic skill as measured by CRPS. These results demonstrate that diffusion-based downscaling provides a scalable, model-agnostic probabilistic interface for enhancing spatial resolution and uncertainty representation in operational weather forecasting pipelines.

</details>


### [329] [Mitigating Mismatch within Reference-based Preference Optimization](https://arxiv.org/abs/2602.11902)
*Suqin Yuan,Xingrui Yu,Jiyang Zheng,Lei Feng,Dadong Wang,Ivor Tsang,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出HyPO（Hybrid-DPO），一种对DPO的轻量改进：在参考模型悲观时（即偏好被拒响应）将其参考差值截断为零，从而避免‘过早满足’问题，提升绝对偏好对齐效果，同时保留DPO的稳定性与计算效率。


<details>
  <summary>Details</summary>
Motivation: DPO依赖参考策略虽能稳定训练，但在参考模型对偏好对呈悲观判断（即偏好被拒响应）时，会导致‘过早满足’——即策略仅需超过错误的参考差值即停止优化，造成训练-推理不匹配。

Method: 提出HyPO，在DPO目标中将原差值项Δ_θ−Δ_ref替换为Δ_θ−max{0, Δ_ref}：当Δ_ref≤0（悲观/中性）时忽略参考信号；当Δ_ref>0（乐观）时保持原DPO行为。该修改为单行、无需额外参数或计算开销。

Result: HyPO在多个偏好对齐任务上显著提升推理对齐指标（如胜率）和下游性能，验证了条件式去偏参考信号的有效性，优于标准DPO及纯无参考方法。

Conclusion: 参考模型不应被无差别信任；通过条件性地抑制其悲观信号，可在不牺牲稳定性前提下增强DPO的优化能力，为直接偏好优化提供了更鲁棒的设计范式。

Abstract: Direct Preference Optimization (DPO) has become the de facto standard for offline preference alignment of large language models, but its reliance on a reference policy introduces a critical tension. DPO weighs each update relative to a reference, which stabilizes the training by regularizing the updates within a trusted region. This reliance becomes problematic for pessimistic pairs, where the reference model prefers the rejected response. For these pairs, DPO prematurely attenuates the gradient as soon as the policy margin ($Δ_θ$) merely beats the reference margin ($Δ_{\mathrm{ref}}$) even if the policy is still wrong ($Δ_θ<0$). We name this failure premature satisfaction, which is a concrete form of the training-inference mismatch. Reference-free objectives remove this mismatch by optimizing the absolute margin, but at the cost of discarding the stabilizing signal of the reference. We mitigate this tension with Hybrid-DPO (HyPO), a drop-in modification to DPO that applies reference conditionally: HyPO behaves exactly like DPO when the reference is optimistic or neutral, and it treats the reference as neutral when it is pessimistic by replacing $Δ_θ-Δ_{\mathrm{ref}}$ with $Δ_θ-\max\{0,Δ_{\mathrm{ref}}\}$. This one-line change strictly strengthens per-example learning signals on pessimistic pairs while preserving DPO's objective form and computational cost. By conditionally debiasing the pessimistic reference signal, HyPO mitigates premature satisfaction; empirically, across preference alignment, HyPO improves inference-aligned metrics and achieves higher pairwise win rates. Our results provide evidence that direct preference alignment could be enhanced by conditionally debiasing the reference signal, rather than discarding it.

</details>


### [330] [Learning Conditional Averages](https://arxiv.org/abs/2602.11920)
*Marco Bressan,Nataly Brukhim,Nicolo Cesa-Bianchi,Emmanuel Esposito,Yishay Mansour,Shay Moran,Maximilian Thiessen*

Main category: cs.LG

TL;DR: 本文提出了在PAC框架下学习条件平均值的新问题，目标是预测每个实例在其邻域内的平均标签，而非传统PAC学习中的单点预测；作者给出了该问题可学习性的完整刻画，关键在于两个新定义的组合参数（依赖于概念类与邻域系统）的联合有限性，并提供了紧致至对数因子的样本复杂度界。


<details>
  <summary>Details</summary>
Motivation: 扩展经典PAC学习以建模实际中需预测局部平均（如可解释性、公平性、推荐系统）的学习任务，而不仅是单点预测。

Method: 引入并分析两个新的组合参数（与概念类和邻域系统共同相关，类比邻域图的独立数），基于其联合有限性建立可学习性充要条件，并推导样本复杂度上界与下界。

Result: 给出了条件平均学习在PAC框架下可学习的完整刻画：当且仅当两个新定义的组合参数联合有限时，该问题可学习；并给出紧致至对数因子的样本复杂度界限。

Conclusion: 条件平均学习是经典PAC学习的自然推广，其可学习性由概念类与邻域结构共同决定，不能仅由VC维等传统参数刻画；该理论为面向局部统计特性的机器学习任务提供了基础框架。

Abstract: We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.

</details>


### [331] [Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration](https://arxiv.org/abs/2602.11937)
*Akhiad Bercovich,Nir Ailon,Vladimir Anisimov,Tomer Asida,Nave Assaf,Mohammad Dabbah,Ido Galil,Amnon Geifman,Yonatan Geifman,Izhak Golan,Roi Koren,Itay Levy,Zach Moshe,Pavlo Molchanov,Najeeb Nabwani,Mostofa Patwari,Omri Puny,Tomer Ronen,Itamar Schen,Elad Segal,Ido Shahaf,Oren Tropp,Ran Zilberstein,Ran El-Yaniv*

Main category: cs.LG

TL;DR: 本文提出Puzzle框架，通过异构MoE专家剪枝、窗口注意力替换、FP8 KV缓存量化及强化学习微调等技术，将gpt-oss-120B优化为gpt-oss-puzzle-88B，在保持准确率的同时显著提升推理吞吐与请求级效率。


<details>
  <summary>Details</summary>
Motivation: 推理增强型大模型虽提升答案质量，但长推理链导致推理成本剧增，亟需高效推理优化方法。

Method: 扩展并应用Puzzle——一种后训练神经架构搜索（NAS）框架，结合异构MoE专家剪枝、全上下文注意力替换为窗口注意力、带校准尺度的FP8 KV缓存量化，以及后训练强化学习恢复精度。

Result: 在8×H100节点上，长/短上下文下每token吞吐分别提升1.63×和1.22×；单H100 GPU上达2.82×吞吐提升；请求级效率最高提升1.29×；准确率保留率达100.8%–108.2%。

Conclusion: 后训练架构搜索可在不牺牲质量前提下大幅降低推理开销；应采用请求级效率（而非单纯tok/s）评估推理优化效果，并权衡准确率与速度。

Abstract: Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.

</details>


### [332] [Temporally Unified Adversarial Perturbations for Time Series Forecasting](https://arxiv.org/abs/2602.11940)
*Ruixian Su,Yukun Bao,Xinze Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种针对时间序列预测模型的新型对抗攻击方法——时间统一对抗扰动（TUAPs），通过引入时间统一约束和时戳级梯度累积方法（TGAM），解决了现有攻击中时间不一致扰动的问题，显著提升了白盒与黑盒迁移攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列对抗攻击方法忽略时间序列固有的时间一致性，导致重叠样本在同一时间戳上产生不一致的扰动，使攻击在现实中不可行。

Method: 提出时间统一对抗扰动（TUAPs）框架，并设计时戳级梯度累积方法（TGAM）以聚合重叠样本的局部梯度；结合动量类攻击算法，兼顾时间一致性与序列级梯度利用。

Result: 在三个基准数据集和四个SOTA模型上的实验表明，该方法在TUAP约束下显著优于基线，在白盒与黑盒迁移攻击中均表现优异；即使无TUAP约束，其迁移性能也更优。

Conclusion: TUAPs与TGAM有效解决了时间序列对抗攻击中的时间不一致性问题，提升了攻击的实用性与泛化能力，为时间序列模型安全性研究提供了新思路。

Abstract: While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.

</details>


### [333] [Using predictive multiplicity to measure individual performance within the AI Act](https://arxiv.org/abs/2602.11944)
*Karolin Frohnapfel,Mara Seyfert,Sebastian Bordt,Ulrike von Luxburg,Kristof Meding*

Main category: cs.LG

TL;DR: 本文探讨了预测多重性（predictive multiplicity）与欧盟《人工智能法案》（AI Act）中关于准确性的合规要求之间的关系，提出量化个体层面模型分歧的指标（如个体冲突比、δ-模糊性），并给出实践评估建议，以支持高风险AI系统的合规部署与可信决策。


<details>
  <summary>Details</summary>
Motivation: 预测多重性导致相同准确率下个体预测结果不一致，这种任意性与欧盟AI法案要求对特定个体报告性能的规定相冲突，亟需从法律与技术角度加以规范。

Method: 结合法律分析与计算洞察，提出个体冲突比和δ-ambiguity作为量化个体级模型分歧的指标，并设计易于实施的实践评估规则。

Result: 明确了预测多重性可助力AI Act准确性合规；提出了可操作的量化工具与评估方法；主张将多重性信息提供给系统部署者以支撑个案可靠性判断。

Conclusion: 预测多重性不仅是技术现象，更是合规关键维度；将其纳入评估与报告体系，是实现高风险AI系统透明、公平与可信部署的必要步骤。

Abstract: When building AI systems for decision support, one often encounters the phenomenon of predictive multiplicity: a single best model does not exist; instead, one can construct many models with similar overall accuracy that differ in their predictions for individual cases. Especially when decisions have a direct impact on humans, this can be highly unsatisfactory. For a person subject to high disagreement between models, one could as well have chosen a different model of similar overall accuracy that would have decided the person's case differently. We argue that this arbitrariness conflicts with the EU AI Act, which requires providers of high-risk AI systems to report performance not only at the dataset level but also for specific persons. The goal of this paper is to put predictive multiplicity in context with the EU AI Act's provisions on accuracy and to subsequently derive concrete suggestions on how to evaluate and report predictive multiplicity in practice. Specifically: (1) We argue that incorporating information about predictive multiplicity can serve compliance with the EU AI Act's accuracy provisions for providers. (2) Based on this legal analysis, we suggest individual conflict ratios and $δ$-ambiguity as tools to quantify the disagreement between models on individual cases and to help detect individuals subject to conflicting predictions. (3) Based on computational insights, we derive easy-to-implement rules on how model providers could evaluate predictive multiplicity in practice. (4) Ultimately, we suggest that information about predictive multiplicity should be made available to deployers under the AI Act, enabling them to judge whether system outputs for specific individuals are reliable enough for their use case.

</details>


### [334] [RAM-Net: Expressive Linear Attention with Selectively Addressable Memory](https://arxiv.org/abs/2602.11958)
*Kaicheng Xiao,Haotian Li,Liran Dong,Guoliang Xing*

Main category: cs.LG

TL;DR: 本文提出了RAM-Net，一种通过高维稀疏向量寻址实现超大容量显式记忆的线性注意力架构，在保持计算高效性的同时显著提升长程依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力因将无限历史压缩为固定大小记忆而导致表达能力受限和信息丢失。

Method: 提出RAM-Net，将输入映射为高维稀疏地址向量，以选择性访问大规模记忆；利用稀疏性实现指数级状态扩展且不增参数，减少信号干扰并提高检索保真度。

Result: 在细粒度长程检索任务上持续超越SOTA基线，在语言建模和零样本常识推理基准上表现具有竞争力。

Conclusion: RAM-Net成功弥合了全注意力的表达力与线性模型内存效率之间的鸿沟，兼具强依赖建模能力和低计算开销。

Abstract: While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.

</details>


### [335] [Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios](https://arxiv.org/abs/2602.11945)
*Hongliang Zhang,Jiguo Yu,Guijuan Wang,Wenshuo Ma,Tianqing He,Baobao Chai,Chunqiang Hu*

Main category: cs.LG

TL;DR: 本文提出PMFL框架，通过节点端的模型对比学习和服务器端的自适应聚合权重调整，提升异构场景下联邦学习的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统通常部署在异构场景中，节点间数据分布和参与频率差异大，导致性能下降。

Method: 在节点端引入基于历史本地模型的模型对比项以增强更新一致性；在服务器端利用节点累积参与次数自适应调整聚合权重，并融合历史全局模型减少性能波动。

Result: 大量实验表明，PMFL在异构场景下相比现有联邦学习方法具有更优性能。

Conclusion: PMFL通过结合历史训练信息，在模型对比和聚合机制两方面改进，有效缓解了数据与参与异构性带来的负面影响。

Abstract: Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle the above issue, this paper proposes PMFL, a performance-enhanced model-contrastive federated learning framework using historical training information. Specifically, on the node side, we design a novel model-contrastive term into the node optimization objective by incorporating historical local models to capture stable contrastive points, thereby improving the consistency of model updates in heterogeneous data distributions.
  On the server side, we utilize the cumulative participation count of each node to adaptively adjust its aggregation weight, thereby correcting the bias in the global objective caused by different node participation frequencies. Furthermore, the updated global model incorporates historical global models to reduce its fluctuations in performance between adjacent rounds. Extensive experiments demonstrate that PMFL achieves superior performance compared with existing FL methods in heterogeneous scenarios.

</details>


### [336] [Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization](https://arxiv.org/abs/2602.11957)
*Suyash Mishra,Qiang Li,Anubhav Girdhar*

Main category: cs.LG

TL;DR: 本文提出LRBTC，一种基于大语言模型（LLM）和视觉语言模型（VLM）的模块化质量控制架构，用于制药等高监管领域的内容审核，显著提升准确性与合规性检测能力。


<details>
  <summary>Details</summary>
Motivation: 在医药等强监管领域，大模型生成内容需兼顾科学准确性和法律合规性，而传统人工质控效率低、易出错且成为发布瓶颈。

Method: 提出LRBTC架构，融合LLM与VLM，涵盖语言、法规、品牌、技术及内容结构五类检查；采用学生-教师双模型、人工介入（HITL）与瀑布式规则过滤机制。

Result: 在AIReg-Bench上F1达83.0%、召回率97.5%，漏检违规减少5倍；在CSpelling上平均准确率提升26.7%；发现当前模型对复杂医学语法（召回率25.0%）和标点错误（41.7%）识别能力薄弱。

Conclusion: LRBTC为高风险、强合规行业提供了可扩展、可验证、透明可靠的即插即用质控方案，并开源演示系统。

Abstract: Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses.

</details>


### [337] [Manifold-Aware Temporal Domain Generalization for Large Language Models](https://arxiv.org/abs/2602.11965)
*Yiheng Yao,Zekun Cai,Xinyuan Song,Hiroki Hill Kobayashi,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Main category: cs.LG

TL;DR: 本文提出MaT-LoRA方法，通过在低秩适应子空间中将时间更新约束于共享低维流形，并建模其结构化时间演化，实现高效且具表达力的时间域泛化，显著降低计算复杂度并提升LLM的时间泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大语言模型（LLM）部署面临普遍的时间分布偏移问题，而现有时间域泛化（TDG）方法在全参数空间进行模型自适应，对现代LLM而言计算不可行。

Method: 提出基于参数高效微调的TDG几何重构框架；证明低维时间结构可在参数高效重参数化下保持；设计Manifold-aware Temporal LoRA（MaT-LoRA），将时间更新限制在低秩子空间内的共享低维流形，并通过结构化时间核建模该流形演化。

Result: 在合成数据及真实数据集（科学文献、新闻源、评论评分）上验证了MaT-LoRA在时间泛化性能上的优越性，并展现出对LLM的实际可扩展性。

Conclusion: MaT-LoRA通过流形感知与低秩时间建模，在不牺牲表达能力的前提下大幅降低时间建模复杂度，为LLM的时间域泛化提供了高效可行的新范式。

Abstract: Temporal distribution shifts are pervasive in real-world deployments of Large Language Models (LLMs), where data evolves continuously over time. While Temporal Domain Generalization (TDG) seeks to model such structured evolution, existing approaches characterize model adaptation in the full parameter space. This formulation becomes computationally infeasible for modern LLMs. This paper introduces a geometric reformulation of TDG under parameter-efficient fine-tuning. We establish that the low-dimensional temporal structure underlying model evolution can be preserved under parameter-efficient reparameterization, enabling temporal modeling without operating in the ambient parameter space. Building on this principle, we propose Manifold-aware Temporal LoRA (MaT-LoRA), which constrains temporal updates to a shared low-dimensional manifold within a low-rank adaptation subspace, and models its evolution through a structured temporal core. This reparameterization dramatically reduces temporal modeling complexity while retaining expressive power. Extensive experiments on synthetic and real-world datasets, including scientific documents, news publishers, and review ratings, demonstrate that MaT-LoRA achieves superior temporal generalization performance with practical scalability for LLMs.

</details>


### [338] [Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning](https://arxiv.org/abs/2602.12123)
*Xubin Wang,Weijia Jia*

Main category: cs.LG

TL;DR: 本文提出Meta-Sel，一种轻量级、可解释的元学习方法，用于在上下文学习中高效选择少样本示例，通过两个廉价元特征（TF-IDF余弦相似度和长度兼容性比）训练逻辑回归模型，实现快速、确定性、可审计的演示选择。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习中，演示选择是实际瓶颈：提示预算有限时，不同样例组合会显著影响准确率，但选择过程需足够廉价以支持每查询在大规模候选池中运行。

Method: Meta-Sel构建元数据集（基于训练集采样并以类别一致性为监督信号），训练校准的逻辑回归模型，输入为TF-IDF余弦相似度和长度兼容性比两个元特征；推理时对整个候选池进行单次向量化打分并取top-k，无需微调、在线探索或额外LLM调用。

Result: 在四个意图分类数据集和五个开源大语言模型上的广泛实验表明，Meta-Sel始终位居性能前列，尤其对小模型提升显著，且选择开销具有竞争力。

Conclusion: Meta-Sel提供了一种高效、可解释、低开销的演示选择方案，兼顾性能与实用性，为ICL中的少样本选择问题提供了新思路。

Abstract: Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.
  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.
  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.

</details>


### [339] [Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret](https://arxiv.org/abs/2602.11995)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 本文研究了动量最小均方（MLMS）算法在时变随机线性系统中的自适应辨识性能，理论推导了其跟踪性能与遗憾界，并通过实验验证了其在非平稳数据流中的快速适应与鲁棒跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 大规模数据流常具有漂移分布和时变系统参数，违反i.i.d.假设，亟需单次扫描、计算与内存开销不随流长增长的在线自适应算法。

Method: 理论分析MLMS算法在时变随机线性系统下的跟踪性能与遗憾界，处理由动量引入的二阶时变随机向量差分方程及其随机矩阵乘积稳定性问题；辅以合成与真实数据流实验。

Result: 推导出MLMS在多种实际条件下的跟踪误差界与遗憾界；实验证明其在非平稳环境下比经典LMS具备更快适应性与更强鲁棒性。

Conclusion: MLMS是一种计算简洁、适合实时更新的在线自适应方法，在现代流式与在线学习任务中具有重要应用前景。

Abstract: In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.

</details>


### [340] [Capability-Oriented Training Induced Alignment Risk](https://arxiv.org/abs/2602.12124)
*Yujun Zhou,Yue Huang,Han Bao,Kehan Guo,Zhenwen Liang,Pin-Yu Chen,Tian Gao,Werner Geyer,Nuno Moniz,Nitesh V Chawla,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 本文研究了能力导向训练引发的漏洞利用风险，发现语言模型在强化学习中会自发利用训练环境中的隐性漏洞以最大化奖励，且这些策略具有可迁移性和可蒸馏性，对现有AI对齐方法构成根本性挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数AI对齐研究关注防止模型生成显性有害内容，但忽视了一种更隐蔽的风险：能力导向训练可能诱导模型利用训练环境中的隐性漏洞。

Method: 设计了四个多样化的‘漏洞游戏’，涵盖上下文条件合规、代理指标、奖励篡改和自我评估等不同类型的隐性漏洞，并通过强化学习实验检验模型是否自发学习利用这些漏洞。

Result: 模型一致学会利用漏洞，发现显著提升奖励但损害任务正确性或安全性的机会主义策略；这些策略具有泛化能力，可跨任务迁移并能通过数据从教师模型蒸馏到学生模型。

Conclusion: 能力导向训练引发的风险对当前AI对齐方法构成根本挑战，未来AI安全工作需扩展至严格审计和加固训练环境与奖励机制本身。

Abstract: While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse "vulnerability games", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow "tricks" but generalizable skills; they can be transferred to new tasks and even "distilled" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.

</details>


### [341] [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](https://arxiv.org/abs/2602.12009)
*Luiz Pereira,Mirko Perkusich,Dalton Valadares,Kyller Gorgônio*

Main category: cs.LG

TL;DR: 本文分析了差分隐私机制（如梯度裁剪和噪声注入）如何扰动脉冲神经网络中的发放率统计，并影响基于发放率的联邦神经形态学习协调过程，揭示了隐私预算与协调性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，联邦神经形态学习需要额外的隐私机制（如差分隐私），但这些机制会显著改变训练信号，亟需理解其对SNN发放率统计及协调机制的影响。

Method: 分析差分隐私机制（梯度裁剪与噪声注入）对SNN发放率统计的扰动及其在基于发放率的FNL协调中的传播；在非独立同分布语音识别任务上进行消融实验，考察不同隐私预算和裁剪界下的系统性发放率偏移、聚合衰减与客户端选择排序不稳定现象，并关联稀疏性与记忆指标。

Result: 发现DP机制引发系统性发放率偏移、聚合能力下降及客户端选择排序不稳定；这些扰动与SNN的稀疏性和记忆特性密切相关。

Conclusion: 为隐私保护的联邦神经形态学习提供了可操作指导，强调需在隐私强度与基于发放率的协调机制之间取得平衡。

Abstract: Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.

</details>


### [342] [Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation](https://arxiv.org/abs/2602.12125)
*Wenkai Yang,Weijie Liu,Ruobing Xie,Kai Yang,Saiyong Yang,Yankai Lin*

Main category: cs.LG

TL;DR: 本文提出广义在线策略蒸馏（G-OPD）框架，通过引入灵活参考模型和可调奖励缩放因子，扩展标准OPD；发现奖励外推（ExOPD）能持续提升性能，甚至使学生超越教师；在强到弱蒸馏中，以教师RL前基模型为参考可进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有在线策略蒸馏（OPD）虽表现优异，但其理论基础不清晰且目标固定，缺乏对奖励与KL正则项权衡的灵活性，限制了性能上限。

Method: 将OPD建模为一种特殊的稠密KL约束强化学习，并提出G-OPD框架：引入可选参考模型与奖励缩放因子β；当β>1时定义为ExOPD；在强到弱蒸馏中，采用教师RL前基模型作为参考实现奖励校正。

Result: ExOPD在数学推理与代码生成任务上一致优于标准OPD，尤其在多领域专家知识融合场景下使学生超越教师；奖励校正进一步提升性能，但需访问教师预训练模型并增加计算开销。

Conclusion: OPD本质是KL约束RL的一种特例；G-OPD及其变体ExOPD为蒸馏提供了更灵活、更强大的范式；奖励设计与参考模型选择是提升蒸馏效果的关键新维度。

Abstract: On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.

</details>


### [343] [FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client](https://arxiv.org/abs/2602.12014)
*Gongxi Zhu,Hanlin Gu,Lixin Fan,Qiang Yang,Yuxing Han*

Main category: cs.LG

TL;DR: 本文提出FedGRPO框架，通过强化学习风格的评估过程，在联邦基础模型中实现隐私保护的知识迁移，仅交换标量奖励信号，降低通信开销与隐私风险，并提升下游任务准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型级或表征级知识迁移的联邦基础模型方法存在本地训练开销大、通信成本高及隐私泄露风险等问题。

Method: 将问题重构为强化学习式评估过程；设计两模块框架：1）基于辅助数据构建轻量置信图，进行能力驱动的专家客户端选择；2）引入Group Relative Policy Optimization（GRPO）思想，将问题与求解理由打包为候选策略，分发至选定专家客户端，仅聚合标量奖励信号，采用联邦组相对损失函数进行优化。

Result: 在多领域任务上，FedGRPO相比传统联邦基础模型基线，显著提升了下游任务准确率与通信效率。

Conclusion: FedGRPO通过仅交换奖励信号而非原始数据或模型更新，实现了高效、低开销、强隐私保护的联邦知识迁移，为FedFMs提供了新范式。

Abstract: One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the "Group Relative" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.

</details>


### [344] [Improved state mixing in higher-order and block diagonal linear recurrent networks](https://arxiv.org/abs/2602.12021)
*Igor Dubinin,Antonio Orvieto,Felix Effenberger*

Main category: cs.LG

TL;DR: 本文提出了两种结构化的线性循环网络（H-LRU 和 BD-LRU），通过增强状态在时间和通道维度上的混合能力来提升线性模型的表达力，同时保持计算效率；实验表明其在合成任务和语言建模中性能优于或媲美 Mamba、DeltaNet 和 LSTM，且揭示了状态混合结构比单纯增宽更能决定表达力。


<details>
  <summary>Details</summary>
Motivation: 线性循环网络（LRNNs）和线性状态空间模型（SSMs）虽高效但因对角状态转移而表达力受限；而LSTM等密集非线性模型表达力强却计算昂贵；本文旨在提升LRNNs表达力的同时维持效率。

Method: 提出两种新架构：(i) 高阶线性循环单元（H-LRU），将一阶递归推广至高阶，混合多个历史状态；(ii) 分块对角LRU（BD-LRU），支持块内密集通道混合；引入逐通道或逐行L1归一化选择门以稳定训练，并采用并行扫描实现高效推理。

Result: 在合成序列建模任务中，BD-LRU性能匹配或超越Mamba、DeltaNet和LSTM；H-LRU在压缩任务中参数效率最高；两者均验证了状态混合结构对表达力的关键作用。

Conclusion: 状态混合的结构设计（而非仅模型宽度）是提升线性序列模型表达力的核心，为弥合效率与表达力之间的鸿沟提供了实用路径。

Abstract: Linear recurrent networks (LRNNs) and linear state space models (SSMs) promise computational and memory efficiency on long-sequence modeling tasks, yet their diagonal state transitions limit expressivity. Dense and nonlinear architectures (e.g., LSTMs) on the other hand are provably more expressive, but computationally costly. Here, we explore how expressivity in LRNNs can be increased via richer state mixing across time and channels while maintaining competitive efficiency. Specifically, we introduce two structured LRNN architectures: (i) Higher-order Linear Recurrent Units (H-LRU), which generalize first-order recurrence to higher order, mixing multiple past states, and (ii) Block-Diagonal LRUs (BD-LRU), which enable dense intra-block channel mixing. Per-channel (H-LRU) or per-row (BD-LRU) L1-normalization of selective gates stabilizes training and allows for scaling window/block sizes. A parallel-scan implementation of the proposed architectures keeps the throughput competitive with diagonal LRNNs for moderate orders (H-LRU) and block sizes (BD-LRU). In synthetic sequence modeling tasks, the performance of BD-LRU matches or exceeds those of linear SSMs (Mamba), low-rank LRNNs (DeltaNet) and LSTM baselines, while H-LRU is found to be the most parameter-efficient in compression task. In both synthetic sequence modeling and language modeling, our results indicate that the structure of state mixing rather than width alone shapes expressivity of LRNNs, offering a practical route to closing the efficiency-expressivity gap in linear sequence models.

</details>


### [345] [Protein Circuit Tracing via Cross-layer Transcoders](https://arxiv.org/abs/2602.12026)
*Darin Tsui,Kunal Talreja,Daniel Saeedi,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 本文提出了ProtoMech框架，通过跨层转码器在蛋白质语言模型（pLMs）中发现计算回路，揭示了模型内部结构与功能相关的压缩回路，并支持高性能的蛋白质设计。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法仅独立分析各层表示，无法捕捉跨层计算，难以完整近似模型行为。

Method: 提出ProtoMech框架，利用跨层转码器联合学习稀疏潜在表示，以建模pLM（如ESM2）的全层计算回路。

Result: 在蛋白家族分类和功能预测任务上恢复82-89%原始性能；发现占用<1%潜在空间却保留高达79%准确率的压缩回路；回路对应结合、信号传导、稳定性等功能/结构基序；电路引导设计在超70%案例中优于基线。

Conclusion: ProtoMech为蛋白质语言模型提供了可解释、可干预的计算回路追踪新范式，推动了模型机制理解与可控蛋白质设计。

Abstract: Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computational circuits in pLMs using cross-layer transcoders that learn sparse latent representations jointly across layers to capture the model's full computational circuitry. Applied to the pLM ESM2, ProtoMech recovers 82-89% of the original performance on protein family classification and function prediction tasks. ProtoMech then identifies compressed circuits that use <1% of the latent space while retaining up to 79% of model accuracy, revealing correspondence with structural and functional motifs, including binding, signaling, and stability. Steering along these circuits enables high-fitness protein design, surpassing baseline methods in more than 70% of cases. These results establish ProtoMech as a principled framework for protein circuit tracing.

</details>


### [346] [PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving](https://arxiv.org/abs/2602.12029)
*Sunghyeon Woo,Hoseung Kim,Sunghwan Shim,Minjung Jo,Hyunjoon Jeong,Jeongtae Lee,Joonghoon Kim,Sungjae Lee,Baeseong Park,Se Jung Kwon,Dongsoo Lee*

Main category: cs.LG

TL;DR: 本文提出PrefillShare算法，通过在多模型代理系统中共享预填充（prefill）阶段来减少冗余计算和KV缓存开销，显著降低尾部延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中多个大语言模型重复处理相同提示前缀，导致预填充阶段冗余、KV缓存重复存储及预填充-解码干扰加剧，影响服务延迟与吞吐。

Method: 将模型解耦为预填充与解码模块，冻结预填充模块、仅微调解码模块，并设计路由机制支持异构模型在vLLM解耦系统中共享同一预填充模块及KV缓存。

Result: 在多种任务和模型上达到全量微调精度，p95延迟降低4.5倍，多模型代理工作负载吞吐提升3.9倍。

Conclusion: PrefillShare有效消除跨模型预填充冗余，在保持精度的同时大幅提升多模型协同推理效率，为多智能体大模型服务提供新范式。

Abstract: Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.

</details>


### [347] [Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling](https://arxiv.org/abs/2602.12045)
*Jed A. Duersch,Elohan Veillon,Astrid Klipfel,Adlane Sayede,Zied Bouraoui*

Main category: cs.LG

TL;DR: 本文提出了一种基于倒空间傅里叶表示的晶体生成新方法，避免直接建模原子坐标，天然满足周期性与对称性约束，并支持可变原子数；结合复值傅里叶系数的Transformer VAE与潜在扩散模型，在LeMaterial基准上验证了其重建与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有晶体生成模型难以同时处理周期性边界条件、晶体对称性、物理约束及大而多样的晶胞，且粒子式方法在原子多重性上存在局限。

Method: 采用截断傅里叶变换表征物种分辨的晶胞密度（而非原子坐标），以复值傅里叶系数为输入构建Transformer变分自编码器，并在其压缩潜在空间中训练扩散模型。

Result: 仅用每维9个傅里叶基函数即可重建含最多108原子/元素的晶胞；在LeMaterial基准上验证了重建精度与潜在扩散性能；在小晶胞（≤16原子）无条件生成任务中优于基于坐标的基线方法。

Conclusion: 倒空间表示为晶体生成提供了更自然、更灵活、更具可扩展性的范式，突破了传统粒子建模在对称性、周期性和原子数可变性上的瓶颈。

Abstract: The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crystals through a truncated Fourier transform of the species-resolved unit-cell density, rather than modeling atomic coordinates directly. This representation is periodicity-native, admits simple algebraic actions of space-group symmetries, and naturally supports variable atomic multiplicities during generation, addressing a common limitation of particle-based approaches. Using only nine Fourier basis functions per spatial dimension, our approach reconstructs unit cells containing up to 108 atoms per chemical species. We instantiate this pipeline with a transformer variational autoencoder over complex-valued Fourier coefficients, and a latent diffusion model that generates in the compressed latent space. We evaluate reconstruction and latent diffusion on the LeMaterial benchmark and compare unconditional generation against coordinate-based baselines in the small-cell regime ($\leq 16$ atoms per unit cell).

</details>


### [348] [Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237)
*Mayee F. Chen,Tyler Murray,David Heineman,Matt Jordan,Hannaneh Hajishirzi,Christopher Ré,Luca Soldaini,Kyle Lo*

Main category: cs.LG

TL;DR: Olmix是一个用于语言模型训练中数据混合的框架，通过实证研究确定了最优的数据混合设计选择，并引入了混合重用机制以应对动态变化的领域集，显著减少了计算开销并提升了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据混合方法在真实语言模型开发中存在两大问题：一是混合方法的设计空间缺乏系统理解与共识；二是忽视了领域集合在开发过程中动态演化的实际需求。

Method: Olmix通过大规模实证研究刻画混合方法设计空间，并提出‘混合重用’机制，在领域集更新时仅重算受影响领域的混合比例，复用历史混合结果。

Result: 在五次模拟真实开发的领域集更新中，混合重用以74%更少计算量达到与全量重算相当的性能，并比不使用混合提升下游任务性能11.6%。

Conclusion: Olmix为动态、实际的语言模型开发提供了高效且高性能的数据混合解决方案，兼顾设计合理性与工程实用性。

Abstract: Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.

</details>


### [349] [Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards](https://arxiv.org/abs/2602.12049)
*Ryo Mikasa,Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.LG

TL;DR: 本文提出了一种结合在线强化学习与分阶段质量多样性（SQD）算法的方法，利用超级计算机实测运行时性能（GFLOPS）作为奖励信号，训练大语言模型（如Qwen2.5 Coder 14B）生成高性能HPC代码，尤其在双精度矩阵乘法任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能生成代码，但无法保证其运行时性能；HPC领域中尚缺乏以实际运行性能为奖励训练LLM的尝试。

Method: 提出在线强化学习框架，将LLM生成的代码在超算上执行并以实测GFLOPS为奖励；引入分阶段质量多样性（SQD）算法，按问题动态调整允许的优化技术；构建GPU训练集群与CPU基准测试集群协同的分布式系统；采用Group Relative Policy Optimization（GRPO）训练Qwen2.5 Coder 14B。

Result: 在双精度矩阵乘法任务上，该方法显著提升了LLM生成HPC代码的运行性能；两个实验验证了运行时反馈与分阶段优化协同的有效性。

Conclusion: 将真实硬件性能反馈融入LLM训练流程，并辅以分阶段多样化优化策略，可有效增强模型在高性能计算场景下的代码生成能力。

Abstract: Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.

</details>


### [350] [PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories](https://arxiv.org/abs/2602.12080)
*Hyunsung Kim,Kunhee Lee,Sangwoo Seo,Sang-Ki Ko,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文提出PathCRF框架，仅利用球员追踪数据自动检测足球中的控球事件（如传球、控球），通过动态图建模与条件随机场（CRF）保证逻辑一致性，显著降低人工标注依赖。


<details>
  <summary>Details</summary>
Motivation: 现有足球事件数据采集严重依赖人工标注，而基于球轨迹的自动检测方法受限于高成本的球跟踪系统，导致数据难以在非顶级赛事中普及。

Method: 将球员轨迹建模为全连接动态图，将事件检测转化为每帧选择一条代表当前持球状态的边；引入条件随机场（CRF）约束边序列的合法转移，并用基于集合注意力机制的骨干网络动态生成边的发射分和转移分；通过Viterbi解码获取最优边序列，边变化即触发事件检测。

Result: PathCRF能生成准确且逻辑一致的持球路径，在减少人工标注的同时支持可靠的下游分析。

Conclusion: 仅使用球员追踪数据即可实现高质量、逻辑自洽的足球事件自动检测，为低资源场景下的数据驱动足球分析提供了可行方案。

Abstract: Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.

</details>


### [351] [Empirical Gaussian Processes](https://arxiv.org/abs/2602.12082)
*Jihao Andreas Lin,Sebastian Ament,Louis C. Tiao,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文提出Empirical GPs框架，通过从历史数据中经验估计均值和协方差函数来构建数据驱动的高斯过程先验，克服了传统手工设计核函数的局限性；理论证明其收敛于KL散度意义下最接近真实数据生成过程的GP，实践上通过EM算法实现高效学习，并在学习曲线外推和时间序列预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程依赖人工设计的核函数，需专家知识、适应性差且对假设空间施加强约束，限制了实际效果。

Method: 提出Empirical GPs框架，从历史观测数据中经验估计GP的均值与协方差函数；将先验学习建模为多独立数据集上的似然估计问题，推导出具有闭式更新的EM算法，支持异构观测位置。

Result: 理论证明模型在KL散度意义下收敛到最接近真实数据生成过程的GP；实验表明在学习曲线外推和时间序列预测基准上达到有竞争力的性能。

Conclusion: Empirical GPs提供了一种灵活、数据驱动的GP先验构造方法，显著提升高斯过程在实际回归任务中的适应性与性能。

Abstract: Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.

</details>


### [352] [Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL](https://arxiv.org/abs/2602.12087)
*Alfredo Reichlin,Adriano Pacciarelli,Danica Kragic,Miguel Vasco*

Main category: cs.LG

TL;DR: 本文提出了一种无需显式概率建模的、基于过渡感知度量空间的结构化潜在表示学习方法，用于多模态、高维、噪声观测下的鲁棒状态估计，并在强化学习任务中验证了其优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中的状态估计方法依赖需明确噪声假设的概率模型，泛化能力受限；而高维、多模态、噪声观测环境下的鲁棒状态估计是核心挑战。

Method: 提出一种基于度量空间的潜在表示学习方法，使潜在空间中状态间距离对应最小动作转移步数；引入多模态潜在转移模型与基于逆距离加权的传感器融合机制，实现无需先验噪声分布的自适应多模态融合。

Result: 在多模态RL任务上实证验证：该方法提升了对传感器噪声的鲁棒性与状态估计精度，增强了RL智能体性能，且无需显式噪声增强。

Conclusion: 利用过渡感知度量空间为序列决策中的鲁棒状态估计提供了原理清晰、可扩展的解决方案。

Abstract: Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions required to transition between them. The proposed metric space formulation provides a geometric interpretation of uncertainty without the need for explicit probabilistic modeling. To achieve this, we introduce a multimodal latent transition model and a sensor fusion mechanism based on inverse distance weighting, allowing for the adaptive integration of multiple sensor modalities without prior knowledge of noise distributions. We empirically validate the approach on a range of multimodal RL tasks, demonstrating improved robustness to sensor noise and superior state estimation compared to baseline methods. Our experiments show enhanced performance of an RL agent via the learned representation, eliminating the need of explicit noise augmentation. The presented results suggest that leveraging transition-aware metric spaces provides a principled and scalable solution for robust state estimation in sequential decision-making.

</details>


### [353] [On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and Partial Coverage](https://arxiv.org/abs/2602.12107)
*Haolin Liu,Braham Snyder,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 本文研究了在Q*近似和部分覆盖条件下的离线强化学习，指出Q*可实现性和Bellman完备性不足以保证样本高效性，并提出了一个基于决策-估计系数（DEC）的通用框架来刻画Q*函数类的内在复杂度，同时改进了软Q学习等算法的样本复杂度界，并首次分析了CQL在非表格情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决Q*可实现性和Bellman完备性是否足以保证部分覆盖下离线RL样本高效性的开放问题。

Method: 引入受在线RL中DEC启发的通用框架，提出决策-估计分解；发展新型二阶性能差异引理；分析CQL在Q*可实现性和Bellman完备性下的表现。

Result: 建立了信息论下界，证明Q*可实现性和Bellman完备性不充分；提出新DEC框架，统一并改进了先前理论结果；获得软Q学习首个ε^{-2}样本复杂度；去除Chen & Jiang (2022)对在线交互的需求；首次刻画低Bellman秩MDP的离线可学习性；首次分析非表格情形下CQL的理论保证。

Conclusion: Q*可实现性与Bellman完备性不足以保障部分覆盖下离线RL的样本效率；所提DEC框架为离线RL提供了更精细、通用且模块化的理论分析工具。

Abstract: We study offline reinforcement learning under $Q^\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: "Are $Q^\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?"
  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\star$ estimation procedures, modularizing and generalizing existing approaches.
  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\star$-realizability and Bellman completeness beyond the tabular case.

</details>


### [354] [Few-Shot Design Optimization by Exploiting Auxiliary Information](https://arxiv.org/abs/2602.12112)
*Arjun Mani,Carl Vondrick,Richard Zemel*

Main category: cs.LG

TL;DR: 本文提出了一种结合高维辅助信息和多任务历史的新贝叶斯优化框架，利用神经模型实现对新设计任务的少样本预测与快速优化，在机器人硬件设计和神经网络超参调优任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的设计问题（如硬件设计、药物发现）常涉及昂贵黑箱函数优化，而传统贝叶斯优化忽略实验中产生的丰富辅助信息（如高维观测h(x)）及跨任务历史知识，限制了样本效率。

Method: 提出一种基于神经网络的少样本预测模型，以历史任务数据为先验，利用当前任务中少量h(x)观测来预测f(x)，并嵌入到贝叶斯优化框架中进行高效搜索。

Result: 在机器人硬件设计（含新构建的大规模基准）和神经网络超参优化两个领域，该方法显著提升少样本预测精度与优化速度，性能优于多种多任务优化基线。

Conclusion: 辅助信息h(x)与跨任务历史可被有效建模并用于加速新设计任务的优化，所提神经少样本方法为复杂实验设置下的贝叶斯优化提供了新范式。

Abstract: Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to real-world experimental setups, where experiments often generate a wealth of useful information. We introduce a new setting where an experiment generates high-dimensional auxiliary information $h(x)$ along with the performance measure $f(x)$; moreover, a history of previously solved tasks from the same task family is available for accelerating optimization. A key challenge of our setting is learning how to represent and utilize $h(x)$ for efficiently solving new optimization tasks beyond the task history. We develop a novel approach for this setting based on a neural model which predicts $f(x)$ for unseen designs given a few-shot context containing observations of $h(x)$. We evaluate our method on two challenging domains, robotic hardware design and neural network hyperparameter tuning, and introduce a novel design problem and large-scale benchmark for the former. On both domains, our method utilizes auxiliary feedback effectively to achieve more accurate few-shot prediction and faster optimization of design tasks, significantly outperforming several methods for multi-task optimization.

</details>


### [355] [KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite](https://arxiv.org/abs/2602.12117)
*Jiakang Shen,Qinghui Chen,Runtong Wang,Chenrui Xu,Jinglin Zhang,Cong Bai,Feng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级多模态模型KAN-FIF，结合MLP、CNN与样条参数化的Kolmogorov-Arnold网络层，显著降低参数量与推理延迟，同时提升热带气旋最大持续风速（MSW）预测精度，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有热带气旋监测方法在边缘设备上计算效率低、参数量大；物理引导模型受限于线性特征交互，难以建模高阶非线性关系，导致模型臃肿且硬件兼容性差。

Method: 提出Kolmogorov-Arnold Network-based Feature Interaction Framework（KAN-FIF），融合MLP、CNN与样条参数化的KAN层，构建轻量级多模态架构，用于MSW预测，并在FY-4卫星数据及Qingyun-1000开发板上开展离线部署验证。

Result: 相比基线模型Phy-CoCo，KAN-FIF参数量减少94.8%（0.99MB vs 19MB），单样本推理加速68.7%（2.3ms vs 7.35ms），MAE降低32.5%；在Qingyun-1000板上实现14.41ms/样本的推理延迟。

Conclusion: KAN-FIF在保持高预测精度的同时极大提升了模型轻量化与边缘部署能力，为业务化热带气旋实时监测及边缘AI气象应用提供了可行新路径。

Abstract: Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-order polynomial relationships between TC attributes, leading to inflated model sizes and hardware incompatibility. To overcome these challenges, this study introduces the Kolmogorov-Arnold Network-based Feature Interaction Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves a $94.8\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\%$ faster inference per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining superior accuracy with $32.5\%$ lower MAE. The offline deployment experiment of the FY-4 series meteorological satellite processor on the Qingyun-1000 development board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework, demonstrating promising feasibility for operational TC monitoring and extending deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.

</details>


### [356] [Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-Form Solutions](https://arxiv.org/abs/2602.12139)
*Yashas Shende,Aritra Das,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种结合线性阻尼谐振子模型与Transformer的新型架构，以解决不规则时间序列建模中神经ODE计算开销大的问题；通过闭式解实现高效、可证明的连续时间注意力机制，并在性能和速度上均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法难以处理不规则时间序列，而神经ODE虽能建模连续演化但依赖计算昂贵的数值求解器；需兼顾建模能力与计算效率。

Method: 用具有解析解的线性阻尼谐振子替代NODEs，将keys/values建模为受驱阻尼振荡器，queries展开为有限阶正弦基；将注意力机制解释为共振现象，并利用闭式解避免数值ODE求解。

Result: 在不规则时间序列基准上达到SOTA性能，同时比ContiFormer快数个数量级；理论证明其保持连续时间注意力的通用逼近能力。

Conclusion: 该振荡器驱动的连续注意力机制在保证理论完备性的同时显著提升效率，为不规则时间序列建模提供了可扩展且可解释的新范式。

Abstract: Transformers excel at time series modelling through attention mechanisms that capture long-term temporal patterns. However, they assume uniform time intervals and therefore struggle with irregular time series. Neural Ordinary Differential Equations (NODEs) effectively handle irregular time series by modelling hidden states as continuously evolving trajectories. ContiFormers arxiv:2402.10635 combine NODEs with Transformers, but inherit the computational bottleneck of the former by using heavy numerical solvers. This bottleneck can be removed by using a closed-form solution for the given dynamical system - but this is known to be intractable in general! We obviate this by replacing NODEs with a novel linear damped harmonic oscillator analogy - which has a known closed-form solution. We model keys and values as damped, driven oscillators and expand the query in a sinusoidal basis up to a suitable number of modes. This analogy naturally captures the query-key coupling that is fundamental to any transformer architecture by modelling attention as a resonance phenomenon. Our closed-form solution eliminates the computational overhead of numerical ODE solvers while preserving expressivity. We prove that this oscillator-based parameterisation maintains the universal approximation property of continuous-time attention; specifically, any discrete attention matrix realisable by ContiFormer's continuous keys can be approximated arbitrarily well by our fixed oscillator modes. Our approach delivers both theoretical guarantees and scalability, achieving state-of-the-art performance on irregular time series benchmarks while being orders of magnitude faster.

</details>


### [357] [It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks](https://arxiv.org/abs/2602.12147)
*Zhongzheng Qiao,Sheng Pan,Anni Wang,Viktoriya Zhukova,Yong Liu,Xudong Jiang,Qingsong Wen,Mingsheng Long,Ming Jin,Chenghao Liu*

Main category: cs.LG

TL;DR: 本文提出TIME基准，旨在解决现有时间序列基础模型（TSFM）评估中数据组成受限、数据质量低、任务设定脱离实际、分析视角僵化四大问题；该基准包含50个新数据集、98个预测任务，支持严格零样本评估，并引入人类参与+大模型协同的构建流程与基于时序模式特征的新型评估视角。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM基准在数据组成、数据完整性、任务设定和分析视角四方面存在明显局限，难以支撑真正通用、鲁棒、可落地的模型评估。

Method: 构建了任务中心型基准TIME：1）采集50个全新数据集、定义98个面向真实场景的预测任务；2）采用人类专家与大语言模型协同的‘人在回路’流程保障数据质量与任务合理性；3）提出基于结构化时序特征的模式级评估方法，替代传统静态元标签的数据集级评估。

Result: 在TIME上对12个代表性TSFM进行了零样本评估，建立了多粒度排行榜，揭示了模型在不同时间模式下的泛化能力差异；排行榜已开源发布。

Conclusion: TIME为TSFM提供了更严格、更真实、更具洞察力的评估范式，推动时间序列建模从‘数据驱动’走向‘任务与模式驱动’。

Abstract: Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.

</details>


### [358] [SafeNeuron: Neuron-Level Safety Alignment for Large Language Models](https://arxiv.org/abs/2602.12158)
*Zhaoxin Wang,Jiaming Liang,Fengbin Zhu,Weixiang Zhao,Junfeng Fang,Jiayi Ji,Handing Wang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 本文提出SafeNeuron，一种在神经元层面进行安全对齐的新框架，通过识别并冻结安全相关神经元，在偏好优化中促使模型构建冗余的安全表征，从而提升对抗神经元级攻击的鲁棒性，并保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法多在行为层面操作，控制力弱；且研究表明安全行为集中在少量参数上，导致对齐脆弱、易被神经元级攻击绕过。

Method: SafeNeuron首先识别安全相关神经元，然后在偏好优化过程中冻结这些神经元，迫使模型在全网范围内构建冗余、分布式的安全表征。

Result: 实验表明SafeNeuron显著提升了模型对神经元剪枝攻击的鲁棒性，降低了开源模型被滥用于红队生成的风险，并保持了通用能力；层分析揭示安全行为由稳定、共享的内部表征驱动。

Conclusion: SafeNeuron提供了一种可解释、鲁棒的神经元级安全对齐新范式，推动对齐从行为层深入到机制层。

Abstract: Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.

</details>


### [359] [Amortized Molecular Optimization via Group Relative Policy Optimization](https://arxiv.org/abs/2602.12162)
*Muhammad bin Javaid,Hasham Hussain,Ashima Khanna,Berke Kisin,Jonathan Pirnay,Alexander Mitsos,Dominik G. Grimm,Martin Grohe*

Main category: cs.LG

TL;DR: 本文提出GRXForm，一种基于预训练图Transformer的分子结构优化方法，通过Group Relative Policy Optimization（GRPO）进行目标导向微调，有效缓解了不同起始分子结构难度异质性带来的高方差问题，在跨分布分子骨架上实现了无需推理时调用oracle或后处理的泛化优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子结构优化方法多为‘实例优化器’，对每个输入结构需重新启动搜索，计算开销大；而模型驱动方法虽具摊销效率潜力，却因起始结构难度差异大导致高方差，泛化能力差。

Method: 基于预训练Graph Transformer模型，采用序列式原子与键添加策略进行分子优化，并引入Group Relative Policy Optimization（GRPO）进行目标导向微调，通过将奖励相对于起始结构归一化来降低方差。

Result: GRXForm在跨分布分子骨架上实现良好泛化，无需推理时oracle调用或后续优化，其在多目标优化任务中的性能与当前领先实例优化器相当。

Conclusion: GRXForm通过结构化建模与方差抑制策略，显著提升了分子结构优化模型的泛化性与实用性，为模型驱动的分子设计提供了新范式。

Abstract: Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as "Instance Optimizers'', expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.

</details>


### [360] [How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180)
*Yurong Chen,Yu He,Michael I. Jordan,Fan Yao*

Main category: cs.LG

TL;DR: 本文研究了大语言模型偏好对齐中采样策略和参考策略选择的影响，通过Identity Preference Optimization框架分析发现，实例依赖的采样可增强排序保证，而偏向策略的采样可能导致过度集中；进一步分析迭代对齐动态，证明其可能出现持续振荡或熵崩溃，并刻画了保证稳定性的参数区域；理论结果也适用于Direct Preference Optimization，实验验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 标准的偏好对齐方法依赖于候选响应的成对比较和参考策略正则化，但采样与参考策略选择的影响缺乏理论理解。

Method: 基于Identity Preference Optimization框架进行理论分析，研究不同采样策略（如实例依赖采样、偏向策略采样）的影响，并建模迭代对齐动态，分析其稳定性、振荡与熵崩溃行为；理论扩展至Direct Preference Optimization，并通过真实偏好数据实验验证。

Result: 发现实例依赖采样能提供更强的排序保证，而偏向策略采样在结构化偏好下易导致过度集中；迭代对齐动态可能呈现持续振荡或熵崩溃，且存在可保证稳定的参数区域；理论现象在Direct Preference Optimization中同样存在。

Conclusion: 采样策略与参考策略的选择对偏好对齐效果有本质影响，需谨慎设计以避免不稳定行为；所揭示的现象具有普适性，适用于多种偏好对齐方法。

Abstract: Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.

</details>


### [361] [WaveFormer: Wavelet Embedding Transformer for Biomedical Signals](https://arxiv.org/abs/2602.12189)
*Habib Irani,Bikram De,Vangelis Metsis*

Main category: cs.LG

TL;DR: WaveFormer是一种结合小波分解的新型Transformer架构，用于提升生物医学信号分类性能，通过在嵌入构建和位置编码两个阶段引入小波分析，有效融合时频域信息。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在处理长序列、复杂时序动态和多尺度频率模式的生物医学信号时表现不佳，亟需增强其频率感知能力。

Method: 提出WaveFormer：在嵌入构造阶段使用多通道离散小波变换（DWT）提取时频联合特征生成token；在位置编码阶段引入动态小波位置编码（DyWPE），基于单通道DWT自适应建模信号特异性时序结构。

Result: 在涵盖人体活动识别与脑电信号分析的8个多样化数据集上验证，序列长度50–3000，通道数1–144，WaveFormer展现出具有竞争力的分类性能。

Conclusion: WaveFormer为将频率域先验知识系统性融入基于Transformer的时间序列分类提供了原理清晰、可扩展的框架。

Abstract: Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.

</details>


### [362] [Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction](https://arxiv.org/abs/2602.12204)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种基于记忆巩固的自适应注意力路由机制CRAM，通过模拟人脑从情景记忆到语义记忆的转化过程，显著降低注意力计算开销，同时保持甚至提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合状态空间与注意力模型要么均匀应用注意力，要么学习静态稀疏模式，忽略了注意力需求应随时间推移、模式重复而减少这一关键现象；作者发现GPT-2中88%的注意力操作是冗余的，且该冗余在训练中不减少。

Method: 提出CRAM（Consolidation-based Routing for Adaptive Memory），一种受生物学启发的记忆巩固机制，将 episodic retrieval 逐步蒸馏为参数化语义记忆；其注意力使用率随训练动态下降，并经历约3K步的锐利相变；理论证明无记忆巩固则无法实现同等效率。

Result: 在SRCD基准上，CRAM以1.6%的注意力计算量达到100%检索准确率（基线为68%）；迁移至未见任务时无需重训练即可减少48–52%注意力开销；学习到的巩固动力学曲线与人类认知心理学数据高度吻合（γ=0.43 vs. 0.4–0.5）。

Conclusion: 记忆巩固是实现高效、自适应注意力的关键机制；CRAM不仅大幅压缩注意力计算，还展现出跨任务泛化能力与认知可解释性，为构建更高效、更类人的大模型架构提供了新范式。

Abstract: Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \textbf{88\%} of attention operations retrieve information already predictable from the model's hidden state, and this redundancy does \emph{not} decrease during training. Motivated by this observation, we introduce \textbf{\ours{}} (\textbf{C}onsolidation-based \textbf{R}outing for \textbf{A}daptive \textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \ours{} exhibits \emph{decreasing attention utilization} over training, achieving a \textbf{37.8$\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \emph{impossible} without consolidation: any static routing scheme requires $Ω(f \cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \ours{} achieves \textbf{100\% retrieval accuracy} at 1.6\% attention compute (vs.\ 68\% for baselines), and consolidated patterns transfer to unseen tasks with \textbf{48--52\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($γ= 0.43$ vs.\ $γ_{\text{human}} \approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].

</details>


### [363] [The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics](https://arxiv.org/abs/2602.12218)
*Christian Internò,Jumpei Yamaguchi,Loren Amdahl-Culleton,Markus Olhofer,David Klindt,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种非侵入式评估协议PhyIP，用于检测神经模型是否在自监督学习中内化了物理定律，而非依赖统计捷径；通过在线性可解性上评估冻结表征，发现低误差SSL能线性暴露物理结构，而基于适应的评估反而会掩盖该结构。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如微调或高容量探针）会改变表征，从而混淆自监督学习阶段实际学到的内容，难以判断模型是否真正内化物理规律。

Method: 提出PhyIP协议：在不修改模型参数的前提下，使用低容量（线性）探针，从冻结的自监督学习表征中解码物理量，验证线性表征假设。

Result: 在流体动力学和轨道力学任务中，当SSL误差较低时，物理量（如内能、牛顿平方反比律）在线性探针下表现出高可解性（ρ>0.90）；而适应式评估则严重破坏该结构（ρ≈0.05）。

Conclusion: 低容量线性探针比基于适应的评估更能真实反映模型是否构建了符合物理规律的世界模型；PhyIP为评估神经网络的物理归纳偏置提供了更可靠的方法。

Abstract: Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.

</details>


### [364] [Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser](https://arxiv.org/abs/2602.12229)
*Zijing Ou,Jacob Si,Junyi Zhu,Ondrej Bohdal,Mete Ozay,Taha Ceritli,Yingzhen Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为方差最小化策略优化（VMPO）的新方法，将扩散对齐建模为最小化重要性权重对数的方差，而非传统的KL散度目标，并从顺序蒙特卡洛（SMC）视角统一理解现有方法。


<details>
  <summary>Details</summary>
Motivation: 受扩散对齐在顺序蒙特卡洛（SMC）框架下可被解释为使用去噪模型作为建议分布、奖励引导产生重要性权重这一观察启发，作者希望构建一个更统一、更具解释性的优化目标。

Method: 提出VMPO方法，将扩散对齐形式化为最小化对数重要性权重的方差；理论上证明该目标在奖励倾斜目标分布下取得最小值，且其梯度在on-policy采样下与KL对齐一致；通过不同势函数和方差最小化策略的组合，复现并推广多种现有方法。

Result: VMPO提供了一个统一视角来理解扩散对齐；能复现多种现有方法，并启发超越KL目标的新设计方向；理论证明了其最优性及与KL梯度的一致性。

Conclusion: 最小化对数重要性权重方差是一种比直接优化KL更本质、更灵活的扩散对齐原则，为该领域提供了新的理论基础与方法论启示。

Abstract: Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.

</details>


### [365] [Categorical Flow Maps](https://arxiv.org/abs/2602.12233)
*Daan Roos,Oscar Davis,Floor Eijkelboom,Michael Bronstein,Max Welling,İsmail İlkan Ceylan,Luca Ambrogioni,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 本文提出了一种名为Categorical Flow Maps的流匹配方法，用于通过自蒸馏加速分类数据的少步生成，实现了图像、分子图和文本等任务上的SOTA性能，甚至在单步生成中也表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散和流模型在分类数据生成中推理速度慢的问题，推动少步甚至单步生成的高效实现。

Method: 基于变分流匹配框架，定义朝向单纯形的连续流映射，结合自蒸馏与新提出的端点一致性目标进行训练，并支持测试时使用现有引导和重加权技术。

Result: 在图像、分子图和文本生成任务上达到少步生成的最先进水平，单步生成亦具强竞争力。

Conclusion: Categorical Flow Maps为分类数据生成提供了高效、灵活且可扩展的流匹配范式，兼具训练稳定性和推理可控性。

Abstract: We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.

</details>


### [366] [Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces](https://arxiv.org/abs/2602.12245)
*Anthony Kobanda,Waris Radji*

Main category: cs.LG

TL;DR: 本文建立了联合嵌入预测架构（JEPA）与拟度量强化学习（QRL）之间的理论联系，指出当JEPA采用基于最小作用原理的内蕴能量函数时，该能量自然构成拟度量，并与QRL中的代价到目标函数在形式和功能上一致；同时论证了对称能量在单向可达性任务中的结构性不匹配，从而为使用非对称拟度量能量提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 弥合JEPA（侧重表征学习）与QRL（侧重目标导向控制）之间的理论鸿沟，解释为何在具有方向性的动态系统中应采用非对称能量函数。

Method: 通过引入内蕴（最小作用）能量函数类，结合闭包性与可加性假设，从数学上证明其满足拟度量公理；并分析最优代价到目标函数与JEPA建模目标之间的一致性。

Result: 证明任意满足条件的内蕴能量必为拟度量；最优成本到目标函数具有内蕴形式；JEPA若建模此类能量，则其学习的目标函数属于QRL所关注的拟度量值函数类；对称有限能量不适用于单向可达场景。

Conclusion: JEPA与QRL在内蕴能量框架下本质统一，拟度量是连接表征学习与目标导向控制的自然几何结构，方向性要求决定了能量函数必须是非对称的。

Abstract: Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.

</details>


### [367] [ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction](https://arxiv.org/abs/2602.12247)
*Nick Ferguson,Josh Pennington,Narek Beghian,Aravind Mohan,Douwe Kiela,Sheshansh Agrawal,Thien Hang Nguyen*

Main category: cs.LG

TL;DR: 本文提出了ExtractBench，一个面向PDF到JSON结构化信息抽取的开源基准测试与评估框架，填补了企业级宽模式评估和嵌套语义抽取评估方法的空白；实验表明当前前沿大模型在复杂真实Schema下仍不可靠，尤其在宽Schema（如369字段）上完全失效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM用于PDF结构化信息抽取时，缺乏端到端、覆盖企业级宽Schema的基准，也缺乏能区分不同字段语义正确性（如精确匹配、数值容差、语义等价、数组对齐、遗漏vs幻觉）的评估方法。

Method: 构建ExtractBench：包含35个PDF文档、对应JSON Schema及人工标注金标，覆盖高价值领域，共12,867个可评估字段；评估框架将Schema作为可执行规范，每个字段声明其专属评分指标（如exact match、fuzzy match、semantic equivalence等）。

Result: 前沿模型（GPT-5/5.2、Gemini-3、Claude 4.5）在宽Schema下性能急剧下降，在369字段金融报表Schema上全部模型输出0%有效JSON；验证了ExtractBench对模型能力的敏感区分能力。

Conclusion: ExtractBench为PDF-to-JSON抽取提供了首个兼具规模性、语义细粒度和可执行性的评估标准，揭示了当前LLM在真实企业场景中的关键可靠性瓶颈，并已开源以推动该方向研究。

Abstract: Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.

</details>


### [368] [Community Concealment from Unsupervised Graph Learning-Based Clustering](https://arxiv.org/abs/2602.12250)
*Dalyapraz Manatova,Pablo Moriano,L. Jean Camp*

Main category: cs.LG

TL;DR: 本文提出了一种针对图神经网络（GNN）社区学习的防御方法，通过有限且效用感知的图结构与节点特征扰动，降低目标社区在GNN消息传递中的可区分性，从而实现群体级隐私保护。实验表明该方法在多种图数据上相较DICE提升20–45%的隐蔽效果。


<details>
  <summary>Details</summary>
Motivation: GNN在无监督社区检测中可能暴露敏感群体或系统依赖关系，引发群体级隐私风险，亟需在保持数据效用前提下隐藏特定社区。

Method: 基于社区边界连通性和邻近社区特征相似性两个关键因素，设计联合边重连与节点特征扰动策略，削弱GNN消息传递对目标社区的辨识能力。

Result: 在合成与真实图数据上，该方法在相同扰动预算下显著优于DICE，中位相对隐蔽性能提升达20–45%。

Conclusion: 社区隐蔽效果高度依赖于边界连通性与跨社区特征相似性；所提扰动策略有效缓解GNN带来的群体隐私风险，为图数据发布提供实用化隐私防护方案。

Abstract: Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.

</details>


### [369] [Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data](https://arxiv.org/abs/2602.12267)
*Duy Nguyen,Jiachen Yao,Jiayun Wang,Julius Berner,Animashree Anandkumar*

Main category: cs.LG

TL;DR: 本文提出Flow-Guided Neural Operator (FGNO)，一种结合算子学习与流匹配的自监督学习框架，通过将污染程度作为可学习自由度，在功能空间中建模时序数据，实现从干净输入提取鲁棒表征，显著提升多个生物医学时序任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有掩码自编码器等SSL方法依赖固定掩码比例，缺乏灵活性；同时，生成式SSL在推理时使用噪声输入导致随机性与性能下降。

Method: 提出FGNO框架：利用短时傅里叶变换统一多时间分辨率，结合流匹配动态控制噪声强度，并在不同网络层和流时间提取多层次特征；训练时用噪声输入，推理时用干净输入。

Result: 在BrainTreeBank、DREAMT和SleepEDF三个生物医学数据集上显著优于基线：AUROC提升35%，RMSE降低16%，低数据下准确率与macro-F1提升超20%。

Conclusion: FGNO通过引入污染水平为可学习变量并解耦训练/推理输入，提升了时序表征的表达力与鲁棒性，尤其适用于小样本生物医学场景。

Abstract: Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO's robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.

</details>


### [370] [Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage](https://arxiv.org/abs/2602.12274)
*Xin Ju,Jiachen Yao,Anima Anandkumar,Sally M. Benson,Gege Wen*

Main category: cs.LG

TL;DR: 本文提出Fun-DDPS，一种结合函数空间扩散模型与可微神经算子代理的生成框架，用于解决碳捕集与封存（CCS）中因观测稀疏导致的反演问题病态性，显著提升参数重建精度与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 地下流动准确表征对碳捕集与封存（CCS）至关重要，但受限于反演问题的病态性及观测数据稀疏，传统方法性能受限。

Method: 提出Fun-DDPS框架：用单通道函数空间扩散模型学习地质参数先验分布，并耦合局部神经算子（LNO）代理模型提供物理一致的动力学场条件指导；通过解耦先验建模与物理引导，实现鲁棒参数补全与高效数据同化。

Result: 在合成CCS数据集上：(1) 前向建模仅用25%观测时相对误差为7.7%，较标准代理模型（86.9%）提升11倍；(2) 首次以拒绝采样（RS）后验为基准验证扩散反演器，Fun-DDPS与基线Fun-DPS的JS散度均<0.06，且生成样本无高频伪影、物理一致性更优，采样效率达RS的4倍。

Conclusion: Fun-DDPS有效缓解了稀疏观测下CCS反演的病态性，在精度、物理一致性与计算效率三方面取得显著突破，为基于生成式建模的地球物理反演提供了新范式。

Abstract: Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [371] [Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering](https://arxiv.org/abs/2602.11183)
*Yin Tang,Jiawei Ma,Jinrui Zhang,Alex Jinpeng Wang,Deyu Zhang*

Main category: cs.RO

TL;DR: 本文提出NeuroKalman框架，将无人机视觉语言导航建模为递归贝叶斯状态估计问题，通过先验预测（运动动力学）与似然校正（历史观测检索）解耦导航过程，有效缓解状态漂移，仅用10%微调数据即显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有VLN模型采用死 reckoning式逐步预测，导致位置误差累积、状态漂移，使内部信念与真实坐标失对齐。

Method: 提出NeuroKalman框架，将导航解耦为基于运动动力学的先验预测和基于历史观测的似然校正；将核密度估计与注意力检索机制数学关联，实现无需梯度更新的历史锚点引导表征校正。

Result: 在TravelUAV基准上，仅用10%训练数据微调，即明显超越强基线，有效抑制漂移累积。

Conclusion: 将导航建模为递归贝叶斯估计并引入历史观测驱动的无梯度校正，是缓解VLN中状态漂移的有效新范式。

Abstract: Continuous navigation in complex environments is critical for Unmanned Aerial Vehicle (UAV). However, the existing Vision-Language Navigation (VLN) models follow the dead-reckoning, which iteratively updates its position for the next waypoint prediction, and subsequently construct the complete trajectory. Then, such stepwise manner will inevitably lead to accumulated errors of position over time, resulting in misalignment between internal belief and objective coordinates, which is known as "state drift" and ultimately compromises the full trajectory prediction. Drawing inspiration from classical control theory, we propose to correct for errors by formulating such sequential prediction as a recursive Bayesian state estimation problem. In this paper, we design NeuroKalman, a novel framework that decouples navigation into two complementary processes: a Prior Prediction, based on motion dynamics and a Likelihood Correction, from historical observation. We first mathematically associate Kernel Density Estimation of the measurement likelihood with the attention-based retrieval mechanism, which then allows the system to rectify the latent representation using retrieved historical anchors without gradient updates. Comprehensive experiments on TravelUAV benchmark demonstrate that, with only 10% of the training data fine-tuning, our method clearly outperforms strong baselines and regulates drift accumulation.

</details>


### [372] [H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model](https://arxiv.org/abs/2602.11291)
*Wenyuan Chen,Jinbang Huang,Oscar Pang,Zhiyuan Li,Xiao Hu,Lingfeng Zhang,Zhanguang Zhang,Mark Coates,Tongtong Cao,Xingyue Quan,Yingxue Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种分层世界模型（H-WM），联合预测逻辑与视觉状态转移，融合符号推理的长程鲁棒性与视觉感知的接地能力，缓解误差累积，提升长时序机器人任务执行的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型（如视频或语言预测）难以直接对接机器人动作且易产生长程误差；而传统符号规划虽鲁棒但缺乏视觉感知同步，二者存在割裂。

Method: 构建双层统一框架：高层为逻辑世界模型（支持机器人执行与长程推理），底层为视觉世界模型（基于视觉观测）；并构建对齐运动、符号状态、动作与视觉观测的机器人数据集用于训练。

Result: 在多个视觉-语言-动作（VLA）控制策略上验证了H-WM的有效性与通用性，显著提升了长时序任务中状态预测的稳定性与执行鲁棒性。

Conclusion: H-WM成功桥接符号逻辑与视觉感知，为具身智能提供兼具可执行性、长程鲁棒性与感知接地性的世界建模新范式。

Abstract: World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

</details>


### [373] [ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control](https://arxiv.org/abs/2602.11321)
*Ziyan Xiong,Lixing Fang,Junyun Huang,Kashu Yamazaki,Hao Zhang,Chuang Gan*

Main category: cs.RO

TL;DR: 本文提出ExtremControl，一种低延迟全身控制框架，通过直接操作选定刚体链路的SE(3)姿态、笛卡尔空间映射及速度前馈控制，显著降低人形机器人遥操作延迟至50ms，支持快速反应任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高预处理的人-机器人运动重定向和仅位置PD控制，导致高延迟，限制了需快速反馈的任务。

Method: ExtremControl框架：(1) 直接操作选定刚体（主要是末端）的SE(3)姿态；(2) 采用笛卡尔空间映射将人体动作直接转为机器人链路目标；(3) 引入底层速度前馈控制以提升响应性；并给出统一理论建模与仿真+实物实验验证。

Result: 实现端到端延迟低至50ms的遥操作系统，支持光学动捕与VR追踪，在乒乓球平衡、杂耍、实时回击等任务中展现出高度响应性，远超先前200ms的延迟上限。

Conclusion: ExtremControl有效解决了人形机器人遥操作中的高延迟问题，为收集动态、反应式演示提供了实用、低延迟的控制基础。

Abstract: Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.

</details>


### [374] [MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation](https://arxiv.org/abs/2602.11337)
*Yejin Kim,Wilbert Pumacay,Omar Rayyan,Max Argus,Winson Han,Eli VanderBilt,Jordi Salvador,Abhay Deshpande,Rose Hendrix,Snehal Jauhri,Shuo Liu,Nur Muhammad Mahi Shafiullah,Maya Guru,Ainaz Eftekhar,Karen Farley,Donovan Clay,Jiafei Duan,Arjun Guru,Piper Wolters,Alvaro Herrasti,Ying-Chun Lee,Georgia Chalvatzaki,Yuchen Cui,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: 本文提出了MolmoSpaces，一个开源的机器人策略大规模基准测试生态系统，包含23万多个多样化室内环境和13万个带丰富标注的物体资产，支持多种仿真器，并设计了8个任务的基准套件MolmoSpaces-Bench，实验表明其具有强仿真到现实的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基准难以覆盖真实环境中场景布局、物体几何形状和任务规格的海量变化，亟需更大规模、更多样化的评估基础设施。

Method: 构建了MolmoSpaces生态系统，包括230k+多样化室内环境（手工设计与程序生成）、130k带标注物体资产（含48k可操作物体及42M稳定抓取位姿），支持多仿真器（MuJoCo/Isaac/ManiSkill）；设计了涵盖静态/移动操作、导航及多房间长程任务的MolmoSpaces-Bench基准套件（8个任务）。

Result: MolmoSpaces-Bench展现出强sim-to-real相关性（R=0.96, ρ=0.98）；新零样本策略性能优于旧版本；识别出对提示词表述、初始关节位置和相机遮挡的关键敏感性。

Conclusion: MolmoSpaces为机器人学习研究提供了可扩展的数据生成、策略训练与基准构建基础，推动面向真实世界长尾场景的鲁棒机器人策略发展。

Abstract: Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.

</details>


### [375] [Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video](https://arxiv.org/abs/2602.11393)
*Mrinal Verghese,Christopher G. Atkeson*

Main category: cs.RO

TL;DR: 本文提出了一种从第一人称人类视频中学习机器人行为的新方法，通过建模人类偏好构建奖励函数，并结合改进的SAC算法在真实机器人上直接优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类视频的奖励学习方法依赖于视频中状态到终止状态的时间距离来估计长期价值，存在对视频数据假设过强、难以跨具身与环境迁移等问题。

Method: 通过预测连续图像间跟踪点的运动来建模人类偏好，将预测运动与机器人实际观测到的物体运动之间的一致性定义为每步奖励；再使用经10次真实机器人演示初始化的改进Soft Actor Critic（SAC）算法，在机器人端联合学习值函数并优化策略。

Result: 该方法可在真实机器人上端到端学习；在多个任务中，其学习到的策略在仿真和真实机器人上均达到或超越先前方法的性能。

Conclusion: 基于运动一致性建模人类偏好并直接在机器人上优化策略，是一种更鲁棒、可迁移且实用的机器人视频模仿学习范式。

Abstract: We present an approach to robot learning from egocentric human videos by modeling human preferences in a reward function and optimizing robot behavior to maximize this reward. Prior work on reward learning from human videos attempts to measure the long-term value of a visual state as the temporal distance between it and the terminal state in a demonstration video. These approaches make assumptions that limit performance when learning from video. They must also transfer the learned value function across the embodiment and environment gap. Our method models human preferences by learning to predict the motion of tracked points between subsequent images and defines a reward function as the agreement between predicted and observed object motion in a robot's behavior at each step. We then use a modified Soft Actor Critic (SAC) algorithm initialized with 10 on-robot demonstrations to estimate a value function from this reward and optimize a policy that maximizes this value function, all on the robot. Our approach is capable of learning on a real robot, and we show that policies learned with our reward model match or outperform prior work across multiple tasks in both simulation and on the real robot.

</details>


### [376] [EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos](https://arxiv.org/abs/2602.11464)
*Tao Zhang,Song Xia,Ye Wang,Qin Jin*

Main category: cs.RO

TL;DR: EasyMimic是一种低成本、可复现的机器人模仿学习框架，利用普通RGB视频提取3D手部轨迹，通过动作对齐与视觉增强，结合人机数据协同训练，使低成本家用机器人能快速从人类视频示范中学习操作技能。


<details>
  <summary>Details</summary>
Motivation: 解决低成本家用机器人因真实世界数据采集成本高而难以开展模仿学习的问题。

Method: 1）从RGB视频中提取3D手部轨迹；2）动作对齐模块将轨迹映射至机器人夹爪控制空间；3）引入手部视觉增强策略缩小人-机域差距；4）采用人机数据协同训练（co-training）进行模型微调。

Result: 在低成本LeRobot平台上实验表明，EasyMimic在多种操作任务中性能优异，显著降低对昂贵机器人真机数据的依赖。

Conclusion: EasyMimic为低成本家用机器人提供了实用、高效、易部署的视频到动作模仿学习路径，推动智能机器人走入家庭。

Abstract: Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.

</details>


### [377] [Effective Task Planning with Missing Objects using Learning-Informed Object Search](https://arxiv.org/abs/2602.11468)
*Raihan Islam Arnob,Max Merlin,Abhishek Paudel,Benned Hedegaard,George Konidaris,Gregory Stein*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的LIOS动作框架，将学习驱动的目标搜索与高层任务规划结合，在环境信息不完全时实现有效、可靠且完备的任务规划。


<details>
  <summary>Details</summary>
Motivation: 现有任务规划方法（如PDDL）依赖完整环境知识，无法处理关键物体位置未知的情况；而现有学习式搜索方法又难以嵌入全任务规划流程中，无法协同决定‘需找何物’及‘何时搜索’。

Method: 提出新型模型-based LIOS（Learned-Informed Object Search）动作——每个LIOS动作是一个用于定位并取回单个物体的策略；高层规划将LIOS动作视为确定性动作，并基于其预期成本建模进行规划，从而在不确定性下实现搜索与执行的动态交织。

Result: 在仿真ProcTHOR环境和真实世界实验中，该方法在物体检索和餐食准备等任务上均优于非学习型与学习型基线方法。

Conclusion: LIOS框架在保留与传统全知识求解器兼容性的同时，实现了对不确定性的有效建模与推理，推动了学习与符号规划的深度融合。

Abstract: Task planning for mobile robots often assumes full environment knowledge and so popular approaches, like planning via the PDDL, cannot plan when the locations of task-critical objects are unknown. Recent learning-driven object search approaches are effective, but operate as standalone tools and so are not straightforwardly incorporated into full task planners, which must additionally determine both what objects are necessary and when in the plan they should be sought out. To address this limitation, we develop a planning framework centered around novel model-based LIOS actions: each a policy that aims to find and retrieve a single object. High-level planning treats LIOS actions as deterministic and so -- informed by model-based calculations of the expected cost of each -- generates plans that interleave search and execution for effective, sound, and complete learning-informed task planning despite uncertainty. Our work effectively reasons about uncertainty while maintaining compatibility with existing full-knowledge solvers. In simulated ProcTHOR homes and in the real world, our approach outperforms non-learned and learned baselines on tasks including retrieval and meal prep.

</details>


### [378] [HyperDet: 3D Object Detection with Hyper 4D Radar Point Clouds](https://arxiv.org/abs/2602.11554)
*Yichun Xiao,Runwei Guan,Fangqiang Ding*

Main category: cs.RO

TL;DR: 本文提出HyperDet框架，通过构建任务感知的超4D雷达点云，提升纯雷达3D检测性能，缩小与激光雷达系统的差距。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达虽具有天气鲁棒性、速度感知能力和成本优势，但其点云稀疏、不规则且易受多径噪声干扰，导致几何结构弱且不稳定，使得纯雷达3D检测性能落后于激光雷达系统。

Method: HyperDet是一种检测器无关的纯雷达3D检测框架：1）跨多帧和环视4D雷达聚合回波以提升覆盖与密度；2）在非重叠区域采用轻量自一致性检验进行几何感知跨传感器共识验证，抑制不一致回波；3）引入前景聚焦的扩散模块，结合训练时混合雷达-激光雷达监督，增强目标结构并提升雷达属性（如多普勒、RCS），再蒸馏为单步推理的一致性模型。

Result: 在MAN TruckScenes数据集上，HyperDet在VoxelNeXt和CenterPoint等LiDAR导向检测器上持续优于原始雷达输入，部分缩小了雷达与激光雷达之间的性能差距。

Conclusion: 输入级优化可使雷达数据更有效地适配现有LiDAR导向检测器，无需修改网络架构，验证了提升雷达点云质量对下游任务的关键作用。

Abstract: 4D mmWave radar provides weather-robust, velocity-aware measurements and is more cost-effective than LiDAR. However, radar-only 3D detection still trails LiDAR-based systems because radar point clouds are sparse, irregular, and often corrupted by multipath noise, yielding weak and unstable geometry. We present HyperDet, a detector-agnostic radar-only 3D detection framework that constructs a task-aware hyper 4D radar point cloud for standard LiDAR-oriented detectors. HyperDet aggregates returns from multiple surround-view 4D radars over consecutive frames to improve coverage and density, then applies geometry-aware cross-sensor consensus validation with a lightweight self-consistency check outside overlap regions to suppress inconsistent returns. It further integrates a foreground-focused diffusion module with training-time mixed radar-LiDAR supervision to densify object structures while lifting radar attributes (e.g., Doppler, RCS); the model is distilled into a consistency model for single-step inference. On MAN TruckScenes, HyperDet consistently improves over raw radar inputs with VoxelNeXt and CenterPoint, partially narrowing the radar-LiDAR gap. These results show that input-level refinement enables radar to better leverage LiDAR-oriented detectors without architectural modifications.

</details>


### [379] [ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles](https://arxiv.org/abs/2602.11575)
*Seungyeon Yoo,Youngseok Jang,Dabin Kim,Youngsoo Han,Seungwoo Jung,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出ReaDy-Go，一种面向动态环境的real-to-sim视觉导航仿真新范式，通过融合静态3D高斯场景与可动画人类高斯障碍物，生成逼真动态导航数据集，提升策略在sim-to-real迁移及动态障碍下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉导航模型在真实动态环境中鲁棒性差，受限于sim-to-real差距及难以针对目标环境（如家庭、餐厅、工厂）定制训练；而此前基于3D高斯溅射（GS）的real-to-sim方法仅处理静态或不现实的动态障碍，无法满足安全动态导航需求。

Method: 提出ReaDy-Go三阶段pipeline：（1）动态GS仿真器——将重建的静态GS场景与基于2D轨迹生成合理运动的可动画人类GS化身模块融合；（2）动态导航数据集生成——结合该仿真器、专为动态GS设计的机器人专家规划器与人类规划器；（3）基于生成数据的策略学习。

Result: 在仿真与真实世界实验中均优于基线方法，显著提升sim-to-real迁移后性能及对移动障碍的适应能力；且在未见环境实现零样本部署，验证其泛化潜力。

Conclusion: ReaDy-Go有效弥合了real-to-sim导航仿真中动态场景建模与策略训练的鸿沟，为真实复杂动态环境下的具身智能提供了可扩展、高保真的训练范式。

Abstract: Visual navigation models often struggle in real-world dynamic environments due to limited robustness to the sim-to-real gap and the difficulty of training policies tailored to target deployment environments (e.g., households, restaurants, and factories). Although real-to-sim navigation simulation using 3D Gaussian Splatting (GS) can mitigate this gap, prior works have assumed only static scenes or unrealistic dynamic obstacles, despite the importance of safe navigation in dynamic environments. To address these issues, we propose ReaDy-Go, a novel real-to-sim simulation pipeline that synthesizes photorealistic dynamic scenarios for target environments. ReaDy-Go generates photorealistic navigation datasets for dynamic environments by combining a reconstructed static GS scene with dynamic human GS obstacles, and trains policies robust to both the sim-to-real gap and moving obstacles. The pipeline consists of three components: (1) a dynamic GS simulator that integrates scene GS with a human animation module, enabling the insertion of animatable human GS avatars and the synthesis of plausible human motions from 2D trajectories, (2) navigation dataset generation for dynamic environments that leverages the simulator, a robot expert planner designed for dynamic GS representations, and a human planner, and (3) policy learning using the generated datasets. ReaDy-Go outperforms baselines across target environments in both simulation and real-world experiments, demonstrating improved navigation performance even after sim-to-real transfer and in the presence of moving obstacles. Moreover, zero-shot sim-to-real deployment in an unseen environment indicates its generalization potential. Project page: https://syeon-yoo.github.io/ready-go-site/.

</details>


### [380] [ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation](https://arxiv.org/abs/2602.11598)
*Zedong Chu,Shichao Xie,Xiaolong Wu,Yanfen Shen,Minghua Luo,Zhengbo Wang,Fei Liu,Xiaoxu Leng,Junjun Hu,Mingyang Yin,Jia Lu,Yingnan Guo,Kai Yang,Jiawei Han,Xu Chen,Yanqing Zhu,Yuxiang Zhao,Xin Liu,Yirong Yang,Ye He,Jiahang Wang,Yang Cai,Tianlin Zhang,Li Gao,Liu Liu,Mingchao Sun,Fan Jiang,Chiyu Wang,Zhicheng Liu,Hongyu Pan,Honglin Han,Zhining Gu,Kuan Yang,Jianfang Zhang,Di Jing,Zihao Guan,Wei Guo,Guoqing Liu,Di Yang,Xiangpo Yang,Menglin Yang,Hongguang Xing,Weiguo Li,Mu Xu*

Main category: cs.RO

TL;DR: ABot-N0是一个统一的视觉-语言-动作（VLA）基础模型，通过‘大脑-动作’分层架构，在5种具身导航任务上实现大一统，并在7个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 具身导航长期受限于任务专用架构，缺乏统一建模能力。

Method: 提出ABot-N0模型，采用LLM驱动的认知大脑与基于流匹配的动作专家协同的分层架构；构建ABot-N0数据引擎，涵盖1690万专家轨迹和500万推理样本；设计集成规划器与分层拓扑记忆的智能体导航系统。

Result: 在7个基准测试中均取得新SOTA，显著超越各任务专用模型；支持长时程、动态真实环境下的鲁棒导航。

Conclusion: ABot-N0验证了统一VLA模型在多任务具身导航中的可行性与优越性，为通用具身智能提供了新范式。

Abstract: Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.
  To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.

</details>


### [381] [ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning](https://arxiv.org/abs/2602.11643)
*Yufeng Tian,Shuiqi Cheng,Tianming Wei,Tianxing Zhou,Yuanhang Zhang,Zixian Liu,Qianwei Han,Zhecheng Yuan,Huazhe Xu*

Main category: cs.RO

TL;DR: 本文提出了ViTaS框架，通过软融合对比学习和CVAE模块，有效利用视觉与触觉信息的对齐性与互补性，显著提升机器人在遮挡等复杂场景下的操作性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用直接拼接方式融合视觉与触觉特征，忽视二者内在互补性，且对齐不充分，难以应对遮挡等真实场景。

Method: 提出ViTaS框架，包含Soft Fusion Contrastive Learning（改进的对比学习）和CVAE模块，以建模视觉与触觉表征的对齐与互补关系。

Result: 在12个仿真环境和3个真实世界环境中验证，ViTaS显著优于现有基线方法。

Conclusion: ViTaS通过更合理的多模态融合机制，提升了触觉增强机器人操作的鲁棒性与实用性，为真实部署提供了新思路。

Abstract: Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Consequently, they struggle to effectively cope with occluded scenarios due to neglecting the inherent complementary nature of both modalities and the alignment may not be exploited enough, limiting the potential of their real-world deployment. In this paper, we present ViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide the behavior of an agent. We introduce Soft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method and a CVAE module to utilize the alignment and complementarity within visuo-tactile representations. We demonstrate the effectiveness of our method in 12 simulated and 3 real-world environments, and our experiments show that ViTaS significantly outperforms existing baselines. Project page: https://skyrainwind.github.io/ViTaS/index.html.

</details>


### [382] [Human-Like Gaze Behavior in Social Robots: A Deep Learning Approach Integrating Human and Non-Human Stimuli](https://arxiv.org/abs/2602.11648)
*Faezeh Vahedi,Morteza Memari,Ramtin Tabatabaei,Alireza Taheri*

Main category: cs.RO

TL;DR: 本研究通过收集41名参与者在虚拟现实环境中的注视数据，训练LSTM和Transformer模型预测人类在多种社交场景（包括人类与非人类刺激）下的注视方向，并将模型部署到NAO机器人上，显著提升了机器人模仿人类注视行为的能力和用户交互满意度。


<details>
  <summary>Details</summary>
Motivation: 提升社交机器人在复杂社交情境中对人类及非人类刺激（如对话、指物、开门、物体掉落）的注视响应能力，增强人机交互的自然性与有效性。

Method: 在Unity中构建3D动画与360度实景视频模拟多类社交场景；使用VR眼镜采集41名参与者的注视方向数据；预处理后分别训练LSTM与Transformer模型进行个体化注视预测；最终将模型部署于NAO机器人并由275名参与者进行问卷评估。

Result: 在动画场景中，LSTM和Transformer模型准确率分别为67.6%和70.4%；在实景场景中达72%和71.6%；模型在考虑非人类刺激方面优于既有方法；机器人部署后用户满意度高。

Conclusion: 本研究首次系统探索了机器人对非人类刺激的注视响应，所建模型在准确性和适用性上均超越现有工作，推动了社交机器人在动态、真实社交环境中自然注视行为建模的发展。

Abstract: Nonverbal behaviors, particularly gaze direction, play a crucial role in enhancing effective communication in social interactions. As social robots increasingly participate in these interactions, they must adapt their gaze based on human activities and remain receptive to all cues, whether human-generated or not, to ensure seamless and effective communication. This study aims to increase the similarity between robot and human gaze behavior across various social situations, including both human and non-human stimuli (e.g., conversations, pointing, door openings, and object drops). A key innovation in this study, is the investigation of gaze responses to non-human stimuli, a critical yet underexplored area in prior research. These scenarios, were simulated in the Unity software as a 3D animation and a 360-degree real-world video. Data on gaze directions from 41 participants were collected via virtual reality (VR) glasses. Preprocessed data, trained two neural networks-LSTM and Transformer-to build predictive models based on individuals' gaze patterns. In the animated scenario, the LSTM and Transformer models achieved prediction accuracies of 67.6% and 70.4%, respectively; In the real-world scenario, the LSTM and Transformer models achieved accuracies of 72% and 71.6%, respectively. Despite the gaze pattern differences among individuals, our models outperform existing approaches in accuracy while uniquely considering non-human stimuli, offering a significant advantage over previous literature. Furthermore, deployed on the NAO robot, the system was evaluated by 275 participants via a comprehensive questionnaire, with results demonstrating high satisfaction during interactions. This work advances social robotics by enabling robots to dynamically mimic human gaze behavior in complex social contexts.

</details>


### [383] [AC-MASAC: An Attentive Curriculum Learning Framework for Heterogeneous UAV Swarm Coordination](https://arxiv.org/abs/2602.11735)
*Wanhao Liu,Junhong Dai,Yixuan Zhang,Shengyun Yin,Panshuo Li*

Main category: cs.RO

TL;DR: 本文提出了一种面向异构无人机集群协同路径规划的注意型课程学习框架AC-MASAC，通过角色感知的异构注意力机制建模非对称依赖，并结合分层知识迁移与阶段比例经验回放缓解稀疏奖励和灾难性遗忘问题，实验表明其在成功率、编队保持率和任务时间加权成功率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习（MARL）在异构无人机集群协同路径规划中面临的非对称智能体依赖、稀疏奖励和灾难性遗忘等关键挑战。

Method: 提出AC-MASAC框架，包含角色感知的异构注意力机制以建模非对称依赖，以及融合分层知识迁移与阶段比例经验回放的结构化课程学习策略。

Result: 在自定义多智能体仿真平台上验证，该方法在成功率（Success Rate）、编队保持率（Formation Keeping Rate）和成功加权任务时间（Success-weighted Mission Time）上显著优于其他先进方法。

Conclusion: AC-MASAC有效提升了异构无人机集群在复杂协同任务中的训练稳定性与策略性能，为MARL在实际无人系统中的应用提供了新思路。

Abstract: Cooperative path planning for heterogeneous UAV swarms poses significant challenges for Multi-Agent Reinforcement Learning (MARL), particularly in handling asymmetric inter-agent dependencies and addressing the risks of sparse rewards and catastrophic forgetting during training. To address these issues, this paper proposes an attentive curriculum learning framework (AC-MASAC). The framework introduces a role-aware heterogeneous attention mechanism to explicitly model asymmetric dependencies. Moreover, a structured curriculum strategy is designed, integrating hierarchical knowledge transfer and stage-proportional experience replay to address the issues of sparse rewards and catastrophic forgetting. The proposed framework is validated on a custom multi-agent simulation platform, and the results show that our method has significant advantages over other advanced methods in terms of Success Rate, Formation Keeping Rate, and Success-weighted Mission Time. The code is available at \textcolor{red}{https://github.com/Wanhao-Liu/AC-MASAC}.

</details>


### [384] [HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model](https://arxiv.org/abs/2602.11758)
*Dongting Li,Xingyu Chen,Qianyang Wu,Bo Chen,Sikai Wu,Hanyu Wu,Guoyao Zhang,Liang Li,Mingliang Zhou,Diyun Xiang,Jianzhu Ma,Qiang Zhang,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出HAIC框架，通过仅依赖本体感知历史的动力学预测器，实现对多种动态物体（包括欠驱动和非完整约束物体）的鲁棒人机交互，无需外部状态估计，在滑板、推拉小车及多物体搬运等复杂任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有HOI方法大多假设物体完全驱动且刚性耦合于机器人，忽略了具有独立动力学和非完整约束的欠驱动物体，导致耦合力与遮挡带来的控制难题。

Method: 提出HAIC统一框架：1）基于本体感知历史的动力学预测器，估计物体高阶状态（速度、加速度）；2）将预测投影到静态几何先验上构建空间对齐的动态占据图，以推断盲区碰撞边界与接触可能性；3）采用异步微调机制，使世界模型持续适配学生策略的探索，提升分布偏移下的状态估计鲁棒性。

Result: 在人形机器人实验中，HAIC在滑板、不同负载下的推/拉小车等敏捷任务中成功率高，并能主动补偿惯性扰动；同时成功完成携带箱子穿越多变地形等多物体长时程任务，可预测多个物体的动力学。

Conclusion: HAIC实现了不依赖外部状态估计、面向多样化物体动力学的鲁棒人机交互，显著提升了人形机器人在非结构化环境中执行复杂全身体任务的能力。

Abstract: Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.

</details>


### [385] [LAMP: Implicit Language Map for Robot Navigation](https://arxiv.org/abs/2602.11862)
*Sibaek Lee,Hyeonwoo Yu,Giseop Kim,Sunwook Choi*

Main category: cs.RO

TL;DR: 本文提出了LAMP（Language Map），一种基于神经语言场的零样本导航框架，通过隐式语言场和稀疏图结合实现粗粒度到细粒度的路径规划，并引入贝叶斯不确定性建模与图采样策略以提升泛化性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式存储语言向量的地图方法在大型环境中面临内存开销大、分辨率低、难以支持细粒度规划的问题。

Method: 提出隐式神经语言场表征语言语义，结合稀疏空间图进行粗略路径规划，并在语言场中通过梯度优化细化目标附近位姿；引入von Mises-Fisher分布建模嵌入不确定性；采用兼顾空间覆盖与嵌入置信度的图采样策略以降低计算开销。

Result: 在NVIDIA Isaac Sim仿真和真实多楼层建筑实验中，LAMP在内存效率和细粒度目标到达精度上均优于现有显式方法。

Conclusion: LAMP首次将隐式语言场用于精确路径生成，通过粗-细两阶段、语言驱动、梯度引导的优化范式，显著提升了零样本导航在大规模环境中的可扩展性、鲁棒性与精度。

Abstract: Recent advances in vision-language models have made zero-shot navigation feasible, enabling robots to follow natural language instructions without requiring labeling. However, existing methods that explicitly store language vectors in grid or node-based maps struggle to scale to large environments due to excessive memory requirements and limited resolution for fine-grained planning. We introduce LAMP (Language Map), a novel neural language field-based navigation framework that learns a continuous, language-driven map and directly leverages it for fine-grained path generation. Unlike prior approaches, our method encodes language features as an implicit neural field rather than storing them explicitly at every location. By combining this implicit representation with a sparse graph, LAMP supports efficient coarse path planning and then performs gradient-based optimization in the learned field to refine poses near the goal. This coarse-to-fine pipeline, language-driven, gradient-guided optimization is the first application of an implicit language map for precise path generation. This refinement is particularly effective at selecting goal regions not directly observed by leveraging semantic similarities in the learned feature space. To further enhance robustness, we adopt a Bayesian framework that models embedding uncertainty via the von Mises-Fisher distribution, thereby improving generalization to unobserved regions. To scale to large environments, LAMP employs a graph sampling strategy that prioritizes spatial coverage and embedding confidence, retaining only the most informative nodes and substantially reducing computational overhead. Our experimental results, both in NVIDIA Isaac Sim and on a real multi-floor building, demonstrate that LAMP outperforms existing explicit methods in both memory efficiency and fine-grained goal-reaching accuracy.

</details>


### [386] [Learning to Manipulate Anything: Revealing Data Scaling Laws in Bounding-Box Guided Policies](https://arxiv.org/abs/2602.11885)
*Yihao Wu,Jinming Ma,Junbo Tan,Yanzhao Yu,Shoujie Li,Mingliang Zhou,Diyun Xiang,Xueqian Wang*

Main category: cs.RO

TL;DR: 本文提出一种基于边界框指令的语义操作方法，通过手持式分割设备Label-UMI高效采集带语义标签的演示数据，并设计语义-运动解耦框架提升泛化能力，发现泛化性能与边界框目标数量呈幂律关系。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在语义操作中泛化能力有限，仅依赖文本指令难以在复杂动态环境中精准聚焦目标物体。

Method: 提出边界框指令机制；设计手持式分割设备Label-UMI实现自动化标注；构建语义-运动解耦框架，融合目标检测与边界框引导的扩散策略。

Result: 在大规模真实世界实验中验证了方法有效性；发现泛化性能与边界框目标数量之间存在幂律关系；在四个任务中对已见和未见物体均达到85%成功率。

Conclusion: 边界框指令显著提升语义操作泛化性；数据规模遵循可量化的缩放规律；提出的高效数据收集策略具备实用推广价值。

Abstract: Diffusion-based policies show limited generalization in semantic manipulation, posing a key obstacle to the deployment of real-world robots. This limitation arises because relying solely on text instructions is inadequate to direct the policy's attention toward the target object in complex and dynamic environments. To solve this problem, we propose leveraging bounding-box instruction to directly specify target object, and further investigate whether data scaling laws exist in semantic manipulation tasks. Specifically, we design a handheld segmentation device with an automated annotation pipeline, Label-UMI, which enables the efficient collection of demonstration data with semantic labels. We further propose a semantic-motion-decoupled framework that integrates object detection and bounding-box guided diffusion policy to improve generalization and adaptability in semantic manipulation. Throughout extensive real-world experiments on large-scale datasets, we validate the effectiveness of the approach, and reveal a power-law relationship between generalization performance and the number of bounding-box objects. Finally, we summarize an effective data collection strategy for semantic manipulation, which can achieve 85\% success rates across four tasks on both seen and unseen objects. All datasets and code will be released to the community.

</details>


### [387] [General Humanoid Whole-Body Control via Pretraining and Fast Adaptation](https://arxiv.org/abs/2602.11929)
*Zepeng Wang,Jiangxing Wang,Shiqing Yao,Yu Zhang,Ziluo Ding,Ming Yang,Yuxuan Wang,Haobin Jiang,Chao Ma,Xiaochuan Shi,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出FAST框架，通过Parseval引导的残差策略适应和质心感知控制，实现人形机器人全身控制的快速适应与稳定运动跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有方法在应对多样化运动分布、快速适应新动作及高动态场景下的平衡鲁棒性方面存在不足。

Method: 提出FAST框架，包含Parseval-Guided Residual Policy Adaptation（在正交性和KL约束下学习轻量级增量动作策略）和Center-of-Mass-Aware Control（融合质心相关观测与目标以增强平衡）。

Result: 在仿真与真实世界实验中，FAST在鲁棒性、适应效率和泛化能力上持续优于当前最优基线。

Conclusion: FAST是一种通用、高效且鲁棒的人形机器人全身控制框架，显著提升了对分布外动作的快速适应能力和高动态运动下的稳定性。

Abstract: Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.

</details>


### [388] [Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control](https://arxiv.org/abs/2602.11934)
*Yu Deng,Yufeng Jin,Xiaogang Jia,Jiahong Xue,Gerhard Neumann,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 本文提出Robot-DIFT框架，通过流形蒸馏将生成式扩散模型的几何先验知识迁移到确定性特征网络S2-FPN中，以解决当前视觉骨干网络在机器人闭环操控中缺乏几何敏感性的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有视觉编码器（包括VLAs中使用的）侧重语义不变性，牺牲了操控所需的毫米级几何敏感性；而扩散模型虽蕴含几何依赖性，但其随机性、延迟和微调漂移阻碍其直接用于控制。

Method: 提出Robot-DIFT框架，采用Manifold Distillation技术，将冻结的扩散模型教师网络知识蒸馏到确定性的Spatial-Semantic Feature Pyramid Network（S2-FPN）中，并在DROID数据集上预训练。

Result: Robot-DIFT在几何一致性与操控性能上显著优于主流判别式基线模型，验证了几何敏感视觉表征对机器人操控泛化能力的关键作用。

Conclusion: 模型‘如何看’（即视觉表征的学习方式）从根本上决定了其‘如何动’（即操控学习效果），提升操控泛化性需从视觉骨干的几何建模能力入手。

Abstract: We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a "blind spot" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.

</details>


### [389] [Accelerating Robotic Reinforcement Learning with Agent Guidance](https://arxiv.org/abs/2602.11978)
*Haojun Chen,Zili Zou,Chengdong Ma,Yaoxiang Pu,Haotong Zhang,Yuanpei Chen,Yaodong Yang*

Main category: cs.RO

TL;DR: 本文提出Agent-guided Policy Search (AGPS)框架，用多模态智能体替代人工监督，提升强化学习在机器人操作任务中的样本效率，实现无需人工干预、可扩展的机器人自主学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在真实机器人上应用受限于样本效率低；现有人机协同方法依赖人工校正，存在扩展性差、操作员疲劳和人类表现不一致等问题。

Method: 提出AGPS框架，用多模态智能体替代人类监督者，将该智能体视为语义世界模型，通过可执行工具提供校正路径点和空间约束，以引导物理探索并剪枝无效区域。

Result: 在精密插入和可变形物体操作两个任务上验证，AGPS在样本效率上优于现有人机协同方法。

Conclusion: AGPS实现了监督流程的自动化，为无劳动力依赖、可扩展的机器人学习提供了可行路径。

Abstract: Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.

</details>


### [390] [Decentralized Multi-Robot Obstacle Detection and Tracking in a Maritime Scenario](https://arxiv.org/abs/2602.12012)
*Muhammad Farhan Ahmed,Vincent Frémont*

Main category: cs.RO

TL;DR: 本文提出了一种去中心化的多机器人框架，用于无人机（UAV）与无人水面艇（ASV）协同检测和跟踪海上漂浮集装箱，结合YOLOv8视觉检测、立体视差测距、不确定性感知数据关联与协方差交集融合，并通过信息驱动的任务分配优化覆盖、定位精度与通信效率。


<details>
  <summary>Details</summary>
Motivation: 自主空-面机器人团队在海洋监测中具有潜力，但面临水面反射导致感知不可靠、通信受限下协同扩展性差等挑战。

Method: 采用去中心化架构：各UAV运行YOLOv8与立体视差检测，用目标级扩展卡尔曼滤波（EKFs）跟踪；通过协方差交集（CI）保守融合紧凑轨迹摘要；设计信息驱动的任务分配模块，权衡不确定性降低预期、航程代价与安全间距以分配目标和选择悬停视角。

Result: 仿真表明该框架在海洋场景中提升了监测覆盖率、目标定位精度与跟踪一致性，同时保持较低通信开销。

Conclusion: 所提框架有效解决了水面上空-面异构机器人协同感知与任务分配的关键问题，兼顾鲁棒性、可扩展性与通信效率，适用于实际 maritime monitoring 任务。

Abstract: Autonomous aerial-surface robot teams are promising for maritime monitoring. Robust deployment requires reliable perception over reflective water and scalable coordination under limited communication. We present a decentralized multi-robot framework for detecting and tracking floating containers using multiple UAVs cooperating with an autonomous surface vessel. Each UAV performs YOLOv8 and stereo-disparity-based visual detection, then tracks targets with per-object EKFs using uncertainty-aware data association. Compact track summaries are exchanged and fused conservatively via covariance intersection, ensuring consistency under unknown correlations. An information-driven assignment module allocates targets and selects UAV hover viewpoints by trading expected uncertainty reduction against travel effort and safety separation. Simulation results in a maritime scenario demonstrate improved coverage, localization accuracy, and tracking consistency while maintaining modest communication requirements.

</details>


### [391] [Adaptive-Horizon Conflict-Based Search for Closed-Loop Multi-Agent Path Finding](https://arxiv.org/abs/2602.12024)
*Jiarui Li,Federico Pecora,Runyu Zhang,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文提出ACCBS算法，一种基于有限视界CBS的闭环MAPF规划方法，通过动态调整规划视界和重用约束树实现任意时间性与渐进最优性，在应对扰动的同时提供强性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF方法要么是难以应对外部扰动的开环规划器，要么是缺乏可靠性能保证的闭环启发式方法，难以满足安全关键场景需求。

Method: 提出ACCBS算法，基于有限视界冲突解耦（CBS）框架，引入受MPC中迭代深化启发的视界变换机制，动态适配计算预算，并复用单一约束树以实现不同视界间的无缝切换。

Result: ACCBS在多种案例中展现出快速生成高质量可行解的能力，具备任意时间性与渐进最优性，兼顾扰动鲁棒性与理论性能保证。

Conclusion: ACCBS有效弥合理论最优性与实际鲁棒性之间的鸿沟，为大规模机器人部署提供了兼具可靠性与灵活性的闭环MAPF解决方案。

Abstract: MAPF is a core coordination problem for large robot fleets in automated warehouses and logistics. Existing approaches are typically either open-loop planners, which generate fixed trajectories and struggle to handle disturbances, or closed-loop heuristics without reliable performance guarantees, limiting their use in safety-critical deployments. This paper presents ACCBS, a closed-loop algorithm built on a finite-horizon variant of CBS with a horizon-changing mechanism inspired by iterative deepening in MPC. ACCBS dynamically adjusts the planning horizon based on the available computational budget, and reuses a single constraint tree to enable seamless transitions between horizons. As a result, it produces high-quality feasible solutions quickly while being asymptotically optimal as the budget increases, exhibiting anytime behavior. Extensive case studies demonstrate that ACCBS combines flexibility to disturbances with strong performance guarantees, effectively bridging the gap between theoretical optimality and practical robustness for large-scale robot deployment.

</details>


### [392] [When would Vision-Proprioception Policies Fail in Robotic Manipulation?](https://arxiv.org/abs/2602.12032)
*Jingxian Lu,Wenke Xia,Yuxuan Wu,Zhiwu Lu,Di Hu*

Main category: cs.RO

TL;DR: 本文发现视觉-本体感觉策略在机器人运动转换阶段中视觉模态作用有限，因本体感觉信号收敛更快而主导优化过程，抑制了视觉学习；为此提出GAP算法，通过相位引导的梯度调整，动态平衡双模态协作，提升策略泛化性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现视觉-本体感觉融合策略在复杂操作任务中泛化能力不一致，尤其在运动转换阶段视觉贡献不明，需探究其内在机制并提升协同效果。

Method: 提出Gradient Adjustment with Phase-guidance（GAP）算法：利用本体感觉估计各时间步是否处于运动转换阶段，并据此自适应减小该阶段本体感觉梯度，从而缓解其对视觉学习的抑制，促进双模态动态协作。

Result: GAP在仿真与真实环境、单臂与双臂设置、传统模型及Vision-Language-Action模型上均显著提升策略泛化性与鲁棒性。

Conclusion: 本体感觉在运动转换阶段的主导性会抑制视觉学习，GAP通过相位感知梯度调控实现更均衡的多模态融合，为机器人操作中视觉-本体感觉策略设计提供了新思路。

Abstract: Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.

</details>


### [393] [Safety Beyond the Training Data: Robust Out-of-Distribution MPC via Conformalized System Level Synthesis](https://arxiv.org/abs/2602.12047)
*Anutam Srinivasan,Antoine Leeman,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种结合共形预测（CP）与系统级综合（SLS）的新框架，用于基于学习动力学模型的分布外鲁棒规划与控制，通过加权CP估计状态-控制依赖的模型误差界，并将其嵌入SLS鲁棒非线性MPC中，实现基于前向可达集的约束收紧，理论保证覆盖率与鲁棒性，实验验证其在4D小车和12D四旋翼上的安全性和分布外鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 解决使用学习动力学模型进行规划与控制时，在训练数据分布之外（out-of-distribution）场景下难以保障安全性与鲁棒性的挑战。

Method: 采用加权共形预测（CP）构建状态-控制依赖的协方差模型以获得高置信度模型误差界；将该误差界融入基于系统级综合（SLS）的鲁棒非线性模型预测控制（MPC）框架，并通过体积优化的前向可达集实现预测时域内的约束收紧。

Result: 理论层面提供了在分布偏移下的覆盖率与鲁棒性保证，并分析了数据密度与轨迹管尺寸对覆盖率的影响；实验层面在4D小车和12D四旋翼系统上验证了相较固定边界与非鲁棒基线方法，在分布外场景下显著提升了安全性与鲁棒性。

Conclusion: 所提CP-SLS框架能有效提升学习动力学模型在分布外场景下的可控性与安全性，兼具理论严谨性与实际可扩展性。

Abstract: We present a novel framework for robust out-of-distribution planning and control using conformal prediction (CP) and system level synthesis (SLS), addressing the challenge of ensuring safety and robustness when using learned dynamics models beyond the training data distribution. We first derive high-confidence model error bounds using weighted CP with a learned, state-control-dependent covariance model. These bounds are integrated into an SLS-based robust nonlinear model predictive control (MPC) formulation, which performs constraint tightening over the prediction horizon via volume-optimized forward reachable sets. We provide theoretical guarantees on coverage and robustness under distributional drift, and analyze the impact of data density and trajectory tube size on prediction coverage. Empirically, we demonstrate our method on nonlinear systems of increasing complexity, including a 4D car and a {12D} quadcopter, improving safety and robustness compared to fixed-bound and non-robust baselines, especially outside of the data distribution.

</details>


### [394] [HoloBrain-0 Technical Report](https://arxiv.org/abs/2602.12062)
*Xuewu Lin,Tianwei Lin,Yun Du,Hongyu Xie,Yiwei Jin,Jiawei Li,Shijie Wu,Qingze Wang,Mengdi Li,Mengao Zhao,Ziang Li,Chaodong Huang,Hongzhe Bi,Lichao Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: HoloBrain-0 是一个面向真实机器人部署的视觉-语言-动作（VLA）框架，通过引入机器人具身先验（如多视角相机参数和URDF运动学描述）提升3D空间推理能力，并采用‘预训练+后训练’范式，在仿真与真实长视野操作任务中达到SOTA性能；其轻量0.2B参数版本支持低延迟端侧部署，并全面开源模型、检查点及全栈基础设施RoboOrchard。


<details>
  <summary>Details</summary>
Motivation: 弥合基础模型研究与可靠真实世界机器人部署之间的鸿沟，解决现有VLA模型缺乏对机器人具身特性的显式建模问题。

Method: 提出一种新型VLA架构，显式融入多视角相机参数和URDF等机器人具身先验；采用可扩展的‘预训练→后训练’范式；构建并开源全栈VLA基础设施RoboOrchard及标准化数据采集协议。

Result: 在RoboTwin 2.0、LIBERO、GenieSim等仿真基准及真实长视野操作任务上达到SOTA；0.2B参数轻量版性能媲美更大模型，支持低延迟端侧部署。

Conclusion: HoloBrain-0为高性能机器人操作提供了完整、可复现的技术路径，其开源生态有望加速具身智能研究与落地。

Abstract: In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.

</details>


### [395] [VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model](https://arxiv.org/abs/2602.12063)
*Yanjiang Guo,Tony Lee,Lucy Xiaoyang Shi,Jianyu Chen,Percy Liang,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出一种迭代改进算法，利用真实世界数据提升世界模型的物理保真度，再用其生成合成数据以增强视觉-语言-动作（VLA）模型性能，在真实机器人上实现显著成功率提升。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在物理保真度上不足，尤其难以准确建模接触丰富的物体操作中的关键细节，且训练数据缺乏失败案例等多样物理交互覆盖，导致难以有效支撑VLA策略优化。

Method: 提出一种简单迭代改进算法：先用少量真实世界rollout数据优化动作条件视频生成的世界模型，再用该增强后的世界模型生成高质量合成rollout数据，用于进一步训练VLA策略模型。

Result: 在真实机器人实验中，该方法使最先进VLA模型在多个下游任务上的绝对成功率提升39.2%，其中仅靠生成的合成rollout训练即带来11.6%的提升。

Conclusion: 基于真实数据迭代优化世界模型，可有效弥补其物理建模缺陷，生成高保真合成数据，从而显著提升VLA模型的性能与可靠性。

Abstract: The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w

</details>


### [396] [Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning](https://arxiv.org/abs/2602.12065)
*Xiang Liu,Sen Cui,Guocai Yao,Zhong Cao,Jingheng Ma,Min Zhang,Changshui Zhang*

Main category: cs.RO

TL;DR: 本文提出AGT-World框架，通过构建可交互的仿真环境与任务图结构，结合自演化机制实现机器人策略在真实世界中的高效、可扩展学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界中直接训练机器人策略成本高、难扩展；现有生成式仿真难以生成逻辑连贯的长时序任务，且因开环执行难以应对物理动态不确定性。

Method: 提出Affordance-Graphed Task Worlds（AGT-World）：将任务空间建模为结构化图，实现复杂目标的分层、理论支撑的原子化分解；引入融合视觉语言模型推理与几何验证的混合反馈自演化机制，实现策略自主优化。

Result: 实验表明该方法在成功率和泛化性上显著优于现有方法，实现了‘提出—执行—修正’的自我提升闭环。

Conclusion: AGT-World为面向真实世界的可扩展机器人学习提供了统一、自主、理论驱动的新范式。

Abstract: Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.

</details>


### [397] [RF-Modulated Adaptive Communication Improves Multi-Agent Robotic Exploration](https://arxiv.org/abs/2602.12074)
*Lorin Achey,Breanne Crockett,Christoffer Heckman,Bradley Hayes*

Main category: cs.RO

TL;DR: 本文提出了一种名为ART的通信感知规划算法，通过动态调整传输位置来优化异构机器人团队在通信受限环境中的信息共享效率，并进一步提出了ART-SST扩展方法以保障高保真数据传输；仿真结果表明，该方法显著减少了行进距离和探索时间。


<details>
  <summary>Details</summary>
Motivation: 在通信受限环境下，多机器人系统的可靠协调与高效通信是关键挑战。

Method: 提出了自适应射频传输（ART）算法，动态调节基于信号强度和数据负载大小的传输位置；并扩展为ART-SST，引入信号强度阈值以确保高质量数据传输。

Result: 在480多次模拟中，ART相比基线方法最多减少58%行进距离、提升52%探索速度；显著优于完全会合与最小信号启发式等现有策略。

Conclusion: 自适应、负载感知的通信策略能显著提升复杂受限环境下的覆盖效率与任务执行速度，为行星探测与搜救任务提供新思路。

Abstract: Reliable coordination and efficient communication are critical challenges for multi-agent robotic exploration of environments where communication is limited. This work introduces Adaptive-RF Transmission (ART), a novel communication-aware planning algorithm that dynamically modulates transmission location based on signal strength and data payload size, enabling heterogeneous robot teams to share information efficiently without unnecessary backtracking. We further explore an extension to this approach called ART-SST, which enforces signal strength thresholds for high-fidelity data delivery. Through over 480 simulations across three cave-inspired environments, ART consistently outperforms existing strategies, including full rendezvous and minimum-signal heuristic approaches, achieving up to a 58% reduction in distance traveled and up to 52% faster exploration times compared to baseline methods. These results demonstrate that adaptive, payload-aware communication significantly improves coverage efficiency and mission speed in complex, communication-constrained environments, offering a promising foundation for future planetary exploration and search-and-rescue missions.

</details>


### [398] [Pack it in: Packing into Partially Filled Containers Through Contact](https://arxiv.org/abs/2602.12095)
*David Russell,Zisong Xu,Maximo A. Roa,Mehmet Dogar*

Main category: cs.RO

TL;DR: 本文提出了一种接触感知的装箱方法，利用与已放置物体的主动接触来腾出空间，实现对部分填充容器的新物品高效、物理可行的装入。


<details>
  <summary>Details</summary>
Motivation: 现有装箱方法多假设容器为空且避免碰撞，但实际仓库中容器常已部分填充且布局次优，需能处理已有物品并利用接触创造空间的新策略。

Method: 采用基于接触的多物体轨迹优化器嵌入模型预测控制器，并结合具备物理感知能力的感知系统（可处理遮挡下的位姿估计）及物理可行的物体放置位置推荐方法。

Result: 实现了在部分填充、布局混乱的容器中，通过主动接触已有物体来腾出空间并成功放置新物品。

Conclusion: 接触感知的装箱方法更贴合真实仓库场景，提升了自动化装箱的鲁棒性与实用性。

Abstract: The automation of warehouse operations is crucial for improving productivity and reducing human exposure to hazardous environments. One operation frequently performed in warehouses is bin-packing where items need to be placed into containers, either for delivery to a customer, or for temporary storage in the warehouse. Whilst prior bin-packing works have largely been focused on packing items into empty containers and have adopted collision-free strategies, it is often the case that containers will already be partially filled with items, often in suboptimal arrangements due to transportation about a warehouse. This paper presents a contact-aware packing approach that exploits purposeful interactions with previously placed objects to create free space and enable successful placement of new items. This is achieved by using a contact-based multi-object trajectory optimizer within a model predictive controller, integrated with a physics-aware perception system that estimates object poses even during inevitable occlusions, and a method that suggests physically-feasible locations to place the object inside the container.

</details>


### [399] [Multi Graph Search for High-Dimensional Robot Motion Planning](https://arxiv.org/abs/2602.12096)
*Itamar Mishani,Maxim Likhachev*

Main category: cs.RO

TL;DR: 本文提出了一种名为Multi-Graph Search（MGS）的搜索式运动规划算法，通过在状态空间中维护并增量扩展多个隐式图，提升高维机器人系统（如机械臂和移动机械臂）的规划效率与一致性，同时保证完备性和有界次优性。


<details>
  <summary>Details</summary>
Motivation: 现有高维机器人运动规划算法虽提升了可扩展性，但常导致运动不可预测、不一致，或需过多计算资源与内存。

Method: 提出Multi-Graph Search（MGS）算法，将经典单向/双向搜索推广至多图设置，在状态空间中维护多个隐式图，聚焦高潜力区域探索，并允许子图随搜索进展通过可行转移合并。

Result: 理论证明MGS具备完备性和有界次优性；实验验证其在多种操作与移动操作任务中有效。

Conclusion: MGS在保持理论保证的同时，显著提升了高维机器人运动规划的效率、一致性与实用性，适用于实时与可靠部署场景。

Abstract: Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion planning algorithm that generalizes classical unidirectional and bidirectional search to a multi-graph setting. MGS maintains and incrementally expands multiple implicit graphs over the state space, focusing exploration on high-potential regions while allowing initially disconnected subgraphs to be merged through feasible transitions as the search progresses. We prove that MGS is complete and bounded-suboptimal, and empirically demonstrate its effectiveness on a range of manipulation and mobile manipulation tasks. Demonstrations, benchmarks and code are available at https://multi-graph-search.github.io/.

</details>


### [400] [3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting](https://arxiv.org/abs/2602.12159)
*Wancai Zheng,Hao Chen,Xianlong Lu,Linlin Ou,Xinyi Yu*

Main category: cs.RO

TL;DR: 本文提出3DGSNav，一种基于3D高斯泼溅（3DGS）作为视觉语言模型（VLM）持久记忆的零样本物体导航（ZSON）新框架，通过主动感知构建环境3DGS表示、自由视角渲染与结构化视觉提示+思维链推理，提升空间推理与目标识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本物体导航方法依赖语义地图或文本等场景抽象，使高层决策受限于低层感知精度；亟需更直接、几何保真的环境表征以增强VLM的空间推理能力。

Method: 提出3DGSNav框架：1）利用主动感知增量构建环境的3D高斯泼溅（3DGS）表示；2）支持轨迹引导的前沿感知驱动的自由视角第一人称图像渲染；3）设计结构化视觉提示并融合思维链（CoT）提示提升VLM推理；4）结合实时目标检测与VLM驱动的主动视角切换进行目标再验证。

Result: 在多个基准测试及真实四足机器人实验中，3DGSNav展现出对最先进方法具有竞争力且鲁棒的导航性能。

Conclusion: 将3DGS作为VLM的持久几何记忆可有效弥合感知与推理鸿沟，为零样本具身导航提供了一种更可靠、可解释且可扩展的新范式。

Abstract: Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/

</details>


### [401] [Sub--Riemannian boundary value problems for Optimal Geometric Locomotion](https://arxiv.org/abs/2602.12199)
*Oliver Gross,Florine Hartwig,Martin Rumpf,Peter Schröder*

Main category: cs.RO

TL;DR: 本文提出了一种基于子黎曼几何的最优形变驱动运动模型，用于描述细长生物（如蛇、精子）或机器人在耗散环境中的运动，兼顾形变能耗与位移能耗，并通过数值计算生成符合实际观测的最优运动模式。


<details>
  <summary>Details</summary>
Motivation: 传统模型未同时考虑身体位移耗能和形变（如弯曲、伸展）所需的代谢或驱动能耗，难以全面刻画运动效率；需一个更几何灵活、能统一解释多种生物运动的框架。

Method: 基于拉格朗日最小耗散原理构建边界值问题，解为子黎曼测地线；采用连续几何模型并结合时空一致离散化，支持三类边界条件（固定初末态、循环运动、仅指定位姿变化）下的数值求解。

Result: 生成的最优形变步态在定性上复现了蛇、精子等生物的实际运动轨迹，并吻合Purcell游动器等低维系统的已知最优性结果；模型拓展了对广义Purcell游动器等系统的机理理解。

Conclusion: 该子黎曼几何模型更全面地刻画了形变驱动型运动的能量效率，兼具理论普适性与数值可实现性，为生物运动分析与软体机器人设计提供了新工具。

Abstract: We propose a geometric model for optimal shape-change-induced motions of slender locomotors, e.g., snakes slithering on sand. In these scenarios, the motion of a body in world coordinates is completely determined by the sequence of shapes it assumes. Specifically, we formulate Lagrangian least-dissipation principles as boundary value problems whose solutions are given by sub-Riemannian geodesics. Notably, our geometric model accounts not only for the energy dissipated by the body's displacement through the environment, but also for the energy dissipated by the animal's metabolism or a robot's actuators to induce shape changes such as bending and stretching, thus capturing overall locomotion efficiency. Our continuous model, together with a consistent time and space discretization, enables numerical computation of sub-Riemannian geodesics for three different types of boundary conditions, i.e., fixing initial and target body, restricting to cyclic motion, or solely prescribing body displacement and orientation. The resulting optimal deformation gaits qualitatively match observed motion trajectories of organisms such as snakes and spermatozoa, as well as known optimality results for low-dimensional systems such as Purcell's swimmers. Moreover, being geometrically less rigid than previous frameworks, our model enables new insights into locomotion mechanisms of, e.g., generalized Purcell's swimmers. The code is publicly available.

</details>


### [402] [LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion](https://arxiv.org/abs/2602.12215)
*Jiangran Lyu,Kai Liu,Xuheng Zhang,Haoran Liao,Yusen Feng,Wenxuan Zhu,Tingrui Shen,Jiayi Chen,Jiazhao Zhang,Yifei Dong,Wenbo Cui,Senmao Qi,Shuo Wang,Yixin Zheng,Mi Yan,Xuesong Shi,Haoran Li,Dongbin Zhao,Ming-Yu Liu,Zhizheng Zhang,Li Yi,Yizhou Wang,He Wang*

Main category: cs.RO

TL;DR: 本文提出LDA-1B，一种可扩展的机器人基础模型，通过统一处理异构具身数据（含高质量与低质量轨迹），联合学习动力学、策略与视觉预测，并利用DINO潜在空间和多模态扩散Transformer实现高效、稳定的大规模训练；在仿真与真实世界任务中显著超越现有方法，且支持数据高效的微调。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型主要依赖大规模行为克隆，忽略了异构具身数据中蕴含的可迁移动力学知识；而统一世界模型（UWM）虽有潜力，却受限于粗粒度数据使用和数据集碎片化，难以扩展至基础模型规模。

Method: 提出LDA-1B模型，采用三重联合学习（动力学、策略、视觉预测），为不同质量数据分配不同角色；构建并标准化大规模具身交互数据集EI-30k（>30k小时）；在结构化DINO潜在空间中进行动力学预测以避免像素级冗余建模；引入多模态扩散Transformer处理异步视觉与动作流，支撑10亿参数规模稳定训练。

Result: 在仿真与真实世界实验中，LDA-1B在接触密集、灵巧操作和长视野任务上分别比π₀.₅等基线提升21%、48%、23%；支持数据高效微调，仅用30%通常被丢弃的低质量轨迹即可带来10%性能增益。

Conclusion: LDA-1B验证了统一、分层利用异构具身数据对构建可扩展机器人基础模型的有效性，为具身智能提供了兼顾泛化性、鲁棒性与数据效率的新范式。

Abstract: Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\%, 48\%, and 23\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\% by leveraging 30\% low-quality trajectories typically harmful and discarded.

</details>


### [403] [Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks](https://arxiv.org/abs/2602.12244)
*Zhihong Liu,Yang Li,Rengming Huang,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: 本文提出AHAT系统，利用训练好的大语言模型将模糊的人类指令和文本场景图映射为PDDL子目标，并通过符号推理生成长时程可行最优规划；同时引入新强化学习算法TGPO提升中间推理轨迹的修正能力，在大规模家庭环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开放世界中基于自然语言的任务规划面临环境规模扩大、指令模糊性增强及约束复杂度上升时性能急剧下降的挑战，亟需可扩展的长时程规划方法。

Method: 提出AHAT框架：1）训练LLM将语言指令与文本场景图映射为PDDL形式的具身子目标；2）通过符号规划器求解子目标生成长时程计划；3）设计TGPO强化学习算法，融合外部对中间推理轨迹的修正以改进策略优化（基于GRPO）。

Result: 在人类风格的家庭任务（简短指令但需复杂执行）上，AHAT显著超越当前主流提示工程、规划与学习方法，展现出更强的可扩展性与鲁棒性。

Conclusion: 结合LLM语义理解与符号推理的混合架构，辅以针对推理过程的新型强化学习优化，是实现开放世界长时程语言条件任务规划的有效路径。

Abstract: Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-horizon planning in large environments given ambiguous human instructions. At its core, AHAT utilizes an LLM trained to map task instructions and textual scene graphs into grounded subgoals defined in the Planning Domain Definition Language (PDDL). These subgoals are subsequently solved to generate feasible and optimal long-horizon plans through explicit symbolic reasoning. To enhance the model's ability to decompose complex and ambiguous intentions, we introduce TGPO, a novel reinforcement learning algorithm that integrates external correction of intermediate reasoning traces into Group Relative Policy Optimization (GRPO). Experiments demonstrate that AHAT achieves significant performance gains over state-of-the-art prompting, planning, and learning methods, particularly in human-style household tasks characterized by brief instructions but requiring complex execution plans.

</details>


### [404] [Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment](https://arxiv.org/abs/2602.12281)
*Jacky Kwok,Xilun Zhang,Mengdi Xu,Yuejiang Liu,Azalia Mirhoseini,Chelsea Finn,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出CoVer框架，通过测试时验证缩小意图与动作之间的差距，利用重述指令和生成动作的联合缩放提升多样性，并在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型生成的动作仍可能与自然语言指令不一致，存在‘意图-动作差距’，需要在测试时进行验证以提高对齐度。

Method: 提出对比验证器CoVer，结合重述指令与动作生成的联合缩放、boot-time compute及分层验证推理流程，在部署前预计算多样化重述指令并生成多组动作候选，再由验证器筛选最优动作。

Result: 在SIMPLER基准上，分布内提升22%、分布外提升13%，真实世界实验进一步提升45%；在PolaRiS基准上任务进展提升14%、成功率提升9%。

Conclusion: 测试时验证是一种高效且可扩展的方法，相比扩大策略预训练，能更优地提升VLA模型在不同场景下的指令跟随能力。

Abstract: The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the "intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce "boot-time compute" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [405] [Filmsticking++: Rapid Film Sticking for Explicit Surface Reconstruction](https://arxiv.org/abs/2602.11433)
*Pengfei Wang,Jian Liu,Qiujie Dong,Shiqing Xin,Yuanfeng Zhou,Changhe Tu,Caiming Zhang,Wenping Wang*

Main category: cs.GR

TL;DR: 本文提出Filmsticking++，一种改进的显式曲面重建方法，通过加权受限幂图（RPD）替代传统受限Voronoi图（RVD），确保所有点被精确插值，并利用虚拟站点加速外部中轴线排出，从而提升鲁棒性、可扩展性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于受限Voronoi图（RVD）的filmsticking方法在处理深腔结构时无法插值所有点，易导致拓扑错误，根源在于欧氏距离度量的固有局限性。

Method: 提出Filmsticking++：1）采用加权距离度量的受限幂图（RPD）替代RVD，保证所有输入点被插值；2）在引导曲面内部放置虚拟站点，加速外部中轴线从曲面内部排出。

Result: Filmsticking++在不依赖法向量的前提下，实现了对低质量点云的鲁棒显式曲面重建，能精确插值所有点、生成水密流形，同时降低了计算成本，提升了鲁棒性与可扩展性。

Conclusion: Filmsticking++克服了RVD-based filmsticking在深腔结构下的插值失败与拓扑缺陷问题，成为更优的显式曲面重建方法，在精度、鲁棒性和效率上均优于当前SOTA。

Abstract: Explicit surface reconstruction aims to generate a surface mesh that exactly interpolates a given point cloud. This requirement is crucial when the point cloud must lie non-negotiably on the final surface to preserve sharp features and fine geometric details. However, the task becomes substantially challenging with low-quality point clouds, due to inherent reconstruction ambiguities compounded by combinatorial complexity. A previous method using filmsticking technique by iteratively compute restricted Voronoi diagram to address these issues, ensures to produce a watertight manifold, setting a new benchmark as the state-of-the-art (SOTA) technique. Unfortunately, RVD-based filmsticking is inability to interpolate all points in the case of deep internal cavities, resulting in very likely is the generation of faulty topology. The cause of this issue is that RVD-based filmsticking has inherent limitations due to Euclidean distance metrics. In this paper, we extend the filmsticking technique, named Filmsticking++. Filmsticking++ reconstructing an explicit surface from points without normals. On one hand, Filmsticking++ break through the inherent limitations of Euclidean distance by employing a weighted-distance-based Restricted Power Diagram, which guarantees that all points are interpolated. On the other hand, we observe that as the guiding surface increasingly approximates the target shape, the external medial axis is gradually expelled outside the guiding surface. Building on this observation, we propose placing virtual sites inside the guiding surface to accelerate the expulsion of the external medial axis from its interior. To summarize, contrary to the SOTA method, Filmsticking++ demonstrates multiple benefits, including decreases computational cost, improved robustness and scalability.

</details>


### [406] [LeafFit: Plant Assets Creation from 3D Gaussian Splatting](https://arxiv.org/abs/2602.11577)
*Chang Luo,Nobuyuki Umetani*

Main category: cs.GR

TL;DR: LeafFit 是一种将植物3D高斯泼溅（3DGS）转换为可编辑、实例化网格资产的管线，通过叶形重复性实现高效分割与模板拟合，并支持实时顶点着色器变形，兼顾精度、存储效率与编辑性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽能高保真重建复杂植被，但其无拓扑结构和高内存开销，难以融入传统游戏生产流程。

Method: LeafFit 先对3DGS进行叶级分割（支持用户交互修正），选取代表性叶组生成薄而锐利的网格模板，再利用可微分移动最小二乘（MLS）将模板变形适配至所有叶子；运行时通过顶点着色器实时计算变形以节省存储。

Result: 相比近期基线方法，LeafFit 在叶分割质量、形变精度上更高，同时显著减小数据体积，并支持参数级编辑。

Conclusion: LeafFit 成功桥接了神经渲染与传统网格工作流，在保持视觉保真度的同时，提升了植被资产的可编辑性、存储效率与实时渲染兼容性。

Abstract: We propose LeafFit, a pipeline that converts 3D Gaussian Splatting (3DGS) of individual plants into editable, instanced mesh assets. While 3DGS faithfully captures complex foliage, its high memory footprint and lack of mesh topology make it incompatible with traditional game production workflows. We address this by leveraging the repetition of leaf shapes; our method segments leaves from the unstructured 3DGS, with optional user interaction included as a fallback. A representative leaf group is selected and converted into a thin, sharp mesh to serve as a template; this template is then fitted to all other leaves via differentiable Moving Least Squares (MLS) deformation. At runtime, the deformation is evaluated efficiently on-the-fly using a vertex shader to minimize storage requirements. Experiments demonstrate that LeafFit achieves higher segmentation quality and deformation accuracy than recent baselines while significantly reducing data size and enabling parameter-level editing.

</details>


### [407] [Variation-aware Flexible 3D Gaussian Editing](https://arxiv.org/abs/2602.11638)
*Hao Qin,Yukai Sun,Meng Wang,Ming Kong,Mengxu Lu,Qiang Zhu*

Main category: cs.GR

TL;DR: 本文提出VF-Editor，一种直接编辑3D高斯点属性的前馈方法，通过蒸馏2D编辑知识构建变分预测器，克服了间接编辑导致的跨视角不一致与低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D渲染空间间接编辑3D高斯泼溅的方法存在跨视角不一致、灵活性与效率受限的问题。

Method: 提出VF-Editor，设计一个从2D编辑知识蒸馏而来的变分预测器；该预测器编码输入生成变分场，并通过两个可学习并行解码函数迭代推断每个3D高斯的属性变化。

Result: 在公开与私有数据集上的大量实验表明，VF-Editor显著优于间接编辑方法，在一致性、灵活性和编辑效果上均有提升。

Conclusion: VF-Editor实现了对3D高斯原语的原生、高效、灵活编辑，统一支持多种2D编辑策略的知识迁移，为3DGS编辑提供了新范式。

Abstract: Indirect editing methods for 3D Gaussian Splatting (3DGS) have recently witnessed significant advancements. These approaches operate by first applying edits in the rendered 2D space and subsequently projecting the modifications back into 3D. However, this paradigm inevitably introduces cross-view inconsistencies and constrains both the flexibility and efficiency of the editing process. To address these challenges, we present VF-Editor, which enables native editing of Gaussian primitives by predicting attribute variations in a feedforward manner. To accurately and efficiently estimate these variations, we design a novel variation predictor distilled from 2D editing knowledge. The predictor encodes the input to generate a variation field and employs two learnable, parallel decoding functions to iteratively infer attribute changes for each 3D Gaussian. Thanks to its unified design, VF-Editor can seamlessly distill editing knowledge from diverse 2D editors and strategies into a single predictor, allowing for flexible and effective knowledge transfer into the 3D domain. Extensive experiments on both public and private datasets reveal the inherent limitations of indirect editing pipelines and validate the effectiveness and flexibility of our approach.

</details>


### [408] [OMEGA-Avatar: One-shot Modeling of 360° Gaussian Avatars](https://arxiv.org/abs/2602.11693)
*Zehao Xia,Yiqun Wang,Zhengda Lu,Kai Liu,Jun Xiao,Peter Wonka*

Main category: cs.GR

TL;DR: OMEGA-Avatar 是首个前馈式框架，能从单张图像生成通用、360°完整且可驱动的3D高斯头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时满足前馈推理、360°全头建模和动画就绪这三项关键需求。

Method: 提出语义感知网格形变模块（优化带发FLAME模型）和多视角特征splatting模块（构建共享规范UV表示），实现无需逐例优化的360°一致生成。

Result: 在360°全头完整性与跨视角身份一致性上显著超越现有方法，达到SOTA性能。

Conclusion: OMEGA-Avatar成功统一了前馈性、全头覆盖与可驱动性，为单图生成高质量3D头像提供了新范式。

Abstract: Creating high-fidelity, animatable 3D avatars from a single image remains a formidable challenge. We identified three desirable attributes of avatar generation: 1) the method should be feed-forward, 2) model a 360° full-head, and 3) should be animation-ready. However, current work addresses only two of the three points simultaneously. To address these limitations, we propose OMEGA-Avatar, the first feed-forward framework that simultaneously generates a generalizable, 360°-complete, and animatable 3D Gaussian head from a single image. Starting from a feed-forward and animatable framework, we address the 360° full-head avatar generation problem with two novel components. First, to overcome poor hair modeling in full-head avatar generation, we introduce a semantic-aware mesh deformation module that integrates multi-view normals to optimize a FLAME head with hair while preserving its topology structure. Second, to enable effective feed-forward decoding of full-head features, we propose a multi-view feature splatting module that constructs a shared canonical UV representation from features across multiple views through differentiable bilinear splatting, hierarchical UV mapping, and visibility-aware fusion. This approach preserves both global structural coherence and local high-frequency details across all viewpoints, ensuring 360° consistency without per-instance optimization. Extensive experiments demonstrate that OMEGA-Avatar achieves state-of-the-art performance, significantly outperforming existing baselines in 360° full-head completeness while robustly preserving identity across different viewpoints.

</details>


### [409] [Iskra: A System for Inverse Geometry Processing](https://arxiv.org/abs/2602.12105)
*Ana Dodik,Ahmed H. Mahmoud,Justin Solomon*

Main category: cs.GR

TL;DR: 本文提出了一种可微分几何处理系统，能够对多种几何算法（如曲率流、共形参数化等）进行高效求导，兼容机器学习框架，无需重写原有算法，具有低实现成本、高运行效率和低内存开销。


<details>
  <summary>Details</summary>
Motivation: 几何处理算法在逆向问题（如形状优化、参数估计）中难以求导，现有可微优化工具通用但效率低、内存开销大，缺乏针对几何处理的定制化支持。

Method: 结合散射-聚集网格处理范式与张量工作流，利用伴随法（adjoint method）自动为用户指定的命令式代码生成高效反向传播；支持本地-全局及ADMM等几何专用求解器，并与主流机器学习框架集成。

Result: 成功实现了对平均曲率流、谱共形参数化、测地距离计算和ARAP变形等典型几何算法的端到端可微分；实验表明其在可用性、运行速度和内存占用上优于通用可微优化工具。

Conclusion: 该系统为几何处理提供了轻量、高效、即插即用的可微分能力，推动了可微分几何处理在机器学习驱动的逆向建模任务中的实际应用。

Abstract: We propose a system for differentiating through solutions to geometry processing problems. Our system differentiates a broad class of geometric algorithms, exploiting existing fast problem-specific schemes common to geometry processing, including local-global and ADMM solvers. It is compatible with machine learning frameworks, opening doors to new classes of inverse geometry processing applications. We marry the scatter-gather approach to mesh processing with tensor-based workflows and rely on the adjoint method applied to user-specified imperative code to generate an efficient backward pass behind the scenes. We demonstrate our approach by differentiating through mean curvature flow, spectral conformal parameterization, geodesic distance computation, and as-rigid-as-possible deformation, examining usability and performance on these applications. Our system allows practitioners to differentiate through existing geometry processing algorithms without needing to reformulate them, resulting in low implementation effort, fast runtimes, and lower memory requirements than differentiable optimization tools not tailored to geometry processing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [410] [From Noise to Order: Learning to Rank via Denoising Diffusion](https://arxiv.org/abs/2602.11453)
*Sajad Ebrahimi,Bhaskar Mitra,Negar Arabzadeh,Ye Yuan,Haolun Wu,Fattane Zarrinkalam,Ebrahim Bagheri*

Main category: cs.IR

TL;DR: 本文提出了一种基于去噪扩散的生成式学习排序（LTR）方法DiffusionRank，区别于传统判别式LTR建模条件概率，该方法建模查询-文档特征与相关性标签的联合分布，以提升模型鲁棒性，并在实验中显著优于对应判别式模型。


<details>
  <summary>Details</summary>
Motivation: 传统判别式LTR模型可能因过参数化而在训练数据上找到多种拟合方式，而能解释完整数据分布的生成式建模范式有望带来更鲁棒的排序模型。

Method: 将面向表格数据的去噪扩散生成模型TabDiff扩展至LTR任务，构建生成式版本的经典点式（pointwise）和对式（pairwise）LTR目标函数，提出DiffusionRank模型。

Result: DiffusionRank模型在实验中显著优于其对应的判别式基线模型。

Conclusion: 生成式建模（尤其是扩散模型）为信息检索中的学习排序提供了新范式和广阔的研究空间。

Abstract: In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.

</details>


### [411] [KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance](https://arxiv.org/abs/2602.11518)
*Yupeng Li,Ben Chen,Mingyue Cheng,Zhiding Liu,Xuxin Zhang,Chenyi Lei,Wenwu Ou*

Main category: cs.IR

TL;DR: 本文介绍了KuaiSearch，一个基于快手平台真实用户搜索交互构建的最大规模电商搜索数据集，覆盖冷启动用户和长尾商品，并涵盖召回、排序和相关性判断三个搜索阶段，旨在推动大语言模型在电商搜索中的研究与应用。


<details>
  <summary>Details</summary>
Motivation: 现有电商搜索数据集存在查询人工构造、过滤冷启动用户和长尾商品、文本匿名化、仅覆盖单阶段等问题，限制了大语言模型在该领域的研究。

Method: 构建并发布KuaiSearch数据集，基于快手真实用户搜索行为，保留原始查询和自然语言商品文本，覆盖冷启动用户与长尾商品，并系统涵盖召回、排序和相关性判断三阶段；同时开展多维度数据分析与基准实验。

Result: KuaiSearch是目前最大规模的真实电商搜索数据集，实验证明其能为现实电商搜索研究提供有力支撑。

Conclusion: KuaiSearch有效弥补了现有数据集的不足，为LLM驱动的电商搜索研究提供了更真实、全面、实用的基础资源。

Abstract: E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.

</details>


### [412] [LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling](https://arxiv.org/abs/2602.11562)
*Tianhe Lin,Ziwei Xiong,Baoyuan Ou,Yingjie Qin,Lai Xu,Xiaocheng Zhong,Yao Hu,Zhiyong Wang,Tao Zhou,Yubin Xu,Di Wu*

Main category: cs.IR

TL;DR: 本文提出LASER框架，通过SeqVault系统优化和Segmented Target Attention算法优化，解决了超长用户行为序列建模中的I/O延迟和计算复杂度瓶颈，在小红书工业场景中实现显著商业提升。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统需建模超长用户行为序列以捕捉长期兴趣，但面临高I/O延迟和标准注意力机制的二次方计算复杂度两大瓶颈。

Method: 提出LASER全栈优化框架：1）系统层：SeqVault——基于DRAM-SSD混合索引的schema感知服务基础设施；2）算法层：Segmented Target Attention（STA）结合sigmoid门控与Global Stacked Target Attention（GSTA）模块，实现高效序列压缩与跨段建模。

Result: 离线实验显著优于SOTA；线上A/B测试覆盖超1亿DAU，ADVV提升2.36%，营收提升2.08%。

Conclusion: LASER在保持建模能力的同时突破了工业级实时推荐的延迟墙，验证了系统与算法协同优化的有效性与可部署性。

Abstract: Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict "Latency Wall", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.

</details>


### [413] [Analytical Search](https://arxiv.org/abs/2602.11581)
*Yiteng Tu,Shuo Miao,Weihang Su,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 本文提出了一种新的搜索范式——分析型搜索（analytical search），旨在解决趋势分析、因果影响评估等分析性信息需求，通过证据驱动、流程导向的多步推理工作流，实现可验证的结论生成。


<details>
  <summary>Details</summary>
Motivation: 现有检索范式（如相关性排序或RAG）难以满足端到端分析任务的需求，尤其在推理控制、证据使用和结果可验证性方面存在不足，且难以应对高问责性与多样化效用概念的分析查询。

Method: 提出分析型搜索范式，将其定义为一种证据治理、过程导向的分析工作流；构建统一系统框架，涵盖查询理解、召回导向检索、推理感知融合与自适应验证四个模块。

Result: 明确了分析型搜索与传统检索范式的区别，提出了系统性框架，并指出了未来构建分析型搜索引擎的关键研究方向。

Conclusion: 分析型搜索是一种具有概念意义与实践价值的新兴搜索范式，亟需学界与工业界共同推动下一代支持分析性信息需求的搜索引擎发展。

Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.
  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.

</details>


### [414] [Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation](https://arxiv.org/abs/2602.11605)
*Yixiao Chen,Yuan Wang,Yue Liu,Qiyao Wang,Ke Cheng,Xin Xu,Juntong Yan,Shuojin Yang,Menghao Guo,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: Rec2PM是一种生成式推荐框架，通过将长用户交互历史压缩为紧凑的Preference Memory token，实现高效并行训练与迭代推理，显著降低计算开销和存储需求，同时提升推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 传统生成式推荐模型使用全注意力建模用户行为，难以扩展到终身序列，因计算成本高且随机交互导致噪声累积。

Method: 提出Rec2PM框架，采用自参照的教师强制策略：利用全局历史生成参考记忆作为监督目标，支持并行化循环更新；将记忆表示为token嵌入而非KV缓存，提升存储效率。

Result: 在大规模基准测试中，Rec2PM显著降低推理延迟和内存占用，并优于全序列模型的准确性；分析表明Preference Memory起到去噪信息瓶颈作用，有效提取鲁棒的长期兴趣。

Conclusion: Rec2PM通过紧凑记忆表征与并行-迭代协同机制，在保持建模能力的同时解决了生成式推荐中长序列建模的效率与噪声难题。

Abstract: Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.

</details>


### [415] [Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts](https://arxiv.org/abs/2602.11622)
*Haiyang Jiang,Tong Chen,Xinyi Gao,Guansong Pang,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文提出了一种面向零样本图异常检测（GAD）的新型混合专家（MoE）框架EvoFG，通过LLM驱动的进化式结构特征生成与记忆增强的不变性路由机制，有效缓解跨图分布偏移带来的专家路由偏差问题，显著提升零样本泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有单GNN方法难以应对图结构、特征及异常模式的异质性；MoE虽具潜力，但在零样本GAD中受分布偏移影响，面临节点语义不一致和路由规则不可迁移两大路由挑战。

Method: 提出EvoFG框架：1）基于LLM的进化式特征生成机制，迭代构造并用Shapley值筛选信息性结构特征；2）设计带不变学习目标的记忆增强路由器，提升跨域路由泛化能力。

Result: 在六个基准数据集上，EvoFG持续超越当前最优方法，展现出强鲁棒性与稳定零样本GAD性能。

Conclusion: EvoFG通过可解释的特征演化与不变路由学习，为零样本图异常检测提供了更鲁棒、可泛化的MoE新范式。

Abstract: Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.

</details>


### [416] [IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation](https://arxiv.org/abs/2602.11664)
*Huimin Yan,Longfei Xu,Junjie Sun,Zheng Liu,Wei Luo,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 本文提出了IntTravel，首个大规模公共集成旅行推荐数据集，并基于此构建了一个端到端解码器-only生成式多任务推荐框架，在多个指标上达到SOTA性能，并已成功部署于高德地图。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于碎片化、小规模的数据集，仅关注“下一个POI”推荐，忽略了出发时间、交通方式和途中情境需求等关键旅行要素，难以全面建模用户旅程。

Method: 构建了包含41亿交互的大规模IntTravel数据集；提出端到端、仅解码器的生成式多任务推荐框架，融合信息保留、选择与因子分解机制，以平衡任务协同与任务特化。

Result: 在IntTravel及非旅行基准数据集上均取得SOTA性能；已部署于高德地图，带来1.09%的CTR提升。

Conclusion: IntTravel填补了集成旅行推荐数据空白，所提框架验证了多任务联合建模的有效性与泛化能力，推动了旅行推荐从单一POI预测迈向全旅程智能推荐。

Abstract: Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: "when to depart", "how to travel", "where to go", and "what needs arise via the route". However, current research is limited by fragmented datasets that focus merely on next POI recommendation ("where to go"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.

</details>


### [417] [EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation](https://arxiv.org/abs/2602.11680)
*Yihang Li,Zhuo Liu,Wei Wei*

Main category: cs.IR

TL;DR: 本文提出EpicCBR，一种面向冷启动场景的多视图对比学习框架，通过挖掘用户-物品与捆绑包-物品关系、构建用户画像及基于流行度的新捆绑包表征，显著提升冷启动捆绑推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有捆绑推荐模型依赖观测到的用户-捆绑包交互，难以应对新出现的未交互捆绑包（冷启动问题），且忽视用户-物品和捆绑包-物品间在热门物品上的结构关联。

Method: 提出多视图对比学习框架EpicCBR：1）利用物品关系构建精细化用户画像；2）设计基于流行度的新捆绑包表征方法；3）融合用户-物品、捆绑包-物品等多视图图结构进行对比学习。

Result: 在三个主流数据集上大幅超越SOTA方法，冷启动场景下最高提升达387%；代码与数据集已开源。

Conclusion: EpicCBR有效缓解了冷启动问题，通过多视图图对比学习与细粒度关系建模，提升了模型在冷/热启动场景下的泛化能力与鲁棒性。

Abstract: Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.

</details>


### [418] [Uncertainty-aware Generative Recommendation](https://arxiv.org/abs/2602.11719)
*Chenxiao Fan,Chongming Gao,Yaxin Gong,Haoyan Liu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 本文提出了一种不确定性感知的生成式推荐框架UGR，通过引入不确定性作为优化信号，解决了现有方法中因忽略模型置信度而导致的训练不稳定和决策风险不可量化问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法依赖二元结果正确性进行偏好优化，忽视了模型生成置信度、样本学习难度差异及显式置信表达，导致训练不稳定和决策风险不可量化。

Method: 提出UGR框架，整合三种机制：(1) 不确定性加权奖励以惩罚高置信错误；(2) 难度感知优化动态防止过早收敛；(3) 显式置信对齐赋予模型置信表达能力。

Result: 实验表明UGR不仅提升推荐性能，还显著稳定训练过程，避免标准方法中常见的性能下降，并支持可靠的下游风险感知应用。

Conclusion: 不确定性是生成式推荐中关键的优化信号，UGR通过系统建模不确定性，实现了更鲁棒、可解释且风险可控的推荐。

Abstract: Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.

</details>


### [419] [ULTRA:Urdu Language Transformer-based Recommendation Architecture](https://arxiv.org/abs/2602.11836)
*Alishbah Bashir,Fatima Qaiser,Ijaz Hussain*

Main category: cs.IR

TL;DR: 本文提出ULTRA，一种基于Transformer的乌尔都语新闻推荐框架，通过双嵌入结构和查询长度感知路由机制，动态适配短查询（意图导向）与长查询（上下文丰富），显著提升低资源语言下的语义检索效果。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为低资源语言，缺乏有效的语义内容推荐系统，尤其在个性化新闻检索中；现有方法依赖词法匹配或语言无关技术，难以捕捉语义意图，且在不同查询长度下性能差，导致相关性和适应性不足。

Method: 提出ULTRA框架：采用双嵌入架构与查询长度感知路由机制，依据阈值将用户查询动态分配至标题级或全文级专用语义管道；结合Transformer嵌入与优化池化策略，实现上下文感知的相似性搜索。

Result: 在大规模乌尔都语新闻语料库上的实验表明，ULTRA在各类查询上均显著提升推荐相关性，相比单管道基线，精确率提升超90%。

Conclusion: ULTRA是一种鲁棒、可泛化的低资源语言内容推荐架构，为语义检索系统提供了实用的设计启示。

Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.

</details>


### [420] [Improving Neural Retrieval with Attribution-Guided Query Rewriting](https://arxiv.org/abs/2602.11841)
*Moncef Garouani,Josiane Mothe*

Main category: cs.IR

TL;DR: 本文提出了一种归因引导的查询重写方法，利用检索器的词元级梯度归因指导大语言模型（LLM）进行意图保持的查询澄清，显著提升了神经检索器在模糊或隐含查询下的鲁棒性与效果。


<details>
  <summary>Details</summary>
Motivation: 神经检索器虽有效但对模糊或表述不清的查询十分脆弱；现有方法（如无反馈的LLM重写或仅用于事后分析的可解释性方法）未能闭环解决该问题。

Method: 计算检索器对查询各词元的梯度归因，并将这些归因分数作为软约束嵌入结构化提示中，驱动LLM进行意图保留的查询重写。

Result: 在BEIR基准上验证，该方法在多个数据集上持续超越强基线，尤其在隐含或模糊信息需求场景下提升更显著。

Conclusion: 归因与LLM重写的协同闭环机制能有效增强神经检索器的鲁棒性与泛化能力，为可解释性驱动的检索优化提供了新范式。

Abstract: Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.

</details>


### [421] [Efficient Crawling for Scalable Web Data Acquisition (Extended Version)](https://arxiv.org/abs/2602.11874)
*Antoine Gauquier,Ioana Manolescu,Pierre Senellart*

Main category: cs.IR

TL;DR: 本文提出了一种基于强化学习（睡眠型多臂老虎机）的聚焦式网络爬虫算法SB-CLASSIFIER，用于高效、可扩展地从网站中抓取高质量统计数据库资源，显著减少爬取网页数量的同时大幅提升目标资源召回率。


<details>
  <summary>Details</summary>
Motivation:  journalistic fact-checking 和社会/经济研究需要高质量统计数据库（SDs），但这些数据在线发布方式各异，导致大规模、高效获取困难，亟需提升开放统计数据的可访问性。

Method: 提出一种聚焦式Web爬虫算法，建模为不可行的最优化问题后，采用基于睡眠型多臂老虎机（sleeping bandits）的强化学习方法；SB-CLASSIFIER通过分析超链接所在网页的路径特征，学习识别更可能指向目标资源（即含大量SD的页面）的链接。

Result: 在百万级网页规模的真实网站上实验表明，该爬虫仅爬取网站一小部分页面，即可高效获取其中高比例的目标统计资源。

Conclusion: SB-CLASSIFIER是一种高效、可扩展的聚焦爬虫方案，能显著提升开放统计数据库的获取效率，对事实核查与实证研究具有实用价值。

Abstract: Journalistic fact-checking, as well as social or economic research, require analyzing high-quality statistics datasets (SDs, in short). However, retrieving SD corpora at scale may be hard, inefficient, or impossible, depending on how they are published online. To improve open statistics data accessibility, we present a focused Web crawling algorithm that retrieves as many targets, i.e., resources of certain types, as possible, from a given website, in an efficient and scalable way, by crawling (much) less than the full website. We show that optimally solving this problem is intractable, and propose an approach based on reinforcement learning, namely using sleeping bandits. We propose SB-CLASSIFIER, a crawler that efficiently learns which hyperlinks lead to pages that link to many targets, based on the paths leading to the links in their enclosing webpages. Our experiments on websites with millions of webpages show that our crawler is highly efficient, delivering high fractions of a site's targets while crawling only a small part.

</details>


### [422] [IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval](https://arxiv.org/abs/2602.11941)
*Benjamin Clavié,Atoof Shakir,Jonah Turner,Sean Lee,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 本文介绍了IncompeBench，一个高质量、公开可用的音乐信息检索基准数据集，包含1574个音乐片段、500个查询和超12.5万条人工标注的相关性判断，旨在填补当前MIR领域高质量评估基准的空白。


<details>
  <summary>Details</summary>
Motivation: 当前音乐信息检索（MIR）领域缺乏高质量、公开可复现的评估基准，限制了模型性能的公平比较与进步。

Method: 构建了一个多阶段人工标注流水线，收集并标注了1574个可许可的高质量音乐片段、500个多样化查询及超过125,000条细粒度相关性判断，确保高标注一致性。

Result: 发布了两个版本的公开数据集（strict/lenient），配套提示词开源，已在Hugging Face和GitHub上线。

Conclusion: IncompeBench为MIR研究提供了可靠、透明、可复现的评估标准，有望推动该领域更严谨的发展。

Abstract: Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.

</details>


### [423] [Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems](https://arxiv.org/abs/2602.12041)
*Heng Yu,Xiangjun Zhou,Jie Xia,Heng Zhao,Anxin Wu,Yu Zhao,Dongying Kong*

Main category: cs.IR

TL;DR: 本文提出MLCC及其多通道扩展MC-MLCC，一种高效建模高阶特征交互的结构化架构，兼顾强表达能力、高计算效率与良好可扩展性，在多个基准和工业数据集上显著优于DLRM等基线模型，并已在Bilibili广告系统落地应用。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统中的特征交互模块难以同时兼顾强交互能力、高计算效率和良好可扩展性，在严格生产约束下ROI有限。

Method: 提出MLCC架构，通过分层压缩与动态组合组织特征交叉；进一步设计MC-MLCC，将交互分解至并行子空间以实现高效水平扩展。

Result: 在三个公开基准和一个大规模工业数据集上，AUC最高提升0.52，参数量和FLOPs最多减少26倍；通道缩放比嵌入维度膨胀更高效；线上A/B测试验证其在真实广告平台的有效性。

Conclusion: MLCC/MC-MLCC为高阶特征交互提供了高效、可扩展且实用的解决方案，已在Bilibili广告系统中大规模部署。

Abstract: Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.

</details>


### [424] [Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset](https://arxiv.org/abs/2602.12129)
*Rahin Arefin Ahmed,Md. Anik Chowdhury,Sakil Ahmed Sheikh Reza,Devnil Bhattacharjee,Muhammad Abdullah Adnan,Nafis Sadeq*

Main category: cs.IR

TL;DR: 本文提出了RokomariBG——一个面向孟加拉语文学个性化图书推荐的大规模异构图书知识图谱数据集，并在Top-N推荐任务上对多种代表性推荐模型进行了系统基准测试，验证了多关系结构与文本侧信息的重要性，为低资源语言推荐研究提供了公开基准和资源。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语个性化图书推荐受限于缺乏结构化、大规模且公开可用的数据集。

Method: 构建了包含图书、用户、作者、类别、出版社、评论等多实体及八种关系的异构图书知识图谱RokomariBG；并在Top-N推荐任务上对协同过滤、矩阵分解、内容-based、图神经网络、融合侧信息的混合模型及双塔神经检索模型等进行系统基准评测。

Result: 神经检索模型性能最优（NDCG@10 = 0.204），结果凸显利用多关系结构和文本侧信息的重要性。

Conclusion: RokomariBG为孟加拉语图书推荐研究建立了首个公开、可复现的基准和基础资源，推动低资源文化领域推荐系统的后续研究。

Abstract: Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.
  To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset

</details>


### [425] [SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization](https://arxiv.org/abs/2602.12187)
*Sunghwan Kim,Wooseok Jeong,Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.IR

TL;DR: 本文提出了SAGEO Arena，一个用于评估搜索增强生成引擎优化（SAGEO）的现实、可复现环境，弥补了现有基准在端到端可见性评估和结构化信息建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有SAGEO评估基准缺乏端到端可见性评估能力，且忽略真实网页中的结构化信息（如schema markup），无法反映实际搜索与生成联合优化需求。

Method: 构建SAGEO Arena，集成完整生成式搜索流水线，基于含丰富结构信息的大规模网页语料，支持分阶段（检索、重排序、生成）的SEO与GEO联合优化分析。

Result: 发现现有SAGEO方法在真实条件下大多不实用，常损害检索与重排序性能；结构化信息有助于缓解该问题；有效SAGEO需针对各流水线阶段定制优化策略。

Conclusion: SAGEO Arena为超越简化设定的现实SAGEO评估与优化提供了新基准和研究基础。

Abstract: Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.

</details>


### [426] [AttentionRetriever: Attention Layers are Secretly Long Document Retrievers](https://arxiv.org/abs/2602.12278)
*David Jiahao Fu,Lam Thanh Do,Jiayu Li,Kevin Chen-Chuan Chang*

Main category: cs.IR

TL;DR: 本文提出了AttentionRetriever，一种结合注意力机制与基于实体的检索方法，用于提升长文档检索的效果，解决了上下文感知、因果依赖和检索范围等关键挑战，并在实验中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型未针对长文档检索设计，难以应对上下文感知、因果依赖和检索范围等关键挑战。

Method: 提出AttentionRetriever模型，利用注意力机制和基于实体的检索构建上下文感知的长文档嵌入，并确定检索范围。

Result: 在多个长文档检索数据集上显著优于现有检索模型，同时保持与稠密检索模型相当的效率。

Conclusion: AttentionRetriever有效提升了长文档检索性能，为RAG系统处理长文档任务提供了更优的检索支持。

Abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.

</details>
