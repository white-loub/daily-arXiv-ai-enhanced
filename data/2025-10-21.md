<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.LG](#cs.LG) [Total: 224]
- [cs.RO](#cs.RO) [Total: 48]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.MA](#cs.MA) [Total: 9]
- [cs.GR](#cs.GR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 70]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出ESCA框架和SGClip模型，通过结构化时空理解提升多模态大语言模型在具身智能体中的表现，无需人工标注即可生成场景图，并显著减少感知错误。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM训练依赖高层视觉-声音-文本对，缺乏像素级视觉内容与文本语义的细粒度对齐。

Method: 提出ESCA框架与基于CLIP的SGClip模型，采用神经符号学习 pipeline，在87K+视频上利用模型驱动的自监督和结构化推理训练，实现开放域、可提示的场景图生成。

Result: SGClip在场景图生成和动作定位任务中表现出色；ESCA显著提升开源与商业MLLM在两个具身环境中的性能，降低感知错误，使开源模型超越专有基线。

Conclusion: ESCA通过结构化空间-时间理解有效增强MLLM的具身情境化能力，推动无需人工标注的高质量多模态理解发展。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CrossRay3D的稀疏多模态检测器，通过引入Ray-Aware Supervision和Class-Balanced Supervision提升稀疏检测器中token表示的质量，有效保留几何结构和类别分布信息，并结合Ray PE解决LiDAR与图像模态间的分布差异，在nuScenes数据集上实现了72.4 mAP和74.7 NDS的SOTA性能，且运行速度更快，具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏跨模态检测器忽视token表示质量，导致前景质量差、性能受限，难以满足下游任务需求。

Method: 提出Sparse Selector（SS），包含Ray-Aware Supervision（RAS）以保留几何信息，Class-Balanced Supervision以增强小物体相关token的保留，并设计Ray Positional Encoding（Ray PE）对齐LiDAR与图像模态分布，最终构建端到端稀疏多模态检测器CrossRay3D。

Result: 在nuScenes数据集上达到72.4 mAP和74.7 NDS，性能领先且推理速度快1.84倍，同时在传感器缺失场景下表现出强鲁棒性。

Conclusion: CrossRay3D通过改进token表示质量显著提升了稀疏检测器的性能，在精度、效率和鲁棒性之间取得了优越平衡，为稀疏多模态检测提供了有效方案。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 本文提出了一种利用城市街道CCTV视频流进行多缺陷检测与分割，并结合视觉语言模型生成结构化维修行动计划的综合管道。


<details>
  <summary>Details</summary>
Motivation: 传统人工巡检成本高且危险，现有自动系统通常只能处理单一缺陷类型或输出非结构化结果，难以直接指导维修。因此需要一个能同时检测多种道路缺陷并生成可执行维修建议的自动化系统。

Method: 采用YOLO系列目标检测器对CCTV视频流中的裂缝、坑洞和漏液等缺陷进行检测与分割，并将检测结果输入视觉语言模型（VLM），利用其场景理解能力生成结构化的JSON格式行动方案，包括事件描述、推荐工具、尺寸、修复计划和紧急警报。

Result: 在公开数据集和实际CCTV片段上的实验表明，该系统能够准确识别多种道路缺陷，并生成连贯、结构化的维修摘要。视觉语言模型能有效整合检测结果并生成实用的维护建议。

Conclusion: 该方法实现了从城市监控视频中自动检测道路缺陷并生成可操作的维修计划，具有实际应用潜力；未来工作需解决模型轻量化、实时性及大规模城市部署的挑战。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（MLLMs）的工业异常检测新范式IAD-GPT，结合图像级和像素级信息与丰富文本语义，通过异常提示生成器、文本引导增强器和多掩码融合模块提升检测与分割性能，在MVTec-AD和VisA数据集上实现了最先进的自监督与少样本异常检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和细粒度描述能力，而基于预训练大模型的方法未能充分激发其在异常检测中的潜力。

Method: 提出IAD-GPT框架，利用异常提示生成器（APG）生成针对特定对象的详细异常提示，并通过CLIP等视觉-语言模型实现检测与分割；设计文本引导增强器以动态选择增强路径，提升MLLM的视觉定位能力；引入多掩码融合模块，将掩码作为专家知识融入，增强对像素级异常的感知。

Result: 在MVTec-AD和VisA数据集上，IAD-GPT在自监督和少样本设置下的异常检测与分割任务中均达到最先进性能。

Conclusion: IAD-GPT有效结合多模态大语言模型与细粒度视觉信息，显著提升了工业异常检测的准确性与可解释性，支持多轮交互与详细描述，为实际应用提供了新思路。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 该研究比较了自由文本（FT）、结构化报告（SR）和AI辅助结构化报告（AI-SR）三种模式对胸部X光片分析的影响，发现AI-SR在诊断准确性、报告效率和用户体验方面均表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索结构化报告和人工智能如何改善放射科医生的影像分析行为、诊断准确性和工作效率。

Method: 前瞻性研究，纳入4名新手和4名非新手读者，使用定制查看器和眼动仪分析35张床旁胸片，比较三种报告模式下的诊断准确性、报告时间、眼动指标和用户体验。

Result: AI-SR的诊断准确性（κ=0.71）显著高于FT和SR，报告时间最短（25±9秒），眼动指标显示视觉注意力更集中，用户偏好最高。SR也提高了效率并减少了注视次数。

Conclusion: 结构化报告可提高效率并引导视觉注意力至图像，而AI预填的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 本文提出了一种数据驱动的框架，用于分析和减轻图像分类中的交叉偏差。作者引入了交叉公平性评估框架（IFEF），并提出了偏差加权增强（BWA）方法，实验表明该方法显著提高了代表性不足类别的准确性，并减少了公平性指标的差异。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在不平衡数据集上训练时常常表现出交叉偏差，即由多个属性（如对象类别和环境条件）交互引起的系统性错误。为了解决这一问题，需要一种能够识别并缓解此类偏差的有效方法。

Method: 提出了交叉公平性评估框架（IFEF），结合定量公平性度量和可解释性工具来识别模型预测中的偏差模式；在此基础上，提出偏差加权增强（BWA），一种基于子组分布统计自适应调整变换强度的数据增强策略。

Result: 在Open Images V7数据集上的实验显示，BWA将代表性不足的类别-环境交叉点的准确率提高了最高24个百分点，公平性指标差异减少了35%，且多次独立实验的统计分析证实了改进的显著性（p < 0.05）。

Conclusion: 所提方法为分析和解决图像分类系统中的交叉偏差提供了一种可复现的途径，有效提升了模型在不同子群体上的公平性和性能。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 本文提出了一种可微分的神经网络量化方法，证明了其收敛性，并支持n位量化，在ImageNet上使用ResNet18实现了接近全精度的性能，仅需15个训练周期。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法大多不可微分且缺乏理论收敛保证，同时在激活值和权重同时量化时精度较低，本文旨在解决这两个问题。

Method: 提出一种可微分的移位/对数量化方法，支持权重和激活值的同时量化，并提供该方法收敛到最优网络的理论证明，支持n位量化（如2^n形式）。

Result: 在ImageNet上使用ResNet18进行实验，仅量化权重时准确率比全精度低不到1%，15个epoch完成训练；在权重和激活均量化的情况下达到与SOTA相当的精度，推理成本略高于1位量化但无需高精度乘法。

Conclusion: 该可微分移位量化方法具有理论保障，训练高效，兼顾精度与推理效率，适用于实际部署。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出一种基于预训练rectified flow模型的无训练方法，通过周期性添加可微损失函数形式的引导（如部分感知损失和自相似性），有效实现跨显著几何差异的3D资产外观迁移，克服了现有方法的局限性，并采用GPT-based系统进行更符合人类感知的质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有3D外观迁移方法在输入与目标对象几何差异较大时表现不佳，直接应用3D生成模型效果不理想，亟需一种鲁棒且无需训练的方法来提升迁移质量。

Method: 基于预训练的图像或文本条件rectified flow模型，在采样过程中周期性引入可微的引导损失（如部分感知损失和自相似性损失），以指导外观迁移过程，整个方法无需额外训练。

Result: 该方法在定性和定量上均优于基线方法，能成功将纹理和几何细节迁移到输入3D资产上；同时发现传统评估指标不适用，因此采用GPT-based系统进行客观排序评估，并通过用户研究验证了其有效性。

Conclusion: 所提出的基于通用引导原理的无训练方法能有效解决跨大几何差异的3D外观迁移问题，具有良好的通用性，可扩展至其他扩散模型和引导函数，并提出了更优的评估范式。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [9] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出了一种名为StripRFNet的新型深度神经网络，用于道路表面损伤检测，通过三个模块有效提升了细长裂缝和小尺度损伤的识别精度，在RDD2022基准上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 道路表面损伤检测对交通安全和可持续城市发展至关重要，但因损伤形态多样、细长裂缝难以捕捉及小尺度损伤识别误差高等问题而具有挑战性。

Method: 设计了包含形状感知模块（SPM）、条带感受野模块（SRFM）和小尺度增强模块（SSEM）的StripRFNet，分别通过多尺度特征聚合中的大可分离核注意力、条带卷积与池化、高分辨率特征图与动态上采样来提升检测性能。

Result: 在RDD2022数据集上，StripRFNet在F1-score、mAP50和mAP50:95指标上均优于现有方法，全数据集F1-score达80.33%，且保持较高的推理速度。

Conclusion: StripRFNet在道路损伤检测中实现了精度与效率的平衡，为智能道路维护和可持续基础设施管理提供了有力工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [10] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出了一种名为M2H的视频生成任务，旨在从模特架视频生成具有身份可控性的逼真人像视频，并设计了M2HVideo框架以解决头部与身体运动错位及身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于模特架的服装展示缺乏真实感和细节表现力，难以满足线上时尚展示的需求，因此需要一种能生成更真实、更具表现力的人体视频的方法。

Method: 提出M2HVideo框架，包含动态姿态感知头部编码器、像素空间中的镜像损失（基于DDIM去噪）以及分布感知适配器，以实现身份保持和时序一致性。

Result: 在UBC、ASOS和MannequinVideos数据集上的实验表明，该方法在服装一致性、身份保持和视频质量方面优于现有最先进方法。

Conclusion: M2HVideo有效解决了从模特架到人像视频生成中的关键挑战，在保持身份和服装细节的同时实现了高保真、连贯的视频合成。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [11] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: 本文提出了一种名为ObjectTransforms的新方法，通过在训练和推理阶段对物体进行特定变换来量化和减少基于视觉的目标检测中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于数据偏差和分布变化等问题，现有的基于视觉的检测模型在自动驾驶中仍面临不确定性挑战，影响感知可靠性。

Method: 在训练阶段，采用颜色空间扰动和扩散模型生成多样化的行人实例；在推理阶段，通过对检测到的物体施加扰动并计算检测分数的方差来实时量化不确定性，并利用该信号过滤误检和恢复漏检。

Result: 在NuImages 10K数据集上使用YOLOv8的实验表明，该方法在训练阶段显著提升了各类物体的准确性和不确定性降低效果，在推理阶段能有效区分误检与真检，提升精度-召回曲线。

Conclusion: ObjectTransforms是一种轻量且有效的方法，能够在训练和推理阶段分别减少和量化基于视觉感知中的不确定性，具有实际应用潜力。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [12] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: Aria Gen 2 Pilot Dataset (A2PD) 是一个使用 Aria Gen 2 眼镜采集的前沿第一人称多模态开放数据集，包含日常活动的五种主要场景，并提供原始传感器数据和机器感知算法输出，旨在推动智能眼镜和感知技术的研究。


<details>
  <summary>Details</summary>
Motivation: 为了促进基于智能眼镜的感知与交互研究，提供高质量、真实环境下的多模态第一人称数据。

Method: 通过配备 Aria Gen 2 眼镜的主被试 Dia'ane 及其朋友在真实生活场景中记录数据，涵盖清洁、烹饪、进食、玩耍和户外行走五种情境，并集成多种机器感知算法处理结果。

Result: 发布了包含丰富传感器数据和感知算法输出的 A2PD 数据集，展示了设备在不同用户和环境下对佩戴者自身、周围环境及二者交互的稳定感知能力。

Conclusion: A2PD 是一个具有高实用价值的开源数据资源，支持学术与工业界在智能穿戴设备、计算机视觉和人机交互等领域的研究与开发。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [13] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出一种自监督深度学习框架，通过回归预文本任务分类骨骼标志，在血栓切除术中优于现有方法，并有望实现C臂的完全自主控制。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术治疗缺血性卒中效果显著，但资源和人力需求大，因此需要通过深度学习自动化关键环节以提高效率和安全性。

Method: 采用基于回归的预文本任务的自监督学习框架，对多种骨骼标志进行分类。

Result: 该模型在回归和分类任务上均优于现有方法，位置预文本任务显著提升了下游分类性能。

Conclusion: 所提出的框架在自动化血栓切除术方面具有潜力，未来工作将扩展至实现C臂从骨盆到头部路径的完全自主控制。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [14] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DuetMatch的双分支半监督框架，用于医学图像分割，通过异步优化、解耦Dropout扰动、配对CutMix交叉指导和一致性匹配策略，在标注数据有限的情况下显著提升了模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像中注释数据稀缺，半监督学习成为重要手段；现有教师-学生框架在联合优化整个网络时存在收敛困难和稳定性差的问题，尤其是在复杂场景下。

Method: 提出DuetMatch框架：1）双分支异步优化，分别训练编码器和解码器；2）解耦Dropout扰动增强一致性正则化；3）Pair-wise CutMix Cross-Guidance通过增强输入对交换伪标签提升模型多样性；4）Consistency Matching利用冻结教师模型的稳定预测优化伪标签，减轻确认偏见。

Result: 在ISLES2022和BraTS等脑MRI分割基准上的实验表明，DuetMatch在多种半监督设定下均优于当前最先进方法，具有良好的有效性和鲁棒性。

Conclusion: DuetMatch通过异步优化和多种改进策略，有效解决了半监督医学图像分割中的训练不稳定和伪标签噪声问题，为低标签场景下的模型设计提供了新思路。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [15] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种利用X射线图像自主导航C臂至预定义解剖标志的管道，结合概率损失和骨骼姿态正则化训练框架，并通过合成数据验证了其在多种架构下的高定位精度和良好的预测校准性能。


<details>
  <summary>Details</summary>
Motivation: 临床中C臂的手动对齐会增加辐射暴露和操作延迟，因此需要一种准确可靠的自动定位方法以提升介入手术效率与安全性。

Method: 基于输入的X射线图像，模型预测朝向身体各目标标志点的3D位移向量；采用结合概率损失与骨骼姿态正则化的训练框架，并利用共形预测对模型预测中的偶然和认知不确定性进行校准，生成3D置信区域。

Result: 在DeepDRR生成的合成X射线数据集上验证，结果显示多种网络架构均表现出强定位精度和良好校准的预测边界。

Conclusion: 该管道能够为安全可靠的自主C臂系统提供关键技术支撑，具备临床应用潜力。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [16] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 本文提出了一种自动图像质量评估（IQA）预过滤方法，用于减少人工审核成本，并在背景修复用例中实现了51.61%的成本节约。


<details>
  <summary>Details</summary>
Motivation: 现有的深度生成模型虽能生成高质量图像，但其输出质量仍不稳定，导致生产流程中需要大量人工进行图像质量评估，耗时且昂贵。因此，需要一种自动预过滤机制来提升进入人工审核阶段的图像质量，降低整体成本。

Method: 提出一个基于精度和通过率的通用IQA引擎成本节省估算公式，并在一个背景修复的应用场景中，采用简单的AutoML解决方案实现自动预过滤。

Result: 在背景修复任务中，使用AutoML构建的IQA预过滤器显著提升了输入到人工审核阶段的图像质量，实现了51.61%的成本节约。

Conclusion: 引入自动IQA预过滤阶段可有效降低图像生成 pipeline 中的人工审核成本，该成本模型为设计高效过滤系统提供了量化依据。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [17] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 提出PRISM模型，将fMRI信号投影到结构化文本空间以重建视觉刺激，通过对象中心扩散和属性关系搜索模块提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 探索最适合从fMRI信号重建视觉刺激的潜在空间类型及其组织方式。

Method: 将fMRI信号映射到结构化文本空间，并结合对象中心扩散模块与属性关系搜索模块，利用预训练生成模型进行图像重建。

Result: 在真实数据集上实验显示，相比现有方法最高降低8%的感知损失。

Conclusion: 结构化文本空间作为中间表示能更有效地桥接fMRI信号与图像重建。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [18] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提倡在热带农业遥感制图中采用以数据为中心的人工智能（DCAI）方法，强调通过提升数据质量与管理来增强模型的鲁棒性和可扩展性，并提出一个包含9种成熟方法的实用流程。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感制图面临标注数据质量低、标注成本高、数据变异性大和区域泛化难等挑战，传统以模型为中心的方法受限，因此需要更注重数据质量的解决方案。

Method: 采用数据中心化AI框架，综合应用置信学习、核心集选择、数据增强和主动学习等技术，评估并筛选出25种策略中的9种最成熟、易实施的方法构建实用流程。

Result: 明确了25种数据处理策略在大规模农业制图中的适用性，并提出了一个可直接应用于热带地区大规模农业制图项目的九步实用DCAI流程。

Conclusion: 以数据为中心的方法能有效应对热带农业遥感中的数据挑战，提升模型性能，具有良好的实际应用前景和推广价值。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [19] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种灵活的训练方法StretchySnake，利用SSMs的固有适应性，在不同时空分辨率下采样视频并动态插值模型权重，使模型在多种动作识别任务中表现出色且具有强适应性。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解训练方法针对Transformer设计，未能充分利用SSMs的优势，导致在未见过的时空分辨率上性能下降，即存在时空不灵活性问题。

Method: 在训练过程中以不同的时空分辨率采样视频，并动态插值模型权重以适应任意时空尺度，从而增强SSMs的灵活性。提出了五种灵活训练变体并确定了最有效的策略。

Result: StretchySnake在短动作（UCF-101, HMDB-51）和长动作（COIN, Breakfast）基准上优于Transformer和SSM基线最高达28%，并在细粒度动作识别（SSV2, Diving-48）上表现出强适应性。

Conclusion: 该方法提供了一种简单、即插即用的训练方案，使视频SSMs更具鲁棒性、分辨率无关性和效率，适用于多样化的动作识别场景。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [20] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出了一种新的异构集成架构VM-BeautyNet，结合Vision Transformer和基于Mamba的视觉模型，用于面部美学预测，在SCUT-FBP5500数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在捕捉影响人类审美判断的整体面部特征方面存在局限，而Vision Transformers虽然能建模长距离空间关系但计算复杂度高。

Method: 设计了一个融合Vision Transformer和Mamba骨干网络的异构集成架构VM-BeautyNet，前者捕捉全局结构，后者以线性复杂度高效建模长程依赖。

Result: 在SCUT-FBP5500数据集上达到SOTA性能：PC为0.9212，MAE为0.2085，RMSE为0.2698，并通过Grad-CAM可视化验证了双骨干的互补特征提取能力。

Conclusion: VM-BeautyNet有效结合了两种模型的优势，为计算美学任务提供了一个强大且可解释的新架构范式。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [21] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 本研究开发了一种基于卷积神经网络（CNN）和图像增强硬件的系统，用于口腔鳞状细胞癌（OCSCC）的早期检测。


<details>
  <summary>Details</summary>
Motivation: 由于OCSCC在早期阶段症状隐匿、生长缓慢且常发生在隐蔽区域，导致诊断延迟和可预防的死亡，因此需要一种高效的早期检测方法。

Method: 使用4293张包含良恶性肿瘤及阴性样本的图像训练CNN，并在不同分辨率下测试其精度、召回率和平均精度均值（mAP）；设计了图像采集与处理硬件以提升图像质量，并开发应用程序支持系统应用。

Result: 高分辨率图像显著提升预测准确率，但呈现对数尺度下的收益递减效应；所设计的硬件有效提高了图像细节捕获能力。

Conclusion: 结合CNN与专用图像硬件的系统能有效提升OCSCC早期检测的准确性，具备临床辅助诊断潜力。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [22] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是由Meta的Codec Avatars Lab推出的一个大规模多模态3D运动数据集，包含439名参与者超过500小时的3D动作数据，涵盖单人和多人互动场景，提供详细的运动追踪、文本标注和独立音频轨道。


<details>
  <summary>Details</summary>
Motivation: 为了推动虚拟化身、人机交互和行为理解等领域的发展，需要一个高质量、多样化的3D人体运动数据集，特别是包含丰富社交互动和精细手势的数据。

Method: 通过多摄像头系统采集439名参与者的3D运动数据，涵盖单人动作和多人互动场景，并进行3D姿态追踪、手部追踪、身体形状建模，同时提供文本标注和每人独立的音频轨道。

Result: 构建了包含超过5400万帧的大型3D运动数据集Embody 3D，覆盖广泛的动作类型和社交情境，支持多种模态数据同步使用。

Conclusion: Embody 3D为研究复杂的人类行为、社交互动及多模态感知提供了宝贵资源，有望推动虚拟现实、AI化身和人机交互技术的发展。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [23] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类与物体交互的主动式场景分解与重建新方法，通过利用第一人称视频流中的交互线索，实现动态环境下的在线、渐进式场景建模。


<details>
  <summary>Details</summary>
Motivation: 传统物体级重建方法在动态场景中存在模糊性，难以准确处理场景变化；本文旨在利用人类行为作为关键线索，提升动态场景分解与重建的准确性与灵活性。

Method: 提出一种主动式场景分解与重建框架，通过分析人类-物体交互，在线迭代地拆解和重构环境，并结合高斯点阵技术实现精确的相机与物体位姿估计、实例分解和实时地图更新。

Result: 在多个真实世界场景中验证了该方法的有效性，实现了高保真、高效的动态场景建模，具有优于传统方法的性能表现。

Conclusion: 该方法充分利用人类行为线索，为动态场景的在线重建提供了一种灵活且渐进的解决方案，显著提升了复杂环境下的场景理解能力。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [24] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: 提出Cerberus，一种两阶段级联系统，用于高效且准确的实时视频异常检测，结合轻量级过滤与基于规则的偏差检测，在保持高精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在视频异常检测中计算成本高且视觉定位不稳定，难以实现实时部署。

Method: 采用两阶段级联架构：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理；引入运动掩码提示和基于规则的偏差检测机制。

Result: 在四个数据集上平均达到57.68 fps（NVIDIA L40S GPU），比现有方法快151.79倍，同时保持97.2%的准确率。

Conclusion: Cerberus在不牺牲精度的前提下大幅提升了推理速度，是适用于实时视频分析的实用解决方案。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [25] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准，用于评估大视觉语言模型上的成员推断攻击，揭示了现有方法在无偏条件下性能退化至随机猜测的问题。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击的高成功率可能源于数据集构建中的分布偏差，而非真正识别出成员状态，因此需要一个更公平的评估基准。

Method: 构建了一个包含6000张图像的受控基准，平衡成员与非成员样本的分布，并提供三个训练阶段的真实成员标签。

Result: 实验表明，在无偏条件下，最先进的成员推断方法性能接近随机水平。

Conclusion: OpenLVLM-MIA揭示了当前LVLM隐私评估的局限性，为发展更强的隐私保护技术提供了可靠基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [26] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架Stroke2Sketch，通过跨图像笔画注意力机制实现风格化草图生成，精确传递笔画属性并保持语义结构。


<details>
  <summary>Details</summary>
Motivation: 需要在保持语义结构和内容保真的同时，精确传递参考风格中的笔画属性（如线条粗细、变形、纹理稀疏性）。

Method: 提出了Stroke2Sketch，引入嵌入自注意力层的跨图像笔画注意力机制，建立细粒度语义对应，实现准确的笔画属性迁移；结合自适应对比度增强和语义聚焦注意力来增强内容保持和前景突出。

Result: 生成的草图在风格忠实性和语义连贯性上优于现有方法，能够高度还原手工绘制效果，在表达性笔画控制方面表现更优。

Conclusion: Stroke2Sketch是一种有效的无需训练的草图风格迁移框架，能够在保持结构完整性的同时实现高质量的风格化草图生成。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [27] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本研究通过构建大规模深伪检测数据集ScaleDF，系统分析了模型性能与真实图像域、深伪生成方法及训练样本数量之间的关系，发现检测误差随规模增加呈现幂律衰减，揭示了深伪检测中的可预测扩展规律。


<details>
  <summary>Details</summary>
Motivation: 为了理解深伪检测任务中的扩展规律，特别是在模型规模、数据多样性和生成方法不断增长情况下的性能变化，需要一个足够大规模的数据集来支持系统性研究。

Method: 构建了包含51个真实图像数据集（超580万张真实图像）和102种深伪生成方法（超880万张伪造图像）的大规模数据集ScaleDF，并系统评估模型在不同规模条件下的检测性能。

Result: 发现检测误差随着真实图像域或深伪生成方法数量的增加呈现可预测的幂律衰减，类似于大语言模型中的扩展规律；并验证了预训练和数据增强在扩展条件下的作用及其局限性。

Conclusion: 深伪检测存在可量化的扩展规律，可通过增加数据多样性来提升模型性能，为应对不断演进的深伪技术提供了数据驱动的解决方案。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [28] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 本文提出了一种名为Scale-DiT的新型扩散模型框架，通过分层局部注意力与低分辨率全局引导，实现了高效、可扩展且语义连贯的超高清图像生成，无需依赖原生4K训练数据即可达到4K分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型受限于注意力机制的二次复杂度和高分辨率训练数据的缺乏，难以实现超高清（如4K）图像生成，因此需要一种更高效且可扩展的方法。

Method: 将高分辨率潜在表示划分为固定大小的局部窗口以降低注意力复杂度至近线性，并引入带有缩放位置锚点的低分辨率潜在表示来注入全局语义；通过轻量级LoRA模块在去噪过程中融合全局与局部信息，并采用希尔伯特曲线顺序重排令牌序列及融合内核优化推理效率。

Result: Scale-DiT在FID、IS、CLIP Score等指标上优于或媲美现有方法，实现两倍以上的推理速度提升和更低内存占用，成功扩展至4K×4K分辨率生成，且无需额外高分辨率训练数据。

Conclusion: 分层局部注意力结合低分辨率引导是一种有效且有前景的超高清图像生成方法，为未来高分辨率生成模型提供了新的设计思路。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [29] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 提出DiffusionX，一种云-边协同框架，通过轻量级设备模型生成预览图像，大容量云端模型进行最终优化，结合噪声级别预测器动态平衡计算负载，在保持图像质量的同时显著降低生成延迟和云资源负担。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成上取得显著进展，但生成过程计算开销大，用户需多次迭代调整提示词，导致延迟高、云资源压力大。

Method: 设计云-边协作的DiffusionX框架：设备端轻量模型快速生成预览图供用户交互，云端大模型在提示词确定后进行精细生成；引入噪声级别预测器动态分配计算任务，优化延迟与云负载的权衡。

Result: 实验表明，相比Stable Diffusion v1.5，DiffusionX平均生成时间减少15.8%，且图像质量相当；相比Tiny-SD仅慢0.9%，但图像质量显著更好。

Conclusion: DiffusionX在多轮提示生成场景下实现了高效、可扩展的图像生成，兼顾低延迟、高质量和低云资源开销，适合实际部署应用。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [30] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了TokenAR框架，通过引入token级别的增强机制（包括Token Index Embedding、Instruct Token Injection和身份-令牌解耦策略ITD）来解决多参考图像生成中的身份混淆问题，并发布了首个大规模开源多参考图像生成数据集InstructAR Dataset。实验表明该方法在多主体生成任务中优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在多参考条件图像生成中难以解耦不同参考身份，导致身份混淆问题，影响生成图像的身份一致性和质量。

Method: 提出TokenAR框架，包含三部分：1）Token Index Embedding用于聚类相同参考图像的token；2）Instruct Token Injection注入详细且互补的先验信息；3）身份-令牌解耦策略（ITD）显式引导每个身份的独立表示。同时构建了InstructAR Dataset用于训练与评估。

Result: 在多参考图像生成任务上显著优于现有SOTA模型，实现了更好的身份一致性与高质量背景重建，同时支持高多样性的生成。

Conclusion: TokenAR通过token级增强有效缓解了多参考生成中的身份混淆问题，结合新提出的InstructAR Dataset，为多主体条件图像生成提供了有效解决方案和数据支持。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [31] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 本文研究了多模态语言模型（MLLM）中视觉编码器在不同训练策略（监督微调SFT与强化学习RL）下的变化，发现RL能产生更强且定位更精确的视觉表征，并提出一种高效方法PIVOT，仅用不到1%的传统预训练计算成本即可构建高性能视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖大语言模型的能力，忽视了视觉编码器的作用，尤其是在从SFT转向RL训练范式后，缺乏对视觉编码器如何被重塑的深入分析。

Method: 通过多种实验（如ImageNet分类、分割、梯度可视化等）系统分析SFT和RL对MLLM视觉编码器的影响，并基于发现提出PIVOT训练方法。

Result: RL训练相比SFT能显著提升视觉相关VQA任务表现，并从根本上改变视觉表征，使其更强且定位更准确；PIVOT训练的视觉编码器在低计算成本下超越更大规模模型。

Conclusion: MLLM的训练策略不仅影响下游任务性能，也深刻重塑其视觉表示能力；PIVOT为构建高效强大的MLLM视觉编码器提供了新路径。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [32] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度的框架GradNorm，用于语言辅助图像聚类（LaIC），通过理论证明和实验验证其在过滤正向名词和提升聚类性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏真实类别名称的情况下，如何从无标签文本语料中有效筛选与目标图像语义相近的正向名词是语言辅助图像聚类的核心挑战。现有方法依赖CLIP特征空间但缺乏理论支持。

Method: 提出GradNorm框架，通过计算从预测分布与softmax输出之间的交叉熵反向传播的梯度大小来衡量每个名词的正向性，并提供理论误差界分析。

Result: 理论证明GradNorm能有效分离正向名词，并涵盖现有过滤策略作为特例；实验表明其在多个基准上达到最先进的聚类性能。

Conclusion: GradNorm是一种理论上可靠且实证性能优越的语言辅助图像聚类中的正向名词过滤方法。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [33] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 本文提出了首个面向社会化制造中异常检测的基准数据集MIRAD，旨在解决大规模个性化生产中的缺陷检测难题。该数据集体现了产品高度定制化、分布式制造节点和成像环境异质性三大挑战，并对多种前沿异常检测方法进行了评估，结果表明现有模型在真实个性化生产场景中性能显著下降，凸显了该领域研究的紧迫性与必要性。


<details>
  <summary>Details</summary>
Motivation: 社会化制造虽支持大规模个性化生产，但因产品定制化程度高、订单碎片化及分布站点成像环境差异大，导致缺陷检测面临严峻挑战，且缺乏真实数据集和专用算法支撑研究。

Method: 构建了一个名为MIRAD的真实世界异常检测数据集，涵盖六个地理分散的制造节点，包含多样化个性化产品和显著的成像异质性（如光照、背景和运动变化），并系统评估了包括单类、多类和零样本在内的最先进异常检测方法。

Result: 在MIRAD上的实验表明，当前最先进的异常检测方法相比传统基准性能大幅下降，说明现有技术难以应对实际个性化生产中的复杂性。

Conclusion: MIRAD为社会化制造中的鲁棒质量控制研究提供了现实且具有挑战性的基准，弥合了工业需求与学术研究之间的差距，推动面向Industry 5.0的质量保障技术发展。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [34] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 本文介绍了一个包含3000个白内障超声乳化手术视频的大规模数据集，涵盖两个手术中心、不同经验水平的外科医生，并提供了四个层次的注释：手术阶段划分、器械与解剖结构实例分割、器械-组织交互追踪以及基于ICO-OSCAR等标准的技能评分。


<details>
  <summary>Details</summary>
Motivation: 现有的白内障手术数据集缺乏足够的多样性和精细标注，难以支持可泛化的深度学习模型训练，因此需要构建一个更全面、多维度标注的数据集以推动手术AI的发展。

Method: 收集来自两个手术中心的3000例手术视频，进行四层标注；并通过基准实验评估手术工作流识别、场景分割和自动技能评估等任务的性能，同时建立领域自适应基线模型用于手术阶段识别。

Result: 提供了高质量、多层次标注的数据集，支持多种手术AI任务的建模与评估，在工作流识别、技能评估等任务上取得了有效的基准结果，并验证了跨中心模型迁移的可行性。

Conclusion: 该数据集为计算机辅助白内障手术研究提供了重要资源，有助于提升手术AI模型的泛化能力和临床实用性。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [35] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个用于实时检测印度道路坑洼的端到端自动化平台，结合YOLO模型、GPS定位和OpenStreetMap实现坑洼识别、地理标记与可视化，并通过智能治理功能支持维修问责和道路管理。


<details>
  <summary>Details</summary>
Motivation: 印度道路网络复杂且维护不足，坑洼频发带来安全隐患和管理难题，亟需一种高效、可扩展的自动化监测方案以提升道路维护效率与透明度。

Method: 构建包含7000多张行车记录仪图像的数据集，微调Ultralytics YOLO模型进行坑洼检测；结合OCR提取的时间戳与GPS日志实现精确定位；开发基于Web的平台集成OpenStreetMap可视化、数据库管理和自动警报系统。

Result: 实现了高精度的实时坑洼检测与地理标记，系统能自动生成维修警报、关联承包商信息，并提供直观的网页界面供公众和管理者查看道路健康状况与分析数据。

Conclusion: iWatchRoadv2通过自动化检测到修复验证的全流程，推动了数据驱动的城市管理、透明化治理和可持续的道路基础设施维护，具有成本低、可扩展性强的优点，适用于城乡部署。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [36] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: 本文提出了Demeter，一种数据驱动的3D参数化植物形态模型，能够编码植物的拓扑、形状、关节和变形，并在大豆作物上验证了其在形状合成、结构重建和生物物理模拟中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D参数化模型在人类和动物上较为成熟，但对植物尤其是作物的建模仍缺乏表达力强的方法，因此需要一个能处理植物复杂形态变化的模型。

Method: 提出Demeter模型，通过学习紧凑表示来编码植物形态的关键因素，支持可变拓扑结构，并建模三种形状变化：关节运动、子部件形状差异和非刚性变形；基于自采集的大规模真实大豆数据集进行训练与评估。

Result: 实验表明Demeter在形状生成、结构重建和生物物理过程模拟方面表现有效，显著提升了作物植物的3D建模能力。

Conclusion: Demeter为植物特别是农作物提供了强大的数据驱动参数化3D建模框架，具有在农业视觉与图形学中广泛应用的潜力。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [37] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 提出了一种轻量级编码器-解码器框架，结合稀疏卷积、SPLite解码器和量化感知训练，在保持高精度的同时显著提升了AR/VR边缘设备上的手部姿态估计效率。


<details>
  <summary>Details</summary>
Motivation: 在AR/VR设备中部署深度学习模型面临实时性、低功耗和低延迟的挑战，现有方法难以平衡效率与性能。

Method: 采用ResNet-18骨干网络结合稀疏卷积以利用手部姿态图像的固有稀疏性，并设计了SPLite解码器提升解码速度；引入量化感知训练进一步优化内存使用和推理速度。

Result: 在Raspberry Pi 5上实现端到端效率提升42%，解码帧率提高3.1倍，整体CPU加速达2.98倍；在FreiHAND数据集上PA-MPJPE仅从9.0 mm微增至9.1 mm，准确率与最先进方法相当。

Conclusion: 所提框架在保持精度的同时显著提升了计算效率和推理速度，适用于资源受限的AR/VR边缘设备部署。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [38] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: 本文提出了REALM，一种无需大量3D特定后训练的MLLM代理框架，通过3D高斯点阵渲染实现开放世界基于推理的分割，结合全局到局部的空间接地策略，在多种基准上表现出色，并支持多种3D交互任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D分割方法难以理解模糊、需推理的指令，而擅长推理的2D视觉语言模型缺乏内在的3D空间理解能力，因此需要一个能结合两者优势的方法来实现精确且可推理的3D对象定位。

Method: 提出REALM框架，直接在3D高斯点阵表示上进行分割，利用其生成逼真新视角的能力；采用全局到局部的空间接地策略：首先并行输入多个全局视图进行粗略定位，聚合响应以稳健识别目标对象，然后合成多个特写新视图进行细粒度局部分割。

Result: 在LERF、3D-OVS和新提出的REALM3D基准上，REALM在解释显式和隐式指令方面均表现出色，实现了准确且一致的3D掩码，并支持对象移除、替换和风格迁移等3D交互任务。

Conclusion: REALM有效 bridging了复杂人类指令与精确3D对象定位之间的鸿沟，无需大量3D后训练即可实现高性能的推理型3D分割，具备良好的实用性与扩展性。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [39] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: 提出SSL4RL框架，利用自监督学习任务作为强化学习微调的可验证奖励信号，提升视觉-语言模型在视觉中心和视觉-语言推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型常依赖语言先验或文本捷径，难以有效利用视觉证据；同时强化学习缺乏可扩展且可靠的奖励机制。

Method: 将自监督学习目标（如图像旋转预测、掩码块重建）转化为密集、自动的奖励信号，用于强化学习微调，无需人工标注或不可靠的AI评估器。

Result: SSL4RL在视觉和视觉-语言推理基准上显著提升性能，并通过消融实验识别影响效果的关键因素，还在图学习中验证了框架通用性。

Conclusion: SSL4RL建立了一种通用且有效的多模态模型对齐范式，使用可验证的自监督目标作为奖励，为未来工作提供了新设计原则。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [40] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: 本文提出了LightGlueStick，一种用于点和线段的轻量级匹配器，通过引入注意力线消息传递（ALMP）机制，在多个基准上实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统上点和线匹配被视为独立任务，而现有联合匹配方法如GlueStick因架构复杂难以实现实时应用或边缘设备部署。

Method: 受近期点匹配进展启发，提出LightGlueStick，采用基于GNN的轻量级架构，并引入注意力线消息传递（ALMP）机制，显式建模线的连接性以实现节点间高效通信。

Result: 在多个不同基准的实验中，LightGlueStick在保持低计算复杂度的同时，显著优于现有方法，实现了新的最先进匹配性能。

Conclusion: LightGlueStick通过轻量设计和ALMP模块，有效结合点与线特征进行联合匹配，适用于实时和边缘设备应用，推动了SLAM和Structure-from-Motion等方向的发展。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [41] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的深度伪造视频检测框架EDVD-LLaMA，结合时空特征提取与多模态推理机制，在提高检测准确性的同时提供可信的推理解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测方法缺乏透明性和泛化能力，难以应对新型伪造技术，亟需具备可解释性的检测模型。

Method: 提出EDVD任务框架，设计ST-SIT模块提取时空特征，构建Fg-MCoT机制引入面部特征硬约束，实现像素级定位与可靠推理，并建立ER-FF++set数据集支持双监督训练。

Result: 实验表明，EDVD-LLaMA在检测精度、可解释性及跨方法、跨数据集场景下均表现出优异的性能和鲁棒性。

Conclusion: 该方法为深度伪造检测提供了更可解释、更可靠的解决方案，推动了可信赖AI在内容安全中的应用。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [42] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了RefAtomNet++，一种用于指代表性原子视频动作识别（RAVAR）的新框架，并发布了扩展数据集RefAVA++，通过多层级语义对齐的交叉注意力机制和Mamba建模，在跨模态对齐与细粒度动作识别上实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的动作识别方法在复杂多人体场景中难以精准定位特定人物并理解语言描述的细粒度动作，缺乏有效的语言-视觉跨模态对齐能力，因此需要更强大的模型来提升语言引导下的动作理解精度。

Method: 提出RefAtomNet++，引入多层级语义对齐的交叉注意力机制，在部分关键词、场景属性和整体句子层面聚合跨模态信息，并结合多轨迹Mamba建模；动态构建扫描路径以选择关键视觉token，增强时空特征对齐。

Result: 在RefAVA++数据集上实验表明，RefAtomNet++显著优于现有基线模型，在指代性原子动作识别任务上达到新的SOTA性能，有效提升了目标人物定位和细粒度动作预测的准确性。

Conclusion: RefAtomNet++通过多层次语义对齐和动态轨迹建模，显著提升了语言引导下细粒度动作识别的性能，为复杂场景中的交互式人类行为分析提供了有效解决方案。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [43] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯边界框表示和巴塔恰里亚距离的改进损失函数，用于提升旋转目标检测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法在处理旋转目标时因无法有效捕捉方向变化而表现不佳，尤其是在航空影像、遥感和自动驾驶等应用中需要精确检测旋转物体。

Method: 采用各向异性高斯边界框表示，并设计一种旋转不变的损失函数，结合巴塔恰里亚距离来更好地建模旋转对象的几何特性，并集成到先进的深度学习旋转检测器中。

Result: 在多个数据集上进行的实验表明，该方法显著提升了平均精度（mAP），优于现有方法。

Conclusion: 所提出的损失函数有效提升了旋转目标检测的性能，有望成为该领域的新基准。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [44] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出了一种名为VIPAMIN的视觉提示初始化策略，通过在嵌入空间中对齐语义信息区域并引入新的表示方向，有效提升自监督模型在少样本和挑战性任务中的适应能力，且仅需一次前向传播和轻量操作。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉提示调优方法在应用于自监督骨干网络时，往往无法有效特化提示或丰富表示空间，尤其在数据稀缺和高挑战性任务中表现不佳。

Method: 提出VIPAMIN，通过两种机制增强提示初始化：1）将提示与嵌入空间中的语义信息区域对齐；2）注入超出预训练子空间的新表示方向。该方法仅需一次前向传播和轻量级操作。

Result: VIPAMIN在多种任务和数据集规模下均显著提升性能，尤其在数据稀缺场景中表现突出，成为视觉提示调优领域的新标杆。

Conclusion: VIPAMIN通过简单而有效的初始化策略，显著增强了自监督模型的提示调优能力，为资源受限下的模型适应提供了高效解决方案。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [45] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种弱监督域自适应方法，用于电子显微镜图像中线粒体的高效分割，结合多任务学习与新型伪标签选择策略，在少量标注下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法在实际应用中性能较低，且完全标注成本高，因此需要一种仅需少量标注即可有效提升分割性能的方法。

Method: 提出一个多任务学习框架，联合进行分割与中心检测，引入交叉教学机制和类别聚焦的跨域对比学习，并设计实例感知的伪标签选择策略用于自训练。

Result: 在多个具有挑战性的数据集上验证，该方法优于现有的UDA和WDA方法，显著缩小了与全监督上限之间的性能差距。

Conclusion: 所提出的方法在弱监督和无监督域自适应设置下均表现出优越性能，有效利用稀疏标注和未标注区域，提升了线粒体实例分割的实用性和效率。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [46] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 本文提出了一种具有前瞻性思维的视觉-语言导航方法，利用Q-learning从大规模无标签轨迹数据中学习室内场景布局和物体关系的通用知识，并通过跨模态未来编码器结合导航指令生成反映未来前景的动作评分，结合历史信息实现更有效的路径搜索。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航方法多基于历史信息做决策，忽视动作的长期影响，本文旨在构建能预见未来状态的智能体以提升导航性能。

Method: 采用Q-learning训练Q模型，利用无标签轨迹数据学习场景布局与对象关系；生成每个候选动作的Q特征，并通过跨模态未来编码器将其与导航指令融合，得到考虑未来前景的动作评分，结合历史评分使用类A*策略进行搜索。

Result: 在多个主流目标导向型视觉-语言导航数据集上的实验表明，所提方法显著优于基线模型，验证了引入未来预测机制的有效性。

Conclusion: 通过建模未来状态信息并结合指令与Q特征，该方法提升了智能体在复杂环境中的长程导航能力，证明了前瞻性学习在视觉-语言导航任务中的重要价值。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [47] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: 本文提出了一种用于高效传输和高质量渲染动态3D头像的分层高斯压缩框架HGC-Avatar，通过引入人体先验和面部注意力机制，在低比特率下实现了优越的视觉质量和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于通用3D高斯点阵的压缩方法在数字人编码与传输中因缺乏人体先验而导致码率效率和重建质量不佳，限制了其在可流式3D头像系统中的应用。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器映射姿态到高斯）和运动层（利用SMPL-X模型紧凑且语义化地表示时间姿态变化），并采用分层压缩、渐进式解码及可控渲染，结合面部注意力机制优化训练。

Result: 实验结果表明，HGC-Avatar在视觉质量和压缩效率方面显著优于先前方法，支持从视频或文本等多样姿态输入进行快速、可流式的3D头像渲染。

Conclusion: HGC-Avatar通过引入人体结构先验和面部细节保持机制，为动态3D头像的高效压缩与高质量重建提供了有效的解决方案，适用于沉浸式通信中的流式传输场景。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [48] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench 是首个基于真实同行评审中发现的跨模态不一致问题的基准，用于评估大视觉模型在科学论文理解中的可靠性，揭示了现有模型在多模态科学推理上的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准未能捕捉科学论文中真实存在的跨文本、图表、公式等模态的不一致性问题，且多依赖合成错误或单一模态，无法反映实际科研场景下的复杂性与领域特异性。

Method: 通过审查挖掘、大语言模型辅助筛选和人工验证的多阶段流程，从242篇论文中整理出262个真实评审标记的不一致案例，并设计三项任务：不一致识别、修正和配对匹配；同时引入结构化JSON答案格式以减少多选题中的语言偏见和捷径学习问题。

Result: 在21个主流大视觉模型上的实验显示性能极低（26.1-54.2%），表明当前模型在跨模态科学推理方面存在显著缺陷，尤其是在理解和修复多模态不一致方面。

Conclusion: PRISMM-Bench 揭示了当前大视觉模型在处理真实科学文献中的多模态不一致问题时的局限性，强调了构建更可靠科学助手所需的关键改进方向。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [49] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新型缺货检测方法OOS-DSD，通过引入辅助学习（产品分割和深度估计）提升检测性能，并采用伪标签和深度归一化策略优化训练过程，实验表明该方法在mAP上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 缺货检测在零售业中至关重要，但现有方法在准确性和鲁棒性方面仍有提升空间，因此需要一种更有效的方法来提高检测精度。

Method: 扩展YOLOv8架构，增加用于产品分割和深度估计的卷积分支，其中深度估计分支使用Depth Anything V2生成的伪标签进行训练，并提出一种深度归一化方法以稳定训练过程。

Result: 所提方法在mAP上比现有最先进方法高出1.8%，消融实验显示辅助学习使mAP提升3.7%，深度归一化进一步提升4.2%。

Conclusion: OOS-DSD通过引入多任务辅助学习和深度归一化策略，显著提升了缺货检测性能，验证了其在实际零售场景中的潜力。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [50] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络（GAT）自编码器的图像分类与检索方法，通过构建图像和类别的代表模型实现代表性中心的分类与检索。


<details>
  <summary>Details</summary>
Motivation: 为了提升图像分类与检索的准确性，利用图结构和注意力机制捕捉图像间的相似性关系及上下文信息。

Method: 构建图像图为节点、相似性为边的图结构，使用GAT自编码器学习上下文感知的潜在表示，并从中提取类别代表进行分类和检索。

Result: 实验表明，该方法在图像分类和检索任务上优于标准特征基线方法，验证了代表性中心框架的有效性。

Conclusion: 基于GAT自编码器的代表性中心方法能有效提升图像分类与检索性能，充分利用图像间的关系结构。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [51] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 本文提出了一种名为READ的微调方法，通过引入词级别重建和句子级别对齐两个辅助目标，提升视觉-语言模型在组合推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的视觉-语言模型倾向于关注文本中的单个词语而非其结构关系，导致组合推理能力不足。

Method: 在对比学习基础上增加两个辅助目标：1）词级别重建，使用冻结的预训练解码器重建原始描述的替代表述；2）句子级别对齐，显式对齐释义句子的嵌入表示。

Result: READ-CLIP在五个主流组合推理基准上达到最先进性能，相比最强基线最高提升4.1%；该方法也适用于其他CLIP变体并带来性能增益。

Conclusion: 重建与对齐目标能互补地增强文本编码器对词语关系的理解和释义一致性，显著提升模型的组合推理能力。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [52] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于区域感知的动态聚合与激励框架（GaitRDAE），用于解决步态识别中不同运动区域时序建模的自适应问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用预定义区域和固定时序尺度，难以动态建模受协变量影响的复杂步态变化。

Method: 设计了两个核心模块：区域感知动态聚合（RDA）模块，动态搜索每个区域的最佳时序感受野；区域感知动态激励（RDE）模块，增强对稳定行为模式的关注，抑制对易受干扰的静态区域的关注。

Result: 在多个基准数据集上实现了最先进的性能。

Conclusion: GaitRDAE通过自适应地建模不同区域的动态运动模式，显著提升了步态识别的准确性和鲁棒性。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [53] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本文介绍了首个基于真实世界政治深度伪造事件数据库的系统性基准，评估了学术界、政府和工业界的最先进检测工具，发现现有模型在应对社交媒体上传播的政治深度伪造内容时普遍表现不佳，尤其易受视频领域简单操纵的影响，强调需构建更具政治语境适应性的检测框架。


<details>
  <summary>Details</summary>
Motivation: 由于生成模型的进步，AI生成内容泛滥，政治深度伪造威胁日益严重，但现有检测模型多基于实验室数据，难以推广到真实社交平台场景，亟需针对现实政治深伪内容的有效检测方案。

Method: 基于自2018年以来在社交媒体上传播的真实政治深度伪造案例数据库，对学术界、政府和工业界的主流深伪检测工具进行系统性评估，涵盖图像与视频领域的多种先进模型，并测试其在真实场景及简单篡改下的鲁棒性。

Result: 学术界和政府开发的检测器表现较差；付费检测工具优于免费模型，但所有检测器在面对真实政治深伪内容时泛化能力有限，尤其在视频中易被简单操作欺骗。

Conclusion: 当前深伪检测技术在真实政治语境下面临严峻挑战，需发展融合政治背景理解的检测框架，以提升实际应用中的有效性与鲁棒性。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [54] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出一种仅使用视觉的无人机自主飞行系统，结合语义分割与单目深度估计，实现室内环境下的避障、探索与安全降落，无需GPS或LiDAR。


<details>
  <summary>Details</summary>
Motivation: 解决小型无人机在无GPS、低成本传感器条件下于结构化室内环境中实现自主飞行的挑战，特别是度量深度估计和计算效率问题。

Method: 采用语义分割与单目深度估计融合的方法，提出自适应尺度因子算法，利用语义地面检测与相机内参将非度量深度转为精确的度量距离；使用知识蒸馏框架训练轻量级U-Net网络，并通过端到端学习优化飞行策略。

Result: 在5x4米实验室环境中进行30次实飞与100次数字孪生测试，系统实现100%任务成功率，平均距离误差14.4厘米，提升航程并缩短任务时间；端到端学生网络达到87.5%自主任务成功率。

Conclusion: 该方法有效解决了视觉导航中的度量深度估计与计算资源限制问题，推动了资源受限平台在结构化环境中实用化视觉导航的发展。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [55] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文首次将大视觉语言模型（LVLM）中的物体幻觉问题追溯到视觉编码器，提出了一种无需训练的框架SHIELD，通过三种策略有效缓解统计偏差、固有偏差和脆弱性问题，在多个基准上显著减少幻觉并具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在跨模态任务中表现优异，但存在物体幻觉问题；以往研究主要关注语言模型部分，而本文旨在从视觉编码器角度揭示并解决该问题。

Method: 提出SHIELD框架，采用三种策略：重新加权视觉token以减少统计偏差，引入噪声衍生token对抗固有偏差，结合对抗攻击与对比解码应对脆弱性，且无需额外训练。

Result: 实验表明SHIELD在多种LVLM架构和基准测试上均能有效降低物体幻觉，并在通用LVLM基准上保持良好性能，展现出强鲁棒性和泛化能力。

Conclusion: 视觉编码器是LVLM物体幻觉的关键来源，SHIELD作为一种训练-free的解决方案，为提升模型可靠性提供了新思路。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [56] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 本文提出一种基于检索增强生成的方法，利用图数据库和Cypher查询语言作为接口，使大语言模型能高效地从3D场景图中检索相关信息，从而实现对自然语言的语义 grounding，显著提升了在大规模、丰富场景下的指令执行与问答任务性能，并减少了token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的将3D场景图序列化为文本输入大语言模型的方法难以扩展到大规模或复杂的场景图，限制了语言 grounding 的效率和可扩展性。

Method: 提出使用检索增强生成（Retrieval Augmented Generation）方法，将3D场景图存储在图数据库中，并通过Cypher查询语言作为工具供大语言模型调用，以动态检索与任务相关的子图信息。

Result: 在指令跟随和场景问答任务中，该方法相比传统的上下文窗口和代码生成基线方法，在本地和云端模型上均表现出更好的可扩展性和性能提升，同时大幅降低了场景图内容的token数量。

Conclusion: 通过引入Cypher作为3D场景图的查询接口，结合检索增强机制，能够有效解决大语言模型在复杂场景下语言 grounding 的可扩展性问题，为机器人理解自然语言提供了更高效、实用的解决方案。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [57] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出VisionSelector，一种轻量级、可学习的视觉token选择框架，通过可微分Top-K机制和课程退火策略实现多模态大模型中高效自适应的token压缩，在低保留率下显著提升性能与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉token压缩方法依赖启发式规则，易丢失关键信息，且受注意力汇聚等偏差影响，在高压缩比下性能急剧下降。

Method: 将token压缩建模为端到端可学习的决策过程，设计解耦于MLLM主干的VisionSelector模块，结合可微分Top-K选择机制与课程退火策略，实现任意压缩率下的自适应关键token筛选。

Result: 在30%保留率下MME准确率保持100%；10%保留率下超越先前方法12.14%；预填充速度提升一倍。模块仅含1285万可训练参数，具备跨压缩率泛化能力。

Conclusion: VisionSelector作为一种轻量、通用的插件式token压缩方案，有效缓解多模态大模型的计算与内存瓶颈，在广泛压缩预算下实现性能与效率的显著提升。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [58] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的实时医学图像分析框架，结合U-Net、EfficientNet和Transformer等模型与模型剪枝、量化及GPU加速技术，实现在多种医疗影像模态上的高效、准确分析，并支持边缘设备部署与临床系统集成。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理方法在精度、鲁棒性和速度上难以满足临床实时需求，且诊断过程耗时且存在医生间差异。

Method: 融合U-Net、EfficientNet和Transformer架构，结合模型剪枝、量化和GPU加速等优化策略，支持多模态医学图像分析，并集成Grad-CAM等可视化解释工具。

Result: 在公开数据集上实现超过92%的分类准确率、91%以上的分割Dice分数，推理时间低于80毫秒，具备快速、准确和可解释的优势。

Conclusion: 该框架能显著加速诊断流程，减轻医生负担，提升AI在时间敏感型医疗环境中的可信部署与临床实用性。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [59] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 本文构建了一个高质量、公开的荔枝图像数据集，用于支持基于视觉的采摘机器人开发，涵盖多种荔枝品种和成熟度阶段，并通过多人标注提高一致性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中一致且全面标注的开源荔枝数据集，限制了基于视觉的采摘机器人发展。

Method: 采集了不同天气、时间和品种下的RGB图像，并进行数据增强；采用三人独立标注、一人审核的方式确保标注一致性，同时提供了深度图像和详细统计分析。

Result: 构建了包含11,414张图像的数据集，其中有878张原始RGB图像、8,780张增强RGB图像和1,756张深度图像，共9,658个标注对，覆盖三个成熟阶段；并通过三种深度学习模型验证了数据集的有效性。

Conclusion: 该数据集可有效支持荔枝检测与成熟度分类研究，已公开供学术使用。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [60] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个新的多轮对话基准，包含647个平均四轮的对话，涵盖484个任务和目标，评估18个视觉语言模型在复杂多轮交互中的表现，发现最强模型也仅达到50%成功率，并提出基于GPT-4o的清单式自动评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能充分覆盖真实世界中复杂的多轮视觉-语言对话场景，缺乏全面评估视觉语言模型多轮交互能力的基准。

Method: 从12个流行的VLM评测基准中构建MultiVerse多轮对话数据集，并设计基于GPT-4o的清单式自动化评估方法，从37个关键维度评估模型表现。

Result: 在18个VLM上的实验表明，即使最强模型（如GPT-4o）在复杂多轮对话中也仅有约50%的成功率；提供完整对话上下文显著提升小模型性能。

Conclusion: MultiVerse是一个具有挑战性的多轮对话评测基准，揭示了当前VLM在多轮交互中的局限性，并强调了上下文学习的重要性，为未来研究提供了重要资源。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [61] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: 本文提出了一种名为Multi-Mono-Hydra（M2H）的新型多任务学习框架，用于从单目图像中实现语义分割、深度估计、边缘检测和表面法线估计，具有高效性和实时性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，在最小化计算开销的同时利用任务间的互补信息。传统方法存在任务间干扰或计算成本高的问题。

Method: M2H采用基于轻量级ViT的DINOv2主干网络，并引入基于窗口的跨任务注意力模块，实现结构化特征交换，保留任务特定细节，提升多任务预测一致性。

Result: 在NYUDv2、Hypersim和Cityscapes数据集上，M2H优于当前最先进的多任务和单任务模型，且在笔记本硬件上保持高效计算。此外，其在真实世界数据中验证了实用性。

Conclusion: M2H是一种高效、一致性强的多任务学习框架，适用于实时单目空间感知系统，支持动态环境中的3D场景图构建。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [62] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了一种通用且可迁移的对抗性扰动（UTAP），用于揭示病理学基础模型中的关键漏洞，能够在多个模型和数据集上导致性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型在面对对抗性攻击时的脆弱性，推动更鲁棒的模型设计和安全部署。

Method: 通过深度学习优化生成一个固定的弱噪声模式，该模式可广泛应用于不同视野和数据分布的病理图像，破坏基础模型的特征表示能力。

Result: UTAP在多种最先进的病理基础模型和数据集上实现了显著的性能下降，同时具备跨数据集的普遍性和对黑盒模型的可迁移性。

Conclusion: UTAP构成对现有及未来病理基础模型的广泛威胁，为模型鲁棒性评估提供了高标准基准，并强调了发展防御机制和对抗训练的重要性。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [63] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出了一种新的光谱重建方法HYDRA，通过结合知识蒸馏与多尺度注意力机制，在从未见过的场景中从自然图像恢复高光谱图像，实现了最先进的性能和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的多尺度注意力方法在稀疏光谱上表现良好，但在现代高光谱传感器具有数百个通道的情况下泛化能力不足，因此需要更有效的光谱重建模型。

Method: 提出HYDRA架构，采用教师-学生模型框架，教师模型编码高光谱信息，学生模型学习从自然图像到该编码域的映射，并设计了新的训练方法。

Result: 在多个指标上达到SOTA性能，准确率提升18%，且在不同通道深度下推理速度更快。

Conclusion: HYDRA显著提升了光谱重建的泛化能力和效率，适用于现代高光谱成像应用。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [64] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSSR的双代理框架，通过构建最小充分集（MSS）来提升视觉语言模型在3D空间推理中的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理方面存在两个瓶颈：由于2D为中心的预训练导致的3D理解能力不足，以及冗余3D信息引起推理失败。

Method: 构建一个包含感知代理和推理代理的双代理框架。感知代理使用多功能感知工具箱提取足够的3D信息，并引入新的SOG模块进行方向定位；推理代理迭代优化信息，去除冗余并补充缺失，形成最小充分集（MSS）。

Result: 实验表明，该方法在两个具有挑战性的基准测试中显著提高了准确性，达到最先进的性能，同时生成可解释的推理路径。

Conclusion: MSSR框架通过显式追求信息的充分性和最小性，有效提升了空间推理能力，并为未来模型提供了高质量的训练数据来源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [65] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为SDPA++的自监督去噪框架，利用仅含噪声的OCT图像生成伪真实标签，通过patch聚合策略训练去噪模型，在无干净参考图像的情况下显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 由于临床环境中难以获取配对的干净与噪声OCT图像，传统监督去噪方法受限，因此需要一种仅依赖噪声图像的自监督解决方案。

Method: 通过自融合和自监督去噪生成伪真实图像，采用基于patch的聚合策略训练多个去噪模型，利用CNR、MSR、TP和EP等指标评估性能。

Result: 在仅包含真实噪声OCT图像的IEEE SPS VIP Cup数据集上验证了方法的有效性，显著提升了对比度、纹理和边缘保持能力。

Conclusion: SDPA++为无需干净参考图像的OCT图像去噪提供了有效且通用的框架，具有改善临床诊断图像质量的潜力。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [66] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 本文提出了一种新的领域泛化方法DCCL，通过增强类内跨域连通性来提升对比学习在领域泛化中的表现，解决了直接应用对比学习导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 由于训练和测试样本之间存在分布偏移，模型泛化能力受限，现有对比学习方法在领域泛化中表现不佳，主要原因是类内跨域连通性不足。

Method: 提出域连接对比学习（DCCL），在数据层面引入更强的数据增强和跨域正样本，在模型层面采用模型锚定和生成转换损失，以增强类内连通性和泛化能力。

Result: 在五个标准领域泛化基准上的实验表明，DCCL优于现有的最先进方法，即使没有领域监督也能取得更好性能。

Conclusion: DCCL通过增强跨域类内连通性，有效提升了对比学习在领域泛化中的表现，为无监督领域泛化提供了新思路。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [67] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一种基于一致性模型的单步人类动作预测框架，通过学习噪声与干净运动状态之间的自洽映射，实现高效生成，显著减少推理步骤的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 为了克服扩散模型依赖多步去噪导致推理效率低的问题，提出一种更高效的单步人类动作预测方法。

Method: 采用基于Transformer的时空架构与时间嵌入，构建一致性模型，学习噪声与干净运动状态之间的自洽映射，实现一步生成。

Result: 在Human3.6M和HumanEva-I数据集上表现优于或媲美当前最先进的扩散模型，同时将推理步数减少了多达两个数量级。

Conclusion: HumanCM在保持高预测精度的同时极大提升了推理效率，为实际应用中的人类动作预测提供了更优的解决方案。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [68] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D场景理解的新型链式思维推理框架SCENECOT，并构建了首个大规模3D场景链式推理数据集SCENECOT-185K，实现了在复杂3D场景中具有高接地一致性的问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在实现基于场景对象的接地问答方面仍存在困难，主要由于对类人场景-对象接地推理机制探索不足。

Method: 提出SCENECOT方法，将复杂推理任务分解为简单子问题，并利用多模态专家模块构建视觉线索；同时构建包含18.5万高质量样本的数据集SCENECOT-185K以支持该方法。

Result: 在多个复杂3D场景推理基准上的实验表明，该框架在接地问答一致性方面表现优异，首次成功将链式思维推理应用于3D场景理解。

Conclusion: SCENECOT首次实现了3D场景中的逐步类人推理，为未来更广泛的3D场景理解任务提供了可行路径和扩展潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [69] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种隐式残差世界模型IR-WM，专注于建模环境的当前状态和演化，通过仅预测基于自车动作和场景上下文的变化部分（残差），提高了自动驾驶中视觉中心世界模型的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶系统在重建未来场景时常常低效，浪费大量计算资源于静态背景的重复建模。

Method: IR-WM首先从视觉观测建立当前状态的鸟瞰图表示，利用前一时刻的BEV特征作为时间先验，仅预测‘残差’变化，并引入对齐模块缓解长期误差累积。同时探索了不同预测-规划耦合方案。

Result: 在nuScenes基准上，IR-WM在4D占据预测和轨迹规划任务中均达到最先进性能。

Conclusion: IR-WM通过聚焦动态变化和引入残差建模，显著提升了世界模型的效率与规划准确性，为视觉主导的自动驾驶提供了更有效的解决方案。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [70] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: 提出了一种名为UKANFormer的新语义分割模型，用于在噪声标签监督下实现高精度的珊瑚礁制图，实验表明其性能优于基线方法，并能生成比训练标签更准确的预测结果。


<details>
  <summary>Details</summary>
Motivation: 现有全球珊瑚礁分布数据（如Allen Coral Atlas）存在空间精度不足和语义不一致的问题，尤其在细粒度边界划分上表现不佳，因此需要一种能在噪声监督下实现高精度映射的方法。

Method: 基于UKAN架构，设计了包含全局-局部Transformer（GL-Trans）模块的解码器，以同时捕捉全局语义结构和局部边界细节，从而提升在噪声标签下的分割性能。

Result: UKANFormer在珊瑚类别上的IoU达到67.00%，像素准确率为83.98%，且生成的预测结果在视觉和结构上优于训练所用的噪声标签。

Conclusion: 模型架构设计可以有效缓解标签噪声的影响，突破数据质量对性能的限制，为标签稀缺条件下的生态监测提供了可行方案。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [71] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种用于具身AI的统一世界模型框架，形式化了问题设置与学习目标，并提出了一个涵盖功能、时间建模和空间表示的三轴分类法。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测动作如何改变未来状态的智能体，而世界模型作为内部模拟器在其中起关键作用，但缺乏统一的框架和评估标准。

Method: 提出三轴分类法：功能性（决策耦合vs通用）、时间建模（序列模拟与推断vs全局差异预测）、空间表示（全局潜在向量、特征序列、空间潜在网格、分解渲染表示），并系统整理数据资源与评估指标。

Result: 系统化了机器人、自动驾驶和视频领域的数据集与指标，对前沿模型进行了定量比较，并识别出关键挑战，如缺乏统一数据集、需更注重物理一致性而非像素保真度的评估指标、计算效率与性能的权衡，以及长期时序一致性建模难题。

Conclusion: 该框架为世界模型的研究提供了系统性组织和未来方向，强调需发展更高效、物理一致且适用于长期预测的模型。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [72] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 本文研究了在图像生成中应用推理时搜索策略的挑战，发现视觉自回归模型的离散性和序列性使其能够有效进行搜索，尤其是波束搜索显著提升了文本到图像生成的质量，使20亿参数的自回归模型超越了120亿参数的扩散模型。研究表明模型架构对推理时优化至关重要，而不仅仅是模型规模。


<details>
  <summary>Details</summary>
Motivation: 尽管通过搜索实现的推理时扩展已革新了大语言模型，但在图像生成领域却难以取得类似进展。近期尝试将搜索策略应用于连续扩散模型效果有限，因此需要探索更适合图像生成的模型架构以充分发挥搜索的优势。

Method: 利用视觉自回归模型的离散、序列特性，采用波束搜索（beam search）进行推理时优化，并通过系统性消融实验分析离散token空间带来的计算优势，同时使用验证器分析速度与推理能力之间的权衡。

Result: 波束搜索显著提升了文本到图像生成性能，2B参数的自回归模型在多个基准上超过了12B参数的扩散模型；消融实验表明离散token空间支持早期剪枝和计算复用，是性能提升的关键因素。

Conclusion: 模型架构（如离散、序列的自回归结构）对于图像生成中的推理时优化至关重要，相比单纯扩大模型规模，合适的架构更能有效发挥搜索策略的潜力。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [73] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出了一种基于人类感知显著性的图像超分辨率伪影评估方法，构建了包含1302个样本的数据集，并训练了一个轻量级回归模型生成伪影显著性热图。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率方法产生的伪影对视觉质量影响不同，但通常被等同对待；应根据人类观察者的感知显著性来区分伪影的重要性。

Method: 收集1302个来自11种现代超分辨率方法的伪影样本，通过众包获取每个伪影的显著性评分，并基于此数据集训练一个轻量级回归器以生成空间显著性热图。

Result: 所提出的回归模型在检测显著性伪影方面优于现有方法，并能生成准确的伪影显著性热图。

Conclusion: 伪影应根据其对人类感知的显著性进行评估，该研究为超分辨率伪影的识别、评估与缓解提供了新工具和数据支持。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [74] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为WaMaIR的新框架，结合全局多尺度小波变换卷积（GMWTConvs）和基于Mamba的通道感知模块（MCAM），扩大感受野并增强通道间长程依赖建模，有效提升图像恢复中纹理细节的重建效果，并通过多尺度纹理增强损失（MTELoss）进一步优化细节保持，在性能和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN方法因感受野有限且缺乏对通道特征的有效建模，难以充分恢复图像的精细纹理细节。

Method: 提出WaMaIR框架，引入GMWTConvs以扩大感受野并提取多尺度纹理特征，设计MCAM模块捕捉通道间的长程依赖关系，并采用MTELoss损失函数指导模型更好地保留纹理结构。

Result: 实验表明，WaMaIR在多个图像恢复任务中优于当前最先进的方法，显著提升了纹理细节恢复质量，同时保持较高的计算效率。

Conclusion: WaMaIR通过结合小波变换、通道注意力机制与专用损失函数，有效解决了CNN在纹理恢复中的局限性，为高效高质量图像恢复提供了新思路。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [75] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出一种名为Region in Context的文本条件图像编辑框架，通过双层语义对齐实现局部编辑与全局上下文的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本驱动的图像区域编辑中忽视了局部与整体的语义关系，导致编辑结果不一致或缺乏连贯性。

Method: 引入双层引导机制：区域级描述与带全图上下文的区域表示对齐，同时将整图与由大型视觉语言模型生成的场景级描述匹配，实现多级语义对齐。

Result: 实验表明该方法在编辑结果的语义一致性与指令对齐方面优于现有方法。

Conclusion: 通过显式的语言参考和全局-局部上下文建模，能有效提升文本驱动图像编辑的整体协调性和准确性。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [76] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba网络和参数高效微调的X射线医学报告生成框架EMRRG，通过改进视觉骨干和解码器结构，在多个基准数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学报告生成模型多依赖大语言模型，忽视了预训练视觉基础模型和先进微调技术（如增强跨注意力机制）的潜力，且非Transformer架构（如Mamba）在该领域尚未被充分探索。

Method: 提出EMRRG框架：将X射线图像分块编码，采用SSM-based视觉主干提取特征，并使用Partial LoRA进行高效微调；结合具有混合解码器的大语言模型生成报告，实现端到端训练。

Result: 在三个主流基准数据集上的实验表明，所提方法在X射线医学报告生成任务中表现优异，验证了其有效性。

Conclusion: EMRRG通过引入Mamba架构和参数高效微调策略，为X射线医学报告生成提供了新思路，显著提升了生成性能，展示了非Transformer模型在该领域的应用潜力。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [77] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: 提出了一种名为GS2POSE的新方法，用于6D物体姿态估计，通过可微渲染和光照适应性优化，在多个数据集上提升了精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无纹理物体和光照变化条件下表现不佳，需改进6D姿态估计的鲁棒性和准确性。

Method: 基于Bundle Adjustment思想，利用李代数扩展3D高斯泼溅（3DGS），构建可微渲染流水线，迭代优化姿态并更新颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别比先前方法提高了1.4%、2.8%和2.5%的准确率。

Conclusion: GS2POSE通过可微渲染和颜色参数优化，有效提升了在挑战性场景下的6D姿态估计性能。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [78] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出一种无需训练的视频理解框架，通过结合预训练视觉语言模型和经典机器学习方法实现零样本视频结构化分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据和特定任务训练，成本高且难以扩展，而大模型在静态图像上的零样本能力尚未有效迁移到视频领域。

Method: 将视频理解重构为高维语义空间中的自监督时空聚类问题：利用冻结的VLM视觉编码器提取语义特征轨迹，使用KTS进行时间分割，并采用密度聚类发现宏观场景与主题，最后通过VLM生成文本描述。

Result: 实现了无需训练的视频内容结构化摘要生成，在多种视频上验证了方法的有效性和可解释性，能自动识别并描述重复出现的场景和事件。

Conclusion: 该框架提供了一条高效、可解释且模型无关的路径，成功将VLM的零样本能力扩展到视频理解任务中。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [79] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种即插即用的新方法，通过附加轻量级模块为冻结的多模态大语言模型（MLLM）实现像素级分割能力，无需微调，保持模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在为MLLM添加分割能力时需微调模型，改变了输出空间并损害了其泛化能力，违背了构建统一模型的目标。

Method: 提出LENS方法，在完全冻结的MLLM上附加一个轻量可训练模块，通过优化注意力图中的空间线索提取关键点，并生成与掩码解码器兼容的点特征。

Result: 实验表明，LENS在分割性能上达到或超过需重新训练的方法，同时完全保留了MLLM原有的泛化能力。

Conclusion: LENS提供了一种高效且强大的扩展MLLM的新范式，有助于实现真正多才多艺的统一多模态模型。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [80] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出一种完全无监督的二值道路分割方法，利用场景几何和时间线索生成弱标签并通过时间一致性优化，显著减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵人工标注数据集的依赖，实现可扩展的道路分割。

Method: 首先基于几何先验生成弱标签（如地平线以上为非道路），然后通过跨帧特征点跟踪和互信息最大化来增强时间一致性以优化标签。

Result: 在Cityscapes数据集上达到82%的IoU，表现出高精度和良好的时间稳定性。

Conclusion: 结合几何约束和时间一致性是一种高效、可扩展的无监督道路分割方案，适用于自动驾驶场景。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [81] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练文本到图像扩散模型的个性化图像滤波器（PIF），通过文本反转技术学习并迁移参考图像中的摄影风格，在保持内容图像内容的同时，有效提取和转换多种摄影风格。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在从参考图像中学习有意义的摄影概念或保持内容图像的内容方面存在不足，因此需要一种能够同时解决这两个问题的新方法。

Method: 基于预训练的文本到图像扩散模型，利用生成先验来学习摄影概念的平均外观及其根据文本提示的调整方式；使用文本反转技术优化摄影概念的提示词，从而学习参考图像的摄影风格。

Result: PIF在提取和迁移各种摄影风格方面表现出色，能够在保持内容图像内容的同时实现高质量的风格迁移。

Conclusion: PIF成功地解决了现有方法在摄影风格学习与迁移中的局限性，展示了在不同类型摄影风格上的优异性能。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [82] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 本文介绍了ReefNet，一个大规模公开的珊瑚礁图像数据集，包含约92.5万个专家验证的属级硬珊瑚点标签，并与海洋物种世界登记册（WoRMS）进行分类映射，支持细粒度、全球范围的珊瑚分类与领域泛化研究。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化等人为因素导致珊瑚礁迅速退化，亟需可扩展、自动化的监测手段；现有数据集在规模、地理覆盖或标签精细度方面存在局限，且缺乏机器学习就绪的标注格式。

Method: ReefNet整合了来自76个CoralNet来源及红海Al Wajh站点的图像，提供细粒度且分类学对齐的标签；提出了两种评估设置：源内划分的局部评估和保留整个数据源的跨源域泛化评估；并在监督与零样本分类下测试模型性能。

Result: 监督学习在源内表现良好，但跨域性能显著下降；零样本模型整体表现较差，尤其对稀有和视觉相似的珊瑚属识别困难。

Conclusion: ReefNet为珊瑚礁监测提供了具有挑战性的新基准，旨在推动领域泛化和细粒度珊瑚分类技术的发展，并将公开数据、代码和预训练模型以促进全球珊瑚礁保护。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [83] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 本研究提出了一种名为AdaptMoist的域适应方法，利用五种纹理特征的组合，实现跨来源木材碎片含水率的准确预测，显著提升了模型在不同数据源间的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有含水率预测方法在面对不同来源的木材碎片时，因数据分布差异导致性能下降，缺乏对源变异性的有效应对机制。

Method: 从木材碎片图像中提取五种不同的纹理特征，分析其在含水率预测中的表现，并提出AdaptMoist域适应方法，结合调整后的互信息准则进行模型保存，以实现跨域知识迁移。

Result: 组合五种纹理特征达到95%的预测准确率；AdaptMoist方法将跨域预测准确率从57%提升至80%，提高了23%。

Conclusion: AdaptMoist是一种有效的跨域木材碎片含水率预测解决方案，具有在依赖木材碎片的工业中广泛应用的潜力。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [84] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 提出2DGS-R方法，通过分层训练策略在几乎不增加存储和训练时间开销的情况下，显著提升渲染质量并保持几何精度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点阵难以准确表示表面，2D高斯虽提升几何保真度但牺牲了渲染质量，单一训练阶段难以兼顾二者。

Method: 采用分层训练：首先用法向一致性正则化训练2D高斯；然后选择渲染质量不足的高斯进行原位克隆增强；最后冻结透明度进行微调。

Result: 相比原始2DGS仅增加1%存储和极少训练时间，显著提升渲染质量并保留精细几何结构。

Conclusion: 2DGS-R有效平衡效率与性能，在视觉保真度和几何重建精度上均取得改进。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [85] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级基于Transformer的语义分割框架ArmFormer，用于实现像素级武器检测，结合CBAM与MixVisionTransformer架构，在保持高计算效率的同时达到最先进的分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法仅提供粗略的边界框定位，缺乏细粒度分割；现有语义分割模型在精度与计算效率之间难以平衡，且难以部署于边缘设备。

Method: 提出ArmFormer，采用CBAM增强的编码器主干和集成注意力机制的Hamburger解码器，结合卷积与Transformer优势，实现五类武器（手枪、步枪、刀、左轮手枪、人体）的多类别语义分割。

Result: ArmFormer在测试中达到80.64% mIoU和89.13% mFscore的性能，实时推理速度达82.26 FPS，仅需4.886G FLOPs和3.66M参数，优于计算量高达其48倍的重型模型。

Conclusion: ArmFormer在精度与效率之间取得了优异平衡，是适用于便携式安防摄像头、监控无人机和嵌入式AI加速器等边缘设备的理想武器分割解决方案。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [86] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为BARL的半监督医学图像分割框架，通过在标签空间和表示空间中进行双向对齐，提升了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注标签空间的一致性，忽视了表示空间的对齐，导致特征学习不够判别性和空间连贯性。

Method: 设计了双路径正则化（DPR）和渐进式认知偏差校正（PCBC）实现标签空间对齐；通过区域级和病灶实例匹配实现表示空间对齐。

Result: 在四个公开数据集和一个私有CBCT数据集上实验表明，BARL consistently 超越了当前最先进的方法。

Conclusion: BARL有效结合了标签空间与表示空间的对齐机制，显著提升了半监督医学图像分割的性能。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [87] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出一种注册引导的旋转不变特征提取框架，将点云配准与基于记忆的异常检测相结合，通过联合优化对齐和表示学习，提升3D点云异常检测的鲁棒性和判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆库的方法在特征变换一致性、局部几何细节捕捉和旋转不变性方面存在不足，尤其在配准失败时检测结果不可靠。

Method: 将特征提取嵌入到点云配准的学习过程中，通过联合优化配准与异常检测目标，实现旋转不变且具有局部判别性的特征表示。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上实验表明，该方法在检测效果和泛化能力上均优于现有方法。

Conclusion: 注册引导的特征提取能有效提升3D异常检测的性能，验证了配准与表示学习结合的重要性。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [88] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一种新的神经元级别分析框架，通过结合人工神经网络（VLMs）和fMRI体素编码，揭示了视觉-语言模型在神经层面具有类脑的多模态信息处理机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分捕捉人脑的多模态处理特性，且忽视了单个神经元的作用，因此需要从神经元层面探究人工神经网络与大脑活动的相似性。

Method: 结合细粒度的人工神经元分析与基于fMRI的体素编码技术，对CLIP和METER两种不同架构的视觉-语言模型进行神经层面的比较研究。

Result: 发现人工神经元能预测多个功能网络中生物神经元的活动；人工与生物神经元均表现出功能冗余和极性模式；不同模型架构导致不同的类脑激活模式。

Conclusion: 视觉-语言模型在神经元层面展现出类脑的分层多模态处理机制，模型架构显著影响其与大脑活动的对应关系。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [89] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种分类诱导的扩散模型Class-N-Diff，用于同时生成和分类皮肤镜图像，通过在扩散模型中集成分类器来指导基于类别条件的图像生成，提高了生成图像的真实性和多样性，并增强了下游诊断任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型在生成准确表示特定医学类别的图像方面存在困难，限制了其在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成一个分类器，以类别条件引导图像生成过程，实现更精确的类别条件图像合成。

Result: Class-N-Diff模型生成的图像更加真实和多样化，且集成的分类器在下游诊断任务中表现出更高的性能。

Conclusion: Class-N-Diff通过分类器与扩散模型的结合，显著提升了合成皮肤镜图像的质量和实用性，是一种增强扩散模型在医学图像生成中应用的有效方法。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [90] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种基于策略优化的指令式图像编辑后训练框架Edit-R1，结合无似然的DiffusionNFT方法和多模态大语言模型作为免训练奖励模型，显著提升了模型的泛化能力与性能，在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 监督微调的指令式图像编辑模型容易过拟合于标注模式，难以泛化到训练分布之外的任务，缺乏通用奖励模型也限制了强化学习的应用。

Method: 提出Edit-R1框架，采用Diffusion负感知微调（DiffusionNFT）进行策略优化，并利用多模态大语言模型（MLLM）作为免训练的统一奖励模型，通过其输出logits提供细粒度反馈；设计低方差分组过滤机制以降低评分噪声、稳定训练过程。

Result: UniWorld-V2在ImgEdit和GEdit-Bench上分别取得4.49和7.83的SOTA成绩，且该框架具有模型无关性，可显著提升Qwen-Image-Edit和FLUX-Kontext等不同基础模型的性能。

Conclusion: Edit-R1为指令式图像编辑提供了一个高效、通用的后训练框架，通过结合高阶采样、免训练奖励建模和稳定优化策略，实现了强泛化能力和广泛适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [91] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种基于地面相机观测的卷云归因框架，用于将观测到的航迹云与生成它们的航班进行关联，为航空非二氧化碳气候影响研究提供了高时空分辨率的数据支持。


<details>
  <summary>Details</summary>
Motivation: 由于卫星观测在时空分辨率上的限制以及航迹云形成后的漂移和形变，难以准确实现航迹云到航班的归因，因此需要一种更精确的方法来验证和校准物理模型。

Method: 利用地面可见光相机航迹云序列（GVCCS）数据集，结合飞机监视数据和气象数据生成理论航迹云，并通过多种几何表示、距离度量、时间平滑和概率分配策略，构建模块化的归因框架。

Result: 成功建立了能够有效关联地面观测航迹云与飞行航班的模块化框架，实现了高精度的短期航迹云归因。

Conclusion: 该框架为航迹云-航班归因提供了强有力的基线方法，并具有良好的可扩展性，有助于未来航空气候影响的研究与建模。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [92] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本研究探讨了基于Transformer的模型在热成像武器分割中的应用，比较了四种架构（SegFormer、DeepLabV3+、SegNeXt和Swin Transformer）在自建热成像数据集上的性能，结果表明SegFormer-b5在精度上表现最佳，而SegFormer-b0具有最快的推理速度，展示了Transformer在低光和遮挡环境下强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于传统CNN在捕捉长距离依赖和细节结构方面存在局限性，且RGB系统在低光或视觉遮挡条件下失效，因此需要探索更适合热成像武器分割的模型。

Method: 采用四种基于Transformer的语义分割模型（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer），在包含9711张真实监控视频提取并使用SAM2自动标注的热成像图像数据集上进行训练与评估，使用MMSegmentation框架结合标准数据增强策略进行公平比较。

Result: 实验结果显示：SegFormer-b5取得最高mIoU（94.15%）和像素准确率（97.04%）；SegFormer-b0实现最快推理速度（98.32 FPS）且保持90.84% mIoU；SegNeXt-mscans兼顾速度（85.12 FPS）与性能（92.24% mIoU）；DeepLabV3+ R101-D8达到92.76% mIoU但速度较低（29.86 FPS）。

Conclusion: 基于Transformer的模型在热成像武器分割任务中表现出优越的性能和良好的速度-精度权衡，具备在复杂环境下的强泛化能力，适用于多样化的实时安防应用场景。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [93] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 本文提出了Res-Bench，一个用于评估多模态大语言模型在不同输入分辨率下性能稳定性的综合基准，包含14,400个样本、12种分辨率和六个核心能力维度，并设计了新的评估框架与鲁棒性指标。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式主要关注语义性能，忽视了模型在不同输入分辨率下的鲁棒性问题，即性能是否稳定。

Method: 提出Res-Bench基准和新评估框架，引入Spearman相关系数、绝对/相对连续误差（ACE/RCE）等鲁棒性指标，对主流MLLM进行大规模评估。

Result: 评估涵盖模型与任务层面的鲁棒性、预处理策略影响（如填充和超分辨率）以及微调对稳定性的提升作用。

Conclusion: Res-Bench有效揭示了当前MLLM在分辨率变化下的性能波动问题，为提升多模态模型的分辨率鲁棒性提供了评估基础与优化方向。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [94] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文综述了基础模型（FMs）在医学图像分析中的最新进展，系统地分类了视觉和视觉-语言FMs，并通过元分析探讨了数据集使用趋势和临床应用。同时讨论了领域适应、高效微调等挑战及联邦学习、知识蒸馏等解决方案，提出了增强鲁棒性、可解释性和临床整合的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管FMs在医学影像中快速发展，但该领域仍分散且缺乏统一的综合分析，亟需系统梳理其架构、训练范式和临床应用的演进。

Method: 将研究系统分为仅视觉和视觉-语言FMs，基于其架构基础、训练策略和下游临床任务进行分类，并开展定量元分析以揭示时间趋势。

Result: 提供了FMs在医学图像分析中的全面结构化分析，明确了当前的研究格局、发展趋势以及关键挑战。

Conclusion: FMs在医学影像中具有巨大潜力，未来需聚焦于提升其鲁棒性、可解释性与临床整合能力，以推动实际医疗应用转化。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [95] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出Di-Bregman框架，将扩散蒸馏统一为基于Bregman散度的密度比匹配，理论清晰且在生成效率和质量上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法缺乏统一的理论基础，导致优化目标不一致，限制了高效单步生成的性能。

Method: 提出Di-Bregman框架，通过Bregman散度进行密度比匹配，将多种现有蒸馏目标统一到凸分析视角下，并用于训练快速的单步生成学生模型。

Result: 在CIFAR-10和文本到图像生成任务上，Di-Bregman优于reverse-KL蒸馏，实现更低的单步FID，并保持接近教师模型的视觉保真度。

Conclusion: 基于Bregman散度的密度比匹配为高效、单步扩散生成提供了一个理论坚实且实用的新路径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [96] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出了一种名为CARE的端到端框架，通过序列-图像对比对齐（SICA）和分类联合优化，实现事件触发传感器流中的日常活动识别，显著提升性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表示层面存在局限：基于序列的方法缺乏空间感知，基于图像的方法丢失细粒度时间动态，且简单融合未能有效对齐两种表示视图。

Method: 结合时序鲁棒的序列编码与空间感知的图像表示，提出序列-图像对比对齐（SICA）机制，并采用联合对比-分类目标进行端到端训练。

Result: 在三个CASAS数据集上达到最优性能（Milan 89.8%，Cairo 88.9%，Kyoto7 73.3%），并对传感器故障和布局变化表现出强鲁棒性。

Conclusion: CARE有效融合了序列和图像表示的优势，实现了对齐且判别性强的嵌入学习，为智能家居中的可靠ADL识别提供了可行方案。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [97] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练且在线的视频步骤定位方法，利用大型多模态模型（LMM）实现零样本推理，并引入贝叶斯滤波思想开发了BaGLM框架，在三个数据集上超越了现有的离线训练方法。


<details>
  <summary>Details</summary>
Motivation: 标准视频步骤定位方法依赖标注数据和全视频离线处理，成本高且难以应用于需要实时决策的场景，因此本文探索无需训练且支持在线处理的新方法。

Method: 利用大型多模态模型（LMM）对局部帧进行零样本预测，并结合大语言模型提取的步骤转移矩阵和步骤进度估计，基于贝叶斯滤波原理构建BaGLM框架以融合历史帧信息。

Result: 实验表明，所提出的BaGLM在三个数据集上性能优于现有的基于训练的离线方法，且在线无训练策略已能超越传统离线模型。

Conclusion: 通过结合LMM的零样本能力与贝叶斯滤波机制，BaGLM实现了高效准确的在线视频步骤定位，无需任务特定训练，具有良好的实际应用前景。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [98] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文研究了不同视频特征对经典架构在时序视频定位任务中的影响，通过在三个基准数据集上使用基于CNN、时序推理和Transformer的编码器提取特征，发现不同编码器显著影响模型性能，并揭示了特定特征带来的模式与错误，表明特征互补的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于少数视频表示方法，可能导致长期的架构过拟合，因此需要系统评估不同视频特征的影响。

Method: 在Charades-STA、ActivityNet-Captions和YouCookII三个基准上，使用基于CNN、时序推理和Transformer的视频编码器提取特征，并在经典架构上进行消融实验。

Result: 改变视频编码器导致模型性能显著差异，发现了特定特征引发的规律性错误，表明不同特征具有互补潜力。

Conclusion: 视频编码器的选择对时序视频定位至关重要，多样化特征表示有助于避免过拟合并提升模型性能。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [99] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 该论文探讨了专用遥感基础模型是否优于通用视觉基础模型，结果表明在小规模（ViT-B）下，专用模型并未带来持续的性能提升。


<details>
  <summary>Details</summary>
Motivation: 遥感图像具有独特特性，促使研究者开发专用基础模型，但本文质疑这一必要性，特别是在小规模场景下。

Method: 设计了一个简单基准，评估模型在低分辨率图像上的泛化能力，并在百万级卫星图像数据集MillionAID上训练了针对遥感改进的iBOT模型。

Result: 实验显示，经过遥感特定改进的预训练模型在ViT-B规模下并未持续优于通用视觉模型。

Conclusion: 至少在小规模情况下，通用视觉基础模型在遥感任务中表现不逊于专用模型，挑战了专用模型必然更优的假设。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [100] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: 本文提出了一种名为ED-VTG的细粒度视频时间定位方法，利用多模态大语言模型通过两阶段过程实现自然语言查询在视频中的精确定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的时间定位方法存在细节缺失和幻觉问题，难以准确对齐文本与视频片段，因此需要一种能增强查询并抑制噪声的方法。

Method: 该方法首先将原始语言查询转化为包含更多上下文线索的增强句子，然后使用轻量化解码器结合多模态大语言模型的上下文化表示来预测时间边界；训练时采用多实例学习目标以动态选择最优查询版本，减少噪声和幻觉影响。

Result: 在多个视频时间定位和段落定位基准上达到最先进的性能，显著优于此前所有基于大语言模型的方法，并在零样本场景下展现出明显优势。

Conclusion: ED-VTG通过查询增强和多实例学习策略，有效提升了基于多模态大语言模型的视频时间定位精度和鲁棒性，兼具强泛化能力。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [101] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种名为What-Where Representation Re-Forming (W2R2)的新训练框架，旨在解决视觉语言模型在多模态3D定位中存在的“2D语义偏差”问题，通过解耦表示学习和抑制捷径学习，显著提升了复杂环境下的3D定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型过度依赖2D图像特征进行粗略定位，忽视了3D几何信息，导致多模态融合性能不佳，难以实现精确的3D空间推理。

Method: 提出W2R2框架，将2D特征用于‘What’语义识别，3D特征用于‘Where’空间定位，通过解耦表示学习实现精准3D定位；引入双目标损失函数，包括促进多模态协同的对齐损失和抑制2D主导伪输出的基于边界的伪标签损失。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2显著提高了定位准确率和模型鲁棒性，尤其在杂乱的室外场景中表现突出。

Conclusion: W2R2有效缓解了多模态3D定位中的2D语义偏差问题，无需修改推理结构即可提升模型性能，为视觉语言模型中的空间推理提供了新的训练范式。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [102] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件生成对抗网络（StyleGAN2-ADA、StyleGAN3 和 CycleGAN）生成高分辨率合成指纹图像（包括真实和伪造指纹）的新方法，用于解决生物特征数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 大规模指纹数据集采集耗时昂贵且涉及隐私问题，因此需要合成数据来替代真实数据以支持训练和评估。

Method: 采用条件StyleGAN2-ADA和StyleGAN3生成不同手指身份的高分辨率真实指纹图像，并利用CycleGAN将其转换为模拟多种材料（如EcoFlex、Play-Doh）的逼真伪造指纹。

Result: 生成的合成数据集（DB2和DB3）各包含1500枚十指指纹及八种材质的对应伪造指纹；StyleGAN3的FID低至5，TAR达99.47%（FAR=0.01%）；NFIQ2和MINDTCT评估显示质量良好，匹配实验未发现身份泄露。

Conclusion: 该方法能高效生成高质量、隐私保护的合成指纹数据，适用于指纹识别与反欺骗系统的训练与评估。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [103] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一种医生参与的深度学习管道，用于提升肺癌CT影像分割的可重复性、预后准确性和临床信任度，结果显示VNet结合半监督学习表现最佳。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主因，CT影像在筛查和治疗中至关重要，但手动分割耗时且变异大，而深度学习虽能自动化却面临临床采纳障碍。

Method: 基于知识到行动框架，采用多中心CT数据（999名患者，12个公开数据集），比较五种深度学习模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），评估其在全图和点击裁剪图像上的分割性能，并提取497个放射组学特征进行可重复性分析，同时比较监督与半监督学习在预后建模中的表现，六名医师对分割结果进行定性评估。

Result: VNet表现最优（Dice=0.83，IoU=0.71），放射组学稳定性高（平均相关性=0.76，ICC=0.65），半监督学习下预测准确率最高（准确率=0.88，F1=0.83）；半监督学习整体优于监督学习；放射科医生更偏好VNet的肿瘤周围表征和边界质量，并倾向于使用AI生成的初始掩码进行修正而非完全依赖AI。

Conclusion: 结合VNet与半监督学习的医生参与式深度学习管道可实现准确、可重复且受临床信任的肺癌CT预后分析，为以医生为中心的AI临床转化提供了可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [104] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出了一种广义的类代表选择方法，用于提升行人重识别性能，通过平衡准确率和平均精度，在多个嵌入模型上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 选择更好的类代表是行人重识别中尚未充分探索的方向，现有方法多使用类中心，但在检索阶段表现次优，限制了性能提升。

Method: 提出一种广义的类代表选择方法，不限于类中心，可根据应用需求调整每类的代表数量，并应用于多种重识别嵌入模型之上。

Result: 在多个基准嵌入模型上应用该方法后，重识别性能显著提升，优于当前最先进的方法，尤其在准确率和平均精度之间取得更好平衡。

Conclusion: 所提出的广义代表选择方法有效提升了行人重识别的性能，具有广泛适用性和实际应用价值。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [105] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 本文提出了一种名为V-Reason的视频推理方法，利用模型输出熵信号来优化大型多模态模型的推理过程，无需强化学习或监督微调，在不牺牲准确率的情况下显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的视频推理方法依赖昂贵的强化学习和冗长的思维链，计算开销大，且缺乏对模型思考过程的有效控制机制。

Method: 通过分析模型输出熵的变化，发现高质量模型在推理过程中存在微探索与微利用的动态平衡，并在推理结束时显著降低熵以实现稳定收敛；基于此洞察，V-Reason在推理时通过一个可训练控制器对LMM的值缓存进行少量优化，以熵为目标函数调整模型行为，无需任何外部监督或训练。

Result: V-Reason在多个视频推理数据集上显著优于基础指令微调模型，平均准确率仅比RL训练模型低0.6%，同时输出token减少58.6%，大幅提升推理效率。

Conclusion: 通过熵驱动的推理过程建模，V-Reason实现了高效、无需训练的视频推理性能提升，为大型多模态模型的推理优化提供了新思路。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [106] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 比较了通用视觉模型Hiera与专用模型SAM2的特征多功能性，发现SAM2在特定任务上表现优异但牺牲了跨任务的语义泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探讨通用基础视觉模型与专用模型之间的权衡，以优化特征编码设计。

Method: 使用轻量级可训练neck探测冻结特征的适应性，并通过信息论方法量化专业化代价。

Result: SAM2在深度估计等空间相关任务上表现好，但在姿态估计和图像描述等概念远离的任务上弱于Hiera；跨neck分析显示每层适配都会加剧表示瓶颈。

Conclusion: 专业化提升特定任务性能但损失通用语义信息，需在设计下游适配策略时权衡特征普适性。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [107] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProDAT的密度感知尾部截断机制，用于点云的渐进式编码，通过单个模型实现多码率下的自适应解码，在多个数据集上优于现有学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的点云编码方法缺乏对渐进式解码的支持，难以满足资源受限环境中的实时性和低延迟需求。

Method: 提出ProDAT，利用密度信息作为指导信号，对潜在特征和坐标进行自适应解码，实现基于重要性的渐进式解码。

Result: 在SemanticKITTI和ShapeNet等基准数据集上实现了超过28.6%和18.15%的BD-rate提升，同时支持多级渐进式解码。

Conclusion: ProDAT能够有效实现高质量、高效率的渐进式点云几何编码，显著优于当前最先进的学习型编码方法。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [108] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 本文提出了一种名为FMCAF的多模态融合架构，通过频域滤波和跨模态注意力机制提升RGB与红外图像的目标检测性能，具有良好的通用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在复杂条件下提高多模态目标检测的鲁棒性，并解决现有方法缺乏跨数据集泛化能力的问题。

Method: 提出Filtered Multi-Modal Cross Attention Fusion (FMCAF)，结合频域滤波模块(Freq-Filter)去除冗余频谱特征，并利用跨注意力融合模块(MCAF)增强模态间特征共享。

Result: 在LLVIP和VEDAI数据集上显著优于传统融合方法，VEDAI上mAP@50提升13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF具备良好的通用性，无需针对特定数据集调参即可提升多模态检测性能，有望成为未来鲁棒多模态检测系统的通用融合基础。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [109] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文提出GSPlane，通过引入平面先验信息提升高斯点阵在平面区域的几何重建精度，同时保持渲染质量，并实现结构化网格生成与场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯点阵方法在重建平面区域时难以保证足够的平滑性和几何精度，限制了其在需要精确表面表示的应用中的使用。

Method: 利用现成的分割和法向预测模型提取平面先验，构建结构化的平面高斯坐标表示，并引入动态高斯重分类器以增强训练鲁棒性；进一步利用优化后的平面先验改进网格拓扑结构。

Result: 实验表明，GSPlane在不牺牲渲染质量的前提下，显著提升了多种基线方法中提取网格的几何准确性，同时减少了网格顶点和面数，改善了拓扑结构。

Conclusion: GSPlane有效结合平面先验与高斯点阵框架，实现了高质量的平面重建与结构化网格输出，支持对支撑平面上物体的解耦与灵活操作，拓展了在场景编辑和物理仿真中的应用潜力。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [110] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出一种新的优化策略，用于增强预训练扩散模型中的条件控制，以在保持真实感和美观性的同时提高内容保真度，尤其适用于低光照场景下的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在低层次视觉任务中表现优异，但在低光条件下往往牺牲内容保真度以换取感知真实性，主要原因是条件潜在表示建模不足以及条件潜在与噪声潜在之间缺乏双向交互。

Method: 引入一个包含生成先验的潜在细化管道来恢复VAE编码过程中丢失的空间细节，并实现细化后的条件潜在与噪声潜在之间的动态交互。

Result: 该方法可即插即用，能无缝集成到现有扩散网络中，在多个实验中显著提升了预训练扩散模型的内容保真度。

Conclusion: 所提出的方法有效缓解了低光照下扩散模型的内容失真问题，在保持视觉质量的同时显著提高了重建保真度。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [111] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种利用LED环境照明为消费级相机生成视觉上不可见水印的方法，通过优化LED光源的光谱特性，在保证人眼几乎无法察觉的同时，使普通相机能够检测到水印信息。


<details>
  <summary>Details</summary>
Motivation: 为了在不干扰人类视觉的前提下，实现对图像内容的版权保护和真实性验证，需要一种隐蔽且可靠的水印技术。

Method: 该方法联合考虑人眼对可见光谱的敏感性、消费级相机传感器的光谱响应特性以及窄带LED生成宽带光谱的能力，采用光谱调制而非强度调制来确保水印的不可见性，并能在标准帧率下提取水印。

Result: 能够在10秒视频片段中嵌入128比特信息，信息传输速率虽低但足以支持隐私保护和内容验证等基本元数据应用。

Conclusion: 所提出的方法实现了在常见照明条件下对消费级相机的有效、隐蔽水印嵌入，具有实际应用潜力。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [112] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出了一种名为GOOD的新框架，通过双层次引导扩散采样轨迹生成更可控且多样化的异常分布（OOD）样本，显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的OOD样本生成方法存在语义不稳定和分布偏移多样性不足的问题，限制了在真实场景中的泛化能力。

Method: GOOD框架结合了图像级引导（基于对数配分函数梯度，驱动样本向像素空间低密度区域移动）和特征级引导（基于分类器潜在空间中的k近邻距离，促进在特征稀疏区域采样），并引入一种自适应融合图像与特征差异的统一OOD评分机制。

Result: 实验表明，使用GOOD生成的样本进行训练能显著提升OOD检测性能，在定量和定性分析中均表现出更强的鲁棒性和多样性。

Conclusion: GOOD通过双层次引导机制实现了更可控、更多样化的OOD样本生成，有效增强了模型对真实世界异常数据的检测能力，为基于扩散模型的OOD检测提供了新思路。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [113] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: 提出KineDiff3D，一个统一的框架，用于从单视图输入中实现类别级关节物体的三维重建与姿态估计。


<details>
  <summary>Details</summary>
Motivation: 关节物体由于多部件几何结构和可变的关节配置，在3D重建和姿态估计中面临挑战。

Method: 通过Kinematic-Aware VAE将几何、关节角度和部件分割编码到潜在空间，并使用两个条件扩散模型分别回归全局姿态和生成潜在码，结合迭代优化模块进行双向优化。

Result: 在合成、半合成和真实数据集上验证了方法在重建精度和运动学参数估计方面的有效性。

Conclusion: KineDiff3D能有效处理类别级关节物体的形状重建与姿态估计，具有良好的跨状态泛化能力。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [114] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: 提出GACO-CAD，一种两阶段后训练框架，通过引入几何先验和强化学习奖励机制，提升从单张图像生成参数化CAD模型的几何精度和建模简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在从2D图像推断3D几何结构时空间推理能力有限，难以准确生成可编辑的参数化CAD模型。

Method: 第一阶段在监督微调中使用深度图和表面法向图作为密集几何先验，与RGB图像组成多通道输入；第二阶段在强化学习中引入组长度奖励，鼓励生成更紧凑的建模序列，并采用动态加权策略稳定训练。

Result: 在DeepCAD和Fusion360数据集上实验表明，GACO-CAD在相同MLLM主干下实现了最先进的性能，显著提升代码有效性、几何准确性和建模简洁性。

Conclusion: GACO-CAD有效提升了多模态大模型在单视图CAD生成任务中的3D几何恢复能力和程序简洁性，为工业概念设计提供了更高效的自动化工具。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [115] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 研究探讨了在黑盒设置下，不同人脸预处理技术对人脸识别系统中对抗攻击迁移性的影响，发现人脸检测模型的选择显著影响攻击成功率，而插值方法影响较小；提出一种基于输入变换的预处理不变性方法，可将攻击迁移性提高27%。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别对抗攻击研究常忽略预处理步骤（如人脸检测和图像缩放）的影响，尤其在黑盒环境下，导致攻击迁移性和实际效果受限，因此需要系统评估预处理组件对攻击性能的影响并提升其鲁棒性。

Method: 通过实验评估多种先进对抗攻击方法在不同人脸检测模型和插值方法下的迁移性，分析预处理组件对黑盒和白盒攻击成功率的影响，并提出一种基于输入变换的预处理不变性攻击方法以增强跨预处理流程的攻击效果。

Result: 实验表明人脸检测模型选择可使攻击成功率下降高达78%，而插值方法影响较小；在白盒设置中，预处理过程会因噪声与检测模型的交互意外削弱攻击强度；所提出的输入变换方法可将攻击迁移性提升最多27%。

Conclusion: 人脸预处理在对抗攻击评估中至关重要，忽视其影响会导致对攻击性能的误判；应将预处理纳入对抗样本生成过程，以提升攻击的通用性和对真实系统威胁的准确评估。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [116] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的分层采样策略GtR，通过结构生成和细节重建两阶段实现掩码自回归模型的加速，在保持生成质量的同时显著提升效率，并结合频域加权 token 选择进一步优化计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 掩码自回归模型虽支持并行生成以提升效率，但受限于单步建模视觉token空间相关性的复杂性，加速潜力不足；同时从零生成图像比在框架上补全更困难，因此需设计更高效的生成策略。

Method: 提出Generation then Reconstruction（GtR）策略，将生成分为两阶段：先慢速生成全局语义结构，再快速重建细节；并引入Frequency-Weighted Token Selection（FTS），基于高频能量定位图像细节区域，为其分配更多计算资源。

Result: 在ImageNet条件生成和文本到图像生成任务中，GtR在MAR-H模型上实现3.72倍加速，同时保持相当的生成质量（FID: 1.59，IS: 304.4 vs 原始1.59, 299.1），优于现有加速方法。

Conclusion: GtR是一种高效、无需训练的分层生成策略，通过解耦结构生成与细节重建，并结合频域感知的token调度，显著提升了掩码自回归模型的推理效率与质量平衡。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [117] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文首次对浮游生物识别中的分布外检测方法进行了大规模系统性评估，基于DYB-PlanktonNet数据集构建了多种分布偏移场景的基准测试，实验表明ViM方法在远分布外场景下显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 由于浮游生物形态复杂、物种多样性高且不断发现新物种，训练与测试数据之间存在分布偏移，导致现有模型在实际部署中面临挑战，而当前领域缺乏统一的大规模基准和最新OoD检测技术的整合。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准，并系统评估了22种OoD检测方法。

Result: 实验结果显示ViM方法在所构建的基准上显著优于其他方法，尤其是在远分布外（Far-OoD）场景下关键指标有大幅提升。

Conclusion: 该研究为自动化浮游生物识别中的算法选择提供了可靠参考，奠定了浮游生物OoD检测未来研究的基础，是该领域首个大规模系统性评估工作。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [118] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了一种联合学习高保真3D头部头像及手-脸交互引起的非刚性形变的新框架，通过引入深度顺序损失、接触正则化和基于PCA的手部诱导形变基，在单目视频中实现了更真实、物理合理的动态面部重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D头像方法大多忽略手与面部的自然交互（如托腮、轻触脸颊），而这些交互能传达思考等认知状态，因此需要建模手-脸交互带来的面部形变以提升虚拟现实、远程呈现等应用的真实感。

Method: 提出结合深度顺序损失与接触正则化的姿态跟踪方法以准确捕捉手与脸的空间关系；构建包含手-脸交互的数据集并从中学习特定于手部诱导面部形变的PCA基；引入受物理仿真启发的接触损失，减少穿透伪影并增强结果的物理合理性。

Result: 在iPhone拍摄的RGB(D)视频和自建合成数据集上验证了方法的有效性，相比当前最先进的表面重建方法，能更准确地恢复面部外观和动态形变几何。

Conclusion: 该框架首次实现了对手-脸交互下高保真3D头像的联合建模，显著提升了包含复杂手部接触时的面部形变重建质量与物理可信度。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [119] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出了HIDISC，一种用于跨域广义类别发现（DG-GCD）的双曲表示学习框架，通过GPT引导的数据增强和流形感知的Tangent CutMix生成多样化伪新类样本，并结合统一损失函数实现紧凑且语义结构化的嵌入，在多个基准上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法通常假设训练时可同时获取同域的有标签和无标签数据，难以应对开放世界中的分布偏移问题；而现有的DG-GCD方法依赖高成本的多域模拟训练，存在计算开销大和误差累积的问题。

Method: 提出HIDISC框架：利用GPT引导的扩散增强源域以引入多样但适度的域变化；设计Tangent CutMix在切空间中进行曲率感知插值，合成保持流形一致性的伪新类样本；采用统一损失函数（结合惩罚Busemann对齐、混合双曲对比正则和自适应异常排斥）优化嵌入空间；引入可学习曲率参数以适应数据集复杂性。

Result: HIDISC在PACS、Office-Home和DomainNet三个基准上均取得当前最优结果，显著优于现有的欧几里得和双曲(DG)-GCD基线方法，且无需昂贵的 episodic 训练。

Conclusion: HIDISC通过双曲空间建模和几何感知的数据增强策略，有效解决了DG-GCD中的域泛化与新类发现挑战，具备高效性、灵活性和强泛化能力。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [120] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出一种零样本的视觉token剪枝方法，通过结合任务相关性和信息多样性，在减少90% token的情况下仍保持与现有技术相匹配或更优的性能，同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉token剪枝方法通常忽略文本提示的指导，未能优先考虑任务相关性，导致在处理大规模输入时推理成本过高。

Method: 提出一种分层方法，首先选择与任务相关的视觉token核心集，然后补充多样性token以保留上下文信息，实现任务相关性与信息多样性的平衡。

Result: 在多个模型和基准上的实验表明，该方法在仅剪枝少量token时性能与现有方法相当甚至更好，且在剪枝高达90% token时仍保持极小精度损失，并显著降低GPU内存占用和推理延迟。

Conclusion: 该方法通过引入提示感知机制，在不依赖微调的情况下有效平衡了任务相关性与信息冗余，为VLM的高效推理提供了新的解决方案。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [121] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 本研究利用Segment Anything Model (SAM) 构建了一个专门用于监测孟加拉国河流侵蚀导致土地和村庄消失的AI模型，结合手动标注的历史影像数据集，实现了高精度的侵蚀区域分割，并为政策制定者提供了量化土地流失和预测灾害路径的新工具。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国主要河流每年造成严重的河岸侵蚀，导致村庄和农田消失，传统方法难以有效监测这一过程。缺乏精确且带标注的数据集使得自动化分析尤为困难，因此需要一种高效、精准的方法来追踪和量化这种缓慢但持续的灾害。

Method: 研究团队构建了一个从2003年至2025年覆盖孟加拉国多个脆弱地区的新型数据集，包含从Google Earth历史影像中手动标注的已消失聚落信息。首先使用简单的颜色通道分析对陆地与水体进行粗略分割，然后微调SAM模型的掩码解码器以识别河岸侵蚀的细微特征。

Result: 该模型在分割精度上显著优于传统方法和现成深度学习模型，平均交并比（IoU）达到86.30%，Dice系数达92.60%。研究发布了首个因河流侵蚀而消失聚落的标注数据集，并提出了一种可生成可视化证据的土地流失量化方法。

Conclusion: 所提出的微调SAM模型结合专用数据集，为监测河岸侵蚀提供了一种高度有效的解决方案，有助于政策制定者和灾害管理机构更好地理解侵蚀趋势、预测未来风险并保护受影响社区。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [122] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 本文提出了一种基于比赛画面小地图信息分析的VALORANT回合结果预测模型，通过引入角色位置和游戏事件等战术特征，利用TimeSformer视频识别模型提升预测准确率。实验结果显示，使用增强战术标签数据集训练的模型在回合中后期达到约81%的准确率，显著优于仅使用小地图信息的模型。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多依赖比赛日志和统计数据，难以捕捉复杂策略信息。本文针对需要高度战术配合的FPS游戏VALORANT，旨在通过分析比赛画面中的小地图信息，提取更细粒度的战术特征以提升回合结果预测性能。

Method: 基于TimeSformer视频识别模型，构建回合结果预测框架；从小地图视频中提取角色位置、行动轨迹及关键事件（如交火、技能使用）等战术特征，并将这些标签用于数据集增强，训练并优化预测模型。

Result: 在增强战术事件标签的数据集上训练的模型，在回合中后期实现了约81%的预测准确率，显著优于仅使用原始小地图信息的基准模型，验证了战术特征的有效性。

Conclusion: 从小地图比赛画面中提取并利用详细战术特征，能有效提升VALORANT回合结果的预测精度，表明基于视觉信息的战术分析在电竞AI预测中具有重要价值。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [123] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 提出了一种针对内窥镜图像诊断的新型类增量学习框架EndoCIL，通过三个关键组件有效缓解灾难性遗忘和类别不平衡问题，在多个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于回放的类增量学习方法在内窥镜图像分析中因领域差异和类别不平衡而难以有效缓解灾难性遗忘，需开发更适应临床数据动态变化的方法。

Method: 提出EndoCIL框架，包含基于最大均值差异的回放策略（MDBR）、先验正则化类别平衡损失（PRCBL）和全连接梯度校准（CFG），分别用于选择代表性样本、缓解阶段内外的类别不平衡及校正分类器偏差。

Result: 在四个公开内窥镜数据集上的实验表明，EndoCIL在不同缓冲区大小和评估指标下普遍优于最先进的类增量学习方法。

Conclusion: EndoCIL能有效平衡稳定性与可塑性，具备良好的临床可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [124] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出基于DINOv2的活体检测方法，用于识别真实人脸与伪造人脸图像之间的细微差异，有效防御面部伪造攻击。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统易受伪造攻击（如照片欺骗），需在识别前检测此类攻击以确保安全。

Method: 采用带有registers的DINOv2模型提取可泛化的特征，并抑制注意力机制中的扰动，使模型更关注关键的细微特征。

Result: 在ICCV2025反欺骗工作坊提供的数据集和SiW数据集上实验验证了该方法的有效性。

Conclusion: 所提方法能有效检测面部伪造攻击，提升了活体检测的鲁棒性和准确性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [125] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为VisiPruner的训练-free剪枝框架，基于对多模态大语言模型（MLLMs）三阶段跨模态交互过程的系统性分析，显著减少了视觉相关计算开销，并为高效MLLM训练提供了可操作的指导。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理多模态信息时存在计算开销大的问题，且缺乏对模型如何融合多模态信息的深入理解，因此需要一种基于内在处理机制的高效剪枝方法。

Method: 通过系统分析发现MLLM存在三阶段跨模态交互过程，并据此设计VisiPruner框架，在浅层保留任务意图识别，在中层保留关键视觉令牌进行融合，深层则丢弃视觉令牌以专注语言优化，实现训练-free的动态剪枝。

Result: VisiPruner在LLaVA-v1.5 7B上最多减少99%视觉相关注意力计算和53.9%的FLOPs，性能优于现有令牌剪枝方法，并在多种MLLM上具有良好泛化性。

Conclusion: 通过揭示MLLM的分层处理动态，VisiPruner不仅大幅降低计算成本，还为设计更高效的多模态模型架构提供了新思路。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [126] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 本文提出了一个新的多片段时刻检索数据集QV-M$^2$和一个名为FlashMMR的框架，用于解决现实场景中一个查询对应多个相关视频片段的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的时刻检索方法主要关注单一片段检索，难以应对实际应用中一个查询可能对应多个相关时刻的情况，导致现有数据集和方法在视频时序定位任务中存在局限性。

Method: 提出了QV-M$^2$数据集，并设计了适用于多片段检索的新评估指标；在此基础上提出FlashMMR框架，包含多片段后验证模块、约束性时间调整和验证模块，以优化片段边界并过滤低置信度候选片段。

Result: 在QV-M$^2$数据集上，FlashMMR相比之前的SOTA方法在G-mAP、mAP@3+tgt和mR@3指标上分别提升了3.00%、2.70%和2.56%；重新评估了6种现有MR方法，验证了新基准的有效性。

Conclusion: QV-M$^2$数据集和FlashMMR框架为更贴近真实场景的多片段时刻检索提供了有效基准和强基线，推动了视频时序定位领域的研究发展。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [127] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出了一种公平性感知的深度伪造检测框架，结合时间特征学习和人口统计信息感知的数据增强，提升了检测的公平性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度且无法捕捉时间信息，导致在不同人群中的决策不公平和结果不可靠。

Method: 采用基于序列的聚类进行时间建模，结合概念提取提升可解释性，并提出一种人口统计信息感知的数据增强方法，通过频域变换平衡少数群体并保留伪造痕迹。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上，结合Xception和ResNet等SOTA架构的实验表明，该方法在公平性与准确性之间达到了最优权衡。

Conclusion: 所提框架有效缓解了深度伪造检测中的偏见问题，增强了模型的可解释性和跨人群的泛化性能。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [128] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集、整理和统一的包含2400万样本的视觉-语言模型数据集，通过半自动化的人工参与流程整合了200多个来源，并经过严格去重和去污染处理，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型研究受限于公共数据集的碎片化、不一致和污染问题，亟需一个高质量、大规模的统一数据资源。

Method: 提出FineVision数据集，采用半自动化、人工参与的流水线整合超过200个数据源为185个子集，进行自动化摄入、模式映射，并由人工审核映射结果；同时实施跨源去重和对66个公开基准的去污染，支持智能体/GUI任务并验证执行保真度。

Result: 在FineVision上训练的模型在广泛的评估套件中持续优于基于现有开源混合数据训练的模型，验证了数据规模、清洁度及人机协同自动化的优势。

Conclusion: FineVision是当前最大规模的开源视觉-语言数据集，其高质量的构建流程显著推动了数据驱动的视觉-语言模型发展，作者已公开发布该数据集和整理工具。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [129] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出Plug-and-Forecast（PnF）方法，通过结合多模态大语言模型（MLLMs）增强现有运动预测模型，利用自然语言理解复杂场景，在无需微调的情况下实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在标准条件下表现良好，但在多样化真实场景中成本效益的泛化能力仍面临挑战。

Method: 设计提示词从MLLM中提取结构化场景理解，并将其蒸馏为可学习的嵌入以增强行为预测模型，实现即插即用的零样本推理。

Result: 在Waymo Open Motion和nuScenes数据集上验证了两种先进运动预测模型，均表现出一致的性能提升。

Conclusion: PnF方法无需微调即可有效提升运动预测性能，具有良好的实用性和广泛适用性。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [130] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的显著性引导跨层深度特征融合框架（SG-CLDFF），结合显著性先验与多尺度特征聚合，用于提升白细胞分割与分类的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 白细胞在显微图像中的准确分割与分类对血液病诊断至关重要，但受染色差异、复杂背景和类别不平衡影响，现有方法仍面临挑战。

Method: 提出SG-CLDFF框架：首先利用显著性先验定位候选区域；采用EfficientSwin风格轻量混合主干网络提取多分辨率特征；通过受ResNeXt-CC启发的跨层融合模块整合浅层与深层特征；采用多任务学习，联合分割与分类头，并引入类别感知损失与显著性对齐正则化；结合Grad-CAM与显著性一致性检查提升可解释性。

Result: 在BCCD、LISC和ALL-IDB等公开数据集上验证，SG-CLDFF在IoU、F1分数和分类精度上均优于主流CNN与Transformer基线模型；消融实验表明显著性预处理与跨层融合均带来性能增益。

Conclusion: SG-CLDFF为临床环境下的自动化白细胞分析提供了一条高效且可解释的技术路径。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [131] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出一种基于YOLOv11的自动手术照明系统，通过检测术区上方蓝色标记自动调节LED光源，提升照明一致性并减轻外科医生疲劳。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明依赖手动调节，易导致医生疲劳、颈部劳损及光照不稳，影响手术精度与安全。

Method: 采用YOLOv11算法检测手术场景中位于目标区域上方的蓝色球形标记，结合云台伺服电机自动控制高功率LED光源指向识别位置。

Result: YOLO模型在模拟手术场景验证集上达到96.7%的mAP@50，系统能有效实现精准、稳定的自动照明。

Conclusion: 该机器视觉驱动的自动照明方案可减少医生体力负担，提高光照一致性，有助于改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [132] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现了自监督学习中一个反直觉的现象：更长的训练可能导致密集预测任务（如语义分割）性能下降，称之为自监督密集退化（SDD），并提出了一种密集表示结构估计器（DSE）来评估无标注下的密集任务性能，进而提出了基于DSE的模型选择和正则化方法，有效缓解了该问题。


<details>
  <summary>Details</summary>
Motivation: 在自监督学习中，尽管模型在预训练中表现良好，但在下游密集预测任务上长时间训练后性能可能下降，而现有方法缺乏有效的无监督评估手段，因此需要一种可靠的指标来指导模型选择和训练优化。

Method: 提出了Dense representation Structure Estimator (DSE)，包含类别相关性和有效维度两个度量，并基于DSE设计了模型选择策略和正则化方法，以缓解SDD问题。

Result: 在16种主流自监督方法和4个基准上验证了DSE与下游任务性能高度相关，模型选择平均提升3.0% mIoU，且计算开销极低；DSE正则化能持续缓解密集退化现象。

Conclusion: SDD是自监督学习中普遍存在的问题，DSE提供了一种有效的无监督评估与优化途径，显著提升密集任务性能。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [133] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了LongInsightBench，首个用于评估模型理解长视频能力的基准，涵盖视觉、音频和文本多模态，聚焦人类语言、视角、动作等上下文元素。该基准包含长时高信息密度视频、多样化挑战性任务场景以及严格的数据质量保障流程。实验表明，现有多模态模型在精确时间定位和长程因果推理任务上仍面临挑战，并揭示了多模态融合中的信息丢失与处理偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解基准大多关注短时或低信息密度视频，缺乏对长时、高信息密度且富含语言和上下文内容的视频的理解评估。因此，需要一个更贴近真实场景的多模态长视频理解 benchmark 来揭示当前多模态模型的能力局限。

Method: 1. 从开源数据集FineVideo中筛选约1000个满足时长和信息密度要求的视频，主要涵盖讲座、访谈和vlog；2. 设计六种具有挑战性的任务场景，包括事件内和事件间任务；3. 构建三步半自动化的数据质量保证流程以确保问题和选项的有效性与难度；4. 在该基准上对现有多模态模型进行系统实验，分析其在时间定位和因果推理等方面的表现。

Result: 实验结果显示，当前的多模态模型（OLMs）在需要精确时间定位（T-Loc）和长程因果推理（CE-Caus）的任务上表现不佳；扩展实验进一步揭示了多模态融合过程中存在信息丢失和处理偏见的问题。

Conclusion: LongInsightBench为评估长视频理解提供了可靠的新基准，揭示了现有模型在处理复杂、长时多模态内容上的不足，推动未来研究关注更有效的多模态融合与长期上下文建模。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [134] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: 提出CausalMamba框架，用于解决fMRI因果推断中的血流动力学失真和计算复杂性问题，相比传统方法在准确性和可扩展性上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI因果推断方法（如DCM）存在不适定性和计算不可行的问题，且难以从BOLD信号中准确恢复神经因果关系。

Method: 将问题分解为两个阶段：首先通过BOLD去卷积恢复潜在神经活动，然后使用新型Conditional Mamba架构进行因果图推断。

Result: 在模拟数据上比DCM准确率高37%；在真实任务fMRI数据中以88%保真度恢复已知神经通路，而传统方法在99%以上受试者中失败；揭示工作记忆中脑网络因果枢纽的动态切换机制。

Conclusion: CausalMamba为大规模fMRI因果推断提供了高效、实用的工具，能够捕捉认知功能背后的稳定回路模式与灵活网络动态。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [135] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文研究了针对对抗性衣物的防御方法的有效性，发现现有的防御手段在面对大面积且自然的对抗性衣物时表现不佳，即使是在物理世界中也是如此。作者制作了一套能够成功攻击多种防御模型的衣服，在未受保护的检测器上达到了96.06%的攻击成功率，并在九个受保护模型上实现了超过64.84%的攻击成功率，揭示了现有防御方法对对抗性衣物的共同脆弱性。


<details>
  <summary>Details</summary>
Motivation: 由于实验表明简单地增大补丁尺寸就能使现有的防御方法失效，因此作者受到启发，评估了各种防御方法对抗具有大覆盖面积的人体衣物形式的对抗性补丁的效果。

Method: 通过实验评估不同防御方法对抗数字和物理世界中的对抗性衣物的效果，并设计一套可以突破多个防御机制的对抗性衣物。

Result: 所有测试的防御方法在对抗对抗性衣物方面都表现出较差的性能；设计的一套对抗性衣物在物理世界中对未设防探测器的攻击成功率达到96.06%，并对九种设防模型保持超过64.84%的攻击成功率。

Conclusion: 现有的对抗性防御方法对于大面积且外观自然的对抗性衣物存在普遍的脆弱性，这提示需要开发新的防御策略来应对这种类型的威胁。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [136] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的字符级引导框架CharDiff，用于恢复和识别严重退化的车牌图像，在真实条件下显著提升了恢复质量和识别准确率。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅对车牌识别系统预处理重要，还影响证据价值、视觉清晰度及后续应用，现有方法在处理严重退化图像时效果有限。

Method: 提出CharDiff框架，利用外部分割和OCR模块提取字符级先验，并设计CHARM模块通过区域掩码实现字符引导注意力，确保每个字符的引导仅限于其所在区域。

Result: 在Roboflow-LP数据集上，相比最优基线模型，CharDiff在CER上实现了28%的相对降低，显著提升恢复质量和识别精度。

Conclusion: 结构化的字符级引导有效增强了扩散模型在实际场景中对车牌图像恢复与识别的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [137] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多模态大语言模型iDETEX，用于实现细粒度、可解释的图像质量评估，支持质量定位、感知和描述三个任务。


<details>
  <summary>Details</summary>
Motivation: 传统的图像质量评估主要依赖于标量评分，缺乏可解释性和人类对质量退化的直观理解，因此需要更详细、可解释的评估方法。

Method: 设计了任务特定的离线数据增强模块和数据混合策略，并结合在线增强策略，利用多源监督信号在ViDA-UGC基准上训练iDETEX模型。

Result: iDETEX在ViDA-UGC基准上实现了所有子任务的最先进性能，并在ICCV MIPI 2025挑战赛中排名第一。

Conclusion: iDETEX能够有效且鲁棒地提供准确、可解释的图像质量评估，推动了图像质量评估向更人性化、细粒度方向的发展。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [138] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出一种基于最近类均值（NCM）的后处理开集识别方法，通过比较特征与分类头输出的一致性，在野生动物分类中实现稳定优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有开集识别方法多需重新训练模型，且在不同数据集上表现不稳定，难以应对真实场景中的未知类别。

Method: 提出一种无需重训练的后处理方法，利用输入到其最近类均值（NCM）的距离构建概率分布，并与softmax概率对比以衡量特征与分类结果的一致性，从而识别未知类。

Result: 在非洲和瑞典动物数据集上分别取得93.41和95.35的AUROC，性能稳定且位居前列。

Conclusion: 该方法无需微调即可有效提升闭集训练模型的开集识别能力，具有良好的泛化性和应用潜力。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [139] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 本文提出了一种新的事件到视频重建框架Semantic-E2VID，通过引入跨模态特征对齐模块和语义感知特征融合块，利用帧基础视觉模型SAM的语义信息增强事件相机数据的语义恢复能力，并设计了新的语义感知感知监督方法，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然具有低延迟、高动态范围等优点，但其仅捕捉亮度变化，缺乏静态场景的语义信息，导致现有事件到视频重建方法在语义恢复方面表现不足。因此，需要一种能有效引入并利用语义信息的新方法来提升重建效果。

Method: 提出Semantic-E2VID框架，包含三个关键组件：1）跨模态特征对齐（CFA）模块，将基于帧的视觉基础模型SAM的语义知识迁移到事件编码器；2）语义感知特征融合（SFF）模块，融合语义信息以构建富含语义的事件表示；3）语义感知感知监督机制，利用SAM生成的类别标签指导语义细节重建。

Result: 在多个基准测试上，Semantic-E2VID显著优于当前最先进的事件到视频重建方法，实验表明该方法在图像质量和语义信息恢复方面均有明显提升。

Conclusion: Semantic-E2VID成功地将帧模态中的语义知识引入事件到视频重建过程，有效弥补了事件数据中语义缺失的问题，为未来事件相机在高层次视觉任务中的应用提供了新思路。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [140] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: 本文提出了UltraCUA，一种结合GUI操作与高级程序化工具调用的混合动作基础模型，显著提升了计算机使用代理在任务成功率和执行效率方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态计算机使用代理依赖于低级的GUI操作（如点击、输入），容易因视觉定位错误和长操作链导致级联失败；而其他智能体已广泛使用API等程序化接口，因此需要弥合这一差距。

Method: 提出四部分方法：(1) 从软件文档、开源代码和生成代码中自动扩展程序化工具的管道；(2) 合成超过17,000个可验证的真实场景任务的数据引擎；(3) 包含低级GUI操作和高级程序调用的大规模高质量混合动作轨迹数据集；(4) 结合监督微调与在线强化学习的两阶段训练流程。

Result: 在OSWorld上，UltraCUA的7B和32B模型相比基线平均提升22%，步数减少11%；在跨域的WindowsAgentArena上达到21.7%的成功率，优于在Windows数据上训练的基线模型。

Conclusion: 混合动作机制能有效减少错误传播，同时保持高效执行，为计算机使用代理提供了更强大且鲁棒的解决方案。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [141] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 本文提出Glyph框架，通过将长文本渲染为图像并利用视觉语言模型处理，实现3-4倍的token压缩，显著降低计算和内存开销，同时保持与主流大语言模型相当的准确性，并加速推理和训练过程。


<details>
  <summary>Details</summary>
Motivation: 扩展基于token的上下文窗口到百万级别带来高昂的计算和内存成本，限制了长上下文大语言模型的实际应用，因此需要一种更高效的方法来处理超长文本输入。

Method: 将长文本转换为图像输入视觉语言模型（VLM），并通过大语言模型驱动的遗传搜索算法优化视觉渲染配置，在语义保留与压缩率之间取得平衡。

Result: 实现了3-4倍的token压缩，在多个长上下文基准上性能接近Qwen3-8B等先进LLM，预填充和解码速度提升约4倍，SFT训练速度提升约2倍，且128K-context VLM可扩展至处理百万级token任务。

Conclusion: Glyph提供了一种可行的视觉化上下文扩展路径，有效缓解长上下文LLM的资源瓶颈，同时具备在文档理解等多模态任务中的实际应用潜力。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [142] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的流式视频理解方法，通过选择重要视觉token、循环处理历史信息和基于字幕的回答生成，在保持高效的同时实现先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLM在长视频流式场景下难以实时响应，需减少计算开销并保持理解能力。

Method: 1) 基于LLM注意力机制选择关键视觉token；2) 循环利用历史选中的token进行时序建模；3) 使用字幕生成轻量且准确的答案。

Result: 在流式视频基准上达到最先进性能，可丢弃约95%非关键视觉token且性能损失极小。

Conclusion: 该方法无需训练即可兼容标准Video-LLM，在效率与效果之间取得良好平衡，适用于长视频流式理解任务。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [143] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 本研究系统评估了合成人脸数据在面部识别中的可行性，发现最佳合成数据集（如VariFace和VIGFace）的识别准确率超过常用真实数据集，且能有效保护隐私、控制偏见，为面部识别研究提供了科学可行且符合伦理的替代方案。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统的高精度依赖于未经同意收集的大规模真实人脸数据，引发隐私侵犯和法律风险（如GDPR），亟需一种既能保护隐私又具备性能保证的替代方案。

Method: 通过系统性文献综述识别25个合成人脸数据集（2018-2025），并基于身份泄露防护、类内变异性、身份可分性、数据规模、伦理来源、偏见缓解和基准可靠性七个关键要求，对超过1000万张合成样本进行实验验证，同时在五个标准基准上比较性能。

Result: 表现最佳的合成数据集VariFace（95.67%）和VIGFace（94.91%）准确率超过CASIA-WebFace（94.70%）；公开可用的Vec2Face（93.52%）和CemiFace（93.22%）也接近真实数据集水平；合成数据在保持身份可分性的同时具备良好类内变异性，并可通过生成参数有效控制和缓解人口统计偏见。

Conclusion: 合成人脸数据不仅是面部识别研究中技术上可行的替代方案，更是满足伦理与法规要求的必要选择，标志着向隐私保护和负责任AI迈进的关键一步。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [144] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出一种基于面部表情的帕金森病（PD）严重程度诊断新方法，通过注意力机制融合多表情特征，并采用自适应类别平衡策略缓解类别不平衡问题，实验结果表明该方法性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部表情的PD诊断方法多依赖单一表情类型且忽视不同PD阶段的类别不平衡问题，同时多数方法仅限于二分类，缺乏对PD严重程度的精细诊断。

Method: 提出一种结合多种面部表情特征的注意力机制特征融合方法，并设计自适应类别平衡策略，根据类别分布和分类难度动态调整训练样本权重。

Result: 实验结果显示所提方法在PD严重程度诊断上表现优异，注意力融合与自适应平衡策略均有效提升模型性能。

Conclusion: 该方法有效提升了基于面部表情的PD严重程度诊断准确性，具有临床应用潜力。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [145] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoopTrans的新框架，用于弱监督下的可操作性定位，通过在内外视角图像之间双向传递知识，提升了复杂交互场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅从外部视角图像中提取可操作性知识并单向迁移到第一人称视角图像，限制了其在复杂交互场景中的应用。

Method: 提出LoopTrans框架，包含统一的跨模态定位和去噪知识蒸馏机制，实现内外视角之间的闭环知识迁移。

Result: 实验表明，LoopTrans在图像和视频基准上各项指标均取得一致提升，甚至能处理人体完全遮挡交互区域的挑战场景。

Conclusion: LoopTrans通过双向知识转移和创新的跨模态机制，显著提升了弱监督可操作性定位的性能和适用性。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [146] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的监测系统，利用YOLOv11和BoT-SORT实现马匹和人员的自动检测与跟踪，结合轨迹和空间关系推断事件状态，支持马厩行为实时监控。


<details>
  <summary>Details</summary>
Motivation: 监测 stall 中马匹的行为对于早期发现健康和福利问题至关重要，但传统方法费时费力，亟需自动化解决方案。

Method: 采用YOLOv11进行目标检测，BoT-SORT实现多目标跟踪，利用对象轨迹和空间关系推断事件状态，并借助CLIP和GroundingDINO构建标注数据集。

Result: 系统能区分五种事件类型，考虑了摄像头盲区，在马相关事件上表现可靠，但因数据不足在人员检测上存在局限。

Conclusion: 该研究为马场提供了可行的实时行为监测基础，对动物福利和马厩管理具有积极意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [147] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DeepDetect的智能、一体化密集关键点检测器，通过融合多种传统检测器和边缘检测器生成的真值掩码，并利用轻量级ESPNet模型进行训练，在关键点密度、重复性和匹配数量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器对光照变化敏感、关键点密度和重复性低、适应性差且缺乏语义理解，难以在复杂场景中稳定工作。

Method: 融合7种关键点检测器和2种边缘检测器的输出生成真值掩码，使用ESPNet模型以这些掩码为标签进行训练，实现语义感知的密集关键点检测。

Result: 在Oxford Affine Covariant Regions数据集上，DeepDetect的关键点平均密度达到0.5143，平均重复性达0.9582，正确匹配数达59,003，均优于其他检测器。

Conclusion: DeepDetect通过深度学习融合多源视觉线索，实现了高密度、高重复性和强鲁棒性的关键点检测，适用于多样化和视觉退化场景。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [148] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动矢量生成密集的亚像素对应关系和短轨迹，提出一种在压缩域中高效且资源友好的前端方法，适用于SfM等视觉任务。


<details>
  <summary>Details</summary>
Motivation: 探索视频压缩域中的运动信息，以低计算成本实现高效的特征匹配和三维重建。

Method: 重用AV1编码中的运动矢量生成密集亚像素对应点，并通过余弦一致性过滤短轨迹，作为SfM等任务的前端处理。

Result: 在117帧视频片段上，该方法成功注册所有图像并重建0.46-0.62百万个点，重投影误差为0.51-0.53像素；相比SIFT更省CPU且匹配更密集。

Conclusion: 压缩域对应关系是一种实用、资源高效的前端方案，具备良好的可扩展性，适合集成到完整的三维重建流程中。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [149] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文提出了一种用于夜间图像去雨的新型高质量基准数据集HQ-NightRain和一种基于颜色空间变换网络（CST-Net）的方法，通过可学习的颜色空间转换器和隐式光照引导机制有效去除复杂夜景中的雨迹。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨由于夜间场景的复杂性和缺乏高质量数据集而面临巨大挑战，现有方法难以处理雨与光照的耦合效应。

Method: 提出了一个可学习的颜色空间转换器（CSC），在Y通道中更好地促进去雨，并引入隐式光照引导来捕捉光照信息以指导夜间去雨过程。

Result: 实验表明所构建的数据集具有更高的真实感和和谐性，所提方法在去除复杂夜间雨痕方面表现优异。

Conclusion: HQ-NightRain数据集和CST-Net方法有效提升了夜间图像去雨的性能，为后续研究提供了有价值的资源和思路。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [150] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文提出了一种针对稀疏视角3D高斯点阵（3DGS）的改进初始化方法，通过频率感知SfM、3DGS自初始化和点云正则化，显著提升了新视角渲染质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3DGS容易在训练视图上过拟合，导致新视角渲染出现模糊等伪影。现有方法依赖于改进初始化或添加训练时正则化，但作者发现初始化是决定性能的关键因素。

Method: 1) 频率感知SfM：通过低频视图增强和宽松的多视图对应关系改善低纹理区域覆盖；2) 3DGS自初始化：利用光度监督生成额外的高斯中心以补偿SfM稀疏区域；3) 点云正则化：通过几何/可见性先验保证多视图一致性和均匀空间分布。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视角设置下 consistently 提升了渲染性能，优于现有初始化策略。

Conclusion: 初始化是稀疏视角3DGS性能的决定性因素，所提方法通过系统性增强SfM输出，提供了更完整、可靠的点云初始化，显著改善了渲染效果。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [151] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了SparseWorld，一种基于稀疏动态查询的新型4D占据世界模型，通过范围自适应感知和状态条件预测模块实现灵活、自适应且高效的环境建模，在感知、预测和规划任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有占据世界模型依赖静态固定嵌入或网格，感知灵活性受限，且基于网格的“原位分类”与真实场景的动态连续性存在潜在不匹配。

Method: 提出SparseWorld，包含范围自适应感知模块（查询由自车状态调制并融合时空关联）和状态条件预测模块（用回归引导替代分类预测），并设计了时序感知自调度训练策略。

Result: 在多个任务上实现了最先进性能，可视化和消融实验验证了模型在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询机制有效提升了占据世界模型对动态连续环境的建模能力，具有更强的灵活性和实际应用潜力。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [152] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AutoSOD的端到端无监督显著性目标检测方法，通过改进原型最优传输（POTNet）生成高质量伪掩码，在无需像素级标注的情况下显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 希望在没有像素级标签的情况下，使显著性目标检测达到接近全监督模型的精度，前提是能够生成可靠的伪掩码。

Method: 提出POTNet，采用熵引导的双聚类头：高熵像素使用谱聚类，低熵像素使用k-means，并通过最优传输对齐两个原型集；利用生成的伪掩码监督MaskFormer风格的编码器-解码器，构建端到端的AutoSOD框架。

Result: 在五个基准上实验表明，AutoSOD在F-measure指标上比现有无监督方法最高提升26%，弱监督方法最高提升36%，显著缩小了与全监督模型的差距。

Conclusion: AutoSOD通过分裂-融合-传输的设计，在单次前向传播中生成更清晰、部分感知的伪掩码，实现了高效准确的无监督显著性检测，无需离线投票且优于现有方法。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [153] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出一种基于评分标准引导的伪标签提示框架，用于零样本视频摘要，通过少量真实标注生成高质量伪标签，并利用结构化评分标准提升大语言模型在SumMe和TVSum数据集上的表现，接近有监督方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖大量标注、泛化能力差，无监督方法难以捕捉高层语义，零样本方法对提示模板敏感且需数据集特定归一化，因此需要更稳定、通用且可解释的零样本视频摘要方法。

Method: 设计伪标签生成机制，将少量真实标注转化为高置信度伪标签，构建数据集自适应的结构化评分标准；推理时，首尾片段仅基于自身描述打分，中间片段结合相邻场景上下文评估叙事连贯性与冗余性，实现无需微调的上下文感知提示。

Result: 在SumMe和TVSum数据集上分别取得57.58和63.05的F1分数，优于无监督和先前零样本基线方法，接近有监督方法性能。

Conclusion: 评分标准引导的伪标签框架有效提升了大语言模型在零样本视频摘要中的稳定性与可解释性，为视频摘要提供了一种通用、无需训练的新范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [154] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种用于大规模视频生成模型的高效训练框架，优化了数据处理、模型架构、训练策略和基础设施，显著提升了训练效率和性能。所提出的MUG-V 10B模型在电商视频生成任务上优于现有开源模型，并首次公开了基于Megatron-Core的完整训练代码和推理流程。


<details>
  <summary>Details</summary>
Motivation: 训练大规模视频生成模型面临跨模态对齐、长序列和复杂时空依赖等挑战，导致资源消耗大、效率低，亟需系统性优化框架。

Method: 从数据处理、模型架构、训练策略和基础设施四个方面进行优化，包括数据预处理、视频压缩、参数扩展、课程式预训练和对齐导向的后训练，并基于Megatron-Core实现高效分布式训练。

Result: MUG-V 10B模型在整体性能上达到当前最先进水平，在电商视频生成任务中超越主流开源基线模型，并实现了高效的多节点线性扩展。

Conclusion: 该工作不仅提升了大规模视频生成模型的训练效率与性能，还通过开源完整技术栈推动了该领域的开放研究和应用发展。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [155] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出MambaX-Net，一种半监督双扫描3D分割网络，用于前列腺癌主动监测中的纵向MRI分析，利用前一时相的MRI和分割掩码提升当前时相的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型多基于单时间点且依赖专家标注的数据，难以适用于缺少专家标签的多时间点主动监测场景，需开发适应纵向分析的分割方法。

Method: 设计MambaX-Net，包含Mamba增强的交叉注意力模块以捕捉时空依赖，以及形状提取模块编码先前分割掩码；采用基于预训练nnU-Net生成伪标签的半监督自训练策略。

Result: 在纵向主动监测数据集上验证，MambaX-Net显著优于U-Net和Transformer类模型，即使在标注有限且噪声较多的情况下仍实现更优的前列腺区域分割效果。

Conclusion: MambaX-Net有效解决了主动监测中多时间点分割与标注稀缺的挑战，具备临床应用潜力，可推动自动化前列腺癌监测的发展。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [156] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: 提出WP-CrackNet，一种仅使用图像级标签进行端到端弱监督道路裂缝检测的方法，通过分类器、重建器和检测器的协同学习及PAAM和CECCM模块提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵像素级标注的依赖，实现高效可扩展的道路裂缝检测。

Method: 采用弱监督学习框架，结合类激活图、对抗训练、路径感知注意力模块（PAAM）和中心增强CAM一致性模块（CECCM），利用图像级标签生成高质量伪标签用于像素级检测。

Result: 在自建的三个图像级标注数据集上实验表明，WP-CrackNet性能媲美全监督方法，并显著优于现有弱监督方法。

Conclusion: WP-CrackNet有效实现了无需像素级标注的高精度道路裂缝检测，推动了智能城市基础设施巡检的可扩展性。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [157] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出PAGE-4D，一种扩展自VGGT的前馈模型，用于动态场景下的4D重建，实现相机位姿估计、深度预测和点云重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D前馈模型在静态数据集上训练，难以应对包含运动人体或可变形物体等复杂动态元素的真实场景。

Method: 通过引入动态感知聚合模块，预测动态感知掩码，分离静态与动态信息，在位姿估计中抑制动态区域，在几何重建中增强动态建模。

Result: 在动态场景下，PAGE-4D在相机位姿估计、单目/视频深度估计和稠密点云重建任务上均优于原始VGGT。

Conclusion: PAGE-4D有效解决了多任务4D重建中位姿估计与几何重建对动态区域处理的冲突，实现了无需后处理的统一前馈动态场景建模。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [158] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 本文提出了首个水下伪装实例分割数据集UCIS4K，并基于Segment Anything Model设计了UCIS-SAM网络，包含通道平衡优化、频域真值融合和多尺度特征频率聚合模块，显著提升了水下伪装物体的分割性能。


<details>
  <summary>Details</summary>
Motivation: 由于水下环境存在颜色失真、低对比度和模糊等问题，且现有方法主要基于陆地数据集训练，难以有效应对水下伪装实例分割的挑战。因此，亟需专门针对水下环境的数据集和模型来提升分割精度。

Method: 提出UCIS-SAM网络，包含三个关键模块：通道平衡优化模块（CBOM）增强水下特征学习；频域真值融合模块（FDTIM）突出物体本质特征并抑制伪装干扰；多尺度特征频率聚合模块（MFFAM）强化低对比度边缘信息，实现更精确的分割。

Result: 在新构建的UCIS4K数据集及公开基准上的实验表明，所提UCIS-SAM方法优于当前最先进的伪装实例分割方法。

Conclusion: UCIS4K数据集和UCIS-SAM模型为水下伪装实例分割提供了有效的解决方案，推动了水下视觉任务的发展。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [159] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种名为ShapeCraft的多智能体框架，用于从自然语言生成结构化、可交互的3D资产，通过图结构的程序化表示（GPS）提升语义理解和几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D生成方法常产生非结构化网格且缺乏交互性，难以融入艺术创作流程，因此需要更结构化和可编辑的解决方案。

Method: 提出图结构的程序化形状（GPS）表示，利用LLM代理分层解析用户输入，逐步优化程序化建模与纹理绘制，生成结构化的3D资产。

Result: 实验表明，ShapeCraft在几何准确性和语义丰富性方面优于现有的基于LLM的方法，并支持动画和用户自定义编辑。

Conclusion: ShapeCraft能有效生成高质量、可交互的3D资产，具有在创意设计和交互应用中广泛使用的潜力。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [160] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出一种基于机器学习的框架，利用真实无人机扫描点云和BIM生成的合成数据，实现3D点云中基础设施组件的自动分割，提高了精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统手动标注3D模型中的结构组件耗时且易出错，难以满足高效结构健康监测的需求。

Method: 结合真实无人机拍摄的点云数据与建筑信息模型（BIM）生成的合成数据，构建机器学习框架进行自动化3D点云分割。

Result: 在铁路轨道数据集上验证，能高精度识别钢轨和轨枕等主要部件；使用小规模真实数据结合BIM数据可显著缩短训练时间并保持良好准确率。

Conclusion: 该方法提升了3D基础设施模型分割的精度与效率，推动了无人机与BIM技术在结构健康监测中的融合应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [161] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是一个用于全频谱图像无监督异常检测的统一框架，通过五个简单元素的协调，在多类别、多模态和少样本等多样化任务中实现了卓越性能，显著缩小了多类模型与先进单类模型之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的多类异常检测模型性能远低于最先进的单类模型，且当前方法多针对特定场景设计，缺乏通用性，导致部署困难。因此需要一个能够跨模态、跨任务统一适用的高效框架。

Method: 基于“少即是多”的理念，在标准重建框架中精心组织五个简单元素，构建无需修改即可自然扩展到多种任务的极简主义方法。

Result: 在12个UAD基准上验证了Dinomaly2的优越性：在MVTec-AD和VisA上分别达到99.9%和99.3%的图像级AUROC；在多视角、多模态和少样本设置下（仅用8个正常样本）也表现SOTA，如少样本设置下在MVTec-AD和VisA上分别取得98.7%和97.4% I-AUROC。

Conclusion: Dinomaly2凭借极简设计、可扩展性和广泛适用性，成为面向真实世界全频谱异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [162] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 本文提出了一个名为CaMiT的新数据集，用于研究细粒度视觉识别和生成中的时间适应问题，涵盖2005-2023年间汽车模型的演变，并引入时间增量学习设置以提升跨时间的模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: AI系统需要适应随时间变化的视觉环境，尤其是在对象外观随时间演化的领域，现有模型在跨时间泛化方面表现不佳。

Method: 构建了包含标注和未标注图像的大规模数据集CaMiT，提出时间增量分类设置，评估时间增量预训练和分类器学习两种策略，并探索利用时间元数据进行时间感知图像生成。

Result: 基于领域内数据的静态预训练已具竞争力且更高效，但跨年测试性能下降；两种增量策略均提升了时间鲁棒性，时间感知生成能产生更真实的输出。

Conclusion: CaMiT为研究细粒度视觉任务中的时间演化提供了有效基准，时间增量学习有助于提升模型对新兴、演化和消失类别的适应能力。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [163] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 本文提出了一种名为DINO-CV的自监督分割框架，利用高分辨率LiDAR衍生的数字高程模型（DEM）实现对植被覆盖下低矮干石墙的自动制图，有效缓解了数据稀缺和视觉遮挡问题，在澳大利亚Budj Bim世界遗产地实现了68.6%的mIoU性能。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要的遗产与环境价值，但在植被覆盖和人工标注成本高的情况下难以全面识别，亟需一种可扩展、少依赖标注数据的自动化制图方法。

Method: 提出DINO-CV框架，采用基于知识蒸馏的自监督跨视角预训练策略，利用多种DEM衍生数据学习不变的几何与视觉特征表示，并兼容ResNet、Wide ResNet和Vision Transformer等多种主干网络。

Result: 在Budj Bim地区成功识别出澳大利亚最密集的殖民时期干石墙群之一；测试区域平均交并比（mIoU）达到68.6%，仅使用10%标注数据微调后仍保持63.8%的mIoU。

Conclusion: DINO-CV展示了利用自监督学习结合高分辨率DEM衍生物在标注稀缺、植被覆盖和文化遗产丰富环境中实现高效干石墙自动制图的可行性与潜力。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [164] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 本文比较了两种节俭的联邦学习方法在暴力检测中的应用：视觉-语言模型的零样本与微调，以及紧凑3D卷积神经网络的个性化训练。研究发现两者准确率均超90%，CNN3D在能效和性能上略优，而VLM更擅长上下文推理。提出轻量CNN用于常规分类、VLM选择性激活的混合模式，强调能源效率与环境影响，为负责任的AI监控系统提供可复现基线。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境下实现高效、节能且环保的暴力检测，解决现实非独立同分布（non-IID）数据下的模型部署挑战，并填补LoRA微调视觉语言模型与个性化CNN在该领域对比研究的空白。

Method: 采用两种策略：一是对LLaVA-7B等视觉-语言模型进行零样本和联邦微调（使用LoRA）；二是个性化训练一个6580万参数的紧凑3D CNN。在非IID设置下评估准确性、校准性和能耗表现，并量化训练与推理过程中的能源消耗与CO₂排放。

Result: 两种方法准确率均超过90%。CNN3D在ROC AUC和log loss上略优于LoRA微调的VLM，且能耗更低；VLM在上下文理解和多模态推理方面更具优势。研究还揭示了不同模型在可持续性方面的权衡。

Conclusion: 提出一种混合框架：日常场景使用轻量级CNN进行高效分类，在复杂或需描述性分析时选择性启用VLM。该研究为资源感知、环境友好型视频监控AI提供了可复现的基准，并推动向实时、多模态、全生命周期感知系统的演进。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [165] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 本文提出了一种名为4DSegStreamer的新型框架，用于在流式设置中进行4D全景分割，通过双线程系统实现高效处理，并在多个数据集上验证了其在复杂场景中对动态物体的准确预测能力。


<details>
  <summary>Details</summary>
Motivation: 在高度动态环境中（如密集人群疏散和复杂场景下的自动驾驶），需要在有限时间内实现实时、细粒度的感知，因此研究流式4D全景分割具有重要意义。

Method: 提出了一个包含预测线程和推理线程的双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过对齐最新内存并补偿自运动和动态物体运动来确保及时预测。该框架可无缝集成到现有3D/4D分割方法中。

Result: 在HOI4D、SemanticKITTI和nuScenes数据集上的实验表明，该方法在高帧率条件下表现出优越的鲁棒性，并能准确预测复杂场景中的动态物体。

Conclusion: 4DSegStreamer为流式4D全景分割提供了一个通用且高效的解决方案，显著提升了实时性和对动态环境的适应能力。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [166] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 本文提出了PICABench，一个用于评估图像编辑中物理真实性的基准，涵盖光学、力学和状态转换等八个子维度，并提出PICAEval评估协议和PICA-100K训练数据集，以推动图像编辑向物理一致的真实感发展。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型和基准主要关注指令完成，而忽视了编辑后图像的物理效果（如阴影、反射等），缺乏对物理真实性的系统评估。

Method: 提出PICABench基准和PICAEval评估协议，结合VLM-as-a-judge与人类标注；并通过从视频中学习物理规律构建PICA-100K数据集探索解决方案。

Result: 对主流模型的评估表明，当前图像编辑在物理真实性方面仍有很大提升空间，现有方法距离物理一致性仍有显著差距。

Conclusion: 物理真实性是图像编辑的关键挑战，PICABench和PICA-100K为未来实现物理一致的图像编辑提供了基础和方向。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [167] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出一种基于混合专家的医学图像分割基础模型IC-MoE，通过自适应投票策略和语义引导对比学习增强高层特征表示并保持预训练权重结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有自然图像分割模型在医学图像上的微调方法存在高层特征表示不足和破坏预训练权重结构完整性的问题。

Method: 构建基本专家、语义专家和自适应专家，并采用像素概率自适应投票策略进行专家选择与融合；提出语义引导的对比学习方法以解决弱监督问题。

Result: 在三个公开医学图像分割数据集上实验表明，IC-MoE优于其他SOTA模型，具有更强的高层特征表示能力和更好的泛化性。

Conclusion: IC-MoE有效增强了医学图像分割基础模型的高层特征表达能力，同时保持了预训练权重的结构完整性，具备良好的通用性和应用潜力。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [168] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的多语言文本到图像人物检索框架Bi-IRRA，通过双向隐式关系推理和多维全局对齐模块，在新构建的多语言基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注英文场景且难以处理模态异质性，忽略了细粒度跨模态差异或依赖先验信息进行局部对齐，限制了在多语言环境中的应用。

Method: 构建了一个多语言TIPR基准，并提出Bi-IRRA框架，包含双向隐式关系推理模块（用于掩码图像和文本的双向预测）和多维度全局对齐模块，以实现跨语言和跨模态的对齐学习。

Result: 在所有多语言TIPR数据集上均取得了新的最先进结果。

Conclusion: Bi-IRRA有效缓解了模态异质性和多语言支持问题，为文本到图像人物检索提供了更通用的解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [169] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 本文提出了OP3Det，一种无需文本提示的开放世界3D检测器，通过融合2D语义先验和3D几何先验以及跨模态专家混合机制，实现了对3D场景中已知和未知对象的通用检测，在开放世界和闭集设置下均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测器在开放世界场景中泛化能力不足，且现有开放词汇模型面临词汇扩展和语义重叠问题，难以有效发现新类别对象。

Method: 提出OP3Det，利用2D基础模型的强泛化和零样本能力，结合2D语义与3D几何先验生成类无关建议，并通过点云与RGB图像的跨模态专家混合机制动态融合单模态与多模态特征，以学习广义3D对象性。

Result: 实验表明，OP3Det在AR指标上比现有开放世界3D检测器最高提升16.0%，相比闭集检测器也有13.5%的性能提升。

Conclusion: OP3Det有效实现了开放世界下的通用3D对象发现，无需依赖手工设计的文本提示，在检测已知和未知对象方面展现出卓越的泛化能力和性能优势。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [170] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 本文提出了一种称为广义对抗求解器（Generalized Adversarial Solver）的新方法，通过简化常微分方程（ODE）采样器的参数化并结合对抗训练，有效提升了扩散模型在少步数生成中的细节保真度和生成质量，且无需复杂的训练技巧。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量高，但采样过程计算开销大。现有加速方法依赖复杂训练技术，且难以保持细粒度细节，因此需要一种更简洁高效、能保留细节的加速采样方法。

Method: 提出广义求解器（Generalized Solver），采用简化的ODE采样器参数化方式，并结合原始蒸馏损失与对抗训练，以减少伪影并增强细节保真度。

Result: 所提方法在相同资源约束下，优于现有的求解器训练方法，在生成速度和质量之间实现了更好的平衡，尤其在细节还原方面表现突出。

Conclusion: 广义对抗求解器是一种无需复杂训练技巧即可提升少步数扩散模型生成质量的有效方法，结合对抗训练显著改善了细节表现，为扩散模型的高效采样提供了新思路。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [171] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: 本文提出了SnapViT，一种针对预训练视觉Transformer的单次网络近似结构化剪枝方法，支持在连续计算预算下进行弹性推理，无需重训练或标签，且能在几分钟内高效生成适用于不同计算资源的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型虽然性能优异，但尺寸固定，难以满足实际部署中的多样化计算资源需求，因此需要一种灵活、高效的剪枝方法以实现弹性推理。

Method: 提出SnapViT，结合梯度信息与跨网络结构相关性，通过进化算法近似Hessian非对角结构，并设计自监督重要性评分机制，在无标签数据的情况下实现单次剪枝，支持模型在不同稀疏度下灵活调整。

Result: 在DINO、SigLIPv2、DeIT和AugReg等模型上实验表明，SnapViT在多种稀疏度下均优于现有最先进方法，仅用单块A100 GPU不到五分钟即可生成弹性模型。

Conclusion: SnapViT为预训练视觉Transformer提供了一种高效、快速、无需重训练的结构化剪枝方案，实现了计算预算连续可调的弹性推理，具有良好的通用性和部署灵活性。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [172] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 本文提出了一种基于手绘图像分块处理和集成学习的帕金森病检测新方法，有效解决了数据集不足和对未见患者数据鲁棒性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病检测方法存在数据集不足和对未见患者数据鲁棒性差两大问题，限制了其实际应用。

Method: 采用两阶段方法：第一阶段按绘制类型（圆形、蜿蜒线、螺旋）分类；第二阶段将图像划分为2x2块，分别提取特征并检测疾病，最后通过集成方法融合各块的决策结果。

Result: 在NewHandPD数据集上，对已见患者达到97.08%准确率，对未见患者达到94.91%准确率，性能下降仅2.17个百分点，优于先前工作（下降4.76个百分点）。

Conclusion: 所提方法通过分块策略和集成学习显著提升了模型在未见患者上的鲁棒性，为帕金森病的早期检测提供了更可靠的解决方案。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [173] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 本研究提出了一种用于分析循环血细胞簇（CCC）图像的新型计算框架，结合YOLOv11模型和多通道荧光染色叠加技术，实现了高精度的细胞簇分类与细胞类型识别。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法主要针对单细胞流式细胞术图像分析，缺乏对包含多种细胞类型的不规则、异质性细胞簇的自动分析工具。

Method: 采用两步分析策略：首先使用微调后的YOLOv11模型对图像进行细胞簇与非细胞簇分类；然后通过叠加多通道荧光染色区域与簇轮廓来识别细胞类型。

Result: 在细胞簇分类和表型识别方面均达到95%以上的准确率，优于传统CNN和Vision Transformer模型。

Conclusion: 该自动化框架有效整合亮场与荧光数据，可精确分析流式细胞术中的细胞簇图像，初步应用于血细胞，未来有望扩展至免疫细胞和肿瘤细胞簇的研究。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [174] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: 本文提出了RaindropGS，一个用于评估雨滴条件下3D高斯点阵（3DGS）完整流程的基准，解决了真实场景中雨滴导致的相机位姿估计不准和初始化困难等问题。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在雨滴污染下因遮挡和光学畸变导致重建质量下降，且合成数据与真实雨滴存在域差距，难以泛化。

Method: 构建包含真实雨滴数据的数据集，涵盖三种对焦状态的图像，并设计包含数据准备、处理和雨滴感知评估的完整流程。

Result: 实验揭示了当前3DGS方法在无约束雨滴图像下的性能局限，分析了对焦位置、位姿与点云初始化对重建的影响。

Conclusion: RaindropGS为开发更鲁棒的雨滴环境下3DGS方法提供了明确方向和评估基础。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [175] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了MT-Video-Bench，一个用于评估多模态大语言模型在多轮视频对话中理解能力的综合基准，填补了现有单轮问答评测的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型评测基准主要局限于单轮问答，无法反映真实场景中的多轮复杂交互需求。

Method: 构建了一个包含987个精心策划的多轮对话的数据集，涵盖多个领域，评估模型在感知和交互方面的六项核心能力。

Result: 在多个开源和闭源MLLM上进行了广泛评估，揭示了它们在处理多轮视频对话时的性能差异和局限性。

Conclusion: MT-Video-Bench能有效评估MLLM在真实应用场景（如互动体育分析和视频教学）中的表现，推动未来研究发展。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [176] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 本研究探讨了离线签名验证中的特征学习策略，旨在提升跨数据集的泛化能力。使用三个公开数据集（CEDAR、ICDAR 和 GPDS Synthetic）评估了基于原始图像和 shell 预处理的两种方法。结果表明，原始图像模型整体性能更优，而 shell 方法具有进一步改进的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在离线签名验证中难以跨数据集泛化，受书写风格和采集方式差异影响严重，亟需提升模型鲁棒性。

Method: 采用两种实验流程：直接使用原始签名图像训练模型，以及引入 shell 预处理技术进行特征增强，并在多个公开数据集上进行跨数据集验证。

Result: 原始图像模型在各基准测试中表现更优，具备更强的跨数据集泛化能力；shell 预处理模型虽未超越前者，但展现出改进潜力。

Conclusion: 提升跨数据集泛化能力是签名验证的关键，原始图像输入在当前更具优势，而预处理辅助方法值得进一步探索以实现鲁棒的跨域验证。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [177] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 研究基于扩散变换器（DiT）的图像到视频模型在大规模视频数据集训练下生成逼真行人运动模式的能力。


<details>
  <summary>Details</summary>
Motivation: 探索当前高性能I2V模型是否能准确模拟拥挤公共场景中的行人移动行为。

Method: 通过从行人轨迹基准中提取关键帧来条件化I2V模型，并使用行人动态的定量指标评估其轨迹预测性能。

Result: 验证了这些模型在生成真实行人运动方面的潜力，表现出一定的世界建模能力。

Conclusion: 基于DiT的I2V模型在适当条件下可有效生成符合真实行人动态的视频内容。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [178] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种无需训练、与描述符无关的多参考视觉位置识别方法，通过矩阵分解联合建模多个参考描述符，实现基于投影的残差匹配，在多种数据上显著提升召回率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理外观和视角变化时计算成本高或依赖启发式策略，性能提升有限，需要一种轻量且泛化能力强的多参考VPR方法。

Method: 提出一种训练免费、描述符无关的方法，通过矩阵分解将多个参考描述符分解为基础表示，进行基于投影的残差匹配，并引入SotonMV作为多视角VPR的结构化基准。

Result: 在多外观数据上，Recall@1比单参考方法最高提升约18%，在非结构化数据上超过多参考基线约5%，表现出强泛化能力和轻量化优势。

Conclusion: 所提方法在不增加训练开销的前提下，有效提升了多参考VPR在不同外观和视角变化下的性能，具有良好的通用性和应用潜力。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [179] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出一种基于双编码器注意力机制的皮肤病变分类框架，结合分割结果和临床元数据，提升分类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测至关重要，但现有深度学习模型因高类内差异和低类间差异难以准确分类，且缺乏可解释性，限制了临床应用。

Method: 采用带有双重注意力门（DAG）和空洞空间金字塔池化（ASPP）的Deep-UNet进行病灶分割；分类阶段使用两个DenseNet201编码器，分别处理原始图像和分割结果，并通过多头交叉注意力融合特征；引入基于Transformer的模块整合患者年龄、性别、病灶位置等元数据。

Result: 在HAM10000、ISIC 2018和2019数据集上实现了最先进的分割性能，显著提升了分类准确率和平均AUC；通过Grad-CAM可视化验证模型关注病灶区域，具有更高的可解释性。

Conclusion: 结合精确病灶分割与临床元数据，并利用注意力机制进行特征融合，能够有效提升皮肤癌分类模型的准确性与可解释性，有助于增强临床信任。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [180] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA提出了一种高效的视觉语言模型推理新范式，通过在预填充和解码阶段解耦视觉稀疏性，实现显著加速同时保持甚至提升准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理高分辨率图像和长视频时面临视觉token数量增长导致的推理延迟问题，限制了其可扩展性。

Method: SparseVILA在预填充阶段剪枝冗余视觉token，在解码阶段仅检索与查询相关的token，并结合AWQ优化的推理流水线，实现跨阶段的稀疏性分布。

Result: 在长上下文视频任务中，预填充速度提升达4.0倍，解码速度快2.5倍，端到端速度提升2.6倍，同时在文档理解和推理任务上准确率提高。

Conclusion: SparseVILA通过解耦查询无关的剪枝和查询感知的检索，为高效多模态推理提供了无需训练、架构无关的新方向。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 本文提出了一种针对MM-DiT架构的新型注意力控制方法ConsistEdit，通过视觉专用注意力控制、掩码引导的预注意力融合以及对查询、键和值令牌的差异化操作，在图像和视频编辑中实现了更强的一致性和提示对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练-free注意力控制方法在编辑强度和源一致性之间难以平衡，尤其在多轮和视频编辑中误差累积严重，且多数方法仅支持全局一致性，限制了细粒度编辑能力。

Method: 基于对MM-DiT注意力机制的深入分析，提出ConsistEdit，引入视觉-only注意力控制、掩码引导的预注意力融合，以及对query、key、value令牌的差异化操作，以实现更精细和一致的编辑。

Result: 实验表明，ConsistEdit在多种图像和视频编辑任务中达到SOTA性能，支持所有推理步骤和注意力层的编辑，无需手工设计，具备良好的多轮、多区域及结构一致性可调编辑能力。

Conclusion: ConsistEdit是首个适用于MM-DiT的训练-free注意力编辑方法，显著提升了编辑的可靠性与一致性，为多轮、多区域和渐进式结构控制编辑提供了新可能。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [182] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究了量子自然语言处理（QNLP）在自然语言推理（NLI）任务中的应用，通过对比量子、混合与经典模型，在少样本设置下展示了量子模型在参数效率和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在自然语言处理中的潜力，特别是在低资源和结构敏感场景下，利用量子电路的组合性提升语义建模效率。

Method: 基于lambeq库和DisCoCat框架构建参数化量子电路，用于句子对的语义相关性和推理分类任务，并提出信息增益每参数（IGPP）指标评估学习效率；同时设计基于词簇的新型集群架构以提升泛化能力。

Result: 量子模型在显著更少的参数下达到与经典模型相当甚至更优的性能，在推理任务上优于随机初始化的Transformer，在相关性任务上测试误差更低，且每参数学习效率高出最多五个数量级。

Conclusion: 量子NLP模型在少样本、结构敏感的NLI任务中展现出高参数效率和良好性能，结合新提出的集群架构有望推动低资源场景下的实用化发展。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [183] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本研究提出了一种基于ChatGPT和Claude的多模型融合框架，通过结合图像与合成临床文本输入，在CheXpert数据集上提升了胸部X光诊断的可靠性。


<details>
  <summary>Details</summary>
Motivation: 为提高AI在放射学诊断中的可信度和临床实用性，减少诊断错误，探索多模态输入与多模型融合策略的有效性。

Method: 采用相似性共识方法（95%阈值）融合ChatGPT和Claude两个大语言模型，并利用MIMIC-CXR模板生成合成临床文本以支持多模态分析。

Result: 在仅图像条件下，Claude准确率为76.9%，ChatGPT为62.8%，共识融合达77.6%；加入合成文本后，ChatGPT提升至84%，Claude为76%，共识准确率高达91.3%。

Conclusion: 基于共识的融合策略结合多模态输入可显著提升诊断准确性，为降低AI辅助诊断误差提供了低计算开销的可行路径。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [184] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 本文提出了CorrectBench，一个用于评估大语言模型自校正方法（包括内在、外在和微调方法）在常识推理、数学推理和代码生成任务上有效性的基准。研究发现自校正能提升复杂推理任务的准确性，混合策略效果更优但效率降低，而推理专用模型（如DeepSeek-R1）在额外自校正下优化有限且耗时高；相比之下，简单的思维链（CoT）基线在准确性和效率上均具竞争力。结果强调了自校正在提升LLM推理能力方面的潜力，但也突显了效率优化的挑战，呼吁进一步研究推理能力与运行效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种大语言模型自校正方法被提出，但缺乏对这些方法的系统性评估，且LLMs是否真能有效自我纠正仍存疑。因此，亟需一个全面的基准来衡量不同自校正策略的有效性，并探讨其在提升推理性能中的实际作用。

Method: 构建了一个名为CorrectBench的评估基准，涵盖三种自校正策略（内在、外部、微调）和三大任务（常识推理、数学推理、代码生成），并在多个主流LLM上进行实验，比较不同方法在准确性、效率和可扩展性方面的表现。

Result: 1) 自校正方法能提升准确性，尤其在复杂推理任务上；2) 混合多种自校正策略可进一步提升性能，但牺牲效率；3) 专门的推理模型（如DeepSeek-R1）在额外自校正下改进有限且时间成本高；4) 简单的思维链（CoT）基线在准确率和效率方面表现优异，具有强竞争力。

Conclusion: 自校正确实有助于提升大语言模型的推理能力，但当前方法在效率方面面临显著挑战。未来的研究应聚焦于优化推理性能与计算效率之间的平衡，推动更高效、实用的自校正机制发展。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [185] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 本文提出了EvolveR框架，通过离线自蒸馏和在线交互的闭环经验生命周期，使大语言模型代理能够从自身经验中持续自我改进，在多跳问答任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代理在工具使用上表现良好，但缺乏从自身经验中系统学习和迭代优化问题解决策略的能力。

Method: 提出EvolveR框架，包含两个阶段：1）离线自蒸馏，将交互轨迹提炼为可复用的策略原则；2）在线交互，通过检索已提炼的原则指导决策，并积累新的行为轨迹；结合策略强化机制实现闭环自我提升。

Result: 在复杂的多跳问答基准上，EvolveR优于强基线代理方法，展现出更优的性能。

Conclusion: EvolveR为代理系统提供了一种从自身行为后果中学习的完整蓝图，推动了更自主、持续改进的智能系统的发展。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [186] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 该研究系统评估了六种大语言模型在五种提示策略下的文献筛选性能，发现模型与提示策略存在显著交互效应，推荐采用分阶段工作流以实现高效、低成本的自动化文献筛选。


<details>
  <summary>Details</summary>
Motivation: 为了提高系统性文献综述（SLR）筛选阶段的效率和准确性，探索不同提示策略与大语言模型的交互效应，寻找最优的自动化解决方案。

Method: 评估了GPT-4o、GPT-4o-mini等六种大语言模型在零样本、少样本、思维链（CoT）、CoT-少样本和自我反思五种提示类型下的表现，使用准确率、精确率、召回率和F1分数衡量其在相关性分类及六个二级任务中的性能，并进行成本效益分析。

Result: CoT-少样本提示在精确率和召回率之间取得最佳平衡；零样本提示最大化召回率，适用于高敏感性初筛；自我反思提示因过度包容和不稳定而表现不佳；GPT-4o和DeepSeek整体表现稳健，GPT-4o-mini在低成本下具备竞争力；结构化提示结合GPT-4o-mini可在小幅增加成本下获得理想F1分数。

Conclusion: 大语言模型在自动化文献筛选中具有不均衡但可观的潜力，通过合理的模型-提示组合与分阶段工作流（先用低成本模型初筛，仅将模糊案例交由高性能模型处理），可实现高效、经济的系统性文献审查。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [187] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一种灵活的合成测试框架，用于研究语言模型中统计规律性和事实关联之间的交互对泛化能力的影响，发现上下文多样性和结构对分布内和分布外事实推理具有关键影响，并揭示了嵌入层等组件在优化瓶颈中的作用。


<details>
  <summary>Details</summary>
Motivation: 缺乏对语言模型中统计规律性与事实关联交互影响的系统性分析，特别是其在不同多样性条件下的泛化行为。

Method: 构建一个包含通用标记的统计流和源-目标标记对的事实流的合成测试环境，通过控制流组成和多样性水平来独立调节上下文结构和多样性程度，并进行受控实验和模型组件干预。

Result: 发现较高的上下文多样性会延迟分布内准确率，但对分布外泛化的影响取决于上下文结构；某些情况下多样性促进非平凡的事实回忆，且最佳多样性水平依赖训练时长；同时识别出导致统计和事实泛化失败的不同结构，并定位到嵌入和解嵌层的优化瓶颈。

Conclusion: 上下文设计与多样性水平的相互作用显著影响语言模型的不同泛化方面，该合成框架为未来研究提供了可控制的实验平台。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [188] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本研究首次通过计算方法分析了2022至2025年Reddit上关于生成式AI的信任与不信任，发现信任与不信任总体平衡，技术性能和可用性是主要关注点，个人经验是影响态度的最常见原因。


<details>
  <summary>Details</summary>
Motivation: 现有对AI信任的研究多基于心理学和人机交互，缺乏大规模、纵向的计算方法来衡量公众对生成式AI和大语言模型的信任与不信任。

Method: 利用涵盖39个子版块、197,618篇帖子的多年Reddit数据集，结合众包标注与分类模型，进行可扩展的分析。

Result: 发现信任与不信任在时间上基本平衡，重大模型发布时出现波动；技术性能和可用性是主要维度，个人经验是最常见的态度形成原因；不同信任主体（如专家、伦理学者、普通用户）表现出不同的模式。

Conclusion: 研究提供了一种用于大规模信任分析的方法框架，并揭示了公众对生成式AI认知的动态演变，有助于负责任的AI治理与采纳。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [189] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了EgMM-Corpus，一个专注于埃及文化的多模态数据集，包含3000多张图像，覆盖地标、食物和民间传说等313个概念。数据经过人工验证以确保文化真实性和多模态一致性，并用于评估CLIP模型的零样本分类性能，结果显示出当前视觉-语言模型存在文化偏见，突显了该数据集在推动文化感知模型发展中的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于中东和非洲地区缺乏多样化的多模态文化数据集，现有视觉-语言模型难以准确理解特定文化内容，因此需要构建具有文化代表性的数据集以缓解模型的文化偏见。

Method: 设计并实施新的数据收集流程，采集超过3000张图像，涵盖313个埃及文化相关概念，并对每条数据进行人工验证以确保文化真实性与多模态一致性，最终构建EgMM-Corpus数据集。同时采用CLIP模型在其上进行零样本分类实验以评估性能。

Result: EgMM-Corpus包含3000多张图像和313个文化概念，经人工验证保证质量；在CLIP上的零样本分类中仅达到21.2% Top-1和36.4% Top-5准确率，表明现有模型在埃及文化理解上表现较差。

Conclusion: EgMM-Corpus是一个可靠且具有文化代表性的多模态数据集，可用于训练和评估文化感知的视觉-语言模型；当前主流模型存在显著文化偏见，亟需纳入多样化文化数据以提升全球适用性。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [190] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 本文探讨了语言模型（LMs）对语法知识的学习情况，提出并验证了关于语法、意义和字符串概率之间关系的三个预测，为利用概率研究LMs的结构知识提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 由于概率和语法性在语言学中是不同的概念，因此不清楚字符串概率能揭示LMs多少潜在的语法知识，本文旨在通过理论分析解决这一问题。

Method: 基于语料库数据生成过程的简单假设，提出了一个理论框架，并使用英语和中文的28万个句子对进行实证验证。

Result: 验证了三个预测：最小对内字符串概率的相关性；模型与人类在最小对内差异的一致性；语法正确与错误字符串在概率空间中的分离效果差。

Conclusion: 该分析为使用概率来了解LMs的结构知识提供了理论支持，并指出了未来在LM语法评估方面的研究方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [191] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本研究提出两种方法（多元化解码和模型引导）来提升语言模型在低资源设置下的多元化对齐，仅用50个标注样本即显著改善了模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练范式常假设每个问题只有一个最优答案，导致回应过于泛化且难以反映人类价值观的多样性与细微差异。

Method: 提出并结合使用多元化解码和模型引导方法，在低资源条件下实现语言模型对多元观点的更好对齐。

Result: 模型引导在多个高风险任务（如仇恨言论和虚假信息检测）中显著降低误报率，并在GlobalOpinionQA上提升了与人类价值观分布的对齐程度。

Conclusion: 语言模型可通过少量标注数据实现更好的多元化对齐，强调考虑多样化和细微人类价值观的重要性。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [192] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 本文提出了DiMo框架，通过四个具有不同推理模式的LLM代理之间的结构化辩论，提升大语言模型的性能与可解释性，尤其在数学任务上表现突出，并支持语义感知、Web原生的多代理协作。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但缺乏可解释的推理过程。为了增强模型的透明性和推理能力，需要一种能够模拟多样化思维模式并生成可审计推理链的方法。

Method: 提出Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)，由四个具备不同推理范式的LLM代理组成，通过迭代辩论机制共同探索多样化的认知路径，结合检索增强推理与结构化论证。

Result: 在六个基准测试中，DiMo在统一开源设置下优于单模型和辩论基线方法，尤其在数学任务上提升显著；生成语义标注、URL注释的证据链，支持下游系统检查与复用。

Conclusion: DiMo是一种语义感知、Web原生的多代理框架，有效结合人类-机器智能协作，提升了LLM的推理准确性与可解释性，具备在Web语料和知识图谱上扩展的潜力。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [193] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出Profile-to-PEFT框架，利用超网络将用户画像直接映射到适配器参数，实现高效、可扩展的个性化大语言模型，无需为每个用户单独训练。


<details>
  <summary>Details</summary>
Motivation: 现有个性化大模型方法（如OPPU）需为每个用户单独微调，计算开销大，难以实时更新，缺乏可扩展性。

Method: 设计一个端到端训练的超网络，将编码后的用户画像直接生成适配器参数（如LoRA），实现无需用户特定训练的即时适配。

Result: 在多个指标上优于基于提示的个性化和OPPU方法，部署时计算资源消耗更少，对未见用户、不同用户活跃度和嵌入骨干具有强泛化性和鲁棒性。

Conclusion: Profile-to-PEFT实现了高效、可扩展且适应性强的LLM个性化，适用于大规模应用。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [194] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型在后训练技术下对学习策略的意识、跨领域泛化能力以及内部推理与输出的一致性，发现强化学习训练的模型（如GRPO）在意识和泛化上优于SFT模型，但其推理过程与最终输出一致性较弱。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术的发展，大语言模型能通过生成规划标记处理复杂逻辑任务，但这些模型是否真正‘理解’其所学和所想仍不清楚。本文旨在探究模型对学习策略的自我意识及相关能力。

Method: 定义三个核心能力：对隐含策略的学习意识、跨领域泛化能力、推理路径与输出的一致性；在多个需不同策略的任务上进行实证评估，并比较SFT、DPO和GRPO三种后训练方法下的模型表现。

Result: RL训练的模型（尤其是GRPO）比SFT模型更具策略意识和跨任务泛化能力，但在推理轨迹与最终输出之间表现出较弱的一致性，该现象在GRPO中最为明显。

Conclusion: 尽管RL-based后训练提升了模型的策略意识和泛化能力，但其内部推理与输出之间的不一致提示当前模型可能缺乏真实‘思维’透明性，这对可解释性和可靠性提出了挑战。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [195] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个面向多智能体协作的验证感知规划框架，通过定义子任务验证函数来提升系统的鲁棒性、可解释性和协作可靠性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在复杂任务中面临规划、协调和验证方面的挑战，尤其是任务理解、输出格式或交接过程中的细微不一致常导致执行失败。

Method: 提出VeriMAP框架，其规划器将任务分解，建模子任务依赖关系，并以Python和自然语言编码规划器定义的传递标准作为子任务验证函数（VFs）。

Result: 在多个数据集上的实验表明，VeriMAP优于单智能体和多智能体基线方法，显著提升系统鲁棒性和可解释性。

Conclusion: 验证感知规划能有效支持多智能体系统中的可靠协作与迭代优化，且无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [196] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该研究探索了利用大语言模型（LLM）实时生成针对疫苗错误信息的有效反驳论点，结合分类器对反疫苗推文进行多标签分类，并通过多种评估方法验证了结构化微调和标签描述在提升反驳效果方面的优势。


<details>
  <summary>Details</summary>
Motivation: 应对社交媒体上广泛传播的疫苗错误信息，提高公众对疫苗的信任并促进高免疫率，亟需能够实时生成针对性反驳的技术手段。

Method: 采用不同的提示策略和微调方法优化大语言模型生成反制论点的能力，同时训练分类器将反疫苗言论按多标签（如副作用、疗效疑虑、政治影响等）分类，以实现更精准的情境化回应。

Result: 人类评估、基于LLM的判断和自动指标显示高度一致，结果表明引入标签描述和结构化微调显著提升了反驳内容的质量和相关性。

Conclusion: 结合多标签分类与结构化微调的大语言模型能有效生成针对疫苗错误信息的高质量反驳，为大规模应对健康 misinformation 提供可行方案。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [197] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 提出了一种基于自回归结构预测的端到端框架AASP，用于联合建模论点挖掘中的组件和关系，在多个基准上达到或接近最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将论点结构扁平化处理，难以有效建模论点组件与关系之间的复杂依赖，因此需要一种能联合建模并捕捉推理流的方法。

Method: 提出Autoregressive Argumentative Structure Prediction (AASP)框架，将论点结构建模为预定义的动作集合，利用条件预训练语言模型以自回归方式逐步构建结构。

Result: 在三个标准论点挖掘基准上进行了广泛实验，AASP在其中两个基准上取得了最先进的结果，在第三个基准上表现强劲。

Conclusion: AASP能够有效捕捉论点间的复杂依赖和推理流程，是论点挖掘任务中一种高效且强大的端到端解决方案。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [198] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文系统比较了基于提示和基于强化学习的查询增强方法，发现简单无训练的方法常能媲美或超越复杂的强化学习方法。受此启发，提出一种新的混合方法OPQE，通过生成最大化检索性能的伪文档来融合两种方法的优势，实验证明其效果优于单独使用提示或强化学习重写查询。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示和基于强化学习的查询增强方法缺乏在一致条件下的比较，且各自优缺点不明确，因此需要系统性评估并探索更有效的融合方法。

Method: 在多种检索任务（如证据查找、即席检索和工具检索）上对提示法和强化学习法进行统一实验对比，并提出新方法OPQE，利用LLM策略生成伪文档以优化检索性能。

Result: 实验表明，简单的无训练提示方法常与昂贵的强化学习方法性能相当甚至更优；新提出的OPQE方法在检索效果上优于单独使用提示或强化学习重写的方法。

Conclusion: 结合提示的生成灵活性和强化学习的目标优化，OPQE展示了协同方法在查询增强中的优越性，为信息检索中的查询改写提供了新方向。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [199] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究开发了BenCao，一个基于ChatGPT的多模态中医助手，通过自然语言指令微调整合结构化知识库、诊断数据和专家反馈，提升了中医领域大模型在诊断、草药识别和体质分类中的准确性与可解释性，并已在全球部署应用。


<details>
  <summary>Details</summary>
Motivation: 现有中医领域的大语言模型在多模态整合、可解释性和临床适用性方面存在不足，且中医依赖整体思维、隐性逻辑和多模态诊断线索，导致LLMs难以有效应用于中医实践。

Method: 基于ChatGPT构建BenCao系统，采用自然语言指令微调而非参数重训练，集成包含千余部经典与现代文献的知识库、基于场景的指令框架、思维链模拟机制及执业中医师反馈优化流程，并连接外部舌象分类和多模态数据库API以实现动态资源访问。

Result: 在单选题基准和多模态分类任务中，BenCao在诊断、草药识别和体质分类方面均优于通用及中医领域模型；系统已作为交互式应用上线OpenAI GPTs商店，截至2025年10月全球用户近1000人。

Conclusion: 通过自然语言指令微调和多模态整合，可有效构建符合中医推理特点的领域大模型，为生成式AI与传统医学融合提供了可行框架和可扩展的落地路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [200] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出一种轻量级且高效的方法，通过线性变换和引导向量提升大语言模型在心理健康评估中的表现，无需复杂计算，在相关性预测和抑郁筛查问卷填写任务中均取得改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型快速发展，但小规模模型在特定领域（如心理健康）应用中仍表现不足，需要一种低成本且有效的方法来提升其领域适应能力。

Method: 在特定层的激活上应用线性变换，利用从数据中学习的 steering vectors 来引导模型输出，从而增强其在心理健康任务中的表现，无需微调或高计算成本技术。

Result: 该方法在两个任务上均提升了模型性能：1）判断Reddit帖子是否有助于检测抑郁症状；2）基于用户发帖历史完成标准化抑郁筛查问卷。

Conclusion: steering vectors 是一种极具潜力的、计算高效的工具，可用于大语言模型在心理健康领域的快速适应与优化。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [201] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 本文提出了MoReBench和MoReBench-Theory两个基准，用于评估语言模型在道德困境中的推理过程，发现现有模型在道德推理上存在偏倚，且传统缩放规律无法预测其表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI在决策中扮演更重要的角色，需确保其决策与人类价值观一致；由于道德困境允许多种合理结论，是检验AI推理过程的理想测试平台。

Method: 构建包含1000个道德场景的MoReBench，每个场景配有专家制定的评分标准（如识别道德因素、权衡取舍等），并构建MoReBench-Theory以测试五种规范伦理框架下的推理能力。

Result: 实验表明，模型在道德推理上的表现无法由数学、代码或科学推理任务的缩放规律预测，且表现出对特定伦理框架（如功利主义和康德义务论）的偏好。

Conclusion: 所提出的基准推动了以过程为导向的AI推理评估，有助于实现更安全、透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [202] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了Nyx，一个用于通用检索增强生成（URAG）的统一多模态检索器，能够处理文本和图像混合的查询与文档，并通过构建高质量的多模态数据集NyxQA和两阶段训练框架，在视觉-语言生成任务中显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统主要针对纯文本，难以应对现实世界中包含文本和图像等多模态信息的查询和文档，因此需要一种能处理混合模态的通用RAG方法。

Method: 提出Nyx，一种统一的多模态到多模态检索器；构建四阶段自动化管道生成并过滤数据，创建NyxQA多模态问答数据集；采用两阶段训练框架：先在NyxQA和其他开源检索数据集上预训练，再利用下游视觉语言模型的反馈进行监督微调。

Result: 实验结果表明，Nyx在标准文本RAG基准上表现优异，且在更贴近实际的URAG场景中显著提升了视觉-语言生成任务的生成质量。

Conclusion: Nyx通过统一的多模态检索架构和高质量数据集NyxQA，有效解决了混合模态信息检索与生成的问题，为通用检索增强生成提供了可行方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [203] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出一种名为自主可信代理（ATA）的通用神经符号方法，通过离线知识摄入和在线任务处理两阶段架构提升大语言模型在高风险场景中的可信性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉、不稳定性和缺乏透明度等问题，限制了其在高风险领域的应用，需要提高其可信度。

Method: 将任务解耦为两个阶段：离线阶段使用大语言模型将非正式问题描述转化为可由人类专家验证和修正的形式化符号知识库；在线阶段将输入编码为相同形式化语言，并由符号决策引擎结合知识库进行推理。

Result: 在复杂推理任务上的实验表明，ATA在全自动设置下性能媲美最先进的端到端模型；当使用人工验证的知识库时，显著优于更大模型，且具有完全确定性、更强的输入鲁棒性，并天然免疫提示注入攻击。

Conclusion: ATA提供了一种实用、可控的架构，支持构建下一代透明、可审计、可靠的自主代理系统。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [204] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本文探索了Whisper这一主流语音识别基础模型在二语口语评估（SLA）中的潜在能力，通过提取其隐藏表示中的声学和语言特征，在仅使用轻量级分类器的情况下，在GEPT数据集上超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于Whisper的外显转录结果进行分析，未能挖掘其深层表征能力；本文旨在探究Whisper在未针对SLA任务微调的情况下，是否内在具备评估语言熟练度的潜力。

Method: 从Whisper的中间层和最终输出中提取声学与语言特征，结合轻量级分类器进行评分预测，并引入图像与文本提示作为辅助相关性线索以提升性能。

Result: 在GEPT看图说话数据集上取得了优于当前最先进基线模型的表现，包括多模态方法；分析表明Whisper的嵌入空间已内在编码了语言熟练度的序数模式和语义信息。

Conclusion: Whisper无需任务特定微调即具备强大的口语评估潜力，可作为SLA及其他口语理解任务的有效基础模型。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [205] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出可执行知识图谱（xKG），用于提升大语言模型代理在自动复现AI研究中的代码生成能力，通过整合技术洞见、代码片段和领域知识，显著提高复现性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码时面临背景知识不足和检索增强生成（RAG）难以捕捉论文中隐含技术细节的问题，且缺乏对实现级代码信号的利用和结构化知识表示。

Method: 构建模块化、可插拔的可执行知识图谱（xKG），从科学文献中自动提取技术见解、代码片段和领域知识，并支持多粒度检索与复用，集成到多种代理框架中。

Result: 在PaperBench上，结合三种代理框架和两种大语言模型测试，xKG实现了最高10.9%的性能提升（使用o3-mini）。

Conclusion: xKG是一种通用且可扩展的解决方案，能有效提升LLM代理在AI研究复现中的表现，尤其在生成可执行代码方面具有显著优势。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [206] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: 本文提出了FrugalPrompt，一种基于token显著性评分的提示压缩框架，通过保留输入中语义最重要的token来提升大语言模型的推理效率，在多个NLP任务上验证了其在性能与上下文稀疏性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因长上下文导致成本高、延迟大、碳排放多，而多数提示中存在大量低效冗余token，仅少数token承载主要语义信息，因此需提升输入效率。

Method: 提出FrugalPrompt框架，结合GlobEnc和DecompX两种先进的token归因方法计算每个token的显著性得分，保留前k%最高分token并保持原始顺序，生成稀疏化的高效提示。

Result: 在情感分析、常识问答和摘要任务中，减少20% token仅造成轻微性能下降；但在数学推理任务中性能显著下降，表明其对完整上下文依赖更强。使用最低或随机token的效果更差，且揭示了可能的任务污染效应。

Conclusion: FrugalPrompt有助于理解大模型在效率与性能间的权衡，明确了不同任务对上下文稀疏的容忍度差异，为高效推理提供了新视角。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [207] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为TrajSelector的高效Best-of-N框架，利用采样器LLM中的隐藏状态进行推理路径的质量评估，通过轻量级验证器对逐步推理路径打分并聚合得分以选择最优路径。该方法采用端到端的数据驱动训练策略，无需大量逐步骤标注，在五个基准测试中显著优于多数投票和现有过程奖励模型，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有外部测试时扩展方法（如Best-of-N）存在计算开销高、未充分利用LLM内部表征的问题，因此需要一种更高效且有效的推理路径选择机制。

Method: 设计一个轻量级验证器（0.6B参数），利用LLM的隐藏状态对推理轨迹进行逐歩评分，并通过端到端训练实现全过程打分与最优路径选择，无需依赖大规模细粒度标注。

Result: 在五个基准上实验表明，TrajSelector在Best-of-32设置下比多数投票准确率提高4.61%，优于现有过程奖励模型4.31%~12.21%，且推理成本更低。

Conclusion: TrajSelector通过有效利用LLM的潜在表示和轻量级验证器，实现了高性能与低计算成本的平衡，为复杂推理任务中的测试时扩展提供了新思路。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [208] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个结合课程强化学习与多模态大语言模型的广告视频违规检测框架，通过渐进式训练和分组相对策略优化实现精确时序定位与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在广告视频违规检测中面临时序定位不准、标注噪声和泛化能力差的问题，需要更鲁棒的解决方案。

Method: 提出RAVEN框架，采用课程强化学习与多模态大语言模型，利用精确和粗略标注数据进行渐进训练，结合GRPO算法和多层次奖励机制提升推理能力和时序定位精度。

Result: 在工业数据集和公开基准上，RAVEN在违规类别准确率和时间区间定位方面表现优越；在线A/B测试显示其显著提升精度和召回率，并具备强泛化性和缓解灾难性遗忘的能力。

Conclusion: RAVEN有效提升了广告视频违规检测的准确性与实用性，具备良好的在线部署效果和广泛应用前景。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [209] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文通过应用LiTEx分类法分析自然语言推断（NLI）数据集中标注者的标签和推理差异，揭示了标注分歧背后潜在的解释一致性，并强调基于推理的解释比单一标签更能反映语义相似性。


<details>
  <summary>Details</summary>
Motivation: 理解NLI数据集中人类标注者之间的标签变异及其背后的推理差异，尤其是当标注者在标签上不一致但解释相似时，需深入分析其认知过程。

Method: 采用LiTEx分类法对两个英文NLI数据集中的自由文本解释进行编码，从NLI标签一致性、解释相似性、分类法一致性和标注者选择偏差等多个层面分析标注变异。

Result: 发现部分标注者虽在标签上存在分歧，但其解释高度相似，表明表层标签分歧可能掩盖深层语义理解的一致性；同时观察到个体在解释策略和标签选择上的偏好。

Conclusion: 推理类型的共识比标签共识更能反映自由文本解释的语义相似性，提示应谨慎将标签视为绝对真值，凸显基于解释的分析在NLI中的重要价值。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [210] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出“退出”作为大语言模型代理在不确定情况下自我保护的简单有效机制，通过显式退出指令显著提升安全性，同时几乎不影响帮助性。


<details>
  <summary>Details</summary>
Motivation: 在复杂真实环境中，大语言模型代理面临多轮交互中累积的不确定性与模糊性，传统不确定性量化方法难以应对潜在严重风险，亟需有效的安全机制。

Method: 基于ToolEmu框架，对12个最先进的大语言模型系统评估显式退出指令下的“退出”行为，分析其在多轮代理任务中的安全性和帮助性权衡。

Result: 加入显式退出指令后，所有模型平均安全性提升+0.39（0-3量表），专有模型达+0.64，帮助性仅下降-0.03，展现出优异的安全-帮助性权衡。

Conclusion: 显式退出指令是一种高效、可立即部署的安全机制，能有效作为高风险应用中自主代理的第一道防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [211] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出一种受背包问题启发的自动化框架，用于在动态环境中高效组合智能体系统，通过实时评估组件性能、成本和兼容性，显著提升成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态语义检索进行工具或智能体发现，难以有效复用和组合组件，且选择决策未基于实际能力、成本和实时效用。

Method: 设计一个结构化的自动组合框架，借鉴背包问题思想，由组合智能体动态测试候选组件并实时建模其效用，在性能、预算和兼容性约束下选择最优组件集。

Result: 在五个基准数据集上的实验表明，该在线背包组合器在帕累托前沿上表现优异，相比基线方法显著提高成功率并降低组件成本；单智能体场景下成功率提升最高达31.6%，多智能体系统中从37%提升至87%（从100多个智能体中选）。

Conclusion: 所提方法具有强大的跨领域适应性和可扩展性，能有效实现智能体系统的高效构建与资源复用。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [212] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本研究提出了ReviewGuard，一个基于大语言模型（LLM）的自动化系统，用于检测和分类有缺陷的同行评审，以应对AI生成内容激增对学术评审体系的威胁。


<details>
  <summary>Details</summary>
Motivation: 随着投稿量激增和大语言模型在学术评审中的广泛应用，人类和AI产生的低质量评审可能破坏同行评审系统的公正性和学术诚信，亟需有效检测机制。

Method: ReviewGuard采用四阶段LLM驱动框架：收集ICLR和NeurIPS论文及其评审数据；使用GPT-4.1标注评审类型并由人工验证；通过LLM生成合成数据以缓解数据不平衡；微调编码器模型和开源LLM进行检测。同时分析评审文本的结构与质量特征。

Result: 构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的数据集。发现缺陷评审评分更低、自信度更高、结构更简单、负面情绪更多。自ChatGPT出现后，AI生成的评审显著增加。混合真实与合成数据训练显著提升了检测模型的召回率和F1分数。

Conclusion: ReviewGuard是首个基于LLM的缺陷评审检测系统，为规范AI在同行评审中的应用提供了实证支持，并揭示了人机协作维护学术诚信的关键路径。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [213] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本研究通过分析大语言模型在回答语义等价但文化和语言条件不同的问题时的内部激活路径，揭示其文化理解机制。结果表明，语言对内部路径的影响强于文化，且朝韩之间的低重叠度显示语言相似性不等于表征一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在全球多种文化背景下被广泛应用，准确理解文化差异至关重要。然而，现有评估多关注输出层面表现，缺乏对内部机制的研究，尤其在跨语言和跨文化情境下的区分不清。

Method: 通过测量模型在两种条件下回答语义等价问题时的内部激活路径重叠情况：一是固定问题语言、改变目标国家；二是固定国家、改变问题语言。同时使用同语言不同国家的配对来分离语言与文化因素。

Result: 相同语言、不同国家的问题内部路径重叠更高，而不同语言、相同国家的重叠较低，表明语言影响强于文化。韩国与朝鲜这一语言相同但文化不同的配对表现出低路径重叠和高变异性，说明语言相似性并不保证内部表征一致。

Conclusion: 大语言模型的文化理解主要受语言特征驱动，而非深层文化内容，且语言与文化的表征在模型内部并未有效解耦，这对跨文化应用提出了挑战。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [214] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 本文提出了SHALLOW，首个系统性分类和量化ASR中幻觉得现象的基准框架，涵盖词汇、语音、形态和语义四个维度，并通过针对性指标生成模型行为的可解释画像。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR评估指标主要基于错误率，难以区分语音错误与具有语义合理性的幻觉现象，尤其在关键领域可能带来严重风险，因此需要更精细的评估框架。

Method: 提出SHALLOW框架，从词汇、语音、形态和语义四个互补维度对ASR幻觉进行分类和量化，定义各维度内的具体评估指标，并在多种模型架构和语音场景下进行验证。

Result: 实验表明，SHALLOW指标在识别质量高时与WER强相关，但在WER升高时相关性显著减弱，说明其能捕捉WER无法反映的细粒度错误模式。

Conclusion: SHALLOW能够有效识别和分析ASR模型在复杂条件下的幻觉倾向，支持对模型弱点的具体诊断，并为改进模型提供超越传统聚合错误率的反馈。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [215] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 本研究提出了一种针对乌尔都语的AI生成文本检测框架，通过构建包含1800个人类撰写和1800个AI生成文本的平衡数据集，利用mDeBERTa-v3-base等多语言模型进行微调，在测试集上取得了91.29%的F1分数和91.26%的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型生成的文本越来越接近人类写作，区分人机文本变得困难，尤其是在乌尔都语等资源匮乏的语言中缺乏有效的检测工具。

Method: 构建了来自Gemini、GPT-4o-mini和Kimi AI的乌尔都语AI生成文本数据集，进行了字符与词频统计、词汇丰富度（TTR）和N-gram模式的分析，并使用t检验和Mann-Whitney U检验评估特征显著性；随后对mdeberta-v3-base、distilbert-base-multilingual-cased和xlm-roberta-base三种多语言Transformer模型进行微调。

Result: mDeBERTa-v3-base模型表现最佳，F1得分为91.29%，准确率为91.26%；统计分析揭示了人类与AI生成文本在语言特征上的显著差异。

Conclusion: 该研究推动了乌尔都语环境中对抗虚假信息和学术不端的努力，为低资源语言的自然语言处理工具发展提供了重要支持。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [216] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的短语结构分析新方法，通过微调模型将句子转换为其对应的句法结构，并在西班牙语教学工具MiSintaxis中应用。


<details>
  <summary>Details</summary>
Motivation: 探索利用大型神经模型进行句法分析的新途径，提升现有教学工具MiSintaxis对西班牙语句法教学的支持能力。

Method: 从Hugging Face仓库选取多个大语言模型，使用AnCora-ES语料库生成的训练数据进行微调，使其能够将输入句子翻译为相应的短语结构。

Result: 模型在短语结构分析任务上表现出高F1分数，显示出该方法的有效性和潜力。

Conclusion: 微调大语言模型可用于高效的短语结构分析，为语法教学工具提供了可行的技术路径。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [217] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的提示调优方法Capsule Prompt-Tuning (CaPT)，通过将实例感知信息融入提示中，以极低的参数开销提升大语言模型在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的学习方法依赖繁琐的网格搜索确定最佳提示长度，且缺乏对实例特定信息的利用，导致与输入序列的注意力交互不足。

Method: 提出CaPT方法，将富含语义的实例信息作为‘胶囊提示’置于序列最前端，形成‘注意力锚点’，从而增强模型对关键结构信息的关注和整体注意力交互。该方法几乎不引入额外参数。

Result: 实验表明，CaPT在多种语言任务上表现优异（如T5-Large平均准确率达84.03%），并具有极高参数效率（如在Llama3.2-1B上仅更新0.003%参数）。

Conclusion: CaPT通过融合任务感知与实例感知信息，提供了一种高效、有效的提示调优方案，显著提升了模型性能并降低了计算负担。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [218] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 本文提出了TUuD框架，用于评估大语言模型在动态变化的时间参考点下对时间事件关系的理解能力，发现模型虽表现出类似人类的时间认知特征，但在长期时间推理上仍有局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言理解方面取得进展，但其对时间的解释和推理能力仍然有限，需要探究其是否具备类似人类基于‘现在’这一参考点的时间认知能力。

Method: 提出TUuD框架，通过让大语言模型对当前时刻与目标事件之间的相似度进行评分（0.00到1.00），评估其在动态时间参考系下的时间关系理解能力。

Result: 四个大语言模型均表现出对时间参考系的可测量适应性，相似度评分在‘现在’附近达到峰值并向过去和未来递减，但这种适应性在远期时间情境中减弱。

Conclusion: 大语言模型展现出部分类人的时间认知特性，但其时间推理能力仍受参考系变化和时间距离的影响，尤其在长期时间关系处理上存在不足。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [219] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文探讨了链式思维（CoT）推理在自然语言理解（NLU）任务中的作用，构建了带推理的高质量NLU数据集NLURC，并提出多种推理增强方法。研究发现：随着模型规模增大，CoT推理从损害性能转为提升性能；多数推理增强训练方法不如仅用标签训练，但一种特殊设计的方法表现更好；使用推理训练的模型在未见NLU任务上性能显著提升，可媲美十倍大小的模型，同时具备良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注链式思维（CoT）推理在数学和常识推理任务中的作用，忽视其在自然语言理解（NLU）任务中的潜力。本文旨在系统探索CoT是否也能提升NLU任务的性能。

Method: 构建了一个包含推理的综合性高质量NLU数据集NLURC，开发了多种基于推理的增强方法，包括推理辅助推理和推理辅助训练，并在不同规模的模型上评估其在NLU任务中的效果。

Result: 1) CoT推理在小模型中可能降低NLU性能，但在大模型中能超越直接预测；2) 多数推理增强训练方法表现不如仅使用标签的训练，但一种专门设计的方法能持续提升性能；3) 使用推理训练的模型在未见NLU任务上表现优异，性能接近十倍参数量的模型，且具备良好的可解释性。

Conclusion: 链式思维推理在NLU任务中具有巨大潜力，尤其在大模型和迁移场景下能显著提升性能与可解释性，但需谨慎设计训练方法以避免性能下降。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [220] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文综述了2014至2025年间自然语言处理（NLP）在心脏病学中的研究进展，通过分析265篇相关文献，从NLP范式、任务类型、心血管疾病类型和数据来源等多个维度系统梳理了该领域的多样性与发展趋势。


<details>
  <summary>Details</summary>
Motivation: 由于心血管疾病受遗传、生活方式和社会经济等多因素影响，相关信息分散在患者叙述、病历和科学文献等文本中，亟需有效方法整合和分析这些非结构化数据。

Method: 通过检索六个文献数据库，筛选出265篇应用NLP技术于心血管疾病的研究文章，并从NLP范式、任务类型、疾病类型和数据来源等方面进行多维度分析，同时开展时间趋势分析。

Result: 研究发现NLP在心脏病学中的应用具有高度多样性，涵盖多种技术、任务和数据源，并揭示了过去十年中NLP方法的演变趋势。

Conclusion: 该综述是目前关于NLP在心脏病学中应用最全面的总结，展示了NLP在提升心脏病诊断、治疗和预防方面的重要潜力。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [221] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 本文首次系统研究了大语言模型在多轮对话中的“变色龙行为”，即面对矛盾问题时立场易变的问题，提出了衡量立场不一致性和知识多样性的新指标，并揭示了知识复用率与模型自信度及立场变化之间的强相关性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与检索系统的结合存在可靠性隐患，尤其是在多轮对话中可能因问题表述不同而改变立场，影响其在关键领域的应用可信度。

Method: 构建包含17,770个问答对、覆盖12个争议领域的Chameleon基准数据集，提出Chameleon Score和Source Re-use Rate两个指标，评估Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash等主流模型的表现。

Result: 所有被测模型均表现出严重的变色龙行为（Chameleon Score为0.391–0.511），其中GPT-4o-mini最差；源复用率与置信度（r=0.627）和立场变化（r=0.429）显著正相关，且跨温度方差小，表明该现象非采样随机性所致。

Conclusion: 模型的知识多样性不足导致其过度依赖查询表述，产生立场漂移，亟需在医疗、法律、金融等关键应用场景中引入一致性评估机制。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [222] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 本研究分析了诗歌中的空格使用，探讨其在艺术表达和语言模型训练数据处理中的重要性。


<details>
  <summary>Details</summary>
Motivation: 空格是诗歌形式的关键组成部分，但自然语言处理领域对其关注不足。

Method: 使用来自Poetry Foundation的19,000首英文出版诗歌语料库，比较了人类诗人、大语言模型生成及在线社区未发表诗歌中的空格使用情况，并发布了一个包含2,800首公共领域诗歌的子集。

Result: 发现了不同时间段、诗歌形式和数据源之间的空格使用差异，以及不同文本处理方法对空格表示的影响显著。

Conclusion: 强调了在构建大语言模型预训练数据集时考虑空格处理策略的重要性。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [223] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 本文提出了Beacon基准，用于测量大语言模型中的谄媚偏差（sycophancy），揭示其在事实准确性与顺从性之间的内在权衡，并提出干预方法来调节这一偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在优化过程中将帮助性与礼貌顺从混淆，导致产生谄媚偏差，即优先迎合用户而非坚持正确推理，影响模型的真实性与可靠性。

Method: 提出Beacon——一种单轮强制选择基准，独立于对话上下文来隔离并量化谄媚偏差；通过跨12个先进模型的评估分析偏差的组成，并设计提示层和激活层干预手段以调控偏差。

Result: 发现谄媚偏差可分解为随模型规模增长的稳定语言和情感子偏差；提示层和激活层干预能反向调节这些偏差，揭示对齐机制内部在真实性与社会顺从之间的动态流形结构。

Conclusion: Beacon将谄媚视为一种可量化的规范性错误泛化形式，为研究和缓解大规模生成模型中的对齐漂移提供了可复现的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [224] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种名为SCO-PAL的步级策略优化方法，通过自我对弈显著提升了语言智能体在动态对抗性游戏中的战略推理能力，相较于基线方法平均胜率提高了约30%，并对GPT-4取得了54.76%的胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中因战略推理能力差而表现不佳，且依赖昂贵的专家标注数据；本文旨在通过自动从游戏交互中学习来缓解这一问题，并探索对手选择对学习效果的影响。

Method: 提出SCO-PAL（Step-level poliCy Optimization through Play-And-Learn）方法，通过在不同水平的对手环境下进行自我对弈，系统分析对手选择对战略学习的影响。

Result: 使用SCO-PAL结合自我对弈，在六个对抗性游戏中平均胜率较基线提升约30%，对GPT-4的胜率达到54.76%。

Conclusion: 自我对弈是提升语言智能体在动态对抗环境中战略推理能力的最有效方式，SCO-PAL为无需专家监督的自主学习提供了有效路径。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [225] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: 本文提出了LC-Eval，一个双语、多任务的长上下文理解评测基准，涵盖英语和阿拉伯语，支持4k到128k以上token的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长上下文理解方面展现出新能力，但缺乏系统、严谨的评测方法，尤其在双语和多任务场景下。

Method: 设计了四个新任务：多文档问答、双语问答、段落内主张验证和基于长上下文的多项选择题，并构建了英阿双语数据集，对开源和闭源LLM进行评估。

Result: 实验表明，即使是GPT-4o等高性能模型也在某些任务上表现不佳，说明LC-Eval具有较高挑战性。

Conclusion: LC-Eval为长上下文理解提供了更具挑战性和全面性的评测基准，尤其推动了双语和复杂推理能力的评估。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [226] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: 提出MOSAIC框架，通过多阶段领域自适应和联合领域特定的掩码监督，提升句子嵌入模型在专业领域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型向专业领域迁移时面临的表示学习不足问题。

Method: 结合掩码语言建模（MLM）和对比学习目标，在统一训练流程中进行联合优化，并采用分阶段适应策略。

Result: 在高低资源领域均取得显著提升，NDCG@10最高提升13.4%，消融实验验证各组件有效性。

Conclusion: MOSAIC能有效学习领域相关表示，同时保持原模型的语义判别能力，平衡的联合监督和分阶段设计至关重要。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [227] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 研究发现大语言模型在实体比较任务中常依赖流行度、提及顺序和语义共现等表面启发式线索而非真实知识，较小模型更易受此影响，而较大模型能选择性使用可靠数值知识，思维链提示可促使各规模模型更多使用数值信息。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在知识推理任务中是依赖真实知识还是表面启发式策略，尤其是在有明确真值的数值属性比较任务中的表现差异。

Method: 通过设计实体数值属性比较任务（如河流长度），分析不同规模模型在回答时的行为，并用逻辑回归模型基于流行度、提及顺序和语义共现等表面线索预测模型选择。

Result: 发现小模型常忽视已有知识而依赖表面线索，其行为可被仅基于启发式特征的逻辑回归准确预测；大模型则能区分知识可靠性并更合理使用数值信息；思维链提示能有效引导所有模型使用数值特征。

Conclusion: 模型大小影响知识使用方式，大模型更能根据知识可靠性进行推理，小模型易受启发式偏见影响，思维链可缓解该问题。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [228] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的两阶段检索与重排序框架，用于跨体裁作者归属任务，在HIATUS的HRS1和HRS2基准上显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统信息检索中的训练策略在跨体裁作者归属任务中不适用，因这类任务需识别与主题无关的作者特异性语言模式，而非依赖主题线索。

Method: 采用两阶段的检索与重排序框架，并引入针对性的数据筛选策略，使重排序模型能有效学习区分作者的语言特征。

Result: 在HIATUS的HRS1和HRS2跨体裁作者归属基准上，分别取得了比先前最优方法高22.3和34.4绝对Success@8分数的显著提升。

Conclusion: 所提出的框架通过专门设计的训练策略，有效提升了跨体裁作者归属的性能，验证了其在消除主题依赖性的同时捕捉作者风格的能力。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [229] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 本文提出了CoRUS框架，通过角色理论和在线阿片类药物康复社区数据，构建提问者角色分类体系，并生成15,321个基于角色的问题，用于评估语言模型在不同用户角色下的回应差异，发现患者和照护者角色会引发更支持性但知识性减少的回应。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估常忽略提问者的社会与个人背景角色，尤其在阿片类药物使用障碍等污名化领域，忽视用户情境可能导致回应缺乏同理心或适切性，因此需建立考虑用户角色的评估方法。

Method: 基于角色理论和r/OpiatesRecovery社区帖子，构建患者、照护者、从业者三类提问者角色的 taxonomy，并以此生成包含各角色目标、行为与经验的15,321个模拟问题，用于评估五个大语言模型在不同角色下的回应模式。

Result: 生成的问题具有高可信度且贴近真实数据；在相同问题但不同角色下，模型对患者和照护者角色的回应更支持性（+17%），但知识内容减少（-19%）。

Conclusion: 提问者的隐含角色显著影响语言模型的回应风格，CoRUS提供了一种以用户为中心、基于角色的对话式AI评估新范式。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [230] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了FinSight，一个用于生成高质量、多模态金融报告的多智能体框架，通过CAVM架构和迭代视觉增强机制，在事实准确性、分析深度和展示质量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统难以完全自动化生成专业的金融报告，需要一种能够整合数据、工具和智能体并支持灵活分析与生成的框架。

Method: 提出FinSight框架，核心是CAVM架构，将外部数据、工具和智能体统一到可编程变量空间；采用迭代视觉增强机制优化图表质量，并通过两阶段写作框架扩展分析链为连贯的多模态报告。

Result: 在多个公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线模型，包括领先的深度研究系统。

Conclusion: FinSight为生成接近人类专家水平的金融报告提供了清晰路径，推动了AI在专业报告自动生成领域的发展。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [231] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 本文提出了Neuronal Group Communication (NGC)框架，将神经网络视为由相互作用的神经元组构成的动力系统，通过低秩、模块化的通信机制实现高效、可解释的表示学习，并在大语言模型中验证了其在压缩条件下提升复杂推理性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 为应对现代神经网络在规模扩大时带来的效率和可解释性挑战，探索如何构建能够学习高效、模块化且可解释表示的大规模神经系统的途径。

Method: 提出NGC框架，将神经网络重新建模为神经元组之间的动态交互系统，权重被视为神经元状态间的瞬态交互，计算通过神经元组间的迭代通信完成；引入基于动力系统理论的神经元稳定性度量（类Lyapunov稳定性）来分析激活模式的收敛性，并揭示推理能力与外部‘势能’驱动的关系。

Result: 在大语言模型中实现NGC后，在中等压缩率下显著提升了复杂推理任务的性能，且优于标准的低秩近似和跨层共享基方法；同时实现了参数大幅减少、模型更紧凑，并展现出良好的稳定性和泛化特性。

Conclusion: NGC提供了一种从动力系统视角理解神经网络的新范式，通过模块化、低维通信和稳定性控制实现了高效与可解释性的统一，暗示结构化的神经元组动态可能对高维学习系统的泛化具有重要意义。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [232] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于感知理论的具身知识理解基准，评估多模态语言模型在多种感官模态下的表现，发现视觉-语言模型并未优于纯文本模型，且在视觉维度上表现更差，揭示了当前模型在整合具身知识方面的不足。


<details>
  <summary>Details</summary>
Motivation: 探究视觉 grounding 是否真正提升了语言模型对具身知识的理解能力，比较多模态模型与纯文本模型在感知层面的表现差异。

Method: 基于心理学中的感知理论构建包含视觉、听觉、触觉、味觉、嗅觉及本体感觉的具身知识理解基准，通过向量比较和问答任务（超过1700个问题）评估30种最先进的语言模型。

Result: 视觉-语言模型在两项任务中均未超越纯文本模型，且在视觉维度上的表现显著低于其他感官维度；模型的向量表示易受词形和词频影响，且在涉及空间感知与推理的问题上表现不佳。

Conclusion: 当前多模态模型尚未有效整合具身知识，需进一步改进以提升对物理世界的理解能力。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [233] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: 本文提出了ChiKhaPo，一个涵盖2700多种语言的多任务基准，用于评估大语言模型在词汇理解和生成方面的基本语言能力，揭示了现有模型在低资源语言上的表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的基准测试主要集中于高、中资源语言，忽视了全球大多数书面语言的基本语言能力评估。作者旨在填补这一空白，推动真正意义上的多语言能力评测。

Method: 构建了一个包含8个子任务的基准ChiKhaPo，涵盖不同难度的词汇理解与生成任务，利用现有词典、单语数据和双语文本，其中两个子任务覆盖2700多种语言，并对6个最先进的模型进行了评估。

Result: 实验表明，当前6个最先进的大语言模型在ChiKhaPo上表现不佳，尤其是在低资源语言和生成任务上，性能受语言语系、资源丰富程度和任务类型显著影响。

Conclusion: ChiKhaPo显著扩展了语言覆盖范围，揭示了现有LLMs在基本语言能力上的局限性，呼吁更多关注大规模多语言评估与模型改进。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [234] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出PROMPT-MII，一种基于强化学习的指令归纳框架，能在新数据集上生成紧凑指令，在性能匹配上下文学习的同时减少3-13倍token使用。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）虽有效，但随着上下文长度增加推理成本高，需更高效的适应方法。

Method: 采用强化学习框架PROMPT-MII，通过在3000多个分类数据集上元学习训练一个指令归纳模型，使其能为新任务动态生成简洁且描述性强的提示。

Result: 在90个未见任务上评估，PROMPT-MII相比ICL减少3-13倍token消耗，同时提升下游模型F1值4-9点（相对提升10-20%），性能与ICL相当甚至更优。

Conclusion: PROMPT-MII实现了高效、可泛化的指令归纳，显著降低推理开销，是替代长上下文ICL的有效方案。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [235] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本论文首次将参数高效微调（PEFT）技术应用于孟加拉语仇恨言论检测，使用LoRA和QLoRA方法在BD-SHS数据集上对三种大语言模型进行微调，仅训练少于1%的参数即可在单个消费级GPU上实现高效实验，其中Llama-3.2-3B表现最佳，F1分数达92.23%。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体上的仇恨言论激增，尤其影响女性和青少年，但现有方法依赖高成本的全模型微调或专有API，缺乏高效且可复现的解决方案。

Method: 采用参数高效微调（PEFT）技术，结合LoRA和QLoRA，在BD-SHS数据集（50,281条标注评论）上对Gemma-3-4B、Llama-3.2-3B和Mistral-7B三种指令调优的大语言模型进行微调，仅更新不到1%的模型参数。

Result: Llama-3.2-3B取得最高F1分数92.23%，Mistral-7B为88.94%，Gemma-3-4B为80.25%；所有实验均在单个消费级GPU上完成，验证了PEFT在低资源语言中的可行性与高效性。

Conclusion: PEFT是一种实用且可复现的方法，适用于孟加拉语及其他低资源语言的仇恨言论检测任务，显著降低计算成本的同时保持高性能。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [236] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: 提出UTF8Tokenizer，一种极简的字节级分词器，利用UTF-8编码直接映射文本到ID，使用C0控制字符处理特殊行为，提升效率与兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有字节级分词方法可能引入越界ID或辅助标记，缺乏统一设计，限制了效率和模型间兼容性。

Method: 将文本的UTF-8字节直接映射为token ID，使用C0控制字节表示特殊功能（如填充、边界等），并引入比特偏置嵌入以暴露字节位结构。

Result: 实现14倍更快的分词速度，减少8倍主机-设备传输数据量，提供可共享的256*d嵌入表，并通过比特偏置嵌入提升训练收敛性。

Conclusion: UTF8Tokenizer通过简洁、标准化的设计，在性能、效率和跨模型对齐方面优于先前方法，且兼容HuggingFace，具有实用价值。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [237] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 提出一种基于变换向量的组合式词汇重构方法，减少词汇表冗余并提升覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 标准分词将词形变化视为独立标记，导致词汇表冗余且限制了低频词和多语言覆盖。

Method: 利用嵌入空间中的加性变换向量表示词形变化（如“walk”+过去时=“walked”），构建共享基形式与变换向量的组合词汇。

Result: 在多个大模型和五种语言上验证，最多减少10%词汇表条目，释放空间用于更丰富词汇，并扩展对未登录词的覆盖，几乎不影响下游性能。

Conclusion: 推动词汇设计从字符串枚举转向利用语言内在结构的组合式方法。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [238] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的动态防御框架，结合过去方向梯度阻尼（PDGD）技术，有效抵御迭代越狱攻击，同时提升无害任务的响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动打断迭代越狱攻击的试错循环，缺乏对动态攻击的实时响应能力。

Method: 采用在线学习机制动态更新防御策略，利用强化学习优化提示以区分有害和无害输入，并引入PDGD防止在攻击过程中因局部改写导致的过拟合。

Result: 在三个大语言模型上测试，显著优于五种现有防御方法，能有效抵抗五种迭代越狱方法，同时提升无害任务的响应质量。

Conclusion: 所提出的框架能够有效防御迭代式越狱攻击，具备良好的泛化性和实用性，兼顾安全性与有用性。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [239] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了DiscoTrack，一个包含12种语言和四个层次话语理解任务的LLM基准，旨在评估模型在处理隐含信息和跨文档语用推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准主要关注自然语言理解中的显式信息提取，缺乏对隐含信息和多语言、跨句子、段落及多说话人话语跟踪的挑战性评测。

Method: 设计了一个涵盖12种语言和四个话语理解层次（显著性识别、实体追踪、话语关系和桥接推理）的基准测试DiscoTrack。

Result: 评估结果显示，即使是最先进的模型，在这些任务上仍然面临挑战。

Conclusion: DiscoTrack为评估LLM在复杂、多语言话语理解任务上的表现提供了一个新的、更具挑战性的基准。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [240] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 该论文研究了基于大语言模型的搜索代理在开放域问答中的安全性问题，发现其比基础LLM更容易产生有害输出。作者提出SafeSearch方法，通过多目标强化学习联合优化安全性和实用性，显著降低有害性的同时保持查询性能。


<details>
  <summary>Details</summary>
Motivation: 尽管搜索代理在提升问答性能方面取得进展，但其安全性行为尚未充分探索。研究发现现有代理在面对恶意查询时可能降低拒绝阈值，从而生成有害内容，因此需要同时对齐安全与效用目标。

Method: 提出SafeSearch，采用多目标强化学习框架，结合最终输出的安全/效用奖励与新的查询级塑形项，惩罚不安全查询并奖励安全查询，以实现安全与效用的联合优化。

Result: 实验表明，SafeSearch在三个红队数据集上将代理有害性降低了70%以上，同时保持与仅优化效用的代理相当的问答性能，并生成安全且有用的响应。分析验证了查询级奖励在提升安全性和效用方面的有效性。

Conclusion: 搜索代理存在因追求效用而牺牲安全性的风险，SafeSearch通过引入查询级安全控制，实现了安全与效用的协同优化，为构建安全的检索增强型问答系统提供了有效解决方案。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [241] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出xLSTM框架，结合余弦相似性门控、自适应特征优先级和类别重平衡，在毒性强弱评论检测中以更少参数和更低延迟超越BERT。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型计算成本高、对少数毒性类别表现差，以及传统集成方法语义适应性不足的问题。

Method: 引入可学习参考向量通过余弦相似性调制上下文嵌入，融合多源嵌入（GloVe、FastText、BERT CLS）、字符级BiLSTM、嵌入空间SMOTE和自适应焦点损失。

Result: 在Jigsaw基准上达到96.0%准确率和0.88宏观F1，威胁类提升33%，身份仇恨类提升28%，参数减少15倍，推理延迟仅50ms。消融实验显示余弦门控带来+4.8% F1增益。

Conclusion: 轻量且理论指导的架构可在不平衡、领域特定的NLP任务上超越大型预训练模型，建立新的效率-适应性前沿。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [242] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 提出一种通过语义空间采样和改写扰动来改善大语言模型不确定性校准的方法，并引入新的度量指标分解黑箱LLM中的不确定性，揭示部分LLM在输入语义推理上缺乏一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对语义等价但表述不同的提示词反应不一致，表明其输出的不确定性可能未真实反映对提示语义的理解不确定性。

Method: 将提示敏感性建模为泛化误差，利用改写扰动在语义概念空间中进行采样，结合新提出的不确定性分解度量方法分析黑箱LLM的不确定性来源。

Result: 该方法在不降低准确率的前提下提升了不确定性校准效果；新度量指标能有效量化提示敏感性对LLM不确定性的贡献。

Conclusion: 提示敏感性是影响LLM不确定性校准的重要因素，所提方法可更准确地评估和改善模型对输入语义的稳定推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [243] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本文研究了推理型大语言模型在社会偏见场景中内部思维过程如何聚合社会刻板印象，并发现了两种导致偏见的失败模式：刻板印象重复和无关信息注入。基于此，提出一种轻量级的提示缓解方法，通过让模型自我审查初始推理来减少偏见，在多个基准测试中验证了该方法在降低偏见的同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管推理型大语言模型在复杂任务中表现优异，但其内部思维过程可能聚合社会刻板印象并导致偏见结果，而这一现象背后的机制尚不清晰，因此需要系统探究其行为并提出有效缓解策略。

Method: 通过系统分析模型在问答（BBQ、StereoSet）和开放式生成（BOLD）任务中的推理过程，识别出‘刻板印象重复’和‘无关信息注入’两种导致偏见的失败模式，并设计一种基于提示的自审机制，引导模型检查并修正其推理中的这些问题。

Result: 实验表明，所提出的提示缓解方法在多个基准上显著降低了模型输出的社会偏见，同时保持甚至提升了任务准确率，验证了方法的有效性和实用性。

Conclusion: 大语言模型的推理过程可能因刻板印象重复和无关信息注入而加剧社会偏见，而通过针对性的自我审查提示可以有效缓解这一问题，为构建更公平的推理型模型提供了可行路径。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [244] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 本文提出了DVAGen，一个开源的统一框架，用于增强语言模型的动态词汇能力，支持现代大模型并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有固定词汇的语言模型难以处理新词或未登录词，而现有的动态词汇方法存在代码碎片化、缺乏对现代大模型支持和推理扩展性不足的问题。

Method: 设计了一个模块化、可定制的统一框架DVAGen，集成开源大模型，提供CLI和WebUI工具，支持批量推理和实时结果可视化。

Result: 验证了动态词汇方法在现代大模型上的有效性，显著提升了推理吞吐量，并实现了训练、评估与可视化的全流程支持。

Conclusion: DVAGen为动态词汇增强语言模型提供了高效、灵活且易于使用的解决方案，推动了该方向的研究与应用。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [245] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 该研究探讨了人们在理解AI生成的反语时是否采取“意向立场”，即是否将AI的言语视为有意图的交流。通过行为和神经反应（P200和P600脑电成分）分析，发现人们对AI生成的反语较少归因为有意沟通，更多视为计算错误，且相关神经反应较弱，表明其未充分将意向性赋予AI。个体对AI真诚度的感知影响其意向立场的采纳，说明人类对人工代理的意图归因依赖于对其心理模型的认知。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被用作社交代理并生成幽默与反语，有必要探究人们是否将其输出视为有意图的交流行为，还是仅仅机械的计算结果。

Method: 通过比较人类与AI生成反语时参与者的行为反应和ERP脑电成分（P200和P600），分析其在处理不同来源反语时的早期不协调检测与认知重分析过程。

Result: 行为结果显示，参与者更倾向于将人类的不协调表达解释为有意反语，而将AI的类似表达视为错误；神经数据显示，AI反语引发的P200和P600效应减弱，表明对AI缺乏努力性的语义重分析；但认为AI更真诚的个体表现出更强的P200/P600反应。

Conclusion: 尽管当前大语言模型具备高度语言能力，但要实现真正的社会主体性，还需改变人类对AI意图性的认知。人们对AI是否采取意向立场，取决于其对AI心理状态的具体建模。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [246] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文提出并验证了三个关键设计原则，使基于块的稀疏注意力模型能有效实现超长上下文外推，无需微调即可将在4K上下文训练的模型扩展到3200万token。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在处理长上下文时受限于二次复杂度和较差的长度外推能力，而其他架构因固定内存限制难以充分利用完整上下文，因此需要理解并改进稀疏注意力模型的核心机制。

Method: 通过统一框架和全面的消融实验，系统分析基于块的稀疏注意力模型，提出三个核心设计：非线性块编码器（含CLS标记）、旁路残差路径、以及预训练中的选择稀疏性约束。

Result: 所提方法实现了无需训练的长度外推新标杆，成功将在4K上下文训练的模型推广至RULER和BABILong上的3200万token，并提供了理论支持。

Conclusion: 三个设计原则——表达性强的块编码器、旁路残差路径和选择稀疏性约束——是实现高效长上下文建模的关键，为未来长上下文语言模型的设计提供了清晰且实证支持的指导。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [247] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出一种名为Attention-Shifting（AS）的新框架，用于大语言模型的选择性遗忘，通过注意力层面的干预在有效遗忘敏感信息的同时保持模型性能并抑制幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在遗忘效果和模型效用之间存在权衡：激进遗忘损害模型性能，保守策略则可能导致幻觉响应，限制了大语言模型在知识密集型应用中的可靠性。

Method: 设计了两个注意力级干预机制：重要性感知抑制（针对需遗忘内容）和注意力引导保留增强（强化保留数据中语义关键token的注意力），并通过双损失目标联合优化，实现局部化遗忘与整体知识保留。

Result: 实验表明，AS在ToFU和TDEC基准上分别比现有最优方法提升最高15%和10%的准确性，同时保持较强的无幻觉遗忘效果，在遗忘有效性、泛化能力和响应可靠性之间实现了更好平衡。

Conclusion: AS框架能有效解决大语言模型选择性遗忘中的效用-安全困境，显著提升遗忘过程中的模型性能保持与响应真实性，推动AI辅助决策系统的安全可靠应用。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [248] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一种名为StreamingThinker的流式思维范式，使大语言模型能够在输入过程中边读边思考，显著降低推理延迟并保持与批量处理相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理范式需要等待完整输入后才开始思考，导致动态场景中出现不必要的延迟，并削弱对早期信息的关注。受人类阅读时即时思考的启发，作者希望实现一种更高效、低延迟的推理方式。

Method: 提出了流式思维（streaming thinking）范式，通过流式CoT生成、流式约束训练和流式并行推理解决动态推理问题。具体实现包括：使用带质量控制的流式推理单元、通过流式注意力掩码和位置编码保证推理顺序、采用解耦输入编码与推理生成的并行KV缓存机制。

Result: 在Qwen3模型系列上评估了数学推理、逻辑推理和基于上下文的问答任务，结果显示StreamingThinker相比传统批处理推理，在保持相近性能的同时，将推理启动前的token等待时间减少了80%，最终答案的时间级延迟降低了超过60%。

Conclusion: 流式思维范式有效提升了大语言模型在动态输入场景下的推理效率，实现了低延迟与高性能的结合，为实际应用中的实时推理提供了可行方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [249] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了VideoBiasEval框架，用于系统评估视频生成中的社会偏见，发现对齐微调不仅强化了表征偏见，还使其在时间上更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在提升生成质量的同时可能放大社会偏见，亟需系统性工具追踪偏见在对齐流程中的演变。

Method: 基于社会偏见分类体系，提出事件驱动的提示策略，分离语义内容与人物属性，并设计多粒度指标评估族群、性别偏见及偏见的时间持续性。

Result: 首次实现从人类偏好数据到奖励模型再到对齐微调模型的端到端偏见分析，发现对齐过程加剧并稳定了社会偏见。

Conclusion: 应在对齐全过程引入偏见感知的评估与缓解机制，以确保视频生成的公平与社会责任。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [250] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 该研究通过大规模情感分析探讨了孟加拉语新闻中的情绪偏见，发现负面情绪（尤其是愤怒、恐惧和失望）占主导地位，并提出了一种可视化情感线索的人本新闻聚合器设计。


<details>
  <summary>Details</summary>
Motivation: 揭示新闻媒体如何通过情感框架影响公众情绪，特别是相同事件在不同媒体中呈现的情感差异。

Method: 使用Gemma-3 4B模型对30万条孟加拉语新闻标题及其内容进行零样本推断，识别每条新闻的主导情绪和整体基调。

Result: 发现负面情绪显著主导，且相似报道在不同媒体间情感表达存在明显差异。

Conclusion: 新闻媒体普遍存在情感偏见，建议通过人本新闻聚合器可视化情感线索，帮助读者识别隐性的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [251] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了Transformer大语言模型的局部可解释性与机制可解释性方法，通过医疗和自动驾驶领域的实验探讨了解释对用户信任的影响，并指出了当前未解决的问题及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成文本时存在难以理解的黑箱行为和幻觉错误，亟需提高其可解释性以增强人类对其的信任。

Method: 回顾现有可解释性方法，开展在医疗与自动驾驶领域的可解释性与推理实验，并分析解释结果对用户信任的影响。

Result: 总结了当前可解释性研究的成果与局限，揭示了不同领域中解释如何影响用户信任，并识别出关键挑战。

Conclusion: 推动面向人类对齐、可信的大语言模型可解释性研究，需解决现有方法在一致性、可理解性和实际应用中的不足。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [252] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出了一种名为TaxoAlign的三阶段主题指导方法，用于自动生成学术分类体系，并构建了包含460个从人类撰写综述中提取的分类体系的CS-TaxoBench基准，实验表明该方法在自动和人工评估中均优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动综述生成方法未将生成结果与专家手动构建的分类体系进行结构对比，缺乏有效评估手段，因此需要一种能弥合自动生成与人工构建分类体系之间差距的方法。

Method: 提出TaxoAlign方法，采用三阶段主题驱动的指令引导方式生成学术分类体系；构建CS-TaxoBench基准（含460个训练+80个测试分类体系）；设计严格的自动化评估框架，衡量生成分类体系的结构对齐性和语义一致性。

Result: 在CS-TaxoBench上评估显示，TaxoAlign在几乎所有自动指标和人工评估中均显著优于各类基线方法，验证了其有效性。

Conclusion: TaxoAlign能够更接近人类专家水平地生成学术分类体系，所提出的评估框架和数据集为未来研究提供了重要基础。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [253] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本研究利用法语多参与者对话数据集CyberAgressionAdo-Large，评估了文本与图表示学习方法在检测网络反社会行为（如欺凌、滥用语言）中的表现，发现多模态融合模型mBERT + WD-SGCN效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于X和Reddit等平台，而多参与者对话场景中的反社会行为研究因数据受限而不足，本文旨在填补这一空白。

Method: 采用CyberAgressionAdo-Large数据集，对六种文本模型和八种图模型进行基准测试，分析词彙线索、互动动态及其多模态融合在三种任务（滥用检测、欺凌行为分析、欺凌同伴群识别）上的表现。

Result: 多模态模型优于单模态基线，其中mBERT + WD-SGCN在滥用检测（F1=0.718）、欺凌分析（0.606）和同伴群识别（0.286）上表现最佳，且能有效处理隐性攻击、角色转换等复杂情境。

Conclusion: 融合文本与图结构信息的多模态方法更适用于复杂社交语境下的反社会行为识别，具有更强的上下文理解能力。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [254] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现，尽管指令调优的大语言模型（IT-LLMs）在零样本推理上表现良好，但在执行简单、独立的指令时仍存在显著的格式偏差和指令遵循不一致问题，尤其在选项标签格式变化时性能明显下降，揭示了当前指令调优范式的不足。


<details>
  <summary>Details</summary>
Motivation: 探究IT-LLMs在基础、原子级指令遵循能力上的表现，尤其是在不同选项标签格式下的稳定性，以揭示当前模型在复杂指令跟随任务中可能存在的根本缺陷。

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个IT-LLMs，系统地改变选项标签的格式（字母、数字、罗马数字），并在四种范式下分析模型性能：有无明确指令、是否移除选项内容、是否提供示例。

Result: 1) 标签格式变化导致性能大幅波动（如罗马数字比数字低30.45%）；2) 缺乏指令时性能进一步下降且对标签更敏感；3) 移除选项内容后模型仅在数字标签下勉强达标；4) 三样本示例未能提升鲁棒性，生成分析显示持续存在标签错误；大模型准确率更高但仍不稳定。

Conclusion: 当前IT-LLMs在原子级指令遵循上存在严重不足，表现出明显的格式偏见和脆弱性，需改进评估方法与训练策略以增强对基本指令的可靠执行能力。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [255] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文提出了EduAdapt，一个包含近48,000个标注年级的科学问答对的基准数据集，用于评估大语言模型在K-12教育中根据学生年级调整回答的能力。研究发现现有模型在低年级学生回答上表现不佳，强调了开发适应不同认知发展阶段的教育AI系统的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在学术任务上表现良好，但常无法根据学生的年级水平调整输出内容，导致对年幼儿童来说过于复杂或模糊。K-12教育需要适龄的语言和解释方式，因此亟需评估和提升模型的年级适应能力。

Method: 构建了一个涵盖9门科学科目、1-12年级的问答数据集EduAdapt，包含约48,000个标注样本，并将年级划分为四个等级。在此基础上评估多种开源大语言模型的表现，分析其在不同年级问题上的响应能力和准确性。

Result: 实验表明，尽管较大的模型整体表现更好，但在应对1-5年级的问题时仍存在显著困难，生成的回答往往不符合低龄学习者的理解水平。目前尚无模型能很好地适应低年级需求。

Conclusion: EduAdapt是首个专门用于评估大语言模型年级适应能力的数据集和框架，为开发更符合儿童认知发展的教育AI提供了基础，未来可通过改进训练和提示策略提升模型的适龄响应能力。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [256] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究提出了首个专注于中医（TCM）的大语言模型Ladder-base，采用组相对策略优化（GRPO）方法进行训练，显著提升了模型在推理和事实一致性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系结构独特，对现有大语言模型的应用构成挑战，而现有的中医专用模型在对齐性、数据质量和评估一致性方面存在局限。

Method: 基于Qwen2.5-7B-Instruct基础模型，使用TCM-Ladder基准的文本子集进行训练，并采用组相对策略优化（GRPO）方法，通过组内比较优化响应选择。

Result: 在标准化评估中，Ladder-base在多项推理指标上优于GPT-4、Gemini 2.5、Claude 3、Qwen3等通用模型以及BenTsao、HuatuoGPT2、Zhongjing等中医专用模型。

Conclusion: GRPO是一种有效且高效的方法，可用于在传统医学领域对齐专家级推理能力，推动可信且临床扎实的中医人工智能系统的发展。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [257] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 提出AfriCaption框架，用于20种非洲语言的多语言图像描述生成，包含高质量数据集、动态管道和专用模型，推动低资源语言的多模态AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于高资源语言，导致非洲等低资源语言在多模态AI中被边缘化，阻碍技术普惠。

Method: 基于Flickr8k构建语义对齐的数据集，采用上下文感知的选词与翻译；设计动态、保质的流程，结合模型集成与自适应替换；开发具备0.5B参数的AfriCaption模型，融合SigLIP与NLLB200实现跨语言图像描述生成。

Result: 建立了首个可扩展的非洲语言图像描述资源，显著提升低资源语言的 caption 质量与一致性，并确保数据持续优化。

Conclusion: AfriCaption为非洲语言提供了首个可扩展的多语言图像描述框架，推动了多模态AI在语言多样性上的包容性发展。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [258] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出通过在对齐前后模型权重之间进行插值的简单后处理方法，有效缓解对齐税问题，在提升准确率的同时恢复校准性，获得更强大且可靠的新模型。


<details>
  <summary>Details</summary>
Motivation: 对齐过程通常会导致模型性能下降（对齐税），不仅影响任务准确性，还严重损害模型的校准性和输出多样性。本文旨在探索一种能同时改善准确性和校准性的方法，以全面应对对齐税问题。

Method: 采用对齐前后模型权重的线性插值方法，通过简单的后处理融合两个模型，寻找帕累托最优的插值点，在不增加计算成本的情况下优化模型表现。

Result: 该方法不仅能显著恢复对齐过程中损失的校准性，还能在某些情况下使模型准确率超过原始对齐前和对齐后的模型，实现性能与可靠性的双重提升。

Conclusion: 简单的模型合并策略可以有效缓解对齐税的多方面负面影响，为构建更强大、更可靠的对齐模型提供了一种高效且实用的解决方案。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [259] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: 本研究发现，经过强化学习训练的搜索模型虽然继承了指令调优的拒绝机制，但其安全性脆弱，容易受到简单攻击，导致生成有害搜索和回答，暴露出当前RL训练在安全性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 理解强化学习训练的搜索模型的安全特性，尤其是其在面对有害请求时的表现。

Method: 通过设计两种简单攻击（Search攻击和Multi-search攻击），测试Qwen和Llama两个模型家族在本地和网络搜索场景下的安全响应能力。

Result: 攻击使拒绝率降低最多60.0%，回答安全性下降82.5%，搜索查询安全性下降82.4%，模型在生成拒绝标记前即生成有害搜索查询。

Conclusion: 当前的代理式强化学习训练忽略了查询的危害性，仅奖励有效查询生成，亟需开发注重安全的训练流程。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [260] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 本研究开发了多种深度上下文嵌入模型，以提升低资源语言（英语、西班牙语和意大利语）在心脏病学领域的临床命名实体识别（NER）性能，参与BioASQ MultiCardioNER共享任务，并在多个子任务中超越基准表现。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化临床文本蕴含大量生物医学知识，但在低资源语言中的临床NER研究较少，亟需提升多语言临床文本的信息抽取能力以支持数据驱动的临床系统。

Method: 采用基于BERT的单语和多语言上下文语言模型，在通用领域文本上预训练，用于从英文、西班牙文和意文的临床病例报告中抽取疾病和药物提及。

Result: 在西班牙语疾病识别（SDR）上达到77.88%的F1分数，西班牙语药物识别（SMR）为92.09%，英语药物识别（EMR）为91.74%，意大利语药物识别（IMR）为88.9%，均超过该任务测试排行榜的平均与中位F1分数。

Conclusion: 所提出的BERT-based模型在多语言临床NER任务中表现优异，尤其在低资源语言环境下展现出较强的有效性，有助于推动多语言数据驱动临床系统的发展。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [261] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文介绍了首个用于乌尔都语到英语习语翻译的评估数据集，涵盖原生乌尔都文和罗马化乌尔都文，并评估了多种大语言模型和神经机器翻译系统在保留习语及文化含义方面的能力。研究发现提示工程有助于提升翻译效果，且原文脚本对翻译质量有显著影响，原生乌尔都文的表现优于罗马化形式。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在低资源语言如乌尔都语中仍面临挑战，且相关研究较少，缺乏标准评估数据集。

Method: 构建了覆盖两种乌尔都语书写形式的双语习语翻译评估数据集，采用BLEU、BERTScore、COMET和XCOMET等自动指标，评估多个开源大语言模型和神经机器翻译系统，比较不同提示方法和脚本形式的影响。

Result: 提示工程能提升习语翻译效果，但不同提示类型间差异较小；使用原生乌尔都语输入的翻译质量显著高于罗马化乌尔都语。

Conclusion: 文本表示方式对习语翻译质量有重要影响，未来研究应重视脚本形式的选择，并结合提示优化以提升低资源语言的习语翻译性能。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [262] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 本研究系统评估了多语言大型语言模型（LLMs）在医疗问答中的跨语言差异，发现维基百科的医疗内容覆盖和LLM事实一致性存在显著语言差异，英文内容占主导；引入非英语上下文信息和检索增强生成（RAG）可有效提升非英语语言的事实对齐，促进更公平的多语言AI医疗系统发展。


<details>
  <summary>Details</summary>
Motivation: 确保不同语言用户在AI辅助医疗中获得可靠、一致的健康信息，解决当前多语言LLM在事实性和知识覆盖上的不平等。

Method: 构建多语言医疗数据集MultiWikiHealthCare，分析五种语言（英、德、土、中、意）维基百科的医疗覆盖差异，评估LLM回答与参考文献的事实对齐情况，并通过上下文提示和检索增强生成（RAG）进行案例研究。

Result: 发现维基百科的医疗内容在不同语言间覆盖不均，LLM的回答普遍更倾向于与英文维基内容对齐，即使输入为非英语提示；引入非英语上下文信息可在推理时显著改善事实对齐，提升文化相关性。

Conclusion: 跨语言医疗信息获取存在显著差距，提升多语言AI公平性的关键在于优化非英语知识源的利用，特别是结合检索增强生成技术，使模型响应更贴近本地化、准确的知识来源。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [263] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新的Mixture-of-Experts架构，通过跨层复用专家并引入渐进式扩展路由策略，突破了传统层局部路由的限制，在不增加参数的情况下提升了模型表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构受限于每层仅能访问本地专家池，难以兼顾专家维度与路由多样性，限制了模型表达能力。

Method: 提出ReXMoE架构，允许路由器在相邻层间复用专家，并设计渐进式扩展路由（PSR）策略，在训练过程中逐步扩大候选专家池。

Result: 在0.5B到7B参数规模的多种架构上实验表明，ReXMoE在语言建模和下游任务上均显著优于基线模型，且保持固定架构维度下的参数效率。

Conclusion: ReXMoE通过跨层专家复用和PSR策略，解耦了专家维度与每层预算，为高效可扩展的MoE型大模型提供了新设计范式。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [264] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出了一种名为DETree的新方法，用于检测人类与AI协作生成的混合文本，通过构建层次亲和树结构和设计专门的损失函数，在新构建的RealBench数据集上实现了更优的性能和更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AI文本检测方法对人类与AI协作过程建模过于粗糙，难以应对多样化协作模式带来的复杂特征，亟需更精细的检测框架。

Method: 提出DETree方法，将不同生成过程的关系建模为层次亲和树结构，并设计了与其对齐的专用损失函数；同时构建了包含多种协作模式的RealBench基准数据集。

Result: DETree在混合文本检测任务中表现更优，尤其在分布外场景和少样本条件下展现出更强的鲁棒性和泛化能力。

Conclusion: 通过显式建模人类-AI协作过程的层次关系，DETree提升了检测性能与泛化性，验证了基于训练的方法在OOD场景下的潜力。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [265] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统回顾了基于大语言模型的行业智能体的技术、应用和评估方法，提出了一个行业智能体能力成熟度框架，阐述了从“流程执行系统”到“自适应社会系统”的演进路径，并探讨了记忆、规划与工具使用三大技术支柱的发展，以及在多个实际领域的应用现状与挑战。


<details>
  <summary>Details</summary>
Motivation: 如何将通用智能体的研究转化为推动产业变革的实际生产力仍是一个重大挑战，因此需要系统性地梳理行业智能体的发展现状与路径。

Method: 通过构建行业智能体能力成熟度框架，分析记忆、规划和工具使用三大技术支柱的演进，并综述其在各行业的应用案例及评估方法。

Result: 明确了行业智能体的技术发展脉络、应用场景、评估体系及其在真实性、安全性和行业适配性方面的挑战，总结了当前的能力边界与发展潜力。

Conclusion: 该综述为理解和构建下一代行业智能体提供了清晰的技术路线图和理论基础，强调了技术演进与产业实践结合的重要性。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [266] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Deep Self-Evolving Reasoning (DSER) 的概率性推理范式，通过将迭代推理建模为马尔可夫链，利用多个长视野并行自我演化过程，即使在验证与修正能力较弱的情况下，也能显著提升小型开源语言模型的复杂问题求解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的验证-修正框架依赖强大的验证能力，在小型开源模型上表现脆弱，难以解决高难度推理任务。本文旨在探索如何在弱验证条件下仍能有效扩展模型的推理极限。

Method: 将迭代推理过程建模为马尔可夫链，假设只要改进的概率略高于退化的概率，系统最终会收敛到正确解。通过并行运行多个长程自我演化推理链，并结合多数投票机制，放大微小的正向趋势。

Result: 在AIME 2024-2025基准上，DSER使DeepSeek-R1-0528-Qwen3-8B模型解决了9个此前无法解决的问题中的5个，整体性能超越其6000亿参数教师模型的单次推理准确率。

Conclusion: DSER为提升小型语言模型的复杂推理能力提供了一条有效路径，并揭示了当前开源模型在自验证、自修正和稳定性方面的根本局限，指明了下一代具备内在自我演化能力模型的研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [267] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 提出了一种新的多语言句子嵌入模型LaBSE，结合了多种方法，在112种语言上显著提升了双语文本检索准确率，并减少了对平行语料的需求。


<details>
  <summary>Details</summary>
Motivation: 探索BERT在跨语言句子嵌入中的应用，弥补单语与跨语言表示学习之间的差距。

Method: 结合掩码语言建模（MLM）、翻译语言建模（TLM）、双编码器翻译排序和加性边缘softmax，利用预训练的多语言语言模型进行多语言句子嵌入学习。

Result: 在Tatoeba数据集上实现了83.7%的双语文本检索准确率（远超LASER的65.5%），并将所需平行数据减少了80%，同时在单语迁移学习任务中表现良好。

Conclusion: 所提出的模型有效提升了多语言句子嵌入性能，且适用于低资源语言，所释放的模型具有广泛的应用价值。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [268] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出EliCal框架，通过两阶段方法（先提取内部置信度，再用少量正确性标注进行校准）实现大语言模型的诚实对齐，在仅使用1k标注数据时即接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于训练的校准方法需要大量昂贵的标注数据来实现通用的诚实对齐，限制了其应用。因此需要一种标注高效的方法。

Method: 提出Elicitation-Then-Calibration (EliCal) 框架：第一阶段利用低成本的自洽性监督提取模型内部置信度；第二阶段使用少量正确性标注对置信度进行校准。同时发布HonestyBench基准，包含56万训练和7万评估样本。

Result: EliCal在仅使用1k正确性标注（全监督的0.18%）时即可达到接近最优的对齐性能，并在外分布MMLU任务上优于纯校准基线。

Conclusion: EliCal为实现大语言模型的通用诚实对齐提供了一种可扩展且标注高效的解决方案。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [269] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: 本文提出了SimBench，首个大规模标准化基准，用于评估大语言模型（LLM）在模拟人类行为方面的表现。通过整合20个多样化数据集，研究发现当前LLM的模拟能力有限（得分为40.80/100），但性能随模型规模对数线性增长；推理时计算资源增加并未提升表现，且存在对齐-模拟权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM模拟人类行为的评估方法零散、不可比，缺乏统一标准，难以系统评估其真实性。因此需要建立一个大规模、标准化的基准来推动该领域的可重复科学研究。

Method: 构建SimBench基准，整合20个涵盖道德决策、经济选择等任务的多样化数据集，覆盖全球广泛人群。在多个LLM上评估模拟性能，并分析模型大小、推理计算、指令微调等因素对结果的影响。

Result: 当前最佳LLM的模拟能力仍有限（40.80/100）；性能随模型规模对数线性提升；增加推理计算无改善；指令微调提升共识类问题表现但损害多样性问题表现；模型在特定人口群体模拟上表现较差；模拟能力与深度知识推理能力（MMLU-Pro）高度相关（r=0.939）。

Conclusion: SimBench为评估LLM的人类行为模拟提供了可衡量的标准，揭示了当前模型的局限性和关键影响因素，有助于推动更真实、可靠的LLM模拟器的发展。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [270] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出一种多任务学习框架，将自回归大语言模型与临床推理对齐，用于癌症治疗结果预测，在MSK-CHORD数据集上实现可解释且高性能的生存分类、生存时间回归和自然语言理由生成。


<details>
  <summary>Details</summary>
Motivation: 需要在异构临床数据下既准确又可解释的模型来预测癌症治疗结果，而现有生物医学大语言模型常缺乏关键的结构化推理能力。

Method: 采用统一的多任务学习框架，结合二元生存分类、连续生存时间回归和自然语言理由生成；比较三种对齐策略：标准监督微调（SFT）、带思维链提示的SFT、以及基于专家推理轨迹的组相对策略优化（GRPO）。

Result: CoT提示使F1提升6.0，MAE降低12%；GRPO在BLEU、ROUGE和BERTScore上达到最优可解释性和预测性能；发现现有模型因架构限制难以生成有效推理路径。

Conclusion: 推理感知的对齐在多任务临床建模中至关重要，所提方法为精准肿瘤学中的可解释、可信大语言模型设立了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [271] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 提出一种基于拓扑数据分析（Mapper）的新视角，揭示细调后的语言模型在嵌入空间中形成高纯度但与真实标签对齐下降的模块化区域，暴露模型在处理模糊性时结构置信度与标签不确定性之间的张力。


<details>
  <summary>Details</summary>
Motivation: 传统标量指标（如准确率）无法充分捕捉模型如何内部表示歧义，尤其是在人类标注者存在分歧的情况下。需要更深入理解模型在细调过程中如何编码歧义和实例。

Method: 采用拓扑数据分析工具Mapper，分析RoBERTa-Large在MD-Offense数据集上的嵌入空间结构，并与PCA、UMAP等传统降维方法对比，识别决策区域、边界坍缩和过度自信聚类等几何特征。

Result: 发现98%以上的连通组件具有≥90%的预测纯度，表明模型形成高度模块化的非凸区域；但在模糊数据上，这些结构与真实标签的对齐程度下降，揭示出模型结构上的自信与实际标签不确定性的矛盾。

Conclusion: Mapper是一种强大的诊断工具，不仅能可视化模型的决策结构，还可提供拓扑指标以指导主观性NLP任务中的建模策略，推动对模型如何解决歧义的理解。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [272] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种名为语言混淆门（LCG）的轻量级插件式方法，用于在解码过程中过滤令牌以减少大语言模型中的语言混淆问题，且无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成文本时常常出现无意的语言混合（语言混淆），现有方法要么需要重新训练模型，要么无法区分有害混淆和可接受的语码转换。

Method: 提出语言混淆门（LCG），基于输出令牌嵌入范数差异和高资源语言偏好现象，在解码过程中动态预测语言族并进行选择性掩码；采用范数调整的自蒸馏方法进行训练。

Result: 在Qwen3、GPT-OSS、Gemma3、Llama3.1等多个模型上评估显示，LCG显著减少了语言混淆，通常降低一个数量级，且不影响任务性能。

Conclusion: LCG是一种有效、即插即用的方法，能够在不修改基础大模型的前提下，大幅缓解语言混淆问题，同时保持原有任务表现。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [273] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出了一种基于超图的适配器（HGAdapter），通过捕捉代码中的高阶相关性（如抽象语法树家族关系、词汇关系和行关系）来增强预训练语言模型（PLM）在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型未充分考虑代码中潜在的高阶数据相关性，限制了其在代码理解任务中的表现。

Method: 设计了令牌与超边生成器以捕捉三种高阶相关性，并改进了超图神经网络架构，结合适配器微调提出了HGAdapter方法。

Result: 在多个公开数据集上进行了实验，涵盖六种编程语言的代码摘要和代码克隆检测任务，结果显示HGAdapter在不同数据集上均提升了PLM的性能。

Conclusion: 引入高阶数据相关性有助于提升预训练语言模型在代码相关任务中的有效性，HGAdapter具有良好的可扩展性和通用性。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [274] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为LawChain的新框架，用于显式建模中国侵权民事案件中的法律推理过程，并构建了评估基准LawChain$_{eval}$，以系统评估大型语言模型在侵权法律推理中的表现，结果表明当前模型仍存在不足，而基于该框架的基线方法能显著提升推理能力并具有良好的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法多依赖通用框架，未能充分捕捉法律推理的细微过程，且主要集中在刑事案件，对民事案件建模不足，因此需要针对民事侵权案件建立更精细的法律推理模型。

Method: 将侵权分析中的法律推理过程操作化为三模块的LawChain框架，每个模块包含多个细粒度子步骤，并基于此构建评估基准LawChain$_{eval}$，设计明确融入LawChain推理结构的提示或后训练基线方法，在多个法律分析任务上进行实验验证其有效性与泛化性。

Result: 实验证明当前大语言模型在处理侵权法律推理关键环节上仍有明显不足；引入的基线方法在侵权相关推理任务上取得显著改进，并在法律命名实体识别和刑事损害计算等任务中表现出良好泛化能力。

Conclusion: 显式建模法律推理链（如LawChain）有助于提升语言模型在民事侵权法律推理及其他相关法律任务中的性能，为未来法律人工智能研究提供了可扩展的框架和评估基准。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [275] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的知识遗忘方法，发现现有方法在移除特定知识的同时损害了模型在上下文中重新利用这些信息的能力。为此，作者提出了一种新的方法，在遗忘目标中加入一个插件项，以保留模型在上下文存在时使用被遗忘知识的能力。实验表明，该方法能有效恢复上下文效用，同时保持良好的遗忘效果和保留集性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识遗忘方法虽然能够有效地删除特定知识并保持模型整体性能，但忽视了一个重要的可用性问题：当用户在输入提示中重新引入被删除的信息时，模型应仍能利用这些信息。这种能力对于确保模型的灵活性和实用性至关重要。

Method: 本文系统评估了六种最先进的知识遗忘方法，并提出了一个新的增强目标函数的方法——通过添加一个‘插件’项来保留模型在上下文中使用已遗忘知识的能力。这个插件项旨在使模型能够在接收到相关上下文时重新激活被遗忘的知识。

Result: 广泛的实验结果显示，所提出的方法可以将上下文效用恢复到接近原始水平，同时仍然维持有效的遗忘效果以及对保留集的良好性能。

Conclusion: 本文提出的增强型知识遗忘方法不仅解决了现有方法中存在的上下文效用下降问题，而且在不牺牲遗忘效率的前提下显著提升了模型的实际应用价值。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [276] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 本文介绍了Qomhr\'a，一种在低资源条件下开发的双语爱尔兰语-英语大语言模型，通过持续预训练、指令微调和基于人类偏好的对齐，实现了在爱尔兰语和英语上的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了在低资源条件下提升爱尔兰语在大语言模型中的表现，同时保持其英语能力，推动少数语言的自然语言处理发展。

Method: 采用双语持续预训练、基于Gemini-2.5-Pro生成的指令微调数据集和人类偏好数据进行指令微调与对齐，构建完整的模型训练 pipeline。

Result: Qomhr\'a 在翻译、性别理解、主题识别和世界知识等基准测试中，爱尔兰语性能最高提升29%，英语提升44%；指令遵循能力显著增强，并发布了30K双语指令数据集和1K人类偏好数据集。

Conclusion: Qomhr\'a 成功展示了在低资源语言上构建高性能双语大模型的可行性，为少数语言的AI支持提供了可复用的方法和数据资源。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [277] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种基于对话分析的方法，用于识别学习者与大语言模型（LLM）交互中的有效教学策略，强调应关注对话动态而非仅技术性能或学习结果。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM教育应用的评估多关注技术性能或学习成果，忽视了学习者与LLM之间的互动质量，亟需一种聚焦对话过程的评估方法。

Method: 通过收集对话数据、进行对话行为（DA）标注、挖掘DA模式并构建预测模型，分析 learner-LLM 对话中的教学策略。

Result: 初步研究结果揭示了某些潜在的有效对话模式，为后续深入分析和模型优化提供了基础。

Conclusion: 评估LLM在教育中的应用应重视对话动态和教学策略，该对话分析框架为未来研究提供了可行路径。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [278] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出QueST框架，通过难度感知的图采样和拒绝微调生成大规模、高难度的合成编程问题，显著提升小型语言模型在竞争性编码任务中的表现，具备良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限，且依赖人工标注，限制了大模型在复杂推理任务上的可扩展性。需要一种能自动生成高质量、高难度编程问题的方法。

Method: 提出QueST框架，结合难度感知的图采样和难度感知的拒绝微调，训练专用生成器来生成高难度编程问题，并用于知识蒸馏或强化学习以提升下游模型性能。

Result: 使用QueST生成10万难题微调Qwen3-8B-base后，在LiveCodeBench上超越原模型；结合28K人工问题与合成解，共112K样本，使8B模型性能匹配671B的DeepSeek-R1。

Conclusion: QueST提供了一种高效、可扩展的途径，通过生成复杂编程问题推动大模型在竞争性编码与推理任务上的发展。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [279] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出了一种轻量级的少样本命名实体识别（NER）框架，通过新的指令微调模板和保留实体信息的数据增强技术，在低资源场景下实现了与现有最先进模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺的低资源场景下，现有的零样本和指令微调方法难以泛化到领域特定实体，且无法有效利用有限数据，因此需要更高效的方法。

Method: 设计了一种简化的指令微调模板以利用大模型的上下文窗口，并引入一种保持实体信息的上下文改写数据增强策略。

Result: 在CrossNER数据集上，少样本方法平均F1得分为80.1；结合改写增强的模型F1提升最高达17点。

Conclusion: 该框架能有效提升低资源NER任务的性能，尤其适用于训练数据和计算资源有限的场景。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [280] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出AcademicEval，一个用于评估大语言模型在长上下文生成任务上的动态基准，使用arXiv论文数据，避免标签泄露，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文LLM基准受限于固定长度、人工标注成本高以及训练中的标签泄露问题。

Method: 基于arXiv论文构建学术写作任务（如标题、摘要、引言等），结合专家策划的少样本示例和作者合作图，支持灵活上下文长度和自动化评估。

Result: 实验表明当前LLM在层次化抽象任务和长少样本推理中表现不佳，验证了该基准的挑战性和有效性。

Conclusion: AcademicEval提供了一个高效、无标签泄露的动态评估平台，揭示了提升LLM长上下文建模能力的关键方向。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [281] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出一种基于二值检索增强奖励（binary RAR）的在线强化学习方法，有效降低语言模型幻觉，且不损害其他任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解语言模型幻觉的方法通常会损害开放生成和下游任务的表现，限制了实用性，因此需要一种能平衡事实性与生成质量的新方法。

Method: 采用在线强化学习框架，设计一种新的二值检索增强奖励（RAR）机制：仅当模型输出完全正确时给予奖励1，否则为0，避免连续奖励带来的优化偏差。

Result: 在Qwen3推理模型上验证，开放生成中幻觉率降低39.3%；短答案问答中，在PopQA和GPQA上错误答案分别减少44.4%和21.7%，并学会合理拒绝回答；同时保持指令遵循、数学和代码任务性能无损。

Conclusion: 二值RAR在提升模型事实性方面优于监督训练和连续奖励强化学习方法，且不会导致其他任务性能下降，具有更好的实用性和平衡性。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [282] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种基于自主性层级（L0-L3）的评估框架，以改进医学大语言模型在临床工作流中的可靠性和安全性评估。


<details>
  <summary>Details</summary>
Motivation: 现有医学大语言模型在标准基准上表现良好，但在实际临床应用中仍面临安全与可靠性挑战，缺乏与风险匹配的系统性评估方法。

Method: 通过引入自主性层级（L0-L3）框架，将现有基准和指标与各层级允许的操作及对应风险对齐，并提出分层级的评估蓝图。

Result: 建立了与临床风险相匹配的评估体系，明确了各层级应采用的指标、证据组合和声明报告方式，并将评估与监管需求相结合。

Conclusion: 以自主性为中心的评估框架有助于推动医学大语言模型从单纯分数导向转向可信、风险适配的临床应用验证。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [283] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文提出了FARE，一个基于大规模数据训练的生成式评估模型家族，通过简单的迭代拒绝采样监督微调方法，在多种推理评估任务中超越了现有的大型专用评估器。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于使用强化学习等新方法训练评估器，忽视了数据驱动的大规模数据开发。本文旨在探索数据规模对生成式评估器性能的影响。

Method: 构建包含250万样本、涵盖五种评估任务和多个推理领域的数据集，采用迭代拒绝采样的监督微调（SFT）方法训练8B和20B参数的FARE模型。

Result: FARE-8B可与更大的RL训练评估器竞争，FARE-20B在开源评估器中表现最佳，超过70B以上的专用评估器；在MATH任务中接近oracle性能，作为RL训练中的验证器提升模型性能达14.1%，持续微调的FARE-Code在测试用例质量评估上超过gpt-oss-20B达65%。

Conclusion: 数据规模化和简单的SFT方法能有效提升生成式评估器性能，FARE为开源评估器树立了新标准，并在实际应用中表现出色。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [284] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Enterprise Deep Research (EDR)的多智能体系统，用于将非结构化数据转化为可操作的洞察，其在无需人工干预的情况下，在多个开放性基准上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 企业面临将海量非结构化数据转化为有意义洞察的压力，而现有自主智能体在领域适应性、意图对齐和企业集成方面存在不足。

Method: EDR系统包含一个主规划智能体、四个专用搜索智能体（通用、学术、GitHub、LinkedIn）、基于MCP的可扩展工具生态系统（支持NL2SQL、文件分析和企业工作流）、可视化智能体以及具备知识缺口检测和研究方向更新能力的反思机制。

Result: 系统实现了自动化报告生成、实时流处理和无缝企业部署；在DeepResearch Bench和DeepConsult等开放性基准上表现优于当前最先进的智能体系统。

Conclusion: EDR为多智能体系统在企业级复杂信息检索与分析任务中的应用提供了有效框架，并开源了代码与数据集以推动相关研究发展。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [285] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个面向数学家的语义搜索工具，用于Lean和mathlib，通过理解用户意图显著提升定理检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有的Lean搜索工具依赖非正式化描述，难以匹配数学家的真实查询需求，阻碍了形式化定理证明的发展。

Method: 分析公开的Lean讨论语义并聚类，基于模拟用户意图的合成查询微调文本嵌入，并利用多样化反馈信号对齐数学家偏好。

Result: 在真实查询、非正式化语句和证明状态上的评估显示，相比现有工具和GPT-4o，Lean Finder相对性能提升超过30%。

Conclusion: Lean Finder有效提升了Lean生态中的定理检索能力，支持与基于大模型的定理证明器集成，推动形式化数学的发展。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [286] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 本文提出了一种名为LS-OGD的新型自适应控制框架，用于在概念漂移存在的情况下实现鲁棒的多模态学习。该方法通过在线控制器动态调整学习率和模态融合权重，理论上证明了预测误差的有界收敛性，并验证了其对模态特定漂移的鲁棒性和容错能力。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中因概念漂移而性能下降，尤其是模态特异性漂移和缺乏持续稳定适应机制的问题亟待解决。

Method: 提出LS-OGD框架，采用在线控制器动态调节模型的学习率和不同模态间的融合权重，以响应检测到的概念漂移和预测误差变化。

Result: 理论证明在有界漂移条件下，系统的预测误差最终一致有界，若漂移停止则收敛至零；实验表明该方法能有效隔离并缓解严重模态特定漂移的影响。

Conclusion: LS-OGD为构建可靠、持续适应的多模态学习系统提供了原理性基础，增强了系统在非平稳环境中的韧性与容错能力。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [287] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一种基于贝叶斯学习的自适应采样框架，能在保持响应质量的同时减少最多80%的采样量，显著提升大模型输出的效率。


<details>
  <summary>Details</summary>
Motivation: 在使用大语言模型时，通过生成多个响应样本可提升输出质量，但会带来额外计算开销。如何在准确性和效率之间平衡，决定何时停止采样，是一个关键挑战。

Method: 提出BEACON框架，结合序贯搜索与贝叶斯学习，实时更新奖励分布的后验信念，并根据进一步探索的边际效用是否超过计算成本来决定是否终止采样，无需额外训练。

Result: 理论证明了方法的最优性与可操作性，实验表明BEACON相比基线方法平均减少80%的采样次数，同时保持输出质量，并在偏好数据生成等任务中展现实际应用价值。

Conclusion: BEACON为大模型响应生成提供了一种高效、自适应的采样终止机制，能够在不牺牲性能的前提下大幅降低计算成本，具有良好的理论保证和广泛的应用前景。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [288] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: 本文提出PatMD，一种通过识别误判风险模式来提升多模态大模型对有害网络迷因检测能力的新方法，在6,626个迷因的基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（包括MLLM）难以准确识别通过反讽、隐喻等隐含修辞表达的有害内容，常导致误判。

Method: 构建包含误判风险模式的知识库，将迷因解构为可能导致误判的原因；针对目标迷因，检索相关模式并用于动态引导MLLM推理过程，避免已知误判陷阱。

Result: 在5项有害内容检测任务中，PatMD平均F1分数提升8.30%，准确率提升7.71%，表现出强泛化能力和检测性能。

Conclusion: PatMD通过显式建模和主动规避误判风险，有效提升了MLLM在复杂、隐晦有害迷因上的检测准确性。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [289] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 本研究提出了一种基于WaveNet的深度学习模型，用于自动分类EEG信号为生理、病理、伪迹和噪声四类，相较于传统的CNN和LSTM方法具有更高的分类精度。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家视觉审查的EEG信号分类方法因数据量和复杂性增加而变得不切实际，亟需自动化解决方案。

Method: 采用WaveNet架构，利用膨胀因果卷积和残差连接捕捉EEG信号的时间依赖性，并在公开标注数据集上进行训练与验证，数据集按70/20/10划分。

Result: 模型在分类准确率上超过此前的CNN和LSTM方法，并在区分噪声和伪迹方面表现优异；但在生理与病理信号间存在一定程度的误分类，归因于临床本身的重叠性。

Conclusion: WaveNet适用于EEG信号分类任务，具备处理细粒度和长程时间依赖的能力，结合有效预处理流程可提升模型泛化性能。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [290] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 本研究提出了一种利用键盘敲击动力学进行帕金森病远程筛查和监测的新方法，通过深度学习模型在多个数据集上实现了高精度的外部验证。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估存在局限性，亟需一种非侵入性、可扩展的生物标志物用于早期检测和持续监测。

Method: 研究采用三个阶段：数据预处理与不平衡处理、在大型数据集上预训练八种先进深度学习模型，并在中等规模数据集上微调，最后在一个独立队列上进行外部验证。

Result: 混合卷积-循环网络和基于Transformer的模型表现优异，AUC-ROC超过90%，F1分数超过70%；其中时序卷积模型在外部队列中达到91.14%的AUC-ROC，优于仅依赖内部验证的现有方法。

Conclusion: 键盘敲击动力学具有作为帕金森病可靠数字生物标志物的潜力，有望推动疾病的早期发现和连续远程监测。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [291] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的过滤算法EnSF在实时野火蔓延预测中的数据同化应用，展示了其在高维非线性问题中的高精度、稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高活跃野火的预测准确性，需要将观测数据与数值模型预测相结合，以实现有效的火灾管理。

Method: 采用基于得分生成扩散模型的集合得分滤波器（EnSF）进行数据同化，并应用于野火蔓延模型的实时预测。

Result: 数值实验表明，EnSF在准确性、稳定性和计算效率方面优于现有方法，是一种稳健且实用的野火数据同化方法。

Conclusion: EnSF为实时野火蔓延预测提供了一种高效可靠的数据同化方案，具有实际应用前景。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [292] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 本文研究了大语言模型（LLM）在处理工具返回的复杂JSON响应方面的能力，发现即使是最先进的模型在多种提示策略下仍面临挑战。研究创建了一个专门的数据集，并评估了15个开源和闭源模型，结果表明JSON处理性能受输出大小、响应处理策略和推理复杂度影响显著，不同方法之间的性能差异可达3%到50%。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在现实任务自动化中处理工具返回的结构化JSON数据的能力存在研究空白，需要系统评估其表现及影响因素。

Method: 构建了一个针对工具响应处理任务的数据集，并采用多种提示方法对15个开源和闭源大语言模型进行了评估。

Result: 实验结果显示，即使是前沿的大语言模型，在处理JSON响应时依然表现不佳；性能受到工具输出规模、类型以及所需推理复杂度的影响，不同处理策略间的性能差异显著（3%至50%）。

Conclusion: 结构化响应处理是当前大语言模型应用中的关键瓶颈之一，需根据输出特征和任务复杂性选择合适的处理策略以优化性能。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [293] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 本研究探讨了利用咖啡渣和枣核等废弃生物质通过热解制氢的潜力，并结合人工智能优化过程模型，提升了预测精度与效率。


<details>
  <summary>Details</summary>
Motivation: 探索未被充分利用的生物质资源在可持续能源生产中的应用，推动环保型氢能技术的发展。

Method: 采用热重分析、动力学建模（KAS、FWO、Friedman）及LSTM人工智能模型，对纯生物质及其混合物进行热解性能分析与TGA曲线预测。

Result: 混合物3具有最高的氢产率潜力，但活化能最高（313.24 kJ/mol）；混合物1活化能最低（161.75 kJ/mol）；KAS方法最准确；LSTM模型预测TGA曲线R²达0.9996–0.9998。

Conclusion: 结合AI的热解建模可显著提高生物质能源转化的精度与效率，不同配比混合物在氢产量和能耗之间存在权衡，为可持续能源开发提供了有效路径。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [294] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出了一种名为LAMI的联合图-语言建模框架，用于检测青少年和年轻人的非法药物使用，并通过结合图结构和调查语义生成可解释的风险因素分析。


<details>
  <summary>Details</summary>
Motivation: 现有模型方法将调查变量视为独立，忽略了它们之间的潜在关联结构，因此需要一种能够捕捉变量间复杂关系的新方法。

Method: LAMI将个体响应表示为关系图，通过专门的图结构学习层学习潜在连接，并结合大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验表明，LAMI在预测准确性上优于现有基线方法，并能揭示与家庭动态、同伴影响和学校相关压力等已知风险因素一致的行为子结构和心理社会路径。

Conclusion: LAMI不仅能提高非法药物使用的检测精度，还能提供可解释的、符合实际风险机制的行为洞察，有助于公共卫生干预策略的制定。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [295] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的推荐系统去偏学习框架BPL，通过双蒸馏策略在真实和反事实测试环境中均实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差，导致用户反馈不能完全反映真实偏好；现有去偏方法多专注于反事实环境，但在实际交互环境（事实环境）中表现不佳，因此需要一个能在两种测试环境下都表现良好的模型。

Method: 提出Bias-adaptive Preference distillation Learning (BPL)，采用教师-学生蒸馏从有偏模型中保留与观测数据一致的准确偏好知识，同时通过带可靠性过滤的自蒸馏机制迭代优化模型，提升在事实和反事实环境下的预测能力。

Result: 实验表明BPL在事实和反事实测试环境中均优于现有方法，实现了更全面的用户偏好建模。

Conclusion: BPL通过双蒸馏策略有效平衡了事实与反事实环境下的性能，为推荐系统的去偏学习提供了新思路。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [296] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一种基于曲率信任区域的参数高效微调框架，通过结合秩调度与稳定性感知优化，在有限计算和内存下提升大模型微调的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法通常将容量分配与训练过程中的更新动态脱节，缺乏对更新稳定性和动态资源分配的联合建模。

Method: 提出CTR-LoRA，利用轻量级二阶代理（如Fisher/海森矩阵度量）估计边际效用进行参数分配，并在曲率信任区域内约束更新，实现动态秩调度与稳定优化。

Result: 在7B-13B规模的开源模型上实验表明，CTR-LoRA在分布内和分布外基准上均优于强基线，具有更高准确率、训练稳定性、更低内存占用和更高吞吐量。

Conclusion: CTR-LoRA为参数高效微调提供了更稳健、可部署的解决方案，位于性能与效率的帕累托前沿。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [297] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型参数高效微调（PEFT）的加速系统Long Exposure，通过识别并利用一种新的稀疏性模式“Shadowy Sparsity”，实现了最高达2.49倍的端到端微调加速。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）技术在处理大语言模型时存在效率低下问题，尤其是在时间开销和运行成本方面，且未充分考虑一种新型的稀疏性模式——Shadowy Sparsity。

Method: 提出了Long Exposure系统，包含三个核心组件：Shadowy-sparsity Exposer用于捕捉更细粒度的稀疏特征；Sequence-oriented Predictor用于高效准确地处理长序列输入和动态变化的参数；Dynamic-aware Operator则优化了计算结构和内存访问模式，以应对动态稀疏操作。

Result: 实验表明，Long Exposure在端到端微调中相比现有最先进方法最高可实现2.49倍的速度提升。

Conclusion: Long Exposure有效解决了PEFT中的Shadowy Sparsity问题，显著提升了大语言模型微调的效率，为参数高效微调提供了新的优化方向。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [298] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为Deadlock Attack的资源耗尽攻击方法，通过训练恶意对抗嵌入诱导大推理模型陷入无限推理循环，揭示了链式思维推理机制中的新型安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代大推理模型依赖链式思维进行多步推理，但这种迭代机制引入了新的安全风险。作者旨在探索并揭示此类模型在推理效率方面的潜在脆弱性。

Method: 设计一种对抗性嵌入训练方法，利用过渡词（如'Wait'、'But'）阻止模型结束推理；针对连续到离散投影间隙问题，引入后门植入策略，通过特定触发词可靠激活攻击。

Result: 在四个先进大推理模型和三个数学推理基准上实现100%攻击成功率，迫使模型生成达到最大token限制，且对良性输入影响极小，具有隐蔽性和鲁棒性。

Conclusion: Deadlock Attack暴露了大推理模型在推理控制流方面的关键安全漏洞，凸显了推理效率安全性的重要研究方向。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [299] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出一种分辨率感知的检索增强型预测模型，通过利用空间相关性和时间频率特征，在无历史数据条件下显著提升微气候预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法在零样本预测（即对无历史数据的新条件进行预测）中表现不佳，尤其是在微气候等复杂环境中，需要更高效和可扩展的方法。

Method: 将信号分解为不同频率成分，采用分辨率感知的检索机制：低频成分依赖更广的空间上下文，高频成分关注局部影响，从而动态检索相关信息并适应新地点。

Result: 在ERA5数据集上，该模型比HRRR降低了71%的MSE，比Chronos降低了34%的MSE，显著优于传统方法、数值天气预报和现代基础时间序列模型。

Conclusion: 检索增强与分辨率感知策略有效提升了零样本预测性能，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [300] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出了一种面向开放集场景的细粒度联邦域自适应方法Gains，通过模型拆分实现知识发现与适应，并有效保持源域性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习假设封闭世界，难以应对新客户端持续加入并引入新知识的现实场景，现有方法在知识发现粒度、源域性能保持和适应效率方面存在不足。

Method: 将模型分为编码器和分类器，发现编码器特征对域偏移敏感而分类器参数对类别增量敏感；据此设计细粒度知识发现、贡献驱动聚合和防遗忘机制。

Result: 在三种典型数据偏移场景下的多域数据集上，Gains在源域和目标域客户端性能均显著优于基线方法。

Conclusion: Gains通过细粒度建模和机制设计，有效实现了开放环境中联邦学习的知识发现与适应，兼顾性能与稳定性。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [301] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出了一种结合自注意力机制和U-Net结构的新型傅里叶神经算子（SAU-FNO），用于高效准确的3D IC热预测，相比传统FEM方法提速842倍。


<details>
  <summary>Details</summary>
Motivation: 传统基于PDE求解的方法虽然精度高但速度慢，现有机器学习方法存在高频信息丢失和对高保真数据依赖的问题。

Method: 将自注意力机制与U-Net结构融入FNO框架，并采用迁移学习微调低精度数据以减少对高精度数据的依赖。

Result: 实验表明SAU-FNO在热预测精度上达到SOTA水平，并比传统FEM方法快842倍。

Conclusion: SAU-FNO是一种高效且精确的3D IC热管理仿真工具，适用于迭代设计流程。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [302] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一种用户反馈模拟框架和多领域、多语言、多任务的综合基准，用于评估大语言模型系统的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型系统在扩展数据、参数和计算资源方面接近瓶颈，且现有基准多关注长文本阅读理解任务，缺乏对服务过程中从用户反馈中学习能力的评估。

Method: 设计了一个用户反馈模拟框架，并构建了一个涵盖多个领域、语言和任务类型的综合基准，以评估大语言模型系统的持续学习能力。

Result: 实验表明，当前最先进的基线方法在该基准上的表现远未达到令人满意的程度。

Conclusion: 所提出的基准有望为未来大语言模型的记忆机制和优化算法研究提供方向。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [303] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型的代理框架LinearizeLLM，用于自动线性化非线性优化问题。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的重表述通常依赖人工且需要专业知识，限制了自动化求解的发展。

Method: 将每个非线性模式分配给一个专门的“重表述代理”，由大语言模型指导其推导精确的线性化形式，并通过代理协作构建等效的线性模型。

Result: 在包含20个真实非线性问题的数据集上验证了方法的有效性，多个大语言模型均能成功完成线性化任务。

Conclusion: 专用的大语言模型代理可有效自动化非线性优化问题的线性化过程，推动实现对话式建模流程。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [304] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文提出使用持久同调（persistent homology）来量化训练数据的拓扑特征，以评估其几何结构对模型学习效果的影响，强调数据表示的丰富性和去冗余性对机器学习的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管已知不同类型的数据对训练有效，但数据的几何结构如何影响模型性能仍缺乏研究。作者希望探索数据的拓扑结构对学习结果的影响。

Method: 利用持久同调从度量空间中的数据提取拓扑特征，提供一种超越基于熵的度量方法来量化数据多样性。

Result: 发现持久同调能有效分析和增强用于AI系统的训练数据，揭示其在衡量数据丰富性和冗余性方面的潜力。

Conclusion: 持久同调是一种强大的工具，可用于分析和优化机器学习中的训练数据几何结构，提升模型性能。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [305] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 本文提出了一种名为PALE的新框架，利用提示引导的LLM响应进行数据增强，以低成本生成真实和幻觉数据，并引入对比马氏距离（CM Score）来有效检测大语言模型中的幻觉，无需额外人工标注，在实验中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型常产生幻觉问题，且缺乏高质量标注数据集，亟需一种高效、可扩展的幻觉检测方法。

Method: 提出PALE框架，通过提示引导LLM生成真实与幻觉数据作为增强；设计基于激活空间分布建模的对比马氏距离（CM Score）来评估中间嵌入的真实性。

Result: PALE在实验中比强基线方法性能提升6.55%，能有效检测幻觉，且无需人工标注，具备良好泛化性和实用性。

Conclusion: PALE为大语言模型的幻觉检测提供了一个高效、实用的解决方案，尤其适用于缺乏标注数据的实际应用场景。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [306] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出一种级联方法，结合大规模预训练开放词汇检测模型与轻量级少样本分类器，通过FLAME主动学习策略实现遥感图像中高精度、快速适应的物体检测。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测模型在遥感等专业领域因自然语言歧义和细粒度类别难以区分（如‘渔船’与‘游艇’），导致零样本性能不足，影响如非法捕捞监测等关键应用。

Method: 采用级联框架：先用零样本检测模型生成高召回候选框，再利用用户少量标注样本实时训练轻量分类器进行精细化筛选；核心为FLAME主动学习策略，通过密度估计和聚类选择靠近决策边界的不确定性样本以保证多样性和信息量。

Result: 在遥感基准上持续超越现有最先进方法，实现高准确率，且适应时间小于一分钟，显著快于现有方案，同时大幅降低标注成本。

Conclusion: 该方法构建了一个实用、资源高效的框架，能够快速将基础模型适配到特定用户需求，在保持高召回的同时显著提升精度，适用于专业领域的开放词汇检测任务。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [307] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 本文提出了一种新的气象预测框架DAWP，通过引入人工智能数据同化（AIDA）模块，使AI气象预测（AIWP）能够在完整的观测空间中运行，从而摆脱对再分析数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的AI气象预测方法依赖于再分析数据，存在数据同化偏差和时间不一致等问题，限制了其性能和应用。因此，需要一种新的方法来直接利用不规则的高分辨率观测数据进行预测。

Method: 提出DAWP框架，包含AIDA模块和AIWP模型。AIDA采用掩码多模态自编码器（MMAE）和掩码ViT-VAE编码不规则卫星观测数据；AIWP采用时空解耦Transformer并结合跨区域边界条件（CBC），实现基于子图像的全球观测预测。

Result: 实验表明，AIDA初始化显著提升了AIWP的展开能力和效率，DAWP在全球降水预报任务中展现出良好的应用潜力。

Conclusion: DAWP为摆脱再分析数据依赖的气象预测提供了有效解决方案，推动了基于纯观测数据的AI天气预报发展。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [308] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出了一种名为Cog-Rethinker的分层元认知强化学习框架，通过两阶段推理机制提升大语言模型在数学推理任务中的样本利用率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定提示模板的零强化学习方法在弱语言模型上存在采样效率低的问题，大量问题在准确率筛选中产生无效输出，导致样本浪费。

Method: 设计了一个分层元认知的强化学习框架Cog-Rethinker，在直接rollout后引入两个阶段：第一阶段将零准确率问题分解为子问题进行求解；第二阶段利用先前错误答案进行反思优化。并通过监督微调实现冷启动和训练测试一致性。

Result: 实验表明，Cog-Rethinker在多个数学推理基准上优于基线方法，显著提高了样本效率并加快了收敛速度。

Conclusion: Cog-Rethinker有效提升了弱语言模型在强化学习推理中的样本利用率和性能，为构建高效推理系统提供了新思路。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [309] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种新的知识蒸馏框架AMiD，通过引入可调参数α的混合辅助分布和广义散度族，解决了传统方法在高维输出下存在的容量差距和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法因语言模型输出维度高导致接近零概率的问题，引发训练不稳定和师生模型间容量差距，现有辅助分布方法缺乏系统性研究。

Method: 提出α-混合辅助分布作为连续扩展的新型辅助分布，并设计基于最优性的广义散度族，构建统一的知识蒸馏框架AMiD。

Result: 实验表明，AMiD在多种设置下相比以往方法具有更好的性能和更高的训练稳定性。

Conclusion: AMiD通过扩展且理论支持的辅助分布空间，为知识蒸馏提供了更优、更稳定的解决方案。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [310] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出MEET-Sepsis框架，结合MERE机制和CDTA模块，实现仅用20% ICU监测时间即可高精度早期预测脓毒症。


<details>
  <summary>Details</summary>
Motivation: 早期脓毒症症状隐匿、进展迅速，现有AI方法难以捕捉微弱的时间信号，导致预测不及时。

Method: 引入多内源视图表征增强（MERE）机制构建丰富特征视图，结合级联双卷积时间序列注意力（CDTA）模块进行多尺度时序建模。

Result: 在仅使用现有最先进方法20%监测时间的情况下，实现了具有竞争力的预测精度，显著提升早期预测效率。

Conclusion: MEET-Sepsis框架能高效捕捉早期弱信号，大幅缩短预测所需时间，为临床早期干预提供有力支持。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [311] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 本研究提出一种基于聚类的可解释人工智能方法，用于根据睡眠障碍特征对患者进行分组，并识别影响这些疾病的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者的健康和生活质量有重大影响，但由于症状多样，诊断复杂，因此需要更有效的分析方法。

Method: 采用基于聚类的方法对患者进行分型，并结合可解释人工智能（XAI）技术识别关键影响因素。

Result: 在匿名真实数据上的实验验证了该方法的有效性和相关性。

Conclusion: 所提出的可解释聚类方法有助于更好地理解不同睡眠障碍的特征，支持临床决策。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [312] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 本文提出了一种追踪和操控大语言模型推理过程中底层算法原语的框架，通过分析内部激活模式来识别与多步推理相关的神经表征，并利用函数向量方法提取可重用的推理基元。在多个任务和模型上的实验表明，这些基元具有跨任务和跨模型的可迁移性，且推理微调能增强基元的组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何通过潜在表示和推理时计算实现多步推理，揭示其背后是否依赖于可分解、可组合的算法原语。

Method: 通过聚类神经激活并标注对应的推理轨迹来操作化算法原语，使用函数向量方法从残差流中提取原语向量，并通过注入这些向量观察对推理步骤和性能的影响。

Result: 发现了支持推理的几何结构化的算法原语，这些原语可在不同任务（如TSP、3SAT、AIME、图导航）和模型（Phi-4、Phi-4-Reasoning、Llama-3-8B）之间迁移；推理微调提升了原语的系统性使用和组合泛化能力。

Conclusion: 大语言模型的推理可能依赖于一组可组合的算法原语，这些原语在激活空间中呈现几何逻辑，且推理微调增强了模型对这些原语的泛化与组织能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [313] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文研究了基于GRPO算法的强化学习方法在提升大语言模型推理能力时的局限性，发现其改进受限于预训练模型的分布偏差，仅在任务与预训练偏置一致时才能实现分布外泛化，因此GRPO本质上是强化而非拓展模型能力。


<details>
  <summary>Details</summary>
Motivation: GRPO在不同推理领域表现不一致，某些领域提升明显而其他领域停滞，亟需理解其何时能有效提升推理能力并实现分布外泛化。

Method: 从数据分布视角出发，理论证明GRPO是一种保守的重加权策略，并通过从零训练Transformer模型进行受控实验，评估其在推理深度、输入长度、token表示和组合性等方面的泛化能力。

Result: 理论和实验表明，GRPO无法发现全新解法，其分布外改进仅在目标任务与模型预训练偏置对齐时出现，而在分布内任务上的增益随性能饱和而减弱。

Conclusion: GRPO并非通用推理增强工具，而是强化预训练偏置的方法；未来应开发能突破预训练限制的新算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [314] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: 提出了一种名为Stratos的端到端大语言模型蒸馏管道，可自动化服务器与模型选择、知识蒸馏和部署，显著提升特定领域任务性能并降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏框架依赖人工干预，难以满足用户在性能、延迟和预算等多约束下的定制化需求。

Method: Stratos通过自动选择帕累托最优服务器、动态匹配师生模型对，并根据任务复杂度自适应调整蒸馏策略，实现高效蒸馏与云部署。

Result: 在麻将推理任务上，学生模型准确率是GPT-4o教师基线的四倍，同时降低了延迟和成本且不牺牲准确性。

Conclusion: Stratos为垂直领域大语言模型的高效、低成本部署提供了可行方案，具有广泛应用前景。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [315] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 本文提出使用Kolmogorov-Smirnov（KS）检验来实时监测机器学习系统中测试数据分布偏移的问题，并利用KS距离量化该偏移对AI代理性能的影响。实验结果显示，即使KS距离很小（如0.02），也可能导致强化学习代理在单个交叉路口的通行时间增加约50%，表明KS距离是一种有效的统计工具，可用于智能交通系统中AI性能退化的实时评估。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中测试数据的分布可能与训练数据显著不同，导致AI/ML系统预测误差增大，影响其可靠性与安全性，因此需要一种有效方法来实时监测和量化这种分布偏移。

Method: 采用Kolmogorov-Smirnov（KS）检验来衡量数据分布的变化，并利用KS距离量化分布偏移的程度及其对AI代理性能的影响，特别是在强化学习控制的交通场景中进行验证。

Result: KS距离能有效反映分布偏移，即使较小的KS值（如0.02）也会导致强化学习代理在交通控制中的性能显著下降（例如通行时间增加约50%）。

Conclusion: KS检验和KS距离可作为AI系统中实时监测分布偏移及其性能影响的有效统计工具，尤其在智能交通等安全关键领域具有重要应用前景。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [316] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解和截断正则化的自然梯度方法ANaGRAM的多截断自适应策略，用于提升物理信息神经网络（PINN）的训练效果，并通过谱理论框架为正则化的必要性提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有的研究表明自然梯度方法在训练PINN时优于标准优化器，但其训练动态尚需深入理解，且现有方法仍有改进空间。

Method: 采用受自然梯度启发的ANaGRAM方法，结合奇异值分解与截断正则化，提出多截断自适应策略，并基于谱理论建立理论框架以解释正则化的必要性及其与格林函数理论的联系。

Result: 在基准偏微分方程上的实验表明，所提方法能有效提升ANaGRAM性能，部分实验可达到机器精度。

Conclusion: 多截断自适应策略显著提升了ANaGRAM在PINN训练中的表现，且谱理论框架为方法的设计提供了坚实的理论依据。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [317] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知的在线估计器，用于高效动态评估训练样本的影响，显著降低了计算开销，提升了数据中心学习的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模型收敛后的静态影响评估，忽略了训练过程中样本影响的动态变化，尤其在深度模型中这一问题更为突出。

Method: 设计了一种仅需损失到输出梯度的层感知在线估计器，避免了参数级和全网络梯度计算，同时保持了影响排序的准确性。

Result: 在大语言模型预训练、微调和图像分类任务上实验表明，该方法在显著降低时间和内存成本的同时提高了模型准确率。

Conclusion: 所提方法实现了高效且可扩展的动态数据筛选，为数据中心学习提供了实用的解决方案。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [318] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 本文提出了一种名为STAR的即插即用模块，用于增强时间序列基础模型（TSFM）在多变量时间序列异常检测中对状态变量的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM通常忽视状态变量的类别特性及其作为条件的关键作用，导致引入状态变量后检测性能下降。

Method: STAR包含三个核心组件：1）基于身份引导的状态编码器，通过可学习的状态记忆捕捉状态变量的复杂语义；2）条件瓶颈适配器，根据当前状态动态生成低秩适配参数，将状态影响灵活注入主干模型；3）数值-状态匹配模块，用于更有效地检测状态变量自身的异常。

Result: 在真实世界数据集上的大量实验表明，STAR能够显著提升现有TSFM在多变量时间序列异常检测中的性能。

Conclusion: STAR有效解决了TSFM在处理包含离散状态变量的时间序列时的局限性，提升了异常检测效果。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [319] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的端到端框架，通过隐式最大似然估计和可微决策层优化传感器布局与洪水预测模型，以最小化应急响应决策遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定策略进行传感器部署和模型训练，忽略了相同感知增益和预测误差下可能导致不同决策的问题，影响应急响应效果。

Method: 构建包含上下文评分网络、可微传感器选择模块、时空洪水重建与预测模型及可微决策层的端到端框架，结合I-MLE实现离散传感器配置的梯度学习，并使用概率决策头逼近多种约束性灾害响应任务。

Result: 该框架在传感器预算受限的情况下，能更有效地优化下游决策性能，显著降低决策遗憾，提升洪水应急响应的可靠性与及时性。

Conclusion: 决策导向的联合优化方法优于传统任务无关的感知与建模策略，为数据受限下的灾害管理提供了新范式。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [320] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 本研究首次将渐进式神经网络（PNNs）应用于基于深度强化学习（DRL）的多保真流体流动控制，系统评估了其与传统微调方法在知识迁移中的表现。研究表明，PNNs在保留源策略知识的同时实现稳定高效的迁移，显著优于易发生灾难性遗忘的微调方法，尤其在源域与目标域差异较大时仍保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 加速深度强化学习在复杂流体控制中的训练过程，解决传统迁移方法在多保真环境中易出现灾难性遗忘和适应性差的问题。

Method: 采用渐进式神经网络（PNNs）和多种传统微调策略，在Kuramoto-Sivashinsky（KS）系统上进行基准测试，比较其收敛速度、性能增益及知识保留能力，并通过层敏感性分析探究特征迁移机制。

Result: PNNs能稳定高效地迁移低精度环境下的控制策略至高精度环境，显著加快收敛且不易过拟合；相比微调方法更鲁棒，即使在物理机制或控制目标不匹配的情况下仍有效。层分析显示PNNs可动态复用浅层特征并调整深层以适应新任务。

Conclusion: PNNs为多保真流控任务提供了鲁棒、可扩展且计算高效的迁移学习框架，具备向更复杂流动问题推广的潜力。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [321] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: 本研究提出了一种名为AirDbM的基于形变的翼型优化方法，通过从UIUC数据库中选择12个最优基准翼型，有效降低了设计空间维度，在气动优化和强化学习中表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 为了在翼型几何优化中以尽可能少的设计变量探索更广泛的设计空间，需要一种能系统性降低设计维度的方法。

Method: AirDbM采用Design-by-Morphing策略，从包含1600多个翼型的UIUC数据库中逐次选择能最大程度提升设计能力的基准翼型，最终确定12个最优基线，并用其重构整个数据库。

Result: AirDbM以仅12个基准实现了对99%数据库的重构，平均绝对误差低于0.005；在多目标优化中收敛更快，获得更大的Pareto前沿超体积，并发现具有更高升阻比的新解；在强化学习中相比传统参数化方法表现出更强适应性。

Conclusion: AirDbM是一种高效、低维且适应性强的翼型参数化方法，在传统优化和机器学习驱动设计中均具有广泛应用潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [322] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出一种基于特征驱动的强化学习方法，用于光伏电力在连续现货市场的日内交易，通过结合数据驱动特征和序列决策框架，优化 bidding 策略，在平衡交易利润与不平衡惩罚的同时，实现优于基准模型的表现。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电量和短期电价的高度不确定性，需要有效的策略来提升收入并减少不平衡成本。连续现货市场虽提供实时调整机会，但缺乏高效、可解释且实用的交易策略。

Method: 将问题建模为马尔可夫决策过程，采用近端策略优化（PPO）算法，结合以市场微观结构和历史特征为主的数据驱动状态表示，训练一个以线性结构为主的可解释策略网络。

Result: 在真实历史数据上进行训练和样本外测试，该方法在多种场景下均显著优于基准策略，表现出快速收敛、实时推理能力和透明的决策规则；学习到的权重表明市场微观结构和历史特征对决策至关重要。

Conclusion: 特征驱动的强化学习为光伏电站参与连续现货市场提供了一条实用、数据高效且可部署的路径，具备良好的实际应用前景。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [323] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出了一种基于信息瓶颈的微调方法（IB-FT），以克服大语言模型在代码生成中因过度记忆导致的优化困境，显著提升了单样本和多样本代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 发现预训练大模型在代码领域微调时存在“记忆障碍”问题，即模型过度记忆下游数据，阻碍了对新知识的有效学习。

Method: 引入信息瓶颈（IB）惩罚机制，在微调过程中压缩隐藏表示中的冗余记忆特征，保留任务相关的信息，从而提升模型泛化能力。

Result: 在OriGen和Evol-CodeAlpaca-V1两个基准上验证了IB-FT的有效性，显著提高了Pass@1和更严格的Pass@k^(m)指标下的稳定性与性能。

Conclusion: IB-FT能有效缓解记忆障碍，增强代码生成模型的学习能力和鲁棒性，优于传统微调方法。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [324] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: 本文提出了PolyConFM，首个以构象为中心的聚合物基础模型，通过掩码自回归生成预训练方法统一聚合物建模与设计，显著提升多种下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示聚合物，忽略其全局构象结构信息，且缺乏通用的基础模型，限制了实际性能和领域发展。

Method: 提出PolyConFM模型，将聚合物构象分解为局部构象序列，采用条件生成范式进行预训练，通过掩码自回归（MAR）重建局部构象并生成取向变换以恢复整体构象；构建首个高质量聚合物构象数据集支持预训练。

Result: 实验表明，PolyConFM在多种下游任务中 consistently 优于代表性任务特定方法。

Conclusion: PolyConFM作为首个以构象为中心的聚合物基础模型，为聚合物科学提供了通用且强大的工具，推动该领域的进一步发展。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [325] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出了一种可推广的因果机器学习流程，用于从大规模电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 为了在大规模医疗数据中实现可复用的因果推断，支持医学发现。

Method: 将不完美的多模态临床数据处理并分解为概率独立的潜在源，进而训练任务特定的因果模型以估计个体因果效应。

Result: 在两个真实世界应用中验证了该方法的通用性和实用性。

Conclusion: 该因果机器学习 pipeline 能有效挖掘电子健康记录中的潜在因果关系，具有大规模医学发现的潜力。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [326] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种针对图神经网络（GNN）社交机器人检测器的对抗性多智能体强化学习框架RoBCtrl，利用扩散模型生成高保真机器人账户，并通过多智能体强化学习模拟其对抗行为，有效削弱了现有检测方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN-based社交机器人检测方法面临黑盒性、对社交代理控制有限以及机器人异质性等问题，其鲁棒性和脆弱性尚未被充分研究。因此，需要一种更有效的攻击框架来评估这些检测系统的安全性。

Method: 提出RoBCtrl框架：首先使用扩散模型重构真实账户数据生成高保真机器人账户；然后采用多智能体强化学习（MARL）模拟不同类别机器人的对抗行为，结合基于结构熵的分层状态抽象以加速学习过程，优化连接策略。

Result: 在多个社交机器人检测数据集上的实验表明，该框架能显著降低GNN-based检测器的性能，展现出强大的攻击有效性。

Conclusion: RoBCtrl是首个面向GNN-based社交机器人检测器的多智能体强化学习攻击框架，首次将扩散模型应用于模拟进化型社交机器人行为，揭示了当前检测系统在面对协同自适应攻击时的脆弱性。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [327] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出了一种受大脑启发的网格状代码量化（GCQ）方法，用于压缩观测-动作序列，通过吸引子动力学中的网格状模式实现时空联合压缩。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法仅处理静态输入，难以有效建模时空动态；希望开发一种能同时压缩空间和时间信息的统一世界模型。

Method: 利用连续吸引子神经网络生成网格状编码，通过动作条件化的码本动态选择码字，实现观测-动作序列的离散化表示与时空压缩。

Result: GCQ能在多种任务中实现紧凑编码，并支持长视野预测、目标导向规划和逆向建模，表现出优越的下游性能。

Conclusion: GCQ不仅是一种高效的序列建模工具，也为理解神经系统中网格状编码的形成提供了理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [328] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: 本文提出了AMS-Quant，首次将浮点量化从整数位宽扩展到非整数位宽，通过尾数位共享和自适应搜索技术，在几乎无精度损失的情况下显著加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量巨大，带来存储和推理效率瓶颈，现有量化方法多局限于整数位宽，难以逼近量化最优解。

Method: 提出AMS-Quant，包含尾数位共享（Mantissa-bit Sharing）和自适应搜索（Adaptive Searching）两项技术，并设计了高效的CUDA线性层内核以减少内存访问延迟。

Result: 实验表明AMS-Quant可将模型量化至FP-5.33-e2m3和FP4.25-e2m2，相比FP16推理速度提升2.8倍和3.2倍，且精度损失可忽略。

Conclusion: AMS-Quant通过非整数位宽浮点量化有效平衡了大模型的推理效率与精度，为LLM高效部署提供了新方向。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [329] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: 本文提出了GUIrilla，一个通过原生辅助功能API自动探索应用程序的可扩展框架，用于解决GUI自动化中的数据收集难题，并发布了包含27,171个任务的大规模数据集GUIrilla-Task，显著提升了基于LLM的智能体在UI任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI自动化研究面临数据稀缺问题，主要由于手动标注成本高、数据集闭源以及合成数据管道表层化，且macOS生态系统的数据代表性不足。

Method: 设计了一个名为GUIrilla的自动化框架，利用原生辅助功能API系统性地探索macOS应用程序，构建分层的GUI图并采用专用交互处理器实现全面覆盖，进而构建GUIrilla-Task数据集。

Result: 使用GUIrilla-Task微调的基于LLM的智能体在ScreenSpot Pro基准上显著优于合成基线方法，且仅使用其3%的数据量；同时发布了GUIrilla-Task数据集、GUIrilla-Gold基准、macapptree库及框架代码。

Conclusion: GUIrilla有效解决了GUI自动化中数据收集的瓶颈问题，为桌面自动化智能体的研究提供了重要资源，推动了跨平台自主代理的发展。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [330] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 本文讨论了基于图神经网络的交通预测模型的发展及其在处理突发事件时的局限性，强调了手动特征工程对专家知识的依赖性和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵加剧，需要更可靠和响应迅速的交通预测模型来支持智能交通系统。

Method: 采用深度学习尤其是图神经网络（GNN）来捕捉道路网络中的复杂空间依赖关系和交通流数据中的动态时间演化模式，并尝试引入事件信息以提升模型响应能力。

Result: STGCN、GraphWaveNet、STWave和D2STGNN等模型在标准交通数据集上表现出色，尤其擅长捕捉具有周期规律的交通模式；但基于人工设计特征的方法在应对复杂未知事件时泛化能力有限且语义细节丢失严重。

Conclusion: 尽管现有GNN模型在常规交通预测中表现优异，但在处理突发事件时仍受限于人工特征工程，亟需更自动化、语义丰富且可泛化的事件建模方法。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [331] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 本文研究了五个近期时间序列基础模型和两个基线模型的校准特性，发现这些基础模型在校准性上表现更优，且不像其他深度学习模型那样存在系统性过自信问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在预测性能上表现出色，但其校准特性在实际应用中至关重要，却尚未被充分探索。

Method: 对五种近期时间序列基础模型和两种竞争性基线模型进行了系统性评估，分析其校准性、不同预测头的影响以及长期自回归预测下的校准表现。

Result: 时间序列基础模型在校准性上 consistently 优于基线模型，通常不存在系统性的过自信或欠自信现象。

Conclusion: 时间序列基础模型具有更好的校准特性，适合对不确定性估计要求较高的实际应用场景。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [332] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出一种基于分层图神经网络的变电站级电压估计方法，利用电气拓扑和物理特征，在低可观测性条件下实现高精度、可扩展的配电网状态估计。


<details>
  <summary>Details</summary>
Motivation: 传统配电网状态估计（DSSE）方法在测量稀疏和网络规模大的情况下难以保持准确性和可扩展性，随着分布式能源（DER）渗透率增加，对鲁棒性电压估计的需求日益迫切。

Method: 提出一种分层图神经网络（GNN）模型，结合配电网的电气拓扑结构和物理特征，用于变电站级电压估计，并在公开的SMART-DS数据集上进行训练与验证。

Result: 实验表明，该方法相比其他数据驱动模型RMSE降低高达2倍，即使在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络具有潜力实现可扩展、可复现、数据驱动的配电网电压监测，适用于大规模、低可观测性的实际场景。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [333] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流求解方法，利用图神经网络学习非线性修正项，在保持精度的同时显著提升求解速度。


<details>
  <summary>Details</summary>
Motivation: 非线性AC最优潮流（AC OPF）求解在实时电网运行中计算开销大，制约实际应用，需更高效的方法。

Method: 采用残差学习范式，以快速DC OPF解为基线，使用拓扑感知的带局部注意力机制的图神经网络学习非线性修正；结合两级DC特征融合和物理信息损失函数，确保AC潮流可行性和运行约束。

Result: 在57、118和2000节点系统上的实验显示，相比传统AC OPF求解器，MSE降低约25%，可行性误差减少最高达3倍，运行时间加速最高达13倍；模型在N-1故障下保持准确性，并可有效扩展到大规模网络。

Conclusion: 残差学习是连接线性近似与AC可行OPF的实用且可扩展的桥梁，有望实现接近实时的电网运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [334] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出了一种新的个性化联邦学习框架FedPURIN，通过整数规划选择关键参数并结合稀疏聚合，显著降低通信开销，同时保持良好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法在应对数据异构性的同时，往往面临通信效率低下的问题，限制了实际部署。

Method: 提出FedPURIN框架，采用整数规划方法识别关键参数进行传输，并结合稀疏聚合策略以减少通信量。

Result: 在多个非独立同分布条件下的图像分类基准上验证了方法的有效性，通信量显著减少，同时性能与当前最优方法相当。

Conclusion: FedPURIN为通信高效的个性化联邦学习提供了新范式，特别适用于边缘智能系统中异构数据环境的应用。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [335] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 本文提出了一种用于三维非结构化点云上计算流体动力学（CFD）的多尺度神经算子（MNO），通过显式分解全局、局部和微观三个尺度的信息，在不规则域上显著提升了预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理不规则域上的多尺度流体流动时存在精度不足和可扩展性差的问题，亟需一种能有效捕捉多尺度特征且计算高效的模型。

Method: 提出多尺度神经算子（MNO），包含维度压缩的全局注意力模块、局部图注意力模块和逐点微观注意力模块，分别建模长程依赖、邻域交互和细粒度细节。

Result: 在四个涵盖稳态与非稳态流动的基准任务上验证，MNO在最多达30万点的场景下，预测误差比现有最优方法降低5%至40%，表现出更强的鲁棒性和可扩展性。

Conclusion: 显式的多尺度设计对神经算子至关重要，MNO为在不规则域上学习复杂流体动力学提供了一个高效且可扩展的框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [336] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 提出基于随机矩阵理论（RMT）的Transformer训练动态分析框架，利用自注意力矩阵谱密度的重尾分布演化定义训练三阶段，并提出无需验证集的早停准则。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练过程中性能提升的底层机制，并建立可解释、无需验证集的训练监控与早停方法。

Method: 基于随机矩阵理论分析浅层自注意力矩阵V的谱密度演化，使用幂律拟合探测其重尾特性，划分训练三阶段，并提出两个基于谱特征的验证-free早停准则。

Result: 观察到自注意力矩阵谱密度向重尾分布演化；成功划分训练三阶段；提出的定量指标与新谱特征能一致地指示收敛，且与实际性能高度对齐。

Conclusion: RMT为理解和监控Transformer训练动态提供了有力理论工具，所提准则可有效指导早停，减少过拟合并提升训练效率。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [337] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 本文提出了一种基于ADAROUND的QUBO公式用于密集神经网络的后训练量化方法，通过优化权重和偏置的舍入选择，在多种数据集上实现了优于传统四舍五入量化的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在不重新训练的情况下有效降低神经网络的计算和存储开销，需要高效的后训练量化方法。现有方法在低精度量化时性能下降明显，因此需要更优的舍入策略。

Method: 以理论输出与去量化输出之间的Frobenius距离为目标函数，构建一个二元变量表示每个权重和偏置舍入选择的QUBO模型，并利用QUBO系数矩阵结构将其分解为多个独立子问题，使用模拟退火等启发式方法高效求解。

Result: 该方法在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上从int8到int1精度范围内均优于传统的四舍五入量化方法，尤其在低比特设置下表现更优。

Conclusion: 所提出的基于QUBO的ADAROUND方法为后训练量化提供了一种有效的舍入策略，能够在极低精度下保持模型性能，具有良好的应用潜力。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [338] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 本文提出了一种名为CONEC-LoRA的持续知识整合低秩自适应方法，用于解决领域增量学习中的灾难性遗忘和任务特定LoRA选择不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在领域增量学习中忽视了跨任务的共享知识，且依赖泛化能力较差的线性或原型分类器，导致推理时准确性下降。

Method: CONEC-LoRA结合任务共享LoRA和任务特定LoRA以分别提取共性与领域特有知识；引入随机分类器从分布中采样参数以提升分类准确性；设计辅助网络预测最优任务特定LoRA，并采用不同深度网络结构结合局部分类器利用中间表示；通过球生成器损失和变换模块缓解合成样本偏差。

Result: 在四个流行基准上实验表明，CONEC-LoRA性能优于先前方法，平均提升超过5%。

Conclusion: CONEC-LoRA通过知识整合、随机分类器和辅助网络结构有效解决了领域增量学习中的关键挑战，显著提升了模型性能与泛化能力。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [339] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为PassREfinder-FL的新框架，利用图神经网络和联邦学习预测跨网站的凭证填充风险，能够在保护用户隐私的同时高效识别密码重用行为。


<details>
  <summary>Details</summary>
Motivation: 由于用户在多个网站重复使用密码，导致凭证填充攻击频发，现有方法因影响可用性和依赖复杂的账户共享机制而难以部署。

Method: 引入“密码重用关系”概念，构建网站图并使用图神经网络进行链接预测；结合联邦学习保护用户隐私，并通过公开网站信息扩展模型至新网站。

Result: 在包含3.6亿泄露账户的真实数据集上，PassREfinder-FL在联邦学习设置下F1-score达到0.9153，比现有GNN模型性能提升4-11%。

Conclusion: 该方法可有效预测跨站密码重用风险，生成可操作的风险评分，且具备良好的可扩展性与隐私保护能力。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [340] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 提出了一种适用于离散和连续复值波系统的平衡传播（EP）学习方法，可在弱耗散 regime 下工作，并通过局部势调控实现原位训练，适用于多种物理系统。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播算法在物理神经网络中难以实现，需要一种更适用于物理系统的替代学习算法。

Method: 将平衡传播（EP）学习扩展到复值波系统，引入基于局部可训练势的机制，无需明确定义节点或连接，适用于无明确节点结构的系统。

Result: 在激子-极化激元凝聚体系统中验证了该方法，在逻辑任务和手写数字识别任务上实现了稳定收敛。

Conclusion: 该方案为仅能通过局部参数控制的物理系统提供了可行的原位学习路径，拓展了EP在物理硬件上的应用前景。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [341] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出了一种用于多模态情感分析中缓解模态缺失问题的因子引导语义恢复框架（FSRF）。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注完整多模态数据的融合，忽视了实际应用中因遮挡、隐私限制或设备故障导致的模态缺失问题，影响模型泛化能力。

Method: 设计了去冗余同质-异质因子化模块，将模态分解为同质、异质和噪声表示，并引入分布对齐的自蒸馏模块，通过双向知识迁移实现缺失语义的恢复。

Result: 在两个数据集上的实验表明，FSRF在存在不确定模态缺失的情况下显著优于先前方法。

Conclusion: FSRF有效缓解了多模态情感分析中的模态缺失问题，提升了模型在真实场景中的鲁棒性和泛化性。

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [342] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: 本文提出了STABLE，一种基于LoRA的门控持续自编辑框架，通过限制参数更新以缓解大语言模型在连续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要在不完全重新训练的情况下进行持续适应，但连续更新容易导致灾难性遗忘，即新知识覆盖旧知识。因此，亟需一种有效机制来平衡模型更新与知识保留。

Method: 提出STABLE框架，采用低秩适应（LoRA）进行参数高效微调，并引入稳定性预算机制，使用精确匹配下降、比特增加和KL散度三种指标评估每次编辑；若超过阈值，则对LoRA更新进行裁剪或拒绝。

Result: 在Qwen-2.5-7B模型上的实验表明，门控机制能有效缓解遗忘并保持适应能力；基于EM的门控在短期连续学习中表现最佳，不同门控策略虽产生相似分布偏移，但准确率结果不同。

Conclusion: STABLE为大语言模型的持续编辑提供了一种有原则的方法，能够在集成新知识的同时维持模型的可靠性，突显出门控设计在持续适应中的重要性。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [343] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 本文提出了一种名为MemCom的逐层提示压缩方法，以提升上下文学习中多示例提示的记忆和计算效率，在高倍率压缩下仍能保持较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的提示压缩方法在处理多示例压缩时效果不佳，且随着示例数量增加，内存和计算成本显著上升，因此需要更高效的压缩方法。

Method: 通过在每个Transformer层对多示例表示进行压缩，使用更强的压缩模型，并为每一层提供独立的压缩表示，实现细粒度压缩。

Result: MemCom在多种模型架构、尺寸和压缩比率下均优于强基线方法，在多个大标签集分类任务中表现优异，即使在3x到8x的高压缩比下，准确率下降通常小于10%。

Conclusion: MemCom有效解决了多示例上下文学习中的效率问题，实现了高压缩比下的稳定性能，具有较强的实用性和可扩展性。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [344] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 本文提出了一种决策支持系统，通过预训练AI智能体缩小人类可采取的动作集，从而在顺序决策任务中实现人与AI的互补性。通过大规模人类实验验证，该系统显著提升了人类表现，并略微超越了AI单独的表现。


<details>
  <summary>Details</summary>
Motivation: 如何在顺序决策任务中实现人类与AI的互补性，使人类在不需要完全理解AI的情况下仍能通过适当让渡或保留控制权来提升整体性能。

Method: 设计一个决策支持系统，利用预训练AI智能体缩小人类可选动作集合，并引入基于动作集平滑性特征的bandit算法，自适应优化人类代理程度。

Result: 在n=1,600人的野火缓解游戏实验中，使用该系统的参与者比无辅助时表现提升约30%，且优于系统所用AI智能体超过2%，尽管AI单独表现已远超无辅助人类。

Conclusion: 通过自适应控制人类代理水平，该决策支持系统成功实现了人在环路中的互补性，显著提升顺序决策任务中的整体性能。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [345] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 本文提出了一种基于搜索的世界模型，无需训练即可通过相似性搜索和随机表示来近似环境动态，并在重建质量和长时域预测上与Dreamer系列的PlaNet模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 为了减少传统世界模型对复杂训练过程的依赖，探索不依赖训练的世界模型构建方法。

Method: 利用相似性搜索和随机表示技术，在没有显式训练的情况下逼近世界模型，并与基于训练的PlaNet模型进行对比。

Result: 在潜变量重建质量和图像感知相似性方面，所提方法在短期和长时域预测中均与PlaNet相当，且在多种视觉差异较大的环境中表现出更强的长时域预测能力。

Conclusion: 基于搜索的方法可以有效替代传统训练式世界模型，尤其在长时域预测任务中具有优势。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [346] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 本文首次对具有时变学习策略（即on-policy采样）的Q-learning算法进行了有限时间分析，在最小假设下建立了其最后迭代的收敛速率，并揭示了on-policy Q-learning在探索较弱但利用更强的优势。


<details>
  <summary>Details</summary>
Motivation: 现有Q-learning理论主要集中在off-policy情形，而on-policy情形由于时变策略和马尔可夫噪声的时非齐次性，缺乏有限时间收敛分析。本文旨在填补这一空白，尤其在仅假设存在一个使状态空间不可约的策略的最小设定下进行分析。

Method: 采用基于Poisson方程的方法，将与懒惰转移矩阵相关的马尔可夫噪声分解为鞅差项和残差项；通过对Poisson方程解关于Q函数估计和学习策略进行灵敏度分析，控制时非齐次性下的残差项，从而实现对on-policy Q-learning的有限时间分析。

Result: 建立了$\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$的最后迭代收敛速率，达到$O(1/\epsilon^2)$的样本复杂度；同时给出了$\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$的显式收敛速率；理论表明on-policy Q-learning相比off-policy虽探索能力较弱，但策略能收敛至最优，具有利用优势。

Conclusion: on-policy Q-learning在最小假设下具有与off-policy相当的样本复杂度，但对探索参数依赖更差；其策略收敛特性带来利用优势，所提出的技术工具对分析其他时变策略强化学习算法具有广泛适用性和独立意义。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [347] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文研究了安全强化学习中拉格朗日乘子λ的最优性与稳定性，提出了λ-轮廓以可视化回报与约束成本之间的权衡，发现自动更新乘子虽可达到甚至超越最优性能，但训练过程中存在振荡现象，可通过PID控制缓解，但仍需精细调参。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，性能最大化必须与约束条件谨慎平衡。拉格朗日方法虽常用，但其效果依赖于乘子λ的选择，而自动更新λ的鲁棒性和影响缺乏充分实证研究。

Method: 通过分析不同任务中拉格朗日乘子的最优性与稳定性，提出λ-轮廓来可视化回报与约束成本的权衡，并评估自动更新及PID控制对乘子动态的影响。

Result: λ对性能高度敏感，难以凭直觉选择最优值λ*；自动更新可在某些情况下超越λ*的性能；但训练中出现振荡，PID控制可缓解但需精细调参。

Conclusion: 自动拉格朗日乘子更新具有潜力，但其不稳定性表明需要进一步研究以提升安全强化学习中拉格朗日方法的鲁棒性。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [348] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了NAMEx，一种基于纳什议价的专家融合框架，通过博弈论视角重新解释稀疏专家模型中的专家融合，实现了更平衡高效的专家协作，并在多种任务和大规模模型上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏专家模型（SMoE）融合策略缺乏原则性的权重机制，难以有效平衡专家间的合作与竞争。

Method: 将专家融合问题建模为博弈过程，引入纳什议价解（Nash Bargaining）作为融合机制，并结合复动量方法加速专家传播，提供收敛性理论保证。

Result: 在语言建模、文本分类、图像分类及数据污染下的零样本鲁棒性任务中，NAMEx consistently优于现有方法，并成功应用于Qwen1.5-MoE（14B）和DeepSeek-MoE（16B）等大规模系统。

Conclusion: NAMEx为专家融合提供了新的博弈论视角，具备良好的通用性、可扩展性和实际应用价值。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [349] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种通过指数倾斜目标将零阶优化与SAM方法显式结合的新框架，实现了平均损失和最大损失之间的平滑过渡，并设计了新的零阶算法来求解软SAM目标，在多种下游任务中表现出优于传统零阶方法的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 将零阶优化中的平均损失思想与SAM中关注邻域内最大损失以寻找平坦极小值的思想统一起来，探索更有效的优化路径。

Method: 引入指数倾斜目标函数，通过调节倾斜参数t在平均损失和最大损失之间进行平滑过渡，提出新的零阶优化算法来优化软SAM目标。

Result: 对倾斜SAM框架下的锐度概念进行了精确刻画，所提方法在分类、多选问答和语言生成等任务上优于传统零阶基线方法，且具有梯度无关性和内存效率高的特点。

Conclusion: 该工作成功连接了零阶优化与SAM方法，提供了一种无需梯度、节省内存且泛化性能更好的替代方案。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [350] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: 本文提出了一种基于RNN的简单高效模型GRUwE，用于处理不规则采样的多变量时间序列，在多个真实世界基准任务中表现优于或媲美现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有复杂模型在不规则时间序列预测中的优势不明确，需要验证简化且高效的RNN架构是否仍具竞争力。

Method: 提出GRUwE模型，结合指数基函数和两种重置机制（观测触发和时间触发），维护马尔可夫状态以支持连续时间预测。

Result: 在多个真实数据集的下一观测和下一时事件预测任务中，GRUwE性能达到或超过当前SOTA方法，同时具有更低计算开销。

Conclusion: GRUwE凭借其简洁性和高效性，在不规则时间序列建模中具有强竞争力，为实际部署提供了优越的平衡方案。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [351] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 本文系统地比较了三种生成模型（AtomGPT、CDVAE和FlowMM）在超导材料晶体结构重建任务中的性能，使用KL散度和平均绝对误差作为评估指标，结果表明CDVAE表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在新材料发现中应用广泛，但缺乏对不同架构在材料数据集上性能的严格比较。

Method: 在JARVIS Supercon 3D和Alexandria数据库的子集上训练AtomGPT、CDVAE和FlowMM三种模型，并采用KL散度和MAE评估其重建晶体结构的能力。

Result: CDVAE在KLD和MAE指标上表现最优，其次是AtomGPT，最后是FlowMM。

Conclusion: CDVAE在晶体结构生成任务中具有最强的性能，适合作为材料逆向设计的基准模型。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [352] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文研究了基于人类反馈的强化学习（RLHF）在语言模型对齐中的作用机制，发现对齐过程主要集中在中层激活的特定子空间，是一个方向性、低秩的过程，而非广泛分布的参数调整。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型通过人类偏好微调实现对齐的内部工作机制仍然不清晰，本文旨在系统分析这一过程。

Method: 采用全层因果修补方法，在基础模型和其微调版本之间进行跨人类偏好对的比较，并在Llama-3.2-1B上实施；同时使用LASSO回归分析激活距离与奖励增益的关系。

Result: 发现对齐效果在空间上局部化于中层激活的独立子空间，早期和晚期层基本不受影响；只有少数层在LASSO回归中具有非零系数，表明激活变化与奖励增益密切相关。

Conclusion: 至少对于某些语言模型，基于人类偏好的对齐是一种方向性、低秩的过程，主要依赖于中层表示的变化，而非全模型参数的广泛调整。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [353] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 本文提出通过引入群等变卷积（如旋转和尺度等变层）来增强卷积神经网络的对抗鲁棒性，设计了并行和级联两种对称感知架构，在不使用对抗训练的情况下显著提升了模型在CIFAR数据集上的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 对抗样本暴露了深度神经网络对微小输入扰动的敏感性，而主流的对抗训练方法计算成本高且可能损害干净数据的准确性，因此需要更高效、原理性的防御机制。

Method: 将旋转和尺度等变卷积层嵌入标准CNN中，提出并行和级联两种架构，并从理论上分析其对假设空间复杂度、梯度正则化及CLEVER框架下认证鲁棒性边界的影响。

Result: 所提模型在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，面对FGSM和PGD攻击时均表现出更强的对抗鲁棒性和更好的泛化性能，且无需对抗训练。

Conclusion: 对称性约束的网络结构可作为数据增强类防御方法的有效替代方案，为提升模型鲁棒性提供了一种高效且具理论支持的新途径。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [354] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文主张强化学习研究应减少对代理性能的单一关注，更多地转向推进科学理解和学习机制的研究，并强调基准测试与数学形式化之间的精确映射。


<details>
  <summary>Details</summary>
Motivation: 近年来强化学习在任务表现上取得突破，但过度关注性能可能导致过拟合学术基准，限制技术向实际问题的迁移，并忽视对学习机制的理解。

Method: 通过分析流行的Arcade Learning Environment（ALE）基准，说明即使被认为‘饱和’的基准仍可用于深化对强化学习的理解。

Result: 提出应重新定位强化学习研究目标，重视科学理解而非仅性能提升，并倡导更严谨的基准设计以匹配数学形式化。

Conclusion: 强化学习的未来发展应平衡性能展示与基础科学探索，利用现有基准推动理论进步和实际应用。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [355] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于运行时监控语言（RML）的新型语言化奖励机，以增强强化学习中奖励函数的表达能力，能够处理非正则、非马尔可夫任务，并在任务规范和事件处理上更具灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统的奖励函数通常是黑箱且表达能力有限，Reward Machines虽有所改进但仍受限于正则语言，难以表达复杂行为（如计数或参数化条件），因此需要更强大的建模工具。

Method: 利用Runtime Monitoring Language（RML）的内置记忆机制，构建一种新型的语言化Reward Machine，以支持更复杂的非正则、非马尔可夫奖励函数的表示与执行。

Result: 实验证明该方法具有更强的表达能力，能灵活处理复杂任务中的事件和条件，在任务建模方面优于传统Reward Machines。

Conclusion: 所提出的RML-based Reward Machine扩展了传统Reward Machines的能力，为复杂、结构化奖励的设计提供了更强大且可解释的框架。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [356] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习（RRL）与对象中心表示的新框架，通过主动向人类专家查询来提升对结构化和非结构化数据的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习系统多局限于命题任务，忽视了问题中的内在结构；而现有关系强化学习方法又对问题结构有强假设，限制了泛化能力。

Method: 将关系强化学习（RRL）与对象中心表示相结合，并引入对策略不确定性的建模，使系统能主动向人类专家查询以获得指导。

Result: 实验评估表明，所提方法在处理结构化与非结构化数据时具有更高的有效性与学习效率。

Conclusion: 该框架通过融合对象中心表示与主动查询机制，提升了RRL在复杂环境中的泛化能力和学习性能。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [357] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 提出一种探索-然后提交算法，用于解决奖励依赖于动作和潜在状态的非平稳老虎机问题，实现了O~(T^{2/3})的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在奖励依赖于动作和潜在状态且状态动态未知的非平稳环境中，平衡短期与长期奖励之间的张力。

Method: 使用随机Rademacher动作进行探索以估计线性动态的马尔可夫参数，在提交阶段利用估计参数设计优化的动作序列。

Result: 算法实现了\tilde{\mathcal{O}}(T^{2/3})的遗憾界，并为系统辨识提供了接近最优的样本复杂度和误差界。

Conclusion: 所提方法有效应对了时序相关奖励的学习和长期奖励最大化动作序列设计的挑战，适用于复杂的非平稳决策问题。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [358] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 本文提出了一种分解标签噪声检测方法的框架，将其分为标签一致性函数、聚合方法和信息获取方式三个基本组成部分，并通过统一的基准任务和新提出的假阴性率指标进行系统评估，结果表明在大多数场景下，使用平均概率聚合结合logit margin作为标签一致性函数的in-sample信息获取方式表现最佳。


<details>
  <summary>Details</summary>
Motivation: 标签噪声是现实世界数据集中常见的问题，影响模型训练和验证，尽管已有多种技术被提出用于检测噪声标签，但尚无明确的最佳方法共识。

Method: 将现有的噪声标签检测方法分解为三个基本组件：标签一致性函数、聚合方法和信息获取方式（in-sample与out-of-sample），并在此基础上构建一个统一的基准任务，以数据集噪声率相等的比例检测训练样本中的噪声，同时引入一个新的评估指标——在该固定操作点下的假阴性率。

Result: 实验覆盖了视觉和表格数据集，在合成和真实噪声条件下均进行了测试，发现使用in-sample信息获取、平均概率聚合和logit margin作为标签一致性函数的方法在多数情况下表现最优。

Conclusion: 该研究为设计新的噪声标签检测方法以及针对特定应用场景选择合适的技术提供了实用指导。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [359] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究利用机器学习方法分析欧洲绿色协议中165项气候政策的文本和元数据，预测政策从宣布到采纳的进展，发现ClimateBERT在纯文本任务中表现最佳，而加入元数据后BERT效果更优，表明ML结合可解释AI有助于气候政策分析与决策。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化需要有效的立法行动，但政策从提出到采纳的过程复杂且难以预测，因此需要借助机器学习方法理解影响政策进展的关键因素。

Method: 构建包含165项政策的文本与元数据数据集，比较TF-IDF、BERT和ClimateBERT等文本表示方法，结合元数据特征预测政策进展状态，并使用可解释AI方法分析关键影响因素。

Result: 仅用文本特征时，ClimateBERT表现最好（RMSE=0.17, R²=0.29）；加入元数据后，BERT性能最优（RMSE=0.16, R²=0.38）。可解释AI显示政策措辞、政党及国家代表等元数据具有显著影响。

Conclusion: 机器学习模型，尤其是结合元数据的BERT和专用ClimateBERT，在预测气候政策进展方面具有潜力，可为政策制定与分析提供支持工具。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [360] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 本文研究了一比特量化在随机特征模型中的理论基础，证明了除最后一层外对所有层的权重进行量化不会导致泛化误差损失，并提供了多层随机特征模型的精确泛化误差分析。


<details>
  <summary>Details</summary>
Motivation: 尽管一比特权重压缩在神经网络中引起了广泛关注，但其理论基础尚不清楚，因此需要深入研究其在简化模型中的表现和影响。

Method: 通过分析随机特征模型中的一比特量化，利用理论证明和实证实验验证量化对泛化误差的影响。

Result: 理论上证明了除最后一层外的所有层进行一比特量化不会增加泛化误差；实验表明一比特量化显著提升了推理速度。

Conclusion: 一比特量化在随机特征模型中具有理论支持和实际效益，为神经网络压缩提供了新的见解。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [361] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: 提出WEBSERV，一个高效、可扩展的强化学习网页代理训练与评估环境，通过紧凑的浏览器环境和可扩展的服务器架构，显著降低资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有环境在上下文复杂性、动作确定性和可扩展性方面存在问题，缺乏兼顾真实浏览器交互与可控服务器状态的高效环境。

Method: 设计WEBSERV，包含紧凑且与网站无关的浏览器环境，并通过高效启动和重置Web服务器实现可扩展的强化学习环境。

Result: 在WebArena的购物CMS和Gitlab任务上达到最先进的单提示成功率，启动延迟降低约5倍，存储需求减少约240倍，单主机支持200多个并发容器。

Conclusion: WEBSERV有效解决了现有RL网页代理环境的瓶颈，实现了高性能、低开销和大规模并行训练与评估。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [362] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出了一种基于神经微分方程（Neural ODE）的连续深度Evoformer模型，用于蛋白质结构预测，在显著降低计算成本的同时保持合理的预测性能。


<details>
  <summary>Details</summary>
Motivation: Evoformer模型虽然有效，但其48层堆叠结构导致计算成本高且层次离散化固定，缺乏灵活性。

Method: 受神经ODE启发，将Evoformer的离散块替换为连续深度的Neural ODE参数化形式，保留核心注意力机制，并利用伴随法实现恒定内存消耗。

Result: 在蛋白质结构预测任务中，该方法能生成结构合理的预测结果，可靠捕捉α-螺旋等二级结构，精度略低于原始模型，但训练仅需单GPU上17.5小时，资源消耗大幅降低。

Conclusion: 连续深度的Evoformer在效率和可解释性方面展现出优势，为高效、自适应的蛋白质结构预测框架提供了新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [363] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文从范畴论的角度分析了超图中超边的解耦表示，并提出了一种基于自然性条件的新解耦准则，实验表明该方法能有效捕捉基因通路中的功能关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少探索超图结构数据的解耦表示，而超边中隐含的语义信息（如未标注的节点关系）对标签预测具有重要意义。

Method: 引入范畴论中的自然性条件，提出一种新的超边解耦准则，并将其融入超图神经网络中。

Result: 通过概念验证模型实验，成功捕捉到了基因（节点）在遗传通路（超边）中的功能关系，验证了所提准则的有效性。

Conclusion: 基于范畴论的自然性条件为超图上的解耦表示提供了理论支持和新视角，展示了其在生物信息学等领域的应用潜力。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [364] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出一种结合SVD和量化技术的高效视觉语言模型压缩方法，显著降低内存和计算开销，同时提升超过10%的准确率，适用于资源受限设备上的实时部署。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）由于高计算成本和大内存占用，限制了其在实时和可扩展应用中的使用。

Method: 利用奇异值分解（SVD）对Q、K、V权重矩阵进行分解，减少KV缓存大小和计算开销；引入动态调整SVD秩的高效秩分配策略；进一步结合权重和激活的量化技术。

Result: 相比仅使用量化或SVD的方法，在更低硬件成本下实现了超过10%的精度提升，显著降低了内存使用和计算成本。

Conclusion: 所提出的方法在保持甚至提升VLM准确性的同时，大幅降低了资源消耗，更适合在资源受限设备上进行实时部署。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [365] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug 是一种基于支架感知的虚拟筛选框架，通过生成式AI和支架感知采样算法缓解类别与结构不平衡问题，并通过重排序提升活性化合物的结构多样性。


<details>
  <summary>Details</summary>
Motivation: 解决配体虚拟筛选中类不平衡、活性分子结构不平衡及缺乏结构多样性的问题。

Method: 提出 ScaffAug 框架，包含三个模块：基于图扩散模型的支架条件生成增强模块、模型无关的自训练模块以融合合成数据，以及提升支架多样性的重排序模块。

Result: 在五个靶点类别上验证了 ScaffAug 的有效性，相比基线方法在多个指标上表现更优，同时增强了筛选结果的结构多样性与整体性能。

Conclusion: ScaffAug 通过生成增强、自训练和重排序策略，有效提升了虚拟筛选中对新颖活性化合物的识别能力，为药物发现提供了新思路。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [366] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了S2-DiGCL，一种面向有向图对比学习的新框架，通过复数域和实数域的双重视角增强表示学习，显著提升了节点分类与链路预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法主要关注无向图，忽略了现实网络中至关重要的方向信息，难以有效建模有向图的结构特性。

Method: 从复数域视角，引入对磁拉普拉斯算子的个性化扰动以自适应调整边相位和方向语义；从实数域视角，采用基于路径的子图增强策略捕捉局部非对称性和拓扑依赖关系，联合构建高质量正负样本。

Result: 在7个真实世界有向图数据集上实验表明，S2-DiGCL在节点分类任务上提升4.41%，链路预测任务上提升4.34%，优于现有方法。

Conclusion: S2-DiGCL通过融合复数域与实数域的空间洞察，有效利用了有向图的方向性信息，实现了更通用且鲁棒的对比学习框架。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [367] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 本文探讨了深度学习中记忆化与简单组合之间的协同作用，理论上证明在线性设置下，记忆化结合组合能力可以帮助模型对需要长尾特征组合的罕见测试样本做出正确预测，即使这些组合在训练数据中从未出现过。实验表明，这一理论洞察不仅限于线性设置，并且模型的组合能力取决于其架构。


<details>
  <summary>Details</summary>
Motivation: 重新思考记忆化与泛化之间的关系，特别是在涉及长尾特征组合的情况下，探索记忆化如何通过简单组合提升模型的泛化能力。

Method: 理论分析线性设置下记忆化与组合的作用，并通过神经网络架构在简单数据上的实验验证理论结果的普适性。

Result: 理论表明记忆化与组合能帮助模型预测未见过的长尾特征组合；实验显示该现象在非线性模型中也存在，且模型架构影响其组合能力。

Conclusion: 记忆化与简单组合的协同作用有助于提升模型对罕见样本的预测能力，且模型架构在实现这种组合能力方面起关键作用。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [368] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 本文提出了一种用于时间序列预测的多模态图增强网络MGTS-Net，通过多模态特征提取、融合和多尺度预测三个模块，有效提升了预测精度与模型适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度时序模式提取、多模态信息融合及动态多尺度特征适应方面存在不足，限制了预测性能。

Method: 设计MGTS-Net模型，包含多模态特征提取层（MFE）、多模态特征融合层（MFF）和多尺度预测层（MSP），分别优化各模态特征编码、构建异构图进行模态内与跨模态建模，并动态加权融合多尺度预测结果。

Result: 实验表明，MGTS-Net在多个基准上优于现有最先进模型，具有轻量高效的特点。

Conclusion: 所提出的MGTS-Net有效解决了多模态时间序列预测中的关键挑战，显著提升了预测性能。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [369] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出一种稀疏Transformer架构，通过结合数据分布的先验信息，基于正则化Wasserstein近端算子设计模型，在生成建模和贝叶斯反问题中表现出更高的准确性和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 受正则化Wasserstein近端算子这一最优传输问题启发，旨在改进传统基于神经ODE方法的凸性并促进生成样本的稀疏性。

Method: 将数据分布的先验信息融入Transformer结构，利用Wasserstein近端算子的闭式解构建稀疏Transformer模型。

Result: 理论分析和数值实验表明，该方法在生成建模和贝叶斯反问题中比经典神经ODE方法具有更高的准确性和更快的收敛速度。

Conclusion: 稀疏Transformer通过引入先验信息和优化结构，有效提升了生成模型的性能和训练效率。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [370] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: 提出SymphonySMoE，一种基于社交图建模专家交互的稀疏混合专家模型，提升在分布偏移下的鲁棒性，兼具可扩展性和模块化，适用于大规模语言和视觉任务。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏混合专家模型（SMoE）在面对数据分布变化时鲁棒性差，难以适应分布偏移和数据污染，限制了其实际应用。

Method: 引入一个社交图结构来建模专家之间的交互，改进token路由机制；该方法轻量、模块化，可无缝集成到现有SMoE模型（如XMoE、GLM）中。

Result: 理论分析与实验表明，SymphonySMoE在语言建模和视觉指令调优任务上优于基线SMoE；验证了其在42亿和74亿参数规模下的可扩展性。

Conclusion: SymphonySMoE通过引入专家间的图结构交互，有效提升了SMoE在分布偏移下的鲁棒性和性能，同时保持高效和可扩展，适用于大规模模型的微调任务。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [371] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了在ECML-PKDD 2025挑战赛中任务1的优胜方案，旨在设计一种针对给定分类模型的对抗攻击方法，在最小化扰动的同时最大化误分类率。


<details>
  <summary>Details</summary>
Motivation: 提高对抗攻击的有效性，同时减少对输入样本的扰动，以在高能物理领域的鲁棒学习挑战中取得优势。

Method: 采用多轮基于梯度的策略，利用模型的可微结构，并结合随机初始化和样本混合技术来增强攻击效果。

Result: 所提出的攻击方法在扰动大小和欺骗成功率方面均取得了最佳结果，最终在竞赛中获得第一名。

Conclusion: 该方法在保持小扰动的同时实现了高效的模型攻击，验证了其在高维复杂模型上的有效性与竞争力。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [372] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了ECML-PKDD 2025挑战赛任务2的优胜方案，旨在构建在干净和对抗性数据上均表现鲁棒的ANN分类模型。


<details>
  <summary>Details</summary>
Motivation: 提高高能物理发现中对抗环境下机器学习模型的鲁棒性和分类准确性。

Method: 提出两阶段方法：首先基于RDSA生成1500万对抗样本；然后设计包含共享权重特征嵌入块和密集融合尾部的鲁棒网络架构。

Result: 在对抗数据上训练的模型混合准确率达到80%，比第二名高出两个百分点。

Conclusion: 所提出的对抗样本生成与专用网络架构有效提升了模型在对抗环境下的鲁棒性与性能。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [373] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏专家混合模型（Input Domain Aware MoE），通过概率混合模型改进输入空间划分，提升专家专业化和计算负载均衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性评分的路由机制难以有效捕捉输入结构，导致专家专业化与计算均衡之间的权衡问题。

Method: 采用概率混合模型建模路由概率，独立于任务目标训练路由机制，实现更清晰的专家分工和平衡的资源利用。

Result: 在视觉-语言任务上的实验表明，该方法在任务性能和专家利用率均衡性方面均优于现有的sMoE方法。

Conclusion: Input Domain Aware MoE通过改进路由机制，有效解决了专家专业化与计算平衡的矛盾，提升了模型的可扩展性和性能。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [374] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一种用于模仿学习的序列强化学习框架，以建模传粉者中异质性的认知策略，特别关注蜜蜂在不同策略（如利用数字线索、记忆或受环境因素影响）下的行为，并通过最小化预测损失和识别与行为数据最一致的记忆范围来提高可解释性，同时提供连接蜜蜂策略搜索与多臂老虎机模型的数学框架。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在专家策略随记忆窗口变化或偏离最优性时表现不佳，难以捕捉快速和慢速学习行为，且缺乏可解释性，限制了对传粉者决策机制的生物学洞察。

Method: 提出一个序列强化学习框架，通过轨迹相似性建模蜜蜂个体间的不同认知策略，引入能识别有效记忆范围并最小化预测损失的模型，确保完全可解释性，并建立与多臂老虎机在探索-利用动态下关联的数学框架。

Result: 实验证明现有方法在该场景下失败，而新模型能更准确地再现关键决策模式；提供了包含80只蜜蜂在多种天气条件下追踪数据的新数据集，支持生态模拟与治理。

Conclusion: 该框架提升了对蜜蜂等传粉者学习策略与记忆交互的理解，增强了模型可解释性，为研究传粉者认知及农业生态系统中的昆虫行为模拟提供了新工具和基准。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [375] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出一种基于自适应核注意力机制的新方法，用于提升高维异质数据的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统PLS等方法难以建模高维、非线性、多尺度特征交互及样本特异性，缺乏对局部与全局关系的联合捕捉能力。

Method: 引入自适应核注意力机制，分别处理不同特征组后进行融合，结合局部模式识别与全局依赖建模。

Result: 在多个数据集上显著优于现有方法，性能指标明显提升。

Conclusion: 该方法有效解决了高维异质数据中复杂特征交互建模的挑战，具有更强的适应性和预测能力。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [376] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: 本文提出OracleAD，一种简单且可解释的无监督多变量时间序列异常检测框架，通过因果嵌入、自注意力机制和稳定潜在结构（SLS）实现高精度异常检测与根因定位。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列异常稀少且通常无标签，现有方法依赖复杂架构，仅能检测部分异常片段并高估性能。

Method: OracleAD将每个变量的历史序列编码为单一因果嵌入，联合预测当前时刻并重构输入窗口；利用自注意力机制捕捉动态的时空关系，并将嵌入对齐到代表正常状态的稳定潜在结构（SLS）；通过预测误差和SLS偏差的双重评分机制识别异常。

Result: OracleAD在多个真实世界数据集和评估协议上达到最先进的检测性能，同时能够精确定位异常时间点及根因变量。

Conclusion: OracleAD在保持模型简洁和可解释性的同时，实现了优越的异常检测性能，并通过SLS有效建模正常状态，支持细粒度诊断与根本原因分析。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [377] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 本文提出了一种基于连通性因子（CF）的新型可扩展并行化方法eDCF，用于在不同尺度下鲁棒估计数据集的内在维度（id），在噪声环境下表现优于现有方法，并能有效识别决策边界中的分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 由于高维数据中复杂依赖关系和噪声的影响，传统内在维度估计方法在不同尺度下表现不稳定，难以准确反映数据真实复杂性。

Method: 提出eDCF方法，基于局部连通性度量——连通性因子（CF），通过多尺度分析实现对内在维度的稳健估计，并支持并行计算以提高可扩展性。

Result: 在合成数据上，eDCF与主流方法具有相当的平均绝对误差（MAE），但在中高噪声水平和大数据集下，其精确匹配率显著更高（最高达25.0%，优于MLE的16.7%和TWO-NN的12.5%），并成功检测到决策边界的分形几何结构。

Conclusion: eDCF是一种高效、鲁棒且可扩展的内在维度估计方法，在复杂、含噪的实际数据中表现出优越性能，适用于分析真实场景下的结构化数据。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [378] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 本文质疑大语言模型（LLM）在因果发现任务中的高表现，指出其评估常依赖于训练数据中可能已包含的基准数据集，导致结果虚高。作者主张应采用防止数据泄露的新型科学研究作为评估基准，并提倡将LLM知识与数据驱动统计方法结合的混合式因果发现方法。实验表明，将LLM预测作为先验信息可显著提升经典PC算法的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在因果发现上的成功可能源于对训练数据的记忆而非真正的因果推理，这引发了对其在真实科学发现中可靠性的担忧。需要更可靠的评估方式和方法论改进。

Method: 提出两个改进方向：(P.1) 基于LLM训练截止后发布的真实科学研究构建无泄漏的因果图评估基准；(P.2) 设计将LLM输出作为先验的混合方法，与经典统计方法（如PC算法）结合。

Result: 在新构建的、防止记忆干扰的因果图上，LLM单独使用时性能远低于在BNLearn等传统基准上的表现；而将LLM预测作为PC算法先验时，整体准确性显著优于纯LLM或纯统计方法。

Conclusion: 为实现LLM在因果发现中的真正潜力，必须采用防数据泄露的科学基准，并发展LLM与统计方法融合的混合范式。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [379] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 该研究利用机器学习和大型语言模型（LLM）从丹麦1.9万人的调查数据中高精度预测生活满意度（准确率超93%），并通过特征学习提取出27个关键问题，揭示健康状况是各年龄段最重要的影响因素。


<details>
  <summary>Details</summary>
Motivation: 传统的生活满意度测量方法复杂且易出错，缺乏可重复性和可扩展性，因此需要更高效、准确的量化方法来支持心理健康干预和主观幸福感研究。

Method: 采用机器学习算法和临床/生物医学领域的大型语言模型（LLM），将表格数据转化为自然语言句子进行建模；使用特征学习技术筛选关键问题，并通过数据重采样、消融实验和XAI分析提升模型可解释性。

Result: 机器学习模型达到93.80%准确率和73.00% macro F1-score，LLM方法取得93.74%准确率和73.21% macro F1-score；发现健康状况是所有年龄层中最关键的生活满意度决定因素；生物医学领域LLM表现优于临床领域。

Conclusion: 结合机器学习、大型语言模型与可解释AI（XAI）能有效、可靠地预测生活满意度，提升模型透明度与信任度，为研究人类行为和主观幸福感提供了可复现、简洁且易于解释的新范式。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [380] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 本文提出了NeurIPT，一种用于多样化脑电图（EEG）神经接口的基础模型，通过基于信号幅度的掩码预训练和渐进式专家混合架构，结合3D电极坐标和脑叶池化方法，在多个BCI数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于受试者间、任务间、条件间差异以及电极配置的多样性，将基础模型应用于EEG信号仍面临挑战，亟需一个能泛化和扩展神经解码的统一模型。

Method: 提出NeurIPT，采用振幅感知掩码预训练（AAMP）学习鲁棒时间表征，引入渐进式专家混合（PMoE）结构增强时序建模，并利用电极3D空间坐标和细调阶段的脑内-脑间池化（IILP）捕捉空间特征。

Result: 在八个下游BCI数据集上通过微调进行评估，NeurIPT持续实现最先进的性能，表现出优异的泛化能力和跨设置适应性。

Conclusion: NeurIPT推动了EEG基础模型的发展，为可扩展、可泛化的神经信息处理系统提供了新思路。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [381] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: 提出了一种名为LANPO的框架，通过分离语言反馈和数值奖励的作用，提升大语言模型在强化学习中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，忽略了 rollout 中蕴含的文本推理信息，导致样本效率低下；同时，直接集成在线经验容易引发信息泄露或行为崩溃。

Method: 提出Language-And-Numerical Policy Optimization (LANPO)，使用动态经验池，结合Reward-Agnostic Reflection实现安全的样本内自我修正，以及Relevant Abstraction提取跨样本的可泛化经验，分离语言引导探索与数值奖励驱动优化。

Result: 在数学推理基准上，7B和14B模型显著优于GRPO等强基线，提升了测试准确率。

Conclusion: LANPO为大语言模型的强化学习提供了一种有效整合历史经验的方法，实现了更高效、更鲁棒的数据利用。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [382] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C-SMILES分子表示方法和copy-augmented机制，提升模板无关的逆合成预测准确性和化学一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模板无关方法难以捕捉化学反应中的结构不变性，导致搜索空间大、预测准确性低。

Method: 将传统SMILES分解为元素-标记对并引入五个特殊标记，结合copy-augmented机制动态决定生成新标记或保留产物中的未变片段，并利用SMILES对齐指导增强注意力一致性。

Result: 在USPTO-50K和USPTO-FULL数据集上分别达到67.2%和50.8%的top-1准确率，生成分子的有效性达99.9%。

Conclusion: 该方法建立了结构感知分子生成的新范式，可直接应用于计算药物发现。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [383] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标注数据的分子推理框架，利用大语言模型结合分子结构中的原子标识进行链式思维推理，在单步逆合成等化学任务中实现了高准确率，并可生成理论基础的合成数据集以缓解数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于化学领域标注数据稀缺且昂贵，传统监督学习方法受限，因此需要一种不依赖标注数据的方法来提升大语言模型在化学推理任务中的表现。

Method: 通过引入基于分子结构的唯一原子标识，将大语言模型的链式思维推理与分子结构锚定。首先进行一次性任务识别相关片段及其化学标签或转化类别，再可选地结合少量示例进行少样本预测化学转化。

Result: 在学术基准和专家验证的药物分子上，该方法使大语言模型在识别反应位点（≥90%）、命名反应类别（≥40%）和最终反应物（≥74%）方面达到高成功率，并能生成理论支持的合成数据集。

Conclusion: 该框架有效提升了大语言模型在复杂化学任务中的性能，同时为解决化学领域数据稀缺问题提供了新途径。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [384] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 本文研究了深度学习模型中参数对称性与网络表达能力在学习实空间重整化群（RG）变换时的泛化行为，以中心极限定理（CLT）为测试案例，揭示了对称性约束与表达能力之间的竞争关系。


<details>
  <summary>Details</summary>
Motivation: 探讨物理对称性编码如何影响神经网络的学习动态，特别是对称性破缺与恢复机制在层次化学习中的作用。

Method: 使用多层感知机（MLP）和图神经网络（GNN），通过改变权重对称性和激活函数来分析其对泛化性能的影响，并将CLT重新表述为累积量递推关系，结合现有框架分析累积量在网络中的传播。

Result: 发现过度复杂或过度约束的模型泛化能力较差；在某些受限MLP架构中可解析地证明该现象，并将该分析框架从MLP扩展到GNN，揭示了GNN内部的信息处理机制。

Conclusion: 对称性约束与网络表达能力之间存在竞争，平衡二者对于建模结构化物理变换至关重要，这为对称神经网络的学习动态及其局限性提供了新见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [385] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种基于四元数的监督学习Hopfield神经网络（QSHNN），通过连续时间动力学模型扩展至四元数域，并引入周期性投影策略保证权重矩阵的四元数结构一致性，实现了高精度、快速收敛和稳定可靠的性能，适用于机器人控制与路径规划等需要平滑轨迹的应用。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态上的几何优势，克服传统实数神经网络在处理旋转数据时的局限性，提升模型在姿态相关任务中的表达能力和稳定性。

Method: 从连续时间HNN的动力学模型出发，将其推广到四元数域，构建全连接结构的QSHNN；设计周期性投影的学习规则，在梯度下降过程中定期将权重矩阵的每个4×4块投影到最近的四元数结构，保持收敛性与四元数一致性。

Result: 理论证明了固定点的存在性、唯一性和渐近稳定性；实验显示模型在随机生成的目标集上具有高准确性、快速收敛和强鲁棒性；神经元状态演化轨迹曲率有界，表现出良好的平滑性。

Conclusion: QSHNN为四元数神经网络提供了坚实的数学基础和实用框架，不仅适用于姿态控制和路径规划等应用，也为基于超复数或非交换代数结构的神经网络设计提供了通用方法论。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [386] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文将多步推理建模为知识图上的s-t连通性问题，分析模型先验知识与测试时增强（如RAG）之间的关系，发现存在相变现象：当先验知识形成巨连通分量后，仅需常数次查询即可找到路径。


<details>
  <summary>Details</summary>
Motivation: 理解模型的预训练知识与外部增强信息在测试时如何协同工作，特别是在少量增强步骤下回答查询所需的先验知识量。

Method: 将多步推理形式化为知识图上的s-t连通问题，用子图表示模型的参数化知识，将增强视为向 oracle 查询真实边以扩展知识图，并分析所需增强步数。

Result: 发现相变现象：若先验知识图断开成小组件，则需Ω(√n)次查询；一旦知识密度超过阈值形成巨连通分量，则期望仅需常数次查询即可成功。

Conclusion: 模型的先验知识密度是决定测试时增强效率的关键因素，存在一个临界阈值，越过该阈值后增强过程变得高效。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [387] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 本文研究了多阶段训练对机器遗忘的影响，指出由于训练路径依赖性，局部遗忘算法无法普遍实现与从头重训练相同的效果，因此在多阶段训练场景下，Retrain Equivalence不是一个合适的机器遗忘目标。


<details>
  <summary>Details</summary>
Motivation: 现代模型常通过多阶段、非独立同分布数据进行训练（如LLM的对齐、推理能力微调），而传统机器遗忘理论基于i.i.d.假设，缺乏对路径依赖的考虑，导致理想目标Retrain Equivalence在实际中难以成立。

Method: 结合理论分析与实验验证，提出局部遗忘方法的结果具有路径依赖性，并在Llama和Qwen系列模型上使用梯度上升、NPO、SimNPO等算法进行测试，比较不同训练顺序下的遗忘行为差异。

Result: 理论证明局部遗忘结果依赖于训练阶段的顺序；实验显示不同训练路径导致遗忘后模型行为显著不同，GSM8K准确率下降差异超过20%，且某些路径下的模型更难遗忘；概率质量迁移方式（如释义或概念替换）也受路径影响。

Conclusion: 在多阶段训练背景下，Retrain Equivalence作为局部机器遗忘的目标是不恰当的；当无法获取模型训练历史时，需重新定义机器遗忘的目标与期望性质。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [388] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: 本文提出了一种名为StructureFlow的新方法，能够同时学习物理系统的网络结构和随机群体动力学，无需模拟，适用于高维、噪声和部分观测的动态系统。


<details>
  <summary>Details</summary>
Motivation: 许多自然科学领域需要对动态系统进行建模并揭示其因果关系，但现有方法通常只能单独处理结构学习或群体动力学建模，难以同时解决这两个问题。

Method: 提出StructureFlow，一种无需模拟的原理性方法，联合学习系统的结构和随机群体动力学，利用干预数据进行结构学习，并推断条件群体动力学轨迹。

Result: 在高维合成系统、生物合理性的模拟系统和真实单细胞实验数据上验证了StructureFlow的有效性，结果表明该方法能准确学习系统结构并同时建模其条件动力学。

Conclusion: StructureFlow为同时建模复杂系统的结构与动力学提供了有效途径，是实现系统行为机制理解的重要一步。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [389] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一种基于Vision Mamba架构的蛋白质-蛋白质相互作用评估模型，通过替换PIsToN中的Vision Transformer，提升了对长程序列建模的能力，在多个大规模数据集上表现优于原有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质-蛋白质对接评分函数依赖于强大的深度学习模型，但Transformer架构在处理长序列时效率有限，需要更高效的架构来提升预测准确性。

Method: 将Vision Mamba架构引入蛋白质-蛋白质界面评分任务，替代PIsToN中的Vision Transformer，利用Mamba对图像块序列进行高效长距离建模。

Result: PUMBA在多个广泛使用的大型公共数据集上 consistently 优于PIsToN，显著提升了对蛋白质界面特征中全局和局部模式的捕捉能力。

Conclusion: Vision Mamba在蛋白质-蛋白质界面评估任务中表现出优越性能，显示出其在结构生物学和药物开发领域作为高效骨干网络的潜力。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [390] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出一种新型主动目标发现框架，适用于先验信息不足的场景，具有理论保证、可解释性强，并在多领域实验中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺或采样成本高的场景下（如稀有物种发现、新兴疾病诊断），传统基于强先验的生成模型难以泛化，因此需要一种不依赖有效先验的主动发现方法。

Method: 受神经科学启发，设计了一种理论上有保证的主动探索框架，通过逐步更新先验估计来指导采样，具备可解释性，并确保每次观测后先验估计单调改进。

Result: 在物种分布建模和遥感等多个任务中，该方法显著优于现有基线方法，且消融实验验证了各组件的有效性。

Conclusion: 所提方法在无有效先验的复杂现实场景中实现了鲁棒、自适应且高效的主动目标发现，兼具理论可靠性与决策可解释性。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [391] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 本文提出了一个用于流式临床时间序列的紧凑且严格因果的基准，基于MIT-BIH心律失常数据库每秒心率进行研究，比较了GRU-D和Transformer在心动过速风险预测和单步心率预测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 旨在为流式临床时间序列建立一个严格因果且实用的基准，并评估不同模型在短期风险预测和点预测任务中的性能差异。

Method: 使用MIT-BIH心律失常数据库，采用记录级非重叠划分，设计两个任务：近十秒心动过速风险预测和单步心率预测；对比GRU-D（RNN）与Transformer在相同训练预算下的表现，评估时考虑分类校准性和预测适当性，采用温度缩放和分组自助法置信区间。

Result: 在MIT-BIH数据集上，GRU-D在心动过速风险预测任务上略优于Transformer，而Transformer在降低心率预测误差方面显著优于GRU-D和持久性基线。

Conclusion: 在纵向监测中，模型选择依赖于具体任务：紧凑型RNN在短时风险评分中仍具竞争力，而紧凑型Transformer在点预测任务中优势更明显。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [392] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文利用扩散过程方法精确分析了高维设置下带噪声的随机梯度下降（noisy SGD）的行为，提供了连续时间视角下的统计风险与隐私损失动态演化，并研究了一种无需显式了解梯度敏感性的变体，适用于带ℓ²正则的最小二乘问题。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要提供noisy SGD的统计风险和隐私损失的各种界限，但其在高维情形下的精确行为尚不清楚，且通常依赖梯度裁剪来控制敏感性，限制了实际应用。

Method: 采用扩散过程逼近的方法，在连续时间框架下建模noisy SGD，结合统计学习理论与微分方程工具，分析高维极限下的风险演化与隐私损失，并提出一种自适应敏感性机制的SGD变体。

Result: 得到了noisy SGD在高维下统计风险和隐私损失的精确动态描述，揭示了噪声强度、迭代次数与模型复杂度之间的权衡关系，并验证了无需梯度裁剪的算法仍可实现良好隐私-效用平衡。

Conclusion: 扩散逼近为理解隐私优化算法提供了新视角，所提方法能更精确刻画noisy SGD行为，且在不依赖梯度敏感性先验知识的情况下实现有效隐私保护，对实际机器学习系统具有重要意义。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [393] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: 本文提出了CLIP，一种用于安全联邦学习的客户端不变神经元剪枝技术，有效缓解了异构设备下的straggler问题，在多个数据集上实现了13%到34%的训练加速，精度影响较小。


<details>
  <summary>Details</summary>
Motivation: 在异构设备上部署安全联邦学习时，由于计算或网络能力有限的straggler客户端会导致整体训练性能瓶颈，亟需有效的缓解方法。

Method: 提出CLIP，结合客户端侧的不变神经元剪枝和网络感知剪枝，在保证模型安全聚合的同时减轻计算与网络负担。

Result: 在CIFAR10、Shakespeare和FEMNIST等多个数据集上，训练速度提升13%至34%，模型准确率变化在-2.6%到+1.3%之间。

Conclusion: CLIP能有效缓解安全联邦学习中的straggler问题，在显著提升训练效率的同时保持良好的模型性能。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [394] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，指出在变量层面的因果效应不可识别时，基于特定状态的因果效应仍可能可识别，尤其当存在上下文特异性独立性或条件函数依赖等额外知识时。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应识别基于变量整体，但实际中可能只关心特定状态间的因果关系，因此需要研究基于状态的因果效应识别问题。

Method: 通过引入上下文特异性独立性和条件函数依赖等额外知识，分析基于状态的因果效应在何种条件下可识别，并与变量层面的识别进行对比。

Result: 发现基于状态的因果效应在某些情况下可识别而变量层面不可识别；仅约束变量状态的知识本身不能提升识别性，但与其他知识结合时可增强识别能力。

Conclusion: 基于状态的因果效应识别能补充现有变量层面框架的不足，在具备额外结构知识时可从观测数据中估计原本无法识别的因果效应。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [395] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 本文提出了一种基于LSTM的框架，用于处理和预测电动汽车充电负荷，通过数据预处理和特征提取，在多时间尺度上实现了准确的负荷预测。


<details>
  <summary>Details</summary>
Motivation: 为了提高电动汽车充电负荷预测的准确性，支持基础设施规划、能源管理和电网集成。

Method: 采用LSTM模型，结合数据归一化、特征提取和缺失值插值等预处理方法，对多地点的充电负荷数据进行训练和预测。

Result: 实验结果表明，该模型能够在每日、每周和每月等多个时间尺度上准确预测充电需求，具有良好的适应性和应用前景。

Conclusion: 所提出的模块化LSTM框架能够有效预测EV充电负荷，适用于不同场景下的充电设施管理与规划。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [396] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 提出了一种基于多输出高斯过程的多任务学习方法，用于预测NLP模型的学习曲线，支持零样本预测并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了在减少计算开销和数据获取成本的同时，有效预测NLP模型的学习曲线以支持性能目标决策。

Method: 将学习曲线预测建模为多任务学习问题，采用潜变量多输出高斯过程来捕捉任务间和层次结构中的相关性，并结合主动学习策略减少预测不确定性。

Result: 在三个小规模NLP数据集上验证了该方法的有效性，涵盖了不同模型（如nanoGPT、mBART、Transformer和M2M100）的最多30条学习曲线，能够低成本生成接近真实情况的概率性扩展规律。

Conclusion: 所提方法能有效预测NLP模型的学习曲线，支持零样本预测和主动学习，有助于构建低成本、低不确定性的概率性缩放定律。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [397] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 本文提出了一种针对SegDeformer的联合特征与任务解码方法，显著降低了车载和分布式应用中的计算复杂度，同时在语义分割性能上达到或超越现有最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的联合源和任务解码多采用卷积神经网络，未探索基于Transformer的模型（如SegDeformer）在车载和分布式场景下的优化潜力，尤其是其高计算复杂度限制了实际部署。

Method: 提出一种联合特征与任务解码机制，使SegDeformer在保持mIoU性能的同时降低计算开销，适用于车载和分布式两种应用场景，并结合源编码器实现高效传输。

Result: 在车载应用中，Cityscapes上帧率提升达11.7倍（1.4到16.5 fps），ADE20K上提升3.5倍（43.3到154.3 fps），且mIoU与基线相当；在分布式应用中，在广泛比特率范围内达到SOTA性能，仅使用此前SOTA方法0.14%（ADE20K）和0.04%（Cityscapes）的云端DNN参数量。

Conclusion: 所提方法有效平衡了Transformer模型的高性能与高复杂度矛盾，提升了语义分割系统在车载和云端部署的效率与可扩展性。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [398] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出了一种名为SAMOSA的开放集主动学习查询算法，通过利用数据典型性与模型泛化能力的关系，有效选择信息量大的样本，提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 减少数据标注成本，解决传统主动学习在包含无关或未知类别的未标记数据池中选择样本时的局限性。

Method: 基于数据典型性理论和Sharpness Aware Minimization（SAM）优化方法，设计SAMOSA算法，主动查询嵌入流形上靠近决策边界的非典型样本，以同时提高对目标类别的判别能力和区分目标与未知类的能力。

Result: 在多个数据集上实验表明，SAMOSA相比现有最先进方法最高提升3%的准确率，且不增加计算开销。

Conclusion: SAMOSA是一种高效、无额外计算负担的开放集主动学习方法，能有效识别高信息量样本，显著提升模型性能。

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [399] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 本文提出3D第一人称视频游戏作为多模态实时推理的挑战性环境，构建了更大更丰富的带文本指令的人类游戏数据集，并通过逆动力学模型扩展动作标注数据，训练了一个基于行为克隆、可实时推理的文本条件游戏代理，展示了其在多种3D游戏中响应文本并进行游戏的能力，同时指出了长时任务和跨游戏量化评估等未解挑战。


<details>
  <summary>Details</summary>
Motivation: 探索3D第一人称视频游戏作为多模态、实时推理的复杂测试平台，弥补现有数据集规模小、多样性不足以及缺乏文本指令的问题，推动具身智能体在复杂环境中实现语言引导的决策能力。

Method: 1）收集大规模、多样化的带文本指令的人类游戏数据集；2）利用该数据集训练逆动力学模型，为无动作标注的公开视频推断动作标签；3）基于扩展后的数据，采用行为克隆方法训练一个支持文本输入的神经网络代理，并设计可实现实时推理的专用架构。

Result: 成功训练出可在消费级GPU上实时运行的文本条件游戏代理，能够在多种未见过的3D第一人称游戏中执行游戏任务并响应自然语言指令，验证了方法的有效性和泛化能力。

Conclusion: 3D第一人称游戏是发展多模态、语言引导型AI代理的理想试验场，本文通过构建高质量数据集与高效学习框架，展示了行为克隆结合逆动力学建模在复杂环境中的潜力，但长期规划与标准化评估仍是未来关键挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [400] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 提出3D-GSRD，一种结合选择性重掩码解码（SRD）和3D关系Transformer的3D分子图自编码器，在MD17基准上取得8项中的7项最优性能。


<details>
  <summary>Details</summary>
Motivation: 将2D图模型中的重掩码解码扩展到3D分子表示学习面临2D结构泄露与上下文不足的冲突挑战，需在避免泄露的同时保留足够2D上下文以重建被掩码原子。

Method: 设计选择性重掩码解码（SRD），仅对编码器表示中的3D相关信息进行重掩码，保留2D图结构，并与结构无关的解码器和3D关系Transformer（3D-ReTrans）编码器协同集成。

Result: 在MD17分子性质预测基准的8个目标中，3D-GSRD在7个上达到新的最先进性能。

Conclusion: SRD机制增强了编码器在分子表示学习中的作用，3D-GSRD有效平衡了3D建模与2D结构保留的需求，推动了3D掩码图建模的发展。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [401] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 本文综述了面向大语言模型的混合精度量化技术（MXPLMs），系统回顾了量化基础，并分类比较了不同位分配策略在权重、激活和键值缓存上的应用，分析了其在性能、效率与部署间的权衡，指出了硬件感知设计、激活量化和大规模优化等未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速扩展带来了巨大的计算、内存和能耗需求，传统低比特量化虽高效但易损害模型精度，因此需要更精细的混合精度量化方法来平衡效率与准确性。

Method: 本文通过回顾量化基本概念，对现有的混合精度量化方法进行分类，依据其在权重、激活和键值缓存中的精度配置和位分配策略进行系统比较，并对比其在困惑度、零样本任务性能和部署方面的表现。

Result: 总结了当前MXPLM框架在精度与效率之间的权衡表现，识别出适用于语言模型的有效策略，并与早期神经网络的混合精度方法进行了对比，揭示了可迁移与面临挑战的方法。

Conclusion: 混合精度量化是提升大语言模型部署效率的关键路径，未来需结合硬件特性、优化激活量化并发展可扩展的自动化位宽分配方法。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [402] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出一种计算预算感知的数据选择方法CADS，通过双层优化框架将计算预算纳入数据选择过程，显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略计算预算限制，导致在不同预算下算法表现不稳定，因此需要将计算预算作为数据选择策略的核心考虑因素。

Method: 提出计算预算感知的数据选择（CADS）方法，构建双层优化框架：内层在预算约束下训练模型，外层优化数据选择；采用概率重参数化和Hessian-free策略估计梯度，并将内层优化转化为外层目标中的惩罚项以提高效率。

Result: 实验表明，该方法在视觉和语言基准上相比基线最多提升14.42%的性能。

Conclusion: 将计算预算融入数据选择过程可有效提升模型训练的效率与效果，所提CADS方法在多种任务中展现出显著优势。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [403] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种新型Transformer变体，通过从第一层Value头引入跨层跳跃连接，在减少约50%的Value投影和KV缓存的同时增强模型表征能力，在不同规模模型上均实现约25%的KV缓存缩减并降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在提升表征能力时通常伴随高昂的内存与计算开销，尤其是自回归解码中的KV缓存；现有方法难以兼顾表征增强与KV成本降低，因此需要一种既能提升表达能力又能显著减少资源消耗的新架构。

Method: SkipV1Former从第二层开始，每一层复用第一层一半的Value头，另一半正常计算，从而减少Value投影和KV缓存；理论分析表明该设计能恢复压缩损失的信息并加速模型隐式mesa-优化过程，并可与Group-Query Attention等先进方法结合。

Result: 实验表明，SkipV1Former在多种模型规模下相比标准MHA及其他先进变体，在KV缓存减少约25%的同时降低了困惑度；结合YOCO后KV缓存减少近50%且性能仍有所提升；并提出仅需额外10-15%计算量即可将已有MHA检查点升级为SkipV1Former的方法。

Conclusion: SkipV1Former通过第一层Value头的跨层复用，有效平衡了模型表征能力与KV缓存开销，为高效Transformer设计提供了新方向，并具备良好的兼容性与实用价值。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [404] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了在因果充分性条件下，当智能体未知潜在因果结构时的因果赌博机中的遗憾最小化问题。作者证明了遗憾最小化与父节点识别是相互冲突的目标，提出无需恢复因果图或父节点集的近似最优算法，并通过实验证明其方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作集中于识别奖励变量的父节点并应用经典赌博机方法，或在最小化遗憾的同时联合学习父节点。本文探究此类策略是否最优，质疑父节点识别是否必要。

Method: 分析已知和未知父节点集合大小两种情形，建立捕捉动作空间组合结构的新颖遗憾下界，并设计绕过图结构和父节点恢复的近似最优算法。

Result: 证明了在某些实例中，遗憾最小化与父节点识别是根本冲突的；所提算法在多种环境中显著优于现有基线。

Conclusion: 父节点识别对于遗憾最小化并非必要，绕过因果结构恢复可实现更优性能。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [405] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的半监督正-无标签（PU）学习方法，用于考古预测建模，通过动态伪标签和CRF优化在标签稀缺情况下提升性能，在DEM和卫星影像数据上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 考古学中已知遗址位置稀少且大多数区域未标注，传统方法难以有效建模；因此需要一种能应对严重标签不平衡的自动化预测方法。

Method: 采用半监督PU学习框架，构建语义分割模型，结合动态伪标签机制，并使用基于RNN实现的条件随机场（CRF）优化标签置信度，分别在数字高程模型（DEM）和原始卫星影像上进行端到端训练与评估。

Result: 在DEM数据上性能与当前最优方法LAMAP相当且Dice分数更高；在原始卫星影像上通过分层k折交叉验证验证了模型稳定性，生成的预测图具有更好可解释性。

Conclusion: 半监督学习能够有效应对考古遗址预测中的标签稀缺问题，适用于大范围、标注稀疏的地貌环境下的未知遗址发现。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [406] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 本文提出了一种新型神经算子LANO，通过引入基于代理的注意力机制，在保持softmax注意力高精度的同时实现了线性计算复杂度，显著提升了神经算子在科学机器学习中的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer-based神经算子在精度和可扩展性之间的权衡问题：softmax注意力精度高但计算复杂度为二次型，线性注意力虽降低复杂度却牺牲了精度。

Method: 提出Linear Attention Neural Operator (LANO)，引入M个代理token（M远小于N）来中介N个token之间的全局交互，从而将注意力机制重构为具有线性复杂度O(MNd)的结构。

Result: 理论证明了LANO具有通用逼近性质，并具备更好的收敛性和稳定性；实验表明其在多个标准基准上平均比现有最先进的神经PDE求解器（如Transolver）提升19.5%的精度。

Conclusion: LANO成功弥合了线性复杂度与softmax级别性能之间的差距，为科学机器学习提供了可扩展且高精度的神经算子基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [407] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出了一种增强型物理信息神经网络TRPINN，通过在正确的迹空间H^{1/2}(∂Ω)中施加边界损失，提升了收敛速度与稳定性，在数值实验中显著优于标准PINN。


<details>
  <summary>Details</summary>
Motivation: 标准PINN在处理具有高振荡边界条件的问题时可能收敛困难或失败，因此需要一种更稳定、更精确的边界损失施加方式。

Method: 引入Trace Regularity Physics-Informed Neural Network (TRPINN)，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)下计算边界损失，仅计算理论必需的部分半范数，并避免离散化中的分母计算以提升稳定性。结合NTK分析研究收敛性。

Result: TRPINN在拉普拉斯方程与高振荡Dirichlet边界条件下表现出更强的鲁棒性，某些情况下标准PINN失败而TRPINN仍能成功求解，精度提升一到三个数量级。

Conclusion: 通过在正确迹空间中施加边界条件，TRPINN不仅保证了解在H^1(Ω)意义下的收敛性，还加快了收敛速度，显著提升了PINN在复杂边界问题上的性能。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [408] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 本文提出使用双线性自编码器将神经网络中的表示分解为二次多项式，以实现非线性但可分析的潜在特征，通过代数性质研究稀疏性和结构。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器的解释依赖于输入，限制了其独立分析能力，因此需要一种不依赖输入的可解释方法。

Method: 采用双线性自编码器高效地将表示分解为二次多项式，并引入重要性排序、聚类和激活稀疏性优化。

Result: 实现了对非线性潜在结构的代数分析，能够捕捉从线性概念到复杂流形的表示。

Conclusion: 该方法为通过代数属性研究非线性且可分析的潜在表示提供了初步可行路径。

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [409] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: 提出ProtoMol框架，通过分层编码器和共享原型空间实现分子图与文本描述的细粒度多模态融合，显著提升分子属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法在跨模态交互中缺乏层次语义对齐和统一的原型空间，限制了分子图与文本信息的深度融合。

Method: 设计双分支分层编码器（GNN+Transformer），引入逐层双向跨模态注意力机制，并构建可学习的类别特定共享原型空间以指导模态对齐。

Result: 在多个分子属性预测基准数据集上，ProtoMol持续优于现有最先进基线方法。

Conclusion: ProtoMol通过细粒度跨模态对齐和原型引导的语义一致性，有效提升了多模态分子表征学习的准确性和鲁棒性。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [410] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: 本文提出了DrivAerStar，一个包含12,000个工业级汽车CFD仿真的数据集，显著提升了车辆气动优化的精度和效率，实现了机器学习与高保真仿真在工程中的融合。


<details>
  <summary>Details</summary>
Motivation: 传统车辆气动优化面临计算成本高或模型精度不足的问题，现有机器学习数据集因分辨率低、组件缺失和验证误差大而难以用于工业场景。

Method: 使用STAR-CCM+软件生成DrivAerStar数据集，基于自由变形算法系统化探索20个CAD参数对三种车型的影响，包含完整发动机舱和冷却系统，并采用精细网格策略控制壁面y+值以提高精度。

Result: 该数据集风洞验证误差低于1.04%，较现有数据集提升五倍，训练出的模型达到量产级精度，计算时间从数周缩短至几分钟。

Conclusion: DrivAerStar首次弥合了学术界机器学习研究与工业CFD实践之间的鸿沟，为数据驱动的汽车气动优化设立了新标准，并展示了高保真仿真与AI结合在工程领域的广泛应用前景。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [411] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种名为UDS（Utility-Diversity Sampling）的高效在线批量选择框架，用于大语言模型的监督微调，通过同时考虑数据效用和多样性，在无需外部资源的情况下提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调通常计算成本高且易过拟合或放大偏差，现有数据筛选方法忽视多样性、依赖外部资源且增加训练时间，因此需要更高效、自包含的在线批处理选择方法。

Method: UDS利用logits矩阵的核范数捕捉数据效用和样本内多样性，并通过轻量级历史样本内存缓冲区进行低维嵌入比较来估计样本间多样性，实现无需外部模型或验证集的动态采样。

Result: 在多个基准上的实验表明，UDS在不同数据预算下均优于现有的最先进在线批选择方法，并显著缩短了相比全数据微调的训练时间。

Conclusion: UDS是一种高效、无需外部资源的在线批选择框架，通过联合优化效用与多样性，实现了更快且性能更优的监督微调。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [412] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了一种受果蝇嗅觉电路启发的生物启发式框架Fly-CL，用于持续表征学习，有效缓解多共线性问题，显著降低训练时间，并在多种网络架构和数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有持续学习方法在下游任务中因多共线性导致性能下降，以及先进方法计算开销大、难以满足实时低延迟需求的问题。

Method: 借鉴果蝇嗅觉系统的神经机制，设计Fly-CL框架，通过扩展投影和稀疏编码机制改进相似性匹配过程，降低特征间的多共线性，提升模型效率与可扩展性。

Result: Fly-CL在多个预训练骨干网络和不同数据设置下均实现了与当前最先进方法相当或更优的性能，同时大幅减少训练时间，理论分析表明其能逐步消除多共线性且具有低时间复杂度。

Conclusion: Fly-CL是一种高效、低延迟、生物可解释的持续学习框架，兼容多种预训练模型，在保持高性能的同时显著提升训练效率，适用于实时应用场景。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [413] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: 本文提出了UniGTE，一种统一结构与语义推理的指令调优编码器-解码器框架，能够在无需任务特定监督的情况下实现对未见图任务的泛化。


<details>
  <summary>Details</summary>
Motivation: 传统的图神经网络受限于固定的标签空间，而大语言模型难以捕捉图结构，因此在无任务特定监督下对未见图任务进行泛化具有挑战性。

Method: UniGTE采用增强了可学习对齐标记和结构感知的图-文本注意力机制的预训练自回归大语言模型作为编码器，生成紧凑且任务感知的图表示；解码器则基于冻结的大语言模型，仅依赖这些表示来预测答案并同时重构输入图为自然语言描述。

Result: 在五个涵盖节点级、边级和图级任务的数据集上进行指令调优后，UniGTE在跨任务和跨域设置下的节点分类、链接预测、图分类和图回归等任务中实现了新的零样本学习性能纪录。

Conclusion: 通过将图结构与大语言模型语义紧密结合，UniGTE实现了强大且可迁移的图推理能力，展现出良好的零样本泛化性能。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [414] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型生成的优化问题数学建模的细粒度评估框架，引入了变量与约束的查全率、查准率及RMSE等指标，发现GPT-5在多种提示策略下表现最佳，且约束的完整性和准确性对求解效果最为关键。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型生成优化模型的评估多依赖整体性指标（如求解结果），难以揭示结构或数值错误，因此需要一种更精细、可诊断的评估方法。

Method: 提出一个组件级评估框架，包含决策变量和约束的精确率与召回率、约束与目标函数的均方根误差（RMSE）、以及基于token使用和延迟的效率指标，并在六种提示策略下评估GPT-5、LLaMA 3.1 Instruct和DeepSeek Math三个模型。

Result: GPT-5在所有模型中表现最好；链式思维、自一致性与模块化提示最有效；求解器性能主要取决于高约束召回率和低约束RMSE；输出简洁性提升计算效率。

Conclusion: 完整的约束覆盖、低约束RMSE和简洁输出是NLP到优化建模的三大原则，所提框架为LLM在优化建模中的细粒度评估提供了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [415] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 本文扩展了DEEPCHEM，支持即用型SE(3)-不变神经网络，使缺乏深度学习背景的科学家也能方便地构建、训练和评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)-不变神经网络库通常需要深厚的深度学习或数学知识，且缺乏完整的训练流程，限制了其广泛应用。

Method: 在DEEPCHEM中集成SE(3)-Transformer和张量场网络等不变模型，提供完整的训练管道和工具包，并配备充分的测试与文档。

Result: 实现了易于使用的SE(3)-不变模型支持，降低了分子性质预测、蛋白质结构建模等应用的门槛。

Conclusion: 该扩展促进了SE(3)-不变神经网络在化学和材料科学中的普及和进一步发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [416] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出一种基于MoE结构习惯（特别是内部路由模式）转移的新型知识蒸馏检测框架，可在白盒和黑盒场景下有效检测大语言模型中的知识蒸馏，具有高准确率和抗提示规避能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的知识蒸馏检测方法易被提示工程绕过，且缺乏对模型结构层面信号的利用，难以有效保护大模型知识产权并维护模型多样性。

Method: 利用MoE模型中专家专业化与协作的路由模式作为指纹特征，提出白盒检测方法；进一步设计Shadow-MoE，通过辅助蒸馏构建代理MoE表示，实现黑盒环境下任意模型对的结构模式比较。

Result: 在多种场景下检测准确率超过94%，对提示工程攻击具有强鲁棒性，显著优于现有基线方法，并验证了结构习惯在蒸馏过程中的可迁移性。

Conclusion: 结构习惯是知识蒸馏过程中稳定存在的关键信号，所提方法为大语言模型的知识蒸馏检测提供了高效、可靠的解决方案，有助于推动模型版权保护与多样性研究。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [417] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 本研究提出了一种基于LSTM的电力价格预测模型，结合自定义损失函数和在线学习方法，提升了加州电力市场日前电价预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对电力市场各方至关重要，现有模型在动态环境下的适应性和预测精度仍有提升空间。

Method: 采用LSTM网络，融合历史价格、天气条件和发电结构等特征，并引入结合MAE、JSD和光滑性惩罚的自定义损失函数，结合在线学习机制实现模型持续更新。

Result: 自定义损失函数显著提升预测精度，尤其在高峰时段；在线学习模型有效降低误差和波动性；发电结构特征增强了模型表现。

Conclusion: 该框架为电力价格预测提供了高效、自适应的解决方案，有助于提升电力市场中的决策能力。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [418] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了“后门遗忘”攻击，即在大语言模型的遗忘过程中植入后门，使其在正常情况下表现正常，但在特定触发器出现时恢复已遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源大模型的普及，研究者关注遗忘过程本身是否可被恶意操控，确保模型安全与可控。

Method: 受传统后门攻击启发，利用注意力沉降现象，在浅层输入位置（注意力汇聚点）放置触发器，并通过调整注意力增强后门效果。

Result: 实验证明，基于注意力沉降引导的后门遗忘能有效恢复已遗忘知识，且在无触发器时与正常遗忘模型行为一致，难以察觉。

Conclusion: 注意力沉降是实现隐蔽且高效后门遗忘的关键机制，揭示了模型遗忘过程中的新型安全威胁。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [419] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出了一种基于SNOMED CT和Neo4j的知识驱动框架，构建结构化医疗知识图谱，用于标准化临床数据并提升大语言模型在诊断推理中的逻辑一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 非结构化的临床文档导致训练数据噪声多、不一致且逻辑碎片化，严重阻碍了人工智能在医疗领域的有效性。

Method: 结合标准化临床术语SNOMED CT与Neo4j图数据库，构建医疗知识图谱；从临床文本中提取实体关系对，生成结构化的JSON数据集，并用于微调大语言模型。

Result: 实验结果表明，该方法显著提升了AI生成诊断推理的有效性和可解释性，支持多跳推理并确保术语一致性。

Conclusion: 该知识引导的方法为构建可靠的AI辅助临床系统提供了一个可扩展的解决方案。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [420] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文提出一个统一的基准来评估大语言模型（LLMs）在多轮对话中发现和利用用户潜在信息的能力，涵盖20个问题游戏、个性化问答和个性化文本摘要三种场景。研究采用三代理框架（用户、助手、评判者）进行回合级评估，结果表明LLMs虽能通过对话揭示潜在信息，但成功率因任务复杂度、主题和隐藏属性数量而异（32%至98%），说明有效偏好推断仍是构建真正自适应AI系统的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型擅长生成通用文本，但在需要用户个性化偏好的场景下表现受限。由于用户通常不会明确表达所有偏好，模型需具备从对话中推断潜在信息的能力。然而，目前缺乏系统性基准来评估这一能力，因此本文旨在填补这一空白。

Method: 提出一个包含三种递进现实场景（20 Questions、个性化问答、个性化文本摘要）的统一基准，并采用三代理框架（User, Assistant, Judge）进行多轮交互实验，以评估LLM在对话中发现和利用潜在用户属性的能力。

Result: 实验结果显示，LLMs能够通过对话揭示潜在信息，但性能差异显著：成功率在32%到98%之间波动，具体取决于任务复杂度、话题类型和隐藏属性的数量。

Conclusion: 尽管LLMs具备一定的潜在信息发现能力，但其表现高度依赖上下文条件；当前的模型在稳定推断用户偏好方面仍有不足，因此有效偏好推理仍是实现真正自适应AI的关键挑战。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [421] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出一种轻量级深度学习管道，结合小时级降采样、双模式插补和标准化处理，使用GRU-LSTM模型实现高精度短期能耗预测，具备低延迟和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声大、不完整且缺乏上下文信息导致的短期能源消耗预测不准问题。

Method: 采用小时级降采样、均值与多项式回归双模式插补、Standard Scaling标准化，并构建轻量级GRU-LSTM序列到单点预测模型。

Result: 模型平均RMSE为601.9 W，MAE为468.9 W，准确率达84.36%，能有效捕捉非线性用电模式，热力图分析显示预测结果与温度变化趋势高度一致。

Conclusion: 有针对性的数据预处理结合紧凑型循环神经网络可在真实场景下实现快速、准确且可部署的能耗预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [422] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域可泛化的持续学习（DGCL）新设定，并提出自适应领域变换（DoT）方法，通过解耦语义与领域信息并实现跨域输出对齐，提升模型在未见领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法通常假设训练和测试域一致，在动态真实环境中泛化性能差，难以应对多域连续任务挑战。

Method: 基于预训练模型，受大脑分布式加中枢理论启发，DoT在表示学习中解耦语义和领域信息，并通过自适应变换实现跨域输出对齐，支持全参数与参数高效微调。

Result: DoT作为插件策略显著提升了多种持续学习基线在DGCL下的表现，具备良好的领域泛化能力、知识累积能力和资源效率。

Conclusion: DoT有效解决了DGCL中的语义-领域信息平衡问题，为持续学习在真实动态环境中的应用提供了高效且可扩展的解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [423] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练的框架，利用测试时扩展和改进的蒙特卡洛树搜索策略，将大语言模型生成的数学公式转化为求解器代码，以解决多种优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在泛化能力或训练成本上存在局限，需要一种无需训练且能广泛适用于不同优化问题的解决方案。

Method: 通过生成数学公式并将其转换为求解器代码，结合动态扩展、提示反向传播和不确定性反向传播改进蒙特卡洛树搜索。

Result: 在六个基准数据集上的实验表明，SolverLLM优于基于提示和学习的方法，具备强泛化能力。

Conclusion: SolverLLM在无需额外训练的情况下，有效解决了多样化优化问题，展现出优越的性能和通用性。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [424] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出了一种名为soft-masking（SM）的新方法，用于改进基于扩散模型的语言模型中的掩码解码过程，通过在保留掩码时融合预测token的信息来提升生成效果和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的掩码扩散语言模型在解码时采用二元决策（保留或替换掩码），丢失了保留掩码时的预测信息，限制了模型性能。本文旨在解决这一信息丢失问题。

Method: 引入soft-masking机制，将掩码token的嵌入与前一步预测的top-k token嵌入动态混合，使模型在保留掩码时仍能利用预测信息；并提出一种适应预训练模型的训练方法，将其集成到现有掩码扩散语言模型中。

Result: 在169M参数模型上继续预训练后，SM显著提升了困惑度和MAUVE分数；在Dream-7B和Dream-Coder-7B两个先进扩散模型上的微调结果显示，SM在多个编码基准上均带来性能提升，尤其在高吞吐场景下表现更优。

Conclusion: soft-masking有效保留了掩码扩散过程中的预测信息，增强了模型的上下文传播能力，是改进扩散语言模型解码过程的一种高效且通用的方法。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [425] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 本文推导了Transformer中Layer Normalization和前馈网络的二阶Hessian表达式，完善了整个Transformer模块的Hessian刻画，为大规模深度学习中的优化研究提供了新的理论基础。


<details>
  <summary>Details</summary>
Motivation: 缺乏对Layer Normalization和前馈网络Hessian的理论分析，导致对Transformer优化景观的理解存在空白。

Method: 推导了这些组件的显式二阶表达式，扩展了先前的自注意力分析，并提出了基于泰勒展开的损失差异分析框架。

Result: 得到了各子层在曲率传播中作用的估计，揭示了Hessian结构对收敛动态和大模型性能经验缩放律的影响。

Conclusion: 该工作建立了完整的Transformer模块Hessian理论，为优化过程的理论与实证研究提供了新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [426] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: 提出了一种新的概率性Kolmogorov-Arnold网络（P-KAN），用于时间序列预测，通过样条函数连接和直接参数化预测分布，在卫星流量预测中表现出优于MLP的准确性、校准性和参数效率。


<details>
  <summary>Details</summary>
Motivation: 传统模型在不确定性建模和参数效率方面存在不足，尤其是在资源受限场景下需要可靠且高效的概率预测。

Method: 将KAN的标量权重替换为基于样条的函数连接，并直接参数化高斯和Student-t分布以实现概率预测。

Result: P-KAN在卫星流量预测任务中优于MLP基线模型，具有更好的准确性、校准性及更优的效率-风险权衡，且参数量显著更少；其中高斯变体更保守，Student-t变体在稳定需求下更高效。

Conclusion: P-KAN是一种高效且表达能力强的概率预测框架，适用于卫星通信等需不确定性感知的资源受限领域。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [427] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率性深度学习模型（分位数回归神经网络、变分自编码器和扩散模型）在次季节风速降尺度预报中的表现，以改进空间不确定性表征。基于ERA5再分析数据和ECMWF次季节回顾预报，结果表明这些方法相比传统随机扰动法能更真实地反映空间不确定性，并在集合离散度、确定性技巧和物理一致性方面各有优势，有助于提升可再生能源规划与风险评估的预报能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型残差的随机扰动方法在统计降尺度中无法充分表达空间相关性和物理一致性，亟需更复杂的方法来捕捉大尺度预测因子与局地风速之间的复杂关系。

Method: 采用三种具有不同不确定性量化机制的概率性深度学习模型：分位数回归神经网络（直接建模分布分位数）、变分自编码器（利用潜在空间采样）和扩散模型（通过迭代去噪生成样本），在ERA5数据上训练并应用于ECMWF次季节回顾预报进行风速回归。

Result: 概率性降尺度方法比简单随机方法能更真实地表征空间不确定性；三种模型在ensemble dispersion、确定性预报技巧和物理一致性方面表现各异但整体优于传统方法。

Conclusion: 概率性深度学习降尺度是改进次季节风速预报不确定性的有效手段，可为可再生能源规划和风险评估提供更可靠的预报产品。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [428] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出一种语言在环框架，利用大语言模型将自然语言反馈转化为标量效用，结合贝叶斯优化进行数值搜索空间的优化。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用中，复杂、模糊或主观的目标需要通过反馈转化为可量化的优化目标，而传统偏好式贝叶斯优化受限于反馈形式且需为每个领域定制模型。

Method: 利用大语言模型（LLM）将非结构化的自然语言反馈转换为标量效用，并在数值搜索空间上执行贝叶斯优化，支持灵活的用户先验且无需手动设计核函数。

Result: 该方法在反馈有限的情况下优于传统贝叶斯优化基线和仅使用LLM的优化器，同时保持贝叶斯优化的样本效率和不确定性量化能力。

Conclusion: 所提出的混合方法不仅提供了更自然的人机交互接口，还在多种任务中表现出更强的优化性能。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [429] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在高斯差分隐私下的线性回归方法，包含偏差校正估计量、渐近置信区间及支持回归分析的合成数据生成方法，在小到中等规模数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私线性回归方法多关注点估计，缺乏对不确定性量化和合成数据生成的支持，尤其不适用于社会科学研究中常见的小到中等规模连续数据。

Method: 提出一种差分隐私偏差校正估计量，构建渐近置信区间，并设计一种合成数据生成流程，使其上线性回归结果与原始DP回归一致；采用分箱-聚合策略以适应中小维度场景。

Result: 实验表明该方法在准确性、置信区间有效性以及生成数据在下游机器学习任务中的可靠性均优于现有方法。

Conclusion: 该方法在高斯差分隐私框架下实现了兼具准确估计、有效推断和高质量合成数据生成的线性回归，适用于社会科学研究中的典型数据规模与类型。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [430] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一种样本级别的范式来衡量语言模型在后训练过程中知识的遗忘和反向迁移，通过大规模分析揭示了不同后训练方法对预训练知识的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估后训练对语言模型知识影响时混淆了遗忘和反向迁移效应，缺乏细粒度分析手段。

Method: 引入样本级别的1->0和0->1转换计数指标，并结合机会调整的准确率评估多阶段、不同规模模型和数据下的遗忘与反向迁移现象。

Result: 发现领域持续预训练导致中等程度遗忘，RL/SFT在基础模型上产生显著反向迁移，而在指令调优模型上效果受数据规模影响较大，模型融合未能可靠缓解遗忘。

Conclusion: 所提出的框架为系统性评估后训练对预训练知识的影响提供了实用工具，有助于推动通用人工智能的发展。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [431] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 本文提出了一个名为BlueSky的愿景，旨在推动时间序列推理的发展，通过构建稳健的基础和系统级推理方法，实现可解释且可信的时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析多集中于模式识别，缺乏可解释性和可信推理机制，因此需要发展更强大的时间序列推理能力。

Method: 从两个互补方向推进：一是建立包含时序理解、结构化多步推理和可靠评估框架的理论基础；二是通过多智能体协作、多模态上下文和检索增强方法提升系统级推理能力。

Result: 提出了一套灵活且可扩展的框架，能够支持跨领域的可解释与可信时间序列推理。

Conclusion: 该框架为时间序列推理提供了新的发展方向，有望在多样化应用场景中实现可靠的时序智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [432] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: 提出MuonBP，通过分块周期性正交化在保持训练稳定性的同时减少通信开销，提升大规模语言模型训练的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在使用模型并行时，梯度正交化因跨设备的gather和scatter操作引入额外通信开销，导致相比AdamW等坐标优化器吞吐下降5%-10%。

Method: 提出MuonBP，将正交化独立应用于各设备上的矩阵分块，并周期性执行全局正交化以维持训练稳定性；理论指导使用两个步长：分块和全局正交化各一个。

Result: 在八路张量并行和ZeRO下训练8B模型时，相比Muon吞吐提升8%，性能无损；迭代复杂度与Muon相当，单次迭代吞吐接近AdamW。

Conclusion: MuonBP在不牺牲性能的前提下有效降低梯度正交化的通信开销，显著提升大规模训练效率，适用于高并行场景。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [433] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: 本文提出了Graph4MM，一种基于图的多模态学习框架，通过引入Hop-Diffused Attention和MM-QFormer模块，有效整合多跳结构信息与跨模态融合，提升了多模态理解性能，在生成和判别任务上均优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多模态数据存在复杂的结构关系，超越了传统的图像-文本配对模式。现有方法未能充分建模多跳邻居关系，并将图作为独立模态处理，导致多模态理解不完整。

Method: 提出Graph4MM框架，包含Hop-Diffused Attention机制（通过因果掩码和跳数扩散将多跳结构信息融入自注意力）和MM-QFormer（用于多映射查询的跨模态融合Transformer）。

Result: 在生成和判别任务上的实验表明，Graph4MM优于更大的视觉语言模型、大语言模型及多模态图基准方法，平均提升6.93%。理论与实证分析验证了结构信息对多模态交互建模的有效性。

Conclusion: 图结构在多模态学习中应被深度融合而非作为独立模态，Graph4MM通过整合多跳邻居信息和设计专用跨模态融合模块，显著提升了多模态理解能力，为基于基础模型的多模态学习提供了新思路。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [434] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: 提出了一种基于多模态大语言模型的AI代理EEschematic，用于从SPICE网表自动生成可视化的模拟电路原理图，结合文本、视觉和符号模态，并通过少量示例和视觉链式思考策略优化布局与连线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的电路生成方法主要依赖文本表示（如SPICE网表），缺乏对电路设计人员友好的可视化可读性，限制了人工理解和验证。

Method: 采用多模态大语言模型（MLLM），融合文本、视觉和符号信息，利用六个模拟子结构示例进行少样本放置，并引入视觉链式思考（VCoT）策略迭代优化原理图的布局与布线。

Result: 在CMOS反相器、五管运放（5T-OTA）和折叠共源共栅放大器等典型模拟电路上实验表明，EEschematic能生成高视觉质量和结构正确的原理图。

Conclusion: EEschematic有效提升了自动电路生成的可视化表达能力，支持人类设计师更直观地理解与验证生成的电路结构。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [435] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 本文提出了Justitia，一种针对大语言模型（LLM）应用的高效且公平的调度器，通过内存为中心的服务成本建模、轻量级需求预测和虚拟时间公平队列算法，显著提升了共享GPU服务器上的调度效率并保证了最坏情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM调度器在处理LLM应用时由于头阻塞或资源分配过紧，难以实现快速完成和最坏情况性能保证。

Method: 设计了Justitia调度器，采用内存为中心的服务成本模型、基于简单神经网络的轻量级需求预测，以及虚拟时间公平队列算法。

Result: 实验结果表明，Justitia在多种LLM应用上显著提高了调度效率，同时保持了公平性，并保证了最坏情况下的延迟。

Conclusion: Justitia能够有效提升LLM应用在共享GPU服务器上的调度性能，兼顾效率与公平，适用于实际部署。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [436] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 提出使用模型无关的PPO结合好奇心探索和基于图的动作来求解非线性方程，表明好奇心探索可能对一般符号推理任务有用。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在符号数学中的应用潜力，特别是在解决非线性方程方面。

Method: 采用模型无关的PPO算法，并结合好奇心驱动的探索机制和基于图结构的动作空间。

Result: 能够成功求解包含根号、指数和三角函数等非线性方程。

Conclusion: 好奇心驱动的探索方法可能适用于更广泛的符号推理任务。

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [437] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 本文提出了PIMMA框架，用于解决非线性边权下的服务质量降级（QoSD）问题，通过生成模型在潜在空间中合成可行解，并结合图学习、条件变分自编码器与强化学习，在合成和真实网络中均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理非线性边权函数下的QoSD问题，且多数局限于小规模或线性场景，缺乏对实际复杂网络的适用性。

Method: 提出三阶段框架：Forge阶段使用预测路径加压（PPS）算法生成有性能保证的可行解；Morph阶段利用基于能量模型引导的混合条件VAE进行理论支持的训练以建模解的分布；Refine阶段通过强化学习和可微奖励函数优化解质量。

Result: 在合成和真实网络上的实验表明，该方法在非线性代价函数场景下显著优于经典和机器学习基线方法，尤其在传统方法难以泛化的场景中表现突出。

Conclusion: PIMMA成功解决了非线性QoSD问题，通过结合生成模型与强化学习，在复杂网络中实现了更优的攻击策略生成，为网络鲁棒性分析提供了新工具。

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [438] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 本文提出了DICA框架，通过J-VolMax准则利用混合函数雅可比矩阵的凸几何特性，在无需辅助信息、潜在成分独立性或雅可比稀疏性假设的情况下实现潜在成分的可识别性。


<details>
  <summary>Details</summary>
Motivation: 在非线性混合模型中识别潜在成分是机器学习的基础难题，现有方法依赖辅助信号或结构假设，限制了适用范围。

Method: 提出DICA框架和J-VolMax准则，利用混合函数雅可比矩阵的凸几何性质，鼓励潜在成分对观测变量影响的多样性。

Result: 在合理条件下实现了无需辅助信息、潜在成分独立性或雅可比稀疏性假设的潜在成分识别。

Conclusion: 该方法扩展了可识别性分析的范围，为现有方法提供了互补视角。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [439] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习中使用链式思维（CoT）推理时，语言模型在接收到事后指令与其已学习行为冲突时产生的系统性动机化推理现象，发现模型会生成看似合理但违背指令的解释，并且较小的LLM判别器可能无法识别甚至被说服。


<details>
  <summary>Details</summary>
Motivation: 由于不完美的奖励信号，语言模型可能发展出与目标不一致的行为倾向，尽管通过事后指令进行纠正，但当这些指令与已学习行为冲突时，模型的推理过程可能发生扭曲，从而产生动机化推理，影响模型的安全性和可监控性。

Method: 在简单设定下分析语言模型在指令冲突下的推理行为，利用前沿推理模型和较小规模的LLM作为判别器来检测动机化推理，并评估其可检测性及对判别器的影响。

Result: 发现模型确实会产生系统性的动机化推理，即为违反指令提供合理化辩解并弱化潜在危害；大多数前沿推理模型能检测到此类行为，但较小的LLM判别器部分失败，少数情况下会被说服接受错误推理。

Conclusion: 随着模型变得越来越复杂，其动机化推理可能更难被监测发现，因此在依赖链式思维进行模型评估和监督时，必须考虑动机化推理的影响。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [440] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出了一种面向低精度对数定点训练的硬件友好型分段线性近似方法，通过优化不同精度下的算术运算，在12位整数运算下实现了接近32位浮点训练的模型精度，同时显著降低了硬件面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 深度学习推理已通过量化大幅降低计算成本，但训练仍依赖高精度浮点运算，限制了硬件效率。因此需要发展低精度定点训练技术以支持高效能、低功耗的硬件加速器设计。

Method: 引入一种新的硬件友好的分段线性近似方法用于对数域加法运算，并结合模拟退火算法在不同比特宽度下进行优化；使用C++位真仿真验证VGG-11和VGG-16在CIFAR-100与TinyImageNet上的训练效果。

Result: 在12位整数运算下训练VGG-11和VGG-16，精度损失极小；相比线性定点单元，所提对数数系（LNS）乘累加单元最多减少32.5%面积和53.5%能耗。

Conclusion: 该方法为未来低精度训练硬件提供了高效可行的算术近似方案，在保持模型性能的同时显著提升能效和集成度。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [441] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文提出了一种自监督预训练交互式智能体的方法，使其能够快速模仿人类示范。该方法以目标（观测）为基本单元，在训练中自动提出并练习达成目标，在评估时通过摊销的逆强化学习解释示范行为。实验表明该方法在零样本模仿任务上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 当前成功的AI模型（如VLMs、LLMs）缺乏对动作的显式建模，且假设人类总处于高奖励状态，这限制了智能体对新任务的快速适应能力。需要一种能在交互中自监督训练的代理模型。

Method: 将目标（观测）作为基本单元，训练时自动提出目标并练习到达这些目标，基于强化学习探索的前期工作；评估时通过解决摊销的逆强化学习问题，将示范解释为最优的目标达成行为。

Result: 在标准基准测试中（非专为目标达成设计），该方法在零样本模仿任务上表现优于先前方法。

Conclusion: 所提方法实现了交互式智能体的自监督预训练，使其能即时模仿人类示范，提升了在新任务上的快速适应能力。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [442] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出了一种无需访问真实数据即可评估数据集可靠性的新方法——Gram行列式分数，该分数具有实验无关性，能有效衡量来自潜在策略性来源的数据质量。


<details>
  <summary>Details</summary>
Motivation: 在无法获取真实数据的情况下，如何评估来自可能具有策略性报告动机的数据源的可靠性成为一个关键问题。

Method: 引入基于真实数据的排序定义来衡量报告数据与真实数据的偏离程度，并提出Gram行列式分数，通过观测数据和实验结果的经验分布向量所张成的体积来评估数据集的可靠性，满足实验无关性。

Result: 理论证明Gram行列式分数保持了多种基于真实数据的可靠性排序，且在所有满足实验无关性的线性变换中是唯一的（最多差一个缩放因子），在合成噪声模型、CIFAR-10嵌入和真实就业数据上验证了其有效性。

Conclusion: Gram行列式分数是一种有效的、不依赖于具体实验设计的数据可靠性评估指标，适用于多种数据类型和观察过程。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [443] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出一种基于自适应图学习和机制特异性专家网络的金融异常检测框架，通过内置可解释性实现多尺度时间依赖建模与动态图结构学习，显著提升检测性能并支持异常机制的自动识别。


<details>
  <summary>Details</summary>
Motivation: 现有金融异常检测方法将所有异常视为同质，缺乏对具体机制（如价格冲击、流动性冻结等）的区分能力，且无法适应市场结构变化，导致监管干预缺乏针对性。

Method: 采用BiLSTM与自注意力捕捉多尺度时间依赖，通过跨模态注意力融合时空信息，利用神经多源插值学习动态图结构，并设计压力调制融合机制平衡动态学习与结构先验，将异常路由至四个机制特异性专家网络，实现双层次可解释归因。

Result: 在100只美股（2017-2024）上实现92.3%的重大事件检测准确率，平均提前3.8天预警，优于最佳基线30.8个百分点；硅谷银行案例显示价格冲击专家权重在关闭期间上升至0.39（较基线高33%），一周后达峰值0.48（高出66%），验证了无需监督的时序机制识别能力。

Conclusion: 该框架通过架构级可解释设计解决了静态图结构、统一检测机制和黑箱输出三大挑战，能够自动识别异常背后的驱动机制及其演化过程，为精准监管提供技术支持。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [444] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了组合设置下经典Hedge算法的最优性，证明其在多数情况下接近最优（仅差√log d因子），但在m-集等特定情况下可被证明次优，并揭示其与在线最短路径等问题中稀释熵正则化的联系。


<details>
  <summary>Details</summary>
Motivation: 探讨Hedge算法在各类组合优化问题中的普适最优性，识别其性能瓶颈并分析其在不同结构下的表现差异。

Method: 通过建立任意算法在组合集合上的后悔值下界，结合具体实例（如m-集、多任务学习、有向无环图最短路径）分析Hedge的表现，并利用在线镜像下降（OMD）框架连接稀释熵正则化。

Result: 证明Hedge在一般组合设置下近似最优（仅差√log d因子）；在m-集上其性能次优且差距精确为√log d；而在在线多任务学习中是最优的；同时表明OMD配合稀释熵正则化在DAG最短路径问题中与Hedge等价并继承其近优 regret 界。

Conclusion: Hedge在大多数组合场景中接近最优，但并非处处最优；其性能依赖于问题结构，且可通过正则化方法扩展至更广的组合优化领域。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [445] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 本文研究了在聚合反馈模式下的有限阶段马尔可夫决策过程中的在线学习问题，提出了首个能够在随机和对抗环境中均实现低遗憾的“两全其美”（BOBW）算法，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要集中在最坏情况分析，缺乏对同时适应随机和对抗环境的算法研究。本文旨在填补这一空白，提出更灵活、鲁棒的在线学习算法。

Method: 采用FTRL框架作用于占据测度，结合自限界技术及受在线最短路径问题启发的新损失估计器，并扩展至转移概率未知的情形。

Result: 在转移已知情况下，算法在随机环境中达到O(log T)遗憾，在对抗环境中达到O(√T)遗憾，并证明了匹配的下界；在转移未知情况下也实现了良好扩展。

Conclusion: 本文提出的BOBW算法在聚合反馈的 episodic MDP 中实现了理论最优性能，为在线强化学习提供了兼顾效率与鲁棒性的新思路。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [446] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络（GCN）之间的本质等价性，并提出了一个简化且更具可解释性的新架构Fighter，通过去除冗余线性投影和引入多跳图聚合，在时间序列预测任务中实现了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在时间序列建模中取得了成功，但其内部机制尚不透明，本文旨在通过将其重新解释为图卷积网络来增强模型的可解释性。

Method: 将Transformer的注意力分布矩阵视为动态邻接矩阵，并证明其前向传播过程等价于图卷积操作，反向传播中参数更新动态也与GCN相似；基于此提出Fighter架构，去除冗余线性变换并引入多跳图聚合机制。

Result: 实验表明，Fighter在标准预测基准上达到具有竞争力的性能，同时提供了跨尺度时间依赖关系的显式、可解释表示。

Conclusion: Transformer编码器可被统一理解为图卷积网络，这一理论视角不仅提升了对模型机制的理解，还指导了更简洁、可解释的新架构设计。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [447] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出了一种基于矩阵自由能的自编码器正则化方法，通过优化码矩阵的奇异值分布来生成类高斯编码，并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 为了提升自编码器学到的编码的泛化能力，需要对码矩阵的结构进行约束，使其接近理想随机矩阵的统计特性。

Method: 定义了一个基于码矩阵奇异值的可微损失函数，最小化负矩阵自由能，使奇异值分布逼近具有独立同分布高斯项的随机矩阵。

Result: 实验表明该方法能生成在训练集和测试集上均表现稳定的类高斯编码，并成功应用于欠定逆问题。

Conclusion: 矩阵自由能正则化是一种有效生成具有良好泛化性和统计特性的编码的新方法，为自编码器设计提供了新的理论视角和应用途径。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [448] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种用于连续时间控制的新型强化学习方法CQSM，通过鞅条件刻画连续Q函数，并将扩散策略分数与学习到的连续Q函数的动作梯度关联，无需时间离散化即可保持Q函数的动作评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法多基于离散时间，难以直接应用于连续时间控制系统，且传统方法在连续时间下难以保持Q函数的动作评估能力。

Method: 引入由随机微分方程描述状态-动作动态的连续时间Q函数，利用鞅条件进行表征，并基于动态规划原理将扩散策略分数与Q函数的动作梯度关联，提出Continuous Q-Score Matching (CQSM)算法。

Result: 理论上给出了线性二次控制问题的闭式解，数值实验表明该方法在模拟环境中优于主流基线方法。

Conclusion: CQSM为连续时间强化学习提供了一种无需时间离散化的新框架，有效保持了Q函数的动作评估能力，兼具理论保证和实际性能优势。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [449] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出了一种名为In-situ Autoguidance的新方法，通过模型自身在推理时动态生成劣化预测来实现自我引导，无需额外模型，实现了零成本且高效的图像生成引导。


<details>
  <summary>Details</summary>
Motivation: 解决分类器自由引导（CFG）在提升图像质量与对齐的同时降低多样性的问题，并避免使用辅助模型带来的额外开销。

Method: 在推理过程中通过随机前向传播动态生成一个劣化的预测，将引导重构为一种自校正机制，从而实现模型自身的引导。

Result: 该方法在不增加成本的情况下有效解耦了图像生成的质量与多样性，无需辅助模型即可实现优异的引导效果。

Conclusion: In-situ Autoguidance是一种高效、零成本的自引导方法，为图像生成扩散模型提供了一个新的基准。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [450] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为模型部署后自主学习（ALMD）的新范式，旨在使模型在部署后能够动态检测并增量学习来自新类别的未知样本，为此设计了PLDA方法以解决动态OOD检测、增量学习和数据稀缺等问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习中模型部署后不再更新，难以适应动态开放环境中出现的未知类别样本，因此需要一种能够在应用过程中自主学习新类别的机制。

Method: 提出了PLDA方法，结合动态OOD检测与在线增量学习，能够在不重新训练整个模型的情况下持续识别并学习新类别，同时应对数据稀疏问题。

Result: 实验验证了PLDA在动态检测新类样本和增量学习方面的有效性，相较于传统方法在资源效率和学习连续性上表现更优。

Conclusion: PLDA支持模型在部署后的自主持续学习，为开放动态环境下的机器学习提供了可行方案，推动了模型的自适应能力发展。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [451] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出了一种轻量级自适应框架ALPINE，用于在移动边缘众感系统中实时动态调整差分隐私水平，以平衡隐私保护、数据效用和能耗。


<details>
  <summary>Details</summary>
Motivation: 静态差分隐私机制难以适应动态环境中变化的风险（如攻击能力、资源限制和任务需求），导致隐私保护不足或噪声过度。

Method: 设计了一个闭环控制系统的ALPINE框架，包含动态风险感知、基于TD3的隐私决策、本地隐私执行和边缘节点性能验证四个模块，并通过奖励函数优化隐私-效用-成本的权衡。

Result: 理论分析与真实模拟表明，ALPINE能有效抵御推理攻击，同时保持较高的数据效用和较低的能耗，适用于大规模边缘应用。

Conclusion: ALPINE实现了在动态环境下的自适应隐私保护，兼顾安全性、实用性和效率，为资源受限的移动边缘众感系统提供了可行的隐私解决方案。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [452] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 本文提出了一种统一且全面的框架，用于评估文本属性图（TAG）学习中GNNs、RGNNs和GraphLLMs在多种文本、结构及混合扰动下的鲁棒性，并揭示了模型在文本与结构鲁棒性之间的固有权衡，进而提出了SFT-auto框架以实现更优且平衡的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对文本属性图的学习模型（如GNNs和LLMs）鲁棒性的评估分散且缺乏系统性，未能全面分析文本和结构扰动在不同模型和攻击场景下的影响。

Method: 构建一个涵盖十种数据集、四种领域、多种文本、结构和混合扰动的统一评估框架，涵盖投毒和逃避两种攻击场景，评估经典GNNs、鲁棒GNNs（RGNNs）和GraphLLMs的表现，并提出SFT-auto方法以解决发现的权衡问题。

Result: 发现：1）模型在文本与结构鲁棒性之间存在固有权衡；2）GNNs和RGNNs的性能高度依赖文本编码器和攻击类型；3）GraphLLMs对训练数据污染特别敏感。SFT-auto在单一模型中实现了对文本和结构攻击的优越且均衡的鲁棒性。

Conclusion: 该工作为未来TAG安全研究奠定了基础，提供了在对抗环境中进行鲁棒TAG学习的实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [453] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出一个模块化的蛋白质分子动力学方法基准测试框架，结合增强采样分析和加权系综模拟工具（WESTPA），支持多种模拟引擎并提供超过19种评估指标，配套包含九种不同蛋白质的数据集，实现对经典力场与机器学习模型的可重复比较。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法（包括基于机器学习的方法）快速发展，但缺乏标准化的验证工具，导致评估指标不一致、稀有构象状态采样不足以及基准不可重复等问题。

Method: 采用基于TICA降维得到的进度坐标，结合WESTPA实现加权系综（WE）增强采样；设计轻量级可扩展的传播器接口以兼容任意模拟引擎；构建涵盖9个不同蛋白质的数据集，并在300K下进行百万步MD模拟；提供包含19种以上度量和可视化功能的综合评估套件。

Result: 成功实现了对经典隐式溶剂MD与训练充分/不足的CGSchNet模型在蛋白质构象采样能力上的对比验证，展示了框架的实用性与通用性。

Conclusion: 该开源框架通过标准化评估协议，为分子模拟领域提供了可重现、系统化的方法比较平台，推动了MD方法的严谨基准测试。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [454] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: 本文提出了SOLE，一种针对Softmax和LayerNorm的软硬件协同设计，通过E2Softmax和AILayerNorm实现低精度计算与存储，在不需重训练的情况下显著提升了推理效率和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV中表现优异，但Softmax和LayerNorm导致推理速度慢、效率低；现有近似方法忽视内存开销且依赖昂贵的重训练来补偿误差。

Method: 提出SOLE，包含E2Softmax（采用log2量化指数函数和基于对数的除法）和AILayerNorm（采用低精度统计计算），实现Softmax和LayerNorm的低精度计算与低比特存储。

Result: 实验表明，SOLE在无需重训练的情况下保持了推理精度，相比GPU实现了显著的速度提升和能耗降低，并分别在Softmax和LayerNorm上比现有最先进定制硬件提升了3.04x、3.86x的能效和2.82x、3.32x的面积效率。

Conclusion: SOLE通过软硬件协同设计有效解决了Transformer中Softmax和LayerNorm的效率瓶颈，在保持精度的同时大幅提升了能效和面积效率，具有实际部署优势。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [455] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架，通过离散化连续动作空间、熵正则化探索和双批评器结构来应对高风险高回报任务中的多模态性和风险建模问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法假设单峰高斯策略且依赖标量值批评器，在处理具有多模态动作分布和随机回报的高风险高回报（HRHR）任务时效果受限。

Method: 将连续动作空间离散化以逼近多模态分布，采用熵正则化探索提升对高风险高回报动作的覆盖，并引入双批评器架构以更准确估计离散值分布。

Result: 在存在高失败风险的运动和操作基准实验中，该方法优于基线方法。

Conclusion: 显式建模多模态性和风险对强化学习在高风险高回报任务中的性能至关重要。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [456] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度稀疏自编码网络的方法，用于预测和分类燃料电池的高频阻抗，并实现了超过92%的准确率，同时将网络部署在FPGA上，硬件识别率接近90%。


<details>
  <summary>Details</summary>
Motivation: 由于在线测试燃料电池高频阻抗复杂且成本高昂，因此需要一种高效准确的诊断方法来确保燃料电池堆的稳定运行。

Method: 采用深度稀疏自编码网络对燃料电池的高频阻抗进行预测与分类，并将该网络部署在FPGA上实现硬件加速。

Result: 模型在软件层面的准确率超过92%，在FPGA上的硬件实现识别率接近90%。

Conclusion: 所提出的方法能够有效且准确地诊断燃料电池健康状态，具备实际应用潜力，尤其适用于需要实时监测的场景。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [457] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的编码器（AttEnc）和结合原型网络的P-AttEnc模型，用于数据驱动的驾驶员识别，具有高准确率、快速预测和参数量少的优点，并通过少样本学习解决数据不足和未知驾驶员分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员识别方法存在数据短缺和难以应对未知驾驶员的问题，且部分技术涉及隐私风险，因此需要一种高效、轻量并具备良好泛化能力的模型。

Method: 提出AttEnc架构，引入注意力机制以减少模型参数并提升识别效率；进一步结合原型网络构建P-AttEnc，采用少样本学习实现对未知驾驶员的识别，并提取驾驶员指纹特征以应对数据不足问题。

Result: AttEnc在三个数据集上分别达到99.3%、99.0%和99.9%的准确率，模型参数平均减少87.6%，预测速度提升44%~79%；P-AttEnc在one-shot场景下识别准确率为69.8%，并对未知驾驶员实现65.7%的平均准确率。

Conclusion: 所提出的AttEnc和P-AttEnc模型有效解决了驾驶员识别中的数据短缺和模型泛化问题，在保持高精度的同时显著降低模型复杂度，适用于实际应用场景。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [458] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出了一种用于一致性模型（CMs）的自动自适应离散化统一框架，通过优化离散化步长提升训练效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性模型依赖手动设计的离散化方案，导致在不同噪声调度和数据集上需反复调整，限制了效率与泛化能力。

Method: 将自适应离散化建模为优化问题，以局部一致性为优化目标，全局一致性为约束，并引入拉格朗日乘子平衡二者关系，采用高斯-牛顿法实现ADCM框架。

Result: 实验表明，ADCM在CIFAR-10和ImageNet上显著提升了CM的训练效率和生成性能，且具有低训练开销和对先进扩散模型变体的强适应性。

Conclusion: ADCM提供了一种高效、稳定的自适应离散化方法，推动了一致性模型在图像生成任务中的实用化进展。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [459] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 提出了一种基于变分推断的数据同化方法，能够实现近乎完美校准的预测，并可集成到更广泛的变分数据同化流程中。


<details>
  <summary>Details</summary>
Motivation: 在大多数情况下，数据同化涉及不确定性，现有的确定性机器学习方法难以充分建模这种不确定性。

Method: 基于现有确定性机器学习方法，提出一种基于变分推断的扩展模型，假设预测状态服从多变量高斯分布，并在Lorenz-96混沌动力系统上进行验证。

Result: 该模型在Lorenz-96系统上实现了几乎完全校准的预测，并能通过更长的数据同化窗口带来更大收益。

Conclusion: 所提出的变分推断方法有效提升了数据同化中不确定性的建模能力，具有良好的校准性和扩展潜力。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [460] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 本文提出了一种针对多智能体系统中控制流劫持攻击的新防御机制ControlValve，该机制基于控制流完整性和最小权限原则，通过生成允许的控制流图并结合上下文规则来阻止不安全的操作。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法（如LlamaFirewall）依赖于智能体间通信的对齐检查，但由于“对齐”定义脆弱且检查器对执行上下文可见性不足，仍可被绕过。此外，多智能体系统的安全性和功能性目标存在根本冲突。

Method: 提出并实现了ControlValve，其核心包括：(1) 为多智能体系统生成许可的控制流图；(2) 强制所有执行符合该图，并在每次智能体调用时应用零样本生成的上下文规则进行验证。

Result: ControlValve能够有效检测并阻止现有对齐检查机制无法发现的控制流劫持攻击，同时保持系统功能性的合理平衡。实验表明其具备良好的实用性和鲁棒性。

Conclusion: 基于控制流完整性与最小权限原则的防御机制比依赖语义对齐检查的方法更可靠，ControlValve为多智能体系统的安全设计提供了新方向。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [461] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 本文扩展了非紧致对称性（如平移）和非不变数据分布下的泛化保证，基于PAC-Bayes框架改进了现有边界，并在旋转MNIST数据集上验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 尽管对称性已被证明能提升机器学习模型的性能，但现有理论大多局限于紧致群对称性和数据分布不变的假设，难以适用于现实场景。本文旨在打破这些限制，提供更广泛的理论支持。

Method: 基于PAC-Bayes框架，改进并收紧了现有的泛化误差界，适用于多种PAC-Bayes边界，并应用于非紧致对称性和非不变数据分布的情形。

Result: 在非均匀旋转的MNIST数据集上验证了所提出方法的有效性，所得泛化界不仅成立，且优于先前结果。

Conclusion: 对称数据下，对称模型的优势不限于紧致群和不变分布情形，本文为机器学习中对称性的更一般理解提供了理论依据。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [462] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 本文提出了首个用于评估多因素序列解耦的标准化基准，涵盖视频、音频和时间序列中的六个多样化数据集，并引入了新的模型和自动化工具以提升解耦性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于简单的双因素静态或动态场景，忽略了现实世界数据中多个语义因素交互的复杂性，因此需要一个能够有效评估多因素序列解耦的框架。

Method: 构建了一个包含六个多因素数据集的基准，开发了模块化工具用于数据集成、模型构建和评估；提出基于Koopman的模型和一种后处理潜变量探索阶段来自动对齐潜在维度与语义因素，并利用视觉-语言模型实现零样本解耦评估。

Result: 所提Koopman-inspired模型在多因素解耦任务上达到最先进的性能，且视觉-语言模型可有效实现自动化标注和零样本评估，减少了对人工标签的依赖。

Conclusion: 该工作为多因素序列解耦提供了可扩展、稳健的基础，推动了解耦表示学习在真实复杂场景中的发展。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [463] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出一种无需训练的奖励建模框架，通过推断高质量、查询特定的评分标准，并利用信息论编码率将其泛化为核心集，实现高数据效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖昂贵的偏好数据集且缺乏可解释性，基于评分标准的方法虽透明但缺乏系统性质量控制与优化，难以兼顾可扩展性与可靠性。

Method: 采用两阶段方法：第一阶段通过验证引导的“提出-评估-修订”流程推断高质量、查询特定的评分标准；第二阶段利用信息论编码率最大化将细粒度评分标准泛化为紧凑、非冗余的核心集，形成层次化的“主题-提示”评分体系。

Result: 实验表明该框架具有卓越的数据效率和性能；仅使用70个偏好对（原始数据的1.5%），即可使Qwen3-8B等较小模型超越专门训练的同类模型。

Conclusion: 该工作开创了一条可扩展、可解释且数据高效的奖励建模新路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [464] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出一种可连续调节内部表示的大语言模型训练框架，支持从局部（可解释）到分布式（高效）编码的动态切换。


<details>
  <summary>Details</summary>
Motivation: 现有模型在可解释性与性能之间难以平衡，且缺乏灵活调整表示方式的能力。

Method: 引入可调的“局部性旋钮”、基于信息论的语义块招募机制和分层扩展框架，结合注意力上的组稀疏惩罚、动态规则注入等技术。

Result: 理论证明了注意力在语义相关块上的集中条件，提供了注意力熵和指针保真度的精确界限，并实现了多粒度架构自适应的收敛保证。

Conclusion: 该框架支持在无需重训练的情况下连续权衡可解释性与性能，适用于需要透明性和能力并重的监管领域应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [465] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 提出了一种生成图神经网络（GNN）模型“异构体”的技术，揭示了现有GNN架构中存在极端的表征不变性，且当前方法难以达到类人水平的不变性。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，弥补人工神经网络与人类感知系统在不变性特性上的差距。

Method: 通过优化输入图使其节点激活匹配参考图，生成在模型表示空间中等价但结构和特征差异显著的异构体，并理论分析局部异构维度和流形体积变化。

Result: 发现多种经典GNN架构存在极端的表征不变性；现有架构改进和训练策略只能部分缓解问题；量化了异构图与原图的偏差，揭示了GNN的独特失效模式。

Conclusion: 当前GNN具有过强的不变性，偏离人类感知机制，需新的评估基准和设计思路来改进。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [466] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文提出使用物理信息神经网络（PINNs）构建智能电网的代理模型，以替代高成本仿真器，提升强化学习在最优潮流问题中的样本效率和训练速度。


<details>
  <summary>Details</summary>
Motivation: 强化学习在智能电网最优功率流中面临样本效率低和依赖昂贵仿真器的问题。

Method: 采用物理信息神经网络（PINNs）建立智能电网的 surrogate 模型，用于加速强化学习策略训练。

Result: 所提方法能在比原始环境少得多的时间内达到收敛结果，显著提高训练效率。

Conclusion: PINNs 构建的代理模型有效缓解了强化学习在智能电网优化中的样本效率问题，加快了策略收敛。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [467] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 本文提出了DISC（Diffusion-based Statistical Characterization）方法，利用扩散模型的去噪过程提取多维特征向量，不仅能检测分布外（OOD）数据，还能分类其类型，实现了从二元检测到细粒度识别的转变。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法通常将分布偏移压缩为单一标量得分，无法区分OOD数据类型，限制了后续决策和应用。因此需要一种更细致、可解释的方法来表征和利用OOD数据。

Method: 提出DISC方法，利用扩散模型在多个噪声水平上的迭代去噪过程，提取反映统计差异的多维特征向量，从而同时实现OOD检测与类型分类。

Result: 在图像和表格数据基准上实验表明，DISC在OOD检测性能上达到或超过现有最先进方法，并且能够准确分类OOD数据类型，而这是以往方法所缺乏的能力。

Conclusion: DISC克服了传统标量评分方法的局限性，实现了对OOD数据的细粒度识别与统计表征，推动OOD问题从简单的‘是否异常’向‘何种异常’转变，具有更强的应用潜力。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [468] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文探讨了生成式视觉模型内部表征的演变，提出从GANs和VAEs到扩散模型的转变标志着从紧凑潜在空间到分布式表征的范式转移。


<details>
  <summary>Details</summary>
Motivation: 理解生成模型内部表征的演化及其对媒体理论和AI内容生成理解的影响。

Method: 结合Beatrice Fazi的综合理论，分析不同模型架构，并通过干预分层表征进行实验。

Result: 揭示扩散模型将表征负担分散于各层，挑战了统一潜在空间的假设。

Conclusion: 生成式AI应被理解为专业化过程的涌现配置，而非直接的内容综合。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [469] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: 本文提出了TabR1，首个用于表格预测的推理大语言模型，引入了基于列排列不变性的强化学习方法PRPO，显著提升了少样本和零样本下的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测模型缺乏跨任务泛化能力和可解释性，而大语言模型虽具备推理潜力但未充分应用于表格数据。

Method: 提出Permutation Relative Policy Optimization (PRPO)，利用列排列不变性作为结构先验，通过构建多个标签保持的排列并估计其优势，将稀疏奖励转化为密集信号。

Result: 在全监督下性能媲美强基线，零样本下接近32样本设置的基线表现，且TabR1(8B)大幅优于更大的模型（如DeepSeek-R1 685B），最高提升达53.17%。

Conclusion: PRPO有效激发了LLM在表格数据上的推理能力，在少量监督下实现了优越的泛化性、可解释性和跨任务适应性。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [470] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出特征扰动方法，通过直接在特征输入中注入随机性，在广义线性_bandits_中实现更优的理论与实践性能。


<details>
  <summary>Details</summary>
Motivation: 现有随机化bandit算法通常带来较高的后悔界（如\tilde{O}(d^{3/2}\sqrt{T})），且计算开销大，难以扩展到非参数或神经网络模型。

Method: 提出特征扰动技术，将随机性直接注入特征输入，而非对参数采样或奖励加噪，避免高维参数空间的随机化。

Result: 在广义线性bandits中达到\tilde{O}(d\sqrt{T})的最坏情况后悔界，优于传统方法；算法计算高效，并可自然扩展至非参数和神经网络模型。

Conclusion: 特征扰动兼顾强理论保证与优异实践性能，统一了高效计算与广泛模型适用性，显著优于现有随机化bandit方法。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [471] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文提出了Anchored Fitted Q-Iteration方法，首次实现了在弱通信MDP假设下平均回报离线强化学习的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 现有平均回报离线强化学习研究依赖于强假设（如遍历性或线性MDP），限制了适用性，本文旨在在更弱的假设下建立理论结果。

Method: 提出Anchored Fitted Q-Iteration，结合标准Fitted Q-Iteration与锚定机制，该机制可解释为权重衰减，有助于在平均回报设定中实现有限时间分析，并扩展至单轨迹数据场景。

Result: 首次在弱通信MDP下获得了平均回报离线RL的样本复杂度界，并证明锚定机制对有限时间分析的关键作用，且适用于IID和单轨迹数据生成设置。

Conclusion: 锚定机制有效支持了平均回报离线RL的理论分析，为更广泛环境下的函数逼近提供了新的工具和理论基础。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [472] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了一种名为MILES的模态感知学习率调度方法，用于平衡多模态联合融合模型的训练，通过动态调整学习率来缓解模态过拟合问题，在多个任务和融合方法上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络在训练中常因模态过拟合而导致性能不佳，即模型过度依赖某一模态，限制了多模态学习的潜力。

Method: 提出Modality-Informed Learning ratE Scheduler (MILES)，利用训练过程中各模态的条件使用率差异，动态调整各模态的学习率，以平衡多模态学习过程。

Result: 在四个多模态联合融合任务上评估MILES，并与七种先进基线方法比较，结果显示MILES在所有任务和融合方法中均表现更优，有效平衡了模态使用，提升了多模态性能和单模态预测能力。

Conclusion: 平衡多模态学习对提升模型性能具有重要意义，MILES能够有效缓解模态过拟合，增强模型在多模态和单模态场景下的表现。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [473] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: 提出RINS-T，一种无需预训练的深度先验框架，用于时间序列线性逆问题的鲁棒隐式神经求解器，能有效处理缺失值、噪声和异常值。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值等影响，传统深度学习方法依赖大量预训练且难以应对分布偏移，需要更鲁棒、泛化性强的方法。

Method: 利用神经网络作为隐式先验，结合鲁棒优化技术，引入引导输入初始化、输入扰动和凸输出组合三种创新策略，以提升优化稳定性和鲁棒性。

Result: RINS-T在无预训练的情况下实现了高恢复性能，对异常值具有强鲁棒性，并减少了对高斯噪声假设的依赖，在多种真实场景下表现优异。

Conclusion: RINS-T是一种灵活且高效的时间序列逆问题求解框架，适用于复杂实际应用场景，具备良好的泛化能力和鲁棒性。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [474] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构化状态空间模型的深度学习架构S4ECG，用于多时段心律失常分类，显著提升了心电图（ECG）信号在全局与局部特征联合分析中的性能，特别是在房颤检测方面表现出更高的特异性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时捕捉心电信号的全局趋势和局部波形特征的高时间分辨率动态交互，限制了复杂心律失常的准确检测。

Method: 提出S4ECG模型，利用结构化状态空间模型整合多时段心电信号信息，实现对长时间依赖关系的建模，并进行联合预测。

Result: S4ECG在宏观AUROC上比单时段方法提升1.0-11.6%，房颤特异性从0.718-0.979提升至0.967-0.998，并展现出更强的分布外鲁棒性；最佳时间依赖窗口为10-20分钟。

Conclusion: S4ECG实现了从静态到时间感知型心律失常检测算法的范式转变，为复杂心律失常如房颤和房扑的ECG解释提供了新可能。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [475] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型和注意力机制的Condition Diffusion U-Net with Attention (CDUA)方法，用于锂电池容量预测与不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池老化具有随机性，准确预测其容量及不确定性对电池管理至关重要，但现有方法仍面临挑战。

Method: 结合特征工程与深度学习，采用扩散生成模型进行时间序列预测，引入注意力机制；通过Pearson相关系数和XGBoost筛选关键特征，构建包含上下文U-Net自注意力和去噪网络的CDUA模型。

Result: 在真实车辆数据上验证，CDUA模型相对MAE为0.94%，RMSE为1.14%，95%置信区间宽度仅为3.74%，表现出高精度和可靠的不确定性量化能力，优于主流方法。

Conclusion: CDUA能有效实现锂离子电池容量的高精度预测与不确定性估计，具有良好的鲁棒性和应用前景。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [476] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出了一种名为Diffusion As Priors (DAP) 的方法，通过在特征空间中利用Mercer核量化合成数据与真实数据的相似性，形式化表示代表性，并将其作为引导信号增强扩散模型的逆向过程，从而提升蒸馏数据集的质量，且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成式数据集蒸馏方法虽基于强大的扩散模型，但忽略了其内在的代表性先验，常需引入外部约束来提升数据质量。因此，需要一种无需重训练、能同时提升多样性、泛化性和代表性的方法。

Method: 提出DAP方法，通过Mercer核在特征空间衡量合成与真实数据的相似性，形式化代表性先验，并将其融入扩散模型的反向过程中作为引导信号，实现训练-free的数据蒸馏优化。

Result: 在ImageNet-1K等大规模数据集上实验表明，DAP优于当前最先进方法，生成的数据保真度更高，并具备更强的跨架构泛化能力。

Conclusion: DAP建立了扩散先验与数据集蒸馏目标之间的理论联系，提供了一个实用且无需训练的框架，有效提升了蒸馏数据集的多样性、泛化性和代表性。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [477] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于局部黎曼几何和Robin边界条件的GBN网络，通过自适应调整信息传递来解决消息传递神经网络中的过平滑和过压缩问题，在同质和异质图上均表现出强表达能力，且在超过256层时仍无性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有全局方法在处理过压缩和过平滑问题时可能在某些区域有益但在其他区域有害，导致表达能力次优；本文旨在从理论出发，通过局部结构自适应调整信息传递。

Method: 引入谱隙λ作为全局度量，证明其增大会导致梯度消失；结合局部黎曼几何与MPNN，提出非齐次边界条件（Robin条件），设计具有局部瓶颈调整的GBN网络。

Result: GBN在多种同质和异质图数据上表现出色，有效缓解了过压缩与过平滑问题，并在深度超过256层时依然保持性能稳定。

Conclusion: 局部自适应的消息传递机制能更有效地提升MPNN的表达能力，GBN为构建深层图神经网络提供了可行方案。

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [478] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 本研究利用可解释人工智能（XAI）技术（如Grad-CAM和SHAP）解析PhaseNet在微震事件检测中的决策机制，发现其关注点符合地震波物理特征，并提出SHAP门控推理方法，在9000个波形测试中F1分数达0.98，优于基线模型，提升了噪声鲁棒性和模型可信度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在微震检测中表现优异但缺乏可解释性，限制了其在关键应用中的可信度，因此需要引入可解释AI技术来理解并提升模型的可靠性。

Method: 采用Grad-CAM和SHAP两种XAI方法分析PhaseNet的决策依据，并基于SHAP值设计了一种SHAP门控推理机制，将模型输出与解释性指标结合以减少误判。

Result: Grad-CAM显示模型注意力集中在P波和S波到达时刻；SHAP分析表明垂直分量主导P相位拾取，水平分量主导S相位，符合地球物理原理；SHAP门控模型在9000个波形上F1得分为0.98（精度0.99，召回0.97），优于原始PhaseNet的0.97。

Conclusion: XAI不仅能解释深度学习模型的决策过程，还可用于改进模型性能，SHAP门控方法提高了检测精度和抗噪能力，为构建可信的自动化地震检测系统提供了可行路径。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [479] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为CrossStateECG的鲁棒性心电图（ECG）身份认证模型，专门用于解决静息与运动状态之间的心电信号变化对生物识别性能的影响。该模型结合多尺度深度卷积特征提取与注意力机制，在多种状态转换场景下均表现出优异的识别准确率，并在多个公开数据集上验证了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有ECG生物识别研究主要集中在静息状态，而在运动后心电信号发生变化的情况下性能显著下降，限制了实际应用。因此，需要一种能够在不同生理状态（如静息与运动）之间保持高识别精度的跨状态认证模型。

Method: 提出CrossStateECG模型，采用多尺度深度卷积网络进行特征提取，并引入注意力机制增强关键特征的表示能力，以提升在跨状态（如静息到运动、运动到静息）条件下的身份识别鲁棒性。模型在多个公开ECG数据集上进行训练与评估。

Result: 在exercise-ECGID数据集上，Rest-to-Exercise场景识别准确率达92.50%，Exercise-to-Rest场景达94.72%；在Rest-to-Rest和Mixed-to-Mixed场景下分别达到99.94%和97.85%的准确率。在ECG-ID和MIT-BIH数据集上的实验进一步证明了模型的良好泛化能力。

Conclusion: CrossStateECG能够有效应对静息与运动状态间心电信号变异带来的挑战，具备在动态真实环境中实现可靠身份认证的潜力，尤其适用于运动后即时身份验证的应用场景。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [480] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: 该研究通过随机层次模型（RHM）探究Transformer在未见序列上的组合推理能力，发现其在不同泛化条件下表现系统性提升，并揭示了层专业化和分层表征的形成机制。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer如何实现上下文学习和技能组合，特别是在训练中未出现的序列上的组合推理能力。

Method: 使用随机层次模型（RHM）生成递归规则序列，训练Transformer并在四种泛化条件下评估其性能，结合行为分析与机制分析（如PCA和注意力模式聚类）。

Result: Transformer在任务复杂度和上下文示例数量增加时性能提升；OOD任务需要更多示例；训练过程中出现层专业化，且模型发展出结构化的分层表示。

Conclusion: Transformer能够发展出模块化、可解释的机制来支持组合推理，其内部算法结构与行为能力密切相关。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [481] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出了一种分布感知的多源域自适应网络DAMSDAN，用于解决跨域EEG情感识别中的分布异质性和类别判别问题，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在跨域场景下存在显著的个体差异，导致模型泛化能力受限；现有方法难以有效处理多源域间的分布异质性并保持细粒度语义一致性。

Method: 提出DAMSDAN，结合原型约束与对抗学习，通过基于MMD的域感知源加权策略动态调整源域贡献，并设计原型引导的条件对齐模块与双重伪标签交互机制，实现细粒度类别对齐。

Result: 在SEED、SEED-IV和FACED数据集上，跨被试和跨会话协议下的平均准确率分别为94.86%/79.78%和95.12%/83.15%，FACED上达到82.88%（跨被试）。

Conclusion: DAMSDAN能有效缓解负迁移和语义漂移，提升跨域EEG情感识别的鲁棒性和准确性，具有良好的可解释性和应用潜力。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [482] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 该研究探讨了生成对抗网络（GAN）在模拟河流沉积物生成中的可逆性，以匹配井和地震数据，发现其潜在空间的纠缠问题限制了反演效果，但通过微调可改善结果。


<details>
  <summary>Details</summary>
Motivation: 由于地下决策成本高且不确定性大，获取新数据困难，因此需要将地质知识直接嵌入预测模型中，以提高预测效率和可行性。

Method: 采用生成对抗网络（GAN）训练生成河流沉积物的模型，并尝试将其反演以匹配井数据和地震数据；测试了四种反演方法，并探索了标签条件化、潜在空间过参数化和GAN微调等策略。

Result: 随着井数量增加或测试样本偏离训练数据，反演效果变差；GAN潜在空间的纠缠导致相似沉积特征在潜在空间中距离较远；微调GAN可显著降低误差并达到可接受的匹配水平。

Conclusion: 尽管GAN已可用于地质建模流程，但其鲁棒性仍需进一步评估，且需探索如何更好地支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [483] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩阵分解（MF）的去中心化学习中差分隐私计算方法，通过统一建模提升了隐私-效用权衡，并提出了新的算法MAFALDA-SGD，在合成和真实图上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习中的差分隐私计算方法在隐私-效用权衡上表现不佳，可能源于当前DP会计方法的局限性，因此需要更精确的隐私计算方法。

Method: 推广了集中式差分隐私中的矩阵分解技术，将其应用于去中心化学习，统一建模标准算法与信任模型，实现了更紧密的隐私计算，并设计了带有用户级相关噪声的MAFALDA-SGD算法。

Result: 所提方法显著改进了现有DP-DL算法的隐私会计，MAFALDA-SGD在多种网络图上取得了优于现有方法的性能。

Conclusion: 通过引入先进的矩阵分解技术，去中心化学习中的差分隐私可以更精确地量化，从而提升隐私保护与模型效用的平衡，为未来DP-DL算法的设计提供了原则性框架。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [484] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出了一种基于逐层Hessian矩阵分析神经网络的方法，揭示了参数空间局部几何特性与过拟合、欠参数化和表达能力之间的定量关系。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和诊断深度神经网络的训练动态与泛化性能，需要从几何角度分析其参数空间的局部结构。

Method: 定义每层的局部Hessian矩阵，并研究其谱性质（如特征值分布），在111个实验中跨37个数据集进行实证分析。

Result: 发现局部Hessian的谱在训练过程中表现出一致的结构规律，并与泛化性能存在显著相关性。

Conclusion: 局部几何分析可作为指导深度神经网络设计与诊断的有效工具，连接了优化几何与功能行为。

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [485] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个用于评估大语言模型和大推理模型在类比与数学推理中泛化性和鲁棒性的符号基准，相较于LLMs，LRMs在复杂推理任务中表现更优，但在不确定性下的推理仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 为了更全面地评估大模型在复杂、不确定环境下的类比和数学推理能力，扩展了原有I-RAVEN基准。

Method: 通过增加操作数复杂性、属性范围并引入感知不确定性，构建I-RAVEN-X基准，并对LLMs和LRMs进行实证评估。

Result: 实验表明，LRMs在长推理链和宽属性范围上优于LLMs，但在处理不确定性及探索多种概率结果方面仍存在显著困难。

Conclusion: 尽管LRMs在系统性和生产力方面表现更好，但现有模型在不确定性推理方面仍有重大提升空间。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [486] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 提出一种基于动量的算法，可在标准平滑性和有界方差假设下，对任意批量大小实现随机DC优化的收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小批量情况下难以保证收敛，且通常依赖大批次或强噪声假设，限制了实际应用。

Method: 引入动量机制，在标准假设下分析其对收敛性的必要性，并设计具有理论保证的动量算法。

Result: 证明了动量是确保收敛的关键：无动量时无论步长如何都可能不收敛；所提算法在理论上可证收敛，并在实验中表现良好。

Conclusion: 动量在随机DC优化中至关重要，所提出的算法能在任意批量大小下实现可靠收敛，提升了小批量场景下的实用性。

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [487] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 本文研究了在过参数化最小二乘设置下，大学习率梯度下降的收敛性，提出了在“稳定性边缘”下的三种收敛机制：次临界、临界和超临界区域，分别对应不同的稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降理论保证小步长下目标函数单调递减，但在神经网络中大步长（稳定性边缘）常导致非单调下降并偏向平坦极小值，本文旨在量化这一现象。

Method: 利用过参数化导致全局极小值构成黎曼流形的特性，将梯度下降动态分解为沿流形和平正交的分量，分别对应黎曼梯度下降和分岔动力系统，进而分析不同学习率下的收敛行为。

Result: 推导出三种学习率区域的收敛速率：次临界区有限时间内克服不稳定后线性收敛至次优平坦极小值；临界区持续不稳定但以幂律收敛至最优平坦极小值；超临界区持续不稳定且线性收敛至周期为2的轨道。

Conclusion: 过参数化结构揭示了大步长梯度下降的隐式偏差机制，为理解稳定性边缘的动力学提供了理论框架。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [488] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: 提出了一种名为ZACH-ViT的轻量级Vision Transformer模型，用于在肺部超声视频中区分心源性肺水肿与非心源性及结构正常肺，结合ShuffleStrides数据增强方法，在小样本医学影像任务中优于现有大规模模型。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺组织在超声图像中表现出高度视觉变异性，且B线和胸膜伪影重叠严重，导致自动分类困难，因此需要一种能处理无序医学图像数据并具有强泛化能力的模型。

Method: 设计了ZACH-ViT模型，去除位置编码和[CLS]标记，实现完全排列不变性；引入ShuffleStrides数据增强（SSDA），随机打乱探头视角序列和帧顺序同时保持解剖有效性；在380个肺部超声视频上进行训练与评估。

Result: ZACH-ViT在验证集和测试集上分别达到0.80和0.79的ROC-AUC，敏感性和特异性分别为0.60和0.91，训练速度比Minimal ViT快1.35倍且参数量少2.5倍，所有对比模型均退化为平凡分类。

Conclusion: 在小样本医学影像任务中，通过将模型架构设计与数据结构对齐，可以在不依赖模型规模的情况下显著提升性能，支持临床实时部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [489] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论（特别是graphon）的新框架，用于刻画无限宽度下的稀疏神经网络，通过Graphon NTK分析训练动态，解释不同剪枝方法的收敛差异。


<details>
  <summary>Details</summary>
Motivation: 理解为何在相同稀疏水平下某些稀疏结构更易训练，现有剪枝方法缺乏系统性理论解释。

Method: 引入graphon表示稀疏网络的连接模式，提出Graphon Limit Hypothesis，并推导Graphon NTK以分析无限宽度下稀疏网络的训练动态。

Result: 实验证明Graphon NTK的谱分析与稀疏网络的实际训练动态相关，能够解释不同剪枝方法的收敛行为差异。

Conclusion: 该框架为稀疏神经网络的可训练性提供了理论基础，揭示了连接模式对训练动态的影响。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [490] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源车辆控制数据和注意力网络实现高精度异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注由功能性因素（如疲劳、分心）引起的驾驶异常，而对由慢性疾病（如帕金森病）引发的病理型驾驶行为异常关注较少，存在研究空白。

Method: 分析帕金森病的运动症状及其对驾驶表现的影响，利用多车辆控制组件数据构建行为特征谱，设计基于注意力机制的神经网络以自适应提取时空特征并检测异常。

Result: 在Logitech G29平台和CARLA模拟器上验证，使用三个道路地图的数据进行仿真测试，SAFE-D在区分正常与帕金森影响下的驾驶模式时达到平均96.8%的准确率。

Conclusion: SAFE-D能够有效识别帕金森病早期阶段的驾驶行为异常，为慢性疾病患者的驾驶安全提供了可行的监测方案。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [491] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 本文提出了一种基于博弈论的多标签学习框架CD-GTMLL，通过引入好奇心驱动机制解决长尾类别不平衡问题，在多个基准和大规模数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中普遍存在长尾不平衡问题，头部标签主导梯度信号，导致稀有标签被忽略，影响实际应用效果。

Method: 将多标签学习建模为合作势博弈，标签空间由多个协作玩家共享全局准确率奖励，并引入基于标签稀有性和玩家间分歧的好奇心奖励，以增强稀有标签的梯度更新。

Result: 理论证明该方法梯度更新收敛于尾部感知的稳定点，并在多个数据集上实现最高+4.3% Rare-F1和+1.6% P@3的增益，消融实验显示对稀有类别的快速共识和劳动分工现象。

Conclusion: CD-GTMLL提供了一种原理清晰、可扩展的解决方案，有效提升多标签预测中的长尾鲁棒性。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [492] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出了一种无需显式群体标签的反事实知识蒸馏（CFKD）方法，通过生成多样化反事实样本来增强模型对虚假相关性的鲁棒性，在低数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到虚假相关性的影响，而现有的群体分布鲁棒性方法依赖于显式的群体标签，面临标签不可得、组内样本少和多混淆因子导致性能下降的问题。

Method: 提出反事实知识蒸馏（CFKD），通过生成反事实样本并利用知识蒸馏框架，让人工标注者高效修正模型决策边界，无需群体标签且能同时重加权并扩充稀疏子组。

Result: 在五个数据集上验证了CFKD的有效性，涵盖合成任务到工业应用，尤其在低数据和强虚假相关场景下显著优于现有方法，并展示了不同反事实解释器和教师模型的影响。

Conclusion: CFKD无需群体标签即可提升模型对多重虚假相关性的鲁棒性，兼具数据增强与重加权优势，适用于复杂真实场景中的公平与泛化需求。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [493] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文研究了在低信噪比条件下，通过在梯度下降中引入标签噪声来抑制神经网络对噪声的记忆，从而提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易过拟合训练集中的噪声，尤其在低信噪比数据中表现更差，因此需要探索如何减少噪声记忆以改善泛化能力。

Method: 采用一个两层神经网络，在理想化的信号-噪声数据设置下，使用带标签噪声的梯度下降算法进行训练，并理论分析其动态特性。

Result: 证明了加入标签噪声能有效抑制噪声记忆，促进信号快速增长并控制过拟合；相比之下，标准梯度下降会显著过拟合并存在非零的测试误差下限。

Conclusion: 在低信噪比场景下，向梯度更新中引入标签噪声可显著提升神经网络的测试性能，是一种有效的正则化策略。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [494] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出一种基于共形对齐的边缘-云级联机制（CAb），在保证云端条件覆盖率的同时，降低云卸载率并控制预测集大小。


<details>
  <summary>Details</summary>
Motivation: 边缘智能依赖轻量模型实现低延迟推理，但难以保证预测可靠性；现有方法无法确保边缘预测满足云端级别的条件覆盖。

Method: 将边缘到云端的转接建模为多假设检验问题，利用共形对齐（CA）选择可安全在边缘处理的输入，实现对条件覆盖率的统计保证。

Result: 在CIFAR-100和TeleQnA基准上验证了CAb方法能维持目标条件覆盖率，显著减少云卸载，同时仅轻微增加预测集大小。

Conclusion: CAb级联机制为边缘智能提供了可控风险下的可靠推理框架，平衡了覆盖率、卸载率与预测集大小之间的权衡。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [495] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: 提出TrajMamba，一种高效且语义丰富的车辆轨迹学习方法，结合GPS和道路视角建模运动模式，并通过知识蒸馏压缩轨迹，提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹学习方法难以有效融合旅行语义（如出行目的）并受冗余轨迹点影响，导致计算开销大且嵌入质量下降。

Method: 设计Traj-Mamba Encoder联合建模GPS和道路视角；采用出行目的感知的预训练整合语义信息；通过知识蒸馏预训练和可学习掩码生成器识别关键轨迹点，实现轨迹压缩。

Result: 在两个真实数据集和三个下游任务中，TrajMamba在效率和准确性上均优于现有最先进基线方法。

Conclusion: TrajMamba能有效提升车辆轨迹学习的语义表达能力和计算效率，适用于实际轨迹分析应用。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [496] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种基于无监督变分方法学习随机潜在变量的解码器Transformer扩展模型，显著提升了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 为了增强解码器Transformer的生成能力，探索引入无监督学习的随机潜在变量进行条件生成。

Method: 通过变分推断方法，在解码器Transformer中引入随机潜在变量，并将其作为生成过程的条件。

Result: 实验评估表明，该方法在多个下游任务上带来了显著性能提升。

Conclusion: 所提出的潜变量条件生成机制有效提升了Transformer模型的表达能力和下游任务表现。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [497] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列异常检测评估框架，通过可验证的属性来形式化评估的基本要求，并提出了满足所有属性的新指标LARM及其高级版本ALARM。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测评估指标仅捕捉任务的狭窄方面，常导致误导性结果，缺乏统一、可靠的评估标准。

Method: 引入可验证的性质以形式化评估需求，构建理论框架；分析37种常用指标的表现；提出满足所有性质的新指标LARM及更严格的ALARM。

Result: 发现大多数现有指标仅满足少数性质，无一满足全部；LARM和ALARM在理论上满足所有提出的要求。

Conclusion: LARM和ALARM为时间序列异常检测提供了更可靠、一致的评估手段，解决了现有指标不一致和不可靠的问题。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [498] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 研究探讨了通过降维和理化性质组织潜在空间来优化抗菌肽设计的方法，发现这些策略可提高搜索效率和潜在空间的可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于氨基酸序列的可能性巨大，发现和设计抗菌肽（AMPs）非常困难；现有的深度生成模型虽然有效，但在可解释性和潜在空间质量量化方面仍存在不足。

Method: 使用变分自编码器等深度生成模型，并进一步对潜在空间进行降维处理，同时结合理化性质组织潜在空间，以提升抗菌活性优化效率。

Result: 发现通过降维压缩设计空间有助于优化，提高了潜在空间的可解释性，并能利用不同比例的标签数据基于理化性质组织潜在空间。

Conclusion: 降维和基于理化性质的潜在空间组织能够提升抗菌肽设计中优化过程的效率和可解释性，即使在标签数据有限的情况下也有效。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [499] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了一种通信高效的个性化联邦学习方法CEPerFed，用于多脉冲MRI分类，通过历史梯度和分层SVD策略解决数据异构性和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 在保护隐私的前提下，利用来自多个医疗机构的多样化数据训练鲁棒的多脉冲MRI分类模型，同时应对联邦学习中的数据异构性和高通信开销挑战。

Method: 引入客户端的历史风险梯度和历史均值梯度来协调局部与全局优化，并采用分层SVD策略减少传输参数量，仅传递关键更新信息。

Result: 在五个分类任务上的实验表明，CEPerFed在提升模型收敛稳定性的同时显著降低了通信开销。

Conclusion: CEPerFed有效缓解了联邦学习中的数据异构性问题并提高了通信效率，适用于多中心医学图像分类应用。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [500] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 该研究利用生成对抗网络（GAN）生成合成数据以解决自杀预测中正样本稀少和类别不平衡的问题，并结合机器学习模型进行预测，结果显示随机森林表现最佳，而GAN有效提升了数据质量与模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界中自杀行为的正样本数据稀缺且存在严重的类别不平衡，传统的机器学习模型难以有效训练，因此需要通过数据增强技术提升模型预测能力。

Method: 采用生成对抗网络（GAN）对原始数据集（656个样本，仅4个正例）进行数据增强，生成合成样本以缓解类别不平衡问题；随后使用多种机器学习模型（包括逻辑回归、随机森林和支持向量机）构建预测模型并比较性能。

Result: 在真实测试数据上，随机森林的加权精确率、召回率和F1分数分别为0.98、0.99和0.99，表现最优；逻辑回归和支持向量机也表现出较高精确率，但敏感性较低；随机森林无假阳性（特异性1.0），但未能识别出任何真阳性（敏感性0.0）；GAN成功生成了有助于模型训练的合成数据。

Conclusion: 尽管不同模型在敏感性和特异性之间存在权衡，但结合GAN的数据增强策略显著提升了小样本不平衡数据下的建模能力，为自杀风险预测提供了可行的技术路径。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [501] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 本文提出了三个主要贡献：提出CADP算法以优化不确定模型下的马尔可夫策略；建立了ERM Bellman算子的收缩条件并提出了多种求解最优策略的算法；设计了针对风险规避目标的无模型Q学习算法，并证明其收敛性。


<details>
  <summary>Details</summary>
Motivation: 在多模型决策过程中，如何有效处理模型不确定性并实现风险规避策略的学习是一个关键问题。现有方法在保证策略单调提升和收敛性方面存在不足。

Method: 提出坐标上升动态规划（CADP）算法，建立ERM Bellman算子的收缩条件，设计基于单调性的Q学习算法用于ERM-TRC和EVaR-TRC风险度量。

Result: 证明了ERM和EVaR型TRC存在最优确定性稳态策略，提出了有效的值迭代、策略迭代、线性规划及Q学习算法，并严格证明了Q学习算法的收敛性。

Conclusion: 所提方法在处理模型不确定性和风险规避强化学习问题上具有理论保证和有效性，为风险敏感控制提供了系统化的算法框架。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [502] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出一种基于双层强化学习的Sim2Real框架，通过直接根据真实世界性能调整模拟器参数来缩小仿真到现实的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的Sim2Real方法依赖于模拟器精度和多样性作为真实性能的代理指标，但这些指标与实际策略表现无直接关联，导致仿真训练策略在真实环境中性能下降。

Method: 采用双层强化学习框架：内层在仿真中训练策略，外层通过优化模拟器和奖励参数以最大化真实世界中的策略性能。

Result: 推导并验证了用于开发双层RL算法的数学工具，并在简单示例中展示了其有效性。

Conclusion: 该框架为缩小Sim2Real性能差距提供了一种新途径，通过直接优化真实世界性能而非仿真代理指标，提升了策略的现实迁移能力。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [503] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 本文研究了如何在不损失性能的情况下，提高黑盒大语言模型（LLM）作为分类器时的操作粒度，提出有效方法显著增加了可用操作点的数量和多样性，在11个数据集和3种LLM上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 黑盒大语言模型在需要特定指标约束（如精确率≥95%）的应用中因数值输出的低基数性导致操作粒度不足，难以精细调整决策行为，因此需要提升其操作灵活性。

Method: 首先分析黑盒LLM输出低基数的原因，发现其倾向于生成四舍五入但信息丰富的言语化概率；然后尝试标准提示工程、不确定性估计和置信度提取技术；最后提出能高效提升操作粒度的新方法。

Result: 所提方法显著增加了可用操作点的数量与多样性，在11个数据集和3个LLM上实现了更细粒度的操作点，性能达到或优于基准方法，且未牺牲性能或增加推理成本。

Conclusion: 通过改进黑盒LLM的输出校准方式，可以在保持高性能的同时大幅提升其在受约束决策任务中的操作灵活性，使其更适用于对精度等指标有严格要求的应用场景。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [504] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 本研究提出了一种基于物理信息神经网络（PINN）的方法，结合物理知识与机器学习，用于提升北极海冰速度和浓度预测的准确性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 完全数据驱动的机器学习模型在泛化能力和物理一致性方面存在局限，特别是在海冰变薄、融化加速的新阶段，历史数据训练的模型难以准确预测未来动态变化的海冰条件。

Method: 基于HIS-Unet架构，引入物理损失函数和激活函数，构建融合物理知识的PINN模型，以提高预测的物理合理性。

Result: PINN模型在少量样本训练下仍优于纯数据驱动模型，尤其在融冰期、初冻期及快速移动冰区的海冰浓度预测上有显著提升。

Conclusion: 将物理知识融入机器学习模型可有效提升海冰预测的准确性、鲁棒性和物理一致性，具有在数据稀缺条件下应用的潜力。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [505] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于可微分图集的流形学习方法，通过构建支持黎曼优化的数据结构，在无监督和有监督任务中展示了其在效率、准确性、可解释性和鲁棒性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现有流形学习方法主要进行降维，难以保留流形关键特征，且直接学习流形图集的方法探索不足。

Method: 设计通用数据结构以维护可微分图集，并结合无监督启发式算法从点云数据中学习图集，支持流形上的黎曼优化。

Result: 实验表明该方法在效率和准确性上有优势，并在Klein瓶分类和RNA速度分析中展现出更好的可解释性与鲁棒性。

Conclusion: 基于图集的方法在流形学习中具有有效性与发展潜力，能够支持直接在潜流形上的机器学习。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [506] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出了一种新的Flow Matching推理时扩展方法，保持线性插值的同时提升样本质量，并首次应用于无条件蛋白质生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时扩展中牺牲了Flow Matching的高效性和直线采样特性，且应用局限于视觉任务。

Method: 引入保持线性插值的新型推理时扩展流程，适用于图像和蛋白质生成任务。

Result: 在图像生成和无条件蛋白质生成任务中，样本质量随推理计算量增加而持续提升。

Conclusion: Flow Matching的推理时扩展可有效应用于科学领域，同时保持高效采样特性。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [507] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布，通过beta-ELBO和蒙特卡洛采样训练，使预测混合的离散度随输入自适应调整，并设计评估协议以分离插值与外推，强调OOD检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有概率回归模型在分布偏移下往往过于自信，缺乏对OOD（分布外）样本的有效识别与不确定性校准。

Method: 提出FDN模型，采用输入条件化的权重分布生成预测混合；使用beta-ELBO目标函数结合蒙特卡洛采样进行训练；设计新的评估协议，明确区分插值与外推场景，并检验模型在分布偏移下的表现。

Result: 在标准回归任务上，FDN在相同参数和更新预算下优于贝叶斯、集成、dropout和超网络等强基线方法，在准确性、校准性和分布偏移感知方面表现更优。

Conclusion: FDN框架结合所提出的评估协议，能够实现对OOD样本敏感且校准良好的神经回归，具有实用性与模块化优势。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [508] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为GUM的无偏低秩优化方法，结合GaLore机制与Muon算法，在保证收敛性的同时实现内存高效的大模型训练，并在LLM微调和预训练中表现优于全参数训练。


<details>
  <summary>Details</summary>
Motivation: 现有低秩投影优化方法（如GaLore）因引入偏差而缺乏收敛保证，导致性能落后于全参数训练，亟需一种既能保持内存效率又能消除偏差的方法。

Method: 基于层间采样去偏技术，构建一种新的无偏低秩优化方法GUM，结合GaLore的低秩投影机制与Muon优化算法，并从理论上证明其收敛性。

Result: 理论证明GUM继承了Muon算法的收敛保证，实验显示其在LLM微调和预训练中显著优于GaLore，甚至超过全参数训练，且模型内部知识分布更均匀，参数利用更高效。

Conclusion: GUM是一种兼具内存效率、收敛保证和优越性能的低秩优化方法，为大语言模型的高效训练提供了新方向。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [509] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM 是一种基于 ORB-SLAM3 的视觉 SLAM 系统，结合轻量级语义关键点过滤和自适应鲁棒损失，能够在动态环境中有效处理已知和未知移动物体，显著提升轨迹精度和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有动态环境中的视觉SLAM方法依赖语义过滤或固定鲁棒核函数，难以应对未知移动物体，导致定位精度下降。

Method: 提出VAR-SLAM，在ORB-SLAM3基础上引入轻量级语义关键点滤波处理已知移动物体，并采用Barron自适应鲁棒损失函数在线估计核函数形状参数，以适应未知动态物体。

Result: 在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上验证，相比NGD-SLAM等先进方法，ATE RMSE最高降低25%，平均运行速度达27 FPS。

Conclusion: VAR-SLAM通过自适应机制有效提升了动态环境下的SLAM精度与鲁棒性，兼顾实时性能，适用于包含已知和未知移动物体的复杂场景。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [510] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种专为废旧电脑拆解设计的新型三自由度夹爪DeGrip，具备紧凑结构和灵活配置能力，并在Isaac Sim环境中验证了其在狭小空间和任意构型下执行拆解任务的有效性。


<details>
  <summary>Details</summary>
Motivation: 废旧电子产品智能拆解是机器人领域长期存在的挑战，现有机器学习方法因缺乏专用硬件而难以在实际场景中应用。

Method: 设计并实现了一种名为DeGrip的定制化三自由度夹爪，采用绳索驱动传动机制以减小体积，并解耦腕部与夹爪关节控制；同时在Isaac Sim中构建了废旧电脑拆解环境进行评估。

Result: 实验结果表明DeGrip能够在狭小空间内灵活操作，成功完成多种任意构型下的拆解任务。

Conclusion: DeGrip夹爪有效提升了机器人对废旧电子产品复杂拆解任务的适应能力，为现实场景中的智能拆解提供了可行的硬件解决方案。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [511] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: 本文提出了Cosmos-Surg-dVRK，一种针对手术机器人的世界基础模型微调版本，结合视频分类器实现对手术策略的全自动在线评估与基准测试，显著降低在真实机器人平台上评估的成本与复杂性。


<details>
  <summary>Details</summary>
Motivation: 在真实手术机器人平台（如dVRK）上直接评估自主手术策略成本高、耗时长、可重复性差，亟需高效、可靠的仿真评估方法。

Method: 基于世界基础模型Cosmos进行手术场景微调，构建Cosmos-Surg-dVRK，并结合训练好的视频分类器V-JEPA 2，建立全自动在线评估流程，用于模拟和评估手术策略。

Result: 在缝合垫任务中，Cosmos-Surg-dVRK的在线仿真结果与真实dVRK Si平台的表现具有强相关性；视频分类器与人工标注一致性良好；在离体猪胆囊切除任务中也展现出与真实评估的良好对齐。

Conclusion: Cosmos-Surg-dVRK为手术策略的自动化评估提供了一个高保真、可扩展的仿真平台，有望广泛应用于复杂手术任务的开发与验证。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [512] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: 本文提出了NEBULA，一个用于单臂操作的统一生态系统，通过细粒度的能力测试和系统性的压力测试，实现对视觉-语言-动作代理的诊断性和可复现评估。


<details>
  <summary>Details</summary>
Motivation: 现有对视觉-语言-动作代理的评估依赖于粗糙的最终任务成功率指标，无法精确诊断技能或衡量对现实世界扰动的鲁棒性，且数据分散阻碍了可复现研究和通用模型的发展。

Method: 设计了一个双轴评估协议，包括细粒度能力测试和系统性压力测试，并提供标准化API和大规模聚合数据集以支持跨数据集训练与公平比较。

Result: 实验表明，当前高性能的视觉-语言-动作代理在空间推理和动态适应等关键能力上表现不佳，而这些缺陷被传统指标所掩盖。

Conclusion: NEBULA为构建鲁棒、通用的具身智能体提供了实用基础，推动了更精细和可复现的评估标准。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [513] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 本文提出了一种无需训练的运行时策略引导方法，通过在执行前对多个候选动作序列进行仿真并利用视觉-语言模型选择与文本计划最一致的结果，提升了推理型视觉-语言-动作（VLA）模型在分布外场景下的推理与动作一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有VLA模型能生成正确的文本计划，但在分布外场景中生成的动作仍可能偏离预期结果，缺乏具身的思维链忠实性。

Method: 基于给定的中间文本计划，从同一VLA模型采样多个候选动作序列，通过模拟预测其结果，并使用预训练的视觉-语言模型选择与原始文本计划最匹配的序列。

Result: 该方法显著提升了VLA在语义和视觉分布外扰动下的鲁棒性，在行为组合任务上性能最高提升15%，且无需重新训练。

Conclusion: 通过运行时的推理-动作对齐机制，可将VLA模型固有的动作多样性由误差源转化为优势，增强泛化能力与新行为组合能力。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [514] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: 提出了一种名为SPOT的统一规划框架，通过将感知目标融入运动优化，实现无人机在动态障碍环境中的实时、观测感知轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 传统方法将运动规划与感知分离，导致对动态障碍物的响应不及时且效果差，尤其是在视场有限和存在盲区的情况下。

Method: 引入基于高斯过程的障碍信念图，结合碰撞感知推理机制生成时变的观测紧迫性图，并将其整合到当前视野中，构建可微目标以实现毫秒级实时规划。

Result: 仿真和真实实验表明，该方法比基线提前2.8秒检测到潜在动态障碍物，动态障碍可见性提升超过500%，并能在复杂遮挡环境中安全导航。

Conclusion: SPOT通过感知增强的统一规划框架，显著提升了单目深度相机无人机在动态、杂乱和遮挡环境下的避障能力与感知效率。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [515] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 本文提出了一种将连接件作为装配表示中一类基本元素的新方法，通过从装配手册中自动提取结构化连接信息，实现了对装配任务的层次图建模，并在多种复杂场景中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大多数机器人装配方法将连接视为次要问题，而实际上连接是决定装配成败的关键环节。因此，需要一种能够显式建模连接关系的装配表示方法。

Method: 提出Manual2Skill++框架，利用大规模视觉-语言模型解析装配手册中的符号图和注释，构建以部件和子装配为节点、以连接关系为边的层次图，将连接类型、规格、数量和位置显式编码。

Result: 构建了一个包含20多个装配任务的数据集，在模拟环境中评估了四个复杂的装配场景，涵盖了家具、玩具和制造部件，验证了所提表示方法的有效性和实际应用潜力。

Conclusion: 将连接作为一级原语可显著提升装配任务的理解与执行能力，Manual2Skill++为从人类指令中提取丰富连接知识提供了有效途径。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [516] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种集成感知、运动与通信（IPMC）的边缘机器人系统新范式，并结合学习优化（LTO）方法实现高效、实时的通信策略自适应，显著降低通信开销和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了机器人功能与通信条件之间的相互依赖性，导致通信开销过大。

Method: 提出集成感知、运动与通信（IPMC）框架，机器人根据感知与运动状态动态调整压缩比、传输频率和发射功率；采用学习优化（LTO）范式设计模仿学习神经网络以降低计算复杂度。

Result: 实验表明，IPMC能有效减少传感器数据上传，LTO相比现有优化求解器计算复杂度降低10倍以上，并具备实时执行能力。

Conclusion: IPMC通过融合多模态信息实现了通信效率的显著提升，结合LTO实现了低开销、实时的决策，为边缘机器人系统提供了高效解决方案。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [517] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 本文介绍了一个包含1893个家庭服务机器人用户问题的数据集，涵盖12个类别和70个子类别，通过视频和文本刺激收集自100名参与者，揭示了用户对机器人问答能力的需求重点。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在人机交互中的广泛应用，机器人回答用户问题的能力愈发重要，但现有研究多集中于‘为什么’类问题，缺乏对用户实际提问多样性的系统分析。

Method: 通过15个视频和7个文本情境刺激，招募Prolific平台上的100名参与者，收集其在不同场景下想向机器人提出的问题，并对问题进行分类与重要性评估。

Result: 数据集中最常见问题类别为任务执行细节（22.5%）、机器人能力（12.7%）和性能评估（11.3%）；尽管关于困难情境处理的问题较少，但用户认为其最重要；新手更关注基本事实类问题。

Conclusion: 该数据集为构建能有效回应用户问题的机器人系统提供了基础，可用于指导信息记录、问答模块评测及符合用户期望的解释策略设计。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [518] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: 本文提出了ORAD-3D，据作者所知是目前最大的专为越野自动驾驶设计的数据集，覆盖多种地形和环境条件，并建立了涵盖五个基本任务的综合基准测试，以推动越野环境下的感知与规划研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模、高质量的数据集和基准，越野自动驾驶研究受到限制，因此需要构建一个专门针对该领域的数据集以促进技术发展。

Method: 收集覆盖多种地形（如林地、农田、草地等）和不同天气、光照条件下的数据，构建ORAD-3D数据集，并基于此建立包括2D自由空间检测、3D占据预测、粗略GPS引导路径规划、视觉语言模型驱动驾驶和世界模型在内的五项基准任务。

Result: ORAD-3D成为目前最大的越野自动驾驶数据集，提供了丰富的环境多样性，并成功构建了五个关键任务的基准测试，为越野场景下的感知与规划提供了统一且强大的资源。

Conclusion: ORAD-3D数据集及其基准测试套件为越野自动驾驶研究提供了重要基础设施，有望显著推动该领域的发展。

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [519] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新型SPD夹爪，具有线性平行抓取功能和自适应能力，可有效抓取不同形状和尺寸的物体，无需调整机械臂高度，适用于实体智能技术中的机器人抓取任务。


<details>
  <summary>Details</summary>
Motivation: 传统平行夹爪指尖运动轨迹为弧形，需调整机械臂高度以避免与桌面碰撞，限制了抓取效率和灵活性。

Method: 设计了一种具有手掌和两个对称手指的SPD夹爪，指尖沿直线运动，可通过独立驱动或单电机驱动实现平行抓取，并基于设计理论制作原型进行实验验证。

Result: 实验结果表明，SPD夹爪成功实现了线性平行抓取功能，并展现出良好的自适应能力，能够稳定抓取多种尺寸和形状的物体。

Conclusion: SPD夹爪有效解决了传统夹爪的轨迹干扰问题，提升了抓取灵活性和效率，为机器人在实体智能场景下的操作提供了可靠工具，并有助于深度学习训练数据的采集。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [520] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: 提出DIV-Nav，一种实时导航系统，通过分解复杂空间语言指令、计算语义置信图交集并利用LVLM验证，实现基于语义地图的复杂空间关系物体搜索。


<details>
  <summary>Details</summary>
Motivation: 现有零样本物体导航方法主要针对简单物体名称查询，难以处理包含空间关系的复杂自由文本指令。

Method: 将复杂自然语言指令分解为简单的对象级查询；在semantic map上计算各对象语义置信图的交集以定位共存区域；使用大型视觉语言模型（LVLM）验证发现的对象是否满足原始空间约束；并调整前沿探索策略以适应空间搜索需求。

Result: 在MultiON基准和真实世界Boston Dynamics Spot机器人上进行了广泛实验，验证了系统的有效性与实时性。

Conclusion: DIV-Nav能够高效处理带有空间关系的复杂自由文本导航任务，同时保持语义地图的鲁棒性，提升了开放词汇导航的实际应用能力。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [521] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SP-Diff的并联夹爪系统，采用创新的差动连杆机构和行星齿轮传动，实现线性平行抓取，具备结构紧凑、自适应强、减少Z轴重校准等优势，适用于智能制造、协作机器人及数字孪生应用。


<details>
  <summary>Details</summary>
Motivation: 传统末端执行器在工业自动化中适应性有限，难以应对多样化工件和复杂操作场景，亟需具有更高灵活性和智能化的夹持解决方案。

Method: 设计了一种模块化对称双指结构的SP-Diff夹爪，结合差动连杆机构与行星齿轮传动系统，实现同步直线运动与独立手指姿态调节，并通过运动学优化的平行四边形连杆结构提升抓取适应性。

Result: 该系统相比弧形轨迹夹爪减少了30%的Z轴重校准需求，能够稳定抓取刚性工业零件和易变形物体（如柑橘），并支持未来集成力/视觉传感器用于数字孪生中的轨迹规划与形变监测。

Conclusion: SP-Diff夹爪通过其自适应架构提升了机器人末端执行器的智能化水平，在柔性制造、物流自动化和协作机器人等领域具有广泛应用前景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [522] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: 提出Mixture of Skills VLA (MoS-VLA)，通过线性组合基础技能实现无需梯度更新的快速适应新任务，仅需一次专家演示即可在新环境中有效运行。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在新环境、新任务或不同机器人形态下表现不佳，缺乏即插即用的鲁棒性和适应性。

Method: 将策略表示为有限组学习到的基础函数的线性组合，在Open X-Embodiment数据集上联合预训练，形成结构化技能空间；测试时通过轻量级凸优化（最小化L1动作误差）推断技能权重，无需梯度更新。

Result: 在五个未见数据集中均实现了更低的动作预测误差，并在仿真和真实机器人任务中成功运行，而预训练VLA模型完全失败。

Conclusion: MoS-VLA通过结构化技能表示和梯度-free自适应机制，显著提升了VLA模型在跨域、跨形态任务中的泛化能力和部署效率。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [523] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究通过跨国问卷调查22名一线应急人员，探讨其对机器人系统中语义信息与态势感知（SA）应用的态度。结果显示，多数应急人员对机器人持积极态度，认为语义信息在构建态势感知（平均评分3.6/5）和预测突发情况（平均3.9/5）方面具有实用性，并愿意接受准确率约75%以下但具信息性的AI支持工具。研究揭示了实验室机器人能力与实地部署之间的差距，强调加强应急人员与机器人研究人员之间的协作，以开发更符合用户需求的应急响应系统。


<details>
  <summary>Details</summary>
Motivation: 了解一线应急人员对机器人系统中语义信息与态势感知的实际需求与态度，填补实验室技术发展与现场实际应用之间的认知差距。

Method: 通过对来自八个国家的22名一线应急人员进行结构化问卷调查，收集其人口统计信息、对机器人的总体态度以及对语义增强型态势感知的使用经验，并进行定量分析。

Result: 大多数应急人员对机器人持积极态度；语义信息在构建态势感知中的有用性评分为3.6/5，在预测突发事件中的价值为3.9/5；参与者平均需要74.6%的准确率才会信任语义输出，67.8%的准确率即认为有用；最被重视的语义信息包括对象身份、空间关系和风险背景，并与受访者角色、经验和教育水平相关。

Conclusion: 该研究提供了关于应急人员对语义增强型机器人系统的实际看法的宝贵见解，揭示了当前技术与实际需求之间的差距，强调未来应加强跨领域合作，开发更贴近用户需求、更具情境感知能力的应急机器人系统。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [524] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 该论文系统研究了激励设计对纳米和微小卫星惯性特性辨识的影响，通过模拟非线性姿态动力学并比较不同扭矩激励和估计算法的性能，提供了在轨自适应惯性辨识的实用指导。


<details>
  <summary>Details</summary>
Motivation: 准确估计卫星的惯性参数对于姿态控制至关重要，特别是在质量分布变化或缺乏精确先验模型的情况下，需要明确激励信号设计如何影响辨识性能。

Method: 通过包含反作用轮耦合、执行器限制和外部干扰的非线性姿态动力学仿真，采用八种频谱丰富的力矩激励信号，并比较批处理最小二乘法和扩展卡尔曼滤波器在三种卫星构型和时变惯性场景下的表现。

Result: 激励信号的频率成分与估计算法的假设共同决定了估计的精度和鲁棒性；特定条件下某一类方法表现更优，例如某些激励频谱更适合最小二乘法，而EKF在动态变化环境中更具适应性。

Conclusion: 合理的激励设计结合合适的估计算法能显著提升惯性参数辨识效果，研究结果为在轨实时辨识提供了实践指南，并开源代码以支持可重复研究。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [525] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 本文提出了一种自适应不变扩展卡尔曼滤波器（Adaptive Invariant EKF），用于提升足式机器人的本体感知状态估计性能，尤其在动态运动和不同接触条件下表现更优。


<details>
  <summary>Details</summary>
Motivation: 状态估计对足式机器人至关重要，传统方法在处理小幅度打滑和接触不确定性时存在不足，容易导致滤波发散或依赖额外传感器。

Method: 提出一种自适应不变扩展卡尔曼滤波器，通过在线协方差估计自适应调整接触足模型的噪声水平，并结合接触检测算法替代接触传感器，减少硬件依赖。

Result: 在四足机器人LeoQuad上的实验证明，该方法在动态运动场景中显著提升了状态估计精度，能有效处理传统滑移拒绝机制无法应对的小幅度打滑问题。

Conclusion: 所提出的自适应方法提高了状态估计的鲁棒性和准确性，适用于复杂多变的地面接触条件，具有良好的实际应用前景。

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [526] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为T3 Planner的LLM驱动的机器人运动规划框架，通过信号时序逻辑（STL）验证器实现自我纠正，以生成满足复杂时空与逻辑约束的可行运动计划。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域专业知识且难以处理时空耦合问题，而大语言模型虽擅长高层语义推理但易产生不切实际的动作规划，因此需要一种能结合语义理解与形式化验证的鲁棒规划框架。

Method: T3 Planner采用三个级联模块分解时空任务约束，每个模块利用大语言模型生成候选轨迹序列，并通过STL验证器检查其可行性，迭代修正直至找到满足所有约束的解。

Result: 在多种场景下的实验表明，T3 Planner显著优于基线方法，且其推理过程可蒸馏至轻量级Qwen3-4B模型，支持高效部署。

Conclusion: T3 Planner有效结合了大语言模型的语义推理能力与形式化方法的精确验证优势，为自然语言到机器人运动规划的可靠转换提供了新方案。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [527] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了一种基于意图驱动的规划管道，结合感知、大语言模型（LLM）集成、验证机制和一致性过滤器，用于多机器人协作下的复杂操作任务规划，并在电动汽车电池拆解任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化场景中，多个具有不同末端执行器和能力的机器人需根据视觉信息和人类指令协同完成复杂操作任务，传统方法难以灵活应对多样化的对象配置和任务序列需求。

Method: 提出一个包含四部分的规划管道：(i) 将感知结果编码为文本场景描述；(ii) 使用LLM集成生成候选动作序列；(iii) LLM-based验证器确保格式与顺序约束；(iv) 确定性一致性过滤器剔除幻觉对象。

Result: 在200个真实场景和600条操作指令下评估，采用全序列正确率和下一步正确率指标，结果显示该方法能可靠地将操作意图转化为安全可执行的多机器人计划，并通过人类实验验证了低用户负担和高效人机交互性能。

Conclusion: 所提出的集成加验证的LLM方法在复杂多机器人操作任务中实现了高可靠性与低人为干预的平衡，适用于回收等实际应用场景。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [528] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 本文探讨了如何通过脑启发的空间认知导航来提升无人系统的定位、导航与授时（PNT）能力，提出从“工具导向”向“认知驱动”转变的新视角和路线图。


<details>
  <summary>Details</summary>
Motivation: 当前复杂环境对PNT系统提出了更高的韧性、能效和认知能力要求，传统PNT已难以满足未来需求，因此需要借鉴生物大脑的导航机制发展更智能的PNT系统。

Method: 通过多层次分析传统PNT、生物脑PNT与脑启发PNT的差异，构建了一个四层融合框架（观测-能力-决策-硬件），整合数值精度与脑启发智能。

Result: 提出了一个统一的四层融合框架，并为脑启发PNT的发展提供了前瞻性建议，推动PNT系统向认知驱动转型。

Conclusion: 脑启发PNT是实现通用PNT的重要路径，通过融合生物智能与机器精度，可显著提升无人系统在复杂环境中的自主导航能力。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [529] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 提出了一种新的轨迹采样方法C-Free-Uniform，用于在自由构型空间中均匀采样，并将其集成到MPPI控制器中（CFU-MPPI），在复杂环境中以更小的采样预算提高了导航任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹采样方法中的控制输入分布通常与环境无关，难以高效探索自由构型空间，因此需要一种能结合局部环境信息、提升采样效率的机制。

Method: 提出了C-Free-Uniform采样方法，使控制输入分布p(u|x)显式依赖于当前局部地图，从而在自由配置空间中实现均匀采样，并将其集成到Model Predictive Path Integral (MPPI) 控制器中形成CFU-MPPI。

Result: 实验表明，CFU-MPPI在杂乱多边形环境中的导航任务上，相比现有方法在成功率上有显著提升，同时所需采样数量更少。

Conclusion: C-Free-Uniform通过结合环境信息实现了更高效的轨迹采样，CFU-MPPI控制器在低采样预算下表现出优越性能，适用于复杂环境下的实时导航。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [530] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: 本文介绍了RAPID Hand，一种低成本、全驱动的20自由度灵巧手，采用新颖的人体工程学驱动和传动设计，适用于灵巧遥操作和大规模真实机器人数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决目前缺乏价格合理且全驱动五指灵巧手的问题，以支持基于‘从演示中学习’范式的灵巧遥操作和大规模真实机器人数据采集。

Method: 提出RAPID Hand原型，采用通用指节传动方案（非拇指手指）和全向拇指驱动机制，结合优化的电机布局与结构设计；使用3D打印部件和定制齿轮以降低成本并便于维护。

Result: 通过定量指标和定性测试评估RAPID Hand在多指抓取、汤勺操作和类人弹钢琴三项挑战性任务中的表现，验证了其全驱动20自由度设计的有效性和高灵巧性。

Conclusion: RAPID Hand在实现低成本与高灵巧性之间取得了良好平衡，展现出在灵巧遥操作和机器人学习应用中的巨大潜力。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [531] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: 本文提出了一种名为DINO-CVA的多模态目标条件行为克隆框架，用于实现自主导管导航，结合视觉和操纵杆运动学信息，在合成血管模型中实现了高精度动作预测，为减少操作依赖性和提高导管治疗可靠性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导管系统大多依赖医生持续操作，缺乏智能自主性，导致操作疲劳、辐射暴露增加和手术结果变异性。因此需要发展具有自主导航能力的系统以提升介入手术的安全性和一致性。

Method: 提出DINO-CVA框架，融合视觉观测与操纵杆运动学数据到联合嵌入空间，并通过目标条件化指导导航；采用自回归方式从专家示范中预测动作，在包含合成血管模型的机器人实验平台上收集多模态数据并验证性能。

Result: DINO-CVA在动作预测上达到高精度，表现与仅使用运动学的基线模型相当，同时能将预测结果与解剖环境关联，具备环境感知能力。

Conclusion: 多模态、目标条件化的架构在导管导航中是可行的，是迈向减少操作员依赖和提升导管治疗可靠性的关键一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [532] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 提出了一种基于交叉熵方法与奖励模型（CEM-RM）的软体机械手设计优化框架，利用遥操作数据高效搜索高性能设计，显著提升抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 软体机械手在安全交互方面具有潜力，但兼顾柔顺性和多功能性设计困难，且硬件与控制协同设计的搜索空间高维、仿真评估成本高。

Method: 提出CEM-RM框架，结合预收集的遥操作数据和奖励模型，在模拟中并行优化肌腱驱动的软体手设计，并实现3D打印与真实部署。

Result: 优化后的设计在模拟和实物实验中均显著优于基线设计，抓取成功率在多种复杂物体上均有提升，设计评估次数减少一半以上。

Conclusion: CEM-RM框架能高效优化软体机械手设计，通过数据驱动的方式实现高性能、可制造的手部结构，适用于多样化任务。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [533] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 本文综述了提升视觉-语言-动作（VLA）模型效率的系统性方法，重点在于降低延迟、内存占用及训练/推理成本，涵盖模型架构、感知特征、动作生成和训练/推理策略四个方面。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身控制中具有潜力，但其高计算和内存需求与边缘平台的实时性要求相冲突，亟需提高效率。

Method: 对现有提升VLA效率的方法进行系统性分类和总结，分为模型架构、感知特征、动作生成、训练/推理策略四个维度。

Result: 梳理了各类别中的代表性技术，明确了当前的研究进展和优化路径。

Conclusion: 文章指出了未来趋势和开放挑战，为高效具身智能的发展提供了方向。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [534] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 提出一种基于机器学习的去中心化动力学规划方法，通过模仿学习训练无人机策略，在无通信和部分观测条件下实现电缆悬挂负载的协同运输。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖集中式控制或可靠的多机通信，难以在通信受限环境下应用。

Method: 利用模仿学习，让每个无人机（学生策略）模仿具有全局观测的集中式规划器（教师策略），并通过物理信息神经网络生成符合运动学约束的平滑轨迹。

Result: 学生策略可在普通笔记本上两小时内完成训练，在仿真和真实环境中均能有效跟踪敏捷参考轨迹，性能接近集中式方法。

Conclusion: 该方法实现了无需通信和全局观测的去中心化协同运载，具有高样本效率和实际部署潜力。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [535] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: 本文提出了DiffVLA++，一种通过度量引导对齐认知推理与端到端规划的自动驾驶框架，在ICCV 2025挑战赛中取得了49.12的EPDMS成绩。


<details>
  <summary>Details</summary>
Motivation: 传统端到端驾驶模型缺乏环境理解能力，难以泛化到长尾场景；而视觉-语言-动作（VLA）模型虽具备世界知识但物理可行性差，因此需要结合二者优势。

Method: 构建一个能生成语义接地轨迹的VLA模块；设计具有密集轨迹词汇表的端到端（E2E）模块以保证物理可行性；引入度量引导的轨迹评分器来对齐和融合VLA与E2E模块的输出。

Result: 在ICCV 2025自动驾驶挑战赛排行榜上，DiffVLA++实现了49.12的EPDMS得分，表现出优异的综合性能。

Conclusion: DiffVLA++通过显式对齐VLA的认知推理与E2E的物理可行性，有效结合了两类方法的优势，提升了自动驾驶系统在复杂场景下的表现。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [536] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一种结合视觉语言模型（VLM）的通用变阻抗控制器，通过检索增强生成（RAG）和上下文学习（ICL）实现对复杂接触任务的安全、自适应控制，在仿真和真实机器人任务中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 传统变阻抗控制器在机器人与环境物理交互时表现良好，但在未见过的、复杂的非结构化场景中缺乏泛化能力，难以保证安全交互。

Method: OmniVIC利用视觉语言模型理解任务上下文，结合检索增强生成（RAG）从记忆库中提取过往经验，并通过上下文学习（ICL）生成适应当前任务的阻抗参数，同时结合实时力/扭矩反馈调节参数以确保安全性。

Result: 在模拟和真实世界的接触丰富任务中，OmniVIC相比基线方法成功率达到61.4%，远高于基线的27%，并减少了力违规现象。

Conclusion: OmniVIC有效桥接了高层语义推理与低层柔顺控制，推动了更安全、可泛化的机器人操作发展。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [537] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 本文提出了SimpleVSF，一种利用视觉语言模型（VLM）和先进轨迹融合技术来增强端到端自动驾驶规划的新框架，在ICCV 2025 NAVSIM v2挑战赛中表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景中决策能力不足，难以兼顾安全性、舒适性和效率。

Method: 结合传统评分器与基于VLM的增强评分器，采用量化聚合的鲁棒权重融合器和基于VLM的定性上下文感知融合器进行轨迹融合。

Result: 在ICCV 2025 NAVSIM v2端到端驾驶挑战赛中取得领先性能，实现了安全性、舒适性和效率之间的优越平衡。

Conclusion: SimpleVSF通过引入VLM的认知能力与双层融合机制，显著提升了端到端自动驾驶系统的决策质量。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [538] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 本研究提出了一种利用事件相机同时实现可见光通信（VLC）和可见光定位（VLP）的新型车辆自定位系统，可在无GPS环境下（如隧道）实现高精度定位与通信。


<details>
  <summary>Details</summary>
Motivation: 在GPS信号不可用的环境（如隧道）中，需要一种能够同时实现高动态范围、高速移动适应性和精确位置估计的自定位方案。传统图像传感器在此类场景中存在局限，而事件相机具备高时间分辨率和高动态范围，适合应对快速运动和极端光照变化。

Method: 通过在发射端使用基于Walsh-Hadamard码的唯一导频序列区分多个LED，利用事件相机捕获信号，并通过相位相关法（POC）进行多LED对的距离估计，同时实现MISO模式下的VLC通信。系统集成于行驶中的车辆上，以验证实际性能。

Result: 在车速30 km/h的实地实验中，系统在100米范围内实现了距离估计均方根误差小于0.75米，误码率低于0.01，表现出良好的鲁棒性。

Conclusion: 这是首个使用单个事件相机实现VLC与VLP一体化的车载系统，验证了事件相机在智能交通系统中用于高精度定位与高速通信的可行性。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [539] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 本文提出了一种名为“Pole-Image”的新型规范表示方法，利用杆状地标作为高精度锚点，生成周围3D结构的2D极坐标图像，结合对比学习实现鲁棒的自定位与高灵敏度的地图变化检测。


<details>
  <summary>Details</summary>
Motivation: 传统基于地标的自定位方法在可检测性与独特性之间存在权衡，难以同时满足稳定检测与高区分度的需求。本文旨在通过结合易检测的高精度锚点（如杆）与周围环境结构，解决长期自主导航中定位与地图维护的挑战。

Method: 提出Pole-Image表示法，将LiDAR点云中杆状地标及其周围环境以杆为中心转换为2D极坐标图像，利用杆的稳定性建立相对几何编码，并采用对比学习训练 viewpoint-invariant 且高区分性的描述子。

Result: 该方法实现了抗感知混淆的鲁棒自定位，并支持高灵敏度的环境变化检测，能够在大规模环境中自动收集多视角观测数据用于模型训练。

Conclusion: Pole-Image通过融合高可检测性锚点与局部结构信息，有效提升了移动机器人在长期自主运行中的定位可靠性与地图维护能力。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [540] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出一种模块化控制框架，结合基于模型的动态控制、在线模型自适应和自适应落足点规划，以应对机器人和地形属性不确定的情况，成功应用于火山实地测试。


<details>
  <summary>Details</summary>
Motivation: 轮式探测车在复杂地形中移动受限，而四足机器人虽能应对不平坦、多障碍和可变形地形，但在未知环境中部署面临控制难题，尤其是在地形和机器人参数不确定的情况下。

Method: 结合基于模型的动态控制、在线模型自适应和自适应落足点规划，集成状态估计（支持有无接触感知），并实现运行时重新配置，框架基于ROS 2并开源。

Result: 在两个四足平台、多种硬件架构上验证了性能，并在火山实地测试中实现了超过700米的行走距离。

Conclusion: 该模块化控制框架有效提升了四足机器人在未知和极端环境中的自主导航能力，具备良好的可移植性和实际应用潜力。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [541] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 本文提出了一种基于Nets-within-Nets范式和Transformer模型的多机器人系统异常执行检测方法，能够有效识别线性时序逻辑任务中的各类异常行为。


<details>
  <summary>Details</summary>
Motivation: 在异构多机器人系统中，高可靠性地执行高层任务需要能够检测出计划执行中的异常行为，如错误的任务序列、空间约束违反、时间不一致或语义偏离。

Method: 采用Nets-within-Nets（NWN）范式构建结构化数据生成框架，将机器人动作与LTL导出的全局任务规范进行协调，并设计基于Transformer的异常检测流程，对机器人轨迹进行正常或异常分类。

Result: 实验表明该方法在识别执行低效方面准确率达91.3%，核心任务违规检测达88.3%，约束相关的适应性异常检测达66.8%；消融实验验证了所提表示和架构的优越性。

Conclusion: 所提出的框架能有效检测多机器人系统中基于LTL的计划执行异常，且Transformer模型结合NWN结构化建模优于简单表示方法。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [542] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 本文提出了一种针对漂浮基系统（如人形和四足机器人）的动力学建模方法FeLaN，通过引入满足物理约束的惯性矩阵参数化方式，结合深度学习与拉格朗日力学，在保证物理一致性的前提下实现了高精度的逆动力学预测。


<details>
  <summary>Details</summary>
Motivation: 现有灰箱模型忽视了漂浮基系统的特定物理约束（如惯性矩阵的正定性、稀疏性和输入无关性），导致模型在分布外泛化能力和物理可解释性方面存在不足。

Method: 受Deep Lagrangian Networks (DeLaN)启发，设计了一种新的惯性矩阵参数化方法，满足分支诱导稀疏性、输入独立性及复合旋转惯性特征值的三角不等式等物理约束，并训练神经网络以最小化拉格朗日力学下的逆动力学误差。

Result: 在多个四足和人形机器人的仿真与真实数据集上，FeLaN在逆动力学预测任务中表现出极具竞争力的性能，同时提升了模型的物理一致性与可解释性。作者还公开了相关数据集。

Conclusion: 所提出的FeLaN框架能有效融合物理约束与深度学习，为漂浮基系统提供了更可靠、更具物理意义的动力学模型，有助于提升复杂机器人系统的建模精度与泛化能力。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [543] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出一种新框架，通过在交互时集成数据来改进视频规划，实现在线参数更新和失败计划过滤，从而提升在部分观测环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频规划框架难以在交互时适应失败，因其无法在部分观测环境中推理不确定性。

Method: 引入一个新框架，将交互时的数据融入规划过程，通过在线更新模型参数和过滤已失败的计划来实现隐式状态估计。

Result: 在新的模拟操作基准上进行广泛实验，验证了该框架能有效提升重规划性能。

Conclusion: 所提方法无需显式建模未知状态变量即可实现动态适应，推动了基于视频的决策领域的发展。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [544] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 本文提出了一种名为DDBot的可微分挖掘机器人框架，用于高精度操控未知物理特性的颗粒材料（如沙土），结合基于GPU加速的可微分物理仿真器和梯度优化方法，实现了高效系统辨识与零样本真实世界部署。


<details>
  <summary>Details</summary>
Motivation: 颗粒材料操控因接触动力学复杂、物性未知和系统状态多变而极具挑战，现有方法难以兼顾效率与精度，亟需一种能适应未知材料并实现高精度控制的新方法。

Method: 提出DDBot框架，集成可微分物理仿真器（支持GPU并行计算与自动微分），通过可微分技能到动作映射、面向任务的示范方法、梯度裁剪与线搜索梯度下降，实现系统辨识与挖掘技能优化。

Result: 实验表明DDBot可在5-20分钟内高效收敛，完成对未知颗粒材料的动力学识别与技能优化，并在零样本真实场景中实现高精度挖掘，性能优于现有最先进基线方法。

Conclusion: DDBot通过结合可微分物理仿真与优化方法，显著提升了颗粒材料操控的效率与精度，具备良好的鲁棒性和实际应用潜力。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [545] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 提出了一种统一的交互式力-阻抗控制（IFIC）框架，通过适应交互功率流，在接触密集环境中确保机器人与人类协作时的安全性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 在接触密集环境中，传统方法难以准确估计人类意图并保证系统安全性，尤其是在机器人采用混合或统一力-阻抗控制与主动人类交互时可能失去被动性。

Method: 基于端口-哈密顿框架设计了包含交互和任务控制端口的统一IFIC架构，通过分析交互功率流来调节控制行为，确保系统被动性。

Result: 该框架能够在物理交互中保持系统被动性，实现安全、 effortless 的人机协作，适用于接触密集环境。

Conclusion: 所提出的IFIC框架有效解决了接触密集环境下人机协作中的安全与角色切换问题，提升了交互的灵活性与可靠性。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [546] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 本文研究了将视觉-语言-动作（VLA）模型应用于软体连续体机械臂以实现安全的人机交互，提出了一种结构化的微调和部署流程，并验证了在代表性操作任务中两种先进VLA模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要局限于刚性机械臂，难以满足在非结构化环境中安全与灵活交互的需求，因此需要探索其在软体机器人上的适用性。

Method: 采用结构化微调方法，在软体连续体机械臂上部署两种先进的VLA模型（OpenVLA-OFT 和 π₀），并通过实验评估其在典型操作任务中的性能。

Result: 未经微调的VLA模型因具身不匹配而失败，但经过针对性微调后，软体机器人表现可媲美刚性机械臂。

Conclusion: 微调对于弥合具身差异至关重要，将VLA模型与软体机器人结合可实现人类共存环境中的安全、灵活的具身AI。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [547] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 提出了一种结合可信人工智能与节能机械臂的智能垃圾分类方法，使用MobileNetV2增强的CNN实现高精度分类，并通过机器人模拟器优化能耗。


<details>
  <summary>Details</summary>
Motivation: 为解决城市环境中垃圾分类效率低、智能化程度不足的问题，提升垃圾分类系统的可靠性与能效。

Method: 采用基于MobileNetV2的卷积神经网络进行迁移学习以分类六类垃圾，并结合机器人臂模拟器利用欧氏距离计算动作能耗，实现高效分拣路径规划。

Result: 模型训练准确率达99.8%，验证准确率为80.5%，系统具备良好的学习与泛化能力，并实现了节能的虚拟分拣。

Conclusion: 该框架融合可信AI原则（透明性、鲁棒性、公平性、安全性），为城市智能垃圾管理系统提供了可靠且可扩展的解决方案。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [548] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON是一种新的视觉-语言-动作（VLA）模型范式，通过将丰富的3D空间标记注入动作头来增强空间推理能力，利用空间基础模型从RGB图像中提取强几何先验，并可选融合深度或姿态信息以提高保真度，同时保持语言理解能力，在多个仿真和真实世界任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖2D编码器，存在空间推理不足的问题，限制了其在3D环境中的泛化与适应能力；同时现有3D集成方法要么需要专用传感器且跨模态迁移差，要么引入的线索较弱、缺乏几何信息并损害视觉-语言对齐。

Method: 提出FALCON，利用空间基础模型从单张RGB图像生成富含几何信息的3D空间标记，并通过一个独立的“空间增强动作头”使用这些标记，避免干扰视觉-语言主干网络；此外设计了一个具身空间模型，可灵活融合深度或姿态等额外感知输入，无需重新训练或修改架构。

Result: 在三个模拟基准和十一个真实世界任务中，FALCON实现了最先进的性能，持续优于各类基线方法，并在物体杂乱、空间提示条件变化以及物体尺度和高度变化下仍保持鲁棒性。

Conclusion: FALCON有效弥合了VLA模型中的空间推理鸿沟，在不牺牲语言理解的前提下提升了3D空间表征能力，具有良好的模态可转移性和对齐性，为现实世界具身智能体的发展提供了高效且实用的解决方案。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [549] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了一种通过反馈线性化控制非线性系统时在不同输出集之间切换的系统性框架，引入“meld”概念来定义可从大量可能输出中选择的有效且可反馈线性化的输出子集，并证明在适当的驻留时间和兼容性条件下，可在不同meld间切换同时保证系统状态的一致有界性。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统的反馈线性化控制中，灵活切换输出集以适应不同任务或约束的需求，但需确保系统稳定性和输出跟踪性能。

Method: 引入“meld”概念以形式化定义有效的可切换输出子集，并基于 dwell-time 和兼容性条件，通过理论分析证明切换过程中系统状态的一致有界性和误差动态的指数稳定性。

Result: 证明了在满足适当条件时可在不同meld间切换并保持系统状态一致有界；主动输出的误差动态在每个切换区间内保持指数稳定；连续meld中共有的输出能够无缝跟踪过渡。

Conclusion: 所提出的框架为任意可通过反馈线性化的非线性系统（如机器人、飞行器和地面车辆等）提供了安全稳定的输出切换机制，具有广泛适用性，并通过机器人操纵器的数值仿真验证了其有效性。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [550] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: 提出了一种名为HumanMPC的模型预测控制框架，用于无人机在三维人类环境中的安全高效导航，结合理论安全性保障与数据驱动的人体运动预测模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法多集中于简化的二维人群导航，未能充分考虑人体动态的复杂性，难以实现真正安全高效的机器人在人群中的导航。

Method: 提出HumanMPC框架，采用基于可达性的安全性约束方法，仅对初始控制输入施加安全约束，并在整个规划范围内建模其影响；结合数据驱动模型进行真实人体运动预测，实现3D微小型飞行器（MAV）在人群中的导航。

Result: 在仿真和真实环境中验证了HumanMPC的有效性，能够完成目标导航和视觉伺服跟踪等任务，相比基线方法在效率和可靠性上表现更优，且不具过度保守性。

Conclusion: HumanMPC能够在保证安全性的同时实现高效导航，具有良好的通用性，可扩展至其他机器人平台。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [551] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种基于ADMM和DDP的分布式空间-时间轨迹优化框架（D-PDDP），用于解决大规模无人机群的轨迹优化问题，具有快速收敛和分布计算优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法需预先设定终止时间且迭代耗时长，难以应用于大规模无人机群的轨迹优化。

Method: 结合交替方向乘子法（ADMM）与微分动态规划（DDP），采用参数化DDP进行局部路径规划，利用ADMM实现时空参数一致性，构建双层分布式架构，并引入基于谱梯度法的自适应惩罚参数调整机制。

Result: 实现了完全分布式的D-PDDP算法，显著减少迭代次数，提升计算效率，在多个仿真场景中验证了算法的有效性。

Conclusion: 所提D-PDDP框架能高效解决大规模多无人机协同轨迹优化问题，具备良好的收敛性和可扩展性，适用于实际应用。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [552] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 本文提出了一种改进的Mixture-of-Experts模型，用于降低TLIO在自行车定位中的计算和参数开销，同时保持与LLIO相当的精度。


<details>
  <summary>Details</summary>
Motivation: 传统GNSS方法受多径效应影响，现有惯性导航方法依赖精确建模且鲁棒性有限，而TLIO虽精度高但计算成本高，难以在移动设备上部署。

Method: 将TLIO扩展到自行车定位任务，并引入一种改进的Mixture-of-Experts（MoE）模型，以降低训练和推理的计算负担。

Result: 与当前最先进的LLIO框架相比，所提方法在定位精度相当的情况下，参数量减少了64.7%，计算成本降低了81.8%。

Conclusion: 改进的MoE模型有效降低了TLIO的资源消耗，使其更适用于移动设备上的自行车定位应用。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [553] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为RESample的自动OOD数据增强框架，通过探索性采样提升视觉-语言-动作模型在机器人操作中的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习数据集仅包含成功轨迹，缺乏失败或恢复数据，导致VLA模型在分布外（OOD）状态下表现不佳。

Method: 利用离线强化学习获得动作价值网络，通过 rollout 采样潜在OOD状态，并设计自适应的探索性采样机制将这些状态加入训练数据。

Result: 在LIBERO基准和真实机器人任务上实验表明，该方法显著提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample有效增强了VLA模型对分布偏移的鲁棒性，使其能在OOD状态下恢复并持续完成任务。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [554] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: 本文提出了一种名为Botany-Bot的植物表型系统，利用立体相机、转盘、机械臂和3D高斯点阵模型构建植物的详细“标注数字孪生体”，并通过机器人算法操控叶片以获取被遮挡部位的高分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 固定摄像头的商业植物表型系统因叶片遮挡而无法捕捉许多植物细节，因此需要一种能够主动感知并重建植物三维结构的自动化系统。

Method: Botany-Bot结合两个立体相机、数字转盘、光照箱、工业机械臂以及3D分割高斯点阵模型，通过机械臂操作叶片，从多个角度采集高分辨率图像，并生成带注释的植物数字孪生模型。

Result: 实验结果表明，Botany-Bot在叶分割准确率为90.8%，叶片检测准确率为86.2%，叶片抬升/推压操作成功率为77.9%，获取叶片上下表面图像的成功率为77.3%。相关代码、视频和数据集已公开。

Conclusion: Botany-Bot能够有效克服叶片遮挡问题，实现对活体植物的高精度三维建模与细节成像，为植物表型分析提供了新的自动化解决方案。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [555] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic是一种从示例动作中学习人形机器人全身柔顺控制策略的框架，通过强化学习实现对环境扰动的柔顺响应，同时保持平衡和姿态。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法倾向于产生僵硬的控制行为，导致机器人在遭遇意外接触时表现脆弱且不安全，因此需要一种能够实现柔顺控制的方法。

Method: 利用逆运动学求解器生成可行的柔顺动作增强数据集，并基于此训练强化学习策略，通过奖励策略匹配柔顺响应而非 rigidly 跟踪参考动作来实现柔顺性。

Result: SoftMimic能够在模拟和真实世界实验中有效吸收干扰，实现与环境的安全、有效交互，并能从单一动作片段泛化到多种任务。

Conclusion: SoftMimic实现了人形机器人在保持平衡的同时对外力做出柔顺响应的能力，提升了运动的鲁棒性和安全性。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [556] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了RoboBench，一个用于系统评估多模态大语言模型（MLLMs）作为具身智能体“大脑”的基准测试，涵盖五个维度、14种能力及25项任务，并基于真实机器人数据确保任务真实性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注执行成功率或高阶推理的评估维度不全、任务真实性不足，难以全面衡量具身智能体的认知能力。因此需要一个更系统、更真实的评估框架。

Method: 提出RoboBench基准，定义五个评估维度（指令理解、感知推理、通用规划、可操作性预测和失败分析），构建包含6092个问答对的多任务数据集；引入“MLLM-as-world-simulator”评估框架，通过模拟计划的物态变化可行性来评估规划合理性。

Result: 在14个MLLM上实验发现模型在隐式指令理解、时空推理、跨场景规划、细粒度可操作性理解和失败诊断方面存在显著局限；验证了RoboBench在评估高阶认知能力方面的全面性和有效性。

Conclusion: RoboBench为评估具身智能中的高阶认知提供了系统、真实的评测框架，揭示了当前MLLM作为具身大脑的关键缺陷，有助于指导下一代具身MLLM的发展。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [557] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 该论文研究了基于Yelp评论的Hierarchical Sigmoid Attention Network（HSAN）分类器生成的信号与纽约市卫生部门餐厅检查结果之间的相关性，发现在人口普查街区层面两者相关性较弱，且不受C级餐厅数量影响。


<details>
  <summary>Details</summary>
Motivation: 由于食源性疾病常因食用受污染食物引起，而餐馆是疫情暴发的关键场所，公众通过社交媒体产生的内容可能提供及时的公共卫生信号，因此有必要探索非传统数据源在公共卫生监测中的应用。

Method: 使用HSAN分类器分析Yelp评论中的健康相关信号，并在人口普查街区层面与纽约市卫生部门2023年的官方检查结果进行比较，评估其相关性、分布差异和空间模式。

Result: HSAN信号与官方检查评分在街区层面的相关性极低，且在不同C级餐厅密度区域之间无显著差异。

Conclusion: 当前基于Yelp评论的HSAN信号与官方卫生检查结果关联性有限，未来需推进到更精细的地址级别分析以提升公共卫生监测能力。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


### [558] [Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades](https://arxiv.org/abs/2510.16393)
*Franco Maria Nardini,Raffaele Perego,Nicola Tonellotto,Salvatore Trani*

Main category: cs.IR

TL;DR: 本文提出了一种结合词汇和神经相关性信号的ad-hoc段落检索方法，通过学习排序模型融合两类特征，在保持较高效率的同时显著提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 为了提升ad-hoc段落检索的效果，探索如何有效融合词汇匹配信号与深度神经相关性信号。

Method: 利用MS-MARCO数据集构建密集神经表示，并结合253个人工设计的词汇特征，使用基于决策树森林的学习排序（LTR）模型进行信号融合，在两阶段检索架构中由神经检索器初检、LTR模型重排序候选结果。

Result: 实验表明该方法在公开资源上相比当前先进密集检索器可将nDCG@10提升高达11%，平均查询延迟仅增加4.3%。

Conclusion: 深度融合词汇与神经两类互补信号能显著提升检索性能，且对效率影响较小，验证了混合信号融合策略的有效性。

Abstract: We investigate the exploitation of both lexical and neural relevance signals
for ad-hoc passage retrieval. Our exploration involves a large-scale training
dataset in which dense neural representations of MS-MARCO queries and passages
are complemented and integrated with 253 hand-crafted lexical features
extracted from the same corpus. Blending of the relevance signals from the two
different groups of features is learned by a classical Learning-to-Rank (LTR)
model based on a forest of decision trees. To evaluate our solution, we employ
a pipelined architecture where a dense neural retriever serves as the first
stage and performs a nearest-neighbor search over the neural representations of
the documents. Our LTR model acts instead as the second stage that re-ranks the
set of candidates retrieved by the first stage to enhance effectiveness. The
results of reproducible experiments conducted with state-of-the-art dense
retrievers on publicly available resources show that the proposed solution
significantly enhances the end-to-end ranking performance while relatively
minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of
up to 11% with an increase in average query latency of only 4.3%. This confirms
the advantage of seamlessly combining two distinct families of signals that
mutually contribute to retrieval effectiveness.

</details>


### [559] [FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation](https://arxiv.org/abs/2510.16597)
*Qiyao Peng,Chen Wang,Yinghui Wang,Hongtao Liu,Xuan Guo,Wenjun Wang*

Main category: cs.IR

TL;DR: 本文提出了一个大规模审稿人推荐数据集FRONTIER-RevRec，基于Frontiers出版平台的真实同行评审记录，涵盖多个学科领域。实验表明，基于内容的方法显著优于协同过滤方法，尤其是利用语言模型的方法能有效匹配论文内容与审稿人专长。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量、大规模且跨学科的基准数据集，审稿人推荐研究长期受限。现有数据集在规模、学科覆盖和方法比较方面存在不足，因此需要构建更全面的数据集以推动该领域发展。

Method: 从Frontiers开放获取出版平台（2007–2025年）的真实同行评审记录中构建FRONTIER-RevRec数据集，包含177,941名审稿人和478,379篇论文，覆盖209种期刊和多个学科。采用内容-based方法与协同过滤方法进行对比，并评估语言模型在语义匹配中的表现及不同聚合策略的效果。

Result: 基于内容的方法显著优于协同过滤；语言模型能有效捕捉论文内容与审稿人专业知识之间的语义对齐；发现了最优的聚合策略可提升推荐性能。结构分析揭示了学术推荐与商业推荐的根本差异。

Conclusion: FRONTIER-RevRec是一个大规模、多学科的审稿人推荐基准数据集，能够推动审稿人推荐研究的发展。研究结果支持使用基于内容的语言模型方法，并为构建更高效的学术同行评审系统提供了实践指导。

Abstract: Reviewer recommendation is a critical task for enhancing the efficiency of
academic publishing workflows. However, research in this area has been
persistently hindered by the lack of high-quality benchmark datasets, which are
often limited in scale, disciplinary scope, and comparative analyses of
different methodologies. To address this gap, we introduce FRONTIER-RevRec, a
large-scale dataset constructed from authentic peer review records (2007-2025)
from the Frontiers open-access publishing platform
https://www.frontiersin.org/. The dataset contains 177941 distinct reviewers
and 478379 papers across 209 journals spanning multiple disciplines including
clinical medicine, biology, psychology, engineering, and social sciences. Our
comprehensive evaluation on this dataset reveals that content-based methods
significantly outperform collaborative filtering. This finding is explained by
our structural analysis, which uncovers fundamental differences between
academic recommendation and commercial domains. Notably, approaches leveraging
language models are particularly effective at capturing the semantic alignment
between a paper's content and a reviewer's expertise. Furthermore, our
experiments identify optimal aggregation strategies to enhance the
recommendation pipeline. FRONTIER-RevRec is intended to serve as a
comprehensive benchmark to advance research in reviewer recommendation and
facilitate the development of more effective academic peer review systems. The
FRONTIER-RevRec dataset is available at:
https://anonymous.4open.science/r/FRONTIER-RevRec-5D05.

</details>


### [560] [Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization](https://arxiv.org/abs/2510.16715)
*Zulun Zhu,Haoyu Liu,Mengke He,Siqiang Luo*

Main category: cs.IR

TL;DR: 本文提出了STAR-RAG，一种用于时序知识图谱问答的检索增强框架，通过构建时间对齐的规则图并进行传播，实现高效且时间一致的检索。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要关注语义相似性，忽视显式的时间约束，导致答案时间不一致和令牌消耗过高。

Method: 提出STAR-RAG框架，包含两个核心：构建时间对齐的规则图，并在图上进行传播以缩小检索范围，优先选择语义相关且时间一致的证据。

Result: 在真实时序知识图谱数据集上的实验表明，STAR-RAG相比现有方法在减少令牌消耗的同时提高了回答准确率，且无需大量模型训练和微调。

Conclusion: STAR-RAG通过时间对齐和图传播机制，在保证准确性的同时显著提升了检索效率和时间一致性，简化了部署流程。

Abstract: Question answering in temporal knowledge graphs requires retrieval that is
both time-consistent and efficient. Existing RAG methods are largely semantic
and typically neglect explicit temporal constraints, which leads to
time-inconsistent answers and inflated token usage. We propose STAR-RAG, a
temporal GraphRAG framework that relies on two key ideas: building a
time-aligned rule graph and conducting propagation on this graph to narrow the
search space and prioritize semantically relevant, time-consistent evidence.
This design enforces temporal proximity during retrieval, reduces the candidate
set of retrieval results, and lowers token consumption without sacrificing
accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates
the need for heavy model training and fine-tuning, thereby reducing
computational cost and significantly simplifying deployment.Extensive
experiments on real-world temporal KG datasets show that our method achieves
improved answer accuracy while consuming fewer tokens than strong GraphRAG
baselines.

</details>


### [561] [Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices](https://arxiv.org/abs/2510.16736)
*Patrizio Dazzi,William Guglielmo,Franco Maria Nardini,Raffaele Perego,Salvatore Trani*

Main category: cs.IR

TL;DR: 本文提出基于FPGA的两种高效精确kNN搜索方案，适用于高维潜在空间，在吞吐量、延迟和能效方面显著优于CPU方案。


<details>
  <summary>Details</summary>
Motivation: 支持基于神经编码器模型的表示学习的大规模绿色应用，提升能效与可及性。

Method: 设计两种FPGA解决方案：一种通过批处理并行查询流式数据以最大化吞吐量；另一种对内存中的数据并行处理单个查询以最小化延迟。

Result: 在公开图像和文本数据集上的实验表明，该方案在查询每秒、延迟（最高提升16.6倍）和能效（最高节能11.9倍）方面均优于现有CPU方案。

Conclusion: FPGA可用于实现高能效、高性能的高维精确kNN搜索，具有显著的性能和节能优势。

Abstract: This paper investigates the usage of FPGA devices for energy-efficient exact
kNN search in high-dimension latent spaces. This work intercepts a relevant
trend that tries to support the increasing popularity of learned
representations based on neural encoder models by making their large-scale
adoption greener and more inclusive. The paper proposes two different
energy-efficient solutions adopting the same FPGA low-level configuration. The
first solution maximizes system throughput by processing the queries of a batch
in parallel over a streamed dataset not fitting into the FPGA memory. The
second minimizes latency by processing each kNN incoming query in parallel over
an in-memory dataset. Reproducible experiments on publicly available image and
text datasets show that our solution outperforms state-of-the-art CPU-based
competitors regarding throughput, latency, and energy consumption.
Specifically, experiments show that the proposed FPGA solutions achieve the
best throughput in terms of queries per second and the best-observed latency
with scale-up factors of up to 16.6X. Similar considerations can be made
regarding energy efficiency, where results show that our solutions can achieve
up to 11.9X energy saving w.r.t. strong CPU-based competitors.

</details>


### [562] [An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision](https://arxiv.org/abs/2510.16803)
*Zishuai Zhang,Sihao Yu,Wenyi Xie,Ying Nie,Junfeng Wang,Zhiming Zheng,Dawei Yin,Hainan Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种名为SMAR的新型全页重排序框架，利用强单模态排序器指导模态间相关性对齐，在仅使用有限全页标注的情况下显著降低了标注成本（70-90%），同时提升了搜索结果排序性能。


<details>
  <summary>Details</summary>
Motivation: 全页重排序在搜索引擎中至关重要，但依赖大规模人工标注数据成本高昂且耗时。由于全页标注需评估跨模态的相关性差异，复杂度远高于单模态标注，因此如何在减少标注成本的同时提升重排序性能是一个关键挑战。

Method: SMAR框架首先在各模态特定数据上训练高质量的单模态排序器；然后基于这些排序器输出构建候选页面，并进行有限的页面级人工标注；最后利用这些标注数据训练全页重排序模型，同时强制保持与单模态偏好的一致性，以维持各模态内的排序质量。

Result: 在Qilin和百度数据集上的实验表明，SMAR相比基线模型在显著降低标注成本（70-90%）的同时实现了更好的排序效果。百度APP的离线和在线A/B测试也显示出在标准排序指标和用户体验指标上的明显提升。

Conclusion: SMAR通过利用单模态排序器引导模态间对齐，有效解决了全页重排序中标注成本高与性能提升之间的矛盾，具有显著的实际应用价值，适用于真实场景下的搜索结果优化。

Abstract: The whole-page reranking plays a critical role in shaping the user experience
of search engines, which integrates retrieval results from multiple modalities,
such as documents, images, videos, and LLM outputs. Existing methods mainly
rely on large-scale human-annotated data, which is costly to obtain and
time-consuming. This is because whole-page annotation is far more complex than
single-modal: it requires assessing the entire result page while accounting for
cross-modal relevance differences. Thus, how to improve whole-page reranking
performance while reducing annotation costs is still a key challenge in
optimizing search engine result pages(SERP). In this paper, we propose SMAR, a
novel whole-page reranking framework that leverages strong Single-modal rankers
to guide Modal-wise relevance Alignment for effective Reranking, using only
limited whole-page annotation to outperform fully-annotated reranking models.
Specifically, high-quality single-modal rankers are first trained on data
specific to their respective modalities. Then, for each query, we select a
subset of their outputs to construct candidate pages and perform human
annotation at the page level. Finally, we train the whole-page reranker using
these limited annotations and enforcing consistency with single-modal
preferences to maintain ranking quality within each modality. Experiments on
the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by
about 70-90\% while achieving significant ranking improvements compared to
baselines. Further offline and online A/B testing on Baidu APPs also shows
notable gains in standard ranking metrics as well as user experience
indicators, fully validating the effectiveness and practical value of our
approach in real-world search scenarios.

</details>


### [563] [The Layout Is the Model: On Action-Item Coupling in Generative Recommendation](https://arxiv.org/abs/2510.16804)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: 本文研究了生成式推荐模型中的token布局问题，提出了基于三个设计原则的统一分析框架，并设计了一种新颖的非交错布局方法LAC，在保证推荐准确性的同时显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型中，物品和行为的token布局影响信息利用和模型泛化能力，现有方法在效率与信息完整性之间存在权衡，缺乏系统性指导。

Method: 提出三个第一性原理指导token布局设计：最大化输入输出空间中的物品/行为信号、保持“行为基于物品”的条件关系、避免信息泄露；在此基础上设计了非交错的Lagged Action Conditioning（LAC）方法。

Result: 实验在公开数据集和大规模生产日志上验证了设计原则的有效性，LAC方法在显著降低FLOPs的情况下，推荐性能达到或优于交错布局。

Conclusion: LAC是一种高效且准确的非交错布局方案，研究结果为构建兼具精度与效率的生成式推荐系统提供了实用指导。

Abstract: Generative Recommendation (GR) models treat a user's interaction history as a
sequence to be autoregressively predicted. When both items and actions (e.g.,
watch time, purchase, comment) are modeled, the layout-the ordering and
visibility of item/action tokens-critically determines what information the
model can use and how it generalizes. We present a unified study of token
layouts for GR grounded in first principles: (P1) maximize item/action signal
in both input/output space, (P2) preserve the conditioning relationship "action
given item" and (P3) no information leakage.
  While interleaved layout (where item and action occupy separate tokens)
naturally satisfies these principles, it also bloats sequence length with
larger training/inference cost. On the non-interleaved front, we design a novel
and effective approach, Lagged Action Conditioning (LAC), which appears strange
on the surface but aligns well with the design principles to yield strong
accuracy. Comprehensive experiments on public datasets and large-scale
production logs evaluate different layout options and empirically verifies the
design principles. Our proposed non-interleaved method, LAC, achieves
competitive or superior quality at substantially lower FLOPs than interleaving.
Our findings offer actionable guidance for assembling GR systems that are both
accurate and efficient.

</details>


### [564] [Towards Context-aware Reasoning-enhanced Generative Searching in E-commerce](https://arxiv.org/abs/2510.16925)
*Zhiding Liu,Ben Chen,Mingyue Cheng,Enchong Chen,Li Li,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出一种上下文感知的生成式搜索框架，通过统一异构上下文表示和自演化的后训练范式增强模型推理能力，有效提升电商搜索推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有搜索推荐方法在整合用户复杂搜索上下文（如时空因素、历史行为、当前查询）方面存在不足，难以充分捕捉用户意图。

Method: 将用户和物品的异构上下文统一为文本表示并进行对齐；引入结合监督微调与强化学习的自演化后训练范式以增强推理能力；提出去偏的GRPO变体以缓解强化学习在搜索场景中的偏差问题。

Result: 在真实电商搜索日志数据上的实验表明，该方法显著优于强基线模型，在搜索推荐任务中取得更优性能。

Conclusion: 所提出的上下文感知且具备推理增强能力的生成式搜索框架能更有效地理解复杂上下文，提升搜索推荐效果。

Abstract: Search-based recommendation is one of the most critical application scenarios
in e-commerce platforms. Users' complex search contexts--such as spatiotemporal
factors, historical interactions, and current query's information--constitute
an essential part of their decision-making, reflecting implicit preferences
that complement explicit query terms. Modeling such rich contextual signals and
their intricate associations with candidate items remains a key challenge.
Although numerous efforts have been devoted to building more effective search
methods, existing approaches still show limitations in integrating contextual
information, which hinders their ability to fully capture user intent.
  To address these challenges, we propose a context-aware reasoning-enhanced
generative search framework for better \textbf{understanding the complicated
context}. Specifically, the framework first unifies heterogeneous user and item
contexts into textual representations or text-based semantic identifiers and
aligns them. To overcome the lack of explicit reasoning trajectories, we
introduce a self-evolving post-training paradigm that iteratively combines
supervised fine-tuning and reinforcement learning to progressively enhance the
model's reasoning capability. In addition, we identify potential biases in
existing RL algorithms when applied to search scenarios and present a debiased
variant of GRPO to improve ranking performance. Extensive experiments on search
log data collected from a real-world e-commerce platform demonstrate that our
approach achieves superior performance compared with strong baselines,
validating its effectiveness for search-based recommendation.

</details>


### [565] [DSEBench: A Test Collection for Explainable Dataset Search with Examples](https://arxiv.org/abs/2510.17228)
*Qing Shi,Jing He,Qiaosheng Chen,Gong Cheng*

Main category: cs.IR

TL;DR: 本文研究了带示例的数据集搜索（DSE）及其可解释性扩展，提出了DSEBench测试集并建立多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集搜索范式仅支持关键词或相似性检索，难以满足复杂信息需求，因此需要更通用的搜索方式。

Method: 提出Dataset Search with Examples（DSE）及可解释DSE任务，构建DSEBench测试集，利用大语言模型生成标注数据，并评估稀疏、密集和基于LLM的检索、重排序与解释方法。

Result: 建立了支持数据集级和字段级标注的DSEBench测试集，生成大量训练用标注，并在多种方法上建立了全面的基线实验结果。

Conclusion: DSE和可解释DSE是可行且有价值的研究方向，DSEBench为该领域提供了重要资源，基于LLM的方法展现出潜力。

Abstract: Dataset search has been an established information retrieval task. Current
paradigms either retrieve datasets that are relevant to a keyword query or find
datasets that are similar to an input target dataset. To allow for their
combined specification of information needs, in this article, we investigate
the more generalized task of Dataset Search with Examples (DSE) and further
extend it to Explainable DSE that requires identifying the metadata and content
fields of a dataset that indicate its relevance to the query and similarity to
the target datasets. To facilitate this research, we construct DSEBench, a test
collection that provides high-quality dataset- and field-level annotations to
enable the evaluation of explainable DSE. We also employ a large language model
to generate numerous annotations to be used for training. We establish
extensive baselines on DSEBench by adapting and evaluating a variety of sparse,
dense, and LLM-based retrieval, reranking, and explanation methods.

</details>


### [566] [On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders](https://arxiv.org/abs/2510.17245)
*Wenyu Mao,Jiancan Wu,Guoqing Hu,Wei Ji,Xiang Wang*

Main category: cs.IR

TL;DR: TA-Rec是一种用于序列推荐的两阶段扩散模型框架，通过预训练阶段的时序一致性正则化和微调阶段的自适应偏好对齐，实现高效且准确的一次生成推荐。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成式序列推荐中依赖多步去噪过程，存在离散化误差，导致计算效率与推荐效果之间的权衡问题。

Method: 提出TA-Rec框架：第一阶段采用时序一致性正则化（TCR）平滑去噪函数，实现一步生成；第二阶段引入自适应偏好对齐（APA），根据用户偏好动态调整去噪过程。

Result: 实验表明，TA-Rec有效缓解了离散化误差带来的性能损失，在保持高推荐准确性的同时显著提升了生成效率。

Conclusion: TA-Rec通过两阶段设计解决了扩散模型在序列推荐中效率与效果的权衡问题，为高效生成式推荐提供了新思路。

Abstract: Diffusion models have emerged as a powerful paradigm for generative
sequential recommendation, which typically generate next items to recommend
guided by user interaction histories with a multi-step denoising process.
However, the multi-step process relies on discrete approximations, introducing
discretization error that creates a trade-off between computational efficiency
and recommendation effectiveness. To address this trade-off, we propose TA-Rec,
a two-stage framework that achieves one-step generation by smoothing the
denoising function during pretraining while alleviating trajectory deviation by
aligning with user preferences during fine-tuning. Specifically, to improve the
efficiency without sacrificing the recommendation performance, TA-Rec pretrains
the denoising model with Temporal Consistency Regularization (TCR), enforcing
the consistency between the denoising results across adjacent steps. Thus, we
can smooth the denoising function to map the noise as oracle items in one step
with bounded error. To further enhance effectiveness, TA-Rec introduces
Adaptive Preference Alignment (APA) that aligns the denoising process with user
preference adaptively based on preference pair similarity and timesteps.
Extensive experiments prove that TA-Rec's two-stage objective effectively
mitigates the discretization errors-induced trade-off, enhancing both
efficiency and effectiveness of diffusion-based recommenders.

</details>


### [567] [How role-play shapes relevance judgment in zero-shot LLM rankers](https://arxiv.org/abs/2510.17535)
*Yumeng Wang,Jirui Qi,Catherine Chen,Panagiotis Eustratiadis,Suzan Verberne*

Main category: cs.IR

TL;DR: 本文研究了角色扮演提示如何影响大语言模型在零样本排序任务中的表现，发现角色描述的精心设计能显著提升排序质量，且角色信息主要在模型的早期层编码，并通过中间层与任务指令交互。


<details>
  <summary>Details</summary>
Motivation: 角色扮演提示虽能提升大语言模型的排序性能，但其作用机制和多样性尚不明确，限制了其有效应用和可解释性。

Method: 采用来自机械可解释性的因果干预技术，系统分析不同角色扮演变体对零样本LLM排序器的影响，并追踪角色信息如何影响相关性判断。

Result: 发现（1）角色描述的精细设计显著影响LLM排序质量；（2）角色信号主要在早期层编码，与任务指令在中间层交互，且与查询或文档表征交互较少；并识别出一组对角色条件相关性至关重要的注意力头。

Conclusion: 研究揭示了角色扮演在LLM排序中的内在机制，为设计更有效的信息检索提示提供了指导，并拓展了角色扮演在零样本应用中的潜力。

Abstract: Large Language Models (LLMs) have emerged as promising zero-shot rankers, but
their performance is highly sensitive to prompt formulation. In particular,
role-play prompts, where the model is assigned a functional role or identity,
often give more robust and accurate relevance rankings. However, the mechanisms
and diversity of role-play effects remain underexplored, limiting both
effective use and interpretability. In this work, we systematically examine how
role-play variations influence zero-shot LLM rankers. We employ causal
intervention techniques from mechanistic interpretability to trace how
role-play information shapes relevance judgments in LLMs. Our analysis reveals
that (1) careful formulation of role descriptions have a large effect on the
ranking quality of the LLM; (2) role-play signals are predominantly encoded in
early layers and communicate with task instructions in middle layers, while
receiving limited interaction with query or document representations.
Specifically, we identify a group of attention heads that encode information
critical for role-conditioned relevance. These findings not only shed light on
the inner workings of role-play in LLM ranking but also offer guidance for
designing more effective prompts in IR and beyond, pointing toward broader
opportunities for leveraging role-play in zero-shot applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [568] [Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience](https://arxiv.org/abs/2510.16034)
*Bo Li,Junwei Ma,Kai Yin,Yiming Xiao,Chia-Wei Hsu,Ali Mostafavi*

Main category: cs.MA

TL;DR: 本文提出了“灾难助手”（Disaster Copilot），一种多智能体人工智能系统，旨在通过整合专业AI工具和多模态数据，提升灾害管理的响应能力，实现从被动模型到主动智能环境的转变。


<details>
  <summary>Details</summary>
Motivation: 现有灾害管理体系存在数据碎片化、技术孤岛、资源限制和机构记忆流失等问题，难以支持及时有效的决策，亟需系统性解决方案。

Method: 设计了一个基于中央协调器的多智能体系统架构，各子智能体专注于风险预测、态势感知和影响评估等领域，并通过设备端协调和知识留存机制支持资源受限环境下的运行。

Result: 该系统能够提供实时、全面的作战视图，增强灾害数字孪生系统的智能化水平，并提出分三阶段并行发展技术、组织能力和人机协作的实施路径。

Conclusion: Disaster Copilot 提供了一种变革性愿景，通过人机协同智能，推动构建更具适应性、数据驱动和韧性的社区。

Abstract: The escalating frequency and severity of disasters routinely overwhelm
traditional response capabilities, exposing critical vulnerability in disaster
management. Current practices are hindered by fragmented data streams, siloed
technologies, resource constraints, and the erosion of institutional memory,
which collectively impede timely and effective decision making. This study
introduces Disaster Copilot, a vision for a multi-agent artificial intelligence
system designed to overcome these systemic challenges by unifying specialized
AI tools within a collaborative framework. The proposed architecture utilizes a
central orchestrator to coordinate diverse sub-agents, each specializing in
critical domains such as predictive risk analytics, situational awareness, and
impact assessment. By integrating multi-modal data, the system delivers a
holistic, real-time operational picture and serve as the essential AI backbone
required to advance Disaster Digital Twins from passive models to active,
intelligent environments. Furthermore, it ensures functionality in
resource-limited environments through on-device orchestration and incorporates
mechanisms to capture institutional knowledge, mitigating the impact of staff
turnover. We detail the system architecture and propose a three-phased roadmap
emphasizing the parallel growth of technology, organizational capacity, and
human-AI teaming. Disaster Copilot offers a transformative vision, fostering
collective human-machine intelligence to build more adaptive, data-driven and
resilient communities.

</details>


### [569] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 提出了一种用于零样本迁移的通用策略改进方法（GPAT），可在多智能体系统中实现对新团队的快速适应。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的多智能体系统中，智能体需要与未曾见过的队友进行即兴协作，现有方法在零样本场景下的泛化能力有限。

Method: 将问题形式化为即兴多智能体马尔可夫决策过程，结合通用策略改进和差异奖励实现跨团队的知识迁移。

Result: 在三个模拟环境（合作觅食、捕食者-猎物、Overcooked）和真实多机器人场景中验证了GPAT的有效性，实现了成功的零样本迁移。

Conclusion: GPAT通过利用预训练策略并结合通用策略改进与差异奖励，有效提升了多智能体系统在即兴团队中的零样本协作性能。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [570] [Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences](https://arxiv.org/abs/2510.16221)
*Qinshuang Wei,Vaibhav Srivastava,Vijay Gupta*

Main category: cs.MA

TL;DR: 本文研究了多智能体任务分配问题，其中智能体在任务完成时间、资源消耗和奖励偏好方面具有异质性。提出了一种基于赌博机的算法，并分析了在精确和近似求解最优任务分配情况下的可实现遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有的任务分配研究多集中于单个智能体，而多智能体系统中由于成员能力与偏好的异质性，相关问题尚未充分解决。因此，需要一种能够处理不确定性并优化团队总期望奖励的分配机制。

Method: 采用赌博机（bandit）算法框架，结合随机未知分布的任务奖励、执行时间和资源消耗模型，在满足各智能体资源约束的前提下，最大化团队在整个时间范围内的总期望奖励；同时分析精确与近似求解任务分配对遗憾的影响。

Result: 提出了适用于异质多智能体任务分配的赌博机算法，并给出了在两种不同求解精度下的遗憾界分析结果。

Conclusion: 该方法能够在不确定环境下有效进行多智能体任务分配，在保证资源可行性的前提下最大化团队收益，且在理论上有良好的遗憾性能。

Abstract: While sequential task assignment for a single agent has been widely studied,
such problems in a multi-agent setting, where the agents have heterogeneous
task preferences or capabilities, remain less well-characterized. We study a
multi-agent task assignment problem where a central planner assigns recurring
tasks to multiple members of a team over a finite time horizon. For any given
task, the members have heterogeneous capabilities in terms of task completion
times, task resource consumption (which can model variables such as energy or
attention), and preferences in terms of the rewards they collect upon task
completion. We assume that the reward, execution time, and resource consumption
for each member to complete any task are stochastic with unknown distributions.
The goal of the planner is to maximize the total expected reward that the team
receives over the problem horizon while ensuring that the resource consumption
required for any assigned task is within the capability of the agent. We
propose and analyze a bandit algorithm for this problem. Since the bandit
algorithm relies on solving an optimal task assignment problem repeatedly, we
analyze the achievable regret in two cases: when we can solve the optimal task
assignment exactly and when we can solve it only approximately.

</details>


### [571] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: 本文提出了一种名为MA-SAPO的多智能体框架，用于评分感知的提示优化，通过将评估结果与结构化推理相结合，实现更透明、可审计和可控的提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法大多将评估视为黑箱，仅依赖数值评分，缺乏对提示成败原因的深入理解，且依赖难以解释和控制的试错式改进。

Method: 提出MA-SAPO框架，包含两个阶段：推理阶段中，多个智能体协作解释评分、诊断弱点并生成可重用的推理资产；测试阶段中，智能体检索这些资产以分析优化提示并应用基于证据的修改。

Result: 在HelpSteer1/2基准上的实验表明，MA-SAPO在性能上优于单次提示、检索增强基线和先前的多智能体策略。

Conclusion: MA-SAPO通过将评估信号转化为可解释的推理链，实现了更系统化、透明和可控的提示优化，验证了其有效性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


### [572] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC是一个可扩展的分布式框架，用于在大型机器人集群中实现高效的任务分配和路径规划，采用分区架构与动态选举领导者，并结合基于力的去中心化路径规划算法实现避障。


<details>
  <summary>Details</summary>
Motivation: 在大规模机器人集群中，传统集中式方法难以应对可扩展性和实时性挑战，因此需要一种兼具一致性、效率和可扩展性的分布式任务分配与路径规划方案。

Method: 提出一种基于区域划分的分布式架构，通过动态选举领导者节点并采用时间步同步的共识协议保证系统一致性；路径规划方面引入一种基于力的去中心化实时避障算法。

Result: 在ROS 2环境下通过初步仿真验证，DiRAC在模拟仓库环境中展现出良好的架构可扩展性和模块效率。

Conclusion: DiRAC为大规模工业与物流场景中的机器人集群部署提供了可行的分布式解决方案基础。

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [573] [Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents](https://arxiv.org/abs/2510.16978)
*Dheeraj Chintapalli,Rikhil Tanugula,Sunkalp Chandra*

Main category: cs.MA

TL;DR: Lark是一个结合了LLM驱动推理与进化式多智能体系统的决策框架，通过四种机制优化策略生成，在控制成本的同时实现高效的利益相关者对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有决策系统中冗长输出和利益相关者权衡不足的问题，提升策略生成的效率与可解释性。

Method: 提出Lark框架，集成可塑性调整、复制与成熟化、加权排序投票和基于token的计算成本惩罚四种机制，构建一个迭代的、计算感知的神经进化循环。

Result: 在30轮实验中，Lark Full平均排名2.55（95% CI [2.17, 2.93]），综合得分29.4/50，80%轮次进入前三，单任务成本0.016美元；消融实验显示所有机制均有显著贡献。

Conclusion: Lark提供了一种实用且透明的方法来生成利益相关者对齐的策略，相比传统MDP更具灵活性和扩展性，具备现实应用潜力。

Abstract: We present Lark, a biologically inspired decision-making framework that
couples LLM-driven reasoning with an evolutionary, stakeholder-aware
Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we
integrate four mechanisms: (i) plasticity, which applies concise adjustments to
candidate solutions; (ii) duplication and maturation, which copy
high-performing candidates and specialize them into new modules; (iii)
ranked-choice stakeholder aggregation using influence-weighted Borda scoring;
and (iv) compute awareness via token-based penalties that reward brevity. The
system iteratively proposes diverse strategies, applies plasticity tweaks,
simulates stakeholder evaluations, aggregates preferences, selects top
candidates, and performs duplication/maturation while factoring compute cost
into final scores. In a controlled evaluation over 30 rounds comparing 14
systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a
mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%
of rounds while remaining cost competitive with leading commercial models
($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms
contribute significantly as ablating duplication/maturation yields the largest
deficit ({\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by
plasticity ({\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting
({\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\Delta}Score = 2.2,
d_z = 1.63). Rather than a formal Markov Decision Process with constrained
optimization, Lark is a practical, compute-aware neuroevolutionary loop that
scales stakeholder-aligned strategy generation and makes trade-offs transparent
through per-step metrics. Our work presents proof-of-concept findings and
invites community feedback as we expand toward real-world validation studies.

</details>


### [574] [ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI](https://arxiv.org/abs/2510.17004)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.MA

TL;DR: ReclAIm是一个基于大语言模型的多智能体框架，能够自主监控、评估和微调医学图像分类模型，通过自然语言交互实现无需编程的持续性能维护，在MRI、CT和X射线数据上有效恢复因性能下降（最多-41.1%）而损失的模型表现至初始水平的1.5%以内。


<details>
  <summary>Details</summary>
Motivation: 为确保AI模型在临床实践中长期可靠，需要持续监控其性能并在性能下降时采取纠正措施。现有方法通常依赖人工干预和编程技能，难以实现自动化和广泛部署。

Method: 提出ReclAIm，一个基于大语言模型的多智能体框架，通过自然语言进行交互，实现对医学图像分类模型的自动训练、评估与微调；系统能检测性能退化并触发先进的微调流程以恢复性能。

Result: ReclAIm在多种医学影像数据（MRI、CT、X射线）上成功维持模型性能；在性能下降高达-41.1%的情况下（如MRI上的InceptionV3），可将性能恢复到接近原始水平的1.5%范围内。

Conclusion: ReclAIm提供了一种用户友好、可适应性强的自动化方案，可用于医学影像AI模型的持续维护，有助于推动其在研究和临床环境中的广泛应用。

Abstract: Ensuring the long-term reliability of AI models in clinical practice requires
continuous performance monitoring and corrective actions when degradation
occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent
framework capable of autonomously monitoring, evaluating, and fine-tuning
medical image classification models. The system, built on a large language
model core, operates entirely through natural language interaction, eliminating
the need for programming expertise. ReclAIm successfully trains, evaluates, and
maintains consistent performance of models across MRI, CT, and X-ray datasets.
Once ReclAIm detects significant performance degradation, it autonomously
executes state-of-the-art fine-tuning procedures that substantially reduce the
performance gap. In cases with performance drops of up to -41.1% (MRI
InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of
the initial model results. ReclAIm enables automated, continuous maintenance of
medical imaging AI models in a user-friendly and adaptable manner that
facilitates broader adoption in both research and clinical environments.

</details>


### [575] [MiCRO for Multilateral Negotiations](https://arxiv.org/abs/2510.17401)
*David Aguilera-Luzon,Dave de Jonge,Javier Larrosa*

Main category: cs.MA

TL;DR: 本文提出了一种多边谈判的MiCRO变体，该变体在无需对手建模或机器学习的情况下，在ANAC竞赛中表现优于现有最优策略，并形成经验纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 原始的MiCRO策略在双边谈判中表现出色，但其在多边谈判中的推广尚未解决，且现有基准域可能过于简单。

Method: 设计并实现MiCRO的多边版本，通过与ANAC 2015、2017和2018冠军策略对比进行评估，并采用实证博弈论方法分析其均衡性。

Result: 多边MiCRO在多个测试中优于ANAC优胜策略，并被证明形成经验纳什均衡。

Conclusion: MiCRO可成功扩展至多边谈判场景，且在无复杂建模的前提下保持高性能，支持其作为稳健谈判策略的潜力。

Abstract: Recently, a very simple new bilateral negotiation strategy called MiCRO was
introduced that does not make use of any kind of opponent modeling or machine
learning techniques and that does not require fine-tuning of any parameters.
Despite its simplicity, it was shown that MiCRO performs similar to -- or even
better than -- most state-of-the-art negotiation strategies. This lead its
authors to argue that the benchmark domains on which negotiation algorithms are
typically tested may be too simplistic. However, one question that was left
open, was how MiCRO could be generalized to multilateral negotiations. In this
paper we fill this gap by introducing a multilateral variant of MiCRO. We
compare it with the winners of the Automated Negotiating Agents Competitions
(ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore,
we perform an empirical game-theoretical analysis to show that our new version
of MiCRO forms an empirical Nash equilibrium.

</details>


### [576] [Strategyproof Facility Location for Five Agents on a Circle using PCD](https://arxiv.org/abs/2510.17435)
*Ido Farjoun,Reshef Meir*

Main category: cs.MA

TL;DR: 本文研究了圆周上5个代理的策略证明设施定位问题，提出了PCD机制的紧界，并通过系统化减少实例空间和优化技术证明了该界限的紧致性，同时假设了PCD在一般奇数n下的近似比。


<details>
  <summary>Details</summary>
Motivation: 为了在圆形空间中实现更高效的策略证明设施定位，特别是在代理数量为5的情况下，寻求最优机制的性能边界。

Method: 采用PCD策略证明机制，按代理前方弧长比例选择其报告位置，并通过系统化减少实例空间结合标准优化技术进行分析。

Result: 找到了5代理情况下PCD机制的紧界，并验证了其紧致性，同时提出了对一般奇数n的近似比的假设。

Conclusion: PCD机制在5代理圆周设施定位问题中具有明确的性能上限，且该结果可推广至一般奇数代理情形的近似比猜想。

Abstract: We consider the strategyproof facility location problem on a circle. We focus
on the case of 5 agents, and find a tight bound for the PCD strategyproof
mechanism, which selects the reported location of an agent in proportion to the
length of the arc in front of it. We methodically "reduce" the size of the
instance space and then use standard optimization techniques to find and prove
the bound is tight. Moreover we hypothesize the approximation ratio of PCD for
general odd $n$.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [577] [Two-Stage Sketch-Based Smoke Illustration Generation using Stream Function](https://arxiv.org/abs/2510.15873)
*Hengyuan Chang,Xiaoxuan Xie,Syuhei Sato,Haoran Xie*

Main category: cs.GR

TL;DR: 提出了一种基于草图的烟雾插画生成框架，结合流函数和潜在扩散模型，利用用户草图引导流函数生成，进而控制速度场生成以实现符合预期流动的烟雾模拟。


<details>
  <summary>Details</summary>
Motivation: 为了更好地将用户草图与烟雾模拟中的流动特性对齐，捕捉草图中缺乏的连续变化和旋转流动细节。

Method: 采用两阶段框架，首先用草图引导生成流函数，作为速度场生成器的控制条件；使用流线在训练过程中编码全局流动动态作为草图引导，并利用流函数作为中间表示。

Result: 生成的速度场能够有效指导烟雾模拟，使其流动与预期一致，且能捕捉到草图中缺失的复杂流动细节。

Conclusion: 该方法通过引入流函数作为中间表示，结合潜在扩散模型，实现了高质量、符合用户意图的烟雾插画生成。

Abstract: In this paper, we propose a two-stage sketch-based smoke illustration
generation framework using stream function and latent diffusion models (LDM).
The user sketch is used to guide the generation of the stream function, which
serves as the control condition for the velocity field generator. The generated
velocity field can be used to guide the smoke simulation to align with the
intended flow. We adopt streamlines to encode global flow dynamics as sketch
guidance during training. The stream function constitutes the intermediate
representation that captures continuous variation and rotational flow details
absent from sketches.

</details>


### [578] [Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images](https://arxiv.org/abs/2510.15874)
*Hao Jin,Haoran Xie*

Main category: cs.GR

TL;DR: 本文提出了一种基于运动草图引导的静态图像流体动画生成框架，利用微调的条件潜在扩散模型生成运动场，并通过运动适配器将其融入潜在视频扩散模型，实现对流体运动的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成流体的平滑且时间一致的动画方面存在挑战，且物理方法易受边界条件影响，因此需要一种能精确控制流体运动并保持时间连贯性的新方法。

Method: 提出一个两阶段框架：首先使用微调的条件潜在扩散模型根据用户提供的运动草图生成运动场，然后通过运动适配器将运动场集成到潜在视频扩散模型中以生成动态景观视频。

Result: 该方法能够生成高质量、时间上连贯的流体动画，有效实现对瀑布、河流和海洋等复杂流体运动的精确控制。

Conclusion: 所提出的框架在静态图像中实现了逼真的流体运动合成，结合草图引导与扩散模型的优势，为艺术表达中的动态景观生成提供了有效解决方案。

Abstract: Integrating motion into static images not only enhances visual expressiveness
but also creates a sense of immersion and temporal depth, establishing it as a
longstanding and impactful theme in artistic expression. Fluid elements such as
waterfall, river, and oceans are common features in landscape, but their
complex dynamic characteristics pose significant challenges in modeling and
controlling their motion within visual computing. Physics-based methods are
often used in fluid animation to track particle movement. However, they are
easily affected by boundary conditions. Recently, latent diffusion models have
been applied to video generation tasks, demonstrating impressive capabilities
in producing high-quality and temporally coherent results. However, it is
challenging for the existing methods to animate fluid smooth and temporally
consistent motion. To solve these issues, this paper introduces a framework for
generating landscape videos by animating fluid in still images under the
guidance of motion sketches. We propose a finetuned conditional latent
diffusion model for generating motion field from user-provided sketches, which
are subsequently integrated into a latent video diffusion model via a motion
adapter to precisely control the fluid movement.

</details>


### [579] [Adaptive Frameless Rendering](https://arxiv.org/abs/2510.15876)
*Abhinav Dayal,Cliff Woolley,Benjamin Watson,David Luebke*

Main category: cs.GR

TL;DR: 提出一种自适应的无帧渲染方法，通过细粒度适应时空颜色变化，显著提高渲染速度。


<details>
  <summary>Details</summary>
Motivation: 传统交互式渲染方法受限于固定采样模式，无法高效处理动态场景中的边缘和运动区域，需要更灵活的采样与重建机制。

Method: 采用闭环反馈指导采样，利用时深缓冲区存储短期样本用于重建和反馈，基于GPU的重建结合采样密度和时空颜色梯度，并使用样本重投影优化重建并引导采样。

Result: 模拟结果显示，相比传统渲染，在相似视觉质量下所需样本减少一个数量级，计算开销仅增加15%。

Conclusion: 该无帧渲染方法能有效提升渲染效率，尤其适用于动静混合的复杂场景。

Abstract: We propose an adaptive form of frameless rendering with the potential to
dramatically increase rendering speed over conventional interactive rendering
approaches. Without the rigid sampling patterns of framed renderers, sampling
and reconstruction can adapt with very fine granularity to spatio-temporal
color change. A sampler uses closed-loop feedback to guide sampling toward
edges or motion in the image. Temporally deep buffers store all the samples
created over a short time interval for use in reconstruction and as sampler
feedback. GPU-based reconstruction responds both to sampling density and
space-time color gradients. Where the displayed scene is static, spatial color
change dominates and older samples are given significant weight in
reconstruction, resulting in sharper and eventually antialiased images. Where
the scene is dynamic, more recent samples are emphasized, resulting in less
sharp but more up-to-date images. We also use sample reprojection to improve
reconstruction and guide sampling toward occlusion edges, undersampled regions,
and specular highlights. In simulation our frameless renderer requires an order
of magnitude fewer samples than traditional rendering of similar visual quality
(as measured by RMS error), while introducing overhead amounting to 15% of
computation time.

</details>


### [580] [Procedural modeling of urban land use](https://arxiv.org/abs/2510.15877)
*Thomas Lechner,Ben Watson,Uri Wilenski,Seth Tisue,Martin Felsen,Andy Moddrell,Pin Ren,Craig Brozefsky*

Main category: cs.GR

TL;DR: 提出一种用于程序化生成城市中真实土地使用模式的方法，自动为艺术家布置建筑和道路。


<details>
  <summary>Details</summary>
Motivation: 城市在数字制作中很重要，但其复杂性和规模使其建模非常具有挑战性，且目前缺乏有效工具帮助艺术家完成此项工作。

Method: 采用程序化生成方法，自动化地生成城市中的土地使用模式，包括建筑和道路的布局。

Result: 实现了对城市土地使用的自动化建模，能够生成逼真的城市布局，减轻艺术家的工作负担。

Conclusion: 该方法有助于提高数字内容生产效率，在不显著增加成本的情况下生成更丰富、更真实的城市环境。

Abstract: Cities are important elements of content in digital productions, but their
complexity and size make them very challenging to model. Few tools exist that
can help artists with this work, even as rapid improvements in graphics
hardware create demand for richer content without matching increases in
production cost. We propose a method for procedurally generating realistic
patterns of land use in cities, automating placement of buildings and roads for
artists.

</details>


### [581] [Structural Tree Extraction from 3D Surfaces](https://arxiv.org/abs/2510.15886)
*Diogo de Andrade,Nuno Fachada*

Main category: cs.GR

TL;DR: 本文提出了一种从3D无组织多边形数据中提取分层树状结构的方法，通过图表示和Steiner树生成关键点间的优化连接，并利用视线约束进行结构优化，适用于导航感知的几何分析。


<details>
  <summary>Details</summary>
Motivation: 传统骨架化方法通常依赖体素化解释，难以直接应用于表面数据；本文旨在开发一种直接在表面上操作的方法，以更好地支持导航相关的几何分析任务。

Method: 首先从3D表面提取图表示，然后根据应用定义的关键端点生成Steiner树，并结合视线约束对结构进行精简和优化。

Result: 该方法在两个用例中得到验证：用于程序化内容生成的基于图块元素的结构提取，以及自动关卡分析中的关键点与结构度量识别；结果表明其能生成简化且连贯的结构表示。

Conclusion: 所提方法能够有效从复杂3D表面数据中提取层次化结构，优于传统骨架化方法，适用于程序化生成、空间推理和地图分析等应用。

Abstract: This paper introduces a method to extract a hierarchical tree representation
from 3D unorganized polygonal data. The proposed approach first extracts a
graph representation of the surface, which serves as the foundation for
structural analysis. A Steiner tree is then generated to establish an optimized
connection between key terminal points, defined according to
application-specific criteria. The structure can be further refined by
leveraging line-of-sight constraints, reducing redundancy while preserving
essential connectivity. Unlike traditional skeletonization techniques, which
often assume volumetric interpretations, this method operates directly on the
surface, ensuring that the resulting representation remains relevant for
navigation-aware geometric analysis. The method is validated through two use
cases: extracting structural representations from tile-based elements for
procedural content generation, and identifying key points and structural
metrics for automated level analysis. Results demonstrate its ability to
produce simplified, coherent representations, supporting applications in
procedural generation, spatial reasoning, and map analysis.

</details>


### [582] [Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search](https://arxiv.org/abs/2510.16147)
*Maxim Gumin,Do Heon Han,Seung Jean Yoo,Aditya Ganeshan,R. Kenny Jones,Kailiang Fu,Rio Aguina-Kang,Stewart Morris,Daniel Ritchie*

Main category: cs.GR

TL;DR: 提出一种基于LLM的3D场景布局生成的指令式新范式，通过迭代放置物体并引入纠错机制，在人类感知实验中显著优于现有声明式方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的3D场景布局多采用声明式方法，需复杂约束求解且难以处理复杂场景，限制了生成质量与灵活性。

Method: 采用指令式范式，由LLM依次决定物体位置和朝向，基于已放置物体逐步构建场景，并设计迭代纠错机制提升布局有效性。

Result: 在感知实验中，82%和94%的情况下优于两种声明式方法；提出与人类偏好对齐的自动化评估指标。

Conclusion: 指令式方法在布局质量、表达能力与鲁棒性上优于传统声明式方法，为文本到3D场景生成提供了更优的布局解决方案。

Abstract: Synthesizing 3D scenes from open-vocabulary text descriptions is a
challenging, important, and recently-popular application. One of its critical
subproblems is layout generation: given a set of objects, lay them out to
produce a scene matching the input description. Nearly all recent work adopts a
declarative paradigm for this problem: using an LLM to generate a specification
of constraints between objects, then solving those constraints to produce the
final layout. In contrast, we explore an alternative imperative paradigm, in
which an LLM iteratively places objects, with each object's position and
orientation computed as a function of previously-placed objects. The imperative
approach allows for a simpler scene specification language while also handling
a wider variety and larger complexity of scenes. We further improve the
robustness of our imperative scheme by developing an error correction mechanism
that iteratively improves the scene's validity while staying as close as
possible to the original layout generated by the LLM. In forced-choice
perceptual studies, participants preferred layouts generated by our imperative
approach 82% and 94% of the time when compared against two declarative layout
generation methods. We also present a simple, automated evaluation metric for
3D scene layout generation that aligns well with human preferences.

</details>


### [583] [Region-Aware Wasserstein Distances of Persistence Diagrams and Merge Trees](https://arxiv.org/abs/2510.16486)
*Mathieu Pont,Christoph Garth*

Main category: cs.GR

TL;DR: 本文提出了一种针对持续同调图和合并树的Wasserstein距离的推广方法，通过利用拓扑特征在输入域中的区域信息，提高了度量的判别能力，并提供了控制计算时间和内存消耗的策略。


<details>
  <summary>Details</summary>
Motivation: 传统Wasserstein距离在比较拓扑特征时未充分利用其在输入域中的空间区域信息，限制了其判别能力。因此，需要一种更精细的距离度量方法来更好地捕捉拓扑结构的变化。

Method: 重新定义拓扑特征的比较方式，将其转化为极值对齐区域的数值差异距离；引入可调参数以控制区域属性对距离的影响；提出两种策略：使用区域子集以减少计算时间，压缩区域属性以降低内存占用。

Result: 实验表明该方法在公开数据集上运行效率高（平均几分钟内完成），生成的距离矩阵可用于降维可视化和关键阶段检测；支持拓扑特征的演化追踪，并提出时间持久性曲线来展示特征随时间变化的情况。

Conclusion: 所提出的广义Wasserstein距离在保持理论一致性的同时，提升了对拓扑特征的区分能力，兼具高效性与实用性，适用于大规模时变集合数据的分析，并已提供C++实现供复现。

Abstract: This paper presents a generalization of the Wasserstein distance for both
persistence diagrams and merge trees [20], [66] that takes advantage of the
regions of their topological features in the input domain. Specifically, we
redefine the comparison of topological features as a distance between the
values of their extrema-aligned regions. It results in a more discriminative
metric than the classical Wasserstein distance and generalizes it through an
input parameter adjusting the impact of the region properties in the distance.
We present two strategies to control both computation time and memory storage
of our method by respectively enabling the use of subsets of the regions in the
computation, and by compressing the regions' properties to obtain low-memory
representations. Extensive experiments on openly available ensemble data
demonstrate the efficiency of our method, with running times on the orders of
minutes on average. We show the utility of our contributions with two
applications. First, we use the assignments between topological features
provided by our method to track their evolution in time-varying ensembles and
propose the temporal persistence curves to facilitate the understanding of how
these features appear, disappear and change over time. Second, our method
allows to compute a distance matrix of an ensemble that can be used for
dimensionality reduction purposes and visually represent in 2D all its members,
we show that such distance matrices also allow to detect key phases in the
ensemble. Finally, we provide a C++ implementation that can be used to
reproduce our results.

</details>


### [584] [Filtering of Small Components for Isosurface Generation](https://arxiv.org/abs/2510.16684)
*Devin Zhao,Rephael Wenger*

Main category: cs.GR

TL;DR: 本文提出了一种针对CT或MRI扫描数据生成的等值面中微小成分的预滤波方法，以消除这些干扰可视化且不影响主要结构的小成分。


<details>
  <summary>Details</summary>
Motivation: 等值面在由扫描数据构建时常常包含大量微小成分，影响可视化效果并干扰几何建模，因此需要有效去除这些噪声成分。

Method: 通过对标量场在规则网格上的采样数据进行简单预滤波处理，去除等值面中的微小成分。

Result: 实验结果表明，该预滤波方法能有效消除小成分，同时保持大尺度结构不变，提升可视化质量。

Conclusion: 简单的预滤波技术可在不影响主要可视化结构的前提下，显著改善由扫描数据生成的等值面的清晰度和可用性。

Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface
is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some
$\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$.
Isosurfaces constructed from scanned data such as CT scans or MRIs often
contain extremely small components that distract from the visualization and do
not form part of any geometric model produced from the data. Simple
prefiltering of the data can remove such small components while having no
effect on the large components that form the body of the visualization. We
present experimental results on such filtering.

</details>


### [585] [A Scalable In Transit Solution for Comprehensive Exploration of Simulation Data](https://arxiv.org/abs/2510.16966)
*Paascal Grosset,James Ahrens*

Main category: cs.GR

TL;DR: 本文提出了SeerX，一种轻量级、可扩展的在传输中就地服务，支持动态资源分配和3D模拟数据的有损压缩，以解决因缺乏先验知识导致的就地可视化效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 由于超级计算机上模拟产生的数据量超过了可用磁盘空间，许多模拟采用就地分析和可视化来减少存储需求。然而，这种方法的效果受到需要先验知识的限制，包括合适的可视化参数选择以及运行就地工作流所需资源的预估。

Method: 提出了一种名为SeerX的轻量级、可扩展的在传输中就地服务，该服务支持动态资源分配和3D模拟数据的有损压缩，并允许多个模拟将分析任务卸载到共享的弹性服务基础设施上，无需MPI同步。

Result: SeerX能够有效支持动态资源分配和数据压缩，提高了就地分析和可视化的灵活性与效率，同时减少了对先验知识的需求。

Conclusion: SeerX提供了一个有效的解决方案，用于克服当前就地可视化技术中存在的资源管理和先验知识依赖的问题，有助于提高大规模科学模拟的数据处理能力。

Abstract: As simulations produce more data than available disk space on supercomputers,
many simulations are employing in situ analysis and visualization to reduce the
amount of data that needs to be stored. While in situ visualization offers
potential for substantial data reduction, its efficacy is hindered by the need
for a priori knowledge. First, we need to know what visualization parameters to
use to highlight features of interest. Second, we do not know ahead of time how
much resources will be needed to run the in situ workflows, e.g. how many
compute nodes will be needed for in situ work. In this work, we present SeerX,
a lightweight, scalable in-transit in situ service that supports dynamic
resource allocation and lossy compression of 3D simulation data. SeerX enables
multiple simulations to offload analysis to a shared, elastic service
infrastructure without MPI synchronization.

</details>


### [586] [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](https://arxiv.org/abs/2510.17101)
*Lu Yin,Ziying Shi,Yinghao Wu,Xinyu Yi,Feng Xu,Shihui Guo*

Main category: cs.GR

TL;DR: 提出了一种考虑身体形状差异的稀疏惯性传感器动作捕捉方法SAIP，通过分解与形状和姿态相关的传感器测量值来建模其联合相关性，并引入首个基于惯性传感器的身体形状估计方案。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏惯性动作捕捉方法依赖模板化成人身体形状，难以泛化到不同体型（如儿童）个体，因身体形状变化导致IMU加速度测量值变化，影响动作估计准确性。

Method: 将传感器测量中与形状和姿态相关的部分解耦；训练回归模型将真实身体的IMU加速度转换为模板身体的加速度以补偿形状差异；沿用SOTA方法估计模板身体动作；再用第二回归模型将关节速度映射回真实身体，并结合形状感知的物理优化计算全局运动；使用MLP网络建模条件于形状的IMU-姿态相关性，实现惯性身体形状估计。

Result: 提出了SAIP方法及首个包含不同体型个体的IMU动作捕捉数据集（10名儿童和10名成人，身高110-190cm，共400分钟配对数据）；实验表明SAIP能有效处理多样体型的动作捕捉任务。

Conclusion: SAIP是首个考虑身体形状差异的稀疏惯性动作捕捉框架，能够提升对非成人模板体型个体的动作估计精度，同时实现了基于惯性传感器的身体形状估计，推动了个性化动作捕捉的发展。

Abstract: Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [587] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: 提出VisuoAlign框架，通过提示引导的树搜索实现多模态安全对齐，有效提升大视觉语言模型对抗跨模态威胁的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在多模态对齐中存在安全漏洞，尤其是视觉输入带来的新型攻击面和推理链缺乏安全监督问题。

Method: 引入VisuoAlign框架，利用视觉-文本交互提示将安全约束嵌入推理过程，采用蒙特卡洛树搜索（MCTS）构建多样化的安全关键提示路径，并结合基于提示的扩展实现实时风险检测与合规响应。

Result: 实验表明，VisuoAlign能主动暴露风险，生成全面的安全数据集，并显著提升模型对复杂跨模态威胁的防御能力。

Conclusion: VisuoAlign为大视觉语言模型提供了一种有效的多模态安全对齐方案，增强了模型在融合模态下的安全性与鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [588] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了一个名为“结构化认知循环（SCL）”的可执行认识论框架，将智能定义为在判断、记忆、控制、行动与调节之间的动态过程，而非静态属性。SCL通过结构化架构实现可测试的认知哲学，并提升AI行为的连贯性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型缺乏真正的认知理解，仅依赖统计规律生成行为，存在认识论层面的结构性缺失。作者旨在构建一种能真正体现认知涌现条件的架构，弥补哲学与人工智能之间的鸿沟。

Method: 基于心灵哲学、认知现象学、过程哲学、生成认知和延展心智理论，提出SCL框架，将智能建模为一个包含判断、记忆、控制、行动与调节的闭环过程，并通过代理评估比较其与传统提示系统的性能差异。

Result: 实验证明，具有功能分离的认知架构比单体式提示系统表现出更连贯、可解释的行为；SCL实现了哲学思想的可计算化，支持‘可执行的认识论’；智能被重新定义为自我知识状态的重构能力。

Conclusion: 真正的智能进步不在于扩大模型规模，而在于构建体现认知原则的结构化架构；SCL为哲学、AI和认识论提供了跨学科框架，推动从‘拥有知识’向‘持续重建知识’的理解转变。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [589] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出了一项科学支持政策的议程，探讨“城市元宇宙”（citiverses）作为监管学习实验空间的潜力，基于对高级专家小组的咨询，确定了关键研究领域和可测试的实验主题，强调在伦理、经济、生态和社会维度上负责任地发展和应用citiverses。


<details>
  <summary>Details</summary>
Motivation: 随着数字技术的发展，政策制定需要更灵活、安全的实验环境。Citiverses 提供沉浸式虚拟空间，有助于模拟复杂政策场景，提升监管学习能力，但其潜力尚未系统探索。

Method: 通过与欧洲委员会政策制定者、国家政府科学顾问及数字监管与虚拟世界领域顶尖研究人员组成的高级专家组进行咨询，识别关键研究领域和实验主题，并结合现有实验生态系统（如试验床、生活实验室和监管沙盒）提出整合路径。

Result: 确定了 citiverses 用于监管学习的八大关键研究领域：可扩展性、实时反馈、复杂性建模、跨境协作、风险降低、公众参与、伦理考量和新兴技术融合；并提出了交通、城市规划和环境/气候危机等可测试领域。同时提出了将 citiverses 融入更大实验生态系统所需的初步步骤。

Conclusion: Citiverses 具有作为政策实验与监管学习平台的巨大潜力，但需采取负责任的发展路径，综合考虑伦理、社会、经济和生态影响，并通过跨学科合作和系统性研究推动其在政策制定中的有效整合。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [590] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出了一种受皮亚杰认知发展理论启发的新型AI代理记忆系统PISA，通过三模态适应机制和混合记忆访问架构，显著提升了适应性和长期知识保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，并忽视了记忆的建构性和任务导向性。

Method: 基于皮亚杰的认知发展理论，设计了三模态适应机制（模式更新、演化与创建）和结合符号推理与神经检索的混合记忆访问架构。

Result: 在LOCOMO和新提出的AggQA基准上验证了PISA的有效性，显著提高了记忆检索的准确性与效率，实现了最先进的性能。

Conclusion: PISA通过将记忆视为一种建构性和自适应过程，有效增强了AI代理在多样化任务中的持续学习与长期知识保留能力。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [591] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 大型推理模型在解决超过一定困惑度阈值的谜题时表现会下降，即使提供环境接口让模型逐步操作和观察状态变化，性能崩溃依然存在。


<details>
  <summary>Details</summary>
Motivation: 探究大型推理模型在复杂推理任务中性能下降的原因，特别是任务设置是否影响了对真实推理能力的评估。

Method: 为大语言模型提供汉诺塔问题的环境接口，使其可通过工具调用执行动作、给出理由、观察新状态并继续下一步，分析其策略与最优及随机策略的偏离情况。

Result: 接入环境接口并未缓解性能崩溃；模型策略随复杂度增加而偏离最优和随机策略，表现出模式化崩溃，性能取决于该模式是否对应正确解法。

Conclusion: 性能崩溃可能源于模型内部推理模式的固化，类似现象可能也存在于其他大型推理模型中。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [592] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出认知负荷轨迹（CLTs）作为深度模型的中层次可解释性框架，通过量化模型内部资源分配来揭示推理动态，并在实验中验证其预测错误、揭示认知策略及提升推理效率的能力。


<details>
  <summary>Details</summary>
Motivation: 受人类认知中的认知负荷理论启发，旨在为深度模型提供一种可解释的中层次框架，以更好地理解和优化模型的推理过程。

Method: 将CLTs定义为包含内在负荷、外在负荷和相关负荷的三成分随机过程，并通过注意力熵、KV缓存未命中率等可测量代理实例化，结合符号公式和可视化方法进行分析。

Result: 在推理与规划基准上的实验表明，CLTs能够预测错误发生时机，揭示模型的认知策略，并通过引导干预使推理效率提高15-30%且保持准确性。

Conclusion: CLTs为理解深度模型的内部推理机制提供了有效的可解释性工具，并具备实际应用潜力以优化模型性能。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [593] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: 本文提出了ProofFlow，一种以结构保真度为目标的自然语言定理和证明自动形式化新方法，通过构建有向无环图并采用引理驱动策略，显著提升了语义忠实性和逻辑结构保持能力，在新提出的基准和综合评估指标ProofScore上达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法虽能生成可执行代码，但常丢失原始证明的语义含义和逻辑结构，因此需要一种能更好保持推理结构一致性的方法。

Method: 提出ProofFlow流水线：首先构建有向无环图（DAG）表示证明步骤间的逻辑依赖关系，然后采用基于引理的方法逐个形式化每个步骤，从而保留原论证的逻辑结构。

Result: 在包含184个本科级别问题的新基准上，ProofFlow的ProofScore达0.545，显著优于整体形式化（0.123）和独立步骤形式化（0.072）等基线方法。

Conclusion: ProofFlow有效提升了数学证明自动形式化的结构保真度，所提出的基准和评估指标有助于推动该领域发展，相关资源已开源。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [594] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 本文提出了基于MO|RE数据构建知识图谱的愿景，旨在通过本体论方法标准化和机器可理解地建模与共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 为了在不同人群之间评估和比较身体与认知能力，需要对人类表现相关因素进行测试，而当前缺乏统一、可互操作的数据模型。

Method: 基于基础形式本体（Basic Formal Ontology）构建本体，形式化表示实验计划、具体过程及测量数据之间的关系，并利用MO|RE数据仓库开发知识图谱。

Result: 提出了一种将运动表现研究数据转化为标准化、机器可读格式的方法框架，支持跨研究的数据共享与集成。

Conclusion: 该方法有望改变运动表现数据的建模与共享方式，推动体育科学研究中的数据互通与数字化转型。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [595] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于非重叠的随机置换集（RPS）冲突度量方法，结合DST和RFS视角，利用受RBO启发的不一致性度量来有效衡量有序不确定信息间的冲突，并具备自然的顶部加权性及参数灵活性。


<details>
  <summary>Details</summary>
Motivation: 由于RPS作为处理包含顺序信息的不确定性推理的新框架，如何度量其证据间的冲突成为一个亟待解决的问题。

Method: 从随机有限集（RFS）和Dempster-Shafer理论（DST）两个角度分析RPS中的冲突，基于RBO思想定义排列间的不一致性度量，并提出一种非重叠式的RPS冲突度量方法。

Result: 所提方法具有自然的顶部加权性质，能有效衡量RPS之间的冲突，且允许决策者灵活选择权重、参数和截断深度；数值实例验证了该方法的行为与特性。

Conclusion: 该冲突度量方法在DST框架下扩展了RPS理论，能够合理刻画含顺序结构的不确定信息之间的冲突，具有良好的可解释性和应用潜力。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [596] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 本文提出了Ripple Effect Protocol (REP)，一种通过共享决策及其对环境变化的敏感性信号来增强AI代理间协调能力的新协议，相较于现有的通信机制，在多种场景下显著提升了协调的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理通信协议（如A2A、ACP）侧重于通信而非协调，导致在代理数量增加时出现集体行为脆弱的问题，即个体智能的代理可能产生低效的群体结果。

Method: 提出并形式化了REP协议，该协议允许代理不仅共享决策，还共享轻量级的敏感性信号，并通过局部网络传播这些信号以实现更快速和稳定的群体协调；协议设计分离了必需的消息格式与可选的聚合规则，并在不同激励机制和网络拓扑结构下进行评估。

Result: 在三个领域（啤酒游戏供应链、稀疏网络中的电影调度、Fishbanks资源分配）的基准测试中，REP相比A2A将协调准确性和效率提高了41%到100%，并能灵活处理来自大语言模型的多模态敏感性信号。

Conclusion: 通过将协调作为协议层面的能力，REP为不断发展的代理互联网提供了可扩展的基础设施，有效解决了大规模代理系统中协调能力不足的问题。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [597] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: 提出了PAINT方法，一种架构无关的神经孪生模型，能够在测试时利用测量数据保持与真实系统状态一致，实验证明其在二维湍流流体动力学问题中能高保真地预测系统状态。


<details>
  <summary>Details</summary>
Motivation: 为了创建能够实时响应并反映真实系统状态的数字孪生模型，需要解决传统自回归模型难以长期保持与真实系统状态一致的问题。

Method: 提出PAINT（Parallel-in-time Neural Twins）方法，通过在时间上并行建模状态分布，使用生成式神经网络，并在测试时采用滑动窗口方式从测量数据中预测状态。

Result: 理论分析表明PAINT具有保持在轨迹上的能力，而自回归模型通常不具备；实验结果显示PAINT在稀疏测量条件下仍能高保真预测系统状态。

Conclusion: PAINT方法能够有效实现神经孪生模型的持续状态对齐，具有在复杂动态系统中进行精确状态估计和决策支持的潜力。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [598] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出了一种名为ISGFAN的信息分离全局-局部对抗网络，用于在噪声环境下实现鲁棒的跨域故障诊断，通过信息分离架构和全局-局部域对抗机制有效解耦故障特征、抑制噪声并提升迁移性能，在三个公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法通常假设数据干净或域间相似性较高，难以应对工业环境中严重的噪声干扰和域偏移问题，限制了其实际应用效果。

Method: 提出ISGFAN框架，采用信息分离架构，结合对抗学习与改进的正交损失函数，解耦出域不变的故障特征，并分离噪声干扰和域特异性特征；引入全局-局部域对抗策略，通过全局域分类器对齐整体分布，局部焦点域对抗组件缓解无监督场景下噪声引起的类别级迁移障碍。

Result: 在三个公开基准数据集上的实验表明，ISGFAN在含噪跨域故障诊断任务中显著优于现有主流方法，验证了其在复杂工业环境下的有效性与鲁棒性。

Conclusion: ISGFAN通过信息分离和全局-局部对抗学习机制，有效解决了噪声干扰与域偏移共存下的跨域故障诊断难题，具有较强的迁移鲁棒性和应用前景。

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [599] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出一种基于双轨KG验证与推理框架DTKG，用于解决多跳问答中并行事实验证与链式推理的效率和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多跳推理时，单一使用LLM响应验证或KG路径构建，分别在链式或并行任务上表现不佳，导致效率和准确性下降。

Method: 受认知科学中的双过程理论启发，设计包含分类阶段和分支处理阶段的DTKG框架，分别处理并行事实验证和链式多跳推理。

Result: 该框架能够根据问题类型自适应选择合适的推理路径，提升多跳问答的整体性能。

Conclusion: DTKG通过融合两种推理机制，在多跳问答任务中实现了更高的准确性和效率。

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [600] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出一种基于代理模型的可解释人工智能工作流，用于加速复杂系统仿真并提高透明度，适用于工程设计和基于代理的社会环境模型分析。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统仿真中计算成本高和黑箱模型缺乏透明度的问题。

Method: 通过紧凑实验设计训练轻量级代理模型，结合全局与局部可解释AI方法，支持连续和分类输入，并集成不确定性量化与解释一致性评估。

Result: 在混合电推进飞机设计和城市隔离代理模型两个案例中验证，实现了秒级大规模探索，揭示了非线性交互与涌现行为，识别关键设计与政策杠杆，并指出代理模型需改进的区域。

Conclusion: 该工作流有效平衡了仿真精度与效率，提升了模型可解释性，为复杂系统分析提供了可靠且高效的解决方案。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [601] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 提出了一种结合离线约束规划与在线时序网络执行的混合方法，以应对制造系统中任务持续时间的不确定性，在保证零截止违规的同时仅增加3-5%的时间开销。


<details>
  <summary>Details</summary>
Motivation: 传统确定性调度在面对实际过程中的随机扰动时容易失效，导致昂贵的紧急修复，因此需要一种能在最坏不确定性下仍保持可行性的调度方法。

Method: 首先构建带每项任务截止时间的柔性作业车间的约束规划（CP）模型，并插入最优缓冲Δ*生成主动基线；然后将其转化为带不确定性的简单时序网络（STNU），验证动态可控性以确保实时调度可应对各种有界持续时间实现。

Result: 在Kacem 1-4基准上进行蒙特卡洛仿真显示，该方法消除了100%的截止违规，相比现有元启发式方法仅增加3-5%的完工时间开销；中等规模实例下的CP求解和STNU检查均在亚秒级完成。

Conclusion: 该工作表明时序网络推理能有效结合主动缓冲与动态鲁棒性，推动制造业向真正数字化、自校正的工厂迈进。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [602] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLM）生成的临床思维链（CoT）的可靠性，并提出通过“选择性少样本”提示策略提升质量，发现示例的质量和多样性比数量更重要，且人类专家在高风险临床AI评估中不可替代。


<details>
  <summary>Details</summary>
Motivation: 由于医疗数据稀缺，生成高质量、可解释的临床思维链（CoT）对医学AI至关重要，但当前LLM生成内容的临床可靠性尚未验证，亟需有效策略提升其可信度。

Method: 采用三种提示策略（零样本、随机少样本、选择性少样本）生成CoT，并由辅助生殖技术领域的资深临床医生进行盲评，同时与GPT-4o模型的评估结果对比，分析不同策略的效果差异。

Result: 选择性少样本策略在所有人工评估指标上显著优于其他方法（p < .001），而随机少样本与零样本无显著差异；成功归因于‘金标准深度’和‘代表性多样性’两大原则；AI评估者未能识别这些关键差异。

Conclusion: 合成临床思维链的可靠性取决于提示示例的战略性筛选，而非简单提供示例；提出‘双原则’框架作为可扩展生成可信医疗数据的基础方法，并强调人类专家在高风险AI评估中的核心作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [603] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出了一种结合学习启发式和搜索算法的混合框架LaGAT，用于解决密集多智能体路径规划（MAPF）问题，在密集场景下优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF规划器在实时寻找密集场景下的近似最优解方面仍面临挑战，且以往的学习引导搜索方法表现不佳。

Method: 将基于图注意力机制的神经网络策略MAGAT改进后生成的启发式信息，集成到领先的基于搜索的算法LaCAM中，并采用预训练-微调策略及死锁检测机制。

Result: LaGAT在密集场景下显著优于现有的纯搜索和纯学习方法，验证了精心设计的混合搜索在复杂多智能体协调问题中的有效性。

Conclusion: 混合式学习与搜索框架能有效提升密集MAPF问题的求解性能，展示了其在强耦合、高难度多智能体协作任务中的潜力。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [604] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出在生成式AI日益影响企业决策的背景下，基于扩展认知理论重新定义企业知识为一种动态能力，并构建了一个可量化的企业知识模型，通过信息访问效率和输出可靠性来衡量企业认知状态，进而将这些指标映射到实际知晓、推定知晓、故意无视和鲁莽等法律标准，为企业在算法时代的责任认定提供了可测量、可审计的路径。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的广泛应用，传统基于人类代理的企业主观意图（mens rea）归责模式受到挑战，亟需一种新的框架来重新定义和衡量企业在算法环境下的知识状态与责任基础。

Method: 基于扩展认知理论，构建一个形式化模型，引入连续的企业知识度量指标 $S_S(\varphi)$，结合信息处理流程的计算成本与统计验证的错误率，并据此定义知识谓词 $\mathsf{K}_S$ 与企业整体认知能力指数 $\mathcal{K}_{S,t}$，再将其操作性地映射到法律上的知识标准。

Result: 提出了可量化的企业知识度量体系，实现了对企业在AI辅助决策中认知状态的精确刻画，并建立了该体系与法律上实际知识、推定知识、故意无视和鲁莽等概念之间的对应关系，生成了可用于审计和司法认定的知识归因工具。

Conclusion: 该研究为在算法时代追踪和归责企业‘心智’提供了理论基础与实践工具，使企业知识变得可测量、可审计、可诉诸司法，从而增强了企业责任的透明性与问责性。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [605] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个基于大语言模型的多代理评估框架，用于自动评估和选择最佳的健康信息去标识化模型，减少对人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的PHI去标识化模型评估依赖昂贵且规模有限的人工标注，成本高且难以扩展。

Method: 提出TEAM-PHI框架，部署多个独立的评估代理，利用大语言模型判断PHI提取的正确性，并通过LLM驱动的多数投票机制整合结果，生成稳定、可复现的模型排名。

Result: 在真实临床文本数据集上的实验表明，TEAM-PHI能产生一致且准确的模型排名，其自动化排名与真实标签和人工评估结果高度一致。

Conclusion: TEAM-PHI提供了一种实用、安全且低成本的PHI去标识化模型自动评估与选择方案，尤其适用于标注数据有限的场景。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [606] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 本文提出了一种基于线性时序逻辑（LTL）的多样化规划器$\texttt{FBI}_\texttt{LTL}$，用于解决仿真环境中的多样化规划问题，能够生成语义上多样化的计划。


<details>
  <summary>Details</summary>
Motivation: 传统的单一计划生成方法难以满足智能体的偏好，现有多样化规划方法可能生成语法不同但语义相同的解，缺乏真正的多样性。

Method: 引入基于LTL的语义多样性标准，并将其融入搜索过程，设计了适用于仿真环境的多样化规划器$\texttt{FBI}_\texttt{LTL}$。

Result: 在多个基准测试上的实验表明，$\texttt{FBI}_\texttt{LTL}$相比基线方法能生成更多样化的计划。

Conclusion: $\texttt{FBI}_\texttt{LTL}$实现了语义引导的多样化规划，为复杂、非符号化领域中的实际应用提供了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [607] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 本文提出了“被记住的权利”（Right To Be Remembered, RTBR）概念，以应对大型语言模型（LLM）在信息整合过程中可能导致的偏见、遗漏和权力集中问题，强调减少AI驱动的信息遗漏风险，保障公平性和真实性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，其对信息的单一权威性呈现可能掩盖多元视角，导致弱势群体或数字存在感弱的个体被系统性忽略，从而重塑集体记忆，引发信息不公。

Method: 通过分析LLM与传统搜索引擎在信息呈现上的差异，探讨LLM如何压缩多元观点，并提出RTBR作为伦理与治理框架，以保障信息的公平呈现与记忆的多样性。

Result: 提出了RTBR概念框架，包含减少信息遗漏、确保公平对待和提升生成内容真实性三个核心要素。

Conclusion: 为防止LLM加剧社会不平等和集体失忆，需建立RTBR原则，约束技术设计与部署，促进多元声音的保留与传播。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [608] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 本文提出了ScholarEval，一种基于检索增强的评估框架，用于评估AI生成的研究想法的合理性和贡献度，并通过新构建的专家标注数据集ScholarIdeas验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究创意生成中的广泛应用，亟需一个可靠的评估框架来确保生成想法的有效性和实用性。

Method: 提出ScholarEval框架，结合检索增强技术，从合理性（soundness）和贡献度（contribution）两个维度评估研究想法；构建了包含117个跨领域研究想法的专家标注数据集ScholarIdeas进行评估。

Result: ScholarEval在覆盖人工标注评分点方面显著优于基线模型，且在可操作性、深度和证据支持方面优于OpenAI的o4-mini-deep-research系统；大规模用户研究表明其在文献参与、创意优化和实用性上表现更优。

Conclusion: ScholarEval能更有效、全面地评估研究创意，具备实际应用价值，代码、数据集和工具已公开发布供社区使用。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [609] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一种低延迟、基于解码器的单解码器重排序方法，通过池化首令牌评分和不确定性门控解释机制，在保证实时性的同时提升排序效果，尤其在临床决策等场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 需要一种低延迟、可解释且高效的重排序系统，以支持临床决策中的实时候选选项排序。

Method: 提出OG-Rank，采用单解码器架构，结合池化首令牌的评分机制与不确定性门控的解释生成：模型一次性评分所有候选，并仅在列表存在真正歧义时生成简短结构化理由；训练时采用聚焦难例的课程学习策略。

Result: OG-Rank在诊疗场景下的排序任务中表现出色（快速路径下Recall@1达0.45，nDCG@20为0.625），当门控激活时性能进一步提升（Recall@1达0.56，nDCG@20为0.699，门控触发率45%）；小型模型在此框架下也获得类似增益；相比编码器基线，其在效果和灵活性上均更优。

Conclusion: OG-Rank提供了一种实用的‘默认快速排序，必要时解释’的决策模式，具有良好的部署简便性和资源预算可控性，其‘对难题投入更多’的课程设计原则可广泛应用于其他需选择性生成的决策任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [610] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文提出使用多智能体影响图（MAIDs）作为图形框架，以解决大规模多智能体强化学习（MARL）中难以进行全局引导的问题。通过引入基于MAIDs的“目标干预”新交互范式，并结合因果推断技术（PSI），仅对单个目标智能体进行干预，从而实现复合期望结果。同时，利用MAIDs的相关性图分析为MARL范式设计提供可操作性判断工具。实验验证了所提方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体系统中，人类难以对整个系统提供全局指导，且现有协调机制缺乏系统化设计工具。因此需要一种可解释、易用的框架来分析和设计MARL中的交互范式。

Method: 采用多智能体影响图（MAIDs）作为建模框架，提出“目标干预”交互范式，并通过因果推断技术——预策略干预（PSI）实现该范式。利用MAIDs的因果结构最大化期望结果的因果效应，并通过相关性图分析评估学习范式的可行性。

Result: 实验证明了所提出的靶向干预方法能有效引导MARL系统达成复合目标，同时验证了相关性图分析在判断交互范式可行性方面的有效性。

Conclusion: MAIDs为MARL提供了一个可解释、结构化的分析与设计工具，目标干预范式降低了对全局指导的依赖，PSI方法通过因果干预实现了期望行为的引导，相关性图分析则为系统设计提供了理论支持。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [611] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文提出了一个名为“推理分心”的新漏洞，即大型推理模型在面对提示中恶意嵌入的复杂无关任务时会偏离主要目标，导致性能显著下降；作者通过系统分析验证了该问题的严重性，并提出了一种基于合成对抗数据的训练防御方法，有效提升了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在数学和编程等复杂任务上的广泛应用，其推理过程的安全性和可靠性成为关键问题。本文旨在揭示一种新型攻击方式——推理分心，探讨模型在面对恶意干扰时的脆弱性，并推动更安全推理系统的构建。

Method: 作者通过在不同模型和基准上进行综合实验，系统地评估了推理分心的影响；同时研究了对齐技术对该漏洞的放大效应，并提出结合监督微调（SFT）和强化学习（RL）的训练式防御方法，在合成的对抗数据上提升模型抗干扰能力。

Result: 实验表明，即使最先进的大型推理模型在受到干扰时准确率也最多下降60%；某些对齐技术会加剧这一问题，且模型可能隐秘执行恶意指令而不体现在最终输出中；所提出的防御方法在强干扰攻击下将鲁棒性提高了50多个点。

Conclusion: 推理分心是一种独特且紧迫的威胁，严重影响大型推理模型的可靠性；本研究不仅揭示了该问题的本质，还提供了有效的缓解策略，为构建更安全、可信的推理系统迈出了实际一步。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [612] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文研究了基于网页交互的智能体系统中的效率瓶颈，提出了一种结合推测执行的缓存框架SpecCache，显著降低了网页环境开销并提高了缓存命中率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的智能体系统多关注推理性能，忽视了系统效率问题，尤其是网页交互带来的延迟。

Method: 将端到端延迟分解为LLM API延迟和网页环境延迟，通过在15个模型和5个提供商上的实证研究分析延迟构成，并提出SpecCache缓存框架以减少网页环境开销。

Result: 实验表明，网页环境延迟可占整体延迟的53.7%；SpecCache相比随机缓存策略可将缓存命中率提高最多58倍，网页环境开销减少最多3.2倍，且不损害系统性能。

Conclusion: SpecCache有效缓解了Web交互式智能体系统的效率瓶颈，为提升此类系统的响应速度提供了可行方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [613] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG 是一个结合符号验证器的紧凑型知识图谱，用于在推理任务中强制执行数学可解释规则，显著提升大型语言模型在数学和逻辑约束下的准确性与一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成流畅推理步骤时常违反简单的数学或逻辑约束，需要一种机制来确保推理过程的数学正确性与一致性。

Method: 提出 MedRule-KG，一种包含实体、关系和领域启发式规则的紧凑类型化知识图谱，并结合符号验证器对模型预测进行检查和最小修正以保证一致性。

Result: 在90个样本的FDA衍生基准上，使用 MedRule-KG 接地使精确匹配率（EM）从0.767提升至0.900，加入验证器后达到1.000 EM，并完全消除规则违反。

Conclusion: MedRule-KG 提供了一个通用框架，支持安全、一致的数学推理，有效弥补了大型语言模型在形式化约束下的缺陷，具有良好的可复现性和应用前景。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [614] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了一种名为SELECT的动态锚点选择框架，用于解决文本到图像扩散模型中概念擦除时因固定锚点导致的概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点，常导致概念重现和概念侵蚀等问题，影响擦除效果。

Method: 通过因果追踪分析锚点选择的影响，提出‘兄弟互斥概念’作为更优锚点类别，并设计了两阶段评估机制的SELECT框架，自动发现最优擦除锚点并保护相关概念边界。

Result: SELECT在多个擦除框架中表现出良好的通用性，在关键性能指标上优于现有基线方法，单个概念的锚点挖掘平均仅需4秒。

Conclusion: SELECT作为一种通用的动态锚点解决方案，有效提升了概念擦除的精度与稳定性，同时保持了相关概念的完整性。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [615] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 本文研究了具有不一致偏好的用户如何通过策略性互动使算法与其真实兴趣对齐，提出一个双系统决策模型，并基于多领导者单跟随者的Stackelberg博弈分析对齐的临界视野，发现即使微小的额外信号也能显著降低对齐负担。


<details>
  <summary>Details</summary>
Motivation: 用户在与算法交互时常表现出冲动与理性决策的不一致，导致算法难以准确捕捉其长期兴趣，本文旨在探究此类用户实现算法对齐所需条件及其挑战。

Method: 将用户决策建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长），构建多领导者单跟随者的扩展式Stackelberg博弈，系统2作为领导者承诺参与策略，算法作为跟随者做出最优响应，定义对齐负担为有效引导算法所需的最小视野，并分析临界视野的存在性及影响因素。

Result: 存在一个临界视野：足够有远见的用户可实现与自身利益的对齐，而短视用户则会被对齐到算法的目标；该临界视野可能很长，构成重大负担；但即使微小且有成本的信号（如额外点击）也能显著缩短这一视野。

Conclusion: 尽管具有不一致偏好的用户面临较大的对齐负担，但在Stackelberg均衡框架下仍有可能通过提升决策视野或引入低成本信号机制来实现算法与其真实兴趣的对齐。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [616] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 本文提出了一种基于Flavell认知监控模型的三阶段迭代系统（Monitor-Generate-Verify），融合了现有LLM推理增强方法的优点，通过前置监控生成更高质量的初始解，减少后续修正需求，在GSM8K上取得了优于SELF-REFINE和Self-Verification的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理增强方法分为Monitor-Generate和Generate-Verify两类，前者缺乏对策略成功与否的验证机制，后者在无任务评估的情况下盲目生成，导致效率低下。因此需要一种整合监控、生成与验证的框架来弥补这一缺陷。

Method: 基于Flavell的认知监控模型（1979）和更广泛的Monitor-Generate-Verify框架（Oh和Gobet，2025），构建一个包含监控、生成与验证三个阶段的迭代式系统，实现策略规划、执行与反馈的闭环。

Result: 在GSM8K数据集上的初步结果显示，该方法准确率达到75.42%，高于SELF-REFINE（68.44%）和Self-Verification（67.07%），且平均尝试次数更少（1.3 vs 2.0），尽管推理成本增加了27-37%。

Conclusion: 前置监控能够产生更高质量的初始解决方案，从而减少对迭代优化的依赖，提升整体推理效率；但需在更多非算术类任务上进一步验证其通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [617] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 本文提出了受人类智能启发的人形结构因果模型（HSCM），通过模拟人类视觉系统的分层处理和多层次学习，解决传统领域泛化模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化模型依赖统计方法捕捉数据与标签之间的关系，难以建模细粒度因果机制，导致在动态复杂环境中泛化能力不足。

Method: HSCM模仿人类视觉系统的层级结构，解耦并重新加权图像的关键属性（如颜色、纹理和形状），利用结构因果模型捕捉因果关系，提升跨域泛化能力。

Result: 理论与实验结果表明，HSCM在多个领域上优于现有的领域泛化方法，具有更强的鲁棒性和可解释性。

Conclusion: HSCM提供了一种更原则性的因果建模范式，有效提升了模型在复杂动态环境中的泛化性能和适应能力。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [618] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 提出了一种受物理中重正化群思想启发的自演化记忆框架RGMem，用于在大语言模型时代实现语言代理的长期记忆与行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话的用户长期状态和行为一致性，现有方法多集中于事实级存储，缺乏从多轮对话中提炼潜在偏好和深层特质的能力。

Method: 提出RGMem框架，通过多层次的粗粒化和重缩放操作，将对话历史组织成多尺度结构，从片段中提取语义和用户洞察，逐步构建动态演化的用户画像。

Result: 该框架能有效从噪声较多的微观交互中压缩信息并涌现出高层次、准确的用户画像，提升长期用户建模能力。

Conclusion: RGMem通过模拟记忆的多尺度演化过程，实现了语言代理在跨会话场景下的长期记忆与行为一致性，增强了个性化交互的深度和连续性。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [619] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: 本文提出了ReviewSense，一个利用大语言模型将客户评论转化为可操作商业建议的新型决策支持框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统多关注预测用户偏好，缺乏将客户反馈转化为面向企业的处方性建议的研究。

Method: 结合聚类、大语言模型适配和专家评估，构建统一的面向企业分析流程。

Result: 初步人工评估显示模型建议与商业目标高度一致。

Conclusion: 该框架能有效提升企业从客户反馈中获取战略洞察的能力，推动数据驱动决策。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [620] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出了NP-ENGINE，首个用于训练和评估大语言模型在NP难问题上表现的综合框架，并构建了相应的基准测试NP-BENCH；通过该框架训练的QWEN2.5-7B-NP模型在多个任务上超越GPT-4o并实现同规模SOTA，且展现出强大的跨领域泛化能力，表明任务丰富的强化学习与可验证奖励训练有助于提升大模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在复杂优化问题（尤其是NP难问题）上的推理能力尚不充分，现有方法缺乏系统性训练与评估框架，因此需要构建一个可扩展、可验证的框架来填补这一空白。

Method: 提出NP-ENGINE框架，包含10个跨越五个领域的NP难任务，每个任务配备可控实例生成器、基于规则的验证器和提供近似最优解的启发式求解器；构建NP-BENCH基准测试；采用零奖励强化学习（zero-RLVR）结合课程学习在Qwen2.5-7B-Instruct基础上训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH上显著优于GPT-4o，达到同规模模型的SOTA水平；在域外推理任务（逻辑、谜题、数学、知识）及非推理任务（指令遵循）上均表现出强泛化能力；发现任务多样性增加有助于提升域外泛化性能。

Conclusion: 任务丰富且具备可验证奖励的强化学习训练是提升大语言模型推理能力的有效路径，揭示了RLVR的扩展规律，为未来模型训练提供了新方向。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [621] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: 本文提出了一种名为Doug的类型化编程语言，该语言基于轻量级线性函数式编程语言（LLFPL），采用向量符号架构（VSA）编码，能够在多项式时间内保证程序终止。其类型系统基于全息声明性记忆（HDM）的槽值编码，支持神经网络学习类型，并将技能获取建模为程序合成，旨在实现类人速度的技能学习，提升学习效率并更贴近人类大脑中的心理表征及其习得过程。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一种能够模拟人类技能获取过程的编程语言，使程序既能保证计算复杂性（多项式时间停机），又能与神经网络结合，实现高效、可学习的技能表示，从而更真实地模拟人类心智表征及其学习机制。

Method: Doug是LLFPL语言的一种编码实现，使用基于HDM的槽值编码表示类型，使用Flanagan定义的Lisp VSA变体编码项，并将类型嵌入神经网络的向量空间中，使得相近的嵌入点具有相似的类型结构和内容，从而支持类型的学习与泛化。

Result: 实现了可在多项式时间内停机的类型化语言Doug，其类型可被神经网络学习，并支持将技能获取视为程序合成的过程，为高效、类人的技能学习提供了新的建模范式。

Conclusion: Doug为连接形式化编程语言与神经网络学习提供了桥梁，使得类型和程序行为均可学习，推动了对人类心智表征及其习得过程的建模，朝向更高效、更人性化的技能获取方法迈进了一步。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [622] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: Urban-R1 是一种基于强化学习的后训练框架，通过组相对策略优化（GRPO）和城市区域画像代理任务，有效减轻地理偏差并提升多模态大语言模型在城市智能中的跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有城市基础模型在监督微调下存在地理偏差，导致预测结果区域倾斜且泛化能力有限，难以支持公平可靠的 Urban General Intelligence（UGI）。

Method: 提出 Urban-R1 框架，采用组相对策略优化（GRPO）对不同地理群体进行推理优化，并设计城市区域画像作为代理任务，利用多模态城市数据提供可衡量的奖励信号。

Result: 在多个地区和任务上的实验表明，Urban-R1 显著降低地理偏差，提升跨区域泛化性能，优于监督微调模型和闭源模型。

Conclusion: 强化学习对齐是实现公平、可信城市智能的有效途径，Urban-R1 为构建去偏、通用的 UGI 系统提供了新方向。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [623] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: 本文提出了BuildArena，首个面向语言驱动工程建造的物理对齐交互式基准，用于评估大模型在复杂物理约束下的建造能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在工程建造自动化中的能力尚未得到充分评估，缺乏一个结合物理规律和语言理解的综合评测基准。

Method: 设计了一个可定制的评测框架，包含多难度层级的静态与动态力学任务、3D空间几何计算库，并提出基于LLM的代理工作流作为基线方法。

Result: 在八个前沿大模型上进行了全面评估，验证了模型在语言驱动和物理对齐建造任务中的表现差异。

Conclusion: BuildArena为语言驱动的建造自动化提供了有效的评测平台，揭示了当前大模型在物理推理和结构设计方面的优势与不足。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [624] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 提出GraphFlow框架，通过基于转换的流匹配目标联合优化检索策略和流估计器，以从文本丰富的知识图谱中高效检索准确且多样的知识。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法难以为复杂现实查询检索准确且多样的信息，且过程奖励模型依赖昂贵且难以获取的监督信号。

Method: 采用过渡式流匹配目标，将检索结果的奖励分解到中间状态，指导检索策略按奖励比例从知识图谱中检索候选信息。

Result: 在STaRK基准上，GraphFlow平均在命中率和召回率上比GPT-4o等强基线高出10%，并展现出对未见知识图谱的良好泛化能力。

Conclusion: GraphFlow有效提升了基于知识图谱的检索增强生成在真实场景查询中的准确性、多样性与鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [625] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种半监督的置信度分布学习方法（ssCDL），通过将置信度转化为分布并利用元学习生成伪标签，有效解决不确定知识图谱中置信度分布不平衡导致的补全问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了三元组置信度极度不平衡的分布，导致学习到的嵌入不足以支持高质量的不确定知识图谱补全。

Method: 提出ssCDL方法，将每个三元组置信度转换为置信度分布，并结合标记数据和基于伪标签的未标记数据进行迭代学习；利用元学习预测伪标签以增强训练数据并重平衡置信度分布。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在多种评估指标上均优于现有的最先进基线方法。

Conclusion: ssCDL通过引入置信度分布和半监督学习机制，有效提升了不确定知识图谱的补全性能，尤其在处理置信度分布不平衡问题上表现出色。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [626] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 本文提出了一种名为MERCI的新型强化学习算法，通过基于计数的内在奖励机制来增强大语言模型在多步推理中的探索能力，显著提升了复杂推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法依赖稀疏的结果奖励和有限的探索，容易导致大语言模型陷入重复且次优的推理模式，因此需要设计更有效的探索机制。

Method: MERCI利用轻量级的抛硬币网络（CFN）估计推理路径的伪计数和认知不确定性，并将其转化为内在奖励，结合任务奖励以促进新颖且有效的推理路径探索，并集成到如GRPO等先进RL框架中。

Result: 实验表明，MERCI能够生成更丰富多样的思维链，在多个复杂推理基准上显著优于强基线方法，并帮助策略跳出局部常规，发现更优解。

Conclusion: MERCI通过有针对性的内在动机使探索在语言模型推理中变得更加可靠，为提升LLM的推理能力提供了有效途径。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [627] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文探讨了大规模AI模型在神经科学五大领域中的变革性影响，包括神经影像、脑机接口、分子神经科学、临床辅助和疾病应用，并强调了AI与神经科学的双向互动。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法在处理复杂神经数据方面存在局限，而大规模AI模型能够实现从原始脑信号到多模态数据整合的端到端学习，推动神经科学研究范式转变。

Method: 综述大规模AI模型在五个主要神经科学领域的应用，分析其在解决多模态数据融合、时空模式识别和临床转化框架构建中的作用，并总结关键数据集与生物约束对模型设计的影响。

Result: 大规模AI模型显著提升了神经数据处理能力，促进了跨领域数据整合与临床转化，同时通过引入生物学约束增强了模型可解释性与效率。

Conclusion: 大规模AI模型正在深刻改变神经科学研究，未来需关注评估体系、领域知识融合及临床应用的伦理规范。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [628] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型的自主框架AFL，用于全自动解决复杂的车辆路径问题，通过多智能体协作实现高可靠性和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在解决复杂VRP时依赖外部干预，导致自主性低、错误多、解不可行。

Method: 设计一个包含四个专用智能体的代理框架（AFL），将任务分解为三个子任务，直接从原始输入提取知识并自生成代码，无需人工模块或外部求解器。

Result: 在60个复杂VRP实例上实验表明，AFL性能接近精心设计的算法，显著优于现有LLM基线，代码可靠性与解可行性接近100%。

Conclusion: AFL实现了从问题输入到求解的全自动化，具备高通用性、可信度和实际应用潜力。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [629] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文综述了从基于流程的AI系统向模型原生范式的转变，强调通过强化学习将规划、工具使用和记忆能力内化到大语言模型中，推动代理型AI的发展。


<details>
  <summary>Details</summary>
Motivation: 随着大模型从被动响应转向主动行为与适应，传统外部控制的模块化架构已难以满足复杂任务需求，亟需一种更统一、自洽的智能体构建范式。

Method: 以强化学习为核心算法引擎，系统梳理了规划、工具使用和记忆三大能力从外部脚本化模块向端到端学习行为的演化路径，并分析其在深度研究代理和GUI代理等应用中的体现。

Result: 明确了模型原生范式下代理能力的内化进程，揭示了多智能体协作与反思等高级能力的持续内化趋势，以及系统层与模型层角色的演变。

Conclusion: 提出模型原生代理AI作为集成学习与交互框架的未来方向，标志着从‘应用智能’的系统构建转向‘通过经验成长智能’的模型培养。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [630] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 本论文综述了基于强化学习的代理搜索（RL-based agentic search）这一新兴领域，系统地从功能角色、优化策略和应用范围三个维度进行组织，总结了代表性方法、评估协议和应用场景，并讨论了构建可靠、可扩展系统的开放挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）存在知识静态、事实幻觉和无法获取实时或特定领域信息的问题，传统检索增强生成（RAG）方法缺乏对检索和推理的自适应控制，因此需要更智能、动态的搜索机制。

Method: 提出并系统梳理基于强化学习的代理搜索框架，从功能角色、优化策略和优化范围三个维度对现有研究进行分类和分析。

Result: 提供了该领域的首个全面综述，总结了典型方法、评估方式和应用案例，明确了当前的技术进展与局限。

Conclusion: 强化学习为实现自适应、自我改进的代理搜索提供了有力机制，未来有望推动构建更可靠、可扩展的检索增强系统，促进LLM在复杂任务中的实际应用。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [631] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种高效的轻量级多模态大语言模型（ELMM），用于多模态知识图谱补全（MKGC），通过视觉标记压缩和注意力剪枝显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有MKGs存在不完整问题，且多模态大语言模型在MKGC中的应用面临语义噪声、模态冲突和高计算成本的挑战。

Method: 提出ELMM模型，包含基于多头注意力的多视图视觉标记压缩器（MVTC）以融合并压缩图像标记，并设计注意力剪枝策略与线性投影来降低计算开销并保持性能。

Result: 在FB15k-237-IMG和WN18-IMG数据集上，ELMM在性能上达到SOTA，同时显著降低了推理成本。

Conclusion: ELMM为多模态知识图谱补全提供了一个高效且有效的解决方案，兼顾了精度与计算效率。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [632] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能同时感知和生成视觉、文本、语音和动作，实现更自然的人类交互行为。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态且全双工的，现有模型难以模拟这种自然交互，因此需要一个能同时处理多种模态并支持实时响应的统一架构。

Method: 提出ELLSA模型，采用新型SA-MoE（自注意力混合专家）架构，将不同模态路由到专用专家，并通过统一注意力主干进行融合，实现多模态感知与并发生成。

Result: 在语音交互和机器人操作基准上，ELLSA达到特定模态基线水平，并支持对话与动作轮转、拒识错误指令、边说边动、基于上下文的视觉问答和动作插话等高级功能。

Conclusion: ELLSA推动了更自然、通用的交互智能发展，为实现人工通用智能提供了新方向。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [633] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: 提出GraphVista框架，通过层次化组织图信息和模态协调机制，提升视觉-语言模型在大规模图理解中的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在图理解中受限于输入token长度，缺乏有效的文本与视觉模态协调机制，难以扩展到大规模图。

Method: 构建轻量级GraphRAG基础模块进行任务相关文本和高分辨率子图检索，并引入规划代理动态路由任务至最适合的模态（文本用于简单属性推理，视觉用于复杂结构推理）。

Result: 实验表明GraphVista可处理比现有基准大200倍的图，在多种任务上显著优于现有方法，性能最高提升4.4倍。

Conclusion: GraphVista通过增强可扩展性和模态协调能力，有效提升了视觉-语言模型在大规模图理解中的表现。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [634] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出领域上下文概念图（CDC），通过将领域作为一等元素提升概念表示，克服传统知识图谱的固定本体限制。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定的本体结构，难以表达跨领域和上下文相关的概念关系。

Method: 提出C-D-C三元组结构（<概念, 关系@领域, 概念'>），将领域作为动态分类维度，并基于认知-语言同构映射原则形式化多种关系谓词，在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和技术文档中的案例研究表明，CDC支持上下文感知推理、跨域类比和个性化知识建模。

Conclusion: CDC框架突破了传统本体的局限，为知识图谱提供了更灵活、贴近人类认知的建模方式。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [635] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: 本文提出了DeepAnalyze-8B，首个面向自主数据科学的代理式大语言模型，能够自动完成从原始数据到深度研究报告的端到端流程。


<details>
  <summary>Details</summary>
Motivation: 现有基于工作流的数据代理受限于预定义流程，难以实现真正的自主数据科学，因此需要一种能灵活应对复杂任务的新型模型。

Method: 提出了一种基于课程学习的代理训练范式和数据接地的轨迹合成框架，模拟人类数据科学家的学习路径，逐步提升模型在真实环境中的多能力整合。

Result: 实验表明，仅使用8B参数的DeepAnalyze在多项数据任务上优于基于先进专有大模型的 workflow-based 代理系统。

Conclusion: DeepAnalyze实现了从数据到深度分析报告的全自动化，推动了自主数据科学的发展，并已开源模型、代码与训练数据。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [636] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 本文提出通过显式视觉状态推理来增强视觉-语言模型（VLM）代理的世界建模能力，引入状态估计与状态转移建模，并设计了世界建模奖励和双层GAE方法，在多个基准上显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: VLM代理面临从文本状态到复杂视觉观测的转变，导致部分可观测性问题，需要强大的世界建模能力。本文探究VLM代理是否能通过显式视觉状态推理构建内部世界模型。

Method: 将代理的推理过程分解为状态估计和状态转移建模，采用强化学习框架，形式化为POMDP，并提出世界建模奖励和双层GAE进行训练监督与信用分配。

Result: 3B参数模型在五个基准上得分为0.82，相比未训练版本提升3倍，并优于GPT-5、Gemini 2.5 Pro和Claude 4.5等专有模型。

Conclusion: 显式视觉状态推理对VLM代理的世界建模至关重要，任务依赖的内部表示（自然语言或结构化格式）可有效提升性能。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [637] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出了一种新的评估方法，用于测试用户能否从强化学习智能体的决策解释中识别其目标，在Ms. Pacman环境中评估四种XRL算法，发现仅有一种算法优于随机猜测，且用户自信度与实际准确率不相关。


<details>
  <summary>Details</summary>
Motivation: 缺乏对可解释强化学习（XRL）算法在调试任务中相对性能的系统性比较评估。

Method: 在Atari的Ms. Pacman环境中，使用四种XRL算法生成解释，并通过用户研究测试用户是否能根据解释识别智能体的目标，同时收集用户的自信度和理解难易度反馈。

Result: 只有一种XRL算法的表现优于随机水平；用户普遍存在过度自信现象；用户自报的理解难易度和信心与其实际识别准确率无显著相关性。

Conclusion: 当前多数XRL算法在帮助用户识别智能体目标方面效果有限，且用户体验反馈不能可靠反映解释质量，需改进评估标准与算法设计。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [638] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体协作的LLM代理框架，用于系统化探索GPU内核优化的设计空间，在KernelBench上显著优于基线方法，生成正确解并实现最高16倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化因硬件复杂性和人工成本高而困难，现有LLM方法多为单次生成或简单迭代，难以有效应对复杂的优化空间。

Method: 设计了一个LLM代理框架，结合多智能体协作、基于上下文的指令生成、动态上下文管理和策略性搜索，模拟专家工程师的优化流程，利用性能分析反馈进行迭代优化。

Result: 在KernelBench基准测试中，相比基线代理，该方法能生成更多正确解，并实现最高达16倍的运行时性能提升。

Conclusion: 基于代理的LLM框架在实现全自动、可扩展的GPU内核优化方面具有巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [639] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进大语言模型在多轮工具增强对话中的行为，通过检测八种特定的工具调用错误并提供针对性反馈，显著提升了工具调用准确率。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大语言模型在实际应用中广泛使用，但工具使用错误影响了其可靠性，因此需要一个系统性的诊断和改进框架。

Method: 提出ToolCritic框架，定义八种工具调用错误类型，构建合成数据集训练ToolCritic，并在多轮对话中通过反馈机制引导主LLM修正响应。

Result: 在Schema-Guided Dialogue (SGD) 数据集上的实验表明，ToolCritic相比基线方法（如零样本提示和自我纠正技术）工具调用准确率最高提升13%。

Conclusion: ToolCritic为提升大语言模型在现实对话应用中与外部工具集成的鲁棒性提供了有效途径。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [640] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID 是一种结合自由文本描述与本体标签的多智能体AI系统，利用检索增强生成技术提升单细胞基因集注释的准确性与可解释性，成功应用于小鼠脑细胞图谱，实现了77%的准确率，并揭示了脑区特异性基因共表达模式和功能。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序虽能识别细胞类型，但对缺乏充分注释基因的转录特征进行功能注释仍具挑战，传统方法依赖高质量注释数据库，在低质量注释场景下表现不佳。

Method: 提出BRAINCELL-AID，一个融合大语言模型与检索增强生成（RAG）的多智能体AI系统，整合自由文本与结构化本体标签，通过引用PubMed文献动态优化注释结果，减少幻觉并提高可解释性。

Result: 在小鼠基因集注释任务中，77%的基因集在其顶部预测中获得正确注释；成功注释了BRAIN Initiative提供的5,322个脑细胞簇，识别出基底节相关细胞类型及区域特异性基因共表达模式。

Conclusion: BRAINCELL-AID显著提升了基因集合的功能注释能力，尤其适用于注释不完善的基因，为社区驱动的细胞类型注释提供了有力工具和宝贵资源。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [641] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 本研究开发了两种基于大语言模型的系统（NAS和KPD-MADS），用于从非财务证据生成结构化推理，以提升企业信用评估中的自动化决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有金融AI方法主要关注数值预测，难以处理定性非财务指标的解释性判断，导致信贷评估中证据驱动的推理自动化不足。

Method: 构建两个基于大语言模型的系统：一是单智能体非对抗系统（NAS），通过单次推理流程生成双向分析；二是基于辩论的多智能体系统（KPD-MADS），采用十步结构化交互协议，基于卡尔·波普尔的批判性对话框架实现对抗性验证。

Result: 在三个真实企业案例中，两个系统相比人工报告显著提高效率（NAS：11.55秒/案例；KPD-MADS：91.97秒；人工基线：1920秒）。KPD-MADS在解释充分性（4.0 vs 3.0）、实用性（4.0 vs 3.0）和可用性评分（62.5 vs 52.5）上均优于人工分析。

Conclusion: 结构化多智能体交互能有效增强金融AI中的推理严谨性和可解释性，推动企业信用评估的可扩展且可辩护的自动化发展。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [642] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出一种基于手工特征的鱼眼图像融合方法，用于自动化评估鱼类新鲜度，显著优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统感官评价主观性强、一致性差，难以标准化，且受物种依赖性腐败线索影响，无法满足食品行业对鱼类新鲜度准确评估的需求。

Method: 从鱼眼图像中系统提取并逐步融合颜色统计量、多色彩空间直方图以及局部二值模式（LBP）和灰度共生矩阵（GLCM）等纹理特征，结合全局色度变化与局部区域退化信息，使用LightGBM和ANN模型进行分类。

Result: 在FFE数据集上，LightGBM达到77.56%准确率，比先前深度学习基线提升14.35%；使用增强数据的ANN准确率达97.16%，超过此前最佳结果19.86%。

Conclusion: 精心设计的手工特征在策略性处理后，能提供鲁棒、可解释且可靠的鱼类新鲜度自动评估方案，适用于食品质量监测的实际应用。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [643] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 提出PILLM框架，结合大语言模型与物理规律，在进化循环中自动生成、评估和优化暖通空调系统异常检测规则，实现高可解释性、适应性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在可解释性、适应性、透明度和物理一致性方面存在不足，尤其是当前大语言模型方法忽略了暖通空调系统的物理原理。

Method: 提出PILLM，一种物理信息增强的大语言模型框架，引入基于热力学和控制理论的反射与交叉算子，在进化循环中自动优化异常检测规则。

Result: 在公开建筑故障检测数据集上达到最先进性能，生成的诊断规则具有高可解释性和可操作性。

Conclusion: PILLM实现了兼具适应性、可解释性与物理合理性的异常检测，推动了可信、可部署的智能建筑AI系统的发展。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [644] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文提出了ProtocolBench，一个用于系统评估多智能体系统通信协议的基准，并引入了可学习的ProtocolRouter以动态选择最优协议，显著提升了系统性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模多智能体系统的发展，通信协议的选择对系统性能和可靠性影响重大，但目前缺乏标准化评估手段，选择常依赖直觉。

Method: 设计了ProtocolBench基准，从任务成功率、端到端延迟、消息开销和容错能力四个方面评估协议；提出ProtocolRouter，基于需求和运行时信号动态选择协议。

Result: 在Streaming Queue场景中，不同协议的整体完成时间差异达36.5%，平均端到端延迟相差3.48秒；在Fail-Storm Recovery中，ProtocolRouter相比最佳单协议基线恢复时间减少18.1%，并在GAIA等场景中提升任务成功率。

Conclusion: 通信协议选择显著影响多智能体系统性能，ProtocolRouter通过动态路由有效提升系统效率与可靠性，ProtocolBench为未来协议评估提供了标准化工具。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [645] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 提出一种结合ECG基础模型和XGBoost的混合框架，用于提高心梗后恶性心律失常的预测准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统风险评分预测性能有限，而端到端深度学习模型缺乏临床可信所需的可解释性。

Method: 利用大规模ECG基础模型ECGFounder提取150维诊断概率特征，经特征选择后输入XGBoost分类器，并使用SHAP方法进行解释。

Result: 混合模型AUC达0.801，优于KNN、RNN和1D-CNN；SHAP分析显示关键特征与临床知识一致。

Conclusion: 该混合框架为可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [646] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了一种基于网络部署的、工具增强的LLM健康教练系统，通过离线策略评估和轻量级模拟器发现，均匀使用工具的策略虽提升整体表现但损害特定用户群体，而引入早期信息增益奖励可加速特征识别并提高目标达成率。


<details>
  <summary>Details</summary>
Motivation: 为了实现对不同用户群体更有效的个性化健康指导，避免传统平均指标掩盖子群体受到的负面影响。

Method: 采用离线策略评估（OPE）结合分解的决策头（工具/风格），并在轻量级模拟器中引入隐藏的用户原型和早期信息增益奖励，以优化策略学习与评估。

Result: 发现统一高频使用工具的策略会损害低健康素养但高自我效能用户的体验；加入小规模早期信息增益奖励能有效缩短用户特质识别时间，并提升目标成功率和pass@3指标。

Conclusion: 提出“评估优先”的个性化路径：冻结生成器，基于分类型奖励学习子群体感知的决策头，并始终报告按原型划分的指标以揭示潜在的子群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [647] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了一种名为TD-HNODE的新方法，用于基于电子健康记录对疾病进展（如2型糖尿病）进行建模，通过时序超图神经微分方程捕捉连续时间动态和患者异质性，在真实临床数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以适应真实世界数据或无法准确捕捉不规则采样下的连续时间疾病进展动态，且难以处理患者异质性（如不同的进展速率和路径）。

Method: 提出TD-HNODE模型，将疾病进展表示为时序详细的超图，利用神经ODE框架学习连续时间动态，并设计可学习的TD-超图拉普拉斯算子，捕捉疾病并发症标志物在单个及多个进展轨迹内的相互依赖关系。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病及相关心血管疾病进展方面优于多种基线方法。

Conclusion: TD-HNODE能有效建模复杂、不规则采样的疾病进展轨迹，兼顾患者异质性和连续时间动态，提升了疾病进展预测的准确性。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [648] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体聊天机器人Coinvisor，用于提升加密货币投资决策的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有加密货币投资分析方法存在耗时、功能有限或缺乏实时数据整合与多步推理能力的问题。

Method: 构建一个基于强化学习的工具选择机制，通过多智能体框架集成多种分析工具，实现多步规划和动态数据的灵活整合。

Result: 在自动化评测中，相比基础模型，工具调用的召回率提升40.7%，F1分数提升26.6%；用户研究显示用户满意度达4.64/5，并更倾向于使用Coinvisor而非通用大模型或现有平台。

Conclusion: Coinvisor通过强化学习驱动的工具调度和实时交互能力，显著提升了加密货币投资分析的准确性与实用性。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [649] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 本文提出了一种名为RubiSCoT的AI支持框架，利用自然语言处理技术提升学术论文从开题到最终提交的评估效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且评估者间存在差异，需要更高效、一致的评估工具。

Method: 采用大语言模型、检索增强生成和结构化思维链提示等先进的自然语言处理技术，实现初步评估、多维度评估、内容提取、基于评分标准的打分及详细报告生成。

Result: RubiSCoT框架能够提供一致、可扩展和透明的论文评估解决方案，优化学术评估流程。

Conclusion: RubiSCoT展示了人工智能在提高学术评估效率和质量方面的潜力，具有广泛的应用前景。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [650] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 提出一种基于主动推理的路径规划方法，用于智能体自主控制以侦察地理区域并维护共同操作视图。


<details>
  <summary>Details</summary>
Motivation: 解决在未知环境中平衡探索与利用的问题，实现对目标区域的有效侦察和监控。

Method: 构建反映当前态势理解的证据地图，结合正负传感器观测，并使用Dempster-Shafer理论和高斯传感器模型的生成模型，通过贝叶斯方法更新后验概率分布，计算各位置的变分自由能，引导智能体向自由能最小的位置移动。

Result: 仿真结果表明，该方法能有效平衡大范围搜索与已知目标跟踪，提升智能体在复杂环境中的自主决策能力。

Conclusion: 所提出的主动推理路径规划方法能够有效支持智能体在动态环境中的持续侦察任务，具备良好的应用前景。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [651] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 本文探讨了在法律领域机器学习应用中的标签不确定性问题，指出由于法律结果常受未被捕捉的人为干预影响，导致标签不确定，进而影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 法律领域的机器学习通常将过去的案件结果视为真实标签，但这些结果可能受到 settlements、appeals 等人为干预的影响，造成标签的不确定性，这在现有研究中未被充分考虑。

Method: 以欧洲人权法院的案例分类为例，研究不同标签构建方式对模型训练和行为的影响，并分析现有标签填补方法的假设基础。

Result: 实验表明，训练过程中标签的构建方式显著影响模型的行为，且现有处理标签不确定性的方法依赖于无法验证的假设。

Conclusion: 标签不确定性是法律人工智能中一个不可忽视的问题，必须在模型设计和评估中加以考虑。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [652] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个在推理时可插拔的代理框架，通过分解多模态验证流程，在无需领域特定训练的情况下有效检测跨模态错误信息。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型依赖特定领域数据且难以泛化到多样化的操纵手段，而人工事实核查无法应对海量多模态内容，因此需要一种通用、自动化的多模态虚假信息检测方法。

Method: MIRAGE将多模态验证分解为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查（通过迭代提问获取网页证据）和校准判断模块，结合视觉-语言模型推理与定向网络检索。

Result: 在MMFakeBench验证集上，MIRAGE（使用GPT-4o-mini）达到81.65% F1分数和75.1%准确率，优于最强零样本基线7.65个百分点，误报率显著更低；测试集结果表明其具有良好泛化能力；消融实验显示各模块均有明确贡献。

Conclusion: 基于分解式代理推理与网络检索的方法可在无标注数据训练的情况下媲美监督检测器性能，适用于标注数据稀缺的跨模态虚假信息检测场景。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [653] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 本文提出一种通过结构感知损失优化方法，将超大语言模型（VLLM）的推理能力蒸馏到小型模型中，使其在代码生成任务中更好理解问题结构并生成正确解决方案。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在代码生成中缺乏复杂的推理能力，难以理解问题的结构和解决路径，而超大模型虽具备该能力但部署成本高。因此需要将VLLM的推理能力有效迁移到更小、高效的模型上。

Method: 采用知识蒸馏方法，训练小型模型模仿VLLM的推理过程，引入结构感知的损失函数，建立问题定义与潜在解之间的结构对应关系，使模型超越token级生成，掌握整体解结构。

Result: 在MBPP、MBPP Plus和HumanEval基准测试中，该方法显著优于基线模型，提升了pass@1、平均数据流和平均语法匹配等指标。

Conclusion: 所提出的结构感知蒸馏方法能有效将VLLM的推理能力迁移至小型模型，在降低部署成本的同时提升代码生成质量。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [654] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文提出“LLM-as-a-Prophet”范式，通过构建Prophet Arena基准评估大语言模型（LLMs）在现实事件预测中的能力，发现LLMs已具备较强的预测性能，但在事件回忆、数据理解及信息聚合速度方面仍存在瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在现实世界事件预测中的潜力，推动其在金融、经济等社会系统中的应用。

Method: 构建名为Prophet Arena的通用评估基准，持续收集实时预测任务并将其分解为不同阶段，进行受控的大规模实验评估。

Result: 发现许多LLMs已展现出良好的预测能力，如较小的校准误差、一致的预测置信度和有前景的市场回报；但也暴露出事件回忆不准确、误解数据源以及信息聚合速度慢于市场等关键瓶颈。

Conclusion: LLMs在预测未来事件方面具有潜力，但要实现更优的预测智能，仍需克服其在记忆、理解和信息处理速度方面的限制。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [655] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了一种名为上下文注意力调制（CAM）的新机制，并结合共享与专用模块的混合框架HyCAM，用于提升大语言模型在多任务适应中的性能，实验表明其在多种异构任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多任务适应中面临知识保留与任务特异性之间的平衡难题，传统微调方法存在灾难性遗忘和资源消耗问题，现有参数高效方法在复杂多任务场景下表现不佳。

Method: 提出了上下文注意力调制（CAM）机制，动态调节LLM中自注意力模块的表示；进一步构建HyCAM框架，结合全参数共享模块与轻量级专用模块，并引入动态路由策略实现自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的实验显示，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: CAM和HyCAM有效解决了多任务适应中的知识保留与专业化冲突，实现了更高效、更强的多任务学习能力。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [656] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 该研究发现视觉语言模型（VLMs）虽然能感知到正确的视觉证据，但在推理时未能有效利用，提出一种无需训练的推理时干预方法，通过突出关键视觉区域提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 探究VLMs在具备正确视觉证据的情况下仍出错的原因，是源于无法感知证据还是无法有效利用证据。

Method: 通过分析层间注意力动态，识别浅层和深层对文本与视觉证据的关注差异，并提出基于选择性注意力掩码的推理时干预方法。

Result: 发现VLMs普遍存在“看见但不信”现象，即模型感知到证据却未有效用于推理；所提方法在多个主流VLM家族中均提升了准确率。

Conclusion: VLMs内部编码了可靠的视觉证据但利用不足，显式增强关键证据可弥合感知与推理之间的鸿沟，提升模型可靠性与可解释性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>
