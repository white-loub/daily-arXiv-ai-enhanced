<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 130]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [A generative machine learning model for designing metal hydrides applied to hydrogen storage](https://arxiv.org/abs/2601.20892)
*Xiyuan Liu,Christian Hacker,Shengnian Wang,Yuhua Duan*

Main category: cs.LG

TL;DR: 本文提出了一种结合因果发现与轻量级生成式机器学习的框架，用于生成新型金属氢化物候选材料，以扩展当前有限的氢化物数据库，并加速氢能存储材料的发现。


<details>
  <summary>Details</summary>
Motivation: 现有材料数据库（如Materials Project）中已表征的金属氢化物数量有限，限制了高效储氢材料的发现。

Method: 整合因果发现与轻量级生成式机器学习模型，在450个样本的数据集上训练，生成1000个候选结构，并通过排序和过滤筛选出新颖候选。

Result: 识别出6个此前未报道的化学式与晶体结构，其中4个经密度泛函理论（DFT）验证，展现出良好的实验潜力。

Conclusion: 该框架为扩展氢存储材料数据库及加速新材料发现提供了一种可扩展且高效的方法。

Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.

</details>


### [2] [Is Parameter Isolation Better for Prompt-Based Continual Learning?](https://arxiv.org/abs/2601.20894)
*Jiangyang Li,Chenhao Ding,Songlin Dong,Qiang Wang,Jianchao Zhao,Yuhang He,Yihong Gong*

Main category: cs.LG

TL;DR: 本文提出了一种基于提示共享的持续学习框架，通过全局提示池、任务感知门控路由和历史感知调制器，提升参数利用效率并缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法为每个任务分配固定提示集，导致任务间知识完全隔离、参数利用不充分。

Method: 构建全局提示池，设计任务感知门控路由机制实现稀疏激活与动态解耦，并引入历史感知调制器利用提示激活统计保护高频提示免受过度更新。

Result: 在多个基准上显著优于静态提示分配方法，在有效性与效率上均取得更好表现。

Conclusion: 提示共享与动态协同优化能更高效地利用参数，缓解灾难性遗忘，是持续学习中更实用的提示管理范式。

Abstract: Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.

</details>


### [3] [Faster Predictive Coding Networks via Better Initialization](https://arxiv.org/abs/2601.20895)
*Luca Pinchetti,Simon Frieder,Thomas Lukasiewicz,Tommaso Salvatori*

Main category: cs.LG

TL;DR: 本文提出了一种新的预测编码网络神经元初始化方法，通过保留前一样本迭代过程中的进展，显著减少训练时间并提升收敛速度与测试性能。


<details>
  <summary>Details</summary>
Motivation: 预测编码等基于能量的学习算法因计算需求大（迭代性质导致）而应用受限，亟需提高其计算效率。

Method: 提出一种新的预测编码网络初始化技术，旨在保留前一样本迭代过程中已取得的进展。

Result: 实验表明该方法在监督和无监督设置下均显著提升了收敛速度和最终测试损失。

Conclusion: 该初始化策略为弥合预测编码与反向传播在计算效率和性能上的差距提供了有前景的路径。

Abstract: Research aimed at scaling up neuroscience inspired learning algorithms for neural networks is accelerating. Recently, a key research area has been the study of energy-based learning algorithms such as predictive coding, due to their versatility and mathematical grounding. However, the applicability of such methods is held back by the large computational requirements caused by their iterative nature. In this work, we address this problem by showing that the choice of initialization of the neurons in a predictive coding network matters significantly and can notably reduce the required training times. Consequently, we propose a new initialization technique for predictive coding networks that aims to preserve the iterative progress made on previous training samples. Our approach suggests a promising path toward reconciling the disparities between predictive coding and backpropagation in terms of computational efficiency and final performance. In fact, our experiments demonstrate substantial improvements in convergence speed and final test loss in both supervised and unsupervised settings.

</details>


### [4] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: 本文提出了TwinWeaver框架，将纵向患者病史序列化为文本，结合大语言模型构建Genie Digital Twin（GDT），在癌症患者的临床事件预测与轨迹 forecasting 上显著优于现有方法，并具备可解释性与跨分布泛化能力。


<details>
  <summary>Details</summary>
Motivation: 精准肿瘤学需预测临床事件与疾病轨迹，但稀疏、多模态的临床时间序列建模仍是关键挑战。

Method: 提出TwinWeaver开源框架，将纵向患者历史序列化为文本，利用大语言模型统一建模事件预测与轨迹forecasting；基于93,054名20种癌症患者的临床数据构建Genie Digital Twin（GDT）；评估其在forecasting误差（MASE）、风险分层（C-index）及跨分布临床试验泛化能力上的表现，并扩展可解释临床推理模块。

Result: GDT在forecasting上中位MASE达0.87（优于最强基线0.97，p<0.001）；C-index平均达0.703（优于基线0.662）；在跨分布临床试验中零样本匹配基线，微调后MASE降至0.75–0.88，C-index达0.672（优于基线0.648）；并支持可解释临床推理。

Conclusion: TwinWeaver为纵向临床建模提供了统一、可扩展且可解释的新范式，GDT展现出卓越的预测性能与泛化能力，推动大语言模型在精准肿瘤学中的临床落地。

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [5] [Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach](https://arxiv.org/abs/2601.20916)
*Anni Zhao,Ayca Ermis,Jeffrey Robert Vitt,Sergio Brasil,Wellingson Paiva,Magdalena Kasprowicz,Malgorzata Burzynska,Robert Hamilton,Runze Yan,Ofer Sadan,J. Claude Hemphill,Lieven Vandenberghe,Xiao Hu*

Main category: cs.LG

TL;DR: 本文提出了一种结合系统辨识与排序约束优化的机器学习方法，利用无创信号（如ABP、CBv、R-R间期）估计平均颅内压（ICP），在测试集中约31.88%样本误差≤2 mmHg。


<details>
  <summary>Details</summary>
Motivation: 临床上亟需准确、无创的颅内压（ICP）估测方法，以避免有创监测带来的风险和局限性。

Method: 采用子空间系统辨识算法构建脑血流动力学模型，输入为动脉血压（ABP）、脑血流速度（CBv）和R-R间期信号；再通过引入排序约束的凸优化学习映射函数，关联无创信号特征与ICP估计误差。

Result: 测试集中31.88%样本估计误差≤2 mmHg，34.07%介于2–6 mmHg；验证了该无创ICP估计方法的可行性。

Conclusion: 所提方法在无创估计平均ICP方面具备可行性，为急性脑损伤等患者的广谱、安全ICP监测奠定了基础，但尚需进一步临床验证与技术优化。

Abstract: Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.

</details>


### [6] [A Theory of Universal Agnostic Learning](https://arxiv.org/abs/2601.20961)
*Steve Hanneke,Shay Moran*

Main category: cs.LG

TL;DR: 本文建立了二元分类在非实现（agnostic）设定下的最优通用收敛速率的完整理论，识别出最优收敛速率的四分法：e^{-n}、e^{-o(n)}、o(n^{-1/2}) 或任意慢，并通过简单组合结构判定概念类所属类别。


<details>
  <summary>Details</summary>
Motivation: 扩展Bousquet等人（2021）在可实现情形下的理论，去除分布需满足可实现性假设的限制，建立更普适的非实现设定下最优收敛速率理论。

Method: 通过分析概念类的组合结构，严格刻画不同概念类在非实现设定下所能达到的最优通用收敛速率，并证明其存在且仅限于四种典型形式。

Result: 确立了非实现二元分类中最优通用收敛速率的四分法定理，即对任一概念类，其最优超额误差率收敛速率必为 e^{-n}、e^{-o(n)}、o(n^{-1/2}) 或任意慢之一，并给出判定该类别的组合判据。

Conclusion: 非实现设定下的最优收敛行为由概念类内在组合性质完全决定，且仅呈现四种本质不同的速率类型，从而实现了对学习能力边界的精细刻画。

Abstract: We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.

</details>


### [7] [Monotone Optimisation with Learned Projections](https://arxiv.org/abs/2601.20983)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 本文提出了一种算法感知的学习方法，将结构化神经网络（HM-RI）嵌入Polyblock Outer Approximation（POA）算法中，通过直接预测其核心投影原语（径向逆），避免传统POA中耗时的二分搜索；HM-RI网络显式建模单调性和齐次性，理论保证其输出对应合法单调约束集的径向逆，并在多个单调优化任务上实现显著加速与高质量解。


<details>
  <summary>Details</summary>
Motivation: 单调优化问题虽有专用全局求解器（如POA），但其依赖显式的目标与约束函数；而现实中这些函数常仅以数据形式存在，导致POA难以直接应用。

Method: 提出算法感知学习框架，设计Homogeneous-Monotone Radial Inverse（HM-RI）结构化神经网络，直接预测POA所需的径向逆投影；引入松弛单调性条件以降低训练开销；提供径向逆函数的理论刻画，并证明HM-RI预测器在温和结构条件下等价于合法单调约束集的径向逆。

Result: 在不定二次规划、乘法规划和发射功率优化等多个单调优化基准上，相比直接函数拟合等基线方法，本方法大幅加快POA收敛速度，同时保持高解质量，并优于未利用单调结构的方法。

Conclusion: 将领域知识（单调性、齐次性）嵌入神经网络结构并紧密耦合至优化算法（POA）的核心原语，是提升数据驱动优化效率与可靠性的一条有效路径；HM-RI为单调优化中的数据驱动建模提供了可解释、可验证且实用的新范式。

Abstract: Monotone optimisation problems admit specialised global solvers such as the Polyblock Outer Approximation (POA) algorithm, but these methods typically require explicit objective and constraint functions. In many applications, these functions are only available through data, making POA difficult to apply directly. We introduce an algorithm-aware learning approach that integrates learned models into POA by directly predicting its projection primitive via the radial inverse, avoiding the costly bisection procedure used in standard POA. We propose Homogeneous-Monotone Radial Inverse (HM-RI) networks, structured neural architectures that enforce key monotonicity and homogeneity properties, enabling fast projection estimation. We provide a theoretical characterisation of radial inverse functions and show that, under mild structural conditions, a HM-RI predictor corresponds to the radial inverse of a valid set of monotone constraints. To reduce training overhead, we further develop relaxed monotonicity conditions that remain compatible with POA. Across multiple monotone optimisation benchmarks (indefinite quadratic programming, multiplicative programming, and transmit power optimisation), our approach yields substantial speed-ups in comparison to direct function estimation while maintaining strong solution quality, outperforming baselines that do not exploit monotonic structure.

</details>


### [8] [Distributional Active Inference](https://arxiv.org/abs/2601.20985)
*Abdullah Akgül,Gulcin Baykal,Manuel Haußmann,Mustafa Mert Çelikok,Melih Kandemir*

Main category: cs.LG

TL;DR: 本文提出了一种将主动推理（active inference）无缝集成到分布强化学习框架中的形式化抽象，无需建模状态转移动力学，从而提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习仅关注远见规划而忽视感知信息组织，导致样本效率低；主动推理虽能同时处理感知与规划，但在AI中应用受限于对模型的依赖。

Method: 构建一种覆盖基于模型、分布式和无模型方法的强化学习形式化抽象，并将主动推理融入分布强化学习框架，避免显式建模转移动力学。

Result: 实现了主动推理在人工系统中的高效应用，提升了样本效率，且不依赖环境动力学建模。

Conclusion: 该抽象弥合了强化学习与主动推理之间的鸿沟，为复杂环境下的机器人控制提供了更高效、更生物合理的学习范式。

Abstract: Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.

</details>


### [9] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 本文提出首个面向全球儿童发展的预训练编码器，利用44个国家35.7万儿童的UNICEF调查数据训练，在仅50个样本下AUC达0.65，零样本迁移至未见国家AUC可达0.84，显著提升资源受限地区SDG 4.2.1监测的可行性。


<details>
  <summary>Details</summary>
Motivation: 大量儿童经历本可预防的发展迟缓，但机器学习在新国家部署受制于数据瓶颈：可靠模型需数千样本，而新项目初始仅有不足100个样本。

Method: 基于UNICEF在44个国家采集的357,709名儿童调查数据，构建并训练首个全球儿童发展预训练编码器；结合小样本微调与零样本迁移，并应用迁移学习理论界解释预训练多样性对少样本泛化能力的促进作用。

Result: 仅用50个训练样本时，预训练编码器平均AUC达0.65（95% CI: 0.56–0.72），较冷启动梯度提升8–12%；N=500时AUC达0.73；零样本部署至未见国家AUC最高达0.84。

Conclusion: 预训练编码器能显著提升机器学习在资源受限环境下监测联合国可持续发展目标SDG 4.2.1（儿童早期发展）的可行性与实用性。

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [10] [Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles](https://arxiv.org/abs/2601.20989)
*Lutz Oettershagen*

Main category: cs.LG

TL;DR: 本文研究在昂贵精确评估场景下识别top-k项的问题，提出了一种双oracle设置（弱oracle快速但有噪声，强oracle稀缺但高精度），并设计了ACE和ACE-W两种自适应算法以减少对强oracle的调用次数。


<details>
  <summary>Details</summary>
Motivation: 当精确评估代价高昂时，如何高效识别top-k项是一个基础而具有挑战性的问题；现有方法难以平衡效率与准确性，尤其在弱oracle存在噪声、强oracle稀缺的情况下。

Method: 提出screen-then-certify（STC）基线方法并进行理论分析；设计自适应认证算法ACE，聚焦于边界项的强查询；进一步提出两阶段自适应方法ACE-W，在运行ACE前动态分配弱oracle预算。

Result: 证明STC最多调用m(4ε_max)次强oracle，且该界是条件紧的；ACE达到相同理论界但实践中显著减少强调用；ACE-W进一步降低强oracle使用成本。

Conclusion: ACE和ACE-W在理论保证和实际性能上均优于基线方法，为高成本评估下的top-k识别提供了高效、自适应的解决方案。

Abstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\varepsilon_{\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\varepsilon_{\max}$, where $m(\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $Ω(m(\varepsilon_{\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\varepsilon_{\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.

</details>


### [11] [MADE: Benchmark Environments for Closed-Loop Materials Discovery](https://arxiv.org/abs/2601.20996)
*Shreshth A Malik,Tiarnan Doherty,Panagiotis Tigas,Muhammed Razzak,Stephen J. Roberts,Aron Walsh,Yarin Gal*

Main category: cs.LG

TL;DR: 本文提出MADE框架，用于端到端评估自主材料发现流程，强调闭环、资源受限和迭代特性，并支持模块化代理设计与系统性消融实验。


<details>
  <summary>Details</summary>
Motivation: 现有材料发现基准仅关注静态预测或孤立子任务，忽略了科学发现固有的迭代性和自适应性。

Method: 提出MAterials Discovery Environments（MADE）框架，将材料发现建模为在有限oracle预算下的闭合循环搜索过程，以热力学稳定化合物相对于凸包的目标进行形式化，并支持可插拔的生成模型、过滤器和规划器等组件。

Result: 实现了灵活的端到端发现流程建模，支持从固定流水线到完全自主工具调用系统的广泛代理设计，并通过多系统实验验证了其可扩展性和组件可分析性。

Conclusion: MADE为评估真实世界材料发现的效率与适应性提供了更贴近实际的新基准框架，推动了自主科学发现方法的发展。

Abstract: Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.

</details>


### [12] [Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference](https://arxiv.org/abs/2601.21012)
*Young Kyung Kim,Oded Schlesinger,Qiangqiang Wu,J. Matías Di Martino,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 本文提出了Order-Aware Test-Time Adaptation (OATTA)，一种利用测试时数据流中时间动态信息进行无梯度贝叶斯递归估计的轻量级、模型无关的测试时自适应方法，并引入似然比门控机制保障弱结构化流下的预测安全性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应（TTA）方法忽略测试数据流中蕴含的时间动态监督信号，仅将其视为独立样本处理。

Method: 将测试时自适应建模为无梯度的递归贝叶斯估计任务，使用学习到的动态转移矩阵作为时间先验，并设计似然比门控（LLR）在缺乏时间证据时回退至基础预测器。

Result: 在图像分类、可穿戴与生理信号分析、语言情感分析等多个任务上显著提升基线性能，最高准确率提升达6.35%；验证了建模时间动态可提供标准TTA之外的关键正交信号。

Conclusion: 建模测试数据流中的时间结构是提升TTA鲁棒性与泛化性的有效且通用途径，OATTA作为一种安全、轻量、即插即用的模块，为TTA开辟了新方向。

Abstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.

</details>


### [13] [Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints](https://arxiv.org/abs/2601.21033)
*Omer Rochman-Sharabi,Gilles Louppe*

Main category: cs.LG

TL;DR: 本文提出了一种名为Predict-Project-Renoise（PPR）的约束采样框架，用于在扩散模型生成过程中强制满足物理定律等硬性约束，显著降低约束违反率并提升样本一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的神经模拟器难以保证物理准确性或满足硬性约束（如物理定律、观测一致性），需在生成阶段引入约束机制。

Method: 定义仅在可行解集（满足约束的样本集合）上进行的约束前向扩散过程，并提出PPR迭代算法：交替执行去噪预测、投影到可行集、再加噪。

Result: 在2D分布、偏微分方程求解和全球天气预报任务中，PPR将约束违反率降低一个数量级以上，同时提升样本一致性与真实约束分布的匹配度。

Conclusion: PPR为扩散模型在科学计算中的可靠应用提供了有效且通用的约束生成方案，兼顾物理准确性和生成质量。

Abstract: Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.

</details>


### [14] [Test-Time Adaptation for Unsupervised Combinatorial Optimization](https://arxiv.org/abs/2601.21048)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: 本文提出TACO框架，统一并扩展了无监督神经组合优化中的泛化模型与实例特定模型，通过策略性热启动实现高效无监督适应，显著提升解质量且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 如何在无监督神经组合优化中兼顾模型泛化能力与实例级自适应灵活性，解决现有两类范式（泛化型与实例特化型）互不兼容的问题。

Method: 提出模型无关的测试时自适应框架TACO，采用策略性热启动：部分松弛已训练参数以保留归纳偏置，同时支持快速有效的无监督实例适配。

Result: 在最小顶点覆盖和最大团等经典组合优化问题上，TACO在静态、分布偏移及动态场景下均展现出更优解质量与鲁棒性，且计算成本几乎无增加。

Conclusion: TACO成功架起了泛化型与实例特化型无监督NCO之间的实用桥梁，兼具高效性、适应性与鲁棒性。

Abstract: Unsupervised neural combinatorial optimization (NCO) enables learning powerful solvers without access to ground-truth solutions. Existing approaches fall into two disjoint paradigms: models trained for generalization across instances, and instance-specific models optimized independently at test time. While the former are efficient during inference, they lack effective instance-wise adaptability; the latter are flexible but fail to exploit learned inductive structure and are prone to poor local optima. This motivates the central question of our work: how can we leverage the inductive bias learned through generalization while unlocking the flexibility required for effective instance-wise adaptation? We first identify a challenge in bridging these two paradigms: generalization-focused models often constitute poor warm starts for instance-wise optimization, potentially underperforming even randomly initialized models when fine-tuned at test time. To resolve this incompatibility, we propose TACO, a model-agnostic test-time adaptation framework that unifies and extends the two existing paradigms for unsupervised NCO. TACO applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid and effective unsupervised adaptation. Crucially, compared to naively fine-tuning a trained generalizable model or optimizing an instance-specific model from scratch, TACO achieves better solution quality while incurring negligible additional computational cost. Experiments on canonical CO problems, Minimum Vertex Cover and Maximum Clique, demonstrate the effectiveness and robustness of TACO across static, distribution-shifted, and dynamic combinatorial optimization problems, establishing it as a practical bridge between generalizable and instance-specific unsupervised NCO.

</details>


### [15] [Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization](https://arxiv.org/abs/2601.21058)
*Seungki Hong,Kyeongwon Jeong,Taekwang Jang*

Main category: cs.LG

TL;DR: 本文提出Snowball，一种数字式、可扩展、全连接的伊辛机，通过双模式马尔可夫链蒙特卡洛自旋选择与异步更新机制，显著降低求解时间，并支持高精度耦合系数配置。


<details>
  <summary>Details</summary>
Motivation: 解决伊辛机在实际部署中面临的三大挑战：硬件拓扑限制、自旋选择与更新算法低效、耦合系数精度不足。

Method: 设计Snowball数字架构，集成双模式马尔可夫链蒙特卡洛自旋选择与异步更新机制，并在AMD Alveo U250加速卡上实现原型。

Result: 在相同基准实例上，相比最先进伊辛机，时间至解减少8倍。

Conclusion: Snowball通过数字架构与新型更新策略，在保持高耦合精度的同时显著提升收敛速度与求解效率，推动伊辛机实用化。

Abstract: Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Restricted topologies require minor embedding; naive parallel updates can oscillate or stall; and limited precision can preclude feasible mappings or degrade solution quality.
  This work presents Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates to promote convergence and reduce time-to-solution. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations at high bit widths. A prototype on an AMD Alveo U250 accelerator card achieves an 8$\times$ reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.

</details>


### [16] [Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks](https://arxiv.org/abs/2601.21061)
*Alexandre Larouche,Audrey Durand*

Main category: cs.LG

TL;DR: 本文提出SUBo-GFN，一种结合子模性上界与生成流网络（GFlowNets）的新方法，显著提升采样效率与高质量候选生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNets在奖励函数未知时采样效率低；当奖励具有子模结构时，可利用该结构提供未观测对象的奖励上界，从而提升探索效率。

Method: 利用子模函数性质推导未观测组合对象的奖励上界，并基于乐观主义原则（Optimism in the Face of Uncertainty）将这些上界融入GFlowNet训练目标，构建SUBo-GFN。

Result: SUBo-GFN在相同奖励查询次数下生成的训练数据量比经典GFlowNets高数个数量级，在合成与真实子模任务中均展现出更优的分布匹配与高质量候选生成性能。

Conclusion: 子模结构可被有效用于增强GFlowNets的探索与学习效率，SUBo-GFN为结构化奖励下的高效生成建模提供了新范式。

Abstract: Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.

</details>


### [17] [Out-of-Distribution Generalization in Graph Foundation Models](https://arxiv.org/abs/2601.21067)
*Haoyang Li,Haibo Chen,Xin Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文综述了图基础模型（GFMs）在分布外（OOD）泛化方面的最新进展，重点分析了图学习中分布偏移的挑战、统一问题设定、不同任务设定下的OOD处理策略及预训练目标，并讨论了评估协议与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 图学习模型在跨训练分布应用时泛化能力有限，而实际中图结构、语义、模态或任务形式的变化常导致分布偏移，亟需具备强OOD泛化的图基础模型。

Method: 从OOD泛化视角系统梳理GFMs研究进展，按任务设定是否固定分类现有方法，总结其OOD应对策略与预训练目标，并分析评估协议。

Result: 构建了面向OOD泛化的GFMs统一问题框架，分类归纳了主流方法及其策略，指出了当前评估局限与多个开放研究方向。

Conclusion: 该综述是首篇聚焦GFMs中OOD泛化的系统性工作，为推动鲁棒、通用图表示学习提供了理论支撑与实践指引。

Abstract: Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.

</details>


### [18] [MapPFN: Learning Causal Perturbation Maps in Context](https://arxiv.org/abs/2601.21092)
*Marvin Sextro,Weronika Kłos,Gabriel Dernbach*

Main category: cs.LG

TL;DR: 本文提出MapPFN，一种基于合成数据预训练的先验-数据拟合网络，能够通过上下文学习预测扰动后的基因表达分布，无需梯度优化，且在仅用模拟基因敲除数据训练的情况下，性能媲美使用真实单细胞数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞扰动数据覆盖的生物学背景有限，且模型无法在推理时利用新干预证据进行自适应调整，亟需能泛化至未见生物学背景并识别特异性机制的治疗效应模型。

Method: 提出MapPFN，一种先验-数据拟合网络（PFN），在由因果扰动先验生成的合成数据上预训练；在推理阶段，通过上下文学习（in-context learning）直接预测扰动后分布，无需梯度更新。

Result: 尽管仅在模拟基因敲除数据上预训练，MapPFN仍能准确识别差异表达基因，性能与在真实单细胞数据上训练的模型相当。

Conclusion: MapPFN实现了跨生物学背景的强泛化能力，展示了利用合成先验数据和上下文学习进行元学习扰动效应估计的有效性，为生物干预建模提供了新范式。

Abstract: Planning effective interventions in biological systems requires treatment-effect models that adapt to unseen biological contexts by identifying their specific underlying mechanisms. Yet single-cell perturbation datasets span only a handful of biological contexts, and existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data. To meta-learn a perturbation effect estimator, we present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. Given a set of experiments, MapPFN uses in-context learning to predict post-perturbation distributions, without gradient-based optimization. Despite being pretrained on in silico gene knockouts alone, MapPFN identifies differentially expressed genes, matching the performance of models trained on real single-cell data. Our code and data are available at https://github.com/marvinsxtr/MapPFN.

</details>


### [19] [TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning](https://arxiv.org/abs/2601.21135)
*Shicheng Fan,Kun Zhang,Lu Cheng*

Main category: cs.LG

TL;DR: 本文提出TRACE框架，解决时间因果表征学习中机制连续变化的问题，通过凸组合建模过渡机制，实现潜变量与混合轨迹的联合可识别性，并在合成与真实数据上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设因果机制在离散域间瞬时切换，但现实系统（如车辆动力学、人体步态）常呈现连续机制过渡，需新方法建模。

Method: 将过渡机制建模为有限原子机制的凸组合，由时变混合系数控制；提出TRACE——基于专家混合（Mixture-of-Experts）的框架，每个专家学习一个原子机制，支持测试时恢复机制轨迹并泛化至未见中间状态。

Result: 理论证明潜因果变量与连续混合轨迹 jointly identifiable；实验显示TRACE在混合轨迹恢复上达最高0.99相关性，显著优于离散切换基线。

Conclusion: TRACE成功建模并识别连续机制演化，拓展了时间因果表征学习的适用边界，具备强泛化能力与实证有效性。

Abstract: Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.

</details>


### [20] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文提出了一种动态截断半径方法，用于降低机器学习原子间势（MLIPs）的推理时间和内存消耗，同时保持分子动力学模拟的稳定性与精度。


<details>
  <summary>Details</summary>
Motivation: MLIPs在分子动力学模拟中广泛应用，但其固定截断半径导致推理慢、内存高，限制了实际大规模应用。

Method: 提出动态截断半径机制，按目标邻居数自适应调整每个原子的截断半径，从而在原子图上引入稀疏性；将其集成到MACE、Nequip、Orbv3和TensorNet四种主流MLIP中。

Result: 平均减少2.26倍内存占用、提升2.04倍推理速度；精度损失极小，在材料和分子数据集上均验证有效。

Conclusion: 动态截断是一种通用、高效且精度保持良好的优化策略，可广泛应用于各类MLIP模型，并已开源全部代码。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [21] [Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting](https://arxiv.org/abs/2601.21151)
*Carlos A. Pereira,Stéphane Gaudreault,Valentin Dallerit,Christopher Subich,Shoyon Panday,Siqi Wei,Sasa Zhang,Siddharth Rout,Eldad Haber,Raymond J. Spiteri,David Millard,Emilia Diaconescu*

Main category: cs.LG

TL;DR: PARADIS is a physics-inspired weather forecasting model that decomposes physical processes (advection, diffusion, reaction) into modular neural blocks, using a Neural Semi-Lagrangian operator for efficient advection on the sphere, achieving SOTA accuracy with much lower training cost.


<details>
  <summary>Details</summary>
Motivation: Monolithic ML weather models poorly represent advection—requiring expensive global interactions or deep convolutions—hindering efficiency and physical interpretability.

Method: PARADIS decomposes dynamics into advection (via differentiable Neural Semi-Lagrangian operator on sphere), diffusion (depthwise-separable spatial mixing), and reaction (pointwise channel interactions); all operate on learned latent variables.

Result: On ERA5 benchmarks, 1° PARADIS trained in <1 GPU month matches or exceeds forecast skill of 0.25° baselines—including ECMWF HRES and GraphCast—despite coarser resolution and far lower compute cost.

Conclusion: Functional decomposition with physics-informed inductive biases enables highly efficient, accurate, and interpretable ML-based weather prediction.

Abstract: Recent machine-learning approaches to weather forecasting often employ a monolithic architecture, where distinct physical mechanisms (advection, transport), diffusion-like mixing, thermodynamic processes, and forcing are represented implicitly within a single large network. This representation is particularly problematic for advection, where long-range transport must be treated with expensive global interaction mechanisms or through deep, stacked convolutional layers. To mitigate this, we present PARADIS, a physics-inspired global weather prediction model that imposes inductive biases on network behavior through a functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. We implement advection through a Neural Semi-Lagrangian operator that performs trajectory-based transport via differentiable interpolation on the sphere, enabling end-to-end learning of both the latent modes to be transported and their characteristic trajectories. Diffusion-like processes are modeled through depthwise-separable spatial mixing, while local source terms and vertical interactions are modeled via pointwise channel interactions, enabling operator-level physical structure. PARADIS provides state-of-the-art forecast skill at a fraction of the training cost. On ERA5-based benchmarks, the 1 degree PARADIS model, with a total training cost of less than a GPU month, meets or exceeds the performance of 0.25 degree traditional and machine-learning baselines, including the ECMWF HRES forecast and DeepMind's GraphCast.

</details>


### [22] [A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components](https://arxiv.org/abs/2601.21160)
*Michael Ibrahim,Nagi Gebraeel,Weijun Xie*

Main category: cs.LG

TL;DR: 本文提出FedGEM算法，用于解决联邦聚类中全局簇数未知且客户端数据分布异构但可能存在重叠簇的问题；该方法基于广义EM框架，利用客户端本地EM与不确定性集构建，结合服务器端闭式计算推断全局簇数，并提供理论收敛保证及在各向同性高斯混合模型下的高效实现。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类中全局簇数K未知，且各客户端本地数据具有异构性但簇可能重叠，现有方法难以有效处理。

Method: 提出FedGEM：客户端执行局部EM并为每个局部簇构造不确定性集；服务器利用这些集合推断簇间重叠关系，并通过闭式计算估计全局簇数K；针对各向同性高斯混合模型（GMM）设计低复杂度本地计算，并给出概率收敛性理论分析。

Result: 理论层面提供了概率收敛保证；实验表明FedGEM性能接近集中式EM，且优于多种现有联邦聚类方法。

Conclusion: FedGEM是一种可扩展、具理论保障的联邦聚类新范式，能自适应估计全局簇数并有效应对客户端数据异构与簇重叠挑战。

Abstract: We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.

</details>


### [23] [Efficient Simple Regret Algorithms for Stochastic Contextual Bandits](https://arxiv.org/abs/2601.21167)
*Shuai Liu,Alireza Bakhtiari,Alex Ayoub,Botao Hao,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 本文研究了随机上下文逻辑带臂问题的简单遗憾目标，提出了首个达到简单遗憾O(d/√T)的算法，并引入了一种新的针对简单遗憾设置的Thompson采样变体，实现了O(d^(3/2)/√T)的遗憾界，且主项不依赖于参数κ。


<details>
  <summary>Details</summary>
Motivation: 已有线性情形下的简单遗憾保证，但逻辑情形下尚无相关结果；同时希望消除遗憾界中对参数κ（与未知参数向量幅度S指数相关）的依赖。

Method: 结合上下文线性带臂和自协调分析的思想设计确定性算法；提出适配简单遗憾目标的新Thompson采样变体，并将其推广至逻辑情形。

Result: 确定性算法实现简单遗憾Õ(d/√T)，随机算法（Thompson采样）实现Õ(d^(3/2)/√T)，两者主项均不依赖κ；算法在有限动作集下完全可解；实验验证了理论结果。

Conclusion: 本文首次为随机上下文逻辑带臂问题建立了简单遗憾保证，消除了对κ的依赖，并提供了高效可实现的确定性与随机算法，拓展了Thompson采样在简单遗憾设定下的理论基础。

Abstract: We study stochastic contextual logistic bandits under the simple regret objective. While simple regret guarantees have been established for the linear case, no such results were previously known for the logistic setting. Building on ideas from contextual linear bandits and self-concordant analysis, we propose the first algorithm that achieves simple regret $\tilde{\mathcal{O}}(d/\sqrt{T})$. Notably, the leading term of our regret bound is free of the constant $κ= \mathcal O(\exp(S))$, where $S$ is a bound on the magnitude of the unknown parameter vector. The algorithm is shown to be fully tractable when the action set is finite. We also introduce a new variant of Thompson Sampling tailored to the simple-regret setting. This yields the first simple regret guarantee for randomized algorithms in stochastic contextual linear bandits, with regret $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$. Extending this method to the logistic case, we obtain a similarly structured Thompson Sampling algorithm that achieves the same regret bound -- $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$ -- again with no dependence on $κ$ in the leading term. The randomized algorithms, as expected, are cheaper to run than their deterministic counterparts. Finally, we conducted a series of experiments to empirically validate these theoretical guarantees.

</details>


### [24] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 本文提出一种机器学习方法，通过学习协方差或精度矩阵幂族的最优特征表示，实现对癫痫发作、客户流失等突发性事件的早期检测，并兼顾预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中突发性现象（如癫痫发作、客户流失、疫情暴发）源于隐藏的因果交互，而数据生成过程未知且部分可观测，亟需能揭示并利用潜在因果结构的方法。

Method: 构建一个单参数估计器族（经验协方差或精度矩阵的幂），从中学习最优特征表示；再接入监督学习模块进行分类；理论证明该估计器族具有结构一致性。

Result: 在癫痫发作检测和客户流失预测任务上取得有竞争力的结果；最优协方差幂兼具良好可识别性与结构性表征能力，实现预测性与可解释性的统一。

Conclusion: 该方法为部分可观测复杂系统中突发性事件的早期检测提供了兼顾理论保证、预测性能与统计可解释性的新范式。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [25] [Breaking the Reasoning Horizon in Entity Alignment Foundation Models](https://arxiv.org/abs/2601.21174)
*Yuanning Cui,Zequn Sun,Wei Hu,Kexuan Xin,Zhangjie Fu*

Main category: cs.LG

TL;DR: 本文提出了一种面向实体对齐（EA）的基础模型，通过并行编码策略、种子对作为局部锚点、融合关系图和可学习交互模块，解决现有图基础模型在EA任务中因'推理视野差距'导致的长程依赖建模不足问题，显著提升对未见知识图谱的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有实体对齐模型缺乏迁移能力，无法对未见知识图谱进行对齐；直接适配图基础模型效果差，主因是EA需建模稀疏异构KG中的长程依赖，而图基础模型更擅长短程链接预测，存在‘推理视野差距’。

Method: 提出基于并行编码的EA基础模型：以种子EA对为局部锚点，初始化并同步编码两条信息流，实现锚点引导的消息传递；引入融合关系图建模全局依赖；设计可学习交互模块实现精准匹配。

Result: 实验表明该框架在多个基准上有效，尤其展现出对未见知识图谱的强泛化能力。

Conclusion: 并行编码与锚点引导机制能有效弥合推理视野差距，提升EA模型的可迁移性与泛化性，为构建KG融合基础模型提供了新范式。

Abstract: Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.

</details>


### [26] [Flow Perturbation++: Multi-Step Unbiased Jacobian Estimation for High-Dimensional Boltzmann Sampling](https://arxiv.org/abs/2601.21177)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 本文提出Flow Perturbation++，一种无偏且方差降低的雅可比行列式估计方法，用于提升连续归一化流在高维系统中无偏Boltzmann采样的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 连续归一化流（CNFs）在高维系统中进行无偏Boltzmann采样时受限于雅可比行列式计算开销大（需D次反向传播），而现有随机估计器要么有偏（如Hutchinson法），要么方差高（如Flow Perturbation）。

Method: 提出Flow Perturbation++：将概率流ODE离散化，在每个积分步进行无偏的逐步雅可比估计，结合序贯蒙特卡洛（SMC）框架实现高效采样。

Result: 在1000维高斯混合模型和全原子Chignolin蛋白上，相比Hutchinson法和单步Flow Perturbation基线，Flow Perturbation++显著提升了平衡态采样性能。

Conclusion: Flow Perturbation++在保持无偏性的同时大幅降低方差，有效提升了CNFs在高维分子系统中的Boltzmann采样效率与可扩展性。

Abstract: The scalability of continuous normalizing flows (CNFs) for unbiased Boltzmann sampling remains limited in high-dimensional systems due to the cost of Jacobian-determinant evaluation, which requires $D$ backpropagation passes through the flow layers. Existing stochastic Jacobian estimators such as the Hutchinson trace estimator reduce computation but introduce bias, while the recently proposed Flow Perturbation method is unbiased yet suffers from high variance. We present \textbf{Flow Perturbation++}, a variance-reduced extension of Flow Perturbation that discretizes the probability-flow ODE and performs unbiased stepwise Jacobian estimation at each integration step. This multi-step construction retains the unbiasedness of Flow Perturbation while achieves substantially lower estimator variance. Integrated into a Sequential Monte Carlo framework, Flow Perturbation++ achieves significantly improved equilibrium sampling on a 1000D Gaussian Mixture Model and the all-atom Chignolin protein compared with Hutchinson-based and single-step Flow Perturbation baselines.

</details>


### [27] [Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification](https://arxiv.org/abs/2601.21203)
*Weiguang Wang,Yong Liu,Yingjie Gao,Guangyuan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于自训练范式的跨被试域自适应方法（CSST），结合FBEA滤波器组对齐、对抗预训练（PTAL）与双集成自训练（DEST），并引入时频增强对比学习（TFA-CL），显著提升SSVEP信号的跨被试解码性能。


<details>
  <summary>Details</summary>
Motivation: SSVEP-BCI中存在被试间信号差异大、用户特异性标注成本高，导致识别性能受限。

Method: 提出跨被试自训练框架CSST，包含：1）Filter-Bank Euclidean Alignment（FBEA）利用滤波器组频率信息；2）Pre-Training with Adversarial Learning（PTAL）实现源/目标域分布对齐；3）Dual-Ensemble Self-Training（DEST）提升伪标签质量；4）Time-Frequency Augmented Contrastive Learning（TFA-CL）增强多视图特征判别力。

Result: 在Benchmark和BETA数据集上，该方法在不同信号长度下均达到当前最优性能（state-of-the-art）。

Conclusion: 所提CSST框架有效缓解了SSVEP-BCI中的跨被试泛化难题，兼具鲁棒性与实用性，为少标定/无标定BCI提供了新思路。

Abstract: Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.

</details>


### [28] [Soft Quantization: Model Compression Via Weight Coupling](https://arxiv.org/abs/2601.21219)
*Daniel T. Bernstein,Luca Di Carlo,David Schwab*

Main category: cs.LG

TL;DR: 本文提出了一种通过在训练过程中引入权重间的短程吸引力耦合来实现神经网络模型软量化的新方法，该方法能快速促使权重分布离散化，并在仅增加两个超参数的情况下实现混合精度量化。


<details>
  <summary>Details</summary>
Motivation: 为探索模型压缩与泛化能力之间的权衡，并提供一种灵活的机器学习模型压缩新途径。

Method: 在神经网络训练过程中引入权重间的短程吸引耦合，以诱导权重分布的离散化，实现软量化。

Result: 在ResNet-20/CIFAR-10上，软量化方案在适当超参数范围内优于直方图均衡化的训练后量化。

Conclusion: 软量化不仅是一种有效的模型压缩新方法，还为研究高维损失景观中压缩与泛化之间的权衡提供了新工具。

Abstract: We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our "soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.

</details>


### [29] [PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations](https://arxiv.org/abs/2601.21234)
*Kaiyuan Tan,Kendra Givens,Peilun Li,Thomas Beckers*

Main category: cs.LG

TL;DR: 本文提出了PHDME框架，结合端口-哈密顿结构先验与扩散模型，在稀疏数据和不完全物理知识下实现高精度、物理一致的轨迹预测，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在动力系统轨迹预测中表现优异，但在稀疏数据下不可靠；而现有物理信息机器学习方法大多依赖完整显式控制方程，难以适用于复杂非线性系统中仅部分已知物理规律的情形。

Method: 提出PHDME：先用稀疏观测训练高斯过程分布的端口-哈密顿系统（GP-dPHS）以建模能量动态；再利用GP-dPHS生成物理一致的合成数据并设计结构化物理残差损失指导扩散模型训练；最后采用分裂共形校准为预测提供不确定性估计。

Result: 在PDE基准和真实弹簧系统实验中，PHDME在数据稀缺条件下相比基线方法显著提升了预测精度与物理一致性，并能输出可靠不确定性区间。

Conclusion: PHDME成功弥合了数据驱动建模与不完全物理先验之间的鸿沟，为稀疏观测下的可解释、鲁棒动力系统建模提供了新范式。

Abstract: Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.

</details>


### [30] [Model-Free Neural State Estimation in Nonlinear Dynamical Systems: A Comparative Study of Neural Architectures and Classical Filters](https://arxiv.org/abs/2601.21266)
*Zhuochen Liu,Hans Walker,Rahul Jain*

Main category: cs.LG

TL;DR: 本文系统比较了无需系统模型的神经网络估计器（如Transformer、状态空间模型SSM和循环网络）与经典滤波方法（如粒子滤波和非线性卡尔曼滤波）在非线性动力系统状态估计任务上的性能。结果表明，SSM等神经模型在精度上接近强非线性卡尔曼滤波器，且推理吞吐量显著更高。


<details>
  <summary>Details</summary>
Motivation: 探究数据驱动的神经网络估计器在缺乏显式系统动力学和噪声模型前提下，能否作为有原则的非线性滤波器使用，并与经典方法进行公平比较。

Method: 在多个非线性动力系统场景下，对Transformer、状态空间神经网络（SSM）和循环神经网络等模型自由神经估计器，与粒子滤波、非线性卡尔曼滤波等经典方法进行系统性实证对比评估。

Result: 状态空间模型（SSM）在状态估计精度上接近强非线性卡尔曼滤波器，优于较弱的经典基线，同时推理吞吐量显著更高；其他神经架构也展现出竞争力。

Conclusion: 模型无关的神经估计器（尤其是SSM）可在不依赖系统模型的前提下，实现接近甚至媲美经典非线性滤波器的性能，并具备更高计算效率，为数据驱动状态估计提供了有力替代方案。

Abstract: Neural network models are increasingly used for state estimation in control and decision-making problems, yet it remains unclear to what extent they behave as principled filters in nonlinear dynamical systems. Unlike classical filters, which rely on explicit knowledge of system dynamics and noise models, neural estimators can be trained purely from data without access to the underlying system equations. In this work, we present a systematic empirical comparison between such model-free neural network models and classical filtering methods across multiple nonlinear scenarios. Our study evaluates Transformer-based models, state-space neural networks, and recurrent architectures alongside particle filters and nonlinear Kalman filters. The results show that neural models (in particular, state-space models (SSMs)) achieve state estimation performance that approaches strong nonlinear Kalman filters in nonlinear scenarios and outperform weaker classical baselines despite lacking access to system models, while also attaining substantially higher inference throughput.

</details>


### [31] [EGAM: Extended Graph Attention Model for Solving Routing Problems](https://arxiv.org/abs/2601.21281)
*Licheng Wang,Yuzi Yan,Mingtao Huang,Yuan Shen*

Main category: cs.LG

TL;DR: 本文提出了一种扩展图注意力模型（EGAM），通过多头点积注意力机制同时更新节点和边的嵌入，改进了仅考虑节点特征的传统图注意力模型（GAM），并在多种路由问题上达到或超越现有方法，尤其在强约束问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统图注意力模型（GAM）仅利用节点特征，忽略了边信息，限制了其在复杂图结构（尤其是强约束路由问题）中的表达能力与性能。

Method: 提出扩展图注意力模型（EGAM），采用多头点积注意力机制联合更新节点和边嵌入；使用自回归编码器-解码器架构，并结合定制化基线的策略梯度算法进行训练。

Result: EGAM在多种路由问题上匹配或超越现有方法，尤其在高度约束的问题上展现出卓越性能，验证了其对复杂图结构建模的有效性。

Conclusion: 引入边特征并扩展注意力机制能显著提升图神经网络在神经组合优化任务中的性能，EGAM为解决复杂约束路由问题提供了更强大的建模范式。

Abstract: Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.

</details>


### [32] [TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification](https://arxiv.org/abs/2601.21289)
*Akash Pandey,Payal Mohapatra,Wei Chen,Qi Zhu,Sinan Keten*

Main category: cs.LG

TL;DR: 本文提出TimeSliver框架，通过融合原始时间序列与符号化抽象，构建保持时序结构的可解释表示，实现对每个时间点重要性的线性可解释归因，在多个数据集上显著优于现有时序归因方法，同时保持接近SOTA的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有后验解释方法（如基于梯度或特征归因）存在参考状态敏感、难以泛化、忽略时序依赖等问题；而基于自注意力的解释又常缺乏忠实性，亟需一种既忠实又泛化能力强的时序可解释框架。

Method: 提出TimeSliver框架，联合使用原始时间序列及其符号化抽象，构建一种保持原始时序结构的新型表示；该表示中每个元素线性编码对应时间片段对预测的贡献，从而直接获得各时间点的重要性得分。

Result: 在7个合成与真实多变量时间序列数据集上，时序归因性能比其他方法提升11%；在26个UEA基准数据集上，分类准确率与SOTA基线差距在2%以内。

Conclusion: TimeSliver是一种兼具高解释忠实性、强泛化能力与优异预测性能的时间序列分类可解释框架，为模型决策透明化提供了新范式。

Abstract: Identifying the extent to which every temporal segment influences a model's predictions is essential for explaining model decisions and increasing transparency. While post-hoc explainable methods based on gradients and feature-based attributions have been popular, they suffer from reference state sensitivity and struggle to generalize across time-series datasets, as they treat time points independently and ignore sequential dependencies. Another perspective on explainable time-series classification is through interpretable components of the model, for instance, leveraging self-attention mechanisms to estimate temporal attribution; however, recent findings indicate that these attention weights often fail to provide faithful measures of temporal importance. In this work, we advance this perspective and present a novel explainability-driven deep learning framework, TimeSliver, which jointly utilizes raw time-series data and its symbolic abstraction to construct a representation that maintains the original temporal structure. Each element in this representation linearly encodes the contribution of each temporal segment to the final prediction, allowing us to assign a meaningful importance score to every time point. For time-series classification, TimeSliver outperforms other temporal attribution methods by 11% on 7 distinct synthetic and real-world multivariate time-series datasets. TimeSliver also achieves predictive performance within 2% of state-of-the-art baselines across 26 UEA benchmark datasets, positioning it as a strong and explainable framework for general time-series classification.

</details>


### [33] [Missing-Data-Induced Phase Transitions in Spectral PLS for Multimodal Learning](https://arxiv.org/abs/2601.21294)
*Anders Gjølbye,Ida Kargaard,Emma Kargaard,Lars Kai Hansen*

Main category: cs.LG

TL;DR: 本文研究了在高维稀疏数据下，部分最小二乘（PLS）方法通过奇异值分解（PLS-SVD）提取多模态数据共享结构的统计极限，发现其存在类似BBP相变的临界现象：当信噪比低于阈值时，无法有效恢复潜在共享方向；高于阈值时可实现非平凡对齐，并给出渐近重叠度的闭式表达。


<details>
  <summary>Details</summary>
Motivation: 多模态数据常存在双视图随机缺失，传统PLS-SVD在缺失场景下的理论性能尚不明确，需建立高维统计模型刻画其相变行为。

Method: 在比例高维尖峰模型下，分析独立随机缺失（MCAR）掩码下的经验交叉协方差矩阵，将其归一化后建模为带衰减信号强度（√ρ）的尖峰矩形随机矩阵，并推导其奇异向量与真实共享方向的渐近对齐性质。

Result: 揭示PLS-SVD存在BBP型相变：存在临界信噪比阈值；阈值之上可获得非零渐近重叠，且有闭式公式；仿真与半合成实验验证了理论预测的相图与恢复曲线。

Conclusion: PLS-SVD在随机缺失下的有效性具有严格的相变边界，信号强度需超过√ρ倍阈值才能保证可靠共享结构学习，为缺失多模态数据分析提供了理论判据。

Abstract: Partial Least Squares (PLS) learns shared structure from paired data via the top singular vectors of the empirical cross-covariance (PLS-SVD), but multimodal datasets often have missing entries in both views. We study PLS-SVD under independent entry-wise missing-completely-at-random masking in a proportional high-dimensional spiked model. After appropriate normalization, the masked cross-covariance behaves like a spiked rectangular random matrix whose effective signal strength is attenuated by $\sqrtρ$, where $ρ$ is the joint entry retention probability. As a result, PLS-SVD exhibits a sharp BBP-type phase transition: below a critical signal-to-noise threshold the leading singular vectors are asymptotically uninformative, while above it they achieve nontrivial alignment with the latent shared directions, with closed-form asymptotic overlap formulas. Simulations and semi-synthetic multimodal experiments corroborate the predicted phase diagram and recovery curves across aspect ratios, signal strengths, and missingness levels.

</details>


### [34] [Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with a New Contraction Principle](https://arxiv.org/abs/2601.21301)
*Zijun Chen,Zaiwei Chen,Nian Si,Shengbo Wang*

Main category: cs.LG

TL;DR: 本文研究了平均奖励马尔可夫决策过程（MDP）中同步与异步Q学习的收敛速率，通过引入懒惰采样机制和构造实例依赖的半范数，首次在较弱可达性假设下实现了最优的O~(ε^{-2})样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 平均奖励MDP中贝尔曼算子缺乏收缩性，导致现有非渐近分析需强假设或依赖折扣/回合制近似，带来参数未知或样本效率低的问题。

Method: 在可达性假设下，对MDP进行懒惰动态变换（以固定概率停留在当前状态），并构造一个实例依赖的半范数，在该半范数下变换后的贝尔曼算子具有单步收缩性，从而分析同步与异步Q学习的收敛性。

Result: 获得了同步与异步Q学习在平均奖励MDP下的最优非渐近样本复杂度O~(ε^{-2})，且算法仅需简单修改标准Q学习。

Conclusion: 懒惰变换与实例依赖半范数的结合，为克服平均奖励MDP中缺乏收缩性的根本挑战提供了新思路，并实现了理论最优性。

Abstract: We present the convergence rates of synchronous and asynchronous Q-learning for average-reward Markov decision processes, where the absence of contraction poses a fundamental challenge. Existing non-asymptotic results overcome this challenge by either imposing strong assumptions to enforce seminorm contraction or relying on discounted or episodic Markov decision processes as successive approximations, which either require unknown parameters or result in suboptimal sample complexity. In this work, under a reachability assumption, we establish optimal $\widetilde{O}(\varepsilon^{-2})$ sample complexity guarantees (up to logarithmic factors) for a simple variant of synchronous and asynchronous Q-learning that samples from the lazified dynamics, where the system remains in the current state with some fixed probability. At the core of our analysis is the construction of an instance-dependent seminorm and showing that, after a lazy transformation of the Markov decision process, the Bellman operator becomes one-step contractive under this seminorm.

</details>


### [35] [Transferable Graph Condensation from the Causal Perspective](https://arxiv.org/abs/2601.21309)
*Huaming Du,Yijie Huang,Su Yao,Yiying Wang,Yueyang Zhou,Jingwen Yang,Jinshi Zhang,Han Ji,Yu Zhao,Guisong Liu,Hegui Zhang,Carl Yang,Gang Kou*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果不变性和可迁移性的图数据集压缩方法TGCC，通过因果干预提取域不变特征、增强压缩操作和谱域对比学习，显著提升了跨任务和跨域场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图数据集压缩方法在跨任务和跨域场景中表现不佳，无法匹配原始数据集和任务需求。

Method: TGCC方法包括三步：1）利用因果干预从图的空间域提取域因果不变特征；2）进行增强压缩操作以充分捕获图的结构与特征信息；3）通过谱域增强对比学习将因果不变特征注入压缩图中。

Result: 在5个公开数据集和自建FinReport数据集上的实验表明，TGCC在跨任务和跨域复杂场景下相比现有方法最高提升13.41%，并在6个单数据集单任务场景中的5个上达到SOTA性能。

Conclusion: TGCC是一种有效且可迁移的图数据集压缩方法，能保留原始图的因果信息，在多种实际应用场景中展现出优越性能。

Abstract: The increasing scale of graph datasets has significantly improved the performance of graph representation learning methods, but it has also introduced substantial training challenges. Graph dataset condensation techniques have emerged to compress large datasets into smaller yet information-rich datasets, while maintaining similar test performance. However, these methods strictly require downstream applications to match the original dataset and task, which often fails in cross-task and cross-domain scenarios. To address these challenges, we propose a novel causal-invariance-based and transferable graph dataset condensation method, named \textbf{TGCC}, providing effective and transferable condensed datasets. Specifically, to preserve domain-invariant knowledge, we first extract domain causal-invariant features from the spatial domain of the graph using causal interventions. Then, to fully capture the structural and feature information of the original graph, we perform enhanced condensation operations. Finally, through spectral-domain enhanced contrastive learning, we inject the causal-invariant features into the condensed graph, ensuring that the compressed graph retains the causal information of the original graph. Experimental results on five public datasets and our novel \textbf{FinReport} dataset demonstrate that TGCC achieves up to a 13.41\% improvement in cross-task and cross-domain complex scenarios compared to existing methods, and achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.

</details>


### [36] [Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21312)
*Xiaozhuang Li,Xindi Tang,Fang He*

Main category: cs.LG

TL;DR: 本文提出GAT-PEARL框架，结合图注意力网络与概率嵌入的元强化学习方法，实现对动态充电网络下自主电动出租车车队的自适应调度，显著提升泛化性与运营效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设充电网络静态，难以反映真实世界中充电设施动态变化的现实，导致理论模型与实际运营脱节。

Method: 提出GAT-PEARL元强化学习框架：利用图注意力网络（GAT）建模城市时空关系与基础设施布局；引入概率嵌入用于Actor-Critic强化学习（PEARL），支持无需重新训练即可快速适应充电网络变化。

Result: 在成都真实数据的仿真中，GAT-PEARL显著优于传统强化学习基线，在未见过的充电布局上泛化性强，且动态环境下整体运营效率更高。

Conclusion: GAT-PEARL有效弥合了静态建模与动态现实之间的差距，为AET车队在不确定性充电环境下的智能调度提供了可扩展、自适应的解决方案。

Abstract: With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.

</details>


### [37] [An introductory Generalization of the standard SVMs loss and its applications to Shallow and Deep Neural Networks](https://arxiv.org/abs/2601.21331)
*Filippo Portera*

Main category: cs.LG

TL;DR: 本文提出了一种用于SVM二分类和回归模型的新凸损失函数，通过引入模式相关性改进泛化性能，并在小数据集上验证其有效性，同时初步探索了其在浅层与深层神经网络中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决SVM在大数据集上可扩展性差的问题，并通过在损失函数中引入模式相关性来提升模型泛化能力。

Method: 提出一种新的凸损失函数，推导其对偶问题，并在多个小规模数据集上进行实验验证；进一步将该损失函数应用于浅层和深层神经网络。

Result: 新损失函数在泛化性能上不劣于标准损失，且在多个实验中表现更优；初步结果表明其在神经网络中也具有应用潜力。

Conclusion: 该新凸损失函数是一种有前景的改进方向，值得在SVM及神经网络中进一步深入研究。

Abstract: We propose a new convex loss for SVMs, both for the binary classification and for the regression models. Therefore, we show the mathematical derivation of the dual problems and we experiment them with several small data-sets. The minimal dimension of those data-sets is due to the difficult scalability of the SVM method to bigger instances. This preliminary study should prove that using pattern correlations inside the loss function could enhance the generalisation performances. Coherently, results show that generalisation measures are never worse than the standard losses and several times they are better. In our opinion, it should be considered a careful study of this loss, coupled with shallow and deep neural networks. In fact, we present some novel results obtained with those architectures.

</details>


### [38] [Factored Causal Representation Learning for Robust Reward Modeling in RLHF](https://arxiv.org/abs/2601.21350)
*Yupei Yang,Lin Yang,Wanxi Deng,Lin Qu,Fan Feng,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果视角的分解表示学习框架，用于构建更鲁棒的奖励模型，通过分离因果与非因果特征并约束奖励头仅依赖前者，同时用对抗训练抑制非因果特征中的奖励信息，从而缓解奖励黑客行为并提升RLHF效果。


<details>
  <summary>Details</summary>
Motivation: 标准奖励模型易受与人类标注无因果关系的伪特征（如文本长度、谄媚偏差）影响，导致奖励欺骗（reward hacking），即高预测奖励不反映真实偏好。

Method: 提出一种因子化表示学习框架：将上下文嵌入分解为（1）足以预测奖励的因果因子，和（2）包含奖励无关属性（如长度、谄媚偏差）的非因果因子；奖励头仅依赖因果因子；引入对抗头从非因果因子预测奖励，并结合梯度反转以抑制其编码奖励相关信息。

Result: 在数学与对话任务上的实验表明，该方法学习到更鲁棒的奖励模型，在下游RLHF性能上持续超越当前最优基线；对长度和谄媚偏差的分析进一步验证了其缓解奖励欺骗的有效性。

Conclusion: 从因果角度解耦表征可显著提升奖励模型的可靠性与泛化性，为构建可信的RLHF系统提供了新思路与有效技术路径。

Abstract: A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model's contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.

</details>


### [39] [Expected Improvement via Gradient Norms](https://arxiv.org/abs/2601.21357)
*Joshua Hang Sai Ip,Georgios Makrygiorgos,Ali Mesbah*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯优化采集函数EI-GN，将改进原则应用于梯度感知的辅助目标，以兼顾高性能与一阶平稳性，通过梯度增强代理模型实现高效优化，并在基准测试和控制策略学习中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准期望改进（EI）过于贪婪，易陷入次优驻点；需一种既能提升性能又能导向一阶平稳区域的采集策略。

Method: 提出EI-GN采集函数，基于梯度观测构建梯度增强代理模型，推导出可解析闭式表达式，融入改进框架并支持高效优化。

Result: 在标准BO基准上一致优于基线方法；成功应用于控制策略学习任务。

Conclusion: EI-GN通过梯度感知的改进机制，在理论一致性与实证性能上均展现出优势，拓展了贝叶斯优化在复杂优化与学习任务中的适用性。

Abstract: Bayesian Optimization (BO) is a principled approach for optimizing expensive black-box functions, with Expected Improvement (EI) being one of the most widely used acquisition functions. Despite its empirical success, EI is known to be overly exploitative and can converge to suboptimal stationary points. We propose Expected Improvement via Gradient Norms (EI-GN), a novel acquisition function that applies the improvement principle to a gradient-aware auxiliary objective, thereby promoting sampling in regions that are both high-performing and approaching first-order stationarity. EI-GN relies on gradient observations used to learn gradient-enhanced surrogate models that enable principled gradient inference from function evaluations. We derive a tractable closed-form expression for EI-GN that allows efficient optimization and show that the proposed acquisition is consistent with the improvement-based acquisition framework. Empirical evaluations on standard BO benchmarks demonstrate that EI-GN yields consistent improvements against standard baselines. We further demonstrate applicability of EI-GN to control policy learning problems.

</details>


### [40] [Graph-Free Root Cause Analysis](https://arxiv.org/abs/2601.21359)
*Luan Pham*

Main category: cs.LG

TL;DR: 本文提出PRISM框架，用于在缺乏依赖图的复杂系统中进行根因分析（RCA），通过理论建模与实验验证其高效性与高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有无依赖图的RCA方法常假设异常分最高的组件即为根因，但在故障传播场景下该假设失效（小延迟在下游累积为大异常），亟需更鲁棒的方法。

Method: 提出PRISM——一种简单高效的无依赖图RCA框架，并在一类组件化系统模型下给出理论保证。

Result: 在9个真实数据集共735次故障上，PRISM实现68% Top-1准确率，较最优基线提升258%，单次诊断仅需8ms。

Conclusion: PRISM在无需依赖图的前提下显著提升了RCA的准确性与效率，具备强实用性与理论支撑。

Abstract: Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.

</details>


### [41] [Perceptrons and localization of attention's mean-field landscape](https://arxiv.org/abs/2601.21366)
*Antonio Álvarez-López,Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: cs.LG

TL;DR: 本文将Transformer的前向传播建模为单位球面上的相互作用粒子系统，分析了感知机模块的影响，并证明其临界点通常是原子型且在球面子集上局部化。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer前向传播的几何与动力学本质，特别是层归一化和感知机模块在高维球面结构上的作用。

Method: 将Transformer建模为单位球面上的粒子系统，借助Wasserstein梯度流分析无限上下文长度下的均场极限，并研究感知机模块对临界点分布的影响。

Result: 证明在该模型下，感知机模块导致临界点通常是原子型（即离散分布）且局限于单位球面的特定子集上。

Conclusion: Transformer中感知机模块引入了强局域化和离散化效应，这为其表达能力和优化行为提供了几何解释。

Abstract: The forward pass of a Transformer can be seen as an interacting particle system on the unit sphere: time plays the role of layers, particles that of token embeddings, and the unit sphere idealizes layer normalization. In some weight settings the system can even be seen as a gradient flow for an explicit energy, and one can make sense of the infinite context length (mean-field) limit thanks to Wasserstein gradient flows. In this paper we study the effect of the perceptron block in this setting, and show that critical points are generically atomic and localized on subsets of the sphere.

</details>


### [42] [Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach](https://arxiv.org/abs/2601.21369)
*Yinlin Zhu,Di Wu,Xianzhi Zhang,Yuming Ai,Xunkai Li,Miao Hu,Guocong Quan*

Main category: cs.LG

TL;DR: 本文提出FedGALA框架，通过无监督对比学习在连续嵌入空间中对齐图神经网络（GNN）与冻结的预训练语言模型（PLM），并结合通信高效的提示调优机制，解决联邦图基础模型中的语义-结构正交性与完整性问题，显著提升多领域下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图基础模型（FedGFMs）采用向量量化导致不可逆知识损失，且难以兼顾语义-结构正交性、数据异构性与通信限制。

Method: 提出FedGALA框架：1）在连续嵌入空间中用无监督对比学习对齐GNN与冻结PLM；2）采用通信高效的提示调优机制适配下游任务，避免全参数微调。

Result: 在多领域数据集多个任务上全面超越基线方法，最高提升达14.37%。

Conclusion: 在连续空间中对齐GNN与PLM，并结合轻量提示调优，是构建高效、鲁棒FedGFMs的有效范式。

Abstract: Recent studies of federated graph foundational models (FedGFMs) break the idealized and untenable assumption of having centralized data storage to train graph foundation models, and accommodate the reality of distributed, privacy-restricted data silos. Despite their simplicity and intuition, existing studies that project aligned generalizable knowledge onto a discrete token space via vector-quantized backbones suffer from irreversible knowledge loss during the quantization process. In this context, we argue that reconciling the semantic-structural orthogonality and integrity between pre-trained language models (PLMs) and graph neural networks (GNNs) is paramount for developing effective FedGFMs while simultaneously mitigating the severe data heterogeneity and communication constraints inherent in distributed, resource-limited environments.
  To address these issues, we propose FedGALA (Federated Graph And Language Alignment), a framework that resolves graph-based semantic-structural orthogonality and integrity in federated settings by employing unsupervised contrastive learning to align GNNs and frozen PLMs within a continuous embedding space, thereby capturing robust, transferable general knowledge. Subsequently, FedGALA leverages a communication-efficient prompt tuning mechanism to steer these pre-aligned encoders and frozen PLMs, facilitating effective adaptation to diverse downstream tasks while circumventing the prohibitive overhead of full-parameter fine-tuning. The comprehensive experiments validate that FedGALA outperforms all competitive baselines across multi-domain datasets on multiple tasks with up to 14.37% performance improvement.

</details>


### [43] [DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy and Spearman-correlation for Multivariate Time-series Prediction](https://arxiv.org/abs/2601.21381)
*Tianhao Zhang,Shusen Ma,Yu Kang,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为DA-SPS的多变量时间序列预测模型，通过分别处理目标变量（TVPS）和外部变量（EVPS），结合SSA分解、LSTM、P-Conv-LSTM与L-Attention等模块，有效利用多源时序特征与变量间相关性，在多个公开及私有数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效建模外部变量对目标变量的影响，且难以充分挖掘序列中多种时间模式（如趋势、季节性）所蕴含的复杂信息。

Method: 提出DA-SPS模型，包含目标变量处理阶段（TVPS）和外部变量处理阶段（EVPS）：TVPS使用SSA分解目标序列，并分别用LSTM和P-Conv-LSTM提取趋势与季节性特征；EVPS通过Spearman相关性筛选强相关外部变量，并用L-Attention（LSTM+注意力机制）建模；最后加权融合两阶段输出。

Result: 在四个公开数据集上显著优于现有SOTA方法；在自建的笔记本主板测试项私有数据集上也验证了其实际有效性。

Conclusion: DA-SPS通过解耦建模目标变量与外部变量的异构特性，并融合多粒度时序模式，提升了多变量时间序列预测精度与鲁棒性。

Abstract: Multivariate time-series forecasting, as a typical problem in the field of time series prediction, has a wide range of applications in weather forecasting, traffic flow prediction, and other scenarios. However, existing works do not effectively consider the impact of extraneous variables on the prediction of the target variable. On the other hand, they fail to fully extract complex sequence information based on various time patterns of the sequences. To address these drawbacks, we propose a DA-SPS model, which adopts different modules for feature extraction based on the information characteristics of different variables. DA-SPS mainly consists of two stages: the target variable processing stage (TVPS) and the extraneous variables processing stage (EVPS). In TVPS, the model first uses Singular Spectrum Analysis (SSA) to process the target variable sequence and then uses Long Short-Term Memory (LSTM) and P-Conv-LSTM which deploys a patching strategy to extract features from trend and seasonality components, respectively. In EVPS, the model filters extraneous variables that have a strong correlation with the target variate by using Spearman correlation analysis and further analyses them using the L-Attention module which consists of LSTM and attention mechanism. Finally, the results obtained by TVPS and EVPS are combined through weighted summation and linear mapping to produce the final prediction. The results on four public datasets demonstrate that the DA-SPS model outperforms existing state-of-the-art methods. Additionally, its performance in real-world scenarios is further validated using a private dataset collected by ourselves, which contains the test items' information on laptop motherboards.

</details>


### [44] [Learning to Optimize Job Shop Scheduling Under Structural Uncertainty](https://arxiv.org/abs/2601.21389)
*Rui Zhang,Jianwei Niu,Xuefeng Liu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 本文提出UP-AAC方法，通过非对称架构（演员用随机状态、评论家用回溯重构的确定性状态）和注意力机制的不确定性感知模型（UPM），有效应对作业车间调度问题（JSSP）中的结构不确定性，提升调度稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注JSSP中参数不确定性（如加工时间变化），而忽视了更普遍的结构不确定性（如工序路径受不可预知情境因素影响），导致信用分配错误，影响调度性能。

Method: 提出UP-AAC方法：采用非对称actor-critic架构，actor输入随机状态，critic输入回溯重构的确定性状态；并设计基于注意力机制的不确定性感知模型（UPM）辅助调度决策。

Result: 在基准实例上，UP-AAC显著降低最大完工时间（makespan），性能优于现有方法。

Conclusion: UP-AAC通过解耦状态信息与改进信用分配，有效应对JSSP中的结构不确定性，提升了强化学习调度方法的鲁棒性与实用性。

Abstract: The Job-Shop Scheduling Problem (JSSP), under various forms of manufacturing uncertainty, has recently attracted considerable research attention. Most existing studies focus on parameter uncertainty, such as variable processing times, and typically adopt the actor-critic framework. In this paper, we explore a different but prevalent form of uncertainty in JSSP: structural uncertainty. Structural uncertainty arises when a job may follow one of several routing paths, and the selection is determined not by policy, but by situational factors (e.g., the quality of intermediate products) that cannot be known in advance. Existing methods struggle to address this challenge due to incorrect credit assignment: a high-quality action may be unfairly penalized if it is followed by a time-consuming path. To address this problem, we propose a novel method named UP-AAC. In contrast to conventional actor-critic methods, UP-AAC employs an asymmetric architecture. While its actor receives a standard stochastic state, the critic is crucially provided with a deterministic state reconstructed in hindsight. This design allows the critic to learn a more accurate value function, which in turn provides a lower-variance policy gradient to the actor, leading to more stable learning. In addition, we design an attention-based Uncertainty Perception Model (UPM) to enhance the actor's scheduling decisions. Extensive experiments demonstrate that our method outperforms existing approaches in reducing makespan on benchmark instances.

</details>


### [45] [ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation](https://arxiv.org/abs/2601.21420)
*Zihao Huang,Jundong Zhou,Xingwei Qu,Qiyang Min,Ge Zhang*

Main category: cs.LG

TL;DR: ConceptMoE提出一种动态语义合并机制，将相似token聚为概念表征，通过可学习分块模块压缩序列，实现隐式计算分配，在保持FLOPs和参数量不变前提下显著提升语言与多模态任务性能，并大幅降低注意力计算与KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型对所有token分配均匀计算，忽视了不同token在预测难度上的差异，导致计算资源浪费；需要一种能根据语义重要性动态分配计算的机制。

Method: 提出ConceptMoE：1）引入可学习chunk模块，基于token间语义相似性动态划分并压缩序列至目标压缩比R；2）将压缩后的概念表征输入MoE模型；3）严格控制对比实验条件（匹配baseline的FLOPs与参数量），以隔离架构增益。

Result: 在语言预训练、长上下文理解、多模态基准上分别提升+0.9、+2.3、+0.6点；持续训练中增益达+5.5点；R=2时，注意力计算减少4倍，KV缓存减半，prefill与decoding速度分别提升175%和117%。

Conclusion: ConceptMoE通过自适应概念级处理，在不增加计算与参数负担的前提下，同时提升了大模型的有效性与推理效率，且易于集成到现有MoE架构中。

Abstract: Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio $R$ before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to $R^2\times$ and KV cache by $R\times$. At $R=2$, empirical measurements show prefill speedups reaching 175\% and decoding speedups up to 117\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.

</details>


### [46] [Accurate Network Traffic Matrix Prediction via LEAD: an LLM-Enhanced Adapter-Based Conditional Diffusion Model](https://arxiv.org/abs/2601.21437)
*Yu Sun,Yaqiong Liu,Nan Cheng,Jiayuan Li,Zihan Jia,Xialin Du,Mugen Peng*

Main category: cs.LG

TL;DR: 本文提出LEAD模型，结合大语言模型（LLM）与条件扩散模型，通过‘流量转图像’范式和双条件策略提升网络流量矩阵（TM）预测精度与不确定性建模能力，在Abilene和GEANT数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 6G与AI原生边缘智能推动网络需具备预测性与风险感知的自适应能力；而现有TM预测方法因过平滑、缺乏不确定性建模，在突发流量下保真度差。

Method: 提出LEAD：1）‘Traffic-to-Image’将TM转为RGB图像，利用视觉骨干建模全局依赖；2）冻结LLM+可训练Adapter高效捕获时序语义；3）双条件策略引导扩散模型生成动态TM。

Result: 在Abilene数据集上，20步预测RMSE为0.1134（较最优基线下降45.2%）；在GEANT上20步RMSE为0.0258（下降27.3%），误差随步数增长极小。

Conclusion: LEAD有效克服了传统判别式模型在TM预测中的过平滑与不确定性缺失问题，实现了高精度、鲁棒且可扩展的多步流量预测，适用于严苛时延与算力约束的6G边缘智能场景。

Abstract: Driven by the evolution toward 6G and AI-native edge intelligence, network operations increasingly require predictive and risk-aware adaptation under stringent computation and latency constraints. Network Traffic Matrix (TM), which characterizes flow volumes between nodes, is a fundamental signal for proactive traffic engineering. However, accurate TM forecasting remains challenging due to the stochastic, non-linear, and bursty nature of network dynamics. Existing discriminative models often suffer from over-smoothing and provide limited uncertainty awareness, leading to poor fidelity under extreme bursts. To address these limitations, we propose LEAD, a Large Language Model (LLM)-Enhanced Adapter-based conditional Diffusion model. First, LEAD adopts a "Traffic-to-Image" paradigm to transform traffic matrices into RGB images, enabling global dependency modeling via vision backbones. Then, we design a "Frozen LLM with Trainable Adapter" model, which efficiently captures temporal semantics with limited computational cost. Moreover, we propose a Dual-Conditioning Strategy to precisely guide a diffusion model to generate complex, dynamic network traffic matrices. Experiments on the Abilene and GEANT datasets demonstrate that LEAD outperforms all baselines. On the Abilene dataset, LEAD attains a remarkable 45.2% reduction in RMSE against the best baseline, with the error margin rising only marginally from 0.1098 at one-step to 0.1134 at 20-step predictions. Meanwhile, on the GEANT dataset, LEAD achieves a 0.0258 RMSE at 20-step prediction horizon which is 27.3% lower than the best baseline.

</details>


### [47] [Synthetic Pattern Generation and Detection of Financial Activities using Graph Autoencoders](https://arxiv.org/abs/2601.21446)
*Francesco Zola,Lucia Muñoz,Andrea Venturi,Amaia Gil*

Main category: cs.LG

TL;DR: 本文研究了图自编码器（GAE）在合成数据上学习和区分模拟洗钱行为的拓扑模式的能力，通过生成七种非法活动模式的合成图数据并训练三种GAE变体（GAE-GCN、GAE-SAGE、GAE-GAT），发现GAE-GCN在多数模式中重建性能最稳定，表明基于合成图数据的表示学习可为金融非法行为检测提供可行AI方案。


<details>
  <summary>Details</summary>
Motivation: 由于真实金融交易数据标签稀缺且隐私约束严格，自动检测洗钱等非法金融活动的拓扑模式面临挑战。

Method: 构建七种典型非法活动模式的参数化合成图生成器；分别训练三种基于不同图卷积层（GCN、GraphSAGE、GAT）的无监督图自编码器（GAE），仅依赖重构误差评估其对结构的学习能力。

Result: GAE-GCN在所有七种模式上展现出最一致的重构性能；GAE-SAGE和GAE-GAT仅在少数特定模式中表现具竞争力。

Conclusion: 基于合成图数据的图表示学习是开发AI驱动金融非法行为检测工具的有效路径，可缓解真实金融数据受限问题。

Abstract: Illicit financial activities such as money laundering often manifest through recurrent topological patterns in transaction networks. Detecting these patterns automatically remains challenging due to the scarcity of labeled real-world data and strict privacy constraints. To address this, we investigate whether Graph Autoencoders (GAEs) can effectively learn and distinguish topological patterns that mimic money laundering operations when trained on synthetic data. The analysis consists of two phases: (i) data generation, where synthetic samples are created for seven well-known illicit activity patterns using parametrized generators that preserve structural consistency while introducing realistic variability; and (ii) model training and validation, where separate GAEs are trained on each pattern without explicit labels, relying solely on reconstruction error as an indicator of learned structure. We compare three GAE implementations based on three distinct convolutional layers: Graph Convolutional (GAE-GCN), GraphSAGE (GAE-SAGE), and Graph Attention Network (GAE-GAT). Experimental results show that GAE-GCN achieves the most consistent reconstruction performance across patterns, while GAE-SAGE and GAE-GAT exhibit competitive results only in few specific patterns. These findings suggest that graph-based representation learning on synthetic data provides a viable path toward developing AI-driven tools for detecting illicit behaviors, overcoming the limitations of financial datasets.

</details>


### [48] [Partial Feedback Online Learning](https://arxiv.org/abs/2601.21462)
*Shihao Shao,Cong Fang,Zhouchen Lin,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文研究了部分反馈在线学习问题，提出了新的复杂度维度PFLdim和PMSdim，分别刻画确定性和随机性学习者的极小极大遗憾，并揭示了在集合可实现性之外问题可能变得信息论不可解。


<details>
  <summary>Details</summary>
Motivation: 部分反馈在线学习建模了如语言生成等场景，其中多个标签可能都正确，但数据仅提供一个参考答案；现有理论无法充分刻画该设定下的学习能力与遗憾界。

Method: 引入Partial-Feedback Littlestone dimension（PFLdim）刻画确定性学习者，采用新构型的版本空间与辅助维度；为随机学习者定义Partial-Feedback Measure Shattering dimension（PMSdim）；分析Helly数、嵌套包含结构等条件对确定性与随机性可学习性分离的影响；扩展至集合值在线学习并解决Raman et al. [2024b] 的开放问题。

Result: PFLdim精确刻画确定性学习者的可学习性与极小极大遗憾；PMSdim给出随机学习者的紧遗憾界；发现多种条件下确定性与随机性可学习性不可分离；在非集合可实现情形下，即使假设类大小仅为2，也可能出现线性遗憾，表明需新噪声敏感复杂度度量。

Conclusion: 集合可实现性是部分反馈在线学习中可学习性的关键分水岭；PFLdim和PMSdim是刻画该模型本质复杂度的基础工具；脱离该假设将导致根本性困难，亟需发展新型复杂度理论。

Abstract: We study partial-feedback online learning, where each instance admits a set of correct labels, but the learner only observes one correct label per round; any prediction within the correct set is counted as correct. This model captures settings such as language generation, where multiple responses may be valid but data provide only a single reference. We give a near-complete characterization of minimax regret for both deterministic and randomized learners in the set-realizable regime, i.e., in the regime where sublinear regret is generally attainable. For deterministic learners, we introduce the Partial-Feedback Littlestone dimension (PFLdim) and show it precisely governs learnability and minimax regret; technically, PFLdim cannot be defined via the standard version space, requiring a new collection version space viewpoint and an auxiliary dimension used only in the proof. We further develop the Partial-Feedback Measure Shattering dimension (PMSdim) to obtain tight bounds for randomized learners. We identify broad conditions ensuring inseparability between deterministic and randomized learnability (e.g., finite Helly number or nested-inclusion label structure), and extend the argument to set-valued online learning, resolving an open question of Raman et al. [2024b]. Finally, we show a sharp separation from weaker realistic and agnostic variants: outside set realizability, the problem can become information-theoretically intractable, with linear regret possible even for $|H|=2$. This highlights the need for fundamentally new, noise-sensitive complexity measures to meaningfully characterize learnability beyond set realizability.

</details>


### [49] [A block-coordinate descent framework for non-convex composite optimization. Application to sparse precision matrix estimation](https://arxiv.org/abs/2601.21467)
*Guillaume Lauga*

Main category: cs.LG

TL;DR: 本文提出了一种新的块坐标下降（BCD）框架，用于求解非凸复合优化问题，保证目标函数下降和收敛性，并涵盖多种更新策略，应用于稀疏精度矩阵估计问题（如Graphical Lasso），显著减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 块坐标下降（BCD）在大规模优化中广泛应用，但其在非凸优化中的理论研究仍不足，尤其缺乏对收敛性和通用性的系统分析。

Method: 提出一种通用的块坐标下降（BCD）框架，支持变度量近端梯度、近端牛顿及交替最小化等多种更新方式，并将其应用于稀疏精度矩阵估计问题。

Result: 该框架为Graphical Lasso的三种主流求解器（graphical ISTA、Primal GLasso、QUIC）提供了统一理论基础，并在实验中实现最高100倍的迭代次数减少，同时保持最先进估计质量。

Conclusion: 所提出的BCD框架不仅具有强理论保证（目标下降与收敛性），而且具备高度通用性与实用性，显著提升了非凸稀疏精度矩阵估计的效率与可靠性。

Abstract: Block-coordinate descent (BCD) is the method of choice to solve numerous large scale optimization problems, however their theoretical study for non-convex optimization, has received less attention. In this paper, we present a new block-coordinate descent (BCD) framework to tackle non-convex composite optimization problems, ensuring decrease of the objective function and convergence to a solution. This framework is general enough to include variable metric proximal gradient updates, proximal Newton updates, and alternated minimization updates. This generality allows to encompass three versions of the most used solvers in the sparse precision matrix estimation problem, deemed Graphical Lasso: graphical ISTA, Primal GLasso, and QUIC. We demonstrate the value of this new framework on non-convex sparse precision matrix estimation problems, providing convergence guarantees and up to a $100$-fold reduction in the number of iterations required to reach state-of-the-art estimation quality.

</details>


### [50] [PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization](https://arxiv.org/abs/2601.21470)
*Ruicheng Ao,Hongyu Chen,Haoyang Liu,David Simchi-Levi,Will Wei Sun*

Main category: cs.LG

TL;DR: 本文提出PPI-SVRG算法，统一了基于预测（PPI）和基于参考梯度（SVRG）的方差缩减方法，理论分析表明其收敛性依赖于损失函数几何性质，预测质量仅影响最终收敛邻域大小；实验验证其在标签稀缺场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺但预训练模型预测可用的半监督随机优化场景中，如何有效利用预测信息提升优化效率和稳定性。

Method: 证明PPI与SVRG在数学上等价，并据此提出融合二者优势的PPI-SVRG算法；理论分析其收敛界，分解为标准SVRG速率项与由预测不确定性引起的误差下界项。

Result: 理论表明：收敛速率仅取决于损失函数几何性质，预测质量仅影响最终收敛邻域大小；预测完美时退化为SVRG；预测变差时仍稳定收敛但邻域增大；实验显示在均值估计基准上MSE降低43–52%，MNIST仅用10%标注数据时测试准确率提升2.7–2.9个百分点。

Conclusion: PPI与SVRG本质等价，PPI-SVRG是一种鲁棒且高效的半监督优化方法，能充分利用预训练模型预测，在标注稀缺条件下显著提升优化性能。

Abstract: We study semi-supervised stochastic optimization when labeled data is scarce but predictions from pre-trained models are available. PPI and SVRG both reduce variance through control variates -- PPI uses predictions, SVRG uses reference gradients. We show they are mathematically equivalent and develop PPI-SVRG, which combines both. Our convergence bound decomposes into the standard SVRG rate plus an error floor from prediction uncertainty. The rate depends only on loss geometry; predictions affect only the neighborhood size. When predictions are perfect, we recover SVRG exactly. When predictions degrade, convergence remains stable but reaches a larger neighborhood. Experiments confirm the theory: PPI-SVRG reduces MSE by 43--52\% under label scarcity on mean estimation benchmarks and improves test accuracy by 2.7--2.9 percentage points on MNIST with only 10\% labeled data.

</details>


### [51] [Best Arm Identification with LLM Judges and Limited Human](https://arxiv.org/abs/2601.21471)
*Ruicheng Ao,Hongyu Chen,Siyang Gao,Hanwei Li,David Simchi-Levi*

Main category: cs.LG

TL;DR: 本文研究了在固定置信度下进行最佳臂识别（BAI）的问题，其中存在一个廉价但可能存在偏差的代理（如大语言模型评判器），以及一个昂贵但准确的人工标注真值。作者提出了一种结合代理评分与逆倾向加权残差的新估计器，并设计了自适应审计算法，在保证准确性的同时提高了审计效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理代理偏差和选择性观测真值的问题，导致错误选择最佳臂或资源浪费。

Method: 提出一种结合代理评分与逆倾向加权残差的估计器，并构建任意时刻有效的置信序列；基于此设计自适应审计算法，聚焦于不可靠上下文和相近臂，并采用插件Neyman规则实现近似最优审计效率。

Result: 理论证明所提算法能实现近似最优审计效率，且误选概率随样本增加趋于零；数值实验验证了其优于基线方法的性能。

Conclusion: 通过偏差校正与倾向调整，可显著提升多保真度BAI在代理有偏、真值选择性观测场景下的准确性与审计效率。

Abstract: We study fixed-confidence best-arm identification (BAI) where a cheap but potentially biased proxy (e.g., LLM judge) is available for every sample, while an expensive ground-truth label can only be acquired selectively when using a human for auditing. Unlike classical multi-fidelity BAI, the proxy is biased (arm- and context-dependent) and ground truth is selectively observed. Consequently, standard multi-fidelity methods can mis-select the best arm, and uniform auditing, though accurate, wastes scarce resources and is inefficient. We prove that without bias correction and propensity adjustment, mis-selection probability may not vanish (even with unlimited proxy data). We then develop an estimator for the mean of each arm that combines proxy scores with inverse-propensity-weighted residuals and form anytime-valid confidence sequences for that estimator. Based on the estimator and confidence sequence, we propose an algorithm that adaptively selects and audits arms. The algorithm concentrates audits on unreliable contexts and close arms and we prove that a plug-in Neyman rule achieves near-oracle audit efficiency. Numerical experiments confirm the theoretical guarantees and demonstrate the superior empirical performance of the proposed algorithm.

</details>


### [52] [ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment](https://arxiv.org/abs/2601.21484)
*Xiuyu Li,Jinkai Zhang,Mingyang Yi,Yu Li,Longqiang Wang,Yue Wang,Ju Fan*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的推理方法Energy-Guided Test-Time Scaling（ETS），通过在线蒙特卡洛估计能量项，直接从最优强化学习策略中采样，显著提升掩码语言建模等任务的生成质量，同时保证效率与理论收敛性。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对语言模型进行对齐虽有效，但训练过程复杂、成本高且不稳定，亟需一种更高效稳定的替代方案。

Method: 提出训练无关的推理算法ETS，将掩码语言建模的转移概率建模为参考策略模型与能量项的组合，并利用在线蒙特卡洛估计能量项；结合现代加速框架与定制重要性采样估计器以降低延迟并保证采样质量。

Result: 在推理、编程和科学等多类基准上，ETS在各类MLM模型（包括自回归与扩散语言模型）上均一致提升生成质量，验证了其有效性、效率与理论保障。

Conclusion: ETS是一种高效、稳定、无需训练的推理时对齐方法，为RL对齐提供了实用新范式。

Abstract: Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.

</details>


### [53] [Task-Awareness Improves LLM Generations and Uncertainty](https://arxiv.org/abs/2601.21500)
*Tim Tomov,Dominik Fuchsgruber,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出了一种基于任务依赖潜在结构的决策理论框架，直接在结构空间中建模大语言模型（LLM）输出，通过贝叶斯最优合成与结构感知不确定性估计提升响应质量与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM解码与不确定性估计方法仅在自然语言空间操作，忽视了响应背后固有的结构信息（如离散标签、数值、图等）。

Method: 将LLM输出映射到任务相关的潜在结构空间，并引入该空间上的差异性度量；在此基础上计算贝叶斯最优响应（非采样选择，而是结构空间中合成），并用贝叶斯风险量化结构感知的不确定性。

Result: 在多个任务上，贝叶斯最优响应持续优于束搜索等标准解码方法；结构感知的不确定性估计更准确反映输出质量与正确性。

Conclusion: 该框架通用、任务感知且可靠，适用于任何具有潜在响应结构的LLM下游任务。

Abstract: In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.

</details>


### [54] [Cascaded Transfer: Learning Many Tasks under Budget Constraints](https://arxiv.org/abs/2601.21513)
*Eloi Campagne,Yvenn Amara-Ouali,Yannig Goude,Mathilde Mougeot,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文提出了一种名为级联迁移学习（Cascaded Transfer Learning）的新范式，用于解决多任务学习中大量相关任务关系未知的问题。该方法通过构建以任务为节点的最小生成树，按层次结构级联传递模型参数，并在预算约束下分配训练资源，从而提升大规模任务集合中的准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决多任务学习中任务间关系未知、且需在有限预算下高效学习大量相关任务的问题。

Method: 设计基于最小生成树的级联迁移机制，将任务组织为有根树结构，按顺序学习和精调各任务模型，并沿树分支分配训练预算。

Result: 在合成数据和真实多任务场景实验中，该方法相比其他方法实现了更准确、更具成本效益的大规模任务适应。

Conclusion: 级联迁移学习是一种有效的多任务迁移范式，能在未知任务关系和预算约束下提升模型泛化与资源利用效率。

Abstract: Many-Task Learning refers to the setting where a large number of related tasks need to be learned, the exact relationships between tasks are not known. We introduce the Cascaded Transfer Learning, a novel many-task transfer learning paradigm where information (e.g. model parameters) cascades hierarchically through tasks that are learned by individual models of the same class, while respecting given budget constraints. The cascade is organized as a rooted tree that specifies the order in which tasks are learned and refined. We design a cascaded transfer mechanism deployed over a minimum spanning tree structure that connects the tasks according to a suitable distance measure, and allocates the available training budget along its branches. Experiments on synthetic and real many-task settings show that the resulting method enables more accurate and cost effective adaptation across large task collections compared to alternative approaches.

</details>


### [55] [A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings](https://arxiv.org/abs/2601.21521)
*Chi-Sheng Chen,En-Jui Kuo,Guan-Ying Chen,Xinyu Zhang,Fan Zhang*

Main category: cs.LG

TL;DR: 本文研究了EEG信号空间协方差矩阵（SPD）在黎曼流形上的嵌入几何与优化动力学之间的理论联系，提出了BWSPD嵌入方法，在梯度条件、数值稳定性与距离保持性方面取得理论突破，并通过大规模实验验证其在多种EEG范式下的有效性。


<details>
  <summary>Details</summary>
Motivation: SPD矩阵天然位于黎曼流形上，但现有工作缺乏对嵌入几何选择如何影响梯度优化行为（如梯度条件数、数值稳定性）的理论分析。

Method: 基于Daleckii-Kreĭn矩阵理论分析梯度条件数；提出Embedding-Space Batch Normalization（BN-Embed）近似黎曼归一化；推导bi-Lipschitz界以刻画BWSPD嵌入的距离保持性；在统一Transformer框架下对比BWSPD、Log-Euclidean和Euclidean嵌入，进行1500+次跨范式（MI/ERP/SSVEP）、36被试的大规模实验。

Result: （1）BWSPD实现√κ梯度条件数（优于Log-Euclidean的κ），在高维（d≥22）更优；（2）BN-Embed在56通道ERP数据上提升26%准确率，但在8通道SSVEP上无效，符合理论预测；（3）BWSPD满足bi-Lipschitz性质，距离失真仅取决于条件比κ；Log-Euclidean Transformer在所有数据集上达到SOTA性能。

Conclusion: 嵌入几何深刻影响SPD流形上的优化行为；Log-Euclidean嵌入在实践中综合最优；BWSPD在理论保障（如距离保持、高维梯度改善）和实际性能间取得良好平衡；为SPD深度学习提供了可解释、可验证的几何-优化联合分析范式。

Abstract: Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \geq 22$), with this advantage reducing on low-dimensional inputs ($d \leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\varepsilon^2)$ error, yielding $+26\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.

</details>


### [56] [Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21523)
*Bang Giang Le,Viet Cuong Ta*

Main category: cs.LG

TL;DR: 本文提出了一种结合全局奖励与局部奖励优势的多智能体强化学习方法，通过构建智能体交互图实现更细粒度的贡献分配，并设计了实用的图近似方法，实验表明该方法优于传统设置。


<details>
  <summary>Details</summary>
Motivation: 全局奖励存在噪声、信用分配困难；局部奖励虽学习快但易陷入局部最优，需兼顾二者优势。

Method: 基于智能体交互图进行细粒度个体贡献辨识，同时缓解局部奖励导致的合作问题，并提出交互图的实用近似方法。

Result: 实验验证了该方法的灵活性，在多个任务上均优于传统局部奖励和全局奖励设置。

Conclusion: 融合局部与全局奖励思想、借助交互图建模智能体关系，可有效提升多智能体协作性能。

Abstract: To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.

</details>


### [57] [Fast and Geometrically Grounded Lorentz Neural Networks](https://arxiv.org/abs/2601.21529)
*Robert van der Klis,Ricardo Chávez Torres,Max van Spengler,Yuhui Ding,Thomas Hofmann,Pascal Mettes*

Main category: cs.LG

TL;DR: 本文提出了一种新的洛伦兹线性层，解决了现有超曲面神经网络中输出双曲范数随梯度下降步数对数增长的问题，使其恢复线性增长，并通过洛伦兹激活函数和缓存策略提升了计算效率，使超曲面神经网络在保持几何一致性的同时接近欧氏网络的计算性能。


<details>
  <summary>Details</summary>
Motivation: 超曲面空间在层次化与鲁棒表征学习中展现出潜力，但现有洛伦兹模型下的超曲面神经网络存在输出双曲范数随训练步数对数增长的问题，削弱了超曲面几何的核心优势。

Method: 基于‘到超平面距离’的经典几何思想，提出新的洛伦兹线性层；引入洛伦兹激活函数与新型缓存策略以提升计算效率。

Result: 新洛伦兹线性层保证输出双曲范数随梯度下降步数线性增长；整体网络严格遵循超曲面几何，同时计算开销接近欧氏神经网络。

Conclusion: 本文从几何原理出发重构了超曲面线性变换，不仅修复了理论缺陷，还实现了高效、几何一致的超曲面深度学习架构。

Abstract: Hyperbolic space is quickly gaining traction as a promising geometry for hierarchical and robust representation learning. A core open challenge is the development of a mathematical formulation of hyperbolic neural networks that is both efficient and captures the key properties of hyperbolic space. The Lorentz model of hyperbolic space has been shown to enable both fast forward and backward propagation. However, we prove that, with the current formulation of Lorentz linear layers, the hyperbolic norms of the outputs scale logarithmically with the number of gradient descent steps, nullifying the key advantage of hyperbolic geometry. We propose a new Lorentz linear layer grounded in the well-known ``distance-to-hyperplane" formulation. We prove that our formulation results in the usual linear scaling of output hyperbolic norms with respect to the number of gradient descent steps. Our new formulation, together with further algorithmic efficiencies through Lorentzian activation functions and a new caching strategy results in neural networks fully abiding by hyperbolic geometry while simultaneously bridging the computation gap to Euclidean neural networks. Code available at: https://github.com/robertdvdk/hyperbolic-fully-connected.

</details>


### [58] [HistoPrism: Unlocking Functional Pathway Analysis from Pan-Cancer Histology via Gene Expression Prediction](https://arxiv.org/abs/2601.21560)
*Susu Hu,Qinghe Zeng,Nithya Bhasker,Jakob Nicolas Kather,Stefanie Speidel*

Main category: cs.LG

TL;DR: HistoPrism是一种高效的基于Transformer的模型，用于跨癌种从H&E组织病理图像预测基因表达，并通过通路水平基准评估其生物学意义，显著提升了通路层面预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于单癌种设置且仅以基因变异为评估指标，缺乏对功能相关性和跨癌种泛化能力的探索，难以满足临床实际需求。

Method: 提出HistoPrism——一种基于Transformer的高效架构，支持泛癌种基因表达预测；并构建通路水平基准，从孤立基因转向功能通路进行评估。

Result: HistoPrism在高变异性基因预测上超越现有最优模型，在通路水平预测上取得显著提升，验证了其捕获生物学一致转录模式的能力。

Conclusion: HistoPrism具备强泛癌种泛化性与高效率，为利用常规组织病理图像建模转录组提供了新标准，推动临床转化应用。

Abstract: Predicting spatial gene expression from H&E histology offers a scalable and clinically accessible alternative to sequencing, but realizing clinical impact requires models that generalize across cancer types and capture biologically coherent signals. Prior work is often limited to per-cancer settings and variance-based evaluation, leaving functional relevance underexplored. We introduce HistoPrism, an efficient transformer-based architecture for pan-cancer prediction of gene expression from histology. To evaluate biological meaning, we introduce a pathway-level benchmark, shifting assessment from isolated gene-level variance to coherent functional pathways. HistoPrism not only surpasses prior state-of-the-art models on highly variable genes , but also more importantly, achieves substantial gains on pathway-level prediction, demonstrating its ability to recover biologically coherent transcriptomic patterns. With strong pan-cancer generalization and improved efficiency, HistoPrism establishes a new standard for clinically relevant transcriptomic modeling from routinely available histology.

</details>


### [59] [Representation Unlearning: Forgetting through Information Compression](https://arxiv.org/abs/2601.21564)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 本文提出了一种名为Representation Unlearning的新框架，通过在模型表征空间中直接进行信息瓶颈式变换来实现高效、稳定且实用的机器遗忘，优于现有参数修改方法。


<details>
  <summary>Details</summary>
Motivation: 隐私法规和鲁棒性需求驱动了机器遗忘研究，但现有基于参数修改的方法存在不稳定、计算昂贵和局部近似受限等问题。

Method: 提出Representation Unlearning框架，在模型表征空间学习一个变换，通过信息瓶颈机制最大化对保留数据的互信息、同时抑制对需遗忘数据的信息；推导了可计算的变分代理目标，并支持全监督（retain+forget数据可用）与零样本（仅forget数据可用）两种设定。

Result: 在多个基准实验中，该方法实现了更可靠的遗忘效果、更好的效用保持能力以及更高的计算效率，显著优于参数中心的基线方法。

Conclusion: Representation Unlearning为机器遗忘提供了一种更本质、更高效、更实用的范式，突破了传统参数更新路径的局限。

Abstract: Machine unlearning seeks to remove the influence of specific training data from a model, a need driven by privacy regulations and robustness concerns. Existing approaches typically modify model parameters, but such updates can be unstable, computationally costly, and limited by local approximations. We introduce Representation Unlearning, a framework that performs unlearning directly in the model's representation space. Instead of modifying model parameters, we learn a transformation over representations that imposes an information bottleneck: maximizing mutual information with retained data while suppressing information about data to be forgotten. We derive variational surrogates that make this objective tractable and show how they can be instantiated in two practical regimes: when both retain and forget data are available, and in a zero-shot setting where only forget data can be accessed. Experiments across several benchmarks demonstrate that Representation Unlearning achieves more reliable forgetting, better utility retention, and greater computational efficiency than parameter-centric baselines.

</details>


### [60] [FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions](https://arxiv.org/abs/2601.21567)
*Yutao Jin,Yuang Tao,Junyong Zhai*

Main category: cs.LG

TL;DR: 本文提出了FlexCausal框架，通过块对角协方差变分自编码器与因子化流式先验，改进因果解耦表征学习，更真实地建模外生噪声的复杂分布，并结合监督对齐与反事实一致性约束，提升因果结构学习精度与生成保真度。


<details>
  <summary>Details</summary>
Motivation: 现有解耦方法依赖均场近似（对角后验协方差）导致潜在维度被强制去相关，且各向同性高斯先验无法刻画现实因果因子中常见的复杂、非高斯统计特性。

Method: 提出FlexCausal：1）基于块对角协方差VAE放松独立性假设；2）引入因子化流式先验建模外生噪声的复杂密度；3）融合监督对齐目标与反事实一致性约束；4）设计流形感知的相对干预策略以提升生成保真度。

Result: 在合成与真实数据集上，FlexCausal显著优于现有方法，在因果结构恢复精度、解耦质量与反事实生成保真度方面均有提升。

Conclusion: FlexCausal通过更灵活的后验建模、更真实的先验设计及结构化学习约束，有效提升了因果解耦表征学习的表达能力与可解释性，为从观测数据中可靠推断因果机制提供了新路径。

Abstract: Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.

</details>


### [61] [Bridging Functional and Representational Similarity via Usable Information](https://arxiv.org/abs/2601.21568)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 本文提出了一种基于“可用信息”（usable information）的统一框架，用于量化表征间的相似性，并从功能相似性、表征相似性及其关系三个维度进行理论与实证分析。


<details>
  <summary>Details</summary>
Motivation: 现有表征相似性度量方法缺乏统一理论基础，且功能相似性与表征相似性的关系不清，亟需一个能整合 stitching、重建指标（如CKA、RSA）并揭示其信息论本质的框架。

Method: 基于条件互信息建立stitching性能与功能相似性的形式化联系；证明重建类指标和CKA/RSA在特定条件下是可用信息的估计量；通过任务粒度层次结构分析相似性的传递性与充分/必要性。

Result: 1) Stitching具有内在不对称性，需双向分析；2) 表征相似性度量本质上依赖预测器族容量，是相对的；3) 表征相似性足以保证功能相似性，但非必要；4) 提出任务粒度层次：输入重建是表征相似性的最大粒度极限。

Conclusion: 可用信息为表征相似性提供了统一的信息论基础，揭示了功能与表征相似性之间的层次化、相对性与非对称性本质，推动了可解释AI与表征学习的理论发展。

Abstract: We present a unified framework for quantifying the similarity between representations through the lens of \textit{usable information}, offering a rigorous theoretical and empirical synthesis across three key dimensions. First, addressing functional similarity, we establish a formal link between stitching performance and conditional mutual information. We further reveal that stitching is inherently asymmetric, demonstrating that robust functional comparison necessitates a bidirectional analysis rather than a unidirectional mapping. Second, concerning representational similarity, we prove that reconstruction-based metrics and standard tools (e.g., CKA, RSA) act as estimators of usable information under specific constraints. Crucially, we show that similarity is relative to the capacity of the predictive family: representations that appear distinct to a rigid observer may be identical to a more expressive one. Third, we demonstrate that representational similarity is sufficient but not necessary for functional similarity. We unify these concepts through a task-granularity hierarchy: similarity on a complex task guarantees similarity on any coarser derivative, establishing representational similarity as the limit of maximum granularity: input reconstruction.

</details>


### [62] [Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity](https://arxiv.org/abs/2601.21577)
*Mutian Yang,Zisen Zhan,Yutong Chen,Haolin Li,Kaiwen Wang,Kaili Zheng,Yuguang Wang,Qi Wang,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度相似性的理论框架来解释大语言模型知识注入中的灾难性遗忘，并据此设计了协作神经学习（CNL）方法，通过冻结冲突神经元、仅更新协作神经元，在理论上消除灾难性遗忘，并在实验中显著降低遗忘率。


<details>
  <summary>Details</summary>
Motivation: 现有缓解灾难性遗忘的方法缺乏基础理论解释，本文旨在建立梯度层面的理论框架以揭示其根本成因。

Method: 构建梯度相似性理论框架，证明强负梯度相似性是遗忘的根本原因；识别冲突神经元与协作神经元；提出CNL方法——冻结冲突神经元、仅更新协作神经元。

Result: CNL在集合内设置下实现零遗忘，在集合外设置下遗忘率降低59.1%–81.7%；实验覆盖5个LLM、4个数据集和4种优化器。

Conclusion: 梯度相似性是理解灾难性遗忘的关键，CNL方法在理论和实验上均有效缓解甚至消除该问题，为持续学习提供了新范式。

Abstract: Catastrophic forgetting during knowledge injection severely undermines the continual learning capability of large language models (LLMs). Although existing methods attempt to mitigate this issue, they often lack a foundational theoretical explanation. We establish a gradient-based theoretical framework to explain catastrophic forgetting. We first prove that strongly negative gradient similarity is a fundamental cause of forgetting. We then use gradient similarity to identify two types of neurons: conflicting neurons that induce forgetting and account for 50%-75% of neurons, and collaborative neurons that mitigate forgetting and account for 25%-50%. Based on this analysis, we propose a knowledge injection method, Collaborative Neural Learning (CNL). By freezing conflicting neurons and updating only collaborative neurons, CNL theoretically eliminates catastrophic forgetting under an infinitesimal learning rate eta and an exactly known mastered set. Experiments on five LLMs, four datasets, and four optimizers show that CNL achieves zero forgetting in in-set settings and reduces forgetting by 59.1%-81.7% in out-of-set settings.

</details>


### [63] [Evaluating Prediction Uncertainty Estimates from BatchEnsemble](https://arxiv.org/abs/2601.21581)
*Morten Blørstad,Herman Jangsett Mostein,Nello Blaser,Pekka Parviainen*

Main category: cs.LG

TL;DR: This paper proposes BatchEnsemble and its extension GRUBE for efficient and scalable uncertainty estimation in deep learning, demonstrating competitive performance with fewer parameters and lower computational cost compared to deep ensembles and Monte Carlo dropout.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often lack reliable uncertainty estimation; existing methods are either computationally expensive or underestimate uncertainty.

Method: The authors adopt BatchEnsemble for uncertainty estimation and extend it to sequential modeling by proposing GRUBE—a BatchEnsemble variant of the GRU cell—then compare it with Monte Carlo dropout and deep ensembles on tabular and time series tasks.

Result: BatchEnsemble matches deep ensembles in uncertainty estimation and outperforms Monte Carlo dropout; GRUBE achieves comparable or better prediction and uncertainty estimation performance.

Conclusion: BatchEnsemble and GRUBE provide efficient, scalable, and high-performing alternatives to traditional ensemble methods for uncertainty estimation.

Abstract: Deep learning models struggle with uncertainty estimation. Many approaches are either computationally infeasible or underestimate uncertainty. We investigate \textit{BatchEnsemble} as a general and scalable method for uncertainty estimation across both tabular and time series tasks. To extend BatchEnsemble to sequential modeling, we introduce GRUBE, a novel BatchEnsemble GRU cell. We compare the BatchEnsemble to Monte Carlo dropout and deep ensemble models. Our results show that BatchEnsemble matches the uncertainty estimation performance of deep ensembles, and clearly outperforms Monte Carlo dropout. GRUBE achieves similar or better performance in both prediction and uncertainty estimation. These findings show that BatchEnsemble and GRUBE achieve similar performance with fewer parameters and reduced training and inference time compared to traditional ensembles.

</details>


### [64] [CORDS: Continuous Representations of Discrete Structures](https://arxiv.org/abs/2601.21583)
*Tin Hadži Veljković,Erik Bekkers,Michael Tiemann,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: CORDS是一种将可变大小集合预测转化为连续推理问题的新方法，通过可逆映射将离散对象集转换为密度场和特征场，在保持精确可解码性的同时实现对未知集合大小的鲁棒处理。


<details>
  <summary>Details</summary>
Motivation: 许多学习任务需预测大小未知的集合（如目标检测、分子建模、天体源探测），而现有方法依赖填充表示或显式推断集合大小，存在挑战。

Method: 提出CORDS方法，构建从离散对象集到连续密度场（编码位置与数量）和特征场（携带属性）的可逆映射，模型在场空间中操作并保证精确还原为离散集合。

Result: 在分子生成与回归、目标检测、基于仿真的推理及局部极大值恢复等任务上验证了CORDS对未知集合大小的鲁棒性与竞争性精度。

Conclusion: CORDS为变量集预测提供了一种通用、可逆、连续的建模范式，兼顾建模灵活性与离散语义保真性。

Abstract: Many learning problems require predicting sets of objects when the number of objects is not known beforehand. Examples include object detection, molecular modeling, and scientific inference tasks such as astrophysical source detection. Existing methods often rely on padded representations or must explicitly infer the set size, which often poses challenges. We present a novel strategy for addressing this challenge by casting prediction of variable-sized sets as a continuous inference problem. Our approach, CORDS (Continuous Representations of Discrete Structures), provides an invertible mapping that transforms a set of spatial objects into continuous fields: a density field that encodes object locations and count, and a feature field that carries their attributes over the same support. Because the mapping is invertible, models operate entirely in field space while remaining exactly decodable to discrete sets. We evaluate CORDS across molecular generation and regression, object detection, simulation-based inference, and a mathematical task involving recovery of local maxima, demonstrating robust handling of unknown set sizes with competitive accuracy.

</details>


### [65] [Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning](https://arxiv.org/abs/2601.21589)
*Wentao Yu,Sheng Wan,Shuo Chen,Bo Han,Chen Gong*

Main category: cs.LG

TL;DR: 本文提出FedSSA方法，通过语义与结构对齐解决图联邦学习中的节点特征与拓扑结构异质性问题：利用变分模型进行类级节点分布推断以实现语义知识共享，结合谱GNN与谱能量度量实现结构知识共享；在多种图数据集上显著优于11种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 图联邦学习（GFL）面临来自不同客户端的节点特征和图结构拓扑的双重异质性挑战，现有方法难以同时有效应对。

Method: 提出FedSSA框架：1）针对节点特征异质性，设计变分模型推断类级节点分布，据此聚类客户端并构建簇级代表分布，最小化局部与簇级分布间的散度；2）针对结构异质性，采用谱图神经网络（GNN），定义谱能量度量刻画结构信息，据此聚类并构建簇级谱GNN，再对齐本地与簇级谱GNN的谱特性。

Result: 在6个同质性和5个异质性图数据集、非重叠与重叠划分设置下，FedSSA持续优于11种前沿方法。

Conclusion: FedSSA通过协同建模语义与结构异质性，实现了更鲁棒、更泛化的图联邦学习，为隐私保护下的分布式图表示学习提供了新范式。

Abstract: Graph Federated Learning (GFL) enables distributed graph representation learning while protecting the privacy of graph data. However, GFL suffers from heterogeneity arising from diverse node features and structural topologies across multiple clients. To address both types of heterogeneity, we propose a novel graph Federated learning method via Semantic and Structural Alignment (FedSSA), which shares the knowledge of both node features and structural topologies. For node feature heterogeneity, we propose a novel variational model to infer class-wise node distributions, so that we can cluster clients based on inferred distributions and construct cluster-level representative distributions. We then minimize the divergence between local and cluster-level distributions to facilitate semantic knowledge sharing. For structural heterogeneity, we employ spectral Graph Neural Networks (GNNs) and propose a spectral energy measure to characterize structural information, so that we can cluster clients based on spectral energy and build cluster-level spectral GNNs. We then align the spectral characteristics of local spectral GNNs with those of cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.

</details>


### [66] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: 本文提出了一种针对Transformer推理的自适应混合精度计算策略，通过选择性地高精度计算部分中间结果，显著提升低精度下的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 混合精度计算是当前AI发展的关键，尤其在大语言模型本地部署中追求高效与准确的平衡；但现有方法缺乏对函数组合（如Transformer中多层结构）中舍入误差传播的精细控制。

Method: 基于复合函数f(g(x))的舍入误差分析，提出一种自适应策略：仅对g(x)中一小部分分量进行高精度计算，其余部分使用低精度；并将该策略扩展至Transformer各模块（如注意力、FFN等）的内部组合计算。

Result: 在GPT-2模型上的实验表明，即使极低的重计算率（recomputation rate），也能实现高达两个数量级的精度提升。

Conclusion: 面向函数组合结构的细粒度混合精度策略，可在几乎不增加计算开销的前提下，显著缓解低精度推理中的误差累积问题，为高效本地化LLM部署提供了新思路。

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [67] [Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps](https://arxiv.org/abs/2601.21624)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 本文综述了现代深度学习训练中非记忆性（即历史依赖性）的多种机制，按来源、生命周期和可见性进行分类，并提出可移植的扰动原语、因果估计量及报告检查清单，旨在实现可复现、因果明确且不确定性感知的训练历史影响评估。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习训练严重依赖历史状态（如优化器动量、数据顺序、EMA、BatchNorm统计等），但当前评估方法常忽略这些依赖，导致实验不可复现、结果不可归因。

Method: 系统性分类训练中的历史依赖机制；提出种子配对的函数空间因果估计量；设计可移植扰动原语（如动量/EMA/BN状态携带或重置、数据顺序窗口交换、队列/教师模型微调）；构建含审计工件（如顺序哈希、缓冲区/BN校验和、RNG契约）的报告检查清单。

Result: 建立了用于量化训练历史影响的协议框架，支持跨模型、数据集与训练范式的可移植、因果明确、不确定性感知的测量。

Conclusion: 训练历史显著影响模型行为，需通过标准化协议进行因果归因与鲁棒评估；该工作为可复现、可审计、可比较的深度学习训练科学奠定基础。

Abstract: Modern deep-learning training is not memoryless. Updates depend on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement, staged augmentations and replay), the nonconvex path, and auxiliary state (teacher EMA/SWA, contrastive queues, BatchNorm statistics). This survey organizes mechanisms by source, lifetime, and visibility. It introduces seed-paired, function-space causal estimands; portable perturbation primitives (carry/reset of momentum/Adam/EMA/BN, order-window swaps, queue/teacher tweaks); and a reporting checklist with audit artifacts (order hashes, buffer/BN checksums, RNG contracts). The conclusion is a protocol for portable, causal, uncertainty-aware measurement that attributes how much training history matters across models, data, and regimes.

</details>


### [68] [Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation](https://arxiv.org/abs/2601.21636)
*Jan Schuchardt,Nikita Kalinin*

Main category: cs.LG

TL;DR: 本文提出了一种无需采样的隐私放大分析框架，用于矩阵分解下的差分隐私模型训练，通过Rényi散度和条件组合方法获得更紧致、更可靠的隐私保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于蒙特卡洛采样的隐私放大分析方法存在概率性保证、需随机弃权、且样本数与δ成反比等问题，限制了其在实际差分隐私训练中的可靠性与效率。

Method: 提出基于Rényi散度的采样自由上界（通过动态规划高效计算）与基于条件组合的互补方法，二者协同提供对小ε更强的隐私保证；框架适用于任意带状与非带状矩阵机制。

Result: 在多种研究与实践中常用的矩阵机制上，数值实验表明所提方法相比现有蒙特卡洛方法具有更紧致、更稳定且计算更高效的隐私放大界。

Conclusion: 该工作为矩阵分解场景下的隐私放大提供了首个兼具理论严谨性、计算可行性与广泛适用性的确定性分析框架，克服了采样方法的根本局限。

Abstract: We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.

</details>


### [69] [Generative Design of Ship Propellers using Conditional Flow Matching](https://arxiv.org/abs/2601.21637)
*Patrick Kruger,Rafael Diaz,Simon Hauschulz,Stefan Harries,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 本文探索了生成式人工智能（GenAI）在船舶螺旋桨设计中的应用，提出基于条件流匹配的双向映射方法，实现从性能目标到多组可行设计的生成，并通过涡格法仿真与伪标签数据增强提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统前向机器学习仅能预测给定设计的性能，而工程设计更需要反向生成满足性能目标的设计方案，因此需引入生成式AI以提升设计灵活性与效率。

Method: 采用条件流匹配（conditional flow matching）构建设计参数与带性能标签的模拟噪声之间的双向映射；使用涡格法（vortex lattice method）生成仿真数据；引入基于轻量前向代理模型的伪标签进行数据增强。

Result: 模型可针对同一性能目标生成多个几何结构不同但性能相近的螺旋桨设计方案；验证了伪标签增强对小样本下模型精度的提升效果；展示了GenAI在工程逆向设计中的可行性与多样性。

Conclusion: GenAI，特别是条件流匹配框架，为船舶推进器等复杂机械部件的性能驱动型设计提供了新范式，兼具生成多样性与物理可解释性，具备实际工程应用潜力。

Abstract: In this paper, we explore the use of generative artificial intelligence (GenAI) for ship propeller design. While traditional forward machine learning models predict the performance of mechanical components based on given design parameters, GenAI models aim to generate designs that achieve specified performance targets. In particular, we employ conditional flow matching to establish a bidirectional mapping between design parameters and simulated noise that is conditioned on performance labels. This approach enables the generation of multiple valid designs corresponding to the same performance targets by sampling over the noise vector.
  To support model training, we generate data using a vortex lattice method for numerical simulation and analyze the trade-off between model accuracy and the amount of available data. We further propose data augmentation using pseudo-labels derived from less data-intensive forward surrogate models, which can often improve overall model performance. Finally, we present examples of distinct propeller geometries that exhibit nearly identical performance characteristics, illustrating the versatility and potential of GenAI in engineering design.

</details>


### [70] [Identifiable Equivariant Networks are Layerwise Equivariant](https://arxiv.org/abs/2601.21645)
*Vahid Shahverdi,Giovanni Luca Marchetti,Georg Bökman,Kathlén Kohn*

Main category: cs.LG

TL;DR: 本文研究了深度神经网络中端到端等变性与逐层等变性之间的关系，证明了若网络整体函数关于输入输出空间上的群作用是等变的，则存在一种参数选择，使得各层在隐空间上也关于某些群作用等变，前提是模型参数具有适当的可识别性。


<details>
  <summary>Details</summary>
Motivation: 解释实践中观察到的神经网络训练过程中权重自发出现等变结构这一现象。

Method: 基于抽象形式化框架进行理论分析，不依赖特定网络架构，并利用参数可识别性假设进行证明。

Result: 证明了端到端等变性可导出逐层等变性（在适当参数选择下），且该结论适用于大量已建立参数可识别性的网络结构。

Conclusion: 为神经网络中等变结构的涌现提供了数学解释，表明端到端对称性约束可自然诱导中间层的对称性结构。

Abstract: We investigate the relation between end-to-end equivariance and layerwise equivariance in deep neural networks. We prove the following: For a network whose end-to-end function is equivariant with respect to group actions on the input and output spaces, there is a parameter choice yielding the same end-to-end function such that its layers are equivariant with respect to some group actions on the latent spaces. Our result assumes that the parameters of the model are identifiable in an appropriate sense. This identifiability property has been established in the literature for a large class of networks, to which our results apply immediately, while it is conjectural for others. The theory we develop is grounded in an abstract formalism, and is therefore architecture-agnostic. Overall, our results provide a mathematical explanation for the emergence of equivariant structures in the weights of neural networks during training -- a phenomenon that is consistently observed in practice.

</details>


### [71] [TabClustPFN: A Prior-Fitted Network for Tabular Data Clustering](https://arxiv.org/abs/2601.21656)
*Tianqi Zhao,Guanyang Wang,Yan Shuo Tan,Qiong Zhang*

Main category: cs.LG

TL;DR: 本文提出了TabClustPFN，一种用于表格数据聚类的先验拟合网络，通过在合成数据上预训练实现对未知数据集的零样本、单次前向聚类，无需微调，能自动推断簇数量并处理混合特征。


<details>
  <summary>Details</summary>
Motivation: 表格数据聚类面临异构特征类型、多样生成机制及缺乏跨数据集归纳偏置等挑战；现有PFN虽在监督学习中表现优异，但直接扩展至无监督聚类（需处理组合输出、置换不变性及簇数推断）困难。

Method: 提出TabClustPFN，基于灵活的合成聚类先验，在大量合成聚类数据集上预训练，以实现对簇分配与簇数量的联合摊销贝叶斯推理；支持数值与分类特征，单次前向即可完成聚类。

Result: 在合成与真实世界表格数据集上，TabClustPFN显著优于经典、深度及摊销式聚类基线，具备强鲁棒性与即开即用能力。

Conclusion: TabClustPFN为表格数据聚类提供了首个有效的零样本、端到端可学习框架，验证了摊销贝叶斯推理在无监督结构化学习中的可行性与优势。

Abstract: Clustering tabular data is a fundamental yet challenging problem due to heterogeneous feature types, diverse data-generating mechanisms, and the absence of transferable inductive biases across datasets. Prior-fitted networks (PFNs) have recently demonstrated strong generalization in supervised tabular learning by amortizing Bayesian inference under a broad synthetic prior. Extending this paradigm to clustering is nontrivial: clustering is unsupervised, admits a combinatorial and permutation-invariant output space, and requires inferring the number of clusters. We introduce TabClustPFN, a prior-fitted network for tabular data clustering that performs amortized Bayesian inference over both cluster assignments and cluster cardinality. Pretrained on synthetic datasets drawn from a flexible clustering prior, TabClustPFN clusters unseen datasets in a single forward pass, without dataset-specific retraining or hyperparameter tuning. The model naturally handles heterogeneous numerical and categorical features and adapts to a wide range of clustering structures. Experiments on synthetic data and curated real-world tabular benchmarks show that TabClustPFN outperforms classical, deep, and amortized clustering baselines, while exhibiting strong robustness in out-of-the-box exploratory settings. Code is available at https://github.com/Tianqi-Zhao/TabClustPFN.

</details>


### [72] [Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching](https://arxiv.org/abs/2601.21662)
*Li Ju,Mayank Nautiyal,Andreas Hellander,Ekta Vats,Prashant Singh*

Main category: cs.LG

TL;DR: This paper proposes REPVLM, a method to quantify epistemic uncertainty in Vision-Language Models (VLMs) by estimating embedding density on a hyperspherical manifold using Riemannian Flow Matching; it shows near-perfect uncertainty–error correlation and enables OOD detection and data curation.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) are deterministic and lack intrinsic mechanisms to quantify epistemic uncertainty—i.e., their ignorance about learned representations.

Method: REPVL computes probability density of VLM embeddings on the hyperspherical manifold via Riemannian Flow Matching, using negative log-density as a proxy for epistemic uncertainty.

Result: REPVL achieves near-perfect correlation between estimated uncertainty and prediction error, outperforming existing baselines; it also enables scalable out-of-distribution detection and automated data curation.

Conclusion: Quantifying epistemic uncertainty in VLMs via geometric density estimation is feasible and effective, enabling more reliable and interpretable multimodal reasoning.

Abstract: Vision-Language Models (VLMs) are typically deterministic in nature and lack intrinsic mechanisms to quantify epistemic uncertainty, which reflects the model's lack of knowledge or ignorance of its own representations. We theoretically motivate negative log-density of an embedding as a proxy for the epistemic uncertainty, where low-density regions signify model ignorance. The proposed method REPVLM computes the probability density on the hyperspherical manifold of the VLM embeddings using Riemannian Flow Matching. We empirically demonstrate that REPVLM achieves near-perfect correlation between uncertainty and prediction error, significantly outperforming existing baselines. Beyond classification, we also demonstrate that the model also provides a scalable metric for out-of-distribution detection and automated data curation.

</details>


### [73] [LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://arxiv.org/abs/2601.21681)
*Qisong Xiao,Xinhai Chen,Qinglin Wang,Xiaowei Guo,Binglin Wang,Weifeng Chen,Zhichao Wang,Yunfei Liu,Rui Xia,Hang Zou,Gencheng Liu,Shuai Li,Jie Liu*

Main category: cs.LG

TL;DR: 本文提出LLM4Fluid框架，利用大语言模型（LLM）作为通用神经求解器进行流体动力学时空预测，通过物理信息驱动的降维与解耦压缩流场、LLM时序建模及模态对齐策略，实现免重训练、零样本与上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在流体动力学建模中泛化能力差，面对新流场条件需重新训练，缺乏通用性。

Method: 1）基于物理信息增强的降阶建模实现流场压缩与空间特征解耦；2）采用预训练大语言模型对物理时间序列进行自回归预测；3）设计专用模态对齐策略弥合文本提示与物理序列间的表征鸿沟。

Result: 在多种流场场景下验证了LLM4Fluid无需重训练即可保持高精度预测，展现出优异的零样本和上下文学习能力，达到当前最优性能。

Conclusion: LLM4Fluid成功将大语言模型迁移至流体力学求解任务，证明了LLM作为通用物理神经求解器的可行性与强大泛化能力。

Abstract: Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at https://github.com/qisongxiao/LLM4Fluid.

</details>


### [74] [Can Local Learning Match Self-Supervised Backpropagation?](https://arxiv.org/abs/2601.21683)
*Wu S. Zihan,Ariane Delrocq,Wulfram Gerstner,Guillaume Bellec*

Main category: cs.LG

TL;DR: 本文建立了局部自监督学习（local-SSL）与端到端全局反向传播自监督学习（global BP-SSL）之间的理论联系，并在深度线性网络中证明了特定条件下局部规则可精确复现全局更新；进而提出改进的非线性局部SSL算法，在CIFAR-10、STL-10和Tiny ImageNet上达到甚至超越现有最优局部SSL方法的性能，并媲美基于InfoNCE/CPC的全局BP-SSL。


<details>
  <summary>Details</summary>
Motivation: 局部自监督学习（local-SSL）在构建深层网络功能表征方面理论进展有限，而端到端全局BP-SSL虽成功却缺乏生物合理性与模块化优势；亟需建立二者间的理论桥梁并提升local-SSL的实际性能。

Method: 首先在深度线性网络中推导local-SSL（如Forward-forward、CLAPP）精确复现global BP-SSL权重更新的理论条件；继而基于该洞见，设计新型非线性CNN适用的local-SSL变体，重点优化其梯度更新与global BP-SSL梯度的相似性。

Result: 所提CLAPP变体在CIFAR-10、STL-10和Tiny ImageNet上性能媲美基于InfoNCE或CPC的global BP-SSL，并显著超越当前最优local SSL方法。

Conclusion: local-SSL可通过理论指导下的梯度对齐策略逼近global BP-SSL性能，为构建高效、模块化、生物启发的自监督学习框架提供了新路径。

Abstract: While end-to-end self-supervised learning with backpropagation (global BP-SSL) has become central for training modern AI systems, theories of local self-supervised learning (local-SSL) have struggled to build functional representations in deep neural networks. To establish a link between global and local rules, we first develop a theory for deep linear networks: we identify conditions for local-SSL algorithms (like Forward-forward or CLAPP) to implement exactly the same weight update as a global BP-SSL. Starting from the theoretical insights, we then develop novel variants of local-SSL algorithms to approximate global BP-SSL in deep non-linear convolutional neural networks. Variants that improve the similarity between gradient updates of local-SSL with those of global BP-SSL also show better performance on image datasets (CIFAR-10, STL-10, and Tiny ImageNet). The best local-SSL rule with the CLAPP loss function matches the performance of a comparable global BP-SSL with InfoNCE or CPC-like loss functions, and improves upon state-of-the-art for local SSL on these benchmarks.

</details>


### [75] [Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold](https://arxiv.org/abs/2601.21686)
*Luca Benfenati,Matteo Risso,Andrea Vannozzi,Ahmet Caner Yüzügüler,Lukas Cavigelli,Enrico Macii,Daniele Jahier Pagliari,Alessio Burrello*

Main category: cs.LG

TL;DR: StiefAttention是一种后训练的KV缓存压缩方法，通过直接最小化解码层输出重建误差来学习正交投影基，并支持按层灵活分配压缩秩，在保持相同压缩率下显著提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: KV缓存压缩在长上下文场景中成为HBM容量和带宽瓶颈；现有SVD类代理目标无法准确反映端到端重建效果（含softmax、value mixing及后续层变换）。

Method: 提出StiefAttention：1）学习正交投影基以最小化解码层输出重建误差；2）为每层预计算误差-秩曲线，支持在给定误差预算下进行层间自适应秩分配。

Result: 在Llama3-8B上，相比EigenAttention，在同等压缩率下C4困惑度降低11.9点，0-shot MMLU准确率提升5.4%，且解码层输出相对误差更低、余弦相似度更高。

Conclusion: StiefAttention通过端到端优化和层自适应秩分配，实现了更优的KV缓存压缩效果，兼顾效率与模型性能。

Abstract: Key--value (KV) caching enables fast autoregressive decoding but at long contexts becomes a dominant bottleneck in High Bandwidth Memory (HBM) capacity and bandwidth. A common mitigation is to compress cached keys and values by projecting per-head matrixes to a lower rank, storing only the projections in the HBM. However, existing post-training approaches typically fit these projections using SVD-style proxy objectives, which may poorly reflect end-to-end reconstruction after softmax, value mixing, and subsequent decoder-layer transformations.
  For these reasons, we introduce StiefAttention, a post-training KV-cache compression method that learns \emph{orthonormal} projection bases by directly minimizing \emph{decoder-layer output reconstruction error}. StiefAttention additionally precomputes, for each layer, an error-rank profile over candidate ranks, enabling flexible layer-wise rank allocation under a user-specified error budget. Noteworthy, on Llama3-8B under the same conditions, StiefAttention outperforms EigenAttention by $11.9$ points on C4 perplexity and $5.4\%$ on 0-shot MMLU accuracy at iso-compression, yielding lower relative error and higher cosine similarity with respect to the original decoder-layer outputs.

</details>


### [76] [Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts](https://arxiv.org/abs/2601.21690)
*Qinglun Li,Anke Tang,Miao Zhang,Mengzhu Wang,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 本文提出了一种基于L2-稳定性理论的统一分析框架，解释模型合并（model merging）在异构微调超参数下的有效性，并为构建可合并的专家模型提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法虽经验上成功，但缺乏在异构微调超参数（如学习率、批量大小）下的统一理论支撑；且开源微调模型超参数不透明，难以预测合并后性能，缺乏构建‘合并友好型’专家模型的指导。

Method: 采用L2-稳定性理论，在异构超参数环境下分析平均合并模型x_avg的泛化性能，推导理论界，并据此解释现有合并算法、提出可操作的微调策略。

Result: 理论分析揭示了不同超参数对x_avg泛化性能的影响；实验在ResNet/ViT系列及20/8个视觉分类任务（数千微调模型）上验证了理论预测的鲁棒性。

Conclusion: 本文建立了首个针对异构超参数下模型合并的统一理论框架，既解释了既有算法，又为预训练-微调流程中设计合并友好型专家提供了明确指导。

Abstract: Model merging efficiently aggregates capabilities from multiple fine-tuned models into a single one, operating purely in parameter space without original data or expensive re-computation. Despite empirical successes, a unified theory for its effectiveness under heterogeneous finetuning hyperparameters (e.g., varying learning rates, batch sizes) remains missing. Moreover, the lack of hyperparameter transparency in open-source fine-tuned models makes it difficult to predict merged-model performance, leaving practitioners without guidance on how to fine-tune merge-friendly experts. To address those two challenges, we employ $L_2$-Stability theory under heterogeneous hyperparameter environments to analyze the generalization of the merged model $\boldsymbol{x}_{avg}$. This pioneering analysis yields two key contributions: (i) \textit{A unified theoretical framework} is provided to explain existing merging algorithms, revealing how they optimize specific terms in our bound, thus offering a strong theoretical foundation for empirical observations. (ii) \textit{Actionable recommendations} are proposed for practitioners to strategically fine-tune expert models, enabling the construction of merge-friendly models within the pretraining-to-finetuning pipeline. Extensive experiments on the ResNet/Vit family across 20/8 visual classification tasks, involving thousands of finetuning models, robustly confirm the impact of different hyperparameters on the generalization of $\boldsymbol{x}_{avg}$ predicted by our theoretical results.

</details>


### [77] [SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models](https://arxiv.org/abs/2601.21706)
*Nan Lin,Yanbo Wang,Jacco Heres,Peter Palensky,Pedro P. Vergara*

Main category: cs.LG

TL;DR: 本文提出一种基于流匹配模型的统一框架，用于解决智能电表数据生成中的多种任务（如合成数据生成、缺失值填补和超分辨率），通过单一条件生成模型避免了为每个任务单独设计和训练模型的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据常因隐私法规不可得、传感器或传输故障而损坏，或分辨率不足；现有方法需为每种生成任务单独建模，导致冗余与低效。

Method: 利用流匹配模型强大的建模能力，构建一个面向条件生成的统一模型，将不同生成任务视为部分观测形式，并将其注入生成过程，从而实现任务统一；模型针对高维时间序列（15分钟粒度的月度电表数据）进行训练。

Result: 所提模型在缺失填补与超分辨率等任务上优于插值法及其他专用机器学习基线方法，生成数据既符合给定观测又保持现实性。

Conclusion: 流匹配模型可有效统一多种智能电表数据生成任务，提升模型复用性与效率，同时保证生成质量。

Abstract: Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.

</details>


### [78] [LoRA and Privacy: When Random Projections Help (and When They Don't)](https://arxiv.org/abs/2601.21719)
*Yaxi Hu,Johanna Düngler,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: 本文提出了一种基于Wishart分布的投影机制，用于向量和矩阵查询的差分隐私保护；证明了该机制对向量查询无需额外加噪即可满足DP，但对矩阵查询在无噪下不满足DP，并揭示其易受成员推断攻击；进一步分析带噪变体，发现低秩投影可放大隐私；最后指出LoRA微调并非天然隐私安全，但低秩微调相比全参数微调在同等噪声下更具隐私优势。


<details>
  <summary>Details</summary>
Motivation: 探索仅依赖随机投影（而非传统加性噪声）实现差分隐私的可能性，并理解当前流行低秩微调方法（如LoRA）的隐私属性。

Method: 提出Wishart投影机制S ↦ M f(S)，其中M服从Wishart分布；理论分析其对向量/矩阵查询的差分隐私性质；构造成员推断攻击验证脆弱性；分析带噪版本的隐私放大效应；将LoRA建模为该机制的特例。

Result: 1) 向量查询下Wishart机制可无噪满足DP；2) 矩阵查询下无噪时非DP，且AUC > 0.99的成员推断攻击成立；3) 带噪低秩投影可实现隐私放大；4) LoRA本质上不具内在隐私性，但低秩微调比全微调更隐私高效。

Conclusion: Wishart随机性本身可作为隐私源，但效果高度依赖查询类型；低秩结构与噪声协同可提升隐私-效用权衡；LoRA等低秩方法需额外隐私设计才能保障DP。

Abstract: We introduce the (Wishart) projection mechanism, a randomized map of the form $S \mapsto M f(S)$ with $M \sim W_d(1/r I_d, r)$ and study its differential privacy properties. For vector-valued queries $f$, we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC $> 0.99$). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank fine-tuning can be more private than full fine-tuning at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved accuracy in practice.

</details>


### [79] [Amortized Spectral Kernel Discovery via Prior-Data Fitted Network](https://arxiv.org/abs/2601.21731)
*Kaustubh Sharma,Srijan Tiwari,Ojasva Nema,Parikshit Pareek*

Main category: cs.LG

TL;DR: 本文提出了一种可解释性驱动的框架，从预训练的Prior-Data Fitted Networks (PFNs)中解耦并提取谱密度与显式核函数，通过机制分析发现注意力隐状态是连接数据与谱结构的关键中介，并设计解码器实现单次前向传播即可获得高质量核估计，显著加速下游任务如贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: PFNs虽能高效实现摊销推断，但其学习到的先验和核函数不透明，难以用于需要显式协方差模型的下游任务（如代理优化）。

Method: 对预训练PFN进行机制分析，识别注意力隐输出为关键中介；基于Bochner定理，设计解码器将PFN隐状态映射为谱密度估计及对应平稳核；在单/多函数样本场景下理论分析谱可识别性与一致性。

Result: 解码器能准确恢复复杂多峰谱混合，生成的显式核在高斯过程回归中性能媲美PFNs和优化基线，且仅需一次前向传播，推理时间比优化基线快数个数量级。

Conclusion: 该框架在保持PFNs高效性的同时赋予其可解释性与显式核建模能力，为摊销推断与贝叶斯优化等任务提供了新范式。

Abstract: Prior-Data Fitted Networks (PFNs) enable efficient amortized inference but lack transparent access to their learned priors and kernels. This opacity hinders their use in downstream tasks, such as surrogate-based optimization, that require explicit covariance models. We introduce an interpretability-driven framework for amortized spectral discovery from pre-trained PFNs with decoupled attention. We perform a mechanistic analysis on a trained PFN that identifies attention latent output as the key intermediary, linking observed function data to spectral structure. Building on this insight, we propose decoder architectures that map PFN latents to explicit spectral density estimates and corresponding stationary kernels via Bochner's theorem. We study this pipeline in both single-realization and multi-realization regimes, contextualizing theoretical limits on spectral identifiability and proving consistency when multiple function samples are available. Empirically, the proposed decoders recover complex multi-peak spectral mixtures and produce explicit kernels that support Gaussian process regression with accuracy comparable to PFNs and optimization-based baselines, while requiring only a single forward pass. This yields orders-of-magnitude reductions in inference time compared to optimization-based baselines.

</details>


### [80] [Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators](https://arxiv.org/abs/2601.21737)
*Rebecca Pelke,Joel Klein,Jose Cubero-Cascante,Nils Bosbach,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.LG

TL;DR: 本文提出了一种面向存内计算（CIM）架构的混合精度训练与编译框架，利用强化学习自动搜索最优量化配置，在显著提升速度（最高2.48倍）的同时仅造成极小精度损失（0.086%）。


<details>
  <summary>Details</summary>
Motivation: 现有CIM编译器大多不支持低于8位的量化，导致单次矩阵-向量乘法（MVM）需多周期完成、权重无法高效存入单个交叉阵列单元，限制了性能与能效。

Method: 提出混合精度训练与编译框架，并设计基于强化学习的策略，在庞大搜索空间中自动优化各层/算子的量化位宽与参数，以权衡延迟与精度。

Result: 在最佳情况下，相比当前最先进方案实现最高2.48倍加速，精度仅下降0.086%。

Conclusion: 强化学习驱动的混合精度量化方法可有效突破CIM硬件位宽限制，显著提升实际部署效率与精度-延迟平衡性。

Abstract: Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.

</details>


### [81] [FISMO: Fisher-Structured Momentum-Orthogonalized Optimizer](https://arxiv.org/abs/2601.21750)
*Chenrui Xu,Wenjing Yan,Ying-Jun Angela Zhang*

Main category: cs.LG

TL;DR: 本文提出FISMO优化器，通过引入Fisher信息几何结构，在保持计算可行性的前提下，将各向同性动量更新推广为能自适应局部损失曲率的各向异性更新，并在随机非凸优化中提供了收敛性保证和实证性能提升。


<details>
  <summary>Details</summary>
Motivation: MuO等严格各向同性优化器虽有效但忽略了梯度谱中蕴含的曲率信息，需平衡几何结构与自适应性。

Method: FISMO将优化更新重构为以Kronecker分解Fisher度量为约束的信赖域问题，实现基于Fisher信息几何的结构化预处理。

Result: 理论证明FISMO在随机非凸设置下具有O(1/√T)的期望梯度范数收敛率，并通过小批量明确刻画方差降低；实验表明其在图像分类与语言建模任务上优于现有基线。

Conclusion: FISMO通过融合Fisher信息几何与动量正交化，在不牺牲计算效率的前提下提升了优化器对损失景观几何结构的适应能力，兼具理论保证与实际性能优势。

Abstract: Training large-scale neural networks requires solving nonconvex optimization where the choice of optimizer fundamentally determines both convergence behavior and computational efficiency. While adaptive methods like Adam have long dominated practice, the recently proposed Muon optimizer achieves superior performance through orthogonalized momentum updates that enforce isotropic geometry with uniform singular values. However, this strict isotropy discards potentially valuable curvature information encoded in gradient spectra, motivating optimization methods that balance geometric structure with adaptivity. We introduce FISMO (Fisher-Structured Momentum-Orthogonalized) optimizer, which generalizes isotropic updates to incorporate anisotropic curvature information through Fisher information geometry. By reformulating the optimizer update as a trust-region problem constrained by a Kronecker-factored Fisher metric, FISMO achieves structured preconditioning that adapts to local loss landscape geometry while maintaining computational tractability. We establish convergence guarantees for FISMO in stochastic nonconvex settings, proving an $\mathcal{O}(1/\sqrt{T})$ rate for the expected squared gradient norm with explicit characterization of variance reduction through mini-batching. Empirical evaluation on image classification and language modeling benchmarks demonstrates that FISMO achieves superior training efficiency and final performance compared to established baselines.

</details>


### [82] [Differentiable Knapsack and Top-k Operators via Dynamic Programming](https://arxiv.org/abs/2601.21775)
*Germain Vivier-Ardisson,Michaël E. Sander,Axel Parmentier,Mathieu Blondel*

Main category: cs.LG

TL;DR: 本文提出了一种将背包（Knapsack）和Top-k等离散选择算子嵌入神经网络的统一可微框架，通过动态规划建模并平滑递归过程实现可导近似，并在算法、理论与实验三方面进行了系统性贡献。


<details>
  <summary>Details</summary>
Motivation: Knapsack和Top-k等离散选择算子在神经网络中难以直接使用，因其分段常数性质导致梯度几乎处处为零，亟需可微松弛方法。

Method: 将Knapsack和Top-k建模为动态规划问题，通过对递归过程施加平滑（如熵正则化）构造可微松弛；设计支持确定性/随机前向传播及高效向量-雅可比乘积的并行算法。

Result: 理论上证明香农熵是唯一能保证置换等变性的正则项，并刻画了诱导稀疏选择的正则器特性；实验上在决策导向学习、受限动态商品组合强化学习及离散VAE扩展任务中验证了有效性。

Conclusion: 该框架为离散组合算子提供了兼具理论严谨性与实用性的可微接口，推动了组合优化与深度学习的深度融合。

Abstract: Knapsack and Top-k operators are useful for selecting discrete subsets of variables. However, their integration into neural networks is challenging as they are piecewise constant, yielding gradients that are zero almost everywhere. In this paper, we propose a unified framework casting these operators as dynamic programs, and derive differentiable relaxations by smoothing the underlying recursions. On the algorithmic side, we develop efficient parallel algorithms supporting both deterministic and stochastic forward passes, and vector-Jacobian products for the backward pass. On the theoretical side, we prove that Shannon entropy is the unique regularization choice yielding permutation-equivariant operators, and characterize regularizers inducing sparse selections. Finally, on the experimental side, we demonstrate our framework on a decision-focused learning benchmark, a constrained dynamic assortment RL problem, and an extension of discrete VAEs.

</details>


### [83] [Quantum LEGO Learning: A Modular Design Principle for Hybrid Artificial Intelligence](https://arxiv.org/abs/2601.21780)
*Jun Qi,Chao-Han Huck Yang,Pin-Yu Chen,Min-Hsiu Hsieh,Hector Zenil,Jesper Tegner*

Main category: cs.LG

TL;DR: 本文提出Quantum LEGO Learning框架，将经典神经网络与变分量子电路解耦为可复用、可组合的学习模块，通过模块化设计提升混合模型的通用性、可迁移性与资源效率，并建立块级泛化理论分析其性能边界。


<details>
  <summary>Details</summary>
Motivation: 现有混合量子-经典学习模型常采用紧耦合架构或任务特定编码器，导致概念不清、泛化能力弱、跨任务迁移困难。

Method: 提出模块化、架构无关的Quantum LEGO Learning框架：冻结预训练经典神经网络作为特征提取块，VQC作为在结构化表征上操作的可训练自适应模块；并发展块级泛化理论，分解学习误差为近似误差与估计误差。

Result: 理论表明量子模块在特定条件下相比同规模经典头部具有表征优势；实验在量子点分类任务中验证了框架的稳定优化能力、对量子比特数不敏感性及对真实噪声的鲁棒性。

Conclusion: Quantum LEGO Learning提供了一种清晰、通用且资源高效的混合建模范式，增强了可解释性、模块复用性与理论可分析性，为量子机器学习的实际部署奠定基础。

Abstract: Hybrid quantum-classical learning models increasingly integrate neural networks with variational quantum circuits (VQCs) to exploit complementary inductive biases. However, many existing approaches rely on tightly coupled architectures or task-specific encoders, limiting conceptual clarity, generality, and transferability across learning settings. In this work, we introduce Quantum LEGO Learning, a modular and architecture-agnostic learning framework that treats classical and quantum components as reusable, composable learning blocks with well-defined roles. Within this framework, a pre-trained classical neural network serves as a frozen feature block, while a VQC acts as a trainable adaptive module that operates on structured representations rather than raw inputs. This separation enables efficient learning under constrained quantum resources and provides a principled abstraction for analyzing hybrid models. We develop a block-wise generalization theory that decomposes learning error into approximation and estimation components, explicitly characterizing how the complexity and training status of each block influence overall performance. Our analysis generalizes prior tensor-network-specific results and identifies conditions under which quantum modules provide representational advantages over comparably sized classical heads. Empirically, we validate the framework through systematic block-swap experiments across frozen feature extractors and both quantum and classical adaptive heads. Experiments on quantum dot classification demonstrate stable optimization, reduced sensitivity to qubit count, and robustness to realistic noise.

</details>


### [84] [NetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification](https://arxiv.org/abs/2601.21792)
*Tongze Wang,Xiaohui Xie,Wenduo Wang,Chuyi Wang,Jinzhou Liu,Boyan Huang,Yannan Hu,Youjian Zhao,Yong Cui*

Main category: cs.LG

TL;DR: NetMamba+ 是首个将 Mamba 架构应用于网络流量分类的框架，通过高效架构、多模态表征与标签分布感知微调策略，显著提升分类精度、推理效率与少样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和深度学习的流量分类方法存在计算效率低、字节级特征丢失且引入偏差、难以处理长尾分布三大问题。

Method: 提出NetMamba+框架，包含三方面创新：(1)融合Mamba与Flash Attention的高效架构；(2)保留关键字节信息并消除偏差的多模态流量表征；(3)标签分布感知的微调策略。

Result: 在四大分类任务上F1分数最高提升6.44%；推理吞吐量达最优基线的1.7倍，内存占用相近；少样本学习能力更强；在线系统实测吞吐达261.87 Mb/s。

Conclusion: NetMamba+为复杂网络环境下的高效精准流量分析提供了新范式，是首个成功适配Mamba架构的流量分类框架。

Abstract: With the rapid growth of encrypted network traffic, effective traffic classification has become essential for network security and quality of service management. Current machine learning and deep learning approaches for traffic classification face three critical challenges: computational inefficiency of Transformer architectures, inadequate traffic representations with loss of crucial byte-level features while retaining detrimental biases, and poor handling of long-tail distributions in real-world data. We propose NetMamba+, a framework that addresses these challenges through three key innovations: (1) an efficient architecture considering Mamba and Flash Attention mechanisms, (2) a multimodal traffic representation scheme that preserves essential traffic information while eliminating biases, and (3) a label distribution-aware fine-tuning strategy. Evaluation experiments on massive datasets encompassing four main classification tasks showcase NetMamba+'s superior classification performance compared to state-of-the-art baselines, with improvements of up to 6.44\% in F1 score. Moreover, NetMamba+ demonstrates excellent efficiency, achieving 1.7x higher inference throughput than the best baseline while maintaining comparably low memory usage. Furthermore, NetMamba+ exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. Additionally, we implement an online traffic classification system that demonstrates robust real-world performance with a throughput of 261.87 Mb/s. As the first framework to adapt Mamba architecture for network traffic classification, NetMamba+ opens new possibilities for efficient and accurate traffic analysis in complex network environments.

</details>


### [85] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的机器遗忘方法KVW，通过直接干预大视觉语言模型（LVLMs）中的知识向量来削弱其对特定数据的记忆，从而在保证模型性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度优化的机器遗忘方法在大规模视觉语言模型上计算开销巨大，亟需更高效的遗忘方案。

Method: 提出Knowledge Vector Weakening（KVW）方法，无需训练或梯度计算，识别并逐步削弱在‘遗忘集’上激活的知识向量，以消除模型对有害或隐私敏感知识的依赖。

Result: 在MLLMU和CLEAR基准测试中，KVW在遗忘效果与保留性能之间取得稳定平衡，并显著优于基于梯度和LoRA的遗忘方法，计算效率更高。

Conclusion: KVW是一种高效、免训练的大模型机器遗忘新范式，为LVLMs的安全可控部署提供了实用可行的技术路径。

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [86] [Nonparametric LLM Evaluation from Preference Data](https://arxiv.org/abs/2601.21816)
*Dennis Frauen,Athiya Deviyani,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出了一种名为DMLEval的非参数统计框架，用于基于人类偏好数据对大语言模型（LLMs）进行比较与排序，引入广义平均排序分数（GARS），具备高效估计、兼容黑盒机器学习方法、支持LLM-as-a-judge及最优数据采集策略等优势。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法或依赖强参数假设，或在使用灵活机器学习方法时缺乏有效的不确定性量化。

Method: 提出基于去偏机器学习（DML）的非参数框架DMLEval，定义广义平均排序分数（GARS），统一建模如Bradley-Terry、PageRank等排序模型，并支持处理平局等复杂人类响应。

Result: 理论和实验表明，DMLEval能高效估计GARS，支持黑盒模型与预训练LLM评判器，并提供预算约束下的最优偏好数据采集策略。

Conclusion: DMLEval为LLM性能评估提供了兼具统计严谨性、灵活性与实用性的先进框架。

Abstract: Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.

</details>


### [87] [DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training](https://arxiv.org/abs/2601.21824)
*Xinwei Qiang,Hongmin Chen,Shixuan Sun,Jingwen Leng,Xin Liu,Minyi Guo*

Main category: cs.LG

TL;DR: 本文提出DASH方法，通过优化确定性注意力反向传播的调度策略，显著提升其计算吞吐量，缩小与非确定性实现的性能差距。


<details>
  <summary>Details</summary>
Motivation: 确定性对大语言模型训练的可复现性至关重要，但现有确定性注意力（如FlashAttention-3）反向传播因需序列化梯度累加而带来高达37.9%的吞吐下降，根源在于计算与梯度规约阶段调度不佳导致硬件利用率低。

Method: 将确定性注意力反向传播建模为有向无环图（DAG）上的调度问题，以最小化关键路径长度为目标；据此提出两种策略：(i) 降序Q-Tile遍历，缓解因果注意力中的流水线停顿；(ii) Shift调度，在DAG模型下理论最优，适用于全连接和因果掩码。

Result: 在NVIDIA H800 GPU上实验表明，DASH使注意力反向传播吞吐最高提升1.28倍，显著缩小确定性与非确定性实现的性能差距。

Conclusion: DASH通过精细化调度设计，在不牺牲确定性的前提下大幅提升训练效率，为可复现的大语言模型训练提供了高效可行的底层支持。

Abstract: Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.
  To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.
  Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.
  Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.

</details>


### [88] [Goal-Driven Adaptive Sampling Strategies for Machine Learning Models Predicting Fields](https://arxiv.org/abs/2601.21832)
*Jigar Parekh,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 本文提出了一种适用于场预测的、与模型架构无关的主动学习策略，通过结合高斯过程模型和协同优化标量与场预测误差，显著降低了计算成本并提高了精度。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法主要针对标量输出，缺乏对场预测（如流场）的通用扩展；而工程中常需场预测，亟需一种模型无关、高效准确的主动学习策略。

Method: 提出一种新型主动学习策略：联合使用高斯过程建模标量参考值，并同步最小化认知模型误差与标量-场预测差异；设计多种具体实现形式，并与纯标量基底的填充准则对比。

Result: 在NASA通用研究模型的不确定性传播任务中，所提方法在显著减少黑箱样本数（即计算成本）的前提下，达到了更高精度，优于无主动学习的基准方法。

Conclusion: 该策略具有模型无关性与泛化能力，为多查询、高成本黑箱仿真（如CFD）中的场预测提供了高效、低样本依赖的主动学习新范式。

Abstract: Machine learning models are widely regarded as a way forward to tackle multi-query challenges that arise once expensive black-box simulations such as computational fluid dynamics are investigated. However, ensuring the desired level of accuracy for a certain task at minimal computational cost, e.g. as few black-box samples as possible, remains a challenges. Active learning strategies are used for scalar quantities to overcome this challenges and different so-called infill criteria exists and are commonly employed in several scenarios. Even though needed in various field an extension of active learning strategies towards field predictions is still lacking or limited to very specific scenarios and/or model types. In this paper we propose an active learning strategy for machine learning models that are capable if predicting field which is agnostic to the model architecture itself. For doing so, we combine a well-established Gaussian process model for a scalar reference value and simultaneously aim at reducing the epistemic model error and the difference between scalar and field predictions. Different specific forms of the above-mentioned approach are introduced and compared to each other as well as only scalar-valued based infill. Results are presented for the NASA common research model for an uncertainty propagation task showcasing high level of accuracy at significantly smaller cost compared to an approach without active learning.

</details>


### [89] [Scalable Linearized Laplace Approximation via Surrogate Neural Kernel](https://arxiv.org/abs/2601.21835)
*Luis A. Ortega,Simón Rodríguez-Santana,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 本文提出了一种可扩展的方法，利用代理深度神经网络学习紧凑特征表示，以近似线性化拉普拉斯近似（LLA）的核，避免计算大规模雅可比矩阵，仅依赖高效的雅可比-向量积进行训练，从而在大规模预训练DNN上实现预测不确定性估计，并在分布外检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLA方法在大规模模型上计算雅可比矩阵开销大，难以高效估计预测不确定性；需要一种可扩展、免显式雅可比计算的核近似方法。

Method: 使用代理深度神经网络学习紧凑特征表示，使其内积逼近神经正切核（NTK），通过雅可比-向量积高效训练，从而近似LLA的核。

Result: 在不确定性估计与校准上达到或优于现有LLA近似方法；引入核偏差后显著提升分布外检测性能。

Conclusion: 所提方法不仅提升了LLA在大规模预训练DNN上的可扩展性，还表明可学习比NTK更优的核以增强预测不确定性建模，尤其利于分布外检测。

Abstract: We introduce a scalable method to approximate the kernel of the Linearized Laplace Approximation (LLA). For this, we use a surrogate deep neural network (DNN) that learns a compact feature representation whose inner product replicates the Neural Tangent Kernel (NTK). This avoids the need to compute large Jacobians. Training relies solely on efficient Jacobian-vector products, allowing to compute predictive uncertainty on large-scale pre-trained DNNs. Experimental results show similar or improved uncertainty estimation and calibration compared to existing LLA approximations. Notwithstanding, biasing the learned kernel significantly enhances out-of-distribution detection. This remarks the benefits of the proposed method for finding better kernels than the NTK in the context of LLA to compute prediction uncertainty given a pre-trained DNN.

</details>


### [90] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 本文提出了一种具有安全性和样本复杂度保证的约束元强化学习算法，用于在真实世界测试任务中确保策略安全性并加速最优策略学习。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如机器人和医疗）在测试阶段有安全约束，而现有约束元强化学习方法缺乏对真实测试任务的安全保障及高效学习能力。

Method: 提出一种在训练中学习策略并在测试时进行安全精炼的算法，并提供理论证明其安全性和样本复杂度界。

Result: 算法实现了对测试任务的可证明安全性与近似最优策略的快速学习，并推导出匹配的样本复杂度下界。

Conclusion: 所提算法在保证安全性的同时达到最优样本效率，填补了约束元强化学习在真实场景安全部署方面的理论空白。

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [91] [READY: Reward Discovery for Meta-Black-Box Optimization](https://arxiv.org/abs/2601.21847)
*Zechuan Huang,Zhiguang Cao,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.LG

TL;DR: 本文提出使用大语言模型（LLM）自动发现Meta-Black-Box Optimization（MetaBBO）中的奖励函数，以克服人工设计奖励带来的偏差与奖励作弊风险；通过引入启发式进化范式和多任务并行进化架构，在保证有效性的同时提升效率，并在实验中验证了所发现奖励函数对多种MetaBBO方法的性能提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有MetaBBO工作依赖人工设计奖励函数，易引入设计偏差和奖励作弊风险，亟需自动化、可靠且高效的奖励发现机制。

Method: 利用大语言模型（LLM）作为自动化奖励发现工具：一方面借鉴启发式进化思想，构建迭代式LLM程序搜索过程以保障有效性；另一方面设计多任务进化架构，支持多种MetaBBO方法的并行奖励发现，并促进跨任务知识共享以加速收敛。

Result: 实验表明，该方法发现的奖励函数能有效提升多种现有MetaBBO方法的优化性能，验证了其有效性与泛化能力。

Conclusion: 奖励函数的设计对MetaBBO至关重要；LLM可作为高效、可扩展的自动化奖励发现工具，为MetaBBO提供新范式。

Abstract: Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.

</details>


### [92] [Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models](https://arxiv.org/abs/2601.21851)
*Sidney Bender,Marco Morik*

Main category: cs.LG

TL;DR: 本文提出了一种名为视觉解耦扩散自编码器（DiDAE）的新框架，用于在不依赖组标签或昂贵梯度优化的情况下，高效、无梯度地为视觉基础模型生成解耦的反事实样本，从而缓解捷径学习问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽具备强大的零样本能力，但仍易受虚假相关性和'Clever Hans'策略影响；现有缓解方法常需不可获取的组标签或计算开销大的梯度对抗优化。

Method: 提出DiDAE框架：冻结基础模型，结合解耦字典学习，在可解释的解耦方向上编辑其嵌入，并通过扩散自编码器解码生成多个多样化解耦反事实；进一步结合反事实知识蒸馏（CFKD）形成DiDAE-CFKD。

Result: DiDAE能比现有基线更快生成多个解耦反事实（而非单个纠缠反事实）；DiDAE-CFKD在缓解捷径学习、提升不平衡数据集下游性能方面达到SOTA。

Conclusion: DiDAE是一种高效、无梯度、无需组标签的反事实生成方法，显著提升了基础模型对虚假相关的鲁棒性，为可信视觉基础模型提供了新路径。

Abstract: Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.

</details>


### [93] [Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions](https://arxiv.org/abs/2601.21873)
*Jinhang Chai,Xuyuan Liu,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 本文提出了一种面向结构化矩阵估计的迁移学习框架，适用于源任务嵌入为子空间的目标任务场景，通过分解目标参数并设计锚定交替投影估计器，在理论和实验上均验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 学习系统常随时间扩展特征或潜在表示空间，但新结构有限；如何在环境维度与内在表示同时增长时有效迁移已学知识，是本文研究动机。

Method: 提出一种通用迁移框架，将目标参数分解为嵌入的源成分、低维低秩创新项和稀疏编辑项；设计锚定交替投影估计器，保持迁移子空间，仅估计低维创新与稀疏修改；建立分离目标噪声、表示增长与源估计误差的确定性误差界。

Result: 理论证明当秩与稀疏性增量较小时，误差率严格优于非迁移方法；在马尔可夫转移矩阵估计（单轨迹、相依噪声）和高维结构协方差估计两个典型问题中验证了框架通用性与一致性迁移增益。

Conclusion: 该框架为结构化矩阵估计中的维度增长式迁移学习提供了统一建模与理论保障，兼顾可解释性与统计效率。

Abstract: Learning systems often expand their ambient features or latent representations over time, embedding earlier representations into larger spaces with limited new latent structure. We study transfer learning for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task.
  We propose a general transfer framework in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small.
  We demonstrate the generality of the framework by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end theoretical guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary theoretical analysis in the appendix and empirically validate consistent transfer gains.

</details>


### [94] [Managing Solution Stability in Decision-Focused Learning with Cost Regularization](https://arxiv.org/abs/2601.21883)
*Victor Spitzer,Francois Sanson*

Main category: cs.LG

TL;DR: 本文研究决策导向学习中组合优化问题目标函数系数估计的稳定性问题，指出扰动强度波动会损害训练效果，并提出一种成本向量正则化方法以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扰动强度在训练过程中波动会导致训练失效，需建立其与组合优化解稳定性之间的理论联系并加以解决。

Method: 提出一种针对估计成本向量的正则化方法，以缓解扰动强度波动带来的负面影响。

Result: 所提正则化方法在大量数值实验中显著提升了学习过程的鲁棒性和可靠性。

Conclusion: 正则化是提升决策导向学习中组合优化系数估计稳定性的有效手段。

Abstract: Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.

</details>


### [95] [Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning](https://arxiv.org/abs/2601.21894)
*Lukas Twist,Shu Yang,Hanqi Yan,Jingzhi Gong,Di Wang,Helen Yannakoudakis,Jie M. Zhang*

Main category: cs.LG

TL;DR: 本文研究了代码的结构复杂性（如控制流和组合结构）如何影响大语言模型（LLM）的推理能力提升，发现相比泛化的代码训练信号，特定结构复杂度范围的代码更有效提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将代码视为通用训练信号，未明确哪些代码特性真正促进推理能力；本文旨在探究代码的结构性复杂度（如控制流、组合性）在细调中对模型多步推理建模的影响。

Method: 基于圈复杂度（cyclomatic complexity）和逻辑代码行数（logical lines of code）构建受控的细调数据集，区分解驱动复杂度（同一问题的不同解）与问题驱动复杂度（不同任务本身复杂度），并在多个开源LLM上评估其在多种推理基准上的表现。

Result: 在83%的实验中，限制细调数据为特定结构复杂度范围的效果优于使用结构多样化的代码；表明代码的结构性属性显著决定其对推理能力提升的有效性。

Conclusion: 代码提升LLM推理能力的关键不在于代码本身，而在于其结构复杂性；通过数据选择（而非单纯扩大规模）可更有效地增强推理能力，提出了一条数据驱动的推理优化路径。

Abstract: Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.

</details>


### [96] [A Low-Complexity Plug-and-Play Deep Learning Model for Generalizable Massive MIMO Precoding](https://arxiv.org/abs/2601.21897)
*Ali Hasanzadeh Karkan,Ahmed Ibrahim,Jean-François Frigon,François Leduc-Primeau*

Main category: cs.LG

TL;DR: 本文提出了一种即插即用的深度学习预编码框架PaPP，通过元学习和自监督训练，实现跨站点、功率和信道误差的泛化能力，在保持高性能的同时大幅降低计算能耗。


<details>
  <summary>Details</summary>
Motivation: 现有mMIMO下行预编码方法（如WMMSE）计算复杂、对SNR和信道估计质量敏感；而现有DL方案鲁棒性差、需为每个部署场景重新训练，缺乏实用性。

Method: 提出PaPP框架：融合高容量教师网络与紧凑学生网络，采用自监督损失（兼顾教师模仿与归一化和速率），结合元学习域泛化和发射功率感知输入归一化；支持全数字（FDP）与混合波束成形（HBF）两种架构，主干网络可跨场景复用。

Result: 在三个未见实测射线追踪站点上，PaPP经少量本地无标签样本微调后，性能均超越传统及DL基线方法；计算能耗建模降低21倍以上，并在信道估计误差下保持良好性能。

Conclusion: PaPP是一种实用、鲁棒、节能且可迁移的mMIMO预编码解决方案，显著提升了DL方法在真实无线部署中的可行性。

Abstract: Massive multiple-input multiple-output (mMIMO) downlink precoding offers high spectral efficiency but remains challenging to deploy in practice because near-optimal algorithms such as the weighted minimum mean squared error (WMMSE) are computationally expensive, and sensitive to SNR and channel-estimation quality, while existing deep learning (DL)-based solutions often lack robustness and require retraining for each deployment site. This paper proposes a plug-and-play precoder (PaPP), a DL framework with a backbone that can be trained for either fully digital (FDP) or hybrid beamforming (HBF) precoding and reused across sites, transmit-power levels, and with varying amounts of channel estimation error, avoiding the need to train a new model from scratch at each deployment. PaPP combines a high-capacity teacher and a compact student with a self-supervised loss that balances teacher imitation and normalized sum-rate, trained using meta-learning domain-generalization and transmit-power-aware input normalization. Numerical results on ray-tracing data from three unseen sites show that the PaPP FDP and HBF models both outperform conventional and deep learning baselines, after fine-tuning with a small set of local unlabeled samples. Across both architectures, PaPP achieves more than 21$\times$ reduction in modeled computation energy and maintains good performance under channel-estimation errors, making it a practical solution for energy-efficient mMIMO precoding.

</details>


### [97] [Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting](https://arxiv.org/abs/2601.21899)
*Zhiqing Cui,Siru Zhong,Ming Jin,Shirui Pan,Qingsong Wen,Yuxuan Liang*

Main category: cs.LG

TL;DR: OmniAir 是一种面向全球空气质量预测的语义拓扑学习框架，通过建模站点物理属性和动态稀疏拓扑，提升跨区域泛化能力，并在新构建的大规模 WorldAir 数据集上实现高效、可扩展的先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决全球空气质量预测中空间异质性极强、现有传导式模型难以泛化到未见区域的问题。

Method: 提出 OmniAir 框架：将不变的物理环境属性编码为通用站点标识，并动态构建自适应稀疏拓扑，以捕捉长程非欧几里得相关性和物理扩散模式；同时构建覆盖全球 7800 多个站点的 WorldAir 数据集。

Result: 在 18 个基线模型上达到最优性能，推理速度接近现有模型的 10 倍，显著缓解数据稀疏区域的监测缺口。

Conclusion: OmniAir 有效提升了全球站点级空气质量预测的泛化性、效率与可扩展性，为跨区域环境建模提供了新范式。

Abstract: Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.

</details>


### [98] [Hardware-Triggered Backdoors](https://arxiv.org/abs/2601.21902)
*Jonas Möller,Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.LG

TL;DR: 本文提出了一种利用硬件数值差异植入模型后门的新攻击方法，通过调整决策边界使同一输入在不同硬件上产生不同预测。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习模型常部署于多种硬件平台，但硬件设计差异导致的微小数值变化可能被恶意利用，现有研究尚未系统探讨这种硬件依赖性后门威胁。

Method: 通过局部调整模型决策边界靠近目标样本，并精细调控数值偏差，使得特定硬件上的推理结果发生预测翻转，从而实现硬件触发的后门。

Result: 实验表明该方法可在常见GPU加速器上稳定构造硬件触发后门，验证了其可行性与可靠性。

Conclusion: 硬件数值差异构成新型模型安全威胁，尤其影响第三方模型部署，需针对性防御机制。

Abstract: Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.

</details>


### [99] [Optimistic Transfer under Task Shift via Bellman Alignment](https://arxiv.org/abs/2601.21924)
*Jinhang Chai,Enpei Zhang,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 本文研究了在线迁移强化学习，提出了重加权目标（RWT）方法，通过贝尔曼对齐实现源任务数据的统计可靠复用，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习中，源任务与目标任务的相似性通常定义在奖励或转移函数上，而算法操作的是贝尔曼回归目标，导致直接复用源数据会引入系统偏差并破坏遗憾界保证。

Method: 提出了一步贝尔曼对齐作为在线RL迁移的正确抽象，并设计了重加权目标（RWT）算子级校正方法，通过测度变换调整后续值函数并补偿转移不匹配。

Result: 构建了两阶段RWT Q-learning框架，分离方差减小与偏差校正；在RKHS函数逼近下，遗憾界仅依赖任务偏移复杂度而非目标MDP复杂度；实验表明其在表格型和神经网络设置下均优于单任务学习和简单数据合并。

Conclusion: 贝尔曼对齐是一种模型无关的在线强化学习迁移原则，RWT为源数据的统计可靠复用提供了理论保障和实用方案。

Abstract: We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.
  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.
  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.

</details>


### [100] [LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution](https://arxiv.org/abs/2601.21929)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: 本文提出LoRIF方法，通过利用梯度的低秩结构解决大规模训练数据归因（TDA）中的存储与计算瓶颈，显著降低I/O开销和Hessian近似内存消耗，在保持甚至提升归因质量的同时实现高达20倍的存储与查询加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的TDA方法（如TRAK、LoGRA）在扩展至大规模训练集和高精度归因时面临两大瓶颈：(i) 投影后每样本梯度的存储/加载I/O开销随投影维数D线性增长；(ii) D×D逆Hessian近似的O(D²)内存成本。而提高D又对归因质量至关重要，形成质量-可扩展性权衡。

Method: LoRIF引入两项关键技术：(1) 对投影后的每样本梯度采用秩-c因子分解存储，将单层单样本的存储与I/O从O(D)降至O(c√D)；(2) 在r维子空间中结合截断SVD与Woodbury恒等式近似Hessian项，将内存从O(D²)降至O(Dr)。

Result: 在0.1B至70B参数模型及百万级数据集上，LoRIF相较LoGRA实现最高20倍的存储节省与查询加速，同时归因质量持平或更优。

Conclusion: LoRIF有效打破TDA中质量与可扩展性的固有矛盾，使基于梯度的大规模、前沿模型训练数据归因真正实用化。

Abstract: Training data attribution (TDA) identifies which training examples most influenced a model's prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \emph{(ii)} forming the $D \times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality -- creating a quality-scalability tradeoff. We introduce \textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.

</details>


### [101] [Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models](https://arxiv.org/abs/2601.21943)
*Ahmad Aghapour,Erhan Bayraktar,Ziqing Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种信息论方法来分析扩散生成模型的收敛性，实现了与维度无关的KL散度上界，并提出了仅依赖训练损失的轻量级Loss-Adaptive Schedule（LAS）采样策略，实验证明其优于常用启发式调度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的收敛性分析通常随环境维度线性或更差地缩放，且常依赖目标分布的内在维数或几何假设；本文旨在摆脱这些限制，建立更普适、更紧致的收敛保证。

Method: 基于信息论（特别是Shannon熵）建立维度无关的KL散度收敛上界；通过KL散度的重构形式，设计无需后处理、仅用训练损失驱动的Loss-Adaptive Schedule（LAS）用于反向SDE离散化。

Result: 在温和假设下，证明KL散度误差上界为O(H²/K)（H为Shannon熵，K为采样步数）；LAS调度在图像生成任务中显著提升采样质量。

Conclusion: 信息论视角可提供更本质、更鲁棒的扩散模型收敛性分析；LAS是一种高效、实用且即插即用的采样调度方法，推动了理论与实践的统一。

Abstract: Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.

</details>


### [102] [Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models](https://arxiv.org/abs/2601.21944)
*Konstantinos P. Panousis,Diego Marcos*

Main category: cs.LG

TL;DR: 本文提出了一种名为'清晰度（clarity）'的新指标，用于系统评估概念瓶颈模型（CBMs）中概念表示的稀疏性、精确性与下游性能之间的权衡，并构建了基于真实概念标注数据集的可解释性评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）常被视为黑箱，缺乏对其决策过程的深入理解；尽管已有多种可解释性方法，但对稀疏感知型方法所学习表征的系统、客观评估仍严重不足。

Method: 聚焦于概念瓶颈模型（CBMs），定义并量化'清晰度'指标，结合具有真实概念标注的数据集构建可解释性评估框架；对比VLM-based与属性预测器-based CBMs，以及三种稀疏诱导策略（ℓ₁、ℓ₀、Bernoulli）。

Result: 实验揭示了模型灵活性与可解释性之间存在关键权衡：即使在相近下游性能下，不同方法的概念表示在稀疏性与精度上表现差异显著。

Conclusion: 单纯追求高下游性能不足以保证可解释性；需通过如'清晰度'等综合指标，系统评估并指导CBMs的设计与选择，以真正实现‘诱导可解释性’的目标。

Abstract: The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to "induce interpretability". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.

</details>


### [103] [Dependence of Equilibrium Propagation Training Success on Network Architecture](https://arxiv.org/abs/2601.21945)
*Qingshan Wang,Clara C. Wanjura,Florian Marquardt*

Main category: cs.LG

TL;DR: 本文研究了基于物理的均衡传播（equilibrium propagation）训练方法在局部连接晶格架构（如XY模型）上的性能，发现稀疏的局部连接网络可达到与全连接密集网络相当的性能，为在现实硬件约束下扩展该方法提供了指导。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展导致能源消耗不可持续，促使人们探索神经形态计算和基于物理的机器学习训练方法；但现有理论研究多聚焦于全连接或密集分层网络，难以在实验中实现（如受连通性限制）。

Method: 采用基于物理的均衡传播（equilibrium propagation）方法，在局部连接的晶格结构（特别是XY模型）上进行训练，并在多个基准任务上评估性能，同时追踪训练过程中空间分布的响应和耦合演化。

Result: 稀疏的、仅具局部连接的网络在基准任务上可实现与密集网络相当的性能。

Conclusion: 局部连接架构适用于均衡传播训练，为在真实物理硬件（受限于连接性）中规模化部署该方法提供了可行路径和设计指南。

Abstract: The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.

</details>


### [104] [Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities](https://arxiv.org/abs/2601.21950)
*Linxiao Gong,Yang Liu,Lianlong Sun,Yulai Bi,Jing Liu,Xiaoguang Zhu*

Main category: cs.LG

TL;DR: 本文提出Aleatoric Uncertainty Modeling (AUM)框架，通过建模单模态的偶然不确定性（以多元高斯分布表示），并在双部图上设计不确定性感知的消息传递机制，以自适应融合多模态信息，有效应对临床中普遍存在的模态缺失问题，在MIMIC-IV和eICU数据集上显著提升死亡率预测性能。


<details>
  <summary>Details</summary>
Motivation: 临床实践中常存在模态缺失问题，而现有方法假设各模态贡献均等且缺失模式随机，忽略了医学数据采集固有的偶然不确定性。

Method: 提出AUM框架：1）将每个单模态表征建模为多元高斯分布以量化偶然不确定性；2）构建患者-模态二部图，设计不确定性感知的动态消息传递机制实现自适应信息聚合。

Result: 在MIMIC-IV死亡率预测任务上AUC-ROC提升2.26%，在eICU上提升2.17%，优于现有SOTA方法。

Conclusion: 显式建模单模态偶然不确定性并结合不确定性驱动的动态融合机制，能更鲁棒地处理医学多模态中的模态缺失问题。

Abstract: Medical multimodal learning faces significant challenges with missing modalities prevalent in clinical practice. Existing approaches assume equal contribution of modality and random missing patterns, neglecting inherent uncertainty in medical data acquisition. In this regard, we propose the Aleatoric Uncertainty Modeling (AUM) that explicitly quantifies unimodal aleatoric uncertainty to address missing modalities. Specifically, AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification. To adaptively aggregate captured information, we develop a dynamic message-passing mechanism within a bipartite patient-modality graph using uncertainty-aware aggregation mechanism. Through this process, missing modalities are naturally accommodated, while more reliable information from available modalities is dynamically emphasized to guide representation generation. Our AUM framework achieves an improvement of 2.26% AUC-ROC on MIMIC-IV mortality prediction and 2.17% gain on eICU, outperforming existing state-of-the-art approaches.

</details>


### [105] [Uncertainty-Aware Data-Based Method for Fast and Reliable Shape Optimization](https://arxiv.org/abs/2601.21956)
*Yunjia Yang,Runze Li,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: 本文提出了一种不确定性感知的数据驱动优化（UA-DBO）框架，通过概率编码器-解码器代理模型量化预测不确定性，并将其融入目标函数以提升优化鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动优化（DBO）依赖训练数据库质量，对分布外样本预测误差大，易误导优化；因此需引入不确定性量化以增强鲁棒性。

Method: 构建概率编码器-解码器代理模型以预测输出不确定性，并设计模型置信度感知的目标函数，在优化中惩罚高不确定性样本。

Result: 在两类多工况翼型优化问题（减阻发散与抗抖振）中，UA-DBO显著降低优化样本的预测误差，性能优于原始DBO；相比全CFD多工况优化，效果相当但速度大幅提升。

Conclusion: UA-DBO通过显式建模与利用代理模型不确定性，有效提升了数据驱动气动形状优化的可靠性与效率，为实际工程应用提供了更稳健的优化范式。

Abstract: Data-based optimization (DBO) offers a promising approach for efficiently optimizing shape for better aerodynamic performance by leveraging a pretrained surrogate model for offline evaluations during iterations. However, DBO heavily relies on the quality of the training database. Samples outside the training distribution encountered during optimization can lead to significant prediction errors, potentially misleading the optimization process. Therefore, incorporating uncertainty quantification into optimization is critical for detecting outliers and enhancing robustness. This study proposes an uncertainty-aware data-based optimization (UA-DBO) framework to monitor and minimize surrogate model uncertainty during DBO. A probabilistic encoder-decoder surrogate model is developed to predict uncertainties associated with its outputs, and these uncertainties are integrated into a model-confidence-aware objective function to penalize samples with large prediction errors during data-based optimization process. The UA-DBO framework is evaluated on two multipoint optimization problems aimed at improving airfoil drag divergence and buffet performance. Results demonstrate that UA-DBO consistently reduces prediction errors in optimized samples and achieves superior performance gains compared to original DBO. Moreover, compared to multipoint optimization based on full computational simulations, UA-DBO offers comparable optimization effectiveness while significantly accelerating optimization speed.

</details>


### [106] [From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation](https://arxiv.org/abs/2601.21964)
*Qianwei Yang,Dong Xu,Zhangfan Yang,Sisi Yuan,Zexuan Zhu,Jianqiang Li,Junkai Ji*

Main category: cs.LG

TL;DR: 本文提出SoftMol框架，通过软片段表示、块扩散语言模型SoftBD及门控蒙特卡洛树搜索，实现靶向分子生成，在有效性、亲和力、多样性与推理效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GPT的分子语言模型难以充分建模分子图结构，且缺乏显式的靶向生成机制。

Method: 提出软片段（soft fragments）作为SMILES的无规则块表示；构建首个结合局部双向扩散与自回归生成的块扩散模型SoftBD；在ZINC-Curated数据集上训练；集成门控蒙特卡洛树搜索实现靶向片段组装。

Result: 相比SOTA模型，SoftMol达到100%化学有效性，结合亲和力提升9.7%，分子多样性提高2–3倍，推理速度提升6.6倍。

Conclusion: SoftMol通过统一设计分子表示、模型架构与搜索策略，有效克服了传统分子语言模型在结构建模与靶向生成上的局限，为AI驱动的药物发现提供了新范式。

Abstract: Drug discovery can be viewed as a combinatorial search over an immense chemical space, motivating the development of deep generative models for de novo molecular design. Among these, GPT-based molecular language models (MLM) have shown strong molecular design performance by learning chemical syntax and semantics from large-scale data. However, existing MLMs face two fundamental limitations: they inadequately capture the graph-structured nature of molecules when formulated as next-token prediction problems, and they typically lack explicit mechanisms for target-aware generation. Here, we propose SoftMol, a unified framework that co-designs molecular representation, model architecture, and search strategy for target-aware molecular generation. SoftMol introduces soft fragments, a rule-free block representation of SMILES that enables diffusion-native modeling, and develops SoftBD, the first block-diffusion molecular language model that combines local bidirectional diffusion with autoregressive generation under molecular structural constraints. To favor generated molecules with high drug-likeness and synthetic accessibility, SoftBD is trained on a carefully curated dataset named ZINC-Curated. SoftMol further integrates a gated Monte Carlo tree search to assemble fragments in a target-aware manner. Experimental results show that, compared with current state-of-the-art models, SoftMol achieves 100% chemical validity, improves binding affinity by 9.7%, yields a 2-3x increase in molecular diversity, and delivers a 6.6x speedup in inference efficiency. Code is available at https://github.com/szu-aicourse/softmol

</details>


### [107] [Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2601.21978)
*Shiqi Fan,Quanming Yao,Hongyi Nie,Wentao Ma,Zhen Wang,Wen Hua*

Main category: cs.LG

TL;DR: 本文提出IGETR框架，结合图神经网络（GNN）的结构化时序建模能力与大语言模型（LLM）的上下文理解能力，通过三阶段流程（路径识别、LLM引导编辑、路径整合）提升时序知识图谱推理性能，在ICEWS等基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的时序知识图谱推理方法偏重上下文而忽视结构关系，难以从动态图中提取相关子图，导致结构信息理解不足、推理易产生幻觉且难以处理时间不一致性。

Method: 提出IGETR混合推理框架，包含三个阶段：1）使用时序GNN识别结构与时间一致的候选路径，确保推理基于可靠图证据；2）引入LLM引导的路径编辑机制，利用外部知识修正逻辑与语义不一致；3）整合优化后的路径生成准确且可解释的预测结果。

Result: 在标准TKG基准（如ICEWS）上取得SOTA性能，Hits@1和Hits@3相对强基线最高提升5.6%和8.1%；消融实验与额外分析验证了各模块有效性。

Conclusion: IGETR通过融合GNN的结构化时序建模与LLM的语义理解能力，有效缓解了纯LLM方法在TKGR任务中因忽略图结构和时间动态性而导致的幻觉与不一致问题，显著提升了推理准确性与可解释性。

Abstract: Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.

</details>


### [108] [Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance](https://arxiv.org/abs/2601.21979)
*Ciaran Bench,Vivek Desai,Carlijn Roozemond,Ruben van Engen,Spencer A. Thomas*

Main category: cs.LG

TL;DR: 本文探讨了Fréchet Inception Distance（FID）在医学图像质量评估中的局限性，提出利用Monte Carlo Dropout计算FID及特征嵌入模型潜在表示的预测方差，以量化输入图像相对于ImageNet1K训练数据的分布外程度，并分析其作为FID可信度指标的有效性。


<details>
  <summary>Details</summary>
Motivation: FID依赖于在自然图像（ImageNet1K）上预训练的InceptionV3模型提取特征，但其在医学图像等分布外数据上的有效性不足，且这种失效程度尚不明确。

Method: 采用Monte Carlo Dropout方法，分别计算FID值和特征嵌入模型潜在表示的预测方差；在ImageNet1K验证集（不同强度增强）及其他外部数据集上进行实验，分析预测方差与分布外程度的相关性。

Result: 所考察的预测方差大小与测试输入相对于训练数据的分布外程度呈现不同程度的相关性，表明其可作为FID可信度的辅助指示器。

Conclusion: 预测方差可为FID在非自然图像（如医学图像）应用中的可靠性提供可量化的参考依据，有助于更审慎地解读FID结果。

Abstract: Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fréchet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.

</details>


### [109] [Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks](https://arxiv.org/abs/2601.21983)
*Andrew Millard,Joshua Murphy,Peter Green,Simon Maskell*

Main category: cs.LG

TL;DR: 本文提出了一种在序贯蒙特卡洛（SMC）采样中引入数据退火（data annealing）的方法，通过逐步增加mini-batch数据量来降低计算开销，在图像分类任务上实现最高6倍加速且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 传统SMC等粒子方法虽不假设后验分布形式，但因依赖全批量数据导致计算昂贵；而变分推断虽高效却受限于变分分布形式的选择。本文旨在兼顾准确性和效率，改进SMC的可扩展性。

Method: 在SMC框架中引入数据退火策略，即随采样迭代逐步增加用于似然与梯度计算的mini-batch数量，从而平衡采样质量与计算成本。

Result: 在标准图像分类基准上，所提方法相比标准SMC实现最高6倍训练加速，同时保持几乎无损的分类精度。

Conclusion: 数据退火是一种有效提升SMC在贝叶斯神经网络中可扩展性的实用技术，为高精度贝叶斯推断提供了更高效的替代方案。

Abstract: Bayesian inference allows us to define a posterior distribution over the weights of a generic neural network (NN). Exact posteriors are usually intractable, in which case approximations can be employed. One such approximation - variational inference - is computationally efficient when using mini-batch stochastic gradient descent as subsets of the data are used for likelihood and gradient evaluations, though the approach relies on the selection of a variational distribution which sufficiently matches the form of the posterior. Particle-based methods such as Markov chain Monte Carlo and Sequential Monte Carlo (SMC) do not assume a parametric family for the posterior by typically require higher computational cost. These sampling methods typically use the full-batch of data for likelihood and gradient evaluations, which contributes to this computational expense. We explore several methods of gradually introducing more mini-batches of data (data annealing) into likelihood and gradient evaluations of an SMC sampler. We find that we can achieve up to $6\times$ faster training with minimal loss in accuracy on benchmark image classification problems using NNs.

</details>


### [110] [PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters](https://arxiv.org/abs/2601.21984)
*Jian Gao,Yiwei Zou,Abhishek Pradhan,Wenhao Huang,Yumin Su,Kaiyuan Yang,Xuan Zhang*

Main category: cs.LG

TL;DR: 本文提出了PowerGenie框架，用于大规模自动化发现高性能可重构电源转换器拓扑结构，结合解析建模与进化微调方法，在有效性、新颖性和性能指标上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统人工设计难以应对指数级增长的电路拓扑搜索空间；现有AI方法受限于模板依赖、规模小或缺乏严格验证，难以实现大规模性能驱动的拓扑发现。

Method: 提出PowerGenie框架：(1) 基于自动解析分析确定拓扑功能与理论性能极限（无需器件尺寸设定或SPICE仿真）；(2) 设计进化式微调方法，协同优化生成模型及其训练分布，通过适应度选择与唯一性验证避免模式坍缩和过拟合。

Result: 发现一种新型8模式可重构转换器，其性能指标（FoM）比最优训练拓扑高23%；SPICE仿真显示8种模式平均绝对效率提升10%，单模式最高达17%；在语法有效性、功能有效性、新颖率和FoM方面均优于现有方法。

Conclusion: PowerGenie为大规模、性能驱动的电源拓扑自动化发现提供了新范式，兼具理论严谨性与工程实用性，显著拓展了AI在电力电子设计中的应用边界。

Abstract: Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie.

</details>


### [111] [Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields](https://arxiv.org/abs/2601.21985)
*Yunyang Li,Lin Huang,Luojia Xia,Wenhe Zhang,Mark Gerstein*

Main category: cs.LG

TL;DR: 本文提出Elign框架，通过在训练阶段引入物理引导（使用预训练MLFF替代DFT）和强化学习方法FED-GRPO，提升E(3)-equivariant扩散模型生成3D分子构象的热力学合理性与机械稳定性，同时保持推理速度不变。


<details>
  <summary>Details</summary>
Motivation: 现有E(3)-equivariant扩散模型易受半经验训练数据偏差影响，难以准确建模高保真哈密顿量下的平衡分布；而实时物理引导因DFT计算昂贵且需每步重复查询，计算开销大。

Method: 提出Elign：1）用预训练机器学习力场（MLFF）替代DFT提供物理信号；2）将物理引导前移至训练阶段，将反向扩散建模为强化学习问题，并设计Force–Energy Disentangled Group Relative Policy Optimization（FED-GRPO）算法，独立优化基于势能的奖励和基于力的稳定性奖励，并进行组归一化。

Result: 实验表明Elign生成的构象在金标准DFT能量和力上更低，结构更稳定；且推理速度与无引导采样相当（生成时无需任何能量评估）。

Conclusion: Elign通过训练阶段的物理引导与解耦式强化学习优化，有效缓解了生成模型对训练数据偏差的依赖，在不牺牲推理效率的前提下显著提升了分子构象的物理合理性与稳定性。

Abstract: Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.

</details>


### [112] [Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains](https://arxiv.org/abs/2601.21999)
*Meng Cao,Jiexi Liu,Songcan Chen*

Main category: cs.LG

TL;DR: 本文提出了负主导对比学习（NDCL）方法，用于解决不平衡域泛化（IDG）问题，通过理论分析建立了IDG泛化界，并设计了增强判别性与跨域后验一致性的技术方案。


<details>
  <summary>Details</summary>
Motivation: IDG同时缓解域偏移和标签偏移，但在异构长尾分布下仍缺乏理论基础与有效方法，且二者耦合带来技术挑战。

Method: 提出负主导对比学习（NDCL）：以负样本为主导增强类间决策边界分离；采用重加权交叉熵提升类内紧凑性；引入预测中心对齐策略保障跨域后验一致性。

Result: 在多个具有挑战性的基准上验证了NDCL的有效性，显著提升了IDG性能。

Conclusion: 直接调控决策边界是IDG的关键路径；NDCL通过协同优化判别性、紧凑性与后验一致性，为IDG提供了新范式与理论支撑。

Abstract: Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.

</details>


### [113] [Rate-Distortion Optimization for Transformer Inference](https://arxiv.org/abs/2601.22002)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 本文提出了一种基于率失真理论的有损压缩框架，用于在多设备推理中高效压缩Transformer中间表示，在降低比特率的同时保持甚至提升模型精度，并从信息论角度分析和解释了其性能边界。


<details>
  <summary>Details</summary>
Motivation: Transformer模型推理时计算和内存开销大，跨设备分割推理需压缩中间表示，但现有方法缺乏对率-精度权衡的原理性建模。

Method: 构建基于率失真理论的可学习压缩框架，显式优化比特率与任务准确率之间的权衡；引入信息论概念定义率与熵的差距，并推导其理论界；进一步给出PAC风格的该差距估计界。

Result: 在语言基准上，所提编解码器显著降低比特率，部分情况下反而提升精度，且优于更复杂的基线方法；实验证明不同架构/任务的编码率受所推导界驱动。

Conclusion: 率失真框架为Transformer中间表示压缩提供了统一、可解释、有理论保证的建模范式， bridging information theory and practical model compression.

Abstract: Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.

</details>


### [114] [Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering](https://arxiv.org/abs/2601.22010)
*Dongxuan Zhu,Ly Tran Ho Khanh,Andy Yat-Ming Cheung,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 本文提出STARS方法，一种无需训练、仅在推理时干预的激活引导技术，通过在Stiefel流形上联合优化多个正交引导方向，提升语言模型多路生成的多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成路径同质化、易发生模式坍塌，现有采样策略难以保障多路并发生成的多样性。

Method: 提出STARS方法：在每个token位置收集多路并发生成的隐藏激活，于Stiefel流形上联合优化多个加性引导方向，以最大化引导后激活的几何体积，并设计轻量级一步闭式更新替代耗时的黎曼梯度下降。

Result: 在测试用例生成与科学发现基准上，STARS持续优于标准采样方法，在不牺牲生成质量前提下显著提升多样性。

Conclusion: STARS是一种高效、通用、无需训练的推理时干预方法，能显式促进激活发散、隐式提升生成轨迹多样性，为提升大模型生成多样性提供了新范式。

Abstract: Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.

</details>


### [115] [Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability](https://arxiv.org/abs/2601.22012)
*Sergi Masip,Gido M. van de Ven,Javier Ferrando,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 本文提出了一种机制性框架，从几何角度解释灾难性遗忘是由于个体特征编码变换导致的，影响特征表征能力和下游读出，并通过可处理模型分析和实验验证，最后用Crosscoder分析ViT在CIFAR-10上的连续学习表现。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习中对灾难性遗忘的度量多停留在性能或最后一层表征层面，忽略了其内在机制，本文旨在从特征编码的几何变换角度深入理解遗忘本质。

Method: 构建一个机制性几何框架，形式化分析特征编码变换如何导致容量下降与读出干扰；在可处理模型上进行理论推导与实验验证；引入Crosscoder工具对实际模型（如Vision Transformer）进行实证分析。

Result: 明确了灾难性遗忘的最优与最差情形；实验证实深度会加剧遗忘；成功将框架应用于ViT在CIFAR-10序列任务中的分析。

Conclusion: 本文提供了以特征为中心的持续学习新视角与分析语言，有助于更深入理解并缓解灾难性遗忘。

Abstract: Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computations. Analysis of a tractable model formalizes this view, allowing us to identify best- and worst-case scenarios. Through experiments on this model, we empirically test our formal analysis and highlight the detrimental effect of depth. Finally, we demonstrate how our framework can be used in the analysis of practical models through the use of Crosscoders. We present a case study of a Vision Transformer trained on sequential CIFAR-10. Our work provides a new, feature-centric vocabulary for continual learning.

</details>


### [116] [TBDFiltering: Sample-Efficient Tree-Based Data Filtering](https://arxiv.org/abs/2601.22016)
*Robert Istvan Busa-Fekete,Julian Zimmert,Anne Xiangyi Zheng,Claudio Gentile,Andras Gyorgy*

Main category: cs.LG

TL;DR: 本文提出一种基于文本嵌入的层次聚类方法，自适应选择少量文档供大语言模型（LLM）评估，以高效估计整个数据集质量，在保证准确率的同时显著减少LLM查询次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练数据筛选缺乏廉价可靠的文档质量度量；直接用LLM逐条评估海量文档（数十亿）不可扩展；而基于稀疏信号训练的分类器效果有限。

Method: 提出基于文本嵌入的层次聚类方法，自适应选择代表性文档交由LLM评估，并理论证明其查询效率：在存在近纯子树假设下，仅需查询与最小纯子树规模成比例的文档即可高概率准确判定全部文档质量。

Result: 理论证明该方法具有查询高效性；实验表明其在数据质量过滤任务上优于现有分类器基方法。

Conclusion: 所提层次聚类+LLM轻量评估框架，可在不牺牲质量判别准确性前提下，大幅降低对LLM的调用开销，为大规模训练数据筛选提供可扩展、可靠的新范式。

Abstract: The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.

</details>


### [117] [From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning](https://arxiv.org/abs/2601.22028)
*Haoran Tang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 本文提出CLReg方法，通过对比表示正则化显式降低遗忘概念与保留知识在表征空间中的纠缠，提升大语言模型的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘方法多在预测空间进行对齐式优化，虽能抑制遗忘内容生成，但遗忘概念仍可能残留在表征中并与保留知识纠缠，导致遗忘不彻底。

Method: 提出对比表征正则器CLReg，识别遗忘特征并将其推离保留特征，在最小扰动保留特征的前提下显式降低二者表征纠缠；并提供首个将表征塑形与纠缠减少联系起来的理论分析。

Result: 在多个遗忘基准和不同规模的大语言模型上，CLReg显著降低了遗忘-保留表征纠缠，提升了主流遗忘方法的效果，且不引入额外隐私风险。

Conclusion: 表征空间的主动塑形是实现更彻底遗忘的有效新路径，为未来研究提供了理论与方法启示。

Abstract: Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.

</details>


### [118] [The Ensemble Inverse Problem: Applications and Methods](https://arxiv.org/abs/2601.22029)
*Zhengyan Huan,Camila Pazos,Martin Klassen,Vincent Croft,Pierre-Hugues Beauchemin,Shuchin Aeron*

Main category: cs.LG

TL;DR: 本文提出了一个名为集合逆问题（EIP）的新多变量统计问题，旨在通过前向过程的推移分布反演先验分布的集合；提出了一类不依赖推理时显式迭代调用前向模型的条件生成模型——集合逆生成模型，利用观测集合中的集合信息提升后验推断，并实现对未见先验的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统逆问题（如高能物理中的unfolding、全波形反演、未知先验的逆成像）通常依赖迭代优化或显式前向模型，计算昂贵且泛化性差；本文旨在构建一种非迭代、推理高效、能泛化至新先验的后验采样方法。

Method: 提出‘集合逆生成模型’，在训练阶段使用多组符合同一前向模型但来自不同先验的真值-观测对，隐式学习似然模型；推理时仅需一次前向生成，融合单次测量与观测集合的统计信息构造后验采样器。

Result: 在逆成像、高能物理和全波形反演等多个合成与真实数据集上验证了方法的有效性：相比现有方法，显著提升后验推断质量与计算效率，并展现出对未见先验的良好泛化能力。

Conclusion: 集合逆问题为一类广泛存在的逆建模任务提供了统一框架；所提出的非迭代、集合感知的生成建模方法，避免了推理时对前向模型的显式迭代调用，兼具高效性、泛化性与实用性。

Abstract: We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.

</details>


### [119] [Per-parameter Task Arithmetic for Unlearning in Large Language Models](https://arxiv.org/abs/2601.22030)
*Chengyi Cai,Zesheng Ye,Jiangchao Yao,Jianzhong Qi,Bo Han,Xiaolu Zhang,Feng Liu,Jun Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种名为PerTA的逐参数任务算术方法，用于大语言模型（LLM）中的高效且精准的信息遗忘，通过梯度或Fisher信息估计各参数对遗忘与保留任务的重要性，并动态缩放任务向量，显著缓解过遗忘问题，同时保持高模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量（TV）方法虽高效但易导致过遗忘，因未区分各参数在遗忘与保留任务中的不同重要性。

Method: 提出逐参数任务算术（PerTA）机制，利用梯度（PerTA-grad）或对角Fisher信息近似（PerTA-fisher）估计每个参数对遗忘与保留的相对重要性，并据此对任务向量进行逐参数缩放。

Result: 实验表明PerTA在遗忘效果和模型整体效用上均持续优于标准TV方法，在许多情况下甚至超过主流基于训练的遗忘方法。

Conclusion: PerTA在保持任务算术高效性的同时，有效缓解过遗忘，为LLM遗忘提供了一个原理清晰、实用性强的新框架。

Abstract: In large language model (LLM) unlearning, private information is required to be removed. Task arithmetic unlearns by subtracting a specific task vector (TV)--defined as the parameter difference between a privacy-information-tuned model and the original model. While efficient, it can cause over-forgetting by disrupting parameters essential for retaining other information. Motivated by the observation that each parameter exhibits different importance for forgetting versus retention, we propose a per-parameter task arithmetic (PerTA) mechanism to rescale the TV, allowing per-parameter adjustment. These weights quantify the relative importance of each parameter for forgetting versus retention, estimated via gradients (i.e., PerTA-grad) or the diagonal Fisher information approximation (i.e., PerTA-fisher). Moreover, we discuss the effectiveness of PerTA, extend it to a more general form, and provide further analysis. Extensive experiments demonstrate that PerTA consistently improves upon standard TV, and in many cases surpasses widely used training-based unlearning methods in both forgetting effectiveness and overall model utility. By retaining the efficiency of task arithmetic while mitigating over-forgetting, PerTA offers a principled and practical framework for LLM unlearning.

</details>


### [120] [Holographic generative flows with AdS/CFT](https://arxiv.org/abs/2601.22033)
*Ehsan Mirafzali,Sanjit Shashi,Sanya Murdeshwar,Edgar Shaghoulian,Daniele Venturi,Razvan Marinescu*

Main category: cs.LG

TL;DR: 本文提出了一种结合AdS/CFT对偶原理与深度学习的生成式机器学习框架，将数据流建模为AdS空间中的标量场体-边界映射，提升了flow-matching算法的收敛速度与质量，并赋予其物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索量子引力中的全息原理（特别是AdS/CFT）能否为生成式建模提供新范式和物理可解释性。

Method: 将数据分布变换建模为反德西特（AdS）时空中的标量场体-边界映射，将flow-matching算法嵌入AdS物理框架，结合深度学习与输运理论实现生成建模。

Result: 在checkerboard和MNIST数据集上，该模型比传统flow-matching方法收敛更快、质量更高，并具备物理可解释性。

Conclusion: AdS物理与几何可有效启发并提升生成建模，为机器学习引入新的理论物理驱动范式。

Abstract: We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.

</details>


### [121] [Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space](https://arxiv.org/abs/2601.22036)
*Xiaolong Zhang,Jianwei Zhang,Xubo Song*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量方法——交叉融合距离（CFD），用于量化表示空间中数据组之间的融合程度与可分性，尤其适用于域偏移场景。CFD能区分影响融合的几何位移因素，同时对不影响融合的全局缩放和采样布局变化保持不变，并具有线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有分布距离度量混淆了影响融合的几何因素（如组间位移）与不影响融合的变换（如全局缩放、采样变化），导致无法准确反映真实融合程度。

Method: 提出交叉融合距离（CFD），通过理论刻画其不变性与敏感性，并在合成实验中验证；在真实域偏移数据集上评估其与下游泛化性能退化的相关性。

Result: CFD具有线性时间复杂度，理论上具备对融合保持变换的不变性及对融合改变几何的敏感性；实验表明其比常用替代指标更贴合下游泛化性能退化趋势。

Conclusion: CFD是一种理论严谨、可解释性强、实用有效的表示学习距离度量，特别适用于域偏移下的融合分析。

Abstract: Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.

</details>


### [122] [Making Foundation Models Probabilistic via Singular Value Ensembles](https://arxiv.org/abs/2601.22068)
*Mehmet Ozgur Turkoglu,Dominik J. Mühlematter,Alexander Becker,Konrad Schindler,Helge Aasen*

Main category: cs.LG

TL;DR: 本文提出了一种名为奇异值集成（SVE）的参数高效隐式集成方法，用于大模型的不确定性估计，仅通过调节共享奇异向量对应的奇异值来构建集成，显著降低计算开销并保持校准性与准确性。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型预测常过度自信且未校准；显式模型集成虽有效但计算成本过高，难以应用于大模型。

Method: 基于权重矩阵奇异向量表征‘知识方向’的假设，SVE冻结奇异向量，仅训练各集成成员对应的可学习奇异值，通过联合训练中随机初始化和小批量采样自然引入多样性。

Result: SVE在NLP与视觉任务上达到与显式深度集成相当的不确定性量化效果，参数增量<1%，同时提升模型校准性且不损害预测精度。

Conclusion: SVE为资源受限场景下大模型提供了轻量、高效、原理清晰的不确定性估计新范式。

Abstract: Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) "knowledge directions". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.

</details>


### [123] [Where Do the Joules Go? Diagnosing Inference Energy Consumption](https://arxiv.org/abs/2601.22076)
*Jae-Won Chung,Ruofan Wu,Jeff J. Ma,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 本文通过大规模实证研究，分析了46个生成式AI模型在NVIDIA H100和B200 GPU上推理时间与能耗的差异，揭示了任务类型、模态（如视频vs图像）和GPU利用率等因素可导致数十倍能耗差异，并提出一个跨算法-软件-硬件层的能耗归因框架，支持对时间、能量及能效（吞吐/瓦特）的系统性诊断。


<details>
  <summary>Details</summary>
Motivation: 能源已成为机器学习计算的关键资源，仅测量能耗趋势不够，需深入理解差异成因以支撑优化。

Method: 开展涵盖46个模型、7项任务、1858种配置的大规模推理时间与能耗测量实验（H100/B200 GPU），并基于结果构建多层归因框架，将时间与能耗映射至内存、利用率等潜在指标，并关联算法、软件、硬件各层影响因素。

Result: 发现LLM任务类型导致最高25倍能耗差异；视频生成能耗超图像生成100倍以上；GPU利用率差异带来3–5倍能耗变化；提出可直接扩展至'吞吐/瓦特'能效评估的归因框架。

Conclusion: 生成式AI推理能耗差异主要源于跨层因素对内存访问与硬件利用率的影响，系统性归因框架为能效优化提供了可解释、可操作的诊断基础。

Abstract: Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.

</details>


### [124] [Boosting CVaR Policy Optimization with Quantile Gradients](https://arxiv.org/abs/2601.22100)
*Yudong Luo,Erick Delage*

Main category: cs.LG

TL;DR: 本文提出了一种结合条件风险价值（CVaR）与期望分位数项的策略梯度方法，以提升CVaR优化的样本效率，同时保持风险规避目标不变。


<details>
  <summary>Details</summary>
Motivation: CVaR-PG方法因仅关注尾部性能而忽略大量采样轨迹，导致样本效率低下。

Method: 在CVaR目标中引入期望分位数项，利用分位数优化的动态规划特性，使所有采样数据得以有效利用。

Result: 在具有可验证风险规避行为的任务中，该算法在马尔可夫策略类下显著优于CVaR-PG及其他现有方法。

Conclusion: 所提方法在不改变CVaR本质的前提下，有效提升了样本效率，并在多个风险敏感任务中展现出更强的性能。

Abstract: Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.

</details>


### [125] [Prior-Informed Flow Matching for Graph Reconstruction](https://arxiv.org/abs/2601.22107)
*Harvey Chen,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: PIFM是一种结合嵌入先验与连续时间流匹配的条件图流模型，用于从部分观测中重建图结构，提升了重建准确率。


<details>
  <summary>Details</summary>
Motivation: 图重建任务中，传统嵌入方法缺乏全局一致性，而现代生成模型难以融入结构先验。

Method: 提出Prior-Informed Flow Matching（PIFM），基于置换等变的失真-感知理论，先用图嵌入（如graphon、GraphSAGE/node2vec）生成初始邻接矩阵估计，再通过修正流匹配进行全局优化。

Result: 在多个数据集上实验表明，PIFM持续提升经典嵌入性能，并优于当前最优生成基线。

Conclusion: PIFM成功融合局部先验与全局流建模，在图重建任务中实现了更准确、一致的性能。

Abstract: We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.

</details>


### [126] [Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment](https://arxiv.org/abs/2601.22111)
*Abdullah Tasim,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一种利用协同无人机集群（UAS）测量数据，结合Bi-LSTM与物理信息神经网络（PINN），重建四维大气风场的新框架，无需专用风传感器或固定基础设施，实现了高精度、可扩展的风场重建。


<details>
  <summary>Details</summary>
Motivation: 传统仪器在近地边界层存在时空观测空白；单架UAS仅沿轨迹采样，难以完整恢复风场。

Method: 使用合成湍流环境与高保真多旋翼仿真生成训练/评估数据；通过双向LSTM从UAS动力学估计局部风分量；再将估计结果融入物理信息神经网络（PINN）进行连续时空风场重建。

Result: Bi-LSTM在低/中/高风速下水平风分量RMSE分别为0.064–0.062、0.122–0.129、0.271–0.273 m/s；垂直分量RMSE为0.029–0.091 m/s；PINN重建风场在1000 m内保持主导结构与垂直切变；五机编队在中风条件下整体RMSE最低（0.118–0.154 m/s）。

Conclusion: 协同UAS测量结合数据驱动与物理约束方法，可实现高精度、可扩展、无专用传感器的四维风场重建，适用于气象预报、灾害预警与风能评估。

Abstract: Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.

</details>


### [127] [Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics](https://arxiv.org/abs/2601.22123)
*Winfried Ripken,Michael Plainer,Gregor Lied,Thorben Frank,Oliver T. Unke,Stefan Chmiela,Frank Noé,Klaus Robert Müller*

Main category: cs.LG

TL;DR: 本文提出了一种学习哈密顿流映射（Hamiltonian Flow Maps）的新框架，通过预测相空间在时间跨度Δt内的平均演化，实现稳定的大步长数值积分，突破传统积分器的时间步长限制。


<details>
  <summary>Details</summary>
Motivation: 传统哈密顿系统长时间演化模拟受限于小时间步长以保证数值稳定性，亟需更高效稳定的积分方法。

Method: 引入均值流一致性条件（Mean Flow consistency condition），用于约束时间平均的哈密顿动力学；基于该条件，直接利用独立相空间样本（无需未来状态或完整轨迹）训练模型，学习Δt时间跨度内的平均相空间演化映射。

Result: 在多种哈密顿系统上验证有效，尤其在基于机器学习力场（MLFF）的分子动力学模拟中显著优于现有方法；模型训练与推理开销相当，但支持更大时间步长，且可直接使用广泛存在的无轨迹MLFF数据集训练。

Conclusion: 该框架为哈密顿系统提供了一种高效、稳定、数据友好的大步长模拟新范式，拓展了MLFF在实际动力学模拟中的应用潜力。

Abstract: Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.

</details>


### [128] [SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2601.22131)
*Leonard Papenmeier,Petru Tighineanu*

Main category: cs.LG

TL;DR: 本文提出SMOG方法，一种基于多输出高斯过程的可扩展、模块化元学习模型，用于多目标贝叶斯优化，能有效利用历史任务数据构建带元数据不确定性的目标任务先验，并支持分层并行训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法未同时解决元学习与多目标贝叶斯优化的结合问题，而实际中常有相关历史任务数据可用，亟需能利用元知识加速多目标黑箱优化的方法。

Method: 提出SMOG模型，基于结构化联合多输出高斯过程，显式建模目标间相关性；通过元数据条件化构建闭式目标先验，并引入残差多输出核；支持元任务GP一次性拟合与缓存，实现线性可扩展性。

Result: SMOG实现了对元数据不确定性向目标代理模型的合理传播，其先验可无缝集成标准多目标BO采集函数，且训练具有分层并行性与线性时间复杂度。

Conclusion: SMOG为元学习增强的多目标贝叶斯优化提供了统一、可扩展、原理严谨的建模范式，显著提升了少样本多目标黑箱优化效率。

Abstract: Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.

</details>


### [129] [Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference](https://arxiv.org/abs/2601.22132)
*Ziming Dong,Hardik Sharma,Evan O'Toole,Jaya Prakash Champati,Kui Wu*

Main category: cs.LG

TL;DR: 本文提出LLM Shepherding框架，通过让大模型仅提供短提示（hint）给小模型，显著提升小模型在数学和编程任务上的准确率，同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽性能优异但推理成本高，小型语言模型（SLMs）成本低但准确率差；现有方法（路由与级联）无法细粒度控制LLM资源使用。

Method: 提出LLM Shepherding框架：LLM仅生成短前缀提示（10–30%长度）供SLM使用；设计两阶段预测器联合决策是否需要提示及提示长度；实现token级预算控制。

Result: 在GSM8K、CNK12、HumanEval、MBPP等基准上，相比纯LLM推理降低成本42–94%；相比先进路由/级联方法，最高实现2.8倍成本下降且精度不损。

Conclusion: LLM Shepherding是一种更细粒度、更高效的SLM-LLM协同范式，首次实现token级预算控制，在保持精度的同时极大提升成本效益。

Abstract: Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.

</details>


### [130] [Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing](https://arxiv.org/abs/2601.22151)
*Daniel Stein,Shaoyi Huang,Rolf Drechsler,Bing Li,Grace Li Zhang*

Main category: cs.LG

TL;DR: 本文提出将神经网络转换为逻辑流（if-else结构为主）以提升其在CPU上的执行效率，通过先转为决策树、再提取压缩决策路径实现，显著降低延迟且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦于高效执行大量乘加（MAC）操作，但CPU擅长控制流而非大规模数学运算，因此需探索更适合CPU的神经网络执行范式。

Method: 将神经网络转换为等价决策树，从中选取具有常数叶子的决策路径，并压缩为以if-else结构为主、MAC操作大幅减少的逻辑流。

Result: 在模拟RISC-V CPU上延迟最高降低14.9%，精度无损。

Conclusion: 将神经网络转化为逻辑流是一种有效提升其在资源受限CPU端推理效率的新方法，兼顾性能与精度。

Abstract: Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.
  The code is open source at https://github.com/TUDa-HWAI/NN2Logic

</details>
