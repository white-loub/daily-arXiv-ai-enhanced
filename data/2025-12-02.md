<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 236]
- [cs.CL](#cs.CL) [Total: 76]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.LG](#cs.LG) [Total: 189]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 80]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MOTION: ML-Assisted On-Device Low-Latency Motion Recognition](https://arxiv.org/abs/2512.00008)
*Veeramani Pugazhenthi,Wei-Hsiang Chu,Junwei Lu,Jadyn N. Miyahira,Soheil Salehi*

Main category: cs.CV

TL;DR: 本文提出了一种基于三轴加速度传感器的高效运动模型，利用AutoML流水线提取关键特征，并在WeBe Band可穿戴设备上实现低延迟、高准确性的实时手势识别，适用于医疗监测场景。


<details>
  <summary>Details</summary>
Motivation: 为了满足医疗监控等领域对快速、可靠且无误报的人机交互需求，需要开发能够在嵌入式设备上高效运行的手势识别方案。

Method: 使用三轴加速度计数据，结合AutoML流水线进行特征提取，并训练多种轻量级机器学习模型，在WeBe Band可穿戴设备上实现完全本地化的手势识别。

Result: 神经网络模型在准确性、延迟和内存使用之间达到了最佳平衡，实验证明WeBe Band能够实现可靠的实时手势识别。

Conclusion: 该研究验证了仅使用加速度计数据并通过AutoML优化特征提取与轻量模型训练，可在资源受限设备上实现高效实时手势识别，具有广泛应用于医疗监测的潜力。

Abstract: The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.

</details>


### [2] [Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions](https://arxiv.org/abs/2512.00042)
*Egemen Sert,Şeyda Ertekin*

Main category: cs.CV

TL;DR: 本文提出一种以数据为中心的多模态推理方法，通过构建高质量、课程对齐的1.61亿token多模态数据集，并结合优化的推理语法QMSA对Qwen-2.5VL-32B进行监督微调，在新基准YKSUniform上达到78.6%准确率，仅比Gemini 2.0 Flash低1.0%，证明优质数据与表征语法对多模态推理至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推理取得进展，但其数据基础（尤其是高质量、课程对齐的多模态训练数据）仍被忽视，现有工作过度聚焦算法改进而忽略数据质量与构成的影响。

Method: 构建包含教科书题目-解答对、课程对齐图表及上下文材料的161.4M token多模态数据集；采用优化的推理语法QMSA对Qwen-2.5VL-32B进行监督微调；在新发布的标准化基准YKSUniform（含1854道跨309课程主题的多模态考题）上评估性能。

Result: 微调后模型在YKSUniform上达78.6%准确率，仅比Gemini 2.0 Flash低1.0%；验证了数据组成与表征语法对多模态推理性能具有决定性影响。

Conclusion: 高质量、课程驱动的多模态数据配合合理表征语法，可使开源视觉语言模型通过监督微调逼近闭源SOTA性能，确立了以数据为中心提升多模态推理能力的有效范式。

Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.

</details>


### [3] [SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control](https://arxiv.org/abs/2512.00413)
*Ji Gan,Lingxu Chen,Jiaxu Leng,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出SplatFont3D，一种基于3D高斯溅射的结构感知文本到3D艺术字体生成框架，支持细粒度部件级风格控制，兼顾3D字体语义精确性与渲染效率。


<details>
  <summary>Details</summary>
Motivation: 现有艺术字体生成研究多集中于2D平面设计，个性化3D艺术字体生成（3D-AFG）缺乏探索；3D字体具有强结构约束和精细部件语义，需兼顾几何精度与风格可控性，而传统方法（如NeRF）在部件控制和渲染效率上存在不足。

Method: 提出SplatFont3D框架：1）Glyph2Cloud模块将2D字形逐步增强并生成对应3D点云以初始化高斯；2）通过Score Distillation Sampling联合预训练2D扩散模型优化3D高斯；3）设计动态部件分配策略，利用高斯几何先验实现部件划分并缓解优化过程中的漂移纠缠。

Result: SplatFont3D在风格-文本一致性、视觉质量和渲染效率上均优于现有3D-AFG方法；相比NeRF，提供更显式、有效的部件级风格控制和更快渲染速度。

Conclusion: SplatFont3D首次将3D高斯溅射引入艺术字体生成，有效解决了3D字体强结构约束与细粒度风格控制的矛盾，为沉浸式3D内容创作提供了高效可控的新范式。

Abstract: Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.

</details>


### [4] [PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2512.00060)
*Abdolazim Rezaei,Mehdi Sookhak*

Main category: cs.CV

TL;DR: 本文提出PEFT-DML，一种参数高效的深度度量学习框架，用于自动驾驶中鲁棒的多模态3D目标检测，支持传感器缺失和未知模态组合下的可靠检测。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型假设传感器始终可用、缺乏对传感器失效或新模态组合鲁棒性的问题。

Method: 将LiDAR、雷达、相机、IMU、GNSS等多模态数据映射到共享隐空间，并结合LoRA与适配器层实现参数高效训练。

Result: 在nuScenes基准上实验表明，该方法在精度、快速运动鲁棒性、天气变化适应性和跨域泛化能力方面均优于现有方法。

Conclusion: PEFT-DML通过参数高效设计显著提升多模态3D检测的鲁棒性与实用性，适用于真实自动驾驶场景。

Abstract: This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.

</details>


### [5] [DL-CapsNet: A Deep and Light Capsule Network](https://arxiv.org/abs/2512.00061)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种深度变体CapsNet（DL-CapsNet），通过引入多个胶囊层和胶囊摘要层，提升了对重叠类别和仿射变换图像的分类精度，同时显著减少了参数量，提高了训练与推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理重叠类别和仿射变换图像时性能受限，而CapsNet虽具潜力但存在复杂度高、参数多的问题，需进一步优化以提升效率与可扩展性。

Method: 设计了包含多个胶囊层的深度CapsNet架构，并创新性地引入胶囊摘要层（Capsule Summarization layer）以压缩参数规模，降低模型复杂度。

Result: DL-CapsNet在保持高准确率的同时，显著减少参数量，加快训练与推理速度，并能有效处理高类别数的复杂数据集。

Conclusion: DL-CapsNet是一种高效、轻量且可扩展的胶囊网络架构，在精度与效率之间取得良好平衡，为CapsNet的实际应用提供了新路径。

Abstract: Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.

</details>


### [6] [Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians](https://arxiv.org/abs/2512.01306)
*Hongru Yan,Xiang Zhang,Zeyuan Chen,Fangyin Wei,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯的表面建模方法——Gaussian Swaying，用于高效、精细的空气动力学模拟，统一了动力学仿真与渲染，并在多项指标上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 提升视觉与图形学中自然运动（如树枝摇曳、旗帜飘动、船只晃动）的空气动力学模拟真实性，克服传统网格或粒子方法在计算成本或离散化建模上的局限。

Method: 提出基于3D高斯的表面表示框架Gaussian Swaying，用高斯椭球片连续建模表面，支持直接力计算与法向量提取，实现仿真与渲染一体化。

Result: 在合成与真实数据集上实验表明，该方法在精度、效率和可扩展性方面均优于现有方法，达到空气动力学场景模拟的SOTA性能。

Conclusion: Gaussian Swaying为真实感空气动力学模拟提供了一种高效、连续、统一的表面建模新范式，适用于视觉与图形学中的动态场景生成。

Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.

</details>


### [7] [Satellite to Street : Disaster Impact Estimator](https://arxiv.org/abs/2512.00065)
*Sreesritha Sai,Sai Venkata Suma Sreeja,Deepthi,Nikhil*

Main category: cs.CV

TL;DR: 本文提出了一种名为Satellite-to-Street: Disaster Impact Estimator的深度学习框架，通过改进的双输入U-Net架构和类别感知加权损失函数，联合处理灾前灾后卫星图像，实现细粒度像素级灾害损伤评估，在公开数据集上优于传统分割与变化检测模型。


<details>
  <summary>Details</summary>
Motivation: 准确的灾后损伤评估对应急响应优先级排序至关重要，但人工解译卫星影像效率低、主观性强且难以规模化；现有深度学习模型（如U-Net和变化检测模型）在细微结构变化识别和严重类别不平衡问题上表现不佳。

Method: 提出一种双输入U-Net改进架构，增强特征融合以同时捕获局部结构变化与全局上下文信息，并引入类别感知加权损失函数缓解未受损像素主导带来的类别不平衡问题。

Result: 在多个公开灾害数据集上的实验表明，该方法在结构损伤的定位与分类精度上优于传统图像分割和基准变化检测模型。

Conclusion: 所提框架可生成快速、一致的像素级损伤图，辅助而非替代专家决策，提升数据驱动的灾害管理效率。

Abstract: Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.

</details>


### [8] [ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN](https://arxiv.org/abs/2512.00073)
*Aswinkumar Varathakumaran,Nirmala Paramanandham*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProvRain的轻量级检测管道，结合MobileNet-U-Net去噪架构与课程学习策略，在雨夜场景下提升车辆前瞻性检测性能，并在PVDN数据集上验证了其优于Faster R-CNN及Transformer方法的效果。


<details>
  <summary>Details</summary>
Motivation: 夜间车辆前瞻性检测面临雨雪等天气及相机噪声干扰，需在去噪同时保持检测精度。

Method: 构建ProvRain管道：采用轻量级MobileNet-U-Net作为去噪主干，结合课程学习策略进行训练；融合合成数据与PVDN真实数据；对比基准为Faster R-CNN（仅在PVDN上训练）。

Result: 在雨夜帧中，检测准确率提升8.94%，召回率提升10.25%；MobileNet-U-Net在图像质量指标上较Transformer方法提升PSNR 10–15%、SSIM 5–6%，LPIPS降低最多达67%。

Conclusion: ProvRain有效缓解雨夜噪声干扰，兼顾去噪与检测性能，验证了轻量架构+课程学习在前瞻性车辆检测中的有效性。

Abstract: Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.

</details>


### [9] [Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation](https://arxiv.org/abs/2512.00075)
*Jun Jia,Hongyi Miao,Yingjie Zhou,Wangqiu Zhou,Jianbo Zhang,Linhan Cao,Dandan Zhu,Hua Yang,Xiongkuo Min,Wei Sun,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出Adapter Shield，一种用于防御零样本图像生成中个人图像滥用的通用且集成认证的解决方案，通过可逆加密和多目标对抗扰动保护图像身份与风格。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的发展，零样本图像到图像生成技术带来了未经授权的身份克隆和风格模仿等知识产权风险，亟需有效的防护机制。

Method: 分析现有零样本方法如何利用图像编码器提取嵌入，并通过交叉注意力注入UNet；设计可逆加密系统将原始嵌入映射为密钥相关的加密表示，并引入多目标对抗扰动使未授权用户生成失真或加密输出。

Result: 实验表明，该方法在阻止未经授权的零样本图像生成方面优于现有最先进防御技术，同时支持授权用户的正常生成与灵活访问控制。

Conclusion: Adapter Shield是首个面向零样本生成场景的通用认证集成防护方案，有效平衡了图像保护与合法使用需求。

Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.

</details>


### [10] [CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball](https://arxiv.org/abs/2512.01478)
*Omer Sela,Michael Chertok,Lior Wolf*

Main category: cs.CV

TL;DR: 本文提出了CourtMotion，一种用于分析和预测职业篮球比赛中游戏事件和战术的时空建模框架。它结合图神经网络和带特殊注意力机制的Transformer，利用骨骼追踪数据而非仅球员位置，显著提升了轨迹预测与多项篮球分析任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统仅使用球员位置的方法无法捕捉身体朝向、防守姿态、投篮准备动作等关键语义信息，难以准确理解篮球事件的战术意义。

Method: 采用两阶段方法：第一阶段用图神经网络处理骨骼追踪数据以捕获精细运动模式；第二阶段用带专用注意力机制的Transformer建模球员交互，并引入事件投影头将运动模式显式关联到传球、投篮、抢断等具体事件。

Result: 在NBA追踪数据上实验表明，相比纯位置模型，轨迹预测误差降低35%，并在选人识别、投篮者识别、助攻预测、投篮位置分类和投篮类型识别等下游任务中持续超越现有方法。

Conclusion: CourtMotion通过融合运动学与语义信息，构建了更符合篮球实战逻辑的时空建模框架，其预训练模型可作为多个下游篮球分析任务的强有力基础。

Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.

</details>


### [11] [Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection](https://arxiv.org/abs/2512.00078)
*Mario de Jesus da Graca,Jörg Dahlkemper,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本研究提出使用基于U-Net的扩散模型生成合成亮场显微图像，用于增强细胞检测任务中的训练数据。实验表明，结合合成数据可提升YOLOv8、YOLOv9和RT-DETR等检测模型的精度，且专家难以区分合成与真实图像（判别准确率仅50%），验证了方法在减少人工标注依赖方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在单细胞检测中受限于真实数据稀缺和标注成本高，亟需有效数据增强方法以缓解标注瓶颈。

Method: 采用U-Net结构的无条件扩散模型生成合成亮场显微图像，并构建不同合成/真实比例的混合训练集，用于训练YOLOv8、YOLOv9和RT-DETR等目标检测模型。

Result: 使用合成数据训练显著提升了细胞检测精度；人类专家无法有效区分合成与真实图像（判别准确率为50%），表明生成图像具有高度真实性。

Conclusion: 基于扩散模型的合成数据生成是显微图像分析中有前景的数据增强策略，可降低对人工标注的依赖，并提升细胞检测模型的鲁棒性。

Abstract: Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.

</details>


### [12] [Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels](https://arxiv.org/abs/2512.00080)
*André Dehne,Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本文探讨了在欧洲XFEL加速器隧道中，使用深度视觉立体里程计（DVSO）作为MARWIN机器人自主导航的替代方案，以应对传统定位方法在长而单调隧道中的局限性。


<details>
  <summary>Details</summary>
Motivation: MARWIN机器人目前的导航系统在预定义区域表现稳健，但在未知几何结构和障碍物面前缺乏灵活性，需要更灵活、自主的导航解决方案。

Method: 提出采用基于视觉的深度视觉立体里程计（DVSO），结合立体视差、光流和自监督学习来联合估计深度和自我运动，并可通过融合绝对参考或其它传感器提高全局一致性。

Result: DVSO有望减少尺度漂移、降低传感成本并支持可扩展的数据采集，但面临低纹理表面、光照变化、计算负载以及辐射环境下的鲁棒性等挑战。

Conclusion: 本文为MARWIN机器人在受限且安全关键的基础设施中实现更高程度的自主导航制定了研究议程。

Abstract: The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.

</details>


### [13] [Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages](https://arxiv.org/abs/2512.00082)
*Divendar Murtadak,Yoon Kim,Trilokya Akula*

Main category: cs.CV

TL;DR: 本研究探讨诊断式提示（diagnostic prompting）是否能提升多模态大语言模型（MLLM）在亚马逊搜索结果页（SRP）视觉复杂度评估中的可靠性，发现其F1分数显著提升（+858%），但绝对性能仍较低；模型更关注视觉设计元素（如徽章杂乱度），而人类更关注内容相似性，二者推理存在部分对齐但仍有明显偏差。


<details>
  <summary>Details</summary>
Motivation: 提升MLLM在视觉复杂度评估任务中与人类判断的一致性与可靠性，解决当前基于整体原则（gestalt）提示方法性能低下问题。

Method: 采用诊断式提示与标准基于格式塔原理的提示进行对比实验，使用200个亚马逊SRP页面及人类专家标注数据，结合决策树分析特征重要性，并开展失败案例分析。

Result: 诊断式提示使F1-score从0.031提升至0.297（相对提升858%），Cohen's κ=0.071；决策树显示模型最重视‘徽章杂乱度’（38.6%重要性），而人类侧重内容相似性；失败案例集中于产品相似性与颜色强度判断。

Conclusion: 诊断式提示是迈向人类对齐MLLM评估的有希望的初步尝试，但因持续存在的模型-人类分歧，需进一步优化提示策略并扩大高质量标注数据集以支撑实际部署。

Abstract: This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $κ$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.

</details>


### [14] [A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation](https://arxiv.org/abs/2512.00084)
*Venkata Siddharth Dhara,Pawan Kumar*

Main category: cs.CV

TL;DR: 本文提出FastTextDiff，一种结合医学文本标注的标签高效扩散模型，利用ModernBERT将临床文本与医学图像语义对齐，提升分割性能并降低对密集像素标签的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的医学图像分割方法受限于对昂贵、耗时且需专家标注的密集像素标签的依赖。

Method: 提出FastTextDiff模型，集成ModernBERT（可处理长临床文本的Transformer）来建立图像与文本间的跨模态注意力，并在MIMIC-III/IV上训练；采用FlashAttention 2和交替注意力机制提升效率。

Result: FastTextDiff在分割准确率和训练效率上均优于传统扩散模型，验证了ModernBERT作为Clinical BioBERT替代方案的有效性与可扩展性。

Conclusion: 多模态融合（图像+临床文本）结合高效Transformer架构，为标签受限的医学图像分割提供了新范式。

Abstract: In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.

</details>


### [15] [Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs](https://arxiv.org/abs/2512.00086)
*Davide Nadalini,Manuele Rusci,Elia Cereda,Luca Benini,Francesco Conti,Daniele Palossi*

Main category: cs.CV

TL;DR: 本文提出一种用于超低功耗物联网设备的多模态在线学习技术，通过内存优化的稀疏更新策略，在设备端实现单目深度估计模型的微调，显著减小域偏移带来的精度下降。


<details>
  <summary>Details</summary>
Motivation: 现有适用于物联网节点的单目深度估计深度神经网络参数量有限，在实际场景中因传感器数据与训练数据差异大而导致严重精度下降，需解决域偏移问题。

Method: 提出一种部署在物联网设备上的多模态在线学习（ODL）方法，利用8×8像素深度传感器在新环境中采集伪标签，并在MCU上对轻量级μPyD-Net模型进行完全本地化微调；引入一种内存驱动的稀疏更新方案以减少反向传播过程中的内存占用。

Result: 在真实场景中仅用3千个自标注样本，可在17.8分钟内在物联网节点完成微调，内存消耗降至1.2MB（比全量更新低2.2倍），KITT和NYUv2数据集上精度仅下降2%和1.5%，均方根误差从4.9m降至0.6m。

Conclusion: 该工作首次实现了在超低功耗物联网设备上进行单目深度估计的在线学习，有效应对域偏移问题，兼顾能效与精度，具备实际部署潜力。

Abstract: Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.

</details>


### [16] [Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data](https://arxiv.org/abs/2512.00087)
*Ivo Bueno,Ruikun Hou,Babette Bühler,Tim Fütterer,James Drimalla,Jonathan Kyle Foster,Peter Youngs,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 该论文探索了基于AI的课堂录像多模态分析，用于自动识别教学活动和话语，以实现可扩展的教师反馈系统。


<details>
  <summary>Details</summary>
Motivation: 现有课堂互动观察依赖人工标注，成本高且难以扩展，因此需要自动化、可扩展的解决方案。

Method: 使用164小时视频和68个课 transcript 构成的数据集，分别构建视频和文本的模态专用管道：视频采用零样本多模态大模型、微调的视觉-语言模型和自监督视频Transformer；文本则微调基于Transformer的分类器，并与基于提示的大模型对比。采用按标签阈值、上下文窗口和抗类别不平衡损失函数应对多标签和数据不平衡问题。

Result: 微调模型在视频和文本任务上均优于基于提示的大模型，macro-F1分数分别为0.577和0.460。

Conclusion: 研究表明，基于微调模型的自动化课堂分析是可行的，为可扩展的教师反馈系统奠定了基础。

Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.

</details>


### [17] [SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features](https://arxiv.org/abs/2512.00088)
*Mohammad Zare*

Main category: cs.CV

TL;DR: 本文提出SemImage方法，将文本文档表示为二维语义图像，用CNN处理；每个词映射为HSV色彩空间中的像素，分别编码主题（Hue）、情感（Saturation）和强度（Value），并通过多任务学习与动态句间边界行增强语义结构可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升文本分类模型的性能与可解释性，使语义特征（如主题、情感、句子边界）在视觉层面显式可辨，并兼容成熟CNN架构。

Method: 构建SemImage：按句子排布词像素，插入动态计算的边界行；采用解耦HSV色彩空间编码语言特征（Hue含cos/sin分量表主题，Saturation表情感，Value表强度）；通过ColorMapper网络与多任务学习（联合预测主题、情感及主任务）实现特征解耦；结合标准2D CNN（如ResNet）进行文档分类。

Result: 在多标签（主题+情感）与单标签文本分类基准上，SemImage达到或超越BERT、分层注意力网络等强基线；消融实验证明HSV多通道表示与动态边界行的关键作用；可视化显示主题转换与情感变化在图像中形成清晰模式。

Conclusion: SemImage是一种有效、可解释的文本到图像表示方法，能将语言学特征自然映射为视觉可识别结构，在保持高性能的同时显著增强模型透明度与人类可理解性。

Abstract: We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.

</details>


### [18] [TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts](https://arxiv.org/abs/2512.00089)
*Ioannis Prapas,Nikolaos Papadopoulos,Nikolaos-Ioannis Bountos,Dimitrios Michail,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 本文提出了一种名为TeleViT的多尺度视觉Transformer模型，用于亚季节至季节尺度的野火预测，通过融合局地火险驱动因子、全球气象场和遥相关指数，在SeasFire数据集上显著优于U-Net++、ViT和气候学基准。


<details>
  <summary>Details</summary>
Motivation: 提前数周至数月预测野火十分困难但至关重要；传统短时预测依赖局地天气，而长时预测需考虑地球系统大尺度关联（如遥相关），现有模型缺乏对这种多尺度地球系统背景的显式建模。

Method: 提出TeleViT：一种遥相关感知的Vision Transformer，采用非对称分块策略生成异构token（对应局地精细输入、粗化全球场、遥相关指数），由共享Transformer编码器联合处理，再经保持空间结构的解码器输出预测。

Result: 在SeasFire数据集（2001–2021）上，TeleViT在所有预测时效（最高达约4个月）下AUPRC均优于U-Net++、ViT和气候学基准；零时效AUPRC达0.630，4个月时效仍维持0.601–0.603；区域分析显示在非洲稀树草原等季节性稳定火区技能最高；归因分析表明预测主要依赖局地token，全球与指数信息提供粗粒度上下文。

Conclusion: 显式编码大尺度地球系统背景（如遥相关）的架构可有效延伸野火在亚季节至季节尺度上的可预测性，为长期野火风险规划提供新工具。

Abstract: Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.
  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.

</details>


### [19] [Deep Filament Extraction for 3D Concrete Printing](https://arxiv.org/abs/2512.00091)
*Karam Mawas,Mehdi Maboudi,Pedro Achanccaray,Markus Gerke*

Main category: cs.CV

TL;DR: 本文提出了一种适用于挤出式和喷射式3D混凝土打印的自动化灯丝几何质量控制方法，支持多种传感器和材料状态（新鲜或硬化），可用于在线及打印后检测。


<details>
  <summary>Details</summary>
Motivation: 由于灯丝是决定3D打印结构性能的关键几何单元，其几何质量直接影响打印结构的整体质量与可靠性，因此亟需一种通用、自动化的灯丝质量控制方法。

Method: 提出一种与传感器无关的自动化灯丝质量控制流程，兼容相机、结构光系统（SLS）和地面激光扫描仪（TLS）等不同数据采集设备，并适用于新鲜态和硬化态混凝土材料。

Result: 实现了适用于两种主流大型3D混凝土打印技术（CC与SC3DP）的通用灯丝QC方法，支持在线与离线检测，验证了其跨传感器与材料状态的鲁棒性。

Conclusion: 该方法提升了3D混凝土打印过程的质量可控性与自动化水平，为AEC行业实现可持续、高效建造提供了关键技术支撑。

Abstract: The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.

</details>


### [20] [Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images](https://arxiv.org/abs/2512.00103)
*Ifeanyi Okala*

Main category: cs.CV

TL;DR: 本研究比较了VGG16、ViT-B/16和CoAtNet-Tiny三种图像方法在使用活动记录数据识别抑郁症、精神分裂症和健康对照中的表现，结果表明CoAtNet-Tiny在准确率和稳定性方面最优。


<details>
  <summary>Details</summary>
Motivation: 探索适用于基于活动记录图像的精神疾病自动识别的深度学习模型性能差异。

Method: 将腕部活动信号转换为30x48像素图像，采用三折交叉验证评估VGG16、ViT-B/16和CoAtNet-Tiny模型的表现。

Result: CoAtNet-Tiny在平均准确率、精确率、召回率和F1分数上表现最佳且最稳定，尤其对样本较少的疾病类别效果更好；VGG16收敛较慢，ViT-B/16结果波动大。

Conclusion: CoAtNet-Tiny在处理基于活动记录生成的图像任务中表现最一致，表明混合架构可能更适用于此类精神健康检测任务。

Abstract: This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.

</details>


### [21] [TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening](https://arxiv.org/abs/2512.00117)
*Ishwaryah Pandiarajan,Mohamed Mansoor Roomi Sindha,Uma Maheswari Pandyan,Sharafia N*

Main category: cs.CV

TL;DR: 本文提出TinyViT，一种仅使用可见光图像即可实现光伏板表面故障分类与严重程度评估的轻量级深度学习框架，无需依赖昂贵的电致发光或红外传感器，提升了太阳能健康监测的可及性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有光伏资产运维依赖多模态成像（如EL、IR），存在成本高、部署难等问题，亟需一种低成本、易部署的可见光图像分析方案。

Method: 提出TinyViT框架，融合Transformer分割、光谱-空间特征工程与集成回归；输入为消费级彩色相机拼接图像，输出七类表面故障分类及严重程度评分。

Result: 在真实公开数据集上验证，分类与回归性能媲美专用方法，兼具高准确率与可解释性。

Conclusion: 仅用可见光图像即可实现鲁棒、可扩展、低成本的光伏表面故障诊断，推动太阳能健康监测向普惠化、现场化发展。

Abstract: Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.

</details>


### [22] [Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance](https://arxiv.org/abs/2512.00125)
*Ruo-Syuan Mei,Sixian Jia,Guangze Li,Soo Yeon Lee,Brian Musser,William Keller,Sreten Zakula,Jorge Arinez,Chenhui Shao*

Main category: cs.CV

TL;DR: 本文提出了一种基于合成数据生成（SDG）的混合框架，用于实现无需人工标注的工业零件质量检测零样本学习。该方法结合仿真渲染、域随机化和真实背景合成，在一小时内生成12,960张标注图像，并使用YOLOv8n和MobileNetV3-small构建两阶段模型。仅用合成数据训练的模型在真实工业零件上达到0.995的mAP@0.5、96%分类准确率和90.1%平衡准确率，显著优于少样本基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在工业质检中潜力巨大，但依赖大量标注数据且面临缺陷样本稀少导致的类别不平衡问题，限制了其在实际制造中的应用。因此需要一种高效、低成本且无需人工标注的数据生成方法来克服这些挑战。

Method: 提出一种融合基于仿真的渲染、域随机化和真实背景合成的混合合成数据生成（SDG）框架。通过变化零件几何形状、光照和表面属性生成多样化的合成图像，并将其叠加到真实背景上。采用YOLOv8n进行目标检测，MobileNetV3-small进行质量分类，模型完全在合成数据上训练，实现零样本迁移至真实场景。

Result: 该方法在一小时内生成12,960张标注图像；在300个真实工业零件测试集上取得mAP@0.5为0.995，分类准确率为96%，平衡准确率为90.1%；在严重类别不平衡下仍保持90-91%平衡准确率，远超基线的50%。

Conclusion: 所提出的SDG框架可有效支持无标注、可扩展且鲁棒的工业质检系统，能够在不使用任何真实标注数据的情况下实现高性能的零样本迁移，推动深度学习在现实制造环境中的广泛应用。

Abstract: Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.

</details>


### [23] [Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation](https://arxiv.org/abs/2512.00129)
*Jayan Adhikari,Prativa Joshi,Susish Baral*

Main category: cs.CV

TL;DR: 本文提出了一种结合ResNet50的OOD过滤与YOLO系列（v8/v11/v12）检测的联合框架，通过余弦相似度构建域内图库严格拒斥非乳腺X线图像，显著提升乳腺癌检测系统的分布外鲁棒性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在面对分布外（OOD）输入（如CT、MRI、X光等非乳腺X线模态或设备差异）时可靠性差，易导致误诊，亟需提升临床部署中的鲁棒性。

Method: 采用ResNet50作为最优骨干网络构建OOD过滤模块，基于余弦相似度建立域内图库以拒绝非乳腺X线图像；随后将通过筛选的图像送入YOLOv8/v11/v12进行病灶检测，并辅以Grad-CAM增强可解释性。

Result: OOD检测模块总体准确率达99.77%，OOD测试集上达100%；联合框架在乳腺X线数据上mAP@0.5达0.947，显著降低假阳性且保持高检测精度。

Conclusion: 该方法为面向多源异构临床环境的可靠AI乳腺癌检测系统提供了坚实基础，兼顾OOD鲁棒性、检测性能与可解释性。

Abstract: Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\% general accuracy with immaculate 100\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.

</details>


### [24] [Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition](https://arxiv.org/abs/2512.00130)
*Fadi Dornaika,Danyang Sun*

Main category: cs.CV

TL;DR: 本文提出LGCOAMix，一种基于超像素的网格混合数据增强方法，兼顾局部上下文与物体部件信息，避免传统CutMix中标签不一致和语义损失问题，在分类与弱监督定位任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CutMix方法过度关注全局语义、忽略判别性局部上下文，且矩形裁剪导致物体部件信息丢失；标签混合与图像不一致问题依赖低效的双前向传播或外部网络。

Method: 提出LGCOAMix：基于超像素划分的网格化混合策略，引入超像素注意力机制进行标签混合，学习判别性超像素区域特征及跨图像超像素对比。

Result: 在多个基准数据集上超越现有CutMix类方法，提升图像分类与CUB200-2011弱监督目标定位性能，并在CNN和Transformer架构上均有效。

Conclusion: LGCOAMix首次将超像素注意力引入CutMix框架，实现了上下文感知与物体部件感知的数据增强，解决了局部语义缺失与标签不一致问题，具有通用性和高效性。

Abstract: Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.

</details>


### [25] [Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems](https://arxiv.org/abs/2512.00179)
*Mohamed Abdallah Salem,Nourhan Zein Diab*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级CNN模型，专为激光散斑模式设计，用于59类材料的高精度识别，在SensiCut数据集上达到95.05%准确率，参数仅341k，可在树莓派等边缘设备实时运行。


<details>
  <summary>Details</summary>
Motivation: 准确识别材料对激光切割的安全性和有效性至关重要，而现有散斑传感方法或计算开销大，或覆盖材料类别有限。

Method: 设计了一个轻量级卷积神经网络，针对激光散斑图像特征优化结构，减少参数量并保持判别力；在完整的SensiCut数据集（59类）上训练和评估，并按实用材料族进行归组分析。

Result: 模型测试准确率达95.05%，宏/加权F1分数均为0.951，参数仅341k（约为ResNet-50的1/70），推理速度295帧/秒，支持树莓派与Jetson部署；按9类和5类材料族划分时召回率分别超98%和近100%。

Conclusion: 紧凑、领域定制的CNN在散斑材料分类任务中可超越大型通用骨干网络，显著提升边缘端材料感知激光切割系统的可行性。

Abstract: Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.

</details>


### [26] [AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI](https://arxiv.org/abs/2512.00194)
*Zag ElSayed,Grace Westerkamp,Gavin Gammoh,Yanchen Liu,Peyton Siekierski,Craig Erickson,Ernest Pedapati*

Main category: cs.CV

TL;DR: ICVision 是首个利用多模态大语言模型（GPT-4 Vision）直接解析EEG ICA可视化图表（如地形图、时序、功率谱、ERP）并进行专家级成分分类与解释的AI系统，在专家共识评估中达到k=0.677，超越ICLabel，且输出高度可解释、可操作。


<details>
  <summary>Details</summary>
Motivation: 传统EEG ICA成分分类方法（如ICLabel）依赖手工特征，缺乏视觉理解与可解释性；亟需能像神经科医生一样‘看’和‘解释’EEG成分的AI代理。

Method: 基于GPT-4 Vision构建多模态AI代理，输入ICA标准可视化四联图（拓扑图、时间序列、功率谱、ERP），输出六类成分标签、置信度及自然语言解释；集成于开源EEG Autoclean平台。

Result: 在3168个ICA成分（124个数据集）上与专家共识达成k=0.677一致性，优于MNE ICLabel；97%以上输出被专家评为可解释且可操作；在模糊案例中更好保留临床相关脑信号。

Conclusion: ICVision首次实现神经生理学中AI代理的视觉认知，标志着科学AI从单纯分类迈向‘看见—推理—交流’范式，推动可扩展、可解释、可复现的EEG分析。

Abstract: We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.

</details>


### [27] [Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting](https://arxiv.org/abs/2512.00198)
*Shantanu Ghosh,Vedant Parthesh Joshi,Rayan Syed,Aya Kassem,Abhishek Varshney,Payel Basak,Weicheng Dai,Judy Wawira Gichoya,Hari M. Trivedi,Imon Banerjee,Shyam Visweswaran,Clare B. Poynton,Kayhan Batmanghelich*

Main category: cs.CV

TL;DR: Mammo-FM是首个专为乳腺X光摄影设计的基础模型，基于大规模多样化数据集训练，在多个临床任务中表现优于现有通用模型。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性主要死因之一，现有模型在乳腺影像分析中存在局限，需要更专业、可解释且高效的基础模型。

Method: 提出Mammo-FM，使用来自四个美国机构的140,677名患者（821,326张 mammograms）进行预训练，支持癌症诊断、病灶定位、结构化报告生成和风险预测等多任务统一框架，并实现图像与文本对齐以增强可解释性。

Result: 在多个公有和私有基准上，Mammo-FM在诊断、预后和报告生成任务中均优于当前最先进的通用基础模型，且仅使用其三分之一参数并处理原生分辨率图像。

Conclusion: 领域特定的基础模型在医疗影像中更具效率和应用价值，强调了任务全面覆盖和领域对齐评估的重要性。

Abstract: Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.

</details>


### [28] [ReactionMamba: Generating Short &Long Human Reaction Sequences](https://arxiv.org/abs/2512.00208)
*Hajra Anwar Beg,Baptiste Chopin,Hao Tang,Mohamed Daoudi*

Main category: cs.CV

TL;DR: ReactionMamba 是一种基于 Mamba 的新框架，用于生成长时序、高保真的人体反应运动，兼顾 realism、diversity 与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成长序列复杂反应动作（如舞蹈、武术）时存在时间不一致、计算效率低或真实性不足等问题。

Method: 结合运动变分自编码器（motion VAE）进行高效编码，并采用基于 Mamba 的状态空间模型进行时序一致的解码。

Result: 在 NTU120-AS、Lindy Hop 和 InterX 三个数据集上，相比 InterFormer、ReMoS 和 Ready-to-React，在真实性、多样性及长序列生成质量上达到竞争性水平，且推理速度显著提升。

Conclusion: ReactionMamba 验证了状态空间模型（如 Mamba）在长时序 3D 运动生成任务中的有效性与高效性，为实时交互式动作生成提供了新思路。

Abstract: We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.

</details>


### [29] [DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation](https://arxiv.org/abs/2512.00226)
*Zirui Wang,Tao Zhang*

Main category: cs.CV

TL;DR: 本文提出了DenseScan数据集，利用多视角2D图像和多模态大语言模型（MLLMs）自动生成密集、多层级语义描述，弥补现有3D场景理解数据集在细粒度视觉-语言标注上的不足，并支持导航与问答等下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解数据集缺乏丰富、上下文敏感的语义标注，难以支撑细粒度视觉-语言任务（如导航、问答）。

Method: 提出自动化标注流程：基于多视角2D图像输入，结合多模态大语言模型（MLLMs）生成对象级密集描述；进一步通过场景驱动的问题生成，构建融合属性、空间关系与场景上下文的高层次问题。

Result: 实验表明，该方法显著提升3D环境中的对象级理解和视觉问答性能；并开源了DenseScan数据集及标注流水线。

Conclusion: DenseScan通过融合几何细节与语义丰富性，拓展了3D场景理解的数据基础，推动机器人、增强现实等实际应用中更上下文感知的AI发展。

Abstract: 3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.

</details>


### [30] [Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views](https://arxiv.org/abs/2512.00255)
*Kunwar Maheep Singh,Jianchun Chen,Vladislav Golyanik,Stephan J. Garbin,Thabo Beeler,Rishabh Dabral,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出了一种名为Relightable Holoported Characters (RHC) 的新方法，用于仅基于稀疏视角RGB视频实现动态人体的自由视点渲染和重光照，通过Transformer架构在单次前向传播中高效完成光照重置。


<details>
  <summary>Details</summary>
Motivation: 传统基于OLAT的人体重光照方法需要昂贵的逐光源捕捉与生成过程，且难以处理全身体、高动态和复杂光照情况，因此需要一种更高效、高质量的重光照方法。

Method: 提出基于Transformer的RelightNet，利用从粗略网格代理和输入视图提取的物理启发特征（几何、反照率、阴影、视角），结合新的光照条件进行交叉注意力操作，并回归到附着于网格的3D高斯点阵以生成重光照结果；训练数据来自多视角LightStage系统，交替采集随机环境光照与均匀光照帧以兼顾运动追踪与光照多样性。

Result: 实验表明，该方法在视觉保真度和光照再现质量上优于现有最先进方法，能够实现高质量的自由视点重光照效果。

Conclusion: RHC通过物理启发的特征设计和高效的Transformer架构，实现了仅需稀疏RGB视频输入的高质量、实时级人体重光照与自由视点渲染，显著优于现有技术。

Abstract: We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/

</details>


### [31] [UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations](https://arxiv.org/abs/2512.00261)
*Yuzhen Hu,Saurabh Prasad*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniDiff的参数高效框架，用于在稀疏标注条件下适应多模态遥感数据。该方法通过调整约5%的参数，结合FiLM时间步-模态条件和伪RGB锚定，实现对预训练扩散模型的无监督域适应，有效缓解了标注数据不足的问题，并提升了多模态遥感数据的融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督方法受限于标注数据的稀缺，难以在多模态遥感中广泛应用；尽管ImageNet预训练模型提供了丰富的视觉表征，但其在异构模态（如高光谱和SAR）上的适应仍具挑战。

Method: 提出UniDiff框架，采用基于FiLM的时间步-模态条件机制，对单个ImageNet预训练扩散模型进行参数高效微调（仅调整约5%参数），并引入伪RGB锚定策略以保持原有表征能力，防止灾难性遗忘。

Result: 在两个多模态遥感基准数据集上验证了方法的有效性，结果显示该方法在无监督适应下显著缓解了标注限制，并实现了有效的多模态数据融合。

Conclusion: UniDiff能够在仅有目标域未标注数据的情况下，高效适应多种遥感模态，为稀疏标注下的多模态遥感分析提供了一种可行且高效的解决方案。

Abstract: Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.

</details>


### [32] [HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction](https://arxiv.org/abs/2512.00264)
*Zhengda Ma,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 提出首个基于点云表示的几何深度学习框架HeartFormer，用于从电影MRI数据进行3D四腔心脏重建，并发布大规模数据集HeartCompv1。


<details>
  <summary>Details</summary>
Motivation: 传统电影MRI仅提供2D切片图像，限制了对心脏形态和生理机制的全面理解。

Method: 提出HeartFormer，包含SA-DSTNet生成初始粗略点云和SA-GFRTNet逐步优化几何细节，实现多类别点云补全。

Result: 在HeartCompv1和UK Biobank数据集上实验表明，HeartFormer在准确性、鲁棒性和泛化能力方面均优于现有最先进方法。

Conclusion: HeartFormer为3D心脏重建提供了高效解决方案，推动基于点云的心脏分析研究发展。

Abstract: We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.

</details>


### [33] [USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing](https://arxiv.org/abs/2512.00269)
*Jun Wang,Peirong Liu*

Main category: cs.CV

TL;DR: 本文提出了USB（Unified Synthetic Brain）框架，首次实现了病理与健康脑图像的双向生成与编辑，通过配对扩散机制建模病灶与解剖结构的联合分布，并引入一致性引导算法保证解剖结构与病灶对应关系。


<details>
  <summary>Details</summary>
Motivation: 配对的病理-健康脑影像数据极难获取，导致现有方法多为单向、领域特定且独立建模，难以支撑疾病诊断、预测与治疗规划等下游任务。

Method: 提出基于配对扩散机制的统一生成框架USB，支持双向（病理↔健康）图像生成与编辑；设计一致性引导算法以维持解剖结构一致性和病灶空间对应性。

Result: 在六个公开脑MRI数据集（含健康对照、卒中和阿尔茨海默病患者）上验证了USB生成结果的多样性与真实性，并建立了首个统一的脑影像生成与编辑基准。

Conclusion: USB为可扩展的数据集构建与鲁棒神经影像分析提供了新范式，推动了病理-健康脑结构关系建模的发展。

Abstract: Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.

</details>


### [34] [HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention](https://arxiv.org/abs/2512.00275)
*Yi Liu,Yi Wan,Xinyi Liu,Qiong Wu,Panwang Xia,Xuejun Huang,Yongjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为HIMOSA的轻量级遥感图像超分辨率框架，通过内容感知稀疏注意力机制和分层窗口扩展，在保持高性能的同时显著提升推理速度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 遥感应用（如灾害检测与响应）对实时性和模型轻量化要求高，现有超分辨率方法难以兼顾性能与计算效率。

Method: 提出HIMOSA框架，包含内容感知稀疏注意力机制以利用图像冗余，并引入分层窗口扩展策略，通过调节注意力稀疏度降低多尺度重复模式下的计算复杂度。

Result: 在多个遥感数据集上实验表明，HIMOSA在保持计算高效性的同时达到当前最优性能。

Conclusion: HIMOSA有效平衡了遥感图像超分辨率任务中的精度与效率，为实时轻量级遥感处理提供了新思路。

Abstract: In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.

</details>


### [35] [Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth](https://arxiv.org/abs/2512.00281)
*Sylvain Bodard,Pierre Baudot,Benjamin Renoust,Charles Voyton,Gwendoline De Bie,Ezequiel Geremia,Van-Khoa Le,Danny Francis,Pierre-Henri Siot,Yousra Haddou,Vincent Bobin,Jean-Christophe Brisset,Carey C. Thomson,Valerie Bourdes,Benoit Huet*

Main category: cs.CV

TL;DR: 本文提出了一种新型AI系统，能在低剂量CT扫描中直接在结节层面同时进行检测与恶性程度诊断，克服了传统基于大小和生长速度筛查的延迟问题；该系统采用浅层深度学习与特征驱动模型的集成方法，在大规模数据集上训练验证，性能全面超越放射科医生、Lung-RADS及主流AI模型，并具备高敏感性、低假阳性率及优异的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统肺癌筛查依赖结节大小和生长速度，导致早期恶性结节诊断延迟；现有AI模型受限于数据规模小和可解释性差。

Method: 设计了浅层深度学习模型与特征驱动专用模型的集成系统，在25,709例低剂量CT扫描（含69,449个标注结节）上训练和评估。

Result: 内部测试AUC达0.98，独立队列达0.945；在每扫描0.5个假阳性下实现99.3%敏感性；在所有结节大小、分期（尤其I期）及生长指标（包括最难预测的体积倍增时间）上均优于放射科医生，对不确定/缓慢生长结节的诊断提前达一年。

Conclusion: 该AI系统重新定义了肺癌筛查范式，显著提升早期诊断能力，突破临床采纳的关键障碍（如准确性、鲁棒性、可解释性），具备落地应用潜力。

Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.

</details>


### [36] [Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR](https://arxiv.org/abs/2512.00294)
*Lixing Guo,Tobias Höllerer*

Main category: cs.CV

TL;DR: 本文提出了一种模块化的增强现实（AR）代理系统，结合多模态大语言模型（MLLMs）与具身视觉模型，实现基于自然语言的空间关系推理和精确的3D定位。


<details>
  <summary>Details</summary>
Motivation: 传统AR系统依赖固定类别检测器或标记物，难以理解开放词汇的自然语言查询，限制了其在复杂场景中的应用。

Method: 通过构建动态的AR场景图（包含九种类型的空间、语义和因果关系），并利用自适应任务代理协调MLLMs与具备坐标感知能力的视觉工具，实现从简单识别到多对象关系推理的任务。

Result: 系统能返回米级精度的3D锚点，支持任务自适应的兴趣区域高亮与上下文感知的空间检索，并引入了评估基准GroundedAR-Bench。

Conclusion: 该模块化架构无需重新训练即可集成不同的视觉-语言模型，使AR代理成为增强MLLM空间智能、实现交互式场景理解的中介。

Abstract: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.

</details>


### [37] [TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion](https://arxiv.org/abs/2512.00300)
*Rui Qian,Haozhi Cao,Tianchen Deng,Tianxin Hu,Weixiang Guo,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: 本文提出TGSFormer，一种可扩展的时间高斯点绘框架，用于具身3D语义场景补全，通过持久化高斯记忆和双时间编码器实现高效时序融合与体素对齐，显著提升精度、可扩展性与长期场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的方法存在初始化冗余、难以扩展至无界场景的问题；深度引导方法虽缓解但仍有局部性、延迟和内存开销大等缺陷。

Method: 提出TGSFormer：1）构建持久化高斯记忆用于时序预测；2）设计双时间编码器，结合置信度感知的跨注意力机制联合处理当前与历史高斯特征；3）引入置信度感知体素融合模块，将重叠高斯原语融合为体素对齐表示，并调控密度以保持紧凑性。

Result: 在局部与具身SSC基准上均达到SOTA性能，精度更高、所需高斯原语更少、可扩展性强，并保持长期场景完整性。

Conclusion: TGSFormer有效克服了现有高斯方法在冗余性、局部性和可扩展性方面的局限，为具身语义场景补全提供了高效、鲁棒且可扩展的新范式。

Abstract: Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.

</details>


### [38] [Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation](https://arxiv.org/abs/2512.00308)
*Xiao Cui,Yulei Qin,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于最优传输（OT）的新型数据集蒸馏方法，通过在全局和实例层面进行细粒度对齐，提升蒸馏数据集的质量与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集蒸馏方法仅匹配全局统计量（如均值、方差），忽略了实例级特征和类内差异，导致泛化性能受限。

Method: 将数据集蒸馏建模为最优传输距离最小化问题，并设计三个核心组件：OT引导的扩散采样、标签-图像对齐的软重标注、基于OT的logit匹配。

Result: 在ImageNet-1K等大规模数据集上，IPC=10设置下，各网络架构平均提升至少4%准确率，显著优于现有SOTA方法。

Conclusion: 最优传输为数据集蒸馏提供了几何保真、细粒度可控的分布对齐框架，能有效保留局部模态与类内结构，是提升蒸馏质量的有效范式。

Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.

</details>


### [39] [ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays](https://arxiv.org/abs/2512.00310)
*Qinyi Cao,Jianan Fan,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了一种解剖结构感知的真实纹理异常合成框架（ART-ASyn），用于胸部X光片的无监督异常检测，通过基于纹理的增强与新提出的渐进式二值阈值分割（PBTSeg）方法生成解剖一致、视觉逼真的肺部不透明异常，并支持显式的像素级分割监督及零样本异常分割。


<details>
  <summary>Details</summary>
Motivation: 现有合成异常方法生成的异常在视觉上与真实病理模式差异大，且忽略解剖结构，限制了模型在真实场景中的泛化能力。

Method: 提出ART-ASyn框架，结合纹理驱动的异常合成与PBTSeg肺部分割方法，生成解剖一致、高保真的合成异常及对应精确掩码；支持显式分割监督，并拓展至零样本异常分割任务。

Result: 在胸部X光数据上验证了ART-ASyn的有效性，不仅提升异常检测性能，还首次在无目标域标注情况下实现零样本异常分割，展现出强跨域泛化能力。

Conclusion: ART-ASyn通过解剖感知与纹理建模的协同设计，显著提升了合成异常的真实性与实用性，为医学图像无监督异常检测与分割提供了新范式。

Abstract: Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.

</details>


### [40] [Odometry Without Correspondence from Inertially Constrained Ruled Surfaces](https://arxiv.org/abs/2512.00327)
*Chenqi Zhu,Levi Burner,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像空间中直线轨迹所形成的直纹面（ruled surface）来估计视觉里程计和重建3D场景的新方法，结合事件相机的边缘响应特性和IMU惯性测量以降低解空间维度，避免传统点对点光流匹配的计算开销与精度波动问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉里程计依赖特征点提取与光流匹配，计算成本高且精度不稳定；尽管已有工作尝试使用线特征或多传感器融合，仍大多依赖对应关系；而直线运动在图像时空域中形成具有几何意义的直纹面，可提供更鲁棒的运动信息。

Method: 利用相机运动下直线在图像时空域中扫过的直纹面建模，通过点到线的微分关联估计该表面，并融合事件相机的边缘检测能力与IMU的惯性测量对直纹面进行约束，从而联合求解视觉里程计与3D场景重建。

Result: 实现了无需显式点对应、计算效率更高且对纹理缺失或快速运动更鲁棒的视觉里程计与3D重建；IMU约束显著降低了直纹面参数解空间的维度，提升了估计稳定性与精度。

Conclusion: 直纹面为视觉里程计提供了新的几何先验，结合事件相机与IMU的协同感知范式，可有效绕过传统对应难题，在挑战性场景中展现出优越性能与潜力。

Abstract: Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.

</details>


### [41] [MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection](https://arxiv.org/abs/2512.00336)
*Mengxue Hu,Yunfeng Diao,Changtao Miao,Jianshu Li,Zhe Li,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文介绍了首个专门用于检测AI生成的多模态视频-音频内容的综合数据集MVAD，旨在解决现有数据集在多模态覆盖和真实性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有合成视频数据集主要关注视觉模态，少数包含音频的数据集又局限于面部深度伪造，无法应对日益增长的通用多模态AI生成内容，阻碍了可信检测系统的发展。

Method: 构建了一个名为MVAD的多模态视频-音频数据集，涵盖三种真实视频-音频伪造模式、多种先进生成模型、现实与动漫视觉风格、四类内容（人类、动物、物体、场景）以及四种视频-音频多模态数据类型。

Result: 提出了首个全面支持AI生成多模态视频-音频内容检测的数据集MVAD，具备真实多模态性、高感知质量与广泛多样性。

Conclusion: MVAD填补了多模态AI生成内容检测数据集的空白，为开发更鲁棒、泛化性更强的检测方法提供了关键资源。

Abstract: The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.

</details>


### [42] [Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models](https://arxiv.org/abs/2512.00343)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出AMDET框架，无需先验知识即可检测视觉-语言预训练模型中的后门攻击，利用文本编码器中特征同化现象和梯度反演技术识别并区分自然与恶意后门特征，在多种设置下达到89.90% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法依赖训练数据、触发器、目标标签或下游分类器等先验知识，在实际场景中难以获取，因此亟需一种无需这些假设的模型级检测方法。

Method: 提出AMDET框架：首先发现后门文本编码器中所有token表征高度相似（特征同化），归因于注意力集中在触发token；继而通过梯度驱动的token嵌入反演恢复可激活后门的隐式特征；最后通过损失景观分析区分自然出现的类后门特征与真实注入后门。

Result: 在3600个后门/良性微调模型上实验表明，AMDET达到89.90% F1分数；单次检测耗时约5分钟（RTX 4090），且对自适应攻击具有强鲁棒性。

Conclusion: AMDET是一种高效、实用、无需先验知识的VLP后门检测方法，首次揭示并利用文本编码器中的特征同化特性，并成功识别CLIP中天然存在的类后门特征，提升了模型安全评估的可靠性。

Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET

</details>


### [43] [mmPred: Radar-based Human Motion Prediction in the Dark](https://arxiv.org/abs/2512.00345)
*Junqiao Fan,Haocong Rao,Jiarui Zhang,Jianfei Yang,Lihua Xie*

Main category: cs.CV

TL;DR: 本文首次提出将毫米波雷达用于人体运动预测（HMP），并设计了基于扩散模型的mmPred框架，通过双域历史运动表征和全局骨骼关系Transformer，在噪声和不一致的雷达信号下实现领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB-D相机的人体运动预测方法对光照敏感且存在隐私问题，而毫米波雷达具有鲁棒性和隐私保护优势，因此本文探索其在HMP中的新应用。

Method: 提出mmPred，首个基于扩散模型的雷达HMP框架；引入双域历史运动表示，包括时域姿态优化分支（TPR）和频域主导运动分支（FDM），并设计全局骨骼关系Transformer（GST）作为扩散主干网络。

Result: 在mmBody和mm-Fi数据集上分别比现有方法提升8.6%和22%，实现了最先进的雷达HMP性能。

Conclusion: 毫米波雷达是可行且有前景的HMP传感模态，mmPred通过专门设计有效应对雷达信号噪声与不一致问题，为未来低光照、高隐私需求场景下的运动预测提供了新方向。

Abstract: Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.

</details>


### [44] [SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction](https://arxiv.org/abs/2512.00355)
*Junqiao Fan,Pengfei Liu,Haocong Rao*

Main category: cs.CV

TL;DR: 本文提出SMamDiff，一种基于空间Mamba的单阶段扩散模型，用于人体运动预测，通过残差DCT运动编码和stickman-drawing空间Mamba模块提升时空一致性，在保持低延迟与内存占用的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有HMP方法在确定性预测中忽略不确定性，或在概率建模中牺牲运动学合理性；扩散模型虽改善精度-多样性权衡，但多阶段流程不利于边缘部署。

Method: 提出SMamDiff模型：(i) 采用残差-DCT运动编码，先减去最后观测姿态再做时域DCT，削弱DC分量主导、增强高频运动特征学习；(ii) 设计stickman-drawing空间Mamba模块，按关节顺序逐点处理，使后续关节条件依赖前序关节，建模长程跨关节依赖。

Result: 在Human3.6M和HumanEva数据集上，SMamDiff在单阶段概率HMP方法中达到SOTA性能，且延迟和内存开销低于多阶段扩散基线。

Conclusion: 单阶段扩散模型可通过结构化时空编码与有序关节建模实现高保真、高效率的人体运动预测，为边缘智能服务机器人提供实用化HMP方案。

Abstract: With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.

</details>


### [45] [MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters](https://arxiv.org/abs/2512.00363)
*Jianhong Han,Yupei Wang,Yuan Zhang,Liang Chen*

Main category: cs.CV

TL;DR: 本文提出MM-DETR，一种轻量高效多模态遥感目标检测框架，通过Mamba-based双粒度融合编码器、区域感知2D选择性扫描补全分支和轻量频域感知模态适配器，在保持高性能的同时显著降低计算与参数开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态融合中难以兼顾性能与轻量化：注意力/可变形卷积融合复杂；共享骨干网缺乏模态特异性建模；双流结构参数翻倍，不利于部署。

Method: 提出MM-DETR框架，包含：（1）基于Mamba的双粒度融合编码器，用通道动态门控与1D选择性扫描实现线性复杂度跨模态建模；（2）区域感知2D选择性扫描补全分支，以双向金字塔路径完成模态特异性线索恢复；（3）轻量频域感知模态适配器，采用空间-频率协同专家结构与像素级路由器实现高效融合。

Result: 在四个多模态遥感基准数据集上验证了方法的有效性与泛化能力，显著提升检测精度同时降低模型复杂度与参数量。

Conclusion: MM-DETR通过创新的Mamba架构设计与模态补全机制，在多模态遥感目标检测任务中实现了性能与效率的更好权衡，为实际部署提供了可行方案。

Abstract: Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.

</details>


### [46] [Towards aligned body representations in vision models](https://arxiv.org/abs/2512.00365)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 研究表明，在资源受限的情况下，较小的视觉分割模型会自然形成类似人类的粗略身体表征，而较大的模型则倾向于过度精细的编码，揭示了机器模型在模拟人类物理推理方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模型是否能在语义分割任务中发展出类似人类使用的粗略身体表征，并理解这些表征的内部结构。

Method: 改编针对50名人类参与者的心理物理学实验，应用于七种不同规模的语义分割网络，比较其表征特性。

Result: 发现较小的模型表现出类似人类的粗略表征，而较大的模型则产生更精细、非人类-like的编码。

Conclusion: 计算资源限制有助于粗略表征的形成，机器模型可为研究大脑中物理推理的结构提供可扩展的路径。

Abstract: Human physical reasoning relies on internal "body" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.

</details>


### [47] [THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2512.00368)
*Jian Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种可信的分层对比表示学习方法（THCRL），通过深度对称分层融合（DSHF）模块和平均K近邻对比学习（AKCL）模块，解决多视图聚类中因视图噪声和忽略簇内结构信息导致的不可靠融合问题，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法存在不可靠融合问题：一是忽视各视图内在噪声；二是传统对比学习仅利用同一样本的不同视图计算相似性，忽略了同一簇内近邻样本的结构信息，导致融合方向错误。

Method: 提出可信分层对比表示学习（THCRL）框架，包含两个核心模块：1）基于UNet并集成多种去噪机制的深度对称分层融合（DSHF）模块，实现可信的多视图融合；2）平均K近邻对比学习（AKCL）模块，通过增强同一簇内样本间的表示相似性，而非仅依赖同一样本跨视图一致性，来对齐融合表示与单视图表示。

Result: 在多个基准数据集上的大量实验表明，THCRL在深度多视图聚类任务中达到当前最优性能。

Conclusion: THCRL通过引入去噪融合与簇内结构感知的对比学习，有效缓解了多视图聚类中的不可靠融合问题，为提升多视图表示学习的鲁棒性和判别性提供了新思路。

Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.

</details>


### [48] [POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models](https://arxiv.org/abs/2512.00369)
*Wenshuo Chen,Haosen Li,Shaofeng Liang,Lei Wang,Haozhe Jia,Kaishen Yuan,Jieming Wu,Bowen Tian,Yutao Yue*

Main category: cs.CV

TL;DR: 本文提出POLARIS方法，通过将扩散模型反演过程中的误差补偿问题转化为误差起源问题，以步进式可变引导尺度ω最小化每步反演误差，显著缓解噪声近似误差并提升下游任务精度。


<details>
  <summary>Details</summary>
Motivation: 揭示了基于扩散模型的Inversion-Denoising范式中被忽视的关键问题：由于用t-1步预测近似t步噪声所导致的近似噪声误差及其在反演过程中严重累积，造成重建质量下降。

Method: 提出Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS)，将反演建模为误差起源问题；不优化嵌入或潜在码来补偿漂移，而是将引导尺度ω设为每步变量，并推导出数学上严谨的公式，在每步最小化反演误差。

Result: 仅需一行代码即可显著提升反演潜在表示质量；在几乎无性能开销前提下，有效缓解噪声近似误差，一致提升下游图像编辑与重建任务的准确性。

Conclusion: POLARIS通过重新建模反演机制，从根源上抑制误差积累，是一种简洁、鲁棒且自适应的扩散模型反演改进方案。

Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.

</details>


### [49] [Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features](https://arxiv.org/abs/2512.00381)
*Dong Li,HuaLiang Lin,JiaYu Li*

Main category: cs.CV

TL;DR: 本文提出了PorePatch数据集和数据-模型协同进化（DMCE）框架，用于生成高质量的毛孔级面部图像块数据集，并在此基础上训练和评估深度学习描述子。实验表明，现有SOTA模型在匹配任务中显著优于传统方法，但在3D重建任务中优势不明显，表明深度学习描述子在弱纹理面部区域仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 面部皮肤区域纹理较弱，传统局部描述子难以有效匹配；同时缺乏高质量的毛孔级图像块数据集，限制了深度学习描述子在该领域的应用与发展。

Method: 提出PorePatch数据集与DMCE框架，从高分辨率面部图像中逐步生成高质量、渐进优化的数据集，并基于此训练现有的SOTA深度学习描述子模型。

Result: SOTA模型在匹配任务上达到1.91%的FPR95，远优于PSIFT的22.41%；但在3D重建任务中性能提升有限，与传统描述子相比无显著优势。

Conclusion: 尽管深度学习描述子在面部图像匹配任务中表现优异，但在3D重建等实际应用中仍面临挑战，尤其在处理弱纹理区域时仍有待进一步研究。

Abstract: The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.

</details>


### [50] [EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation](https://arxiv.org/abs/2512.00385)
*Louis Geist,Loic Landrieu,Damien Robert*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的全GPU超点划分算法，显著加速了超点生成过程，并在保持高精度的同时大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于超点的3D语义分割流程受限于CPU密集型的划分步骤，效率低下。

Method: 设计了一个轻量级、可学习、端到端可微的GPU划分模块，配合轻量超点分类器，无需手工特征。

Result: 相比先前方法快13倍；推理速度快72倍、参数少120倍；在S3DIS、KITTI-360和DALES三个数据集上达到点基SOTA精度；全流程VRAM占用<2MB，支持实时推理与百万级点云场景。

Conclusion: 该方法在速度、内存和精度之间实现了优异平衡，推动超点分割走向高效实用化。

Abstract: Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\times$ faster inference and 120$\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at github.com/drprojects/superpoint_transformer.

</details>


### [51] [WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing](https://arxiv.org/abs/2512.00387)
*Kaihang Pan,Weile Chen,Haiyi Qiu,Qifan Yu,Wendong Bu,Zehan Wang,Yun Zhu,Juncheng Li,Siliang Tang*

Main category: cs.CV

TL;DR: 本文提出了WiseEdit，一个面向认知与创意驱动图像编辑的知识密集型基准，通过分解编辑过程为意识、解释和想象三个阶段，并融合三种知识类型，全面评估现有模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准评估范围过窄，无法全面衡量先进图像编辑模型在认知与创造力方面的综合能力。

Method: 提出WiseEdit基准，将图像编辑类比人类认知创作，划分为Awareness、Interpretation、Imagination三阶段任务，并整合Declarative、Procedural、Metacognitive三类知识，构建1220个测试用例。

Result: WiseEdit客观揭示了当前SOTA图像编辑模型在知识驱动的认知推理与创意构图能力上的局限性。

Conclusion: WiseEdit为评估认知与创意导向的图像编辑提供了更全面、更深入的基准，推动该领域向知识增强方向发展。

Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.

</details>


### [52] [Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction](https://arxiv.org/abs/2512.00395)
*Jiazhen Liu,Mingkuan Feng,Long Chen*

Main category: cs.CV

TL;DR: 本文提出STAMP模型，通过all-mask预测新范式解决多模态大语言模型（MLLM）在对话能力、分割性能与推理速度之间的三难困境。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时兼顾MLLM的对话能力、高分割精度和快速推理：嵌入预测损害对话能力，自回归分割则牺牲性能或速度。

Method: 提出all-mask预测范式，将分割建模为对图像块的并行‘填空’任务；设计STAMP模型，在生成文本响应后，单次前向传播即可预测完整分割掩码。

Result: STAMP在多个分割基准上显著超越SOTA方法，同时保持强对话能力与极快推理速度。

Conclusion: all-mask预测成功解耦对话生成与分割预测，实现了三者兼顾的无妥协解决方案。

Abstract: Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.

</details>


### [53] [Low-Bitrate Video Compression through Semantic-Conditioned Diffusion](https://arxiv.org/abs/2512.00408)
*Lingdong Wang,Guan-Ming Su,Divya Kothandaraman,Tsung-Wei Huang,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.CV

TL;DR: 本文提出了一种语义视频压缩框架DiSCo，通过文本描述、时空降质视频和可选草图/姿态三种紧凑模态来捕捉语义、外观和运动信息，并利用条件视频扩散模型重建高质量视频，在低码率下显著提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器在超低码率下因追求像素保真度而产生严重失真，根本原因在于像素精度与人类感知不一致。

Method: 提出DiSCo框架：将源视频分解为文本描述、时空降质视频和可选草图/姿态三类紧凑模态；使用条件视频扩散模型进行重建；引入时间前向填充、token交织和模态特定编码器以提升多模态生成效果和模态紧凑性。

Result: 实验表明，该方法在低码率下感知指标上优于基线语义和传统编解码器2-10倍。

Conclusion: 语义驱动的视频压缩结合生成先验可有效克服传统编解码器在超低码率下的局限性，提升人眼感知质量。

Abstract: Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.

</details>


### [54] [PhysGen: Physically Grounded 3D Shape Generation for Industrial Design](https://arxiv.org/abs/2512.00422)
*Yingxuan You,Chen Zhao,Hantao Zhang,Mingda Xu,Pascal Fua*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理引导的3D形状生成新方法，通过结合流匹配模型与物理约束（如空气动力学性能），在潜在空间中交替进行速度更新和物理优化，并引入物理感知正则项及联合编码形状与物理信息的SP-VAE，显著提升了工业设计场景下生成形状的物理真实性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型缺乏对物理属性（如汽车的空气动力学效率）的建模能力，导致生成形状虽视觉逼真但物理上不真实，难以满足工业设计需求。

Method: 提出一种带显式物理引导的流匹配模型，采用速度更新与物理精炼交替迭代的方式优化潜在码；引入物理感知正则项；构建联合编码形状与物理信息的SP-VAE以支持物理引导更新。

Result: 在三个基准测试中验证了该方法能显著提升生成3D形状的物理真实性和整体 realism，超越仅依赖视觉合理性的传统方法。

Conclusion: 物理知识可有效融入生成式建模流程，所提统一物理引导框架为工业级3D设计生成提供了新范式。

Abstract: Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.

</details>


### [55] [Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali](https://arxiv.org/abs/2512.00424)
*Nthenya Kyatha,Jay Taneja*

Main category: cs.CV

TL;DR: 本文提出了一种基于现有车载CCTV的公交OD流识别基线方案，融合YOLOv12检测、BotSORT跟踪、OSNet重识别、OCR时间戳和车辆遥测数据停靠点分类，在理想条件下表现良好，但在非洲真实拥挤场景下性能显著下降，揭示了部署特异性失效模式，并呼吁开发更鲁棒、面向实际部署的重识别方法。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲（SSA）公共交通普遍拥挤，现有自动化系统难以可靠获取乘客流动数据；而当地已广泛部署车载CCTV用于安保，具备潜在再利用价值。

Method: 构建端到端基线流程：使用YOLOv12进行行人检测，BotSORT进行多目标跟踪，OSNet提取外观特征实现重识别，OCR识别视频帧时间戳，结合车辆遥测数据（如GPS/IMU）进行站点分类，最终推断上下车位置并生成OD矩阵。

Result: 在内罗毕与基加利标注的CCTV片段上，低密度、光照良好条件下计数准确率高（召回率≈95%，精确率≈91%，F1≈93%），OD矩阵与人工统计高度一致；但在真实压力场景（如拥挤、黑白画面、姿态多变、非标车门使用）下性能骤降（如高峰上车计数低估约40%，黑白片段召回率下降约17个百分点）。

Conclusion: 当前通用模型在SSA真实公交场景中泛化能力不足，暴露了部署特异性失效模式，亟需发展更鲁棒、面向本地部署条件优化的行人重识别方法。

Abstract: Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\approx$95\%, precision $\approx$91\%, F1 $\approx$93\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\sim$40\% undercount in peak-hour boarding and a $\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.

</details>


### [56] [What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards](https://arxiv.org/abs/2512.00425)
*Minh-Quan Le,Yuanzhi Zhu,Vicky Kalogeiton,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出NewtonRewards，一种基于可验证奖励的物理感知视频生成后训练框架，通过光学流和外观特征作为速度与质量代理，结合牛顿动力学约束与质量守恒奖励，显著提升生成视频的物理合理性与时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽视觉逼真，但常违反基本物理规律（如漂浮、加速度不一致、碰撞异常），缺乏物理真实性，需一种无需人工或VLM反馈的可验证物理约束方法。

Method: 提出NewtonRewards框架，利用冻结的辅助模型提取光学流（速度代理）和高层外观特征（质量代理），设计两种奖励：牛顿运动学约束（恒定加速度）和质量守恒奖励，以增强物理一致性。

Result: 在新构建的NewtonBench-60K基准上评估五种牛顿运动基元，NewtonRewards在视觉与物理指标上均优于先前方法，提升了物理可信度、运动平滑性和时间连贯性，并在高度、速度、摩擦等分布外场景下保持鲁棒。

Conclusion: 基于物理原理的可验证奖励为实现可扩展的物理感知视频生成提供了有效路径。

Abstract: Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\texttt{NewtonRewards}$ extracts $\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.

</details>


### [57] [Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana](https://arxiv.org/abs/2512.00428)
*Jiachuan Peng,Kyle Lam,Jianing Qiu*

Main category: cs.CV

TL;DR: 该研究使用Google最新AI模型Nano Banana生成的合成胸部X光图像训练分类器，并在真实世界数据上实现了较高的肺炎识别性能，验证了合成数据在医学AI开发中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索合成数据在医疗AI开发中的可行性，减少对真实患者数据的依赖。

Method: 利用Nano Banana模型生成合成胸部X光图像，训练分类器并在两个公开真实数据集（RSNA 2018和Chest X-Ray）上进行外部验证。

Result: 在RSNA数据集上达到AUROC 0.923，AUPR 0.900；在Chest X-Ray数据集上达到AUROC 0.824，AUPR 0.913，显示模型在真实数据上的良好表现。

Conclusion: 合成数据训练的模型在真实医疗影像中具有应用潜力，但仍需解决提示设计、数据对齐问题，并面临临床转化前的验证、监管与伦理挑战。

Abstract: We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.

</details>


### [58] [FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal](https://arxiv.org/abs/2512.00438)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Filling-Based Reward (FR)的新方法，用于解决测试时扩展（TTS）在下一个词预测（NTP）任务中难以应用的问题。通过估计中间序列的未来完成轨迹，FR显著提高了中间样本与最终样本奖励之间的相关性，并在此基础上提出了FR-TTS策略，在多个基准上展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展（TTS）技术在图像生成中有效，但在下一个词预测（NTP）中因中间token序列的奖励与最终输出奖励相关性低而难以适用，主因是中间序列语义与规模不完整，无法可靠指导剪枝。

Method: 提出基于填充的奖励（FR），通过为中间token序列寻找合理填充方案来估计其未来完成轨迹；在此基础上设计FR-TTS策略，结合动态加权的多样性奖励，高效搜索优质填充方案并综合评估中间样本。

Result: FR显著提升了中间样本与最终样本奖励间的相关系数，并改善了token置信度等内在指标；FR-TTS在多个基准和不同奖励模型下均优于现有方法。

Conclusion: FR及其衍生的FR-TTS策略有效克服了TTS在NTP任务中的核心瓶颈，为语言模型测试时解码优化提供了新范式。

Abstract: Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.

</details>


### [59] [RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications](https://arxiv.org/abs/2512.00450)
*Amit Kumar Gupta,Farhan Sheth,Hammad Shaikh,Dheeraj Kumar,Angkul Puniya,Deepak Panwar,Sandeep Chaurasia,Priya Mathur*

Main category: cs.CV

TL;DR: 本文提出了RecruitView数据集和CRMF模型，用于从多模态行为数据中自动评估人格与软技能，通过在双曲、球面和欧氏流形上建模行为表征，显著提升了评估性能并降低了参数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于数据集规模小且无法有效捕捉人类特质中固有的几何结构，导致自动化人格与软技能评估效果不佳。

Method: 提出Cross-Modal Regression with Manifold Fusion (CRMF)，一种几何深度学习框架，利用几何特异性专家网络分别建模双曲、球面和欧氏流形上的行为表征，并通过自适应路由与切空间融合实现多流形联合回归。

Result: CRMF在RecruitView数据集上显著优于基线模型，Spearman相关系数提升达11.4%，一致性指数提升6.0%，同时训练参数减少40–50%。

Conclusion: CRMF验证了显式建模多模态行为数据几何结构的有效性，RecruitView数据集为未来人格与软技能评估研究提供了高质量基准资源。

Abstract: Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView

</details>


### [60] [CausalAffect: Causal Discovery for Facial Affective Understanding](https://arxiv.org/abs/2512.00456)
*Guanyu Hu,Tangzheng Lian,Dimitrios Kollias,Oya Celiktutan,Xinyu Yang*

Main category: cs.CV

TL;DR: 本文提出了CausalAffect，首个用于面部情感分析的因果图发现框架，通过两层极性与方向感知的因果层次结构建模动作单元（AU）间及AU与表情间的依赖关系，并引入特征级反事实干预机制以增强因果效应、抑制虚假相关；无需联合标注数据或人工先验，即可恢复符合心理学理论且揭示新抑制性依赖的因果结构，在多个基准上提升了AU检测与表情识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法很少从数据中直接推断动作单元（AUs）与表情之间心理上合理的因果关系，而理解人类情感需要结构化地推理驱动肌肉激活及其表达结果的潜在依赖。

Method: 提出CausalAffect框架：构建两层极性与方向感知的因果层次结构，整合群体规律与样本自适应结构；引入特征级反事实干预机制以强化真实因果效应并抑制虚假相关；不依赖联合标注数据或手工因果先验。

Result: 在六个基准数据集上显著提升AU检测和表情识别性能；恢复的因果结构与心理学理论一致，并揭示了新的抑制性及未被表征的依赖关系。

Conclusion: CausalAffect首次将因果发现系统引入面部情感分析，建立了因果建模与可解释面部行为分析之间的原则性联系，为情感计算提供了更可靠、可解释的新范式。

Abstract: Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.

</details>


### [61] [RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards](https://arxiv.org/abs/2512.00473)
*Junyan Ye,Leiqi Zhu,Yuncheng Guo,Dongzhi Jiang,Zilong Huang,Yifan Zhang,Zhiyuan Yan,Haohuan Fu,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出了RealGen，一种用于提升文本到图像生成真实感的框架，通过引入检测器奖励机制和GRPO算法优化生成流程，在 realism、细节和美学方面显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前先进图像生成模型在照片级真实感生成方面仍存在不足，常产生具有AI伪影的“假”图像，无法达到“与现实无法区分”的目标。

Method: RealGen结合LLM进行提示优化和扩散模型生成图像，引入基于语义级和特征级合成图像检测器的“检测器奖励”机制，并利用GRPO算法优化整个生成 pipeline。同时提出RealBench作为自动化评估基准。

Result: 实验表明，RealGen在真实感、细节和美学质量上显著优于GPT-Image-1、Qwen-Image等通用模型及FLUX-Krea等专用真实感模型；RealBench实现了无需人工的高效准确评估。

Conclusion: RealGen通过检测器奖励机制有效提升了生成图像的真实感和细节，推动了文本到图像生成向‘难以与现实区分’的目标迈进，且RealBench为真实感评估提供了可靠自动化方案。

Abstract: With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.

</details>


### [62] [Structured Context Learning for Generic Event Boundary Detection](https://arxiv.org/abs/2512.00475)
*Xin Gu,Congcong Li,Xinyao Wang,Dexiang Hong,Libo Zhang,Tiejian Luo,Longyin Wen,Heng Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为结构化上下文学习（Structured Context Learning）的新方法，通过结构化序列划分（SPoS）为视频帧序列提供结构化上下文，结合组相似性计算与轻量全卷积网络进行通用事件边界检测（GEBD），并在多个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用事件边界检测（GEBD）任务中对人类感知事件边界的精准识别问题，同时克服现有方法依赖特定时序模型（如LSTM、Transformer）带来的灵活性与效率限制，并缓解标注模糊性问题。

Method: 提出结构化序列划分（SPoS）以线性复杂度划分视频帧序列并构建结构化上下文；计算帧间组相似性以捕捉差异；采用轻量全卷积网络基于相似性图预测事件边界；使用高斯核对真值边界进行预处理以缓解标注模糊。

Result: 在Kinetics-GEBD、TAPOS和镜头转换检测等多个具有挑战性的数据集上，该方法显著优于现有最先进方法，实现了更优的速度-精度权衡。

Conclusion: 结构化上下文学习是一种灵活、高效且端到端可训练的GEBD方法，不依赖特定时序模型，具备良好的泛化能力与实际部署潜力。

Abstract: Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.

</details>


### [63] [Learning What Helps: Task-Aligned Context Selection for Vision Tasks](https://arxiv.org/abs/2512.00489)
*Jingyu Guo,Emir Konuk,Fredrik Strand,Christos Matsoukas,Kevin Smith*

Main category: cs.CV

TL;DR: 提出了一种名为Task-Aligned Context Selection (TACS)的框架，通过联合训练选择器网络和任务模型，学习选取真正能提升任务性能的上下文示例，而非仅表面相似的示例。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer（ViTs）缺乏识别哪些示例能够改善其预测的能力，导致在处理视觉不确定性时效果受限。

Method: TACS采用混合优化方案，结合基于梯度的监督和强化学习，将检索过程融入学习目标中，使选择器网络与任务模型共同训练，并通过任务奖励对齐示例选择。

Result: 在涵盖细粒度识别、医学图像分类和分割的18个数据集上，TACS consistently 优于基于相似性的检索方法，尤其在具有挑战性或数据有限的情况下表现更优。

Conclusion: TACS通过将上下文示例的选择与任务目标对齐，有效提升了模型性能，展示了将检索机制嵌入学习过程中的潜力。

Abstract: Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.

</details>


### [64] [CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration](https://arxiv.org/abs/2512.00493)
*Boshi Tang,Henry Zheng,Rui Huang,Gao Huang*

Main category: cs.CV

TL;DR: 本文提出CC-FMO，一种零样本、相机条件化的单图到3D场景生成方法，通过混合实例生成器和相机条件尺度求解算法，实现语义合理、几何高质量且空间一致的场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单图3D场景生成中面临泛化能力差、物体姿态估计不准及空间不一致等问题，尤其难以实现场景级一致性。

Method: 提出CC-FMO框架：1）混合实例生成器，融合语义感知的向量集表示与细节丰富的结构化潜在表示；2）基于基础姿态估计模型的相机条件化尺度求解算法，保障场景级空间对齐与一致性。

Result: 在多项实验中，CC-FMO持续生成高保真、相机对齐、可组合的3D场景，性能全面超越当前最先进方法。

Conclusion: CC-FMO实现了零样本下兼顾实例质量与场景一致性的单图3D场景生成，为AR/VR和具身AI提供了实用新范式。

Abstract: High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.

</details>


### [65] [Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching](https://arxiv.org/abs/2512.00514)
*Tanaka Nobuaki*

Main category: cs.CV

TL;DR: 本文提出了一种基于智能手机的结构光系统，利用投影网格图案并结合新提出的拓扑约束二维动态时间规整（2D-DTW）算法，实现对地面不平整度的单设备实时重建，适用于低成本移动机器人在非结构化地形中的稳定运动感知。


<details>
  <summary>Details</summary>
Motivation: 低成本移动平台在不平坦地形中运行时，难以视觉察觉微小起伏或倾斜，但这些因素会严重影响运动稳定性，亟需轻量、低成本、单设备的局部地形感知方案。

Method: 设计基于智能手机的结构光系统，投影地面网格图案；针对二维网格匹配难题，提出拓扑约束的二维动态时间规整（2D-DTW）算法，实现列对齐与全局网格一致性联合优化，并适配资源受限平台。

Result: 所提2D-DTW算法能鲁棒匹配受透视畸变和部分遮挡影响的变形网格，在手机端实时运行；系统成功实现局部地形不平整度重建；该算法亦可泛化用于其他图像处理中的结构化网格匹配任务。

Conclusion: 本文验证了智能手机结构光+2D-DTW方案在低成本地形感知中的可行性与有效性，其核心算法兼具实用性与通用性，为嵌入式视觉感知提供了新思路。

Abstract: Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.

</details>


### [66] [Image Generation as a Visual Planner for Robotic Manipulation](https://arxiv.org/abs/2512.00532)
*Ye Pang*

Main category: cs.CV

TL;DR: 本文探索了如何利用预训练图像生成模型（如语言-图像模型）作为机器人操作的视觉规划器，通过LoRA微调实现文本或轨迹条件下的机器人操作视频生成，无需显式时间建模，且在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型依赖大量领域特定数据且泛化能力差，而预训练图像生成模型展现出强组合性与潜在的时间一致性生成能力，启发研究者探索其作为轻量级视觉规划器的可能性。

Method: 提出两阶段框架：(1) 文本条件生成，输入语言指令和首帧；(2) 轨迹条件生成，输入2D轨迹叠加图和首帧；均基于LoRA微调预训练图像生成模型。

Result: 在Jaco Play、Bridge V2和RT1数据集上，两种生成模式均能生成平滑、连贯且条件对齐的机器人操作视频。

Conclusion: 预训练图像生成模型隐含可迁移的时间先验，仅需极少监督即可充当类视频的机器人视觉规划器。

Abstract: Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.
  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.
  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.

</details>


### [67] [Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update](https://arxiv.org/abs/2512.00534)
*Zeyuan An,Yanghang Xiao,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 本文提出Cross-Temporal 3D Gaussian Splatting（Cross-Temporal 3DGS），一种利用稀疏图像和历史场景先验跨时间高效重建与更新3D场景的新框架，包含跨时间相机对齐、基于干扰的置信度初始化和渐进式跨时间优化三阶段，支持非连续采集与双向时序重建，在重建质量与数据效率上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中（如城市规划、灾后评估、遗址保护）常缺乏密集扫描数据，亟需从稀疏视角观测中持续更新3D场景表示。

Method: 提出三阶段框架：1）跨时间相机对齐以估计并统一不同时刻相机位姿；2）基于干扰的置信度初始化识别跨时刻不变区域，指导更新区域；3）渐进式跨时间优化，迭代融合历史先验信息提升重建质量。支持非连续采集及新旧数据互辅助重建。

Result: 在重建质量与数据效率上显著优于基线方法，可实现仅用稀疏图像感知时序变化，并按需生成高细节3D表示。

Conclusion: Cross-Temporal 3DGS为场景版本管理、跨时间数字孪生与长期空间存档提供了高效可行的新范式。

Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.

</details>


### [68] [SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning](https://arxiv.org/abs/2512.00539)
*Yongkang Hu,Yu Cheng,Yushuo Zhang,Yuan Xie,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 本文提出了一种具有场景感知和重要性引导动态优化的持续学习检测框架SAIDO，用于提升AI生成图像检测在新场景和新生成方法下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法在面对新兴生成模型和内容类型时泛化能力差，难以适应真实开放环境。

Method: 设计了基于场景感知的专家模块（SAEM），利用VLLMs动态识别并引入新场景，并为每个场景分配独立专家模块；同时提出重要性引导的动态优化机制（IDOM），通过重要性引导的梯度投影策略优化神经元，平衡模型的可塑性与稳定性。

Result: 在持续学习任务中，相比当前SOTA方法，平均检测错误率和遗忘率分别相对降低44.22%和40.57%；在开放世界数据集上，平均检测准确率提升9.47%。

Conclusion: SAIDO框架显著提升了生成图像检测在跨场景和多生成器环境下的检测性能与持续学习能力，有效缓解了灾难性遗忘问题，具备良好的实际应用潜力。

Abstract: The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.

</details>


### [69] [Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions](https://arxiv.org/abs/2512.00547)
*Sandika Biswas,Qianyi Wu,Biplab Banerjee,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本文提出了一种混合方法，结合3D生成模型、语义感知形变（刚体变换+LBS人体形变）和3D高斯泼溅（GS）优化，用于单目动态多人体-多物体场景的高质量、结构一致的三维几何重建。


<details>
  <summary>Details</summary>
Motivation: 现实世界中人类构建的动态环境包含多人与物体的复杂交互，但现有3D建模方法（尤其是基于GS的方法）难以应对多主体、频繁遮挡、运动多样性等挑战，尤其在单目设置下结构一致性更难保障。

Method: 提出混合框架：1）用3D生成模型生成各场景元素（人/物）的高保真网格；2）进行语义感知形变——对刚体对象做刚性变换，对人体使用LBS形变，并将形变后网格映射到动态场景；3）对各元素分别进行GS优化以精调空间对齐。

Result: 在HOI-M3数据集（当前唯一支持多人体-多物体动态交互评估的数据集）上，该方法在表面重建质量上超越了现有最先进方法，且能保持多视角与时间一致性。

Conclusion: 所提混合方法有效缓解了单目多动态主体场景中因遮挡和运动复杂性导致的结构退化问题，为AR/VR、具身AI等应用提供了更鲁棒、高保真的动态3D几何建模方案。

Abstract: Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.

</details>


### [70] [NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives](https://arxiv.org/abs/2512.00557)
*Haomiao Chen,Keith W Jamison,Mert R. Sabuncu,Amy Kuceyeski*

Main category: cs.CV

TL;DR: 本文提出了NeuroVolve框架，利用预训练视觉-语言模型的嵌入空间，通过优化神经目标函数实现脑引导图像合成，支持单区域或多区域激活/去激活约束，揭示了脑区间的协同与拮抗关系，并支持个性化、可解释的神经表征研究。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型仅能复现孤立脑区的类别选择性（如FFA对人脸的选择性），难以揭示脑区在自然视觉过程中的交互机制。

Method: 提出NeuroVolve：基于预训练视觉-语言模型（如CLIP）的嵌入空间，定义可编程神经目标函数（如指定fMRI激活模式），通过梯度优化生成满足该目标的图像；支持单ROI、多ROI联合激活/抑制、共激活/去相关等约束，并追踪优化轨迹以分析语义演化路径。

Result: 成功复现已知脑区选择性（如枕面区对人脸响应）；生成符合复杂多区域约束的连贯场景；揭示低级与高级特征特异性刺激；发现脑区间合作与对抗调谐关系；支持被试个体化偏好建模。

Conclusion: NeuroVolve将脑引导图像编辑与偏好刺激生成统一为单一可微优化过程，为解析分布式视觉表征、构建个性化脑-机接口及可解释神经映射提供了新范式。

Abstract: What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.

</details>


### [71] [Describe Anything Anywhere At Any Moment](https://arxiv.org/abs/2512.00565)
*Nicolas Gorlo,Lukas Schmid,Luca Carlone*

Main category: cs.CV

TL;DR: DAAAM是一个用于大规模实时4D场景理解的新型时空记忆框架，通过优化前端和分层4D场景图实现几何与语义兼顾的实时语言-空间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成丰富开放词汇描述与实时3D定位之间存在权衡，难以兼顾准确性、语义丰富性和实时性。

Method: 提出DAAAM框架：1）基于优化的前端，利用局部化字幕模型（如DAM）并结合批量处理加速推理；2）构建层次化4D场景图（SG）作为全局时空一致的记忆表示。

Result: 在NaVQA和SG3D基准上达到SOTA：OC-NaVQA问题准确率提升53.6%，位置/时间误差分别降低21.9%/21.6%，SG3D任务定位准确率提升27.8%。

Conclusion: DAAAM成功实现了高精度、细粒度、几何对齐的语义描述与实时性能的统一，并支持工具调用型智能体进行推理，代码与数据已开源。

Abstract: Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.
  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.

</details>


### [72] [Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models](https://arxiv.org/abs/2512.00572)
*Mohammed Mohiuddin,Syed Mohammod Minhaz Hossain,Sumaiya Khanam,Prionkar Barua,Aparup Barua,MD Tamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架表示的瑜伽姿势自动分类方法，构建了Yoga-16数据集，并系统比较了VGG16、ResNet50和Xception在三种输入模态（原始图像、MediaPipe骨架图、YOLOv8骨架图）下的性能，发现MediaPipe骨架+VGG16效果最佳（96.09%），并结合Grad-CAM进行可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 现有瑜伽姿态识别研究多依赖原始图像或单一关键点模型，缺乏系统基准评测；同时，错误姿势易致伤，亟需可靠自动分类方法降低对专家依赖。

Method: 构建新数据集Yoga-16；对比VGG16、ResNet50、Xception三种网络在原始图像、MediaPipe骨架图、YOLOv8骨架图三种输入下的分类性能；采用Grad-CAM进行可视化可解释性分析，并结合交叉验证评估稳定性。

Result: 骨架表示显著优于原始图像输入；VGG16配合MediaPipe骨架输入达到最高准确率96.09%；Grad-CAM揭示模型关注关键关节区域，验证方法合理性。

Conclusion: 骨架驱动的深度学习方法（尤其MediaPipe+VGG16）是瑜伽姿态识别的有效范式；Yoga-16数据集与系统评测框架为后续研究提供了可靠基准。

Abstract: Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.

</details>


### [73] [SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension](https://arxiv.org/abs/2512.00582)
*Yue Jiang,Haiwei Xue,Minghao Han,Mingcheng Li,Xiaolu Hou,Dingkang Yang,Lihua Zhang,Xu Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的框架SatireDecoder，用于提升视觉讽刺图像的理解能力，通过多智能体系统和基于不确定性分析的链式推理策略，有效分解局部与全局语义并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在理解纯视觉讽刺方面存在困难，难以整合局部实体关系与全局上下文，导致误解、偏见和幻觉。

Method: 提出SatireDecoder，采用多智能体系统进行视觉级联解耦，将图像分解为细粒度的局部和全局语义表示，并结合基于不确定性分析的思维链推理策略，分步完成讽刺理解任务。

Result: 实验结果表明，SatireDecoder在讽刺图像理解任务上优于现有基线方法，显著提升了解释准确性并减少了幻觉现象。

Conclusion: SatireDecoder为复杂语义下的视觉-语言推理提供了有效且有前景的解决方案，尤其适用于需要高阶语义理解的任务。

Abstract: Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.

</details>


### [74] [Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models](https://arxiv.org/abs/2512.00597)
*Thuraya Alzubaidi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出了MedCT-VLM，一种参数高效的医学CT视觉-语言模型，通过LoRA方法适配CT-CLIP模型，在零样本多标签病理分类任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言基础模型在医学体数据（如CT）上的应用有限，尤其缺乏适用于零样本、少标注场景的高效适配方法。

Method: 基于CT-CLIP（在25692例胸部CT上预训练的对比式VLM），采用低秩自适应（LoRA）在视觉与文本编码器的注意力层中插入可训练低秩矩阵，仅训练1.67M参数（占总数0.38%）。

Result: 在18种胸部病理的零样本分类任务中，平均AUROC提升7.6个百分点（61.3%→68.9%），准确率提升6.4个百分点（67.2%→73.6%），macro-F1提升4.8个百分点（32.1%→36.9%）。

Conclusion: 参数高效微调（如LoRA）可有效将大规模CT视觉-语言预训练模型迁移到临床下游任务，尤其适合标注稀缺的零样本医学影像分析场景。

Abstract: Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\% to 68.9\% (+7.6 pp), accuracy from 67.2\% to 73.6\% (+6.4 pp), and macro-F1 from 32.1\% to 36.9\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.

</details>


### [75] [Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning](https://arxiv.org/abs/2512.00625)
*Tzu-I Liao,Mahmoud Fakhry,Jibin Yesudas Varghese*

Main category: cs.CV

TL;DR: This paper evaluates five deep learning models (YOLOv9, U-Net, Swin Transformer, DeepLabV3, Mask R-CNN) for automated pith detection in tree cross-sections, finding Swin Transformer most accurate (0.94), and improving Mask R-CNN’s IoU from 0.45 to 0.80 with NMS; generalizability is tested on an external oak dataset.


<details>
  <summary>Details</summary>
Motivation: Pith detection in tree cross-sections is currently manual and error-prone, hindering forestry and wood quality analysis; automation via deep learning is needed.

Method: Five deep learning models were evaluated on a dataset of 582 labeled tree cross-section images, dynamically augmented; performance was assessed via accuracy and IoU; Non-Maximum Suppression (NMS) was applied to Mask R-CNN; generalizability was tested on an external oak dataset (11 images) and a small additional dataset (64 images) to retrain the worst-performing model.

Result: Swin Transformer achieved highest accuracy (0.94) for fine segmentation; YOLOv9 excelled in bounding box detection but lacked boundary precision; U-Net handled structured patterns well; DeepLabV3 captured multi-scale features with slight boundary imprecision; Mask R-CNN’s IoU improved from 0.45 to 0.80 after NMS; challenges like tensor mismatches and boundary inconsistencies were mitigated via hyperparameter tuning and augmentation.

Conclusion: Deep learning shows strong potential for automating pith detection in tree cross-sections; optimal model selection depends on dataset characteristics and application requirements (e.g., segmentation vs. detection, boundary precision).

Abstract: Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.

</details>


### [76] [XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance](https://arxiv.org/abs/2512.00626)
*Kim Gerard A. Villanueva,Priyanka Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种结合DCGAN数据增强、微调ResNet-50分类器与LIME/SHAP可解释性技术的可信皮肤病变CAD系统，在HAM10000数据集上实现92.50%准确率和98.82% Macro-AUC，兼顾高性能与临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决皮肤病变多类别诊断中主观性强、数据不平衡（如HAM10000）及深度学习模型“黑箱”问题。

Method: 采用DCGAN进行各类别数据增强以缓解类别不平衡；微调ResNet-50进行七类皮肤疾病分类；集成LIME和SHAP可解释AI技术验证预测依据是否为临床相关特征（如不规则形态）。

Result: 系统整体准确率达92.50%，Macro-AUC达98.82%，优于多个基准模型；Melanoma NOS类别F1-Score为0.8602。

Conclusion: 构建了高性能且临床可解释的可信CAD框架，验证了可解释AI在皮肤诊断安全落地中的关键作用；未来需提升关键病种（如黑色素瘤）的判别能力。

Abstract: Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the "black box" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).

</details>


### [77] [Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation](https://arxiv.org/abs/2512.00639)
*Mahmoud El Hussieni*

Main category: cs.CV

TL;DR: 本研究探讨了基于YOLOv5算法在超声图像上进行甲状腺结节实例分割的有效性，发现包含多普勒图像可显著提升分割性能，其中YOLOv5-Large表现最佳。


<details>
  <summary>Details</summary>
Motivation: 甲状腺癌发病率上升，亟需高效的计算机辅助检测方法，而准确的结节分割是构建AI临床决策支持系统的关键前提。

Method: 采用多种YOLOv5变体（Nano至XLarge）在包含与不包含多普勒图像的两种数据集版本上进行甲状腺结节实例分割，并比较其性能。

Result: YOLOv5-Large在包含多普勒图像的数据集上表现最优，Dice分数达91%，mAP为0.87；加入多普勒图像后所有模型性能均提升，表明其对分割效果有积极影响。

Conclusion: YOLOv5可用于实现实时、准确的甲状腺结节实例分割，结合多普勒图像能进一步提高性能，具有应用于自动化诊断系统的潜力。

Abstract: The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.

</details>


### [78] [Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition](https://arxiv.org/abs/2512.00641)
*Razieh Ghaedi,AmirReza BabaAhmadi,Reyer Zwiggelaar,Xinqi Fan,Nashid Alam*

Main category: cs.CV

TL;DR: 本文提出GAT-ADA框架，结合图注意力网络与对抗域对齐方法，显著提升跨域人脸表情识别性能。


<details>
  <summary>Details</summary>
Motivation: 跨域人脸表情识别（CD-FER）因训练与部署数据间存在严重域偏移而困难。

Method: 提出GAT-ADA：以ResNet-50为骨干，引入批级图注意力网络（GAT）建模样本间关系；将每mini-batch构建为稀疏环图，利用注意力聚合跨样本线索；结合梯度反转层（GRL）的对抗学习与CORAL、MMD统计对齐进行域分布对齐。

Result: 在无监督域自适应标准协议下（源域RAF-DB，目标域CK+、JAFFE、SFEW 2.0、FER2013、ExpW），平均跨域准确率达74.39%；RAF-DB→FER2013达98.0%，较相同设置下的最优基线提升约36个百分点。

Conclusion: GAT-ADA通过联合建模样本关系与多策略域对齐，有效缓解域偏移，显著提升CD-FER性能。

Abstract: Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.

</details>


### [79] [MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba](https://arxiv.org/abs/2512.00647)
*Shanhui Liu,Rui Xu,Yunke Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Coarse-to-Fine Vision Mamba (CF-ViM)的自适应框架，通过动态调整图像分辨率分配来提升视觉Mamba模型的推理效率，在保持精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有token压缩或剪枝方法普遍存在信息丢失问题，且对所有图像统一处理，忽视了不同图像复杂度的差异，导致计算资源分配不合理。

Method: CF-ViM首先将图像划分为大块进行粗粒度推理以减少token数量；当模型预测置信度较低时，对选定区域进行细粒度重处理，以恢复关键视觉细节。

Result: 在ImageNet上的实验表明，CF-ViM在准确率和效率方面均优于基础Vision Mamba及当前最先进的token压缩技术。

Conclusion: CF-ViM通过根据图像复杂度自适应分配计算资源，实现了高效且精确的视觉推理，为Vision Mamba的高效化提供了新思路。

Abstract: Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.

</details>


### [80] [Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges](https://arxiv.org/abs/2512.00676)
*Kiri L. Wagstaff*

Main category: cs.CV

TL;DR: 本文提出了多数字书写者（MDW）数据集，用于更真实地评估数字识别算法在实际场景中的性能，发现孤立数字分类器在多数字识别任务中表现不佳，强调了需要新的方法来提升多数字识别性能。


<details>
  <summary>Details</summary>
Motivation: 孤立数字分类虽然在机器学习研究中长期作为基准问题，但在现实场景中数字常以多位数形式出现且由同一人书写（如邮政编码、支票金额、预约时间等），因此需要更贴近实际的评估基准。

Method: 利用NIST数字图像中书写者的已知信息构建多数字书写者（MDW）基准数据集，并引入面向任务的性能度量指标。

Result: 实验表明，孤立数字分类器在多数字识别任务中性能显著下降；MDW基准提供了更贴近实际影响的评估方式，并为开发融合任务知识的方法提供了新机会。

Conclusion: 解决真实数字识别问题需超越孤立数字分类，发展能利用书写一致性与任务知识的新方法，MDW数据集为此提供了重要测试平台。

Abstract: Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.

</details>


### [81] [Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer](https://arxiv.org/abs/2512.00677)
*Dong In Lee,Hyungjun Doh,Seunggeun Chi,Runlin Duan,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 本文提出了Dynamic-eDiTor，一种无需训练的文本驱动4D场景编辑框架，结合MM-DiT与4D高斯泼溅，通过时空子网格注意力（STGA）和上下文令牌传播（CTP）实现多视角与时间一致的高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动4D编辑方法依赖2D扩散模型逐帧编辑，导致运动失真、几何漂移和编辑不完整，难以兼顾多视角与时间一致性。

Method: 提出Dynamic-eDiTor框架：1）基于MM-DiT和4DGS；2）引入Spatio-Temporal Sub-Grid Attention（STGA）实现局部跨视角与时间融合；3）设计Context Token Propagation（CTP），通过令牌继承与光流引导的令牌替换实现全局一致性优化。

Result: 在DyNeRF数据集上实验表明，Dynamic-eDiTor在编辑保真度、多视角一致性和时间一致性方面均优于先前方法，且无需额外训练，直接优化预训练4DGS。

Conclusion: Dynamic-eDiTor为文本驱动4D编辑提供了高效、一致、免训练的新范式，显著提升了动态场景编辑的质量与鲁棒性。

Abstract: Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/

</details>


### [82] [Silhouette-based Gait Foundation Model](https://arxiv.org/abs/2512.00691)
*Dingqiang Ye,Chao Fan,Kartik Narayan,Bingzhe Wu,Chengwen Luo,Jianqiang Li,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了FoundationGait，首个可扩展的自监督预训练步态理解框架，通过在超过200万条行走序列上预训练大规模模型，实现了跨数据集、任务和输入模态的优异泛化性能，在零样本和微调设置下均显著提升步态识别等多任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有步态模型受限于规模小、泛化能力差，难以遵循扩展规律且无法统一处理多样化的步态任务，因此需要构建一个可扩展且通用的步态基础模型。

Method: 提出FoundationGait框架，采用自监督学习方式，在12个公开步态数据集上进行大规模预训练，最大版本包含约1.3亿参数，并支持多种任务和输入模态的迁移与零样本应用。

Result: 在多个步态数据集和任务（如身份识别、脊柱侧弯筛查、抑郁预测等）中表现出强健的性能，无需微调即在Gait3D数据集上达到48.0%的零样本rank-1准确率，在OU-MVLP上达到64.5%，创下新纪录。

Conclusion: FoundationGait验证了步态模型可遵循扩展规律，并首次证明单一模型能广泛适用于多样化步态分析任务，为未来步态智能系统提供了坚实的基础模型范式。

Abstract: Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.

</details>


### [83] [Affordance-First Decomposition for Continual Learning in Video-Language Understanding](https://arxiv.org/abs/2512.00694)
*Mengzhu Xu,Hanzhi Liu,Ningkang Peng,Qianyu Chen,Canran Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为“Affordance-First Decomposition (AFD)”的新方法，用于视频-语言理解中的持续学习，通过将视频映射到缓慢变化的affordance token上构建稳定的基础，并使用轻量级调度器实现按需适应，在不回放完整视频的情况下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法在处理非平稳数据、领域和查询风格时难以平衡稳定性与可塑性，且常依赖静态结构或需要回放历史视频，存在记忆和隐私限制问题。因此，需要一种能明确区分稳定部分与适应部分的方法。

Method: 提出AFD方法：将视频编码为时间对齐、缓慢变化的affordance tokens作为共享基础（substrate），并通过一个轻量级、基于查询路由且冲突感知的调度器来动态调整模型；使用弱对齐和教师一致性稳定基础层，训练时仅回放问题文本（question-only replay）。

Result: AFD在多种持续学习设定下达到最先进水平：域增量VideoQA上平均准确率为51.6%，遗忘率-1.8%；ViLCo指标中MQ为29.6%（R@1@0.5），NLQ为20.7%，VQ的stAP@0.25为18.4%；时间增量iVQA上准确率达39.5%，遗忘率-1.6%。

Conclusion: AFD通过显式分离稳定的交互中心基础层与目标导向的适应机制，提供了一种高效、可解释的视频-语言持续学习框架，在低遗忘和有限回放条件下表现优异。

Abstract: Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.

</details>


### [84] [CAR-Net: A Cascade Refinement Network for Rotational Motion Deblurring under Angle Information Uncertainty](https://arxiv.org/abs/2512.00700)
*Ka Chung Lai,Ahmet Cetinkaya*

Main category: cs.CV

TL;DR: 本文提出了一种名为CAR-net的级联细化网络，用于处理旋转运动模糊图像的去模糊任务，尤其适用于仅提供模糊角度噪声信息的半盲场景。该方法通过频域反演获得初始估计，并通过多阶段残差校正逐步提升图像质量；同时支持可选的角度检测模块以应对参数不确定性。实验验证了其在合成与真实图像上的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对旋转运动模糊图像去模糊问题，尤其是在仅有模糊角度噪声信息的半盲场景下缺乏有效方法的问题。

Method: 提出CAR-net（CAscade Refinement Network），采用频域反演初始化 + 多级残差细化架构；引入可端到端训练的角度检测模块以应对角度不确定性。

Result: 在合成和真实图像上均验证了方法的有效性，能有效抑制伪影并恢复细节。

Conclusion: CAR-net是一种专为旋转运动模糊设计的高效半盲去模糊方法，具有良好的鲁棒性和实用性。

Abstract: We propose a new neural network architecture called CAR-net (CAscade Refinement Network) to deblur images that are subject to rotational motion blur. Our architecture is specifically designed for the semi-blind scenarios where only noisy information of the rotational motion blur angle is available. The core of our approach is progressive refinement process that starts with an initial deblurred estimate obtained from frequency-domain inversion; A series of refinement stages take the current deblurred image to predict and apply residual correction to the current estimate, progressively suppressing artifacts and restoring fine details. To handle parameter uncertainty, our architecture accommodates an optional angle detection module which can be trained end-to-end with refinement modules. We provide a detailed description of our architecture and illustrate its efficiency through experiments using both synthetic and real-life images. Our code and model as well as the links to the datasets are available at https://github.com/tony123105/CAR-Net

</details>


### [85] [Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation](https://arxiv.org/abs/2512.00706)
*Chengzhi Yu,Yifan Xu,Yifan Chen,Wenyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态样本重加权的迭代直接偏好优化（DPO）方法，结合二元幻觉分类器进行高质量on-policy数据标注，显著降低大视觉语言模型（LVLMs）的幻觉率，在多个基准上超越SOTA方法，甚至使开源LLaVA-13B超越GPT-4V。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM幻觉缓解方法在偏好标注中引入额外幻觉，且多使用低效的off-policy数据；需高效、可靠的on-policy数据标注与更优的对齐策略。

Method: 提出二元幻觉分类器以生成干净的on-policy偏好数据，并设计带动态样本重加权机制的迭代DPO算法。

Result: 在MMHalBench上将LLaVA-1.5-7B幻觉率降低50.8%，在Object HalBench上平均幻觉率降低79.5%；LLaVA-1.5-13B性能超越GPT-4V。

Conclusion: on-policy数据配合高质量标注与鲁棒DPO优化，是有效缓解LVLM幻觉的关键路径，开源模型潜力可被充分释放。

Abstract: Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.

</details>


### [86] [Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks](https://arxiv.org/abs/2512.00714)
*Emmanuella Avwerosuoghene Oghenekaro*

Main category: cs.CV

TL;DR: 本文综述了深度学习在早期癌症检测中的应用，特别是通过多模态医学影像与放射基因组学的结合，实现非侵入性肿瘤特征预测和个性化治疗。


<details>
  <summary>Details</summary>
Motivation: 早期癌症检测对提高生存率至关重要，但传统诊断方法存在延迟问题，且难以发现微小病变。

Method: 采用卷积神经网络、Transformer及混合注意力架构等深度学习模型，分析MRI、CT、PET、乳腺X线、病理学和超声等多模态影像数据，并结合放射基因组学整合基因组、转录组和表观遗传标志物。

Result: 深度学习模型能够识别肉眼不可见的细微组织异常和肿瘤微环境变化，可预测肿瘤基因型、分子亚型、免疫反应及治疗耐药性。

Conclusion: 深度学习与放射基因组学的融合为非侵入性癌症早期检测和精准医疗提供了新范式，具有显著临床转化潜力。

Abstract: Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.

</details>


### [87] [RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images](https://arxiv.org/abs/2512.00718)
*Deliang Wang,Peng Liu*

Main category: cs.CV

TL;DR: 本文提出了RS-ISRefiner，一种面向遥感图像的点击式交互图像分割框架，通过适配器调优、混合注意力机制和改进的概率图调制策略，提升了在遥感图像上的分割精度、效率与交互稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有面向自然图像的交互图像分割方法难以泛化到遥感图像领域，主要受限于标注数据稀缺和计算开销大。

Method: 提出RS-ISRefiner框架：采用适配器微调视觉基础模型；设计融合卷积局部建模与Transformer全局推理的混合注意力机制；引入改进的概率图调制方案以更好利用历史用户交互。

Result: 在iSAID、ISPRS Potsdam等六个遥感数据集上实验表明，该方法在分割精度、效率和交互成本方面均优于当前最优方法。

Conclusion: RS-ISRefiner有效提升了遥感图像交互分割的性能与泛化能力，适用于高质量实例分割的实际遥感应用。

Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.

</details>


### [88] [TrajDiff: End-to-end Autonomous Driving without Perception Annotation](https://arxiv.org/abs/2512.00723)
*Xingtai Gui,Jianbo Zhao,Wencheng Han,Jikai Wang,Jiahao Gong,Feiyang Tan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出TrajDiff，一种面向轨迹的鸟瞰图（BEV）扩散框架，实现无需感知标注的端到端自动驾驶，仅需原始传感器输入和未来轨迹即可生成多样化且合理的行驶轨迹，在NAVSIM基准上达到最先进的无标注方法性能。


<details>
  <summary>Details</summary>
Motivation: 由于手动感知标注成本高昂，开发无需感知标注的规划范式变得愈发重要。现有方法依赖辅助感知任务，限制了完全端到端的学习潜力，因此需要一种完全摆脱感知标注的生成式驾驶框架。

Method: 提出TrajDiff框架：设计轨迹导向的BEV编码器生成无监督的TrajBEV特征，并构建高斯BEV热图作为隐含驾驶模态的目标；引入TB-DiT模型，结合自车状态与TrajBEV特征，通过扩散机制直接生成多样化的合理轨迹，无需手工设计运动先验。

Result: 在NAVSIM基准上，TrajDiff达到87.5 PDMS，为当前最优的无标注方法；通过数据扩展进一步提升至88.5 PDMS，性能媲美先进的基于感知的方法。

Conclusion: TrajDiff成功实现了完全无需感知标注的端到端自动驾驶生成框架，验证了纯数据驱动路径在自动驾驶中的可行性，并展示了数据扩展对性能的积极影响。

Abstract: End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.

</details>


### [89] [Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards](https://arxiv.org/abs/2512.00743)
*Qiang Lyu,Zicong Chen,Chongxiao Wang,Haolin Shi,Shibo Gao,Ran Piao,Youwei Zeng,Jianlou Si,Fei Ding,Jing Li,Chun Pong Lau,Weiqiang Wang*

Main category: cs.CV

TL;DR: 本文提出Multi-GRPO，通过树状轨迹分组和奖励函数独立分组，改进GRPO在文本到图像生成中对齐多目标时的信用分配与奖励混合问题，提升稳定性与对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在共享信用分配不准确（尤其早期去噪步）和多目标奖励混合导致梯度不稳定、更新冲突两大问题。

Method: 提出Multi-GRPO框架：1）树状轨迹（受MCTS启发）构建时间分组，实现早期步骤的精准优势估计；2）按奖励函数独立分组，分别计算优势后再聚合，解耦冲突信号；并构建OCR-Color-10数据集用于多目标对齐评估。

Result: 在PickScore-25k（单奖励）和OCR-Color-10（多目标）基准上，Multi-GRPO展现出更优的稳定性和对齐性能，能有效平衡冲突目标。

Conclusion: Multi-GRPO通过双正交分组机制有效缓解了GRPO在T2I对齐中的关键缺陷，为多目标强化学习对齐提供了新范式。

Abstract: Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \emph{reward-based grouping} to compute advantages for each reward function \textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \textit{PickScore-25k} and multi-objective \textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.

</details>


### [90] [SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead](https://arxiv.org/abs/2512.00903)
*Chaojun Ni,Cheng Chen,Xiaofeng Wang,Zheng Zhu,Wenzhao Zheng,Boyuan Wang,Tianrun Chen,Guosheng Zhao,Haoyun Li,Zhehao Dong,Qiang Zhang,Yun Ye,Yang Wang,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: 本文提出了SwiftVLA，一种高效的轻量级视觉-语言-动作（VLA）模型，通过引入4D视觉几何变换器和Fusion Tokens增强时空理解能力，并采用掩码重建策略在推理时去除4D分支，实现在边缘设备上高效部署。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型依赖大规模视觉语言模型，导致参数量大、难以在实际中应用；轻量模型虽提升效率但牺牲了时空推理能力，且现有方法多依赖额外3D输入和大型模型，缺乏有效的时间理解。因此需要一种兼顾效率与4D理解能力的紧凑VLA架构。

Method: 提出SwiftVLA：1）使用预训练的4D视觉几何变换器结合时间缓存机制，从2D图像中提取4D特征；2）设计Fusion Tokens作为可学习令牌，通过未来预测目标融合2D图像与4D特征，生成统一的动作表示；3）采用掩码与重建策略，在训练时掩码4D输入并让模型重建，使VLM学习到有效的4D表示，从而在推理阶段移除4D分支。

Result: 实验表明，SwiftVLA在真实和模拟环境中优于轻量级基线模型，性能媲美高达7倍参数量的VLA模型，在边缘设备上实现相当性能的同时速度快18倍，内存占用减少12倍。

Conclusion: SwiftVLA通过创新的4D特征提取、Fusion Tokens融合机制和掩码重建训练策略，成功在紧凑模型中实现了强大的4D理解能力，显著提升了轻量级VLA模型的实用性与部署效率。

Abstract: Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.

</details>


### [91] [Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression](https://arxiv.org/abs/2512.00744)
*Zhengxin Chen,Xiaohai He,Tingrong Zhang,Shuhua Xiong,Chao Ren*

Main category: cs.CV

TL;DR: 本文提出了一种新型的联合多尺度门控Transformer与先验引导卷积网络（MGTPCN）用于学习型图像压缩，通过改进卷积和Swin-T模块分别增强局部与非局部特征提取能力，在性能与复杂度间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 提升学习型图像压缩中神经网络对局部和非局部特征的表达能力，以超越传统编解码器（如VVC）并改善性能-复杂度权衡。

Method: 提出先验引导卷积（PGConv），融合不对称卷积（AConvs）和差分卷积（DConvs），并采用重参数化降低计算开销；提出多尺度门控Transformer（MGT），结合不同空洞率的窗口自注意力与多尺寸深度卷积，并引入门机制增强非线性；最终构建MGTPCN端到端压缩框架。

Result: MGTPCN在多个基准上超越当前最优方法，实现更优的率失真性能与更低的计算复杂度。

Conclusion: 通过协同优化局部与非局部建模能力，PGConv与MGT的有效结合显著提升了学习型图像压缩性能，验证了所提架构设计的合理性与先进性。

Abstract: Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.

</details>


### [92] [Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization](https://arxiv.org/abs/2512.00748)
*Ke Liu,Shangde Gao,Yichao Fu,Shangqi Gao,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出ProSeg模型，通过概率建模实现多标注者医学图像分割中的多样性与个性化统一，利用两个隐变量分别刻画专家偏好和边界模糊性，并通过变分推断生成多样化且专家特异的分割结果，在NPC和LIDC-IDRI数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割受数据不确定性影响（如病灶边界模糊、不同医生标注差异），现有方法难以同时兼顾分割结果的多样性与专家个性化。

Method: 提出ProSeg模型，引入两个隐变量分别建模专家标注偏好和图像边界模糊性，采用变分推断估计其条件概率分布，通过采样生成兼具多样性与个性化特征的分割结果。

Result: 在鼻咽癌（NPC）和肺结节（LIDC-IDRI）数据集上取得新SOTA性能，分割结果既多样又具备专家特异性。

Conclusion: ProSeg成功统一了多标注者分割中的多样性与个性化需求，为处理医学标注不确定性提供了可解释、灵活的概率建模范式。

Abstract: Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.

</details>


### [93] [MM-ACT: Learn from Multimodal Parallel Generation to Act](https://arxiv.org/abs/2512.00975)
*Haotian Liang,Xinyi Chen,Bin Wang,Mingkang Chen,Yitian Liu,Yuhao Zhang,Zanxin Chen,Tianshuo Yang,Yilun Chen,Jiangmiao Pang,Dong Liu,Xiaokang Yang,Yao Mu,Wenqi Shao,Ping Luo*

Main category: cs.CV

TL;DR: MM-ACT 是一个统一的视觉-语言-动作（VLA）模型，通过共享上下文的多模态学习，在文本、图像和动作生成中实现跨模态协同，显著提升机器人任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 为了使通用机器人策略同时具备语义理解与环境交互能力，需要将文本、图像和动作统一建模，克服传统方法在跨模态协同和生成效率上的不足。

Method: 提出 MM-ACT 模型，采用重掩码并行解码策略进行文本和图像生成，使用单步并行解码生成动作，并引入上下文共享的多模态学习范式，在共享上下文中监督三模态生成。

Result: 在 LIBERO 模拟环境上达到 96.3% 的成功率，在真实 Franka 机器人三个任务上达到 72.0%，在 RoboTwin2.0 八个双手机器人任务上达到 52.38%，跨模态学习带来额外 9.25% 的性能提升。

Conclusion: MM-ACT 通过统一的多模态建模范式有效提升了机器人策略的泛化能力和执行效率，验证了跨模态学习对视觉-语言-动作集成的重要性。

Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.

</details>


### [94] [Charts Are Not Images: On the Challenges of Scientific Chart Editing](https://arxiv.org/abs/2512.00752)
*Shawn Li,Ryan Rossi,Sungchul Kim,Sunav Choudhary,Franck Dernoncourt,Puneet Mathur,Zhengzhong Tu,Yue Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个名为FigEdit的大规模科学图表编辑基准，包含超过3万样本，涵盖10种图表类型和多种复杂编辑任务，揭示了现有生成模型在结构化图表编辑中的局限性，并强调需要能够理解图表语义与视觉结构的新型模型。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成模型（如扩散模型和自回归模型）在自然图像编辑中表现良好，但将它们直接应用于科学图表存在根本问题：图表是基于图形语法的结构化数据可视化，而非简单的像素排列。因此，图表编辑应被视为结构化变换问题，而非像素操作任务。

Method: 提出FigEdit，一个包含30,000多个真实世界样本的大规模基准，覆盖10种图表类型和五类递进式挑战性编辑任务：单次编辑、多次编辑、对话式编辑、基于视觉引导的编辑和风格迁移。评估了多种SOTA模型在该基准上的表现，并分析传统图像指标（如SSIM、PSNR）在语义正确性评估中的不足。

Result: 实验表明当前最先进的模型在FigEdit上表现不佳，难以完成需理解底层结构的编辑任务；同时发现SSIM、PSNR等传统指标无法有效反映编辑的语义准确性。

Conclusion: 科学图表编辑需要超越像素级操作的结构感知模型。FigEdit为开发和评估此类模型提供了坚实基础，作者已公开该基准以推动对兼具视觉与语义理解能力的图表编辑模型的研究。

Abstract: Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.

</details>


### [95] [Generalized Medical Phrase Grounding](https://arxiv.org/abs/2512.01085)
*Wenjun Zhang,Shekhar S. Chandra,Aaron Nicolson*

Main category: cs.CV

TL;DR: 本文提出了一种广义医学短语定位（GMPG）任务，以解决传统医学短语定位（MPG）在处理多区域、非诊断性及不可定位短语时的局限性，并设计了首个GMPG模型MedGrounder，在多个数据集上展现出优异的零样本迁移能力和更少标注依赖。


<details>
  <summary>Details</summary>
Motivation: 现有MPG系统基于指代表达理解（REC）范式，仅返回每个短语一个边界框，但真实放射报告常含多区域发现、非诊断文本和不可定位短语（如否定描述或正常解剖描述），因此需重新定义任务。

Method: 提出广义医学短语定位（GMPG）任务，允许每句映射零个、一个或多个带分值的图像区域；构建首个GMPG模型MedGrounder，采用两阶段训练：先在报告句子-解剖部位框对齐数据集上预训练，再在报告句子-人工标注框数据集上微调。

Result: 在PadChest-GR和MS-CXR数据集上，MedGrounder在多区域和非可定位短语上显著优于REC式及接地报告生成基线，且仅需更少人工标注框；并能与现有报告生成器组合生成接地报告而无需重训生成器。

Conclusion: GMPG更贴合临床实际需求，MedGrounder为实现灵活、鲁棒、低标注依赖的医学图像-文本对齐提供了新范式。

Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.

</details>


### [96] [Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation](https://arxiv.org/abs/2512.01165)
*Mohamed Abdallah Salem,Ahmed Harb Rabia*

Main category: cs.CV

TL;DR: 本文提出了一种基于边缘设备部署YOLO模型的实时图像标注方法，显著缩短农业等场景下数据集准备时间，同时保持高标注质量。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注耗时耗力，难以满足农业等需快速决策的实际应用需求。

Method: 在边缘设备上部署YOLOv5、YOLOv8、YOLOv12模型，实现图像采集时的实时标注，并对比单类/多类、预训练/从头训练等配置下的性能。

Result: 预训练和单类配置在模型收敛速度、精度和鲁棒性方面表现最优；实验证明该实时标注框架可大幅减少数据集准备时间且标注质量高。

Conclusion: 所提实时标注框架可行有效，为YOLO类模型在农业等实时性要求高的领域落地提供了新思路。

Abstract: Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.

</details>


### [97] [Seeing the Wind from a Falling Leaf](https://arxiv.org/abs/2512.00762)
*Zhiyuan Gao,Jiageng Mao,Hong-Xing Yu,Haozhe Lou,Emily Yue-Ting Jia,Jernej Barbic,Jiajun Wu,Yue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端可微的逆向图形学框架，用于从视频中反演不可见的物理力场（如风场），联合建模物体几何、物理属性与相互作用，并通过反向传播从运动中恢复力表示。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉长期关注视频中的运动建模，但驱动运动的底层物理相互作用（如力）仍缺乏探索；本文旨在从视觉观测中恢复这些不可见的物理力。

Method: 提出端到端可微的逆向图形学框架，联合优化物体几何、物理属性与外力，并利用视频帧间运动通过反向传播反演力场。

Result: 在合成与真实视频上验证了该方法能合理推断力场，并支持物理驱动的视频生成与编辑。

Conclusion: 该工作为连接视觉感知与物理建模提供了新路径，推动从像素理解背后物理过程的研究。

Abstract: A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \href{https://chaoren2357.github.io/seeingthewind/}{project page}.

</details>


### [98] [SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge](https://arxiv.org/abs/2512.01629)
*Yumeng He,Ying Jiang,Jiayin Lu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: SPARK是一个从单张RGB图像重建物理一致、运动学层级的3D可动物体（articulated objects）的框架，结合视觉语言模型、扩散Transformer与可微渲染优化，生成仿真就绪的URDF模型。


<details>
  <summary>Details</summary>
Motivation: 当前构建仿真可用的可动3D物体需专家手动建模部件层级和运动结构，费时费力；亟需一种自动化、图像驱动的方法。

Method: 1）利用视觉语言模型（VLM）提取粗略URDF参数并生成部件级参考图像；2）将部件图像引导与结构图融入生成式扩散Transformer，合成一致的部件及完整形状；3）通过可微前向运动学与可微渲染，在VLM生成的开态监督下联合优化关节类型、轴向与原点。

Result: 在多个类别上生成高质量、仿真就绪的可动3D资产，支持机器人操作与交互建模等下游任务。

Conclusion: SPARK实现了从单图到物理一致、运动学可编辑的3D可动模型的端到端重建，显著降低人工建模门槛，推动具身AI与仿真应用发展。

Abstract: Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.

</details>


### [99] [The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches](https://arxiv.org/abs/2512.00765)
*Haojie Jia,Te Hu,Haowen Li,Long Jin,Chongshi Xin,Yuchi Yao,Jiarui Xiao*

Main category: cs.CV

TL;DR: 本文提出TESP-Attack，一种面向交通标志分类的隐蔽性物理对抗补丁方法，通过边缘对齐掩码、U-Net生成及颜色/纹理与频域约束优化，实现高攻击成功率（>90%）与强视觉隐蔽性，并在多模型、多视角和多距离下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有物理对抗攻击在交通标志上缺乏隐蔽性，扰动集中在中心区域易被人类察觉，限制了实际应用；同时V2X网络中误识别可能引发级联故障，威胁交通安全与系统稳定。

Method: 提出TESP-Attack：利用实例分割生成边缘对齐掩码；采用U-Net生成对抗补丁；结合颜色、纹理约束与频域分析优化补丁，使其与背景无缝融合，提升视觉隐蔽性。

Result: 在多种架构的交通标志分类模型上攻击成功率超90%（查询预算受限下）；具备强跨模型迁移性；在不同角度与距离下实测鲁棒性稳定。

Conclusion: TESP-Attack有效解决了物理对抗攻击隐蔽性不足的问题，兼顾高攻击效能与视觉不可察觉性，提升了对抗攻击在真实交通场景中的可行性与威胁性。

Abstract: Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.

</details>


### [100] [Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching](https://arxiv.org/abs/2512.01850)
*Yue Pan,Tao Sun,Liyuan Zhu,Lucas Nunes,Iro Armeni,Jens Behley,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 本文提出了一种将点云配准视为条件生成的新方法，通过学习连续的逐点速度场将噪声点移动到注册场景中，从而直接生成配准后的点云。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于对应关系匹配和成对变换优化，难以处理低重叠、跨尺度和跨传感器模态的情况，因此需要一种更鲁棒且通用的配准方法。

Method: 使用一个学习得到的连续逐点速度场来传输点云，并结合轻量级局部特征提取器和测试时刚性约束实现配准。

Result: 在成对和多视图配准基准上取得了最先进的结果，尤其在低重叠情况下表现优异，并能泛化到不同尺度和传感器模态。

Conclusion: 该方法不仅提升了配准性能，还支持重定位、多机器人SLAM和多会话地图融合等下游任务。

Abstract: Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.

</details>


### [101] [EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes](https://arxiv.org/abs/2512.00771)
*Xiaoshan Wu,Yifei Yu,Xiaoyang Lyu,Yihua Huang,Bo Wang,Baoheng Zhang,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出EAG3R，一种结合事件流的3D几何估计框架，通过增强RGB与事件数据融合，在动态和低光场景中实现鲁棒的无姿态重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB视频的3D几何估计方法在动态物体和极端光照条件下性能受限，因传统相机存在固有缺陷，难以处理运动模糊和低照度问题。

Method: 基于MonST3R架构，引入两个创新：一是受视网膜启发的图像增强模块和SNR感知的轻量级事件适配器，自适应融合RGB与事件特征；二是基于事件的光度一致性损失，增强全局优化中的时空一致性。

Result: 实验表明，EAG3R在单目深度估计、相机位姿跟踪和动态重建任务上显著优于现有的RGB-only方法，尤其在低光和动态场景中表现突出，且无需夜间数据重训练。

Conclusion: EAG3R通过融合事件数据与RGB信息，有效提升了复杂真实场景下的3D几何估计鲁棒性与精度，为未来视频重建系统提供了新方向。

Abstract: Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.

</details>


### [102] [Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation](https://arxiv.org/abs/2512.01242)
*Zirui Zhao,Boye Niu,David Hsu,Wee Sun Lee*

Main category: cs.CV

TL;DR: 本文提出了一种结合几何约束推理与神经语义的约束引导生成框架，用于解决抽象视觉构图任务（如七巧板拼装），在有效性与语义保真度上优于扩散和自回归基线。


<details>
  <summary>Details</summary>
Motivation: 抽象视觉构图依赖空间结构与几何关系而非纹理细节，但其组合空间稀疏、数据有限、可行性离散，难以用纯像素级统计生成模型处理。

Method: 采用AlphaGo风格的蒙特卡洛树搜索（MCTS）确保几何可行性（如无重叠、合法朝向），以微调的视觉-语言模型提供语义对齐奖励；策略网络作为MCTS启发式，并通过搜索生成的方案进行策略微调；引入类GAN的对抗性奖励精炼机制，使生成样本逐渐逼近真实数据分布。

Result: 在七巧板组装任务中，该方法在有效性（validity）和语义保真度（semantic fidelity）上显著优于扩散模型和自回归模型，尤其在约束更严格时优势更明显。

Conclusion: 将显式几何推理与神经语义学习、强化搜索及对抗性奖励优化相结合，可有效应对抽象视觉构图中稀疏可行解空间与模糊目标描述的挑战。

Abstract: We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.

</details>


### [103] [GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment](https://arxiv.org/abs/2512.01952)
*Haoyang He,Jay Patrikar,Dong-Ki Kim,Max Smith,Daniel McGann,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei,Sebastian Scherer*

Main category: cs.CV

TL;DR: 本文提出了一种名为RLWG的自监督后训练框架，通过几何与感知奖励对预训练视频世界模型进行对齐，提升其在具身导航任务中的空间一致性与长时程稳定性；具体实现GrndCtrl方法基于GRPO算法，在无需人工标注的情况下显著优于监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型虽具高视觉保真度，但缺乏几何基础，难以支持需空间一致性和长时程稳定性的导航任务。

Method: 提出Reinforcement Learning with World Grounding（RLWG）框架，利用姿态循环一致性、深度重投影和时间连贯性等可验证的几何与感知奖励，结合Group Relative Policy Optimization（GRPO）实现奖励对齐的后训练方法GrndCtrl。

Result: GrndCtrl使世界模型在户外环境中展现出更稳定的轨迹、更一致的几何结构和更可靠的滚动预测，空间一致性与导航稳定性优于监督微调方法。

Conclusion: RLWG为视频世界模型提供了从生成式预训练到具身行为落地的有效对齐路径，验证了类比大语言模型对齐范式在具身智能中的可行性。

Abstract: Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.

</details>


### [104] [DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering](https://arxiv.org/abs/2512.00773)
*Toshiki Katsube,Taiga Fukuhara,Kenichiro Ando,Yusuke Mukuta,Kohei Uehara,Tatsuya Harada*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展、可复现的构建日语视觉-语言（V&L）数据集的流程，生成了大规模高质量的DEJIMA-Cap（图像-文本）和DEJIMA-VQA（视觉问答）数据集（各388万对），显著优于现有日语V&L资源；经人工与定量评估，其日语自然度、文化代表性及事实准确性俱佳，并有效提升模型在多个日语多模态基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决日语视觉-语言建模领域高质量、大规模资源稀缺的问题。

Method: 构建了一个融合大规模网络爬取、严格过滤/去重、目标检测驱动的证据提取、以及基于大语言模型（LLM）并在视觉定位约束下的文本精炼的可扩展、可复现流水线。

Result: 构建了两个各含388万图像-文本对的日语V&L数据集（DEJIMA-Cap 和 DEJIMA-VQA）；人工评估显示其日语地道性与自然度显著优于翻译或人工标注数据集，事实准确性媲美人工标注；图像特征分布分析表明其覆盖日本典型视觉领域；模型在日语多模态基准上性能持续提升。

Conclusion: 文化扎根、大规模的日语V&L资源对提升模型性能具有关键作用；本工作发布的数据集、元数据及商用许可的流水线模块，将推动学术研究与工业应用。

Abstract: This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.

</details>


### [105] [PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery](https://arxiv.org/abs/2512.00794)
*Bo Guo,Sijia Wen,Yifan Zhao,Jia Li,Zhiming Zheng*

Main category: cs.CV

TL;DR: 本文提出PolarGS，一种利用偏振信息增强3D高斯泼溅（3DGS）表面重建的方法，通过偏振引导的光度校正和偏振增强的高斯稠密化机制，在反射性和无纹理区域显著提升几何重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在反射性、无纹理等光度模糊区域性能下降，而偏振光可提供表面朝向信息，作为光度线索的光学补充以解决该问题。

Method: 提出PolarGS：1）基于线性偏振度（DoLP）识别反射区域并用颜色精修图校正反射高斯；2）将偏振角（AoLP）与DoLP融入PatchMatch深度补全，实现无纹理区的高斯反投影与融合。

Result: PolarGS在几何重建精度上优于当前最先进方法，且框架无关，有效提升了反射与无纹理区域的重建完整性。

Conclusion: 偏振信息可作为有效的光学先验，显著缓解3DGS在光度模糊场景下的几何估计歧义，为光学感知驱动的神经渲染提供了新范式。

Abstract: Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.

</details>


### [106] [Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion](https://arxiv.org/abs/2512.02017)
*Shaowei Liu,David Yifan Yao,Saurabh Gupta,Shenlong Wang*

Main category: cs.CV

TL;DR: VisualSync 是一种基于多视角动态的优化框架，能够以毫秒级精度对未标定、不同步的视频进行跨相机同步。


<details>
  <summary>Details</summary>
Motivation: 现有跨相机视频同步方法依赖受控环境、特定目标或昂贵硬件，难以适用于日常多相机录制场景。

Method: 利用现成的3D重建、特征匹配和密集跟踪技术提取轨迹片段、相对姿态和跨视角对应关系，并通过联合最小化极线误差来估计每个相机的时间偏移。

Result: 在四个多样化且具挑战性的数据集上实验表明，VisualSync 中位同步误差低于 50 毫秒，优于基线方法。

Conclusion: VisualSync 能有效实现无标记、无需手动干预的高精度跨相机视频同步，适用于真实世界复杂场景。

Abstract: Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.

</details>


### [107] [CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target](https://arxiv.org/abs/2512.00796)
*Jiajian He,Enjie Hu,Shiqi Chen,Tianchen Qiu,Huajun Feng,Zhihai Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 本文提出CircleFlow框架，通过流引导的边缘定位实现高保真点扩散函数（PSF）估计，利用圆网格靶标和二值亮度先验解耦图像与核估计，并结合子像素对齐与隐式神经表示联合优化，显著提升PSF校准精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 准确估计点扩散函数（PSF）对光学表征和计算视觉至关重要，但强度域去卷积固有的模糊性和病态性使其极具挑战性。

Method: CircleFlow采用结构化采集（圆网格靶标）编码局部各向异性和空间变化PSF，利用靶标二值亮度先验解耦图像与核估计；通过光流引导的子像素对齐重建潜在清晰图像，并将PSF建模为能量约束的隐式神经表示；在去马赛克感知的可微框架中联合优化二者。

Result: 在模拟与真实数据上的大量实验表明，CircleFlow在PSF估计精度和可靠性上达到当前最优水平。

Conclusion: CircleFlow是一种物理一致、鲁棒且高精度的PSF校准方法，适用于实际应用场景。

Abstract: The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.

</details>


### [108] [Data-Centric Visual Development for Self-Driving Labs](https://arxiv.org/abs/2512.02018)
*Anbang Liu,Guanzhong Hu,Jiayi Wang,Ping Guo,Han Liu*

Main category: cs.CV

TL;DR: 本文提出了一种融合真实与虚拟数据生成的混合流水线，用于解决自驱动实验室（SDL）中移液操作气泡检测任务的数据稀缺问题，尤其针对负样本获取困难；通过人机协同的真实数据采集和条件引导的图像生成与筛选，构建了类别平衡数据集，在真实测试集上达到99.6%准确率，验证了其在稀有事件检测中的有效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室（SDL）对模型鲁棒性要求高，但关键操作（如移液）所需的高精度气泡检测缺乏大量标注数据，尤其是难以获取的负样本，导致模型训练受限。

Method: 构建真实-虚拟双轨混合数据生成管道：真实轨采用人机协同方案（自动采集+选择性人工校验）；虚拟轨基于参考图像与提示词引导生成图像，并经筛选验证；最终融合构建类别平衡数据集用于训练气泡检测模型。

Result: 仅用自动采集的真实图像训练的模型在真实测试集上达99.6%准确率；混入生成数据后仍保持99.4%准确率，同时显著降低数据采集与审核负担。

Conclusion: 该方法为SDL提供了一种可扩展、低成本的视觉反馈数据供给策略，也为稀有事件检测及更广泛的视觉任务中应对数据稀缺问题提供了实用解决方案。

Abstract: Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.

</details>


### [109] [Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding](https://arxiv.org/abs/2512.00805)
*Pengfei Hu,Meng Cao,Yingyao Wang,Yi Wang,Jiahua Dong,Jun Song,Yu Cheng,Bo Zheng,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出SpecTemp框架，通过轻量级草稿模型与强大目标模型的协同设计，实现高效且准确的长视频理解，显著提升推理速度并保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于帧思考范式的视频多模态大语言模型在长视频理解中存在效率瓶颈，因多模态上下文冗余增长导致计算开销大。

Method: 提出基于强化学习的推测式时序推理框架SpecTemp，采用双模型协作机制：轻量级草稿MLLM快速筛选关键帧，强大目标MLLM专注时序推理与验证，并迭代优化；同时构建含粗粒度和细粒度标注的SpecTemp-80K数据集支持训练。

Result: 在多个视频理解基准上，SpecTemp在保持竞争性准确率的同时，显著加速推理过程，优于现有thinking-with-frames方法。

Conclusion: SpecTemp通过解耦感知与推理、模仿人脑协作机制，在效率与准确性间取得更好平衡，为长视频理解提供了新范式。

Abstract: Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.

</details>


### [110] [IRPO: Boosting Image Restoration via Post-training GRPO](https://arxiv.org/abs/2512.00814)
*Haoxuan Xu. Yi Liu,Boyuan Jiang,Jinlong Peng,Donghao Luo,Xiaobin Hu,Shuicheng Yan,Haoang Li*

Main category: cs.CV

TL;DR: 本文提出IRPO，一种基于GRPO的低层视觉后训练范式，通过优化数据选择策略和多维度奖励建模（结构保真、感知对齐、恢复质量），显著提升图像恢复任务在域内与域外场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复方法依赖像素级硬拟合，存在过平滑和泛化差问题；高层生成任务的后训练进展尚未有效迁移到低层视觉。

Method: 提出IRPO框架：1）基于预训练阶段表现筛选欠佳样本作为后训练数据；2）构建三层奖励系统——通用奖励（结构保真）、专家奖励（Qwen-VL驱动的感知对齐）、恢复奖励（任务特定低层质量）。

Result: 在6个域内和5个域外低层视觉基准上达到SOTA，较AdaIR基线分别提升0.83 dB（域内）和3.43 dB（域外）。

Conclusion: IRPO验证了GRPO类后训练范式在低层视觉任务中的有效性，为图像恢复提供了新范式，兼顾客观精度与人类感知偏好。

Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.

</details>


### [111] [PanFlow: Decoupled Motion Control for Panoramic Video Generation](https://arxiv.org/abs/2512.00832)
*Cheng Zhang,Hanwen Liang,Donny Y. Chen,Qianyi Wu,Konstantinos N. Plataniotis,Camilo Cruz Gambardella,Jianfei Cai*

Main category: cs.CV

TL;DR: 本文提出了一种名为PanFlow的新方法，用于全景视频生成，通过解耦相机旋转与光流条件，实现对大范围动态运动的精确控制，并引入球面噪声扭曲策略提升运动循环一致性。


<details>
  <summary>Details</summary>
Motivation: 现有全景视频生成方法缺乏显式的运动控制，难以处理大而复杂的运动场景。

Method: 利用全景图的球面特性，将相机旋转从输入光流条件中解耦，并采用球面噪声扭曲策略增强全景边界间的运动循环一致性；同时构建了一个大规模、动作丰富的带标注全景视频数据集以支持训练。

Result: 实验表明，PanFlow在运动保真度、视觉质量和时间连贯性方面显著优于先前方法，并在运动迁移和视频编辑等应用中展现出有效性。

Conclusion: PanFlow通过球面建模和新型训练策略，有效提升了全景视频生成中对复杂动态运动的控制能力与生成质量。

Abstract: Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.

</details>


### [112] [StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos](https://arxiv.org/abs/2512.01707)
*Daeun Lee,Subhojyoti Mukherjee,Branislav Kveton,Ryan A. Rossi,Viet Dac Lai,Seunghyun Yoon,Trung Bui,Franck Dernoncourt,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出了StreamGaze基准，首次评估多模态大语言模型（MLLMs）在流式视频中利用实时眼动信号进行时间推理与主动预测的能力；构建了眼动-视频问答生成流程，并揭示了当前MLLMs在眼动引导的时间推理、意图建模和主动预测方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解基准未评估模型对人类眼动信号的解读与利用能力，而眼动对于AR等真实场景中的用户意图预判至关重要。

Method: 提出StreamGaze基准，包含眼动引导的过去、当前与主动任务；构建眼动-视频QA生成流水线，通过注视点提取、区域特异性视觉提示和扫视路径构建，生成时空对齐的问答对。

Result: 在所有StreamGaze任务上，SOTA MLLMs与人类表现存在显著差距，暴露其在眼动驱动的时间推理、意图建模和主动预测上的根本局限；并分析了眼动提示策略、推理行为及任务特异性失败模式。

Conclusion: StreamGaze揭示了当前MLLMs在眼动引导的流式视频理解方面的关键短板，为未来模型需具备的能力提供了明确方向；数据与代码将开源以推动该领域发展。

Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.

</details>


### [113] [AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent](https://arxiv.org/abs/2512.00846)
*Neeraj Anand,Rishabh Jain,Sohan Patnaik,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 本文提出AFRAgent，一种基于instruct-BLIP的轻量级多模态架构，通过自适应特征重归一化技术增强图像嵌入，显著提升GUI自动化性能，同时模型大小不足最强竞品的四分之一。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在GUI自动化中面临两个关键问题：1）因视觉编码器特征空间信息有限，难以准确定位控件和判断操作；2）高性能模型参数量大，训练成本高、推理延迟大。

Method: 提出AFRAgent架构，基于instruct-BLIP；引入自适应特征重归一化（adaptive feature renormalization）技术——一种token级仿射变换，用于增强低分辨率图像嵌入并融合高分辨率细节；整体设计兼顾轻量化与性能。

Result: 在Meta-GUI和AITW基准上达到新SOTA，模型体积小于最强竞品的1/4，同时显著提升控件识别与动作决策准确率。

Conclusion: AFRAgent验证了轻量级多模态架构结合细粒度图像特征增强策略在移动UI自动化中的有效性，为实际部署提供了高效可行的新范式。

Abstract: There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.

</details>


### [114] [Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting](https://arxiv.org/abs/2512.00850)
*Haishan Wang,Mohammad Hassan Vali,Arno Solin*

Main category: cs.CV

TL;DR: Smol-GS是一种用于3D高斯泼溅（3DGS）的紧凑表示学习新方法，通过递归体素层次结构和逐溅射特征实现高效空间-语义联合编码，在保持高质量渲染的同时实现大幅压缩，并支持下游3D理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法参数量大、存储开销高，难以部署于资源受限场景；亟需一种既能大幅压缩又能保持建模灵活性与渲染质量的紧凑表示方法。

Method: 提出Smol-GS：利用递归体素层次结构编码高斯中心的空间坐标，同时为每个高斯分配轻量级特征向量以表征颜色、不透明度、变换及材质等语义属性，实现联合空间-语义压缩。

Result: 在标准基准上达到SOTA压缩率（数量级提升），同时保持高渲染质量；生成的离散表示具备可解释性与结构化特性，初步验证其对导航、规划等下游任务的适配潜力。

Conclusion: Smol-GS证明了在3DGS框架中协同优化几何压缩与语义抽象的可行性，为轻量化神经渲染与通用3D场景表征提供了新范式。

Abstract: We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.

</details>


### [115] [TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models](https://arxiv.org/abs/2512.00872)
*Tim Veenboer,George Yiasemis,Eric Marcus,Vivien Van Veldhuizen,Cees G. M. Snoek,Jonas Teuwen,Kevin B. W. Groot Lipman*

Main category: cs.CV

TL;DR: 本文提出TAP-CT，一种面向3D CT影像的、任务无关的自监督预训练方法，基于ViT和DINOv2改进，支持直接在体数据上高效预训练，无需大量微调即可获得强泛化表征。


<details>
  <summary>Details</summary>
Motivation: 现有医学基础模型常需大量微调或依赖计算密集型解码器，且编码器预训练目标存在任务偏差，亟需轻量、任务无关、即插即用的基础模型。

Method: 对ViT和DINOv2进行适配：修改patch embedding、位置编码，并引入深度感知的体素增强策略，实现端到端3D CT自监督预训练。

Result: 在10.5万例CT体积数据上大规模预训练，所得冻结表征在多个下游任务中表现出强泛化性和稳定性；所有模型、配置与评测代码将开源。

Conclusion: TAP-CT提供了一种简洁高效、可扩展、低资源依赖的医学影像基础模型范式，为后续研究树立了强基线。

Abstract: Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.

</details>


### [116] [Neural Discrete Representation Learning for Sparse-View CBCT Reconstruction: From Algorithm Design to Prospective Multicenter Clinical Evaluation](https://arxiv.org/abs/2512.00873)
*Haoshen Wang,Lei Chen,Wei-Hua Zhang,Linxia Wu,Yong Luo,Zengmao Wang,Yuan Xiong,Chengcheng Zhu,Wenjuan Tang,Xueyi Zhang,Wei Zhou,Xuhua Duan,Lefei Zhang,Gao-Jun Teng,Bo Du,Huangxuan Zhao*

Main category: cs.CV

TL;DR: 本文提出DeepPriorCBCT深度学习框架，仅用常规辐射剂量的六分之一即可实现诊断级锥束CT重建，在多中心回顾性数据和前瞻性临床试验中均验证其图像质量与标准方法无差异，显著降低术中辐射风险。


<details>
  <summary>Details</summary>
Motivation: CBCT引导穿刺虽已广泛应用，但辐射暴露导致继发恶性肿瘤风险升高；现有低剂量策略缺乏大规模多中心验证及前瞻性临床评估。

Method: 提出三阶段深度学习框架DeepPriorCBCT；基于12个中心4102例患者共8675次CBCT扫描进行开发与验证；开展前瞻性交叉对照试验（NCT07035977），纳入138例拟行经皮胸部穿刺患者；由11名医师进行图像判读，并由5名放射科医生和25名介入医师评估图像质量与手术引导适用性。

Result: 重建图像被医师判定为与原始扫描无法区分；诊断性能和整体图像质量与标准重建算法相当；前瞻性试验中，图像质量和病灶评估无显著差异（P>0.05）；介入医师对模型图像与全采样图像无偏好（Kappa<0.2）；辐射剂量降至常规水平的约六分之一。

Conclusion: DeepPriorCBCT可在稀疏采样条件下实现高质量CBCT重建，显著降低术中辐射风险，具备可靠的临床适用性。

Abstract: Cone beam computed tomography (CBCT)-guided puncture has become an established approach for diagnosing and treating early- to mid-stage thoracic tumours, yet the associated radiation exposure substantially elevates the risk of secondary malignancies. Although multiple low-dose CBCT strategies have been introduced, none have undergone validation using large-scale multicenter retrospective datasets, and prospective clinical evaluation remains lacking. Here, we propose DeepPriorCBCT - a three-stage deep learning framework that achieves diagnostic-grade reconstruction using only one-sixth of the conventional radiation dose. 4102 patients with 8675 CBCT scans from 12 centers were included to develop and validate DeepPriorCBCT. Additionally, a prospective cross-over trial (Registry number: NCT07035977) which recruited 138 patients scheduled for percutaneous thoracic puncture was conducted to assess the model's clinical applicability. Assessment by 11 physicians confirmed that reconstructed images were indistinguishable from original scans. Moreover, diagnostic performance and overall image quality were comparable to those generated by standard reconstruction algorithms. In the prospective trial, five radiologists reported no significant differences in image quality or lesion assessment between DeepPriorCBCT and the clinical standard (all P>0.05). Likewise, 25 interventionalists expressed no preference between model-based and full-sampling images for surgical guidance (Kappa<0.2). Radiation exposure with DeepPriorCBCT was reduced to approximately one-sixth of that with the conventional approach, and collectively, the findings confirm that it enables high-quality CBCT reconstruction under sparse sampling conditions while markedly decreasing intraoperative radiation risk.

</details>


### [117] [Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling](https://arxiv.org/abs/2512.00877)
*Zhening Liu,Rui Song,Yushi Huang,Yingdong Hu,Xinjie Zhang,Jiawei Shao,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的前馈式3D高斯点阵压缩框架，通过大规模上下文结构和注意力机制有效建模长距离相关性，实现了20倍压缩比，性能达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点阵压缩方法受限于变换编码网络的感受野和熵模型的上下文容量，难以建模长距离空间依赖关系。

Method: 引入基于Morton序列化的大规模上下文结构，设计细粒度的空-通道自回归熵模型，并开发基于注意力机制的变换编码模型以聚合邻近高斯的特征。

Result: 在前馈推理下实现了20倍的压缩比，且在可泛化的编解码器中达到了最先进的性能。

Conclusion: 所提方法能有效提升3D高斯点阵表示的压缩效率与泛化能力，为实际应用提供了高效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.

</details>


### [118] [Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning](https://arxiv.org/abs/2512.00880)
*Haijian Shao,Wei Liu,Xing Deng*

Main category: cs.CV

TL;DR: 本文提出了一种受量子启发的几何框架，用于神经算子建模，通过Bloch超球面上的归一化奇异值谱表征算子，并证明了谱距离与功能接近性之间的严格等价关系，进而构建了量子度量驱动的功能冗余图（QM-FRG）与一次性结构化剪枝方法，显著提升多模态模型在国产异构硬件上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 多模态智能在资源受限和异构的国产硬件上快速发展，暴露出多模态特征异质性、动态场景实时性要求高、以及硬件特定算子冗余等关键瓶颈。

Method: 提出基于Bloch超球面的量子启发几何框架，用归一化奇异值谱表征神经算子；证明谱距离（Fubini-Study/Wasserstein-2）与功能接近性的严格等价定理；构建量子度量驱动的功能冗余图（QM-FRG）并设计一次性结构化剪枝方法。

Result: 所提度量在受控仿真中优于幅值剪枝和随机基线；实验验证涵盖大规模多模态Transformer及华为昇腾、寒武纪MLU、昆仑芯等国产异构硬件（结果将在扩展期刊版中详述）。

Conclusion: 该工作为跨模态、跨架构的神经算子可替代性提供了首个严格理论基础，并为国产异构硬件上的高效多模态模型部署提供了新范式。

Abstract: The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.

</details>


### [119] [Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints](https://arxiv.org/abs/2512.00882)
*Xisheng Feng*

Main category: cs.CV

TL;DR: 提出“Look, Recite, Then Answer”框架，通过解耦推理过程提升视觉-语言模型在农业领域的细粒度识别能力，有效缓解推理驱动的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型在专业领域（如精准农业）中因语言先验压倒视觉感知而导致的“推理驱动幻觉”和“模态鸿沟”问题。

Method: 将推理过程解耦为三个阶段：Look阶段生成客观视觉描述和候选集；Recite阶段使用轻量级路由器将视觉线索转化为触发特定知识的查询；Answer阶段进行描述与知识间的并行证据对齐以选择最一致标签。

Result: 在AgroBench上达到SOTA效果，杂草识别准确率比Qwen-VL提升23.6%，且优于无外部搜索开销的GPT-4o。

Conclusion: 该模块化设计通过将被动感知转化为主动、可控的知识检索，有效减少了幻觉现象，增强了VLM在专业领域的可靠性。

Abstract: Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to "Reasoning-Driven Hallucination" where linguistic priors override visual perception. A key bottleneck is the "Modality Gap": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose "Look, Recite, Then Answer," a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval

</details>


### [120] [HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics](https://arxiv.org/abs/2512.00885)
*Masatoshi Tateno,Gido Kato,Hirokatsu Kataoka,Yoichi Sato,Takuma Yagi*

Main category: cs.CV

TL;DR: 本文提出了HanDyVQA，一个细粒度的视频问答基准，用于全面评估手-物交互（HOI）中的操作与动态效果，包含11.1K个问答对和10.3K个分割掩码，实验表明现有模型仍有显著差距，尤其在空间关系、运动和部件级理解方面。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集仅粗略覆盖操作或效果，缺乏对HOI动态过程中细粒度时空推理的建模，限制了对复杂交互的理解能力。

Method: 构建HanDyVQA基准，涵盖六类问题（动作、过程、物体、位置、状态变化、物体部件），收集11.1K多选问答对和10.3K分割掩码，并评估最新视频基础模型的表现。

Result: 当前最优模型Gemini-2.5-Pro在HanDyVQA上平均准确率为73%，远低于人类的97%；模型在空间关系、运动理解和部件级几何推理方面表现不足。

Conclusion: HanDyVQA揭示了现有模型在理解HOI动态方面的局限性，引入显式的HOI相关线索可提升性能，为未来建模提供了改进方向。

Abstract: Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.

</details>


### [121] [Multilingual Training-Free Remote Sensing Image Captioning](https://arxiv.org/abs/2512.00887)
*Carlos Rebelo,Gil Rocha,João Daniel Silva,Bruno Martins*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的多语言遥感图像描述生成方法，基于检索增强提示（retrieval-augmented prompting），结合领域适配的SigLIP2编码器与多语言大语言模型（LLM）或视觉-语言模型（VLM），并引入基于PageRank的图重排序策略提升检索内容一致性，在10种语言上达到与监督式英文系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像描述方法依赖大量标注数据且局限于英语，限制其全球适用性；亟需一种无需训练、支持多语言的通用方案。

Method: 采用领域适配的SigLIP2编码器从数据仓库中检索相关图文样本；构建两种生成范式：图像盲式（纯文本提示输入LLM）和图像感知式（图文联合输入VLM）；提出基于图像-文本图的PageRank重排序策略以提升检索内容连贯性。

Result: 在四个基准数据集、十种语言上验证有效，性能媲美全监督英文系统；PageRank重排序带来最高35%指标提升；VLM生成更视觉 grounded 但词汇多样，LLM在BLEU/CIDEr上更强；直接目标语言生成优于翻译策略。

Conclusion: 该工作是首个系统评估遥感图像无训练多语言描述生成的研究，推动了更包容、可扩展的多模态地球观测系统发展。

Abstract: Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.

</details>


### [122] [Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)
*Yiyu Wang,Xuyang Liu,Xiyan Gui,Xinying Lin,Boxue Yang,Chenfei Liao,Tailai Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出Streaming Token Compression (STC)框架，通过缓存相似帧特征（STC-Cacher）和剪枝视觉token序列（STC-Pruner），显著降低流式视频大模型中ViT编码与LLM预填充阶段的延迟和内存开销，同时保持高达99%的精度。


<details>
  <summary>Details</summary>
Motivation: 流式视频大语言模型（VideoLLMs）在实时部署中面临高计算成本问题，主要瓶颈在于ViT编码阶段对时序相似帧的冗余处理，以及LLM预填充阶段过长的token序列带来的延迟和内存负担。

Method: 提出分层即插即用框架STC，包含两个token级加速器：STC-Cacher用于缓存并复用时序相似帧的ViT特征；STC-Pruner基于时空相关性剪枝视觉token序列，仅保留最显著token输入LLM。

Result: 在四个基线流式VideoLLMs和五个基准上验证，STC在ReKV框架上保持99%精度，ViT编码延迟降低24.5%，LLM预填充延迟降低45.3%。

Conclusion: STC是一种高效、通用且无需重训练的轻量化方案，可显著提升流式VideoLLMs的实时推理效率，为实际应用提供可行路径。

Abstract: Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.

</details>


### [123] [Hierarchical Semantic Alignment for Image Clustering](https://arxiv.org/abs/2512.00904)
*Xingyu Zhu,Beier Zhu,Yunfan Li,Junfeng Fang,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的分层语义对齐方法CAE，通过结合名词级概念和描述级文本语义，并利用最优传输对齐图像特征，显著提升了图像聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于名词的图像聚类方法忽略了名词固有的歧义性，导致语义表征失真、聚类质量下降。

Method: 提出CAE方法：1）从WordNet和图文数据集中选取名词与描述构建语义空间；2）利用最优传输将图像特征与名词及描述对齐，形成更具判别力的语义空间；3）融合增强后的语义与图像特征进行聚类。

Result: 在8个数据集上验证有效，在ImageNet-1K上相比当前最优无训练方法，准确率提升4.2%，调整兰德指数（ARI）提升2.9%。

Conclusion: 分层引入互补文本语义并进行无训练对齐，可有效缓解名词歧义问题，显著提升图像聚类效果。

Abstract: Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.

</details>


### [124] [TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model](https://arxiv.org/abs/2512.00909)
*Alireza Javanmardi,Pragati Jaiswal,Tewodros Amberbir Habtegebrial,Christen Millerdurai,Shaoxiang Wang,Alain Pagani,Didier Stricker*

Main category: cs.CV

TL;DR: 提出了一种名为TalkingPose的新型扩散模型框架，用于生成长时序一致的人体上半身动画，通过反馈机制实现无限时长动画生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于计算和内存，难以生成长时间连贯的动画。

Method: 基于图像扩散模型，引入反馈驱动机制，利用驱动帧捕捉面部和手部动作，并通过稳定扩散骨干网络进行迁移。

Result: 实现了长时序一致的高质量人体上半身动画生成，支持无限时长且无需额外计算成本或二次训练。

Conclusion: TalkingPose在生成长时序连贯动画方面优于现有方法，并提供了新的大规模数据集作为基准。

Abstract: Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.

</details>


### [125] [Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision](https://arxiv.org/abs/2512.00911)
*Yuhao Shan,Qianyi Yuan,Jingguo Liu,Shigang Li,Jianfeng Li,Tong Chen*

Main category: cs.CV

TL;DR: 提出了一种双流角度感知生成网络，用于估计相机倾斜角度并重建直立全景图像，在SUN360和M3D数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非直立的全景图像由于机器人姿态不稳定影响下游视觉任务，传统IMU方法存在漂移问题，需更鲁棒的视觉解决方案。

Method: 设计双流网络：CNN分支处理等距柱状投影提取局部几何结构，ViT分支通过立方体投影捕捉全局上下文信息，并通过双投影自适应融合模块整合特征；引入高频增强、环形填充和通道注意力机制提升性能。

Result: 在SUN360和M3D数据集上，该方法在倾斜角估计和直立全景图生成两方面均优于现有方法，消融实验验证了各模块的有效性及任务间的协同效应。

Conclusion: 所提出的双流网络能有效纠正非直立全景图像，兼具高精度角度估计与高质量图像重建能力，适用于复杂环境下的机器人视觉系统。

Abstract: Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360° continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.

</details>


### [126] [ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices](https://arxiv.org/abs/2512.00912)
*Abdelghafour Halimi,Ali Alibrahim,Didier Barradas-Bautista,Ronell Sicat,Abdulkader M. Afifi*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的自动化流程，利用2D微CT切片对12种有孔虫进行分类，构建了高质量数据集并采用先进的CNN模型融合方法，实现了95.64%的准确率，并开发了用于实际部署的交互式仪表板。


<details>
  <summary>Details</summary>
Motivation: 传统有孔虫分类依赖人工判读，耗时且易受主观因素影响，亟需一种高效、可重复的自动化分类方法以推动微古生物学和地质科学的应用。

Method: 使用27个物种共97个标本的微CT扫描数据生成2D切片，选取其中12个物种构建数据集；采用基于标本级别的数据划分策略防止数据泄露；评估七种先进的2D CNN架构并通过集成ConvNeXt-Large与EfficientNetV2-Small提升性能。

Result: 最终模型在测试集上达到95.64%的分类准确率，top-3准确率为99.6%，各类别AUC达0.998；开发了支持实时分类与3D切片匹配的交互式仪表板，集成SSIM、NCC和Dice系数等相似性度量方法。

Conclusion: 该研究为AI辅助的有孔虫分类建立了新基准，提供了一个完整可复现的研究框架，有效连接了深度学习与应用地球科学之间的桥梁。

Abstract: This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.

</details>


### [127] [LAHNet: Local Attentive Hashing Network for Point Cloud Registration](https://arxiv.org/abs/2512.00927)
*Wentao Qu,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种用于点云配准的局部注意力哈希网络（LAHNet），通过引入局部注意力机制和高效的窗口划分策略，增强了特征的感受野和区分性，在室内外真实场景基准上实现了显著的配准效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的点云描述符主要关注局部信息，缺乏合理的感受野来提升特征区分性，限制了配准性能。

Method: 提出LAHNet，包含Group Transformer和Interaction Transformer：前者利用局部敏感哈希将点云均匀划分为非重叠窗口，并通过线性邻域搜索捕捉长程上下文；后者通过构建重叠矩阵增强点云对之间重叠区域的特征交互。

Result: 在多个真实世界的室内和室外基准数据集上验证了LAHNet的有效性，结果表明其能学习到鲁棒且具区分性的特征，显著提升了点云配准性能。

Conclusion: LAHNet通过引入局部注意力机制和有效的跨窗口策略，扩大了特征感受野并增强了特征表达能力，为点云配准提供了新的高效解决方案。

Abstract: Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.

</details>


### [128] [SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding](https://arxiv.org/abs/2512.00936)
*Keita Otani,Tatsuya Harada*

Main category: cs.CV

TL;DR: 本文提出SceneProp方法，将场景图定位（scene-graph grounding）建模为马尔可夫随机场（MRF）中的最大后验（MAP）推理问题，并通过可微信念传播实现端到端优化，在多个基准上显著优于先前方法，且性能随查询图复杂度增加而提升。


<details>
  <summary>Details</summary>
Motivation: 现有短语定位方法难以处理含多对象及关系的复杂视觉查询；而现有场景图定位方法反而在图变大时性能下降，未能有效利用关系结构信息。

Method: 将场景图定位建模为马尔可夫随机场（MRF）上的最大后验（MAP）推理问题，采用可微分的信念传播算法进行全局联合推理，实现图像区域到图节点的最优分配。

Result: 在四个基准数据集上显著超越先前方法；首次验证并实现了定位精度随查询图规模和复杂度增大而持续提升。

Conclusion: 显式建模场景图的结构先验并通过全局推理可有效提升复杂视觉查询的定位能力，关系信息越多、定位越准。

Abstract: Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.

</details>


### [129] [Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation](https://arxiv.org/abs/2512.00944)
*An Yang,Chenyu Liu,Jun Du,Jianqing Gao,Jia Pan,Jinshui Hu,Baocai Yin,Bing Yin,Cong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯点绘（3D-GS）的高效语义分割方法，通过粗到细的二进制编码压缩类别特征、渐进式训练策略分解全景分割任务，并微调不透明度以缓解渲染与分割目标的冲突，显著降低内存开销并提升细粒度分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D-GS分割方法存在高维类别特征导致内存开销大、标签空间拥挤及缺乏稳定多粒度控制机制，难以实现细粒度分割。

Method: 提出粗到细二进制编码方案将每个高斯点的类别特征压缩为单个整数；设计渐进式训练策略将全景分割分解为多个独立子任务；在分割训练中微调高斯点不透明度以缓解光度渲染与语义分割目标间的不兼容性。

Result: 在多个基准上达到最先进的分割性能，同时显著降低内存消耗并加速推理。

Conclusion: 所提方法有效解决了3D-GS语义分割中的内存效率与细粒度精度瓶颈，为3D场景理解提供了更高效、可控的新范式。

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.

</details>


### [130] [Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval](https://arxiv.org/abs/2512.00953)
*Haojian Huang,Kaijing Ma,Jin Chen,Haodong Chen,Zhou Wu,Xianghao Zang,Han Fang,Chao Ban,Hao Sun,Mulin Chen,Zhongjiang He*

Main category: cs.CV

TL;DR: 本文提出了一种用于视频时刻检索的去偏证据学习框架DEMR，通过引入反射翻转融合模块和几何正则化项，改善跨模态对齐与不确定性估计，提升了在复杂查询下的检索准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理细粒度信息和复杂语义对齐时存在困难，且传统证据回归中的不确定性估计易受模态不平衡和结构偏差影响，导致高不确定性错误地分配给简单样本，缺乏对复杂场景的适应能力。

Method: 提出DEMR框架，包含Reflective Flipped Fusion（RFF）模块实现跨模态对齐，引入查询重建任务增强文本敏感性，并设计Geom-regularizer优化不确定性预测，以自适应对齐困难样本。

Result: 在ActivityNet-CD和Charades-CD等标准及去偏数据集上实验表明，该方法显著提升检索效果、鲁棒性和可解释性，尤其在复杂和模糊查询下表现优越。

Conclusion: DEMR通过改进不确定性建模和跨模态融合机制，有效缓解了模态不平衡和证据偏差问题，为基于自然语言的视频 moment retrieval 提供了更可靠和可解释的解决方案。

Abstract: In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.

</details>


### [131] [Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction](https://arxiv.org/abs/2512.00960)
*Boran Wen,Ye Lu,Keyan Wan,Sirui Wang,Jiahong Zhou,Junxuan Liang,Xinpeng Liu,Bang Xiao,Dingbang Huang,Ruiyang Liu,Yong-Lu Li*

Main category: cs.CV

TL;DR: 本文提出4DHOISolver框架，利用稀疏人工标注的接触点，从单目互联网视频中高效重建4D人-物交互（HOI）数据，并构建了大规模Open4DHOI数据集；验证了其在机器人模仿学习中的有效性，同时指出当前3D基础模型尚无法自动精准预测人-物接触关系，凸显人工参与的必要性。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需从海量真实人类-物体交互（HOI）中学习，而单目互联网视频虽数据丰富多样，但从中准确、可扩展地提取4D交互数据仍是一个未解难题。

Method: 提出4DHOISolver——一种新型高效优化框架，通过引入稀疏的人工标注接触点作为约束，解决病态的4D HOI重建问题，同时保持时空一致性和物理合理性。

Result: 构建了Open4DHOI大规模4D HOI数据集（含144类物体、103种动作），并成功用于强化学习代理的动作模仿；实验表明现有3D基础模型尚无法自动预测精确接触对应关系。

Conclusion: 人工参与的接触点标注是当前实现高质量4D HOI重建的关键策略，该工作为社区提出了重要挑战与实用基准。

Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/

</details>


### [132] [PhotoFramer: Multi-modal Image Composition Instruction](https://arxiv.org/abs/2512.00993)
*Zhiyuan You,Ke Wang,He Zhang,Xin Cai,Jinjin Gu,Tianfan Xue,Chao Dong,Zhoutong Zhang*

Main category: cs.CV

TL;DR: 本文提出了PhotoFramer，一个用于摄影构图指导的多模态框架，能对构图不佳的照片生成自然语言改进建议并合成构图良好的示例图像。


<details>
  <summary>Details</summary>
Motivation: 许多普通用户在拍照时难以掌握良好构图，亟需实用、可操作的构图辅助工具。

Method: 构建分层构图子任务（平移、缩放、视角变换），通过数据采样与两阶段退化模型构建大规模配对数据集，并微调支持图文联合理解与生成的多模态模型。

Result: 实验表明，结合文本指导与示例图像的方法显著优于仅用示例图像的基线方法，验证了该框架的有效性与实用性。

Conclusion: PhotoFramer为普通人提供了可访问的专家级构图先验，是迈向实用化构图助手的重要一步，相关代码、模型与数据已开源。

Abstract: Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.

</details>


### [133] [S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud](https://arxiv.org/abs/2512.00995)
*Han Su,Tianyu Huang,Zichen Wan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出S2AM3D，一种融合2D分割先验与3D一致性监督的点云部件分割方法，通过点一致部件编码器和尺度感知提示解码器实现跨视角一致性和实时粒度调整，并发布了一个包含超10万样本的大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云部件分割方法受限于数据稀缺导致泛化能力差，以及引入2D预训练知识后在不同视角下产生不一致的分割结果。

Method: 提出S2AM3D，设计点一致部件编码器，利用原生3D对比学习聚合多视角2D特征，生成全局一致的点特征；引入尺度感知提示解码器，通过连续尺度信号实时调节分割粒度；并构建大规模高质量点云部件数据集用于训练。

Result: 实验表明，S2AM3D在多种评估设置下均达到领先性能，在处理复杂结构和尺寸差异大的部件时表现出优异的鲁棒性和可控性。

Conclusion: S2AM3D有效结合2D先验与3D一致性监督，解决了点云部件分割中的泛化性与跨视角一致性问题，同时新数据集为后续研究提供了有力支持。

Abstract: Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.

</details>


### [134] [Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints](https://arxiv.org/abs/2512.00999)
*Mohsin Rasheed,Abdullah Al-Mamun*

Main category: cs.CV

TL;DR: 提出一种语义感知的医学图像重建框架，结合高阶潜在特征与混合U-Net结构，并引入基于无标度图的轻量级区块链溯源机制，提升重建的解剖保真性与可信赖性。


<details>
  <summary>Details</summary>
Motivation: 医学图像常受噪声、损坏或篡改影响，传统重建方法虽视觉合理但可能破坏解剖结构，威胁临床可靠性，需兼顾结构保真与过程可追溯的方法。

Method: 提出语义感知重建框架，融合高维潜在嵌入与混合U-Net进行结构保持重建；结合轻量级区块链与无标度图设计实现重建过程的可验证溯源。

Result: 在多数据集和多种退化类型下验证，该方法在结构一致性、重建精度和溯源完整性上优于现有方法。

Conclusion: 通过语义引导重建与安全可追溯性的结合，提升了医学AI系统的可信度，增强了临床诊断信心与合规性。

Abstract: Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.

</details>


### [135] [LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency](https://arxiv.org/abs/2512.01008)
*Zhongbin Guo,Jiahe Liu,Wenyu Gao,Yushan Li,Chengzhi Li,Ping Jian*

Main category: cs.CV

TL;DR: LISA-3D是一种两阶段文本驱动3D重建框架，通过在LISA模型中引入几何感知的LoRA层，并复用冻结的SAM-3D重建器，在无需3D文本标注的情况下实现跨视角一致的语言引导3D分割与重建。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动3D重建方法缺乏能同时理解开放词汇指令并保持跨视角一致性的掩码生成器。

Method: 提出LISA-3D框架：第一阶段用几何感知LoRA微调LISA实现跨视角语言图像分割；第二阶段将生成的掩码与RGB图拼为RGBA输入SAM-3D（输出高斯溅射或纹理网格）；训练中利用RGB-D序列和相机位姿构建可微重投影损失，强制跨视角一致性。

Result: 在ScanRefer和Nr3D数据集上，相比单视图基线，语言到3D精度最高提升15.6点；仅微调11.6M参数；支持零样本泛化至未见类别。

Conclusion: LISA-3D是模块化、数据高效且实用的语言引导3D内容生成方案。

Abstract: Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.

</details>


### [136] [Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model](https://arxiv.org/abs/2512.01030)
*Jing He,Haodong Li,Mingzhi Sheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出Lotus-2，一种两阶段确定性框架，用于单图像几何密集预测（如深度、法向），通过适配预训练扩散模型的世界先验，在仅用59K样本下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 单图像几何恢复本质病态；判别式回归依赖大量标注数据且物理推理弱；扩散模型虽具强世界先验，但其随机生成范式不适用于确定性几何推理。

Method: Lotus-2为两阶段确定性框架：第一阶段用单步确定性预测器+局部连续性模块（LCM）生成全局一致结构；第二阶段在核心预测器定义的流形内，通过无噪声的多步校正流（rectified-flow）进行细节增强。

Result: 仅用59K训练样本（<1%现有大数据集），在单目深度估计上达SOTA，在表面法向预测上极具竞争力。

Conclusion: 扩散模型可作为确定性世界先验，支撑高质量几何推理，超越传统判别式与生成式范式。

Abstract: Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.

</details>


### [137] [TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models](https://arxiv.org/abs/2512.01048)
*Maya Varma,Jean-Benoit Delbrouck,Sophie Ostmeier,Akshay Chaudhari,Curtis Langlotz*

Main category: cs.CV

TL;DR: 本文提出TRoVe，一种自动发现时间视觉语言模型（VLM）中导致错误的静态特征偏差的方法，并通过评估框架验证其准确性，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在时序理解任务中可能依赖静态特征偏差（如背景或对象），而非真正的动态变化，这会导致系统性预测错误，需在部署前识别和分析这些偏差。

Method: TRoVe利用训练好的VLM和标注验证集，提取候选静态特征，并根据特征对分类错误的影响及模型对其依赖程度进行评分，以识别错误诱导偏差。

Result: 在包含101个VLM的评估框架上，TRoVe比最佳基线提升28.6%；应用于7个现成VLM和2个任务时，发现了新的静态偏差并有助于提升测试性能。

Conclusion: TRoVe能有效识别时间VLM中的错误诱导静态特征偏差，有助于提升模型鲁棒性和实际部署可靠性。

Abstract: Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.

</details>


### [138] [Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction](https://arxiv.org/abs/2512.01059)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CV

TL;DR: 本文研究了ViT-B/16在ImageNet-1K上的参数冗余问题，提出两种减少MLP参数的策略（GroupedMLP和ShallowMLP），均在降低参数量的同时提升准确率与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩大Vision Transformer规模通常提升性能，但准确率和训练行为并不总随规模单调增长；作者旨在探究ViT-B/16在ImageNet-1K上是否存在MLP模块的参数冗余。

Method: 提出两种MLP参数缩减策略：GroupedMLP（相邻Transformer块共享MLP权重）和ShallowMLP（减半MLP隐藏层维度），均减少32.7%参数，并在标准训练流程下评估其性能与稳定性。

Result: GroupedMLP达81.47% top-1精度且计算成本不变；ShallowMLP达81.25%精度并提升38%推理吞吐量；二者均优于基线（81.05%），且训练稳定性显著增强（峰值到最终精度下降从0.47%降至0.03–0.06%）。

Conclusion: ViT-B/16在该设置下处于过参数化状态，MLP容量可安全削减，甚至轻微提升性能；参数共享与减宽等结构约束可作为有效归纳偏置，参数分配方式对ViT设计至关重要。

Abstract: Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\% of the baseline parameters. Our \emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\% top-1 accuracy while maintaining the baseline computational cost. Our \emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\% top-1 accuracy with a 38\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\% to the range 0.03\% to 0.06\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.

</details>


### [139] [Accelerating Inference of Masked Image Generators via Reinforcement Learning](https://arxiv.org/abs/2512.01094)
*Pranav Subbaraman,Shufan Li,Siyan Zhao,Aditya Grover*

Main category: cs.CV

TL;DR: 本文提出Speed-RL，一种基于强化学习的加速预训练掩码生成模型（MGM）的新范式，通过联合质量与速度奖励微调模型，在减少采样步数的同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有MGM虽能生成高质量图像，但需大量采样步，导致推理速度慢；传统知识蒸馏方法将加速建模为分布匹配问题，效果受限。

Method: 将MGM加速问题建模为强化学习任务，设计融合图像质量与采样步数的复合奖励函数，直接对预训练MGM进行RL微调。

Result: 实验表明，Speed-RL可将基线模型加速3倍（采样步数减少至约1/3），同时保持与原模型相当的图像质量。

Conclusion: 强化学习为MGM加速提供了更自然、更有效的优化视角，优于传统分布匹配式蒸馏方法。

Abstract: Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.

</details>


### [140] [CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions](https://arxiv.org/abs/2512.01095)
*Simon Kohaut,Daniel Ochs,Shun Zhang,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.CV

TL;DR: CycliST is a new benchmark dataset for evaluating Video Language Models (VLMs) on reasoning about cyclical state transitions in videos; experiments show current VLMs struggle with detecting and exploiting cyclic patterns, temporal understanding, and quantitative scene analysis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of targeted evaluation for VLMs' ability to reason about cyclical, time-dependent visual patterns—critical for real-world process understanding.

Method: Introducing CycliST: a synthetic, tiered benchmark with structured video sequences featuring periodic motion and attribute changes (e.g., color, scale), progressively increasing difficulty via object count, clutter, and lighting.

Result: State-of-the-art VLMs (both open-source and proprietary) fail consistently across tasks—unable to reliably detect cycles, understand temporal dynamics, or extract quantitative scene properties; no model dominates, and performance does not correlate strongly with size or architecture.

Conclusion: CycliST reveals a fundamental gap in current VLMs’ spatio-temporal and cyclical reasoning capabilities, and provides a focused framework to drive future progress in visual reasoning about periodic phenomena.

Abstract: We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.

</details>


### [141] [Learning Eigenstructures of Unstructured Data Manifolds](https://arxiv.org/abs/2512.01103)
*Roy Velich,Arkadi Piven,David Bensaïd,Daniel Cremers,Thomas Dagès,Ron Kimmel*

Main category: cs.CV

TL;DR: 提出了一种从非结构化数据中直接学习形状和流形分析谱基的新框架，无需传统算子选择、离散化和特征求解器。


<details>
  <summary>Details</summary>
Motivation: 避免传统几何处理中对算子选择、离散化和特征分解的依赖，提供一种适用于任意维度非结构化数据的统一、无监督学习方法。

Method: 基于最优逼近理论，通过训练网络最小化在选定探针函数分布下的重建误差，隐式分解近似算子，从而学习谱基、采样密度和特征值。

Result: 在3D表面点云和高维图像流形上成功获得了类似拉普拉斯算子的有意义谱基，且无需显式构造算子。

Conclusion: 该框架为非结构化数据（尤其是高维空间）的几何处理提供了原理性、数据驱动的新替代方案。

Abstract: We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.

</details>


### [142] [Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis](https://arxiv.org/abs/2512.01116)
*Yilan Zhang,Li Nanbo,Changchun Yang,Jürgen Schmidhuber,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出SlotSPE框架，利用slot attention将多模态（组织病理图像与基因谱）数据压缩为模态特异、互斥的‘槽’表示，以建模稀疏、患者特异、未标注的关键预后事件（如空间组织模式或通路共激活），从而提升癌症生存预测性能、鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以高效建模高维复杂多模态数据中的关键预后事件，这些事件稀疏、患者特异且无标注，但对预后起决定性作用。

Method: 提出基于slot的结构化预后事件建模框架SlotSPE，采用slot attention将每位患者的组织图像和基因谱分别压缩为紧凑、互斥的模态特异slot集合，用作预后事件编码，并融合生物学先验。

Result: 在10个癌症数据集上，SlotSPE在8个中超越现有方法，整体提升2.9%；对基因数据缺失鲁棒；并通过结构化事件分解显著提升可解释性。

Conclusion: SlotSPE通过slot机制有效捕捉稀疏关键预后事件，实现了更准确、鲁棒且可解释的多模态癌症生存预测。

Abstract: The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.

</details>


### [143] [OmniFD: A Unified Model for Versatile Face Forgery Detection](https://arxiv.org/abs/2512.01128)
*Haotian Liu,Haoyu Chen,Chenhui Pan,You Hu,Guoying Zhao,Xiaobai Li*

Main category: cs.CV

TL;DR: 本文提出OmniFD，一个统一的框架，通过共享Swin Transformer编码器和跨任务交互模块，在图像与视频的伪造检测、空间定位和时间定位四个核心任务上实现联合学习，显著提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为不同伪造检测任务设计独立模型，导致计算冗余且忽视任务间的相关性，缺乏统一、高效的多任务解决方案。

Method: 采用共享的4D时空编码器提取统一表示，引入带可学习查询的跨任务交互模块，通过注意力机制建模任务依赖关系，并配备轻量化解码头完成各任务预测。

Result: 实验表明，OmniFD在多个基准上优于单任务模型，整合图像数据使视频分类准确率提升4.63%，同时减少63%参数量和50%训练时间。

Conclusion: OmniFD实现了高效、可扩展的多任务人脸伪造检测，推动了通用化、实用化伪造检测系统的发展。

Abstract: Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.

</details>


### [144] [Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network](https://arxiv.org/abs/2512.01145)
*Riyadh Mohammed Almushrafy*

Main category: cs.CV

TL;DR: 本文提出了一种仅使用稀疏时间标签（起始、峰值、结束）进行连续微表情强度估计的统一框架，通过三角先验生成伪强度轨迹，并结合ResNet18与双向GRU进行时序回归，无需帧级标注，在SAMM和CASME II数据集上取得优异相关性指标。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于微表情离散类别分类，而对强度随时间连续变化的建模受限于缺乏帧级强度标注，难以开展全监督回归。

Method: 构建基于三角先验的伪强度轨迹生成方法，将稀疏时间点转化为密集强度序列；设计轻量级时序回归模型（ResNet18编码器 + 双向GRU），端到端预测帧级强度；采用统一的时间对齐预处理流程适配多数据集。

Result: 在SAMM上Spearman相关系数达0.9014、Kendall达0.7999；在CASME II上分别达0.9116和0.8168（无apex-ranking项时）；消融实验证明时序建模与结构化伪标签对捕捉微表情‘上升-峰值-下降’动态至关重要。

Conclusion: 这是首个仅依赖稀疏时间标注实现跨数据集统一连续微表情强度估计的方法，有效规避帧级标注成本，为微表情强度建模提供了新范式。

Abstract: Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical.
  We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline.
  Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements.
  To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.

</details>


### [145] [SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models](https://arxiv.org/abs/2512.01148)
*Hamza Tahboub,Weiyan Shi,Gang Hua,Huaizu Jiang*

Main category: cs.CV

TL;DR: 本文发现视觉-语言模型（VLMs）在多社会感知任务联合学习中存在“社会退化”问题，即预训练损害了视觉编码器对细微社会信息的表征能力；为此提出SocialFusion框架，通过轻量连接冻结视觉编码器与语言模型，实现跨五类社会任务的正向迁移，并达到接近任务专用SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强大预训练视觉-语言模型（VLMs）在统一学习多个社会感知任务时表现不佳，出现负向迁移；作者旨在探究其根本原因并提出改进方案。

Method: 提出SocialFusion框架，采用冻结视觉编码器、仅学习最小连接模块的方式，结合线性表征探针分析（decodability）与梯度冲突分析（compatibility）来诊断和缓解‘社会退化’问题。

Result: SocialFusion在全部五个社会感知任务上均实现正向迁移，性能优于现有VLMs，并在多个基准上媲美任务专用的最先进模型。

Conclusion: 当前VLM预训练策略可能不利于通用社会能力的获取，需发展更具社会意识的训练范式；SocialFusion验证了轻量适配路径在社会感知统一建模中的有效性。

Abstract: Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term "social degradation," whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.

</details>


### [146] [DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling](https://arxiv.org/abs/2512.01153)
*Han-Jin Lee,Han-Ju Lee,Jin-Seong Kim,Seok-Hwan Choi*

Main category: cs.CV

TL;DR: 本文提出DPAC方法，通过将对抗梯度投影到由生成分数几何定义的切空间上，以在保持分布保真度的同时提升分类成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗引导扩散采样虽能提高目标类别成功率，但样本质量随对抗轨迹与标称轨迹偏差累积而下降，需建立理论框架量化并缓解该退化。

Method: 基于随机最优控制视角，形式化路径空间KL散度为控制能量，并推导一阶最优性条件；提出DPAC方法，将对抗梯度正交投影至分数梯度的切空间；分析其在离散求解器中的误差阶数优势。

Result: DPAC在ImageNet-100上实现更低FID和估计path-KL，在相同攻击成功率下优于基线；理论证明其使Wasserstein距离误差从O(Δt)提升至O(Δt²)，且对分数或度量近似具有二阶鲁棒性。

Conclusion: 最小化path-KL不仅降低控制能量，还能紧致约束2-Wasserstein距离与FID上界，揭示对抗控制能量与感知保真度间的根本联系；DPAC提供了一种分布保持的、几何感知的扩散引导新范式。

Abstract: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O(Δt) leading error term in the Wasserstein distance, achieving an O(Δt^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.

</details>


### [147] [VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering](https://arxiv.org/abs/2512.01178)
*Zihua Liu,Hiroki Sakuma,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 本文提出了一种名为VSRD++的弱监督框架，用于单目3D目标检测，无需依赖3D标注，通过基于神经场的体渲染和2D弱监督实现高性能检测。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D目标检测方法严重依赖大量3D标注数据，而这些数据通常需要耗费大量人力从LiDAR点云中获取，因此亟需一种不依赖3D标注的弱监督方法。

Method: VSRD++采用两阶段流程：多视角3D自动标注和单目3D检测器训练。在自动标注阶段，使用符号距离场（SDF）表示物体表面，并通过实例感知的体素轮廓渲染生成实例掩码；将SDF分解为立方体SDF和残差距离场（RDF），并引入速度信息和置信度来建模动态物体；同时设计了3D属性初始化模块。在检测阶段，使用优化后的3D边界框作为伪标签训练单目检测器。

Result: 在KITTI-360数据集上的实验表明，VSRD++在静态和动态场景下均显著优于现有的弱监督单目3D目标检测方法。

Conclusion: VSRD++成功实现了无需3D标注的高质量单目3D目标检测，通过神经场体渲染与弱监督学习的有效结合，提升了对静态与动态物体的检测性能，具有良好的应用前景。

Abstract: Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus

</details>


### [148] [TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image](https://arxiv.org/abs/2512.01204)
*Ziqian Wang,Yonghao He,Licheng Yang,Wei Zou,Hongxuan Ma,Liu Liu,Wei Sui,Yuxin Guo,Hu Su*

Main category: cs.CV

TL;DR: 提出TabletopGen，一种无需训练、全自动的框架，用于从参考图像生成高保真、可物理交互的3D桌面场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法主要针对大尺度场景，难以捕捉桌面场景中高密度布局和复杂空间关系。

Method: 输入参考图像（可由文本到图像模型生成），进行实例分割与补全，逐个重建为3D模型并进行规范坐标对齐；通过可微旋转优化器和俯视图空间对齐机制进行姿态与尺度估计，最终组装成无碰撞、可仿真的桌面场景。

Result: 实验和用户研究表明，TabletopGen在视觉保真度、布局准确性和物理合理性方面显著优于现有方法。

Conclusion: TabletopGen能高效生成多样化、实例级、可交互的3D桌面场景，适用于机器人操作策略学习和数据合成。

Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.

</details>


### [149] [Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations](https://arxiv.org/abs/2512.01213)
*Yangbangyan Jiang,Qianqian Xu,Huiyang Shao,Zhiyong Yang,Shilong Bao,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出两种实例级极小极大重构方法，以消除部分AUC（PAUC）优化中的近似误差，实现无偏且可扩展的PAUC优化，并提供紧致泛化界与高效算法。


<details>
  <summary>Details</summary>
Motivation: 现有PAUC优化方法存在不可控近似误差或可扩展性差的问题，而PAUC在类别不平衡和决策约束场景中至关重要。

Method: 提出两种实例级极小极大重构：一种具有渐近消失的近似误差，另一种保持无偏性但引入更多变量；通过阈值学习简化样本选择、结合平滑技术，并设计线性复杂度求解器。

Result: 算法每轮迭代时间复杂度为O(n)，收敛速率为O(ε^{-1/3})；获得紧致泛化界Õ(α^{-1}n_+^{-1} + β^{-1}n_-^{-1})；实验验证了方法在多个基准数据集上的优越性。

Conclusion: 所提方法有效弥合了PAUC优化中的近似鸿沟，在理论保证与实际性能上均优于现有方法。

Abstract: As a variant of the Area Under the ROC Curve (AUC), the partial AUC (PAUC) focuses on a specific range of false positive rate (FPR) and/or true positive rate (TPR) in the ROC curve. It is a pivotal evaluation metric in real-world scenarios with both class imbalance and decision constraints. However, selecting instances within these constrained intervals during its calculation is NP-hard, and thus typically requires approximation techniques for practical resolution. Despite the progress made in PAUC optimization over the last few years, most existing methods still suffer from uncontrollable approximation errors or a limited scalability when optimizing the approximate PAUC objectives. In this paper, we close the approximation gap of PAUC optimization by presenting two simple instance-wise minimax reformulations: one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables. Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity, simplify the complicated sample selection procedure by threshold learning, and then apply different smoothing techniques. Equipped with an efficient solver, the resulting algorithms enjoy a linear per-iteration computational complexity w.r.t. the sample size and a convergence rate of $O(ε^{-1/3})$ for typical one-way and two-way PAUCs. Moreover, we provide a tight generalization bound of our minimax reformulations. The result explicitly demonstrates the impact of the TPR/FPR constraints $α$/$β$ on the generalization and exhibits a sharp order of $\tilde{O}(α^{-1}\n_+^{-1} + β^{-1}\n_-^{-1})$. Finally, extensive experiments on several benchmark datasets validate the strength of our proposed methods.

</details>


### [150] [M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis](https://arxiv.org/abs/2512.01214)
*Hang Wu,Ke Sun,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出M4-BLIP框架，结合BLIP-2模型与面部局部先验信息，通过对齐融合模块提升多模态媒体篡改检测性能，并增强与大语言模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有篡改检测方法常忽略局部信息，而实际篡改多集中于局部区域（如人脸），因此需引入局部特征以提升检测精度。

Method: 基于BLIP-2模型提取局部特征，融合面部局部先验信息，设计对齐与融合模块整合局部与全局特征，并集成大语言模型提升结果可解释性。

Result: 在定量实验与可视化分析中，M4-BLIP在检测准确性上优于现有最先进方法。

Conclusion: M4-BLIP通过有效融合局部与全局特征并结合LLM，显著提升了多模态媒体篡改检测的性能与可解释性。

Abstract: In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.

</details>


### [151] [S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/abs/2512.01223)
*Beining Xu,Siting Zhu,Zhao Jin,Junxian Li,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出S²-MLLM框架，通过隐式空间推理增强多模态大语言模型在3D视觉定位任务中的空间理解能力，避免低效的点云重建，引入结构感知策略与结构增强模块（SE），在ScanRefer、Nr3D和Sr3D数据集上实现性能、泛化性与效率的统一提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLMs）的3D视觉定位方法受限于仅使用2D图像输入，难以有效建模3D空间结构；依赖视角相关的点云渲染导致效率低且空间推理能力有限。

Method: 提出S²-MLLM框架：1）利用前馈式3D重建过程中的结构感知能力，实现隐式3D空间推理；2）设计结构增强模块（SE），融合视内/视间注意力机制与多级位置编码，将视觉表征与空间位置及视角信息对齐。

Result: 在ScanRefer、Nr3D和Sr3D数据集上显著超越现有方法，兼具高性能、强泛化能力和高计算效率。

Conclusion: 隐式空间推理是提升MLLMs在3D视觉定位中性能的有效路径；S²-MLLM无需显式点云重建即可实现鲁棒、高效、通用的3D场景理解。

Abstract: 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

</details>


### [152] [PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards](https://arxiv.org/abs/2512.01236)
*Shulei Wang,Longhui Wei,Xin He,Jianbo Ouyang,Hui Lu,Zhou Zhao,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的多主体数据生成流程和基于强化学习的微调策略，以提升多主体个性化图像生成中的主体一致性和文本可控性，并引入新基准进行综合评估。


<details>
  <summary>Details</summary>
Motivation: 现有单主体个性化生成模型在扩展到多主体场景时性能下降，主要问题在于缺乏高质量多主体数据集和精细的后训练策略。

Method: 构建基于单主体模型的多主体数据生成流水线；设计成对主体一致性奖励和通用奖励，并结合强化学习进行精调；提出涵盖三个维度、七个子集的新评估基准。

Result: 实验表明该方法显著提升了多主体个性化图像生成中主体一致性和文本提示遵循能力。

Conclusion: 通过高质量多主体数据构建与强化学习驱动的微调策略，可有效解决多主体个性化生成中的关键挑战，推动该领域发展。

Abstract: Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR

</details>


### [153] [TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition](https://arxiv.org/abs/2512.01248)
*Junyuan Zhang,Bin Wang,Qintong Zhang,Fan Wu,Zichen Wen,Jialin Lu,Junjie Shan,Ziqi Zhao,Shuya Yang,Ziling Wang,Ziyang Miao,Huaping Zhong,Yuhang Zang,Xiaoyi Dong,Ka-Ho Chow,Conghui He*

Main category: cs.CV

TL;DR: 本文提出TRivia，一种自监督微调方法，使预训练视觉语言模型能直接从无标签表格图像中学习表格识别，无需人工标注，显著提升开源模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格识别（TR）方法依赖大量标注数据，而获取成本高；开源模型因资源有限且受隐私法规限制，性能远落后于专有模型，亟需一种不依赖标注数据的高效训练方法。

Method: 基于组相对策略优化（Group Relative Policy Optimization），设计问答式奖励机制：注意力引导模块为每张表格图像生成多样化问题，模型通过正确回答问题来反馈优化自身识别能力，形成自主学习的闭环。

Result: 所提出的TRivia-3B模型在三个主流基准上超越Gemini 2.5 Pro、MinerU2.5等现有系统，成为当前最优的开源轻量级表格识别模型。

Conclusion: TRivia证明了仅利用无标签表格图像即可实现高性能表格识别，为隐私敏感与资源受限场景下的文档解析提供了可行、高效且开源的新范式。

Abstract: Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia

</details>


### [154] [ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process](https://arxiv.org/abs/2512.01268)
*Jongwon Sohn,Juhyeon Moon,Hyunjoon Jung,Jaewook Nam*

Main category: cs.CV

TL;DR: 本文提出了一种基于计算机视觉的非接触式粘度测量方法，利用混合过程中自由表面形变引起的背景图案光学畸变来推断液体粘度，在多种光照条件下表现出良好性能，并引入不确定性量化以提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统粘度计具有侵入性、需严格实验室环境，难以适应真实工业过程条件，亟需一种非接触、鲁棒、适用于自动化场景的替代方案。

Method: 设计一种基于视觉的非接触式粘度计，通过分析光经动态自由表面折射后导致的固定背景图案畸变，结合多图案策略增强视觉线索，并引入不确定性量化机制评估预测置信度。

Result: 在log(m²/s)单位下回归任务平均绝对误差为0.113，粘度分类准确率达81%；多图案策略可缓解相近粘度类别区分困难的问题；不确定性量化提升了系统可靠性。

Conclusion: 该视觉粘度计是一种实用、可集成于自主实验室与工业过程监控的非接触、免标定、自动化就绪的新一代粘度传感方案。

Abstract: Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.

</details>


### [155] [nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis](https://arxiv.org/abs/2512.01273)
*Xin Li,Wenhui Zhu,Xuanzhao Dong,Hao Wang,Yujian Xiong,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: 本文提出nnMobileNet++，一种结合卷积与Transformer的混合轻量级网络，用于提升视网膜图像分析性能，尤其在捕捉长程依赖和不规则病灶/血管结构方面优于纯CNN。


<details>
  <summary>Details</summary>
Motivation: 纯卷积网络（如nnMobileNet）难以建模视网膜图像中关键的长程依赖、不规则病灶及细长血管结构，而这些对临床诊断至关重要。

Method: 提出nnMobileNet++：（i）动态蛇形卷积用于边界感知特征提取；（ii）在第二下采样阶段后引入阶段特定的Transformer模块以建模全局上下文；（iii）采用视网膜图像预训练提升泛化能力。

Result: 在多个公开视网膜数据集的分类任务上达到SOTA或极具竞争力的精度，同时保持低计算开销。

Conclusion: nnMobileNet++是一种轻量高效、兼具局部细节与全局建模能力的视网膜图像分析新框架，具有临床部署潜力。

Abstract: Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.

</details>


### [156] [Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI](https://arxiv.org/abs/2512.01291)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本文提出了一种新的声纳图像分析框架，通过目标对比式遗忘（TCU）模块减少海床背景偏差，并结合可解释的遗忘框架（UESF）提升模型泛化性、鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在声纳图像分析中过度依赖海床特征，导致泛化能力差。

Method: 提出Targeted Contrastive Unlearning（TCU）模块扩展三元组损失以削弱海床偏差，并构建Unlearn to Explain Sonar Framework（UESF），适配LIME生成更忠实、局部化的归因图用于遗忘评估。

Result: 在真实与合成声纳数据集上的实验表明，该方法显著提升了遗忘效果、模型鲁棒性与可解释性。

Conclusion: 所提框架有效缓解了声纳图像分析中的背景偏差问题，兼顾性能提升与可解释性，为水下目标检测提供了新思路。

Abstract: Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.

</details>


### [157] [Diffusion Model in Latent Space for Medical Image Segmentation Task](https://arxiv.org/abs/2512.01292)
*Huynh Trinh Ngoc,Toan Nguyen Hai,Ba Luong Son,Long Tran Quoc*

Main category: cs.CV

TL;DR: 本文提出MedSegLatDiff，一种基于扩散模型的医学图像分割框架，结合变分自编码器（VAE）与潜在扩散模型，在保证高精度的同时实现高效、多假设分割与不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像分割方法仅输出单一确定性掩码，无法表征分割不确定性；而现有生成式多假设方法计算开销大，难以临床部署。

Method: 提出MedSegLatDiff框架：1）使用VAE将图像压缩至低维潜在空间以降噪并加速训练；2）在潜在空间中构建扩散模型生成多样化的分割掩码；3）在VAE重建路径中采用加权交叉熵损失，增强小结构（如微小结节）的分割保真度。

Result: 在ISIC-2018（皮肤病变）、CVC-Clinic（息肉）和LIDC-IDRI（肺结节）数据集上达到SOTA或极具竞争力的Dice和IoU分数；同时支持生成多样性分割假设及像素级置信图。

Conclusion: MedSegLatDiff兼顾效率、精度与不确定性建模能力，显著提升模型可解释性与临床可靠性，适用于实际医疗场景部署。

Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.

</details>


### [158] [EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly](https://arxiv.org/abs/2512.01296)
*Xiaokun Pan,Zhenzhe Li,Zhichao Ye,Hongjia Zhai,Guofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出EGG-Fusion系统，结合鲁棒稀疏-稠密相机跟踪与几何感知高斯面元映射，利用信息滤波融合方法提升抗噪能力与表面重建精度，在Replica和ScanNet++上达到0.6cm误差、24FPS实时性能，精度较SOTA提升超20%。


<details>
  <summary>Details</summary>
Motivation: 现有可微渲染SLAM方法在实时计算和传感器噪声敏感性方面存在双重挑战，导致几何保真度下降、实用性受限。

Method: 提出EGG-Fusion系统，包含鲁棒稀疏到稠密相机跟踪模块和几何感知高斯面元（Gaussian surfel）映射模块；引入基于信息滤波的融合方法，显式建模传感器噪声；采用可微高斯面元映射实现多视角一致表面建模与高效参数优化。

Result: 在Replica和ScanNet++标准基准上表面重建误差为0.6cm，较当前最优GS方法精度提升超20%；实现实时处理24FPS。

Conclusion: EGG-Fusion是目前最精确的可微渲染实时三维重建系统之一，在精度与速度间取得优异平衡。

Abstract: Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/

</details>


### [159] [TBT-Former: Learning Temporal Boundary Distributions for Action Localization](https://arxiv.org/abs/2512.01298)
*Thisara Rathnayaka,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文提出了Temporal Boundary Transformer (TBT-Former)，通过增强Transformer主干、跨尺度特征金字塔网络和基于分布的边界回归头，显著提升了时序动作定位性能，尤其在模糊边界建模与多尺度上下文融合方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有单阶段无锚点模型（如ActionFormer）在处理动作时间边界模糊及多尺度上下文信息融合方面存在不足。

Method: 提出TBT-Former架构，包含：(1) 更高容量的缩放Transformer主干；(2) 带侧向连接的自顶向下特征金字塔网络（FPN）；(3) 受广义焦点损失（GFL）启发的边界分布回归头，将边界回归建模为概率分布学习问题。

Result: 在THUMOS14和EPIC-Kitchens 100数据集上达到新SOTA，在ActivityNet-1.3上保持竞争力。

Conclusion: TBT-Former有效缓解了时序动作定位中边界模糊性和多尺度融合难题，验证了分布式边界建模对提升定位精度的重要性。

Abstract: Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or "fuzzy" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding

</details>


### [160] [DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy](https://arxiv.org/abs/2512.01302)
*Jaewoo Song,Jooyoung Choi,Kanghyun Baek,Sangyub Lee,Daemin Park,Sungroh Yoon*

Main category: cs.CV

TL;DR: DCText是一种无需训练的视觉文本生成方法，采用分而治之策略，通过文本分解、区域分配及两种注意力掩码（Text-Focus和Context-Expansion）提升长/多文本生成的准确性和图像一致性，同时保持低延迟和高图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在处理长文本或多文本时因全局注意力稀释而表现不佳。

Method: 提出DCText方法：1）对提示词进行文本提取与划分并分配至指定区域；2）在去噪过程中顺序应用Text-Focus和Context-Expansion两种注意力掩码以兼顾局部文本准确性和全局图像连贯性；3）引入Localized Noise Initialization提升文本准确性和区域对齐，不增加计算开销。

Result: 在单句和多句基准测试中，DCText实现了最高的文本准确率，同时不损害图像质量，并具有最低的生成延迟。

Conclusion: DCText通过训练无关、模块化设计有效解决了长/多文本图像生成难题，在文本准确性、图像质量和生成效率三方面取得平衡与突破。

Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.

</details>


### [161] [Lost in Distortion: Uncovering the Domain Gap Between Computer Vision and Brain Imaging - A Study on Pretraining for Age Prediction](https://arxiv.org/abs/2512.01310)
*Yanteng Zhang,Songheng Li,Zeyu Shen,Qizhen Lan,Lipei Zhang,Yang Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 研究探讨了大规模脑成像数据集中不同质量水平的数据在预训练中的作用，发现数据质量显著影响下游任务（如脑龄预测）的性能，强调需要领域感知的数据整理以构建可信且可泛化的领域基础模型。


<details>
  <summary>Details</summary>
Motivation: 脑成像数据常具有高度异质性，包含从高质量到严重失真或不完整的扫描，这引发了一个关键问题：低质量数据是否有助于预训练还是反而阻碍模型学习？

Method: 通过在不同质量水平的脑成像数据集上进行预训练，并在外部队列上微调以执行脑龄预测任务，系统评估数据质量对下游任务性能的影响。

Result: 实验结果显示不同数据质量水平导致显著的性能差异，表明高质量数据更有利于模型学习，而低质量数据可能损害模型表现。

Conclusion: 数据质量对脑成像领域的预训练模型至关重要，直接照搬计算机视觉的做法不可取，需结合临床神经影像标准进行领域适配的数据筛选与管理。

Abstract: Large-scale brain imaging datasets provide unprecedented opportunities for developing domain foundation models through pretraining. However, unlike natural image datasets in computer vision, these neuroimaging data often exhibit high heterogeneity in quality, ranging from well-structured scans to severely distorted or incomplete brain volumes. This raises a fundamental question: can noise or low-quality scans contribute meaningfully to pretraining, or do they instead hinder model learning? In this study, we systematically explore the role of data quality level in pretraining and its impact on downstream tasks. Specifically, we perform pretraining on datasets with different quality levels and perform fine-tuning for brain age prediction on external cohorts. Our results show significant performance differences across quality levels, revealing both opportunities and limitations. We further discuss the gap between computer vision practices and clinical neuroimaging standards, emphasizing the necessity of domain-aware curation to ensure trusted and generalizable domain-specific foundation models.

</details>


### [162] [IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval](https://arxiv.org/abs/2512.01312)
*Ning Han,Yawen Zeng,Shaohua Long,Chengqing Li,Sijie Yang,Dun Tan,Jianfeng Dong,Jingjing Chen*

Main category: cs.CV

TL;DR: 本文提出了交互式视频语料库检索（IVCR）任务，旨在实现用户与检索系统之间的多轮、对话式、真实交互，并构建了高质量双语多轮对话数据集IVCR-200K，同时提出基于多模态大语言模型（MLLMs）的综合框架以支持可解释的多模式交互。


<details>
  <summary>Details</summary>
Motivation: 以往视频检索系统缺乏与用户的有意义交互，单向检索范式无法满足至少80.8%用户的个性化和动态需求。

Method: 提出IVCR新任务；构建IVCR-200K双语、多轮、抽象语义对话数据集；设计基于多模态大语言模型（MLLMs）的综合性交互框架。

Result: 构建了首个支持视频及视频片段检索的高质量多轮对话数据集IVCR-200K，并验证了所提MLLMs框架在多模式、可解释交互上的有效性。

Conclusion: IVCR任务更贴近真实用户需求，IVCR-200K数据集和MLLMs框架为推动交互式视频检索研究提供了坚实基础和新方向。

Abstract: In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.

</details>


### [163] [TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance](https://arxiv.org/abs/2512.01314)
*Pei Yang,Yepeng Liu,Kelly Peng,Yuan Gao,Yiren Song*

Main category: cs.CV

TL;DR: 本文提出TokenPure，一种基于扩散Transformer的新型框架，用于高效且一致地去除数字水印。通过基于token的条件重建，TokenPure在彻底消除水印与保持内容一致性之间取得平衡，并在实验中展现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 在数字经济时代，数字水印是海量可复制内容（如AI生成内容）权属证明的关键技术，而设计能抵抗各类攻击和处理操作的鲁棒水印变得尤为重要；同时，对水印去除技术的需求也日益增长。

Method: TokenPure采用基于扩散Transformer的架构，将水印去除任务重构为条件生成问题，完全绕过初始含水印噪声；它将水印图像分解为视觉token（表征纹理）和结构token（表征几何），二者联合驱动扩散过程以实现高保真、结构完整的无水印图像重建。

Result: TokenPure在水印去除效果与重建保真度方面达到当前最优水平，在感知质量与内容一致性上显著优于现有基线方法。

Conclusion: TokenPure通过创新的token级条件重建机制，有效解决了水印去除中彻底性与内容一致性的权衡难题，为数字内容版权治理提供了新思路和技术支撑。

Abstract: In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.

</details>


### [164] [FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection](https://arxiv.org/abs/2512.01315)
*Ashish Vashist,Qiranul Saadiyean,Suresh Sundaram,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: 本文提出了一种新的数据集FOD-S2R，用于提升飞机燃油箱内异物检测的性能，结合真实与合成图像，首次系统评估了封闭环境中合成数据的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对飞机燃油箱等封闭环境的专用异物碎片（FOD）数据集，现有方法难以有效检测和应对相关安全风险，因此需要构建一个贴近实际且多样化的数据集来推动自动化检测技术的发展。

Method: 提出了FOD-S2R数据集，包含3,114张在受控燃油箱模型中拍摄的真实高清图像和3,137张使用Unreal Engine生成的合成图像，涵盖多种视场、距离、光照、颜色和物体尺寸；并通过多个最先进的目标检测模型进行基准测试，评估合成数据对真实场景检测性能的影响。

Result: 实验表明，引入合成数据能显著提高模型在真实环境中的检测精度和泛化能力，有效缩小仿真到现实（Sim2Real）的差距。

Conclusion: FOD-S2R为航空维护中的自动异物检测提供了重要基础，验证了合成数据在复杂封闭环境中提升检测性能的有效性和应用潜力。

Abstract: Foreign Object Debris (FOD) within aircraft fuel tanks presents critical safety hazards including fuel contamination, system malfunctions, and increased maintenance costs. Despite the severity of these risks, there is a notable lack of dedicated datasets for the complex, enclosed environments found inside fuel tanks. To bridge this gap, we present a novel dataset, FOD-S2R, composed of real and synthetic images of the FOD within a simulated aircraft fuel tank. Unlike existing datasets that focus on external or open-air environments, our dataset is the first to systematically evaluate the effectiveness of synthetic data in enhancing the real-world FOD detection performance in confined, closed structures. The real-world subset consists of 3,114 high-resolution HD images captured in a controlled fuel tank replica, while the synthetic subset includes 3,137 images generated using Unreal Engine. The dataset is composed of various Field of views (FOV), object distances, lighting conditions, color, and object size. Prior research has demonstrated that synthetic data can reduce reliance on extensive real-world annotations and improve the generalizability of vision models. Thus, we benchmark several state-of-the-art object detection models and demonstrate that introducing synthetic data improves the detection accuracy and generalization to real-world conditions. These experiments demonstrate the effectiveness of synthetic data in enhancing the model performance and narrowing the Sim2Real gap, providing a valuable foundation for developing automated FOD detection systems for aviation maintenance.

</details>


### [165] [Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications](https://arxiv.org/abs/2512.01319)
*Feiyang Xiao,Yichi Zhang,Xigui Li,Yuanye Zhou,Chen Jiang,Xin Guo,Limei Han,Yuxin Li,Fengping Zhu,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出首个包含641个3D MRA图像和587个标注的颅内动脉瘤血管分割（IAVS）数据集，强调分割结果在血流动力学分析中的实际应用，并构建了基于CFD适用性的评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有分割方法多关注图像评价指标，忽视其在计算流体动力学（CFD）等下游应用中的有效性，缺乏兼顾拓扑完整性和CFD适用性的数据集。

Method: 构建多中心IAVS数据集，包含图像-掩码对及血流动力学分析结果；设计两阶段评估基准（动脉瘤定位与精细分割）和两阶段分割框架；建立标准化CFD适用性评估系统，实现分割掩码到CFD模型的自动化转换。

Result: 提供了大规模、高质量的IAVS数据集，支持图像分割与CFD应用的联合评估；所提两阶段框架可作为即用方法和强基线；CFD评估系统实现了对分割结果的实用性量化。

Conclusion: IAVS数据集填补了当前在临床导向的动脉瘤分割与CFD应用之间的空白，推动了面向实际医疗应用的医学图像分割方法的发展。

Abstract: The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at https://github.com/AbsoluteResonance/IAVS.

</details>


### [166] [Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI](https://arxiv.org/abs/2512.01333)
*A S M Ahsanul Sarkar Akib,Raduana Khawla,Abdul Hasib*

Main category: cs.CV

TL;DR: 本研究提出了一种结合集成学习与可解释人工智能（XAI）的机器学习框架，用于中风风险预测。通过特征工程、数据预处理和随机过采样（ROS）解决类别不平衡问题，系统评估了10种模型，最终构建了一个由随机森林、ExtraTrees和XGBoost组成的集成模型，在中风预测数据集（SPD）上达到了99.09%的准确率。利用LIME进行可解释性分析，识别出年龄、高血压和血糖水平为三个关键临床特征，提升了模型透明度和临床适用性。该框架有助于实现早期预测、个性化临床决策和心血管风险管理。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要的健康威胁，早期风险评估对及时干预和预防至关重要。现有模型常面临准确性不足或缺乏可解释性的问题，限制了其在临床实践中的应用。因此，亟需一个既高精度又可解释的预测工具以支持临床决策。

Method: 采用集成建模与可解释AI（XAI）技术，结合特征工程与数据预处理，并使用随机过采样（ROS）缓解类别不平衡。通过5折交叉验证在多个数据集上评估10种机器学习模型，最终构建并优化随机森林、ExtraTrees与XGBoost的集成模型，同时利用LIME进行特征重要性分析以提升可解释性。

Result: 所提出的集成模型在中风预测数据集（SPD）上实现了99.09%的准确率，表现优异；LIME分析揭示年龄、高血压和血糖水平为最重要的三个预测因子，增强了模型的透明性和临床可信度。

Conclusion: 结合集成学习与可解释AI的方法能有效提升中风风险预测的准确性与可解释性，具有良好的临床应用前景，有望推动数据驱动的个性化预防与心血管健康管理。

Abstract: Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.

</details>


### [167] [AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation](https://arxiv.org/abs/2512.01334)
*Yexin Liu,Wen-Jie Shu,Zile Huang,Haoze Zheng,Yueze Wang,Manyuan Zhang,Ser-Nam Lim,Harry Yang*

Main category: cs.CV

TL;DR: 本文提出AlignVid框架，通过Attention Scaling Modulation和Guidance Scheduling两种轻量级、无需训练的方法，提升文本引导图像到视频生成中对细粒度提示语义的遵循能力，缓解‘语义忽视’问题，并构建OmitI2V评测基准验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像到视频（TI2V）方法在处理需大幅变换输入图像的提示（如物体增删改）时，存在‘语义忽视’问题，即难以准确遵循细粒度提示语义。

Method: 提出训练无关的AlignVid框架，包含两部分：(i) 注意力缩放调制（ASM），通过轻量级Q/K缩放直接重加权注意力；(ii) 引导调度（GS），在特定Transformer层和去噪步中选择性应用ASM以减少画质下降。同时构建OmitI2V评测集（367个标注样本）评估语义忽视。

Result: AlignVid显著提升了TI2V生成结果对提示语义的忠实度（语义保真度），且几乎不损害视觉质量；OmitI2V为定量评估语义忽视提供了新基准。

Conclusion: 轻微且有针对性的注意力干预（如ASM+GS）可有效缓解TI2V中的语义忽视问题，无需额外训练，兼顾语义准确性与生成质量。

Abstract: Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.

</details>


### [168] [EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans](https://arxiv.org/abs/2512.01340)
*Yingjie Zhou,Xilei Zhu,Siyu Ren,Ziyi Zhao,Ziwen Wang,Farong Wen,Yu Zhou,Jiezhang Cao,Xiongkuo Min,Fengjiao Chen,Xiaoyu Li,Xuezhi Cao,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 本文提出了首个大规模多说话人生成的说话人质量评估数据集THQA-MT，并设计了新型质量评估框架EvalTalker，显著提升了与主观评分的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有多说话人（Multi-Talker）生成方法存在明显质量退化问题，缺乏高质量、大规模的专用评估数据集和有效评估框架。

Method: 构建了包含5492个多说话人生成样本的大规模质量评估数据集THQA-MT；通过主观实验分析感知差异并归纳12类常见失真；提出EvalTalker评估框架，融合全局质量、人物特征、身份一致性和Qwen-Sync多模态同步感知能力。

Result: EvalTalker在与主观评分的相关性上优于现有方法，验证了其作为高质量Multi-Talker生成与评估基准的有效性。

Conclusion: THQA-MT和EvalTalker为多说话人生成质量评估提供了新基准和实用工具，推动高保真、强交互音频视觉生成技术的发展。

Abstract: Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.

</details>


### [169] [InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision](https://arxiv.org/abs/2512.01342)
*Chenting Wang,Yuhan Zhu,Yicheng Xu,Jiange Yang,Ziang Yan,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出InternVideo-Next，通过解耦编码器-预测器-解码器（EPD）框架和两阶段预训练策略，解决现有视频掩码建模中像素重建与语义抽象的冲突及捷径学习问题，显著提升视频表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频-文本预训练依赖噪声大、语义覆盖有限的合成字幕，而掩码视频建模（MVM）虽能直接利用时空结构，却因像素级重建收敛困难、低级要求与高层语义冲突、以及潜在预测易导致捷径学习等问题，在通用任务上落后于文本监督方法。

Method: 提出Encoder-Predictor-Decoder（EPD）框架，将预测器作为潜在世界模型；设计两阶段预训练：第一阶段用条件扩散解码器替代线性解码器，并注入图像级语义先验，兼顾像素保真与语义抽象；第二阶段在冻结的第一阶段目标空间中预测世界知识，缓解捷径学习。

Result: InternVideo-Next在多个公开基准测试中达到SOTA性能，且仅使用公开无标签视频数据训练。

Conclusion: 该工作为构建语义一致又细节保留的视频潜在空间提供了新范式，推动了通用视频表征学习的可扩展发展。

Abstract: Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.

</details>


### [170] [Handwritten Text Recognition for Low Resource Languages](https://arxiv.org/abs/2512.01348)
*Sayantan Dey,Alireza Alaei,Partha Pratim Roy*

Main category: cs.CV

TL;DR: 本文提出了BharatOCR，一种无需分割的段落级手写印地语和乌尔都语文本识别方法，采用ViT-Transformer解码器-语言模型架构，结合DeiT和RoBERTa提升视觉与语言理解，在多个数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如印地语和乌尔都语在段落级手写文本识别方面缺乏足够语言资源，现有方法难以实现高精度识别，需专门设计鲁棒的OCR系统。

Method: 提出一种基于ViT-Transformer解码器-LM的无分割架构：使用DeiT提取图像特征，Transformer解码器从视觉嵌入生成文本序列，并引入预训练RoBERTa语言模型优化输出；通过逐行隐式分割处理整段图像。

Result: 在NUST-UHWR、PUCIT-OUHL和自建Parimal-Urdu数据集上分别达到96.24%、92.05%和94.80%的字符识别率，在Parimal-Hindi数据集上达到80.64%，优于多种现有最先进方法。

Conclusion: 所提模型在低资源手写文本识别任务中表现出优越性能，验证了结合视觉Transformer与预训练语言模型的有效性，为印度语系手写OCR提供了高效解决方案。

Abstract: Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.

</details>


### [171] [OpenBox: Annotate Any Bounding Boxes in 3D](https://arxiv.org/abs/2512.01352)
*In-Jae Lee,Mungyeom Kim,Kwonyoung Ryu,Pierre Musacchio,Jaesik Park*

Main category: cs.CV

TL;DR: 本文提出OpenBox，一种无需自训练的两阶段自动标注方法，利用2D视觉基础模型实现高质量、物理感知的开放词汇3D目标检测标注。


<details>
  <summary>Details</summary>
Motivation: 降低3D目标检测中人工标注成本，支持识别未见过的物体，并提升对物体物理状态（如刚性与运动状态）的建模能力。

Method: OpenBox采用两阶段流程：第一阶段通过跨模态实例对齐，将2D视觉基础模型提取的实例线索关联到3D点云；第二阶段按刚性和运动状态分类实例，并基于类别特定尺寸统计生成自适应3D包围盒。

Result: 在Waymo、Lyft和nuScenes数据集上实验表明，OpenBox在精度和效率上均优于现有基线方法，且无需多轮自训练。

Conclusion: OpenBox为无监督、开放词汇3D检测提供了更高效、更符合物理实际的自动标注范式，显著减少计算开销并提升标注质量。

Abstract: Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.

</details>


### [172] [BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud](https://arxiv.org/abs/2512.01366)
*Yunzhe Li,Jiajun Yan,Yuzhou Wei,Kechen Liu,Yize Zhao,Chong Zhang,Hongzi Zhu,Li Lu,Shan Chang,Minyi Guo*

Main category: cs.CV

TL;DR: BlinkBud是一种基于单耳塞与手机协同的后方危险车辆实时检测系统，利用少量采样图像结合卡尔曼滤波与强化学习驱动的最优采样策略实现低功耗、高精度3D目标跟踪，并通过头部姿态补偿提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 行人和骑行者无法察觉后方快速接近的车辆，构成严重道路安全隐患。

Method: 提出BlinkBud系统：耳塞搭载微型摄像头采集稀疏图像，手机端运行融合卡尔曼滤波轨迹估计与强化学习优化图像采样的3D目标跟踪算法；并利用估计的俯仰角和偏航角校正深度估计、对齐相机与人体坐标系以抑制头部运动干扰。

Result: 实测显示耳塞与手机平均功耗仅为29.8 mW和702.6 mW；平均误报率（FPR）为4.90%，漏报率（FNR）为1.47%。

Conclusion: BlinkBud在极低功耗下实现了高精度、鲁棒的后方危险物体检测，适用于日常可穿戴场景。

Abstract: Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.

</details>


### [173] [SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation](https://arxiv.org/abs/2512.01373)
*Sheng Liu,Tianyu Luan,Phani Nuney,Xuelu Feng,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型（LLM）的3D形状真实感评估指标——Shape-Realism Alignment Metric，通过将网格编码为语言token并设计真实性解码器，实现无需真值参考的真实感评估；构建了含人类标注真实感评分的新数据集RealismGrading，并在多算法、多物体上验证了该指标与人类感知高度一致且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 传统3D形状真实感评估依赖于真值（ground truth），但现实中许多场景（如创意生成）并无真值参考，而真实感应由人类感知主导，因此亟需一种无需真值、更贴合实际应用的真实感评估方法。

Method: 提出Shape-Realism Alignment Metric：1）采用网格编码器将3D mesh映射到LLM的语言token空间；2）设计专用真实性解码器，将LLM输出对齐人类真实感判断；3）构建RealismGrading数据集，包含16种生成算法在十余类物体上的结果及对应人类真实感评分。

Result: 所提指标在k折交叉验证中展现出与人类评分高度相关（显著优于现有方法），且在不同物体和生成算法间具有良好泛化性。

Conclusion: 本工作首次将LLM引入3D形状真实感无参考评估，证明了语言模型可作为形状表征与人类感知之间的有效桥梁；RealismGrading数据集和所提指标为未来无真值3D评估提供了新范式和基准。

Abstract: 3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.

</details>


### [174] [Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network](https://arxiv.org/abs/2512.01380)
*Tianyu Luan,Xuelu Feng,Zixin Zhu,Phani Nuney,Sheng Liu,Xuan Gong,David Doermann,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种名为TGE的新型3D纹理网格保真度评估方法，直接基于带纹理的3D网格，联合利用几何和颜色信息，无需渲染，在真实世界失真数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状保真度评估指标（如Chamfer Distance）与人类感知对齐差，基于学习的方法依赖渲染图像和2D指标，受限于视角选择和结构覆盖不全，且多在合成失真上训练，存在域差距。

Method: 提出Textured Geometry Evaluation (TGE)，直接在3D网格上操作，联合使用几何和颜色信息进行保真度评估；构建包含真实世界失真的人工标注数据集用于训练和评估。

Result: 实验表明，TGE在真实世界失真数据集上优于基于渲染和仅几何的方法，具有更好的人类感知对齐性。

Conclusion: TGE是一种更有效的3D纹理网格保真度评估方法，通过直接处理3D纹理网格并使用真实失真数据训练，显著提升了与人类评价的一致性。

Abstract: Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.

</details>


### [175] [Reversible Inversion for Training-Free Exemplar-guided Image Editing](https://arxiv.org/abs/2512.01382)
*Yuke Li,Lianli Gao,Ji Zhang,Pengpeng Zeng,Lichuan Xiang,Hongkai Wen,Heng Tao Shen,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出了一种无需训练的可逆反转（ReInversion）方法，用于高效且高质量的示例引导图像编辑，结合掩码引导的选择性去噪策略，实现了最先进的性能和最低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有示例引导图像编辑方法依赖大规模预训练，计算成本高；标准反转技术在该任务中表现不佳，导致编辑质量差且效率低。

Method: 提出ReInversion，采用两阶段去噪过程：第一阶段以源图像为条件，第二阶段以参考图像为条件；并引入掩码引导的选择性去噪（MSD）策略，限制编辑区域，保持背景结构一致性。

Result: 在定性和定量比较中，ReInversion在示例引导图像编辑任务上达到了最先进的性能，并具有最低的计算开销。

Conclusion: ReInversion是一种高效、无需训练的图像编辑方法，在保持高质量编辑的同时显著降低了计算成本，适用于实际应用。

Abstract: Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.

</details>


### [176] [PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications](https://arxiv.org/abs/2512.01383)
*Yunze Liu,Zifan Wang,Peiran Wu,Jiayang Ao*

Main category: cs.CV

TL;DR: 本文提出了PointNet4D，一种轻量级4D骨干网络，结合Mamba与Transformer优势，支持实时流式点云视频处理，并通过4DMAP预训练策略增强时序建模能力，在多个任务和机器人应用中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有4D骨干网络依赖计算密集型的时空卷积和Transformer，难以满足实时性与资源受限场景需求，且缺乏对动态4D环境（3D空间随时间演化）高效建模的能力。

Method: 提出PointNet4D，核心为Hybrid Mamba-Transformer时序融合模块；引入帧级掩码自回归预训练策略4DMAP以捕获运动线索。

Result: 在7个数据集、9项任务上实现一致性能提升；构建4D Diffusion Policy和4D Imitation Learning两个机器人系统，在RoboTwin和HandoverSim基准上取得显著增益。

Conclusion: PointNet4D兼顾效率与表达力，适用于在线/离线多种部署场景，为实时4D感知与机器人决策提供了新范式。

Abstract: Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.

</details>


### [177] [FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution](https://arxiv.org/abs/2512.01390)
*Seungho Choi,Jeahun Sung,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出FRAMER，一种即插即用的训练方案，利用扩散模型先验，在不修改骨干网络和推理过程的前提下，通过频域分解与对比学习提升真实图像超分辨率（Real-ISR）中高频细节重建能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在感知质量上优于GAN，但存在低频偏差和‘先低后高’的深度层级特性，导致高频细节重建不足。

Method: FRAMER在每步去噪中以最终层特征图为教师，指导中间各层；通过FFT将教师/学生特征图分解为低频（LF）和高频（HF）分量，并分别设计IntraCL（稳定全局结构）和InterCL（增强实例细节）对比损失；引入FAW和FAM两个自适应调制器，动态加权各层LF/HF信号并调控蒸馏强度。

Result: 在U-Net和DiT骨干（如Stable Diffusion 2、3）上均显著提升PSNR/SSIM及感知指标（LPIPS、NIQE、MANIQA、MUSIQ）；消融实验证实最终层教师与随机层负样本的有效性。

Conclusion: FRAMER有效缓解扩散模型在Real-ISR中的高频重建缺陷，是一种通用、高效且即插即用的训练增强方法。

Abstract: Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise "low-first, high-later" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.

</details>


### [178] [Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries](https://arxiv.org/abs/2512.01419)
*Tushar Pranav,Eshan Pandey,Austria Lyka Diane Bala,Aman Chadha,Indriyati Atmosukarto,Donny Soh Cheng Lock*

Main category: cs.CV

TL;DR: 本文提出了RICE-VL基准，用于评估视觉语言模型（VLMs）在东南亚11国文化理解上的能力，并设计了SEA-LAVE新指标，揭示现有VLM在低资源国家和抽象文化领域存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）存在西方中心主义偏差，在东南亚等文化多元地区表现受限，亟需面向本地文化的评估基准。

Method: 构建包含28,000+人工标注VQA样本和1,000组图像-边界框对的RICE-VL基准；提出扩展指标SEA-LAVE，综合评估文本准确性、文化对齐度与国家识别能力；在6个开源/闭源VLM上开展系统评测。

Result: 实验发现主流VLM在低资源东盟国家及抽象文化领域性能明显下降；视觉定位任务进一步暴露模型对文化关键元素的空间与上下文理解不足。

Conclusion: RICE-VL揭示了当前VLM文化理解能力的严重局限，强调需推动更具包容性的多文化模型开发与评估范式。

Abstract: Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA). To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries. RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories. We propose SEA-LAVE, an extension of the LAVE metric, assessing textual accuracy, cultural alignment, and country identification. Evaluations of six open- and closed-source VLMs reveal significant performance gaps in low-resource countries and abstract cultural domains. The Visual Grounding task tests models' ability to localize culturally significant elements in complex scenes, probing spatial and contextual accuracy. RICE-VL exposes limitations in VLMs' cultural comprehension and highlights the need for inclusive model development to better serve diverse global populations.

</details>


### [179] [MDiff4STR: Mask Diffusion Model for Scene Text Recognition](https://arxiv.org/abs/2512.01422)
*Yongkun Du,Miaomiao Zhao,Songlin Fan,Zhineng Chen,Caiyan Jia,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文首次将Mask Diffusion Models（MDMs）引入场景文本识别（STR）任务，提出MDiff4STR模型，通过六种噪声策略和一种token-replacement噪声机制解决训练-推理噪声差异与推理过自信问题，在保持高效推理（仅3步去噪）的同时，精度超越主流自回归模型。


<details>
  <summary>Details</summary>
Motivation: Mask Diffusion Models在视觉-语言任务中展现出效率与精度的灵活平衡，但尚未应用于场景文本识别；而现有自回归模型虽精度高但效率低，因此需探索适用于STR的高效高质扩散模型。

Method: 提出MDiff4STR：1）设计六种针对性噪声策略以缩小训练与推理间的噪声差距；2）引入token-replacement噪声机制，提供非掩码类噪声，缓解推理阶段过自信错误预测问题。

Result: MDiff4STR在标准及挑战性STR基准（含不规则、艺术字、遮挡、中文文本等）上全面超越主流STR模型，精度超过当前最优自回归方法，同时仅需3步去噪即实现快速推理。

Conclusion: MDM可成功适配STR任务，通过针对性改进能兼顾高精度与高效率，为STR提供了新的有效建模范式。

Abstract: Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.

</details>


### [180] [\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models](https://arxiv.org/abs/2512.01424)
*Xusen Hei,Jiali Chen,Jinyu Yang,Mengchen Zhao,Yi Cai*

Main category: cs.CV

TL;DR: 本文提出了ViRectify，一个用于评估多模态大语言模型（MLLMs）在视频推理中错误识别与纠正能力的综合基准。通过AI辅助标注和人工验证构建了包含3万多个实例的数据集，涵盖动态感知、科学推理和具身决策等领域。同时提出了一种基于轨迹证据的纠错框架，鼓励模型关注错误传播和关键时间戳。实验表明ViRectify具有挑战性，且该方法能有效提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对MLLMs在复杂视频推理任务中错误识别与纠正能力的系统性评估，难以揭示其弱点并推动性能提升。因此需要一个细粒度、多场景的评测基准来推动该领域发展。

Method: 提出ViRectify基准和轨迹证据驱动的纠错框架，采用AI辅助标注加人工验证的方式构建大规模数据集，要求模型进行逐步错误识别并生成基于关键视频证据的推理依据。

Result: 在16个先进MLLM上进行了广泛评估，GPT-5在该基准上的纠错准确率仅为31.94%；所提框架使Qwen2.5-VL-7B模型性能持续超过72B版本变体，验证了其有效性。发现了模型间纠错能力的系统性不对称现象。

Conclusion: ViRectify为评估MLLMs在视频推理中的纠错能力提供了新的方向，不仅是一个具有挑战性的测试平台，也为反思学习提供了宝贵的数据资源。

Abstract: As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.

</details>


### [181] [ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers](https://arxiv.org/abs/2512.01426)
*Yiyang Ma,Feng Zhou,Xuedan Yin,Pu Cao,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出了一种无需训练的高分辨率图像生成方法ResDiT，通过位置编码缩放和局部增强机制解决扩散Transformer在高分辨率下布局崩溃和纹理失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练Diffusion Transformer的高分辨率图像生成方法常出现空间布局崩溃和纹理质量下降，且依赖复杂流程，缺乏对生成机制的深入理解。

Method: 分析DiT的生成机制，发现位置嵌入（PE）在分辨率外推时编码错误导致布局崩溃；提出PE缩放技术修正位置信息，并设计基于局部注意力的增强机制，结合块级融合模块与高斯加权拼接策略提升细节保真度。

Result: ResDiT在多个评估中均实现高质量、高分辨率图像生成，有效消除网格伪影，保持空间结构准确性，并支持空间控制生成等下游任务。

Conclusion: ResDiT是一种无需训练、高效可扩展的高分辨率生成框架，通过改进位置编码和局部细节建模，显著提升了DiT在超分辨率生成中的表现。

Abstract: Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.

</details>


### [182] [Language-Guided Open-World Anomaly Segmentation](https://arxiv.org/abs/2512.01427)
*Klara Reichard,Nikolas Brasch,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: 本文提出了Clipomaly，一种基于CLIP的零样本开放世界与异常分割方法，用于自动驾驶，无需异常训练数据即可对未知物体进行分割并赋予人类可理解的语义名称，且能在推理时动态扩展词汇表。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界和异常分割方法无法为未知区域分配语义上有意义的标签，且难以区分和学习未知类别的表示；开放词汇分割方法虽能泛化到新类别，但受限于固定推理词汇表，不适用于无约束未知类别的异常分割任务。

Method: 提出Clipomaly，利用CLIP共享的图像-文本嵌入空间实现零样本异常分割与语义命名；在推理时动态扩展词汇表，无需重新训练。

Result: 在主流异常分割基准上达到SOTA性能，同时提供可解释性和部署灵活性。

Conclusion: Clipomaly是首个将CLIP应用于开放世界与异常分割的自动驾驶方法，实现了无需异常数据训练、动态词汇扩展、未知物体分割与语义命名的统一框架。

Abstract: Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.

</details>


### [183] [FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation](https://arxiv.org/abs/2512.01444)
*Jian Shu,Nanjie Yao,Gangjian Zhang,Junlong Ren,Yu Feng,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的基于学习的框架，用于3D人体头像动画，通过U-Net解耦纹理与姿态信息以快速生成模板，并引入数据驱动的细化技术提升形变结构完整性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模板构建中依赖繁重骨骼绑定且易产生姿态相关伪影，而目标姿态变形受线性混合蒙皮（LBS）导致的结构失真影响，降低了动画真实性。

Method: 采用两阶段统一学习框架：第一阶段用U-Net架构在前馈过程中解耦纹理与姿态信息，实现高效模板生成；第二阶段引入数据驱动的形变细化技术以增强结构完整性。

Result: 实验表明该模型在多种姿态下保持一致性能，在效率与质量间取得最优平衡，性能超越当前最先进方法。

Conclusion: 所提框架有效解决了模板构建低效/伪影与LBS形变失真两大核心问题，为高质量、高效率3D人体动画提供了新范式。

Abstract: 3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.

</details>


### [184] [ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling](https://arxiv.org/abs/2512.01481)
*Qisen Wang,Yifan Zhao,Peisen Shen,Jialu Li,Jia Li*

Main category: cs.CV

TL;DR: 本文提出ChronosObserver，一种无需训练的方法，通过World State Hyperspace建模4D世界时空约束，并利用Hyperspace Guided Sampling同步多视角扩散采样轨迹，实现高保真、3D一致且时间同步的多视角视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有相机控制的视频生成模型难以直接扩展到生成3D一致且高保真、时间同步的多视角视频，而数据增强或测试时优化等方法受限于泛化性和可扩展性。

Method: 提出ChronosObserver方法，包含World State Hyperspace（表示4D场景时空约束）和Hyperspace Guided Sampling（利用超空间同步多视角扩散采样轨迹），无需训练或微调扩散模型。

Result: 实验表明该方法能生成高保真、3D一致且时间同步的多视角视频，且无需对扩散模型进行训练或微调。

Conclusion: ChronosObserver为生成4D世界内容提供了一种高效、通用且无需训练的新范式，突破了现有方法在泛化性与可扩展性上的瓶颈。

Abstract: Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.

</details>


### [185] [A variational method for curve extraction with curvature-dependent energies](https://arxiv.org/abs/2512.01494)
*Majid Arthaud,Antonin Chambolle,Vincent Duval*

Main category: cs.CV

TL;DR: 提出了一种基于能量离散化和Smirnov分解定理的变分方法，用于从图像中自动提取曲线和一维结构，该方法扩展到曲率依赖的能量，并通过在位置和方向空间中的提升来实现。


<details>
  <summary>Details</summary>
Motivation: 为了实现图像中曲线和1D结构的自动、大多无监督的提取，需要一种能够处理复杂几何形状和曲率依赖能量的方法。

Method: 采用基于能量离散化的变分方法，结合Smirnov的向量场分解定理；进一步将曲线提升至位置-方向空间，使用次黎曼或芬斯勒度量来处理曲率相关的能量。

Result: 设计出一种双层最小化方法，能有效且大多无需监督地从图像中提取出曲线和1D结构，适用于曲率依赖的能量模型。

Conclusion: 该方法为图像中低维结构的提取提供了一个灵活且强大的框架，尤其适合处理涉及曲率的复杂情况。

Abstract: We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.

</details>


### [186] [ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark](https://arxiv.org/abs/2512.01495)
*Joanne Lin,Ruirui Lin,Yini Li,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文提出ELVIS框架，通过无监督合成低光视频流水线、无需校准的退化剖面合成网络（VDP-Net）和解耦退化与内容特征的增强解码头，提升现有VIS模型在低光视频上的域适应能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 低光视频实例分割面临噪声、模糊、低对比度等成像挑战；缺乏大规模标注数据集及难以建模时间退化的合成方法；现有VIS方法对低光退化鲁棒性差，微调后效果仍不佳。

Method: 提出ELVIS框架，包含三部分：1）建模时空退化的无监督低光视频合成流水线；2）无需校准的退化剖面合成网络（VDP-Net）；3）能解耦退化与内容特征的增强解码头。

Result: 在合成低光YouTube-VIS 2019数据集上AP提升达+3.7。

Conclusion: ELVIS有效提升了VIS模型在低光场景下的域适应能力，为低光视频实例分割提供了新思路与实用框架。

Abstract: Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \textbf{ELVIS} (\textbf{E}nhance \textbf{L}ow-light for \textbf{V}ideo \textbf{I}nstance \textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.

</details>


### [187] [Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2512.01510)
*Franz Thaler,Martin Urschler,Mateusz Kozinski,Matthias AF Gsell,Gernot Plank,Darko Stern*

Main category: cs.CV

TL;DR: 本文提出SRCSM方法，通过语义感知的随机卷积增强源域多样性，并在测试时对目标域图像进行强度映射以提升单源域泛化能力，在多种跨模态和跨中心医学图像分割任务中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决单源域医学图像分割中的域泛化问题，即仅用一种模态（如CT）训练模型，直接应用于另一种模态（如MR），无需目标域数据或模型自适应。

Method: 提出SRCSM方法：训练阶段采用基于标注标签的语义感知随机卷积进行区域差异化增强；测试阶段对目标域图像进行强度映射，使其更接近源域分布。

Result: 在腹部、全心脏和前列腺的跨模态与跨中心分割任务中大幅超越现有DG方法；在更难的全心脏CT/MR训练→ cine MR时相数据测试任务中也显著缩小域差距；部分设置下性能媲美域内基线。

Conclusion: SRCSM是医学图像分割单源域泛化的新SOTA方法，兼具强泛化能力与实际部署友好性。

Abstract: We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.

</details>


### [188] [QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions](https://arxiv.org/abs/2512.01519)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.CV

TL;DR: 本文提出QuantumCanvas，一个以双体量子系统为基本单元的大规模多模态基准数据集，旨在提升分子和材料机器学习模型的物理可迁移性；该数据集包含2850种元素对，每对标注18种电子、热力学和几何性质，并配以基于轨道密度等物理量生成的十通道图像；实验表明多种模型在多个量子性质预测任务上取得优异性能，且预训练可提升在QM9等下游数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有分子与材料机器学习模型大多缺乏物理可迁移性，仅拟合整体分子或晶体的相关性，而未能建模决定成键、电荷重分布、轨道杂化等关键量子现象的原子对间两体相互作用。

Method: 构建QuantumCanvas基准：涵盖2850种元素对，每对标注18类量子性质，并生成十通道物理图像（含l/m分辨轨道密度、角场变换、共占据图、电荷密度投影等）；评估8种模型在18个目标上的性能，并验证其在QM9、MD17、CrysMTM等数据集上的迁移效果。

Result: GATv2在能隙预测上MAE为0.201 eV；EGNN在HOMO/LUMO预测上MAE分别为0.265/0.274 eV；DimeNet在总能量和排斥能上MAE分别为2.27/0.132 eV；多模态融合模型在Mermin自由能上MAE为2.15 eV；预训练显著提升下游任务收敛稳定性与泛化性。

Conclusion: QuantumCanvas通过将轨道物理与视觉表征学习统一，为学习可迁移、可解释的量子相互作用提供了原理性基础，推动机器学习模型从黑箱相关性拟合迈向物理驱动的因果建模。

Abstract: Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.

</details>


### [189] [Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling](https://arxiv.org/abs/2512.01533)
*Hailong Yang,Te Zhang,Kup-sze Choi,Zhaohong Deng*

Main category: cs.CV

TL;DR: 本文提出了一种基于模糊规则引导的潜在空间多路径扩散模型——Diffusion Fuzzy System (DFS)，旨在解决传统扩散模型在处理特征差异显著图像集合时表现不佳的问题。DFS通过为不同图像特征分配专用扩散路径、引入规则链推理实现路径协同，并采用模糊隶属度驱动的潜在空间压缩机制降低计算开销，在多个数据集上实现了更稳定训练、更快收敛及更优图文对齐与图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在图像生成中表现出色，但在处理特征差异显著的图像集合时仍存在难以捕捉复杂特征、生成结果冲突等问题；现有基于多路径的改进方法又面临路径协调低效和计算成本高的挑战。

Method: 提出Diffusion Fuzzy System（DFS）：1）在潜在空间中构建多个专用扩散路径，每条路径学习一类特定图像特征；2）引入基于模糊规则链的推理机制，动态引导并协调各路径扩散过程；3）设计模糊隶属度驱动的潜在空间压缩机制以降低多路径计算开销。

Result: 在LSUN Bedroom、LSUN Church和MS COCO三个公开数据集上的实验表明，DFS相比单路径与现有多路径扩散模型具有更稳定的训练过程和更快的收敛速度；在图像质量、图文对齐度以及生成图像与目标参考图像的匹配精度方面均优于基线模型。

Conclusion: DFS通过特征导向的多路径设计、规则链推理协调机制和模糊压缩策略，有效提升了扩散模型对异质图像特征的建模能力与计算效率，为多路径扩散建模提供了新范式。

Abstract: Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.

</details>


### [190] [Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis](https://arxiv.org/abs/2512.01534)
*Alexander Frotscher,Christian F. Baumgartner,Thomas Wolfers*

Main category: cs.CV

TL;DR: 本文提出了一个大规模、多中心的深度无监督脑影像异常检测基准，系统评估了多种算法在不同扫描仪、病变类型与大小、人口统计学因素下的性能与鲁棒性，发现现有方法普遍存在扫描仪偏差、对小/低对比度病灶漏检、假阳性受年龄性别影响等问题，并指出算法瓶颈大于数据瓶颈，强调需发展原生图像预训练、合理偏离度量、公平建模与鲁棒域自适应。


<details>
  <summary>Details</summary>
Motivation: 解决脑MRI无监督异常检测领域因评估碎片化、数据异构、指标不一致而阻碍临床转化的问题。

Method: 构建包含近6000例健康T1/T2加权MRI的多中心训练集，92例验证集用于超参调优与阈值估计，2221+1262例测试集涵盖健康与多种临床队列；系统评估Dice分割性能，并分析扫描仪、病变类型/大小、年龄/性别等因素对鲁棒性的影响；比较重建类（尤其是扩散启发方法）与特征类方法的表现差异。

Result: 所有算法Dice性能介于0.03–0.65；重建类方法（尤其扩散启发）分割性能最强，特征类方法在分布偏移下更鲁棒；多数算法存在扫描仪偏差，小/低对比度病灶易漏检，假阳性率随年龄和性别变化；增加健康训练数据仅带来有限提升。

Conclusion: 当前无监督异常检测的瓶颈在于算法设计而非数据量；需推动图像原生预训练、可解释偏离度量、公平性建模与鲁棒域自适应，以支撑临床落地。

Abstract: Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.

</details>


### [191] [FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention](https://arxiv.org/abs/2512.01540)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: 本文提出FlashVGGT，一种高效的3D重建模型，通过基于描述符的注意力机制减少计算开销，支持长序列图像的在线推理，并在保持精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于全自注意力的3D重建模型（如VGGT）因二次复杂度和大量图像令牌导致可扩展性差，难以处理长序列图像。

Method: 引入描述符令牌压缩每帧的空间信息，使用图像令牌与描述符集之间的交叉注意力替代全局自注意力，并采用块递归机制缓存并复用描述符以实现长序列的在线推理。

Result: 在1000张图像上推理时间仅为VGGT的9.3%，且能高效扩展到超过3000张图像的序列，重建精度与VGGT相当。

Conclusion: FlashVGGT通过描述符注意力机制和递归缓存策略，在保持高精度的同时极大提升了多视角3D重建的效率和可扩展性。

Abstract: 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.

</details>


### [192] [MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration](https://arxiv.org/abs/2512.01563)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Nguyen Chi Thanh,Truong Hoang Viet,Trong-Le Do,Mai-Khiem Tran,Minh-Khoi Pham,Trung-Nghia Le,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: 本文提出了一个新的头颈部肿块分割数据集MasHeNe，包含3779张带像素级标注的增强CT切片，涵盖肿瘤和囊肿，并提出了一种基于Mamba架构的Windowing-Enhanced Mamba with Frequency integration (WEMF)模型，在该数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集主要关注恶性病变，忽略了头颈部其他占位性病变（如囊肿），缺乏全面评估分割模型能力的数据支持，因此需要一个更全面、多样化的数据集及相应基准。

Method: 构建了包含肿瘤和囊肿的MasHeNe数据集，采用三窗增强技术提升输入表观信息，并设计WEMF模型，结合多频率注意力机制与U形Mamba骨干网络，融合跳跃连接中的多频特征进行分割。

Result: WEMF在MasHeNe上表现最佳，Dice为70.45%，IoU为66.89%，NSD为72.33%，HD95为5.12mm，显示出稳定且强大的性能，但误差分析表明该任务仍具挑战性。

Conclusion: MasHeNe填补了非仅恶性头颈部肿块分割数据的空白，提供了新的基准；WEMF模型有效提升了分割效果，推动了该领域的发展，但仍需进一步研究以应对剩余挑战。

Abstract: Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.

</details>


### [193] [RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions](https://arxiv.org/abs/2512.01582)
*Junran Peng,Yiheng Huang,Silei Shen,Zeji Wei,Jingwei Yang,Baojie Wang,Yonghao He,Chuanchen Luo,Man Zhang,Xucheng Yin,Wei Sui*

Main category: cs.CV

TL;DR: 本文介绍了RoleMotion，一个大规模、高质量的人类运动数据集，专注于角色扮演与功能性动作，覆盖25个经典场景、110种功能角色、500+行为及10296条全身（含手部）运动序列，并配备27831条细粒度文本描述；同时构建了更强的评估器，验证了现有文本驱动运动生成方法的性能，并探索了身体与手部运动协同生成问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本-运动数据集存在结构松散、功能缺失、场景覆盖不全、动作质量参差、文本标注粗粒度等问题，难以支撑社会性、多角色、多场景下的文本驱动全身运动生成研究。

Method: 设计并构建了面向场景与角色的高质量人类运动数据集RoleMotion，包含精细划分的场景、角色、行为体系；采集并标注大量高保真全身运动序列（含手部）及细粒度文本描述；构建强于现有方法的评估器，并在该数据集上系统评测主流文本到运动方法；进一步探究身体与手部运动的联合生成机制。

Result: RoleMotion数据集包含25个场景、110个角色、500+行为、10296条高质量运动序列和27831条细粒度文本描述；所建评估器经验证更可靠；实验表明当前文本到运动方法在该数据集上仍有较大提升空间，且身体与手部运动需协同建模。

Conclusion: RoleMotion填补了面向功能角色与真实场景的高质量全身运动数据集空白，为文本驱动运动生成提供了新基准与研究方向，尤其推动了手部与身体协同生成的研究进展。

Abstract: In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.

</details>


### [194] [Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation](https://arxiv.org/abs/2512.01589)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Trong-Le Do,Truong Hoang Viet,Nguyen Chi Thanh,Huynh Nguyen Thuan,Do Vo Cong Nguyen,Minh-Khoi Pham,Mai-Khiem Tran,Viet-Tham Huynh,Trong-Thuan Nguyen,Trung-Nghia Le,Vo Thanh Toan,Tam V. Nguyen,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: 本文介绍了AbscessHeNe数据集，一个包含4926张增强CT切片的头颈部脓肿标注数据集，旨在支持语义分割模型开发以精准勾画脓肿边界并评估深颈间隙受累情况；同时评估了多种主流分割模型性能，最高Dice系数仅0.39，凸显任务难度，并为未来多媒体索引与病例检索提供结构化基础。


<details>
  <summary>Details</summary>
Motivation: 头颈部脓肿若未及时诊断和处理可能导致败血症或死亡，而影像学上精准检测与勾画病变对临床决策至关重要，但目前缺乏高质量、大规模、精细标注的专用数据集。

Method: 构建并发布AbscessHeNe数据集，含4926张对比增强CT切片及像素级脓肿标注与临床元数据；采用CNN、Transformer和Mamba等前沿语义分割模型进行基准测试，并评估Dice相似系数、IoU和归一化表面距离等指标。

Result: 最佳模型取得Dice 0.39、IoU 0.27、NSD 0.67的结果，表明该任务极具挑战性；数据集结构支持后续内容检索与病例检索应用。

Conclusion: AbscessHeNe填补了头颈部脓肿影像分析领域高质量标注数据集的空白，为算法研发与临床智能系统构建提供了重要资源，但当前模型性能仍有限，亟需进一步研究提升分割精度。

Abstract: Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.

</details>


### [195] [Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager](https://arxiv.org/abs/2512.01611)
*Fengfeng Li,Zhou Feng,Hongliang Wu,Hao Zhang,Han Tian,Peng Liu,Lixin Yuan*

Main category: cs.CV

TL;DR: 提出一种基于ShapeDTW算法的井下图像深度匹配方法，有效解决油基泥浆微电阻率成像中上下 pad 图像间的深度错位问题。


<details>
  <summary>Details</summary>
Motivation: 在使用油基泥浆微电阻率成像的测井作业中，即使经过速度校正，上下pad集之间的图像仍存在深度错位问题，影响成像质量与地质解释准确性。

Method: 采用Shape Dynamic Time Warping (ShapeDTW)算法，结合一维梯度方向直方图（HOG1D）与原始信号构成复合特征作为形状描述子，构建对形态敏感的距离矩阵，以提高序列对齐时的结构相似性保留能力。

Result: 现场测试结果表明，该方法能够精确对齐具有复杂纹理、深度偏移或局部缩放的图像，并展现出良好的鲁棒性与灵活性。

Conclusion: 所提方法能有效解决OBM微电阻率成像中的深度错位问题，且提供可扩展的特征融合框架，适用于不同地质条件下的图像对齐任务。

Abstract: In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.

</details>


### [196] [Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2512.01636)
*Xin Wang,Haipeng Zhang,Mang Li,Zhaohui Xia,Yueguo Chen,Yu Zhang,Chunyu Wei*

Main category: cs.CV

TL;DR: 本文提出Fusion-Diff，一种用于零样本组合图像检索（ZS-CIR）的生成式编辑框架，通过在联合视觉-语言空间中进行多模态融合特征编辑，并引入轻量级Control-Adapter实现高效微调，在仅200K合成样本上即达最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本CIR方法难以有效弥合视觉与语言模态间的鸿沟，且监督方法依赖昂贵的三元组标注，亟需高效、无需人工标注的解决方案。

Method: 提出Fusion-Diff框架：1）在联合视觉-语言空间中实施多模态融合特征编辑；2）引入轻量级Control-Adapter，支持仅用200K合成数据微调。

Result: 在CIRR、FashionIQ和CIRCO等标准基准上显著超越先前零样本方法，并通过可视化融合表征提升了模型可解释性。

Conclusion: Fusion-Diff在有效性与数据效率间取得良好平衡，为零样本CIR提供了新范式，验证了联合空间多模态对齐与轻量适配器设计的关键作用。

Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.

</details>


### [197] [ViT$^3$: Unlocking Test-Time Training in Vision](https://arxiv.org/abs/2512.01643)
*Dongchen Han,Yining Li,Tianyu Li,Zixuan Cao,Ziming Wang,Jun Song,Yu Cheng,Bo Zheng,Gao Huang*

Main category: cs.CV

TL;DR: 本文对视觉序列建模中的测试时训练（TTT）设计进行了系统性实证研究，提出了六条实用见解，并据此构建了具有线性复杂度和并行计算能力的纯TTT架构ViT³，在多种视觉任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管TTT在序列建模中展现出潜力，但在视觉任务中缺乏对内模块和内训练设计的深入理解与指导原则，限制了其发展。

Method: 通过一系列实验与分析，系统研究不同TTT设计选择的影响，总结出六项关键设计原则，并基于此提出Vision Test-Time Training (ViT³) 模型。

Result: ViT³在图像分类、生成、目标检测和语义分割等任务上均达到或超过了现有先进线性复杂度模型（如Mamba和线性注意力变体）的性能，缩小了与优化良好的视觉Transformer之间的差距。

Conclusion: 该研究为视觉TTT模型的设计提供了实用指南和有效基线，有望推动未来在该方向的研究进展。

Abstract: Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.

</details>


### [198] [DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation](https://arxiv.org/abs/2512.01657)
*Hongyu Xu,Panpan Meng,Meng Wang,Dayu Hu,Liming Liang,Xiaoqi Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种自适应双分支Kolmogorov-Arnold UNet（DB-KAUNet），结合CNN与Transformer双路径，并引入KANConv、KAT块及跨分支通道交互、空间特征增强与几何自适应融合模块，显著提升视网膜血管分割精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在视网膜血管分割中难以建模长程依赖和复杂非线性关系，亟需更强大的特征表达能力。

Method: 设计异构双分支编码器（HDBE），融合CNN与Transformer路径；引入KANConv和KAT块；嵌入跨分支通道交互（CCI）模块与空间特征增强（SFE）模块；进一步提出几何自适应融合的SFE-GAF模块，采用自适应采样聚焦真实血管形态。

Result: 在DRIVE、STARE和CHASE_DB1数据集上取得领先分割性能，具备优异鲁棒性。

Conclusion: DB-KAUNet通过协同建模局部与全局特征、增强通道与空间交互、实现几何感知的自适应融合，有效克服了传统方法的局限，为医学图像分割提供了新范式。

Abstract: Accurate segmentation of retinal vessels is crucial for the clinical diagnosis of numerous ophthalmic and systemic diseases. However, traditional Convolutional Neural Network (CNN) methods exhibit inherent limitations, struggling to capture long-range dependencies and complex nonlinear relationships. To address the above limitations, an Adaptive Dual Branch Kolmogorov-Arnold UNet (DB-KAUNet) is proposed for retinal vessel segmentation. In DB-KAUNet, we design a Heterogeneous Dual-Branch Encoder (HDBE) that features parallel CNN and Transformer pathways. The HDBE strategically interleaves standard CNN and Transformer blocks with novel KANConv and KAT blocks, enabling the model to form a comprehensive feature representation. To optimize feature processing, we integrate several critical components into the HDBE. First, a Cross-Branch Channel Interaction (CCI) module is embedded to facilitate efficient interaction of channel features between the parallel pathways. Second, an attention-based Spatial Feature Enhancement (SFE) module is employed to enhance spatial features and fuse the outputs from both branches. Building upon the SFE module, an advanced Spatial Feature Enhancement with Geometrically Adaptive Fusion (SFE-GAF) module is subsequently developed. In the SFE-GAF module, adaptive sampling is utilized to focus on true vessel morphology precisely. The adaptive process strengthens salient vascular features while significantly reducing background noise and computational overhead. Extensive experiments on the DRIVE, STARE, and CHASE_DB1 datasets validate that DB-KAUNet achieves leading segmentation performance and demonstrates exceptional robustness.

</details>


### [199] [Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery](https://arxiv.org/abs/2512.01665)
*Zhicheng Zhao,Yin Huang,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了ScaleBridge-Det，首个面向遥感图像中微小物体检测的大规模检测框架，通过尺度自适应专家路由和密度引导的查询分配，实现了对多尺度物体的均衡检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中存在大量密集的微小物体与大尺寸物体共存的情况，现有方法难以在不同尺度上保持平衡的检测性能，且大模型在该领域的应用尚未探索。

Method: 提出ScaleBridge-Det框架，包含两个核心模块：1）路由增强混合注意力（REM）模块，通过自适应路由选择并融合尺度特定的专家特征，缓解标准MoE模型偏向主导尺度的问题；2）密度引导动态查询（DGQ）模块，根据预测的物体密度自适应调整查询的位置和数量，实现对不同尺度物体的高效资源分配。

Result: 在AI-TOD-V2和DTOD数据集上达到最先进的检测性能，并在VisDrone上表现出优异的跨域鲁棒性。

Conclusion: ScaleBridge-Det是首个专为微小物体检测设计的大规模检测框架，通过REM和DGQ模块有效解决了遥感图像中多尺度、高密度带来的挑战，实现了无需权衡的多尺度物体同步优化检测。

Abstract: Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.

</details>


### [200] [GRASP: Guided Residual Adapters with Sample-wise Partitioning](https://arxiv.org/abs/2512.01675)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出了一种名为GRASP的新方法，用于改善长尾分布下文本到图像扩散模型对罕见类别的生成质量与多样性，通过引入基于外部先验的样本聚类和残差适配器来缓解梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理长尾分布数据（如医学影像中的罕见病理）时表现不佳，容易发生模式崩溃，导致罕见类别生成效果差，限制了其在数据增强中的应用。

Method: 提出GRASP方法：利用外部先知静态地将样本划分为减少组内梯度冲突的簇，并在Transformer的前馈层中注入簇特定的残差适配器进行微调，避免使用学习门控机制以提升稳定性和效率。

Result: 在MIMIC-CXR-LT长尾数据集上，GRASP显著优于基线方法（如普通微调和专家混合变体），在FID和多样性指标上表现更优，尤其提升罕见类别的生成效果；在NIH-CXR-LT上的下游分类任务中也取得明显改进，并在ImageNet-LT上验证了泛化能力。

Conclusion: GRASP是一种轻量、可扩展且易于集成到现有扩散模型管道中的方法，有效解决了长尾场景下的梯度冲突问题，提升了罕见类别的生成质量与多样性，具有广泛的应用潜力。

Abstract: Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.

</details>


### [201] [Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation](https://arxiv.org/abs/2512.01677)
*Haodong Yan,Hang Yu,Zhide Zhong,Weilin Yuan,Xin Gong,Zehang Luo,Chengxi Heyu,Junfeng Li,Wenxuan Song,Shunbo Zhou,Haoang Li*

Main category: cs.CV

TL;DR: 本文提出了一种结构与接触感知的表示方法（SCAR），用于生成物理真实、时间连贯的手-物交互（HOI）视频，无需3D标注，兼顾可扩展性与交互保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在2D与3D HOI表示间存在权衡困境：难以同时保证可扩展性和交互物理保真度；且难以建模接触、遮挡等物理约束。

Method: 提出结构与接触感知表示（SCAR），建模手-物接触、遮挡及整体结构上下文；设计共享-特化联合生成范式，协同生成该表示与视频。

Result: 在两个真实数据集上显著超越SOTA，在物理真实性、时间一致性及开放世界泛化性方面表现优异。

Conclusion: SCAR提供了一种无需3D标注、可扩展且高保真的HOI监督信号，有效提升了视频生成中细粒度交互物理建模能力与泛化性。

Abstract: Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.

</details>


### [202] [Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies](https://arxiv.org/abs/2512.01681)
*Farzaneh Seyedshahi,Francesca Damiola,Sylvie Lantuejoul,Ke Yuan,John Le Quesne*

Main category: cs.CV

TL;DR: 本文提出了一种基于自监督学习的计算病理学方法，利用大体切除组织训练编码器，并成功迁移到小活检样本上，用于恶性间皮瘤的亚型分类和生存预测。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理模型多依赖大体积切除组织图像，难以适用于临床常见的小活检样本，亟需可迁移、适应小样本的AI诊断工具。

Method: 采用自监督学习方式在切除组织图像上预训练特征编码器，再将其迁移应用于小活检图像，提取形态学特征以实现亚型分类与生存预测。

Result: 该模型在小活检样本上成功实现了肿瘤亚型分类和患者生存预测，验证了跨样本类型（切除→活检）特征迁移的有效性。

Conclusion: 自监督预训练编码器具备良好的泛化能力，可有效支持恶性间皮瘤的小活检病理诊断与个体化治疗决策。

Abstract: Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.

</details>


### [203] [DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models](https://arxiv.org/abs/2512.01686)
*Patrick Kwon,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了DreamingComics，一种布局感知的故事可视化框架，通过改进的视频扩散Transformer模型、区域感知位置编码（RegionalRoPE）和掩码条件损失，显著提升了角色一致性和风格相似性。


<details>
  <summary>Details</summary>
Motivation: 现有故事可视化方法仅依赖文本定位主体，难以保持艺术风格一致性。

Method: 基于预训练视频扩散Transformer（DiT），提出RegionalRoPE区域感知位置编码，并引入掩码条件损失约束主体视觉特征；结合LLM布局生成器，从自然语言脚本推断漫画式布局。

Result: 相比先前方法，角色一致性提升29.2%，风格相似性提升36.2%，且空间定位精度高。

Conclusion: DreamingComics有效解决了故事可视化中身份与风格一致性难题，实现了布局可控、高质量的图像生成。

Abstract: Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/

</details>


### [204] [SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation](https://arxiv.org/abs/2512.01701)
*Xiuli Bi,Die Xiao,Junchao Fan,Bin Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种语义与空间校正（SSR）方法，通过跨模态原型对齐（CMPA）和超像素引导校正（SGC）解决CLIP-based WSSS中前景非目标区域和背景的过激活问题，在PASCAL VOC和MS COCO上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的弱监督语义分割方法存在非目标前景区域和背景区域过激活的问题。

Method: 提出语义与空间校正（SSR）方法：语义层面采用跨模态原型对齐（CMPA）进行对比学习以对齐多模态特征空间；空间层面采用超像素引导校正（SGC）利用超像素空间先验在亲和传播中过滤非目标区域干扰。

Result: 在PASCAL VOC和MS COCO数据集上mIoU分别达79.5%和50.6%，优于所有单阶段及更复杂的多阶段方法。

Conclusion: SSR方法能有效缓解CLIP-based WSSS中的过激活问题，显著提升分割精度。

Abstract: In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.

</details>


### [205] [FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing](https://arxiv.org/abs/2512.01755)
*Yucheng Liao,Jiajun Liang,Kaiqian Cui,Baoquan Zhao,Haoran Xie,Wei Liu,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: 本文提出FreqEdit，一种无需训练的框架，通过高频信息注入、自适应注入策略和路径补偿机制，解决基于指令的图像编辑在多轮编辑中的质量退化问题，显著提升编辑稳定性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的图像编辑模型在多轮连续编辑时会出现严重的质量退化，主要表现为细节丢失，本文旨在解决这一问题。

Method: 提出FreqEdit框架，包含三个核心组件：1）从参考速度场注入高频特征以保留细节；2）自适应注入策略，实现空间上局部区域的精确控制；3）路径补偿机制，周期性校正编辑路径，避免过度约束。该方法无需额外训练。

Result: 实验表明，FreqEdit在超过10轮连续编辑中保持稳定性能，在身份保持和指令遵循方面优于七个最先进的基线模型。

Conclusion: FreqEdit有效缓解了多轮语言指导图像编辑中的高频信息丢失问题，实现了高质量、长序列的稳定编辑，为实际应用提供了可行方案。

Abstract: Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.

</details>


### [206] [HiconAgent: History Context-aware Policy Optimization for GUI Agents](https://arxiv.org/abs/2512.01763)
*Xurui Zhou,Gongwei Chen,Yuquan Xie,Zaijing Li,Kaiwen Zhou,Shuai Wang,Shuo Yang,Zhuotao Tian,Rui Shao*

Main category: cs.CV

TL;DR: 本文提出HiconAgent，一种通过历史上下文感知策略优化（HCPO）训练的GUI智能体，包含动态上下文采样（DCS）和锚点引导的历史压缩（AHC）两个组件，以高效利用历史信息，在多个GUI导航基准上实现更高精度、更快速度和更低计算开销。


<details>
  <summary>Details</summary>
Motivation: GUI智能体在顺序导航任务中需有效利用历史上下文，但直接使用全部历史会导致计算开销大且引入无关信息干扰。

Method: 提出History Context-aware Policy Optimization（HCPO），包含两个核心组件：(1) Dynamic Context Sampling（DCS），在采样时动态调整历史长度；(2) Anchor-guided History Compression（AHC），在策略更新中采用双分支结构，以历史动作为锚点压缩观测，并通过历史增强对齐损失保持一致性。

Result: HiconAgent-3B在GUI-Odyssey上较GUI-R1-7B提升8.46%定位准确率和11.32%步成功率达，同时在AndroidControl和AITW上达到相当性能，并实现最高2.47倍加速与60% FLOPs降低。

Conclusion: HCPO能显著提升GUI智能体对历史信息的利用效率与效果，HiconAgent在精度、速度与资源消耗间取得更好平衡。

Abstract: Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.

</details>


### [207] [VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis](https://arxiv.org/abs/2512.01769)
*Hafsa Billah*

Main category: cs.CV

TL;DR: 本文提出了一种通用的视频情境分析（VSA）框架，结合扩展关系模型（R++）与图模型，支持连续查询处理和跨领域情境检测，无需为每个新场景或领域定制算法。


<details>
  <summary>Details</summary>
Motivation: 现有视频分析方法难以自动识别有意义的情境（如两个物体靠近），依赖人工或专用算法，缺乏通用性和可扩展性。

Method: 提出通用VSA框架：1）利用先进视频内容提取技术一次性提取内容；2）采用R++模型支持连续查询处理；3）引入图模型增强复杂情境检测能力；4）设计参数化模板实现跨领域泛化。

Result: 在辅助生活（AL）、公民监控（CM）和普通监控（SL）三个领域开展大量实验，验证了该方法在准确性、效率和鲁棒性方面的优越性。

Conclusion: 所提框架克服了现有方法的局限性，实现了通用、高效、鲁棒且领域无关的视频情境分析。

Abstract: Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.
  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.

</details>


### [208] [Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels](https://arxiv.org/abs/2512.01771)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Malik Galijasevic,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 本文提出了一种结合可学习边缘核与基于学习的刚性/非刚性配准方法，通过自适应边缘检测提升多模态医学图像配准性能。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像配准方法在处理对比度差异、空间畸变和模态差异时存在困难，亟需更鲁棒、结构感知更强的配准方法。

Method: 引入预定义边缘检测核并叠加随机噪声，使其在训练中可学习地优化以提取任务导向的边缘特征；结合学习型刚性与非刚性配准框架，并设计多组消融变体模型进行模块贡献分析。

Result: 在医学院数据集（含三种配准设置）及两个公开数据集上，该方法持续超越现有最先进方法，显著提升多模态图像对齐与解剖结构分析效果。

Conclusion: 可学习边缘核能有效增强配准模型对关键结构特征的感知能力，所提方法为医学图像配准提供了更鲁棒、可解释且高性能的新范式。

Abstract: Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.

</details>


### [209] [Evaluating SAM2 for Video Semantic Segmentation](https://arxiv.org/abs/2512.01774)
*Syed Hesham Syed Ariff,Yun Liu,Guolei Sun,Jing Yang,Henghui Ding,Xue Geng,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文探讨了将SAM2模型扩展用于视频语义分割（VSS）的两种方法，旨在提升空间精度、时间一致性及多目标复杂边界的跟踪能力；实验表明SAM2能显著提升VSS性能，主要归功于其对物体边界的精准预测。


<details>
  <summary>Details</summary>
Motivation: SAM2虽在视频目标分割中表现出色，但直接扩展至需更高空间精度、时间一致性和多目标处理能力的视频语义分割（VSS）面临挑战。

Method: 提出两种扩展SAM2用于VSS的方法：1）并行使用SAM2提取对象掩码与分割网络生成/优化初始预测；2）用SAM2预测掩码提取特征向量，输入简单分类网络，再融合分类结果与掩码输出最终分割。

Result: 实验表明，利用SAM2可提升VSS整体性能，尤其在物体边界预测精度方面优势明显。

Conclusion: SAM2作为基础模型，在适配VSS任务时展现出潜力，两种集成策略均验证了其对提升分割精度和鲁棒性的有效性，但也揭示了时空一致性与多尺度目标处理等共性挑战。

Abstract: The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.

</details>


### [210] [Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks](https://arxiv.org/abs/2512.01788)
*Christian Mollière,Iker Cumplido,Marco Zeulner,Lukas Liesenhoff,Matthias Schubert,Julia Gottfriedsen*

Main category: cs.CV

TL;DR: 本文评估了在卫星地球观测数据快速增长背景下，任务特定的学习型压缩算法在减少数据量同时保留关键信息方面的潜力，比较了传统压缩与学习型压缩在三种遥感分割任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于卫星遥感数据迅速增长，传统数据传输和存储面临挑战，需要高效压缩方法在减小数据体积的同时保留对下游任务至关重要的信息。

Method: 比较了传统压缩算法（JPEG 2000）与学习型压缩算法（离散化混合高斯似然）在火灾、云层和建筑物检测三个EO分割任务上的表现，并评估其在重建质量（PSNR）和分割精度上的差异，同时探讨了端到端联合优化的效果。

Result: 学习型压缩在大规模多通道光学遥感影像上显著优于JPEG 2000，但在小规模单通道热红外数据上传统方法仍具竞争力；联合端到端优化未带来性能提升。

Conclusion: 学习型压缩在处理大规模多通道遥感数据时更具优势，适用于需高保真信息的任务，而传统压缩在数据有限的情况下仍有效，且独立优化压缩与分割模型已足够。

Abstract: The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.

</details>


### [211] [SAM3-UNet: Simplified Adaptation of Segment Anything Model 3](https://arxiv.org/abs/2512.01789)
*Xinyu Xiong,Zihuang Wu,Lei Lu,Yufa Xia*

Main category: cs.CV

TL;DR: 本文提出了SAM3-UNet，一种简化版的SAM3模型，用于低成本适配下游任务，包含SAM3图像编码器、参数高效微调的适配器和轻量级U-Net解码器，在多个任务上优于SAM2-UNet和其他SOTA方法，并且训练内存低于6GB。


<details>
  <summary>Details</summary>
Motivation: 为了以较低成本将SAM3应用于下游任务，解决现有模型资源消耗大、适应性差的问题。

Method: 构建SAM3-UNet，由SAM3图像编码器、参数高效的适配器和轻量级U-Net风格解码器组成，实现高效微调与低内存占用。

Result: 在镜像检测和显著目标检测等任务上表现优于SAM2-UNet及其他SOTA方法，训练时批量大小为12的情况下GPU内存使用少于6GB。

Conclusion: SAM3-UNet是一种高效、轻量且性能优越的模型，适用于多种下游视觉任务，具备良好的实用性和扩展性。

Abstract: In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.

</details>


### [212] [Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos](https://arxiv.org/abs/2512.01803)
*Xavier Thomas,Youngsun Lim,Ananya Srinivasan,Audrey Zheng,Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: 本文提出了一种基于真实人类动作潜在空间的新评估指标，融合骨骼几何与外观特征，以更准确衡量生成视频中人体动作的时空合理性，并在新构建的多维度基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有纯视觉编码器和多模态大语言模型在评估生成视频中复杂人体动作时存在严重外观偏差、缺乏时间理解能力，难以识别运动动态细节和解剖学不合理性。

Method: 构建融合外观无关的人体骨骼几何特征与外观特征的动作表征空间，学习真实人类动作的潜在分布；通过计算生成视频表征与该真实分布之间的距离来量化动作质量。

Result: 在自建多维度基准上较现有最优方法提升超68%，在外部基准上表现具竞争力，且与人类感知相关性更强。

Conclusion: 该指标揭示了当前视频生成模型的关键缺陷，为视频生成研究设立了新标准。

Abstract: Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.

</details>


### [213] [Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights](https://arxiv.org/abs/2512.01816)
*Juanxi Tian,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.CV

TL;DR: 提出Envision基准和Envision-Score指标，用于评估多模态模型在因果事件序列生成中的表现，揭示现有模型在时空一致性与动态世界知识建模上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型依赖静态单图生成，导致过度拟合于静态模式匹配，难以建模随时间展开的动态过程，缺乏对因果时序和世界知识的真正理解。

Method: 构建Envision——基于世界知识和时空因果关系的1000个四阶段文本到多图像生成提示，并提出Envision-Score综合评估指标，涵盖多维度一致性、物理合理性和美学质量。对15个模型（10个专用T2I模型，5个统一多模态模型）进行评测。

Result: 专用T2I模型擅长美学渲染但缺乏内在世界知识；统一多模态模型在因果叙事连贯性上表现更优，但仍逊于闭源模型，且普遍难以解决时空一致性问题。

Conclusion: 仅关注孤立单图生成会阻碍多帧推理与生成能力，促使模型偏向静态模式匹配而非动态世界建模，限制了对世界知识的内化与应用。

Abstract: Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.

</details>


### [214] [Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling](https://arxiv.org/abs/2512.01821)
*Meng Cao,Haokun Lin,Haoyuan Li,Haoran Tang,Rongtao Xu,Dong An,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出MILO隐式空间世界建模范式和相对位置编码RePE，结合几何感知数据集GeoGen，提升多模态大语言模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要依赖文本描述调优，缺乏对视觉空间结构的理解（视觉文盲），难以真正掌握3D空间概念。

Method: 提出MILO隐式空间建模范式，引入视觉生成器提供几何感知反馈；设计RePE相对位置编码以建模相机姿态变换；构建大规模几何感知视频数据集GeoGen用于训练。

Result: 在多个基准和基线上显著提升了MLLM的空间推理能力，实现了对3D空间更整体、具身化的理解。

Conclusion: 通过将符号推理与感知经验隐式对齐，MILO为解决MLLM的空间理解瓶颈提供了新范式。

Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.

</details>


### [215] [CauSight: Learning to Supersense for Visual Causal Discovery](https://arxiv.org/abs/2512.01827)
*Yize Zhang,Meiqi Chen,Sirui Chen,Bo Peng,Yanxi Zhang,Tianyu Li,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文提出了视觉因果发现任务，构建了大规模数据集VCG-32K，并提出CauSight模型，结合因果感知推理方法，在视觉因果发现上显著超越GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 为了让AI系统具备类似人类的因果理解能力，而不仅仅是感知视觉元素的存在，需要模型能够推断不同场景中视觉实体之间的因果关系。

Method: 构建了包含3.2万张图像的Visual Causal Graph数据集（VCG-32K），并提出CauSight模型，结合Tree-of-Causal-Thought（ToCT）生成推理路径，以及基于因果奖励的强化学习来优化推理策略。

Result: CauSight在视觉因果发现任务上性能超过GPT-4.1，绝对提升达21%，性能提升超过三倍。

Conclusion: CauSight通过因果感知的推理机制，有效实现了视觉因果发现，推动AI向类人因果理解迈进，且代码、模型和数据集均已开源。

Abstract: Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.

</details>


### [216] [OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic](https://arxiv.org/abs/2512.01830)
*Songyan Zhang,Wenhui Huang,Zhan Chen,Chua Jiahao Collister,Qihang Huang,Chen Lv*

Main category: cs.CV

TL;DR: 本文提出了OpenREAD，一种基于开放性推理强化的视觉-语言模型自动驾驶框架，通过端到端的强化微调（RFT）在从高层推理到低层轨迹规划的全范围内提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调（SFT）限制了推理泛化能力，而当前的强化微调（RFT）难以应用于开放式的场景理解任务，因缺乏可量化的奖励信号。

Method: 构建大规模的思维链（CoT）标注数据，利用Qwen3大语言模型作为RFT中的评判模型（critic），对开放式问题的推理质量进行量化，并实现从感知到规划的端到端强化微调。

Result: 实验表明，所提方法在上下游任务中均带来显著性能提升，在推理与规划基准上达到最先进水平。

Conclusion: OpenREAD通过引入基于LLM的奖励建模和端到端RFT，有效提升了自动驾驶系统在开放场景下的推理与决策能力。

Abstract: Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.

</details>


### [217] [PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models](https://arxiv.org/abs/2512.01843)
*Zeqing Wang,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: 本文构建了一个物理不合理性检测数据集PID，并提出了一种轻量级微调方法PhyDetEx，使视觉语言模型能够检测并解释视频中的物理不合理现象，进而评估现有文本到视频生成模型的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）模型虽在质量、长度和指令遵循能力上进步显著，但其是否理解并遵守物理规律仍不明确；同时，通用视觉语言模型（VLMs）难以识别生成视频中的物理不合理内容。

Method: 构建PID数据集（含500个标注测试视频和2588对训练视频），提出轻量级微调方法PhyDetEx，使VLM既能检测物理不合理事件，又能生成违反的物理原理的文字解释。

Result: 利用PhyDetEx对多个SOTA T2V模型进行评测，发现尽管近期模型在物理合理性上有提升，但仍普遍存在对物理规律理解与遵循不足的问题，尤其是开源模型。

Conclusion: 物理合理性是当前T2V模型的重要短板；PhyDetEx为评估和提升模型的物理理解能力提供了有效工具和基准。

Abstract: Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) dataset, which consists of a \textit{test split} of 500 manually annotated videos and a \textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.

</details>


### [218] [COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis](https://arxiv.org/abs/2512.01853)
*Tsz-To Wong,Ching-Chun Huang,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 本文提出了一种可重构的多智能体系统（MAS）框架，用于解决体育视频分析中时间层次理解不足、泛化性差、开发成本高及可解释性弱等问题；各智能体作为专用‘认知工具’，通过灵活组合与迭代调用，支持从细粒度动作识别到宏观比赛总结的跨任务分析，在羽毛球场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端模型难以兼顾体育视频中微观动作与宏观策略的多层次时间上下文理解，且存在泛化能力弱、新任务适配成本高、决策过程不可解释等缺陷。

Method: 设计一个可重构的多智能体系统（MAS），每个智能体专精于某类分析子任务（如事件检测、语义聚合等），通过动态编排与迭代调用构建适应不同时间尺度和任务需求的分析流水线。

Result: 在羽毛球视频分析的两个典型任务（回合问答Rally QA与全场总结生成）上验证了框架的有效性，实现了细粒度事件检测与全局语义组织的统一建模。

Conclusion: 该MAS框架代表了体育视频理解范式的转变——从固定结构模型转向灵活、可扩展、可解释的模块化智能系统。

Abstract: Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page

</details>


### [219] [TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals](https://arxiv.org/abs/2512.01885)
*Florian Bürger,Martim Dias Gomes,Nica Gutu,Adrián E. Granada,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 本文提出TransientTrack，一种基于深度学习的多通道显微视频细胞追踪框架，可识别有丝分裂和凋亡等关键事件，构建含谱系信息的完整轨迹，并在化疗药物单细胞疗效分析中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有细胞追踪方法多针对单通道、恒定荧光信号视频，难以检测细胞死亡等关键动态事件，无法支持单细胞水平的群体动力学定量分析。

Method: TransientTrack采用Transformer网络进行检测嵌入匹配，结合多阶段全检测框匹配与卡尔曼滤波插补缺失轨迹段，无需提取特定追踪特征，直接在细胞检测嵌入空间完成轻量级匹配。

Result: 该框架在多种实验条件下均表现出强鲁棒性，能准确追踪细胞并识别有丝分裂与凋亡事件；成功应用于单细胞水平评估化疗药物疗效。

Conclusion: TransientTrack为研究癌细胞动态（如治疗响应与耐药机制）提供了新工具，推动了单细胞定量生物学的发展。

Abstract: Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.

</details>


### [220] [KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM](https://arxiv.org/abs/2512.01889)
*Zaid Nasser,Mikhail Iumanov,Tianhao Li,Maxim Popov,Jaafar Mahmoud,Malik Mohrat,Ilya Obrubov,Ekaterina Derevyanka,Ivan Sosin,Sergey Kolyubin*

Main category: cs.CV

TL;DR: KM-ViPE 是一种面向动态环境、无需标定单目相机的实时开词汇SLAM框架，融合DINO视觉特征与几何约束，支持在线定位与语义建图，适用于自主机器人与AR/VR。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统多依赖深度传感器、离线标定或无法鲁棒处理动态场景，难以适用于基于互联网视频的大规模训练和以自我为中心的动态应用。

Method: 提出KM-ViPE框架，将DINO视觉特征与几何约束通过自适应鲁棒核紧密耦合，融合几何与语言对齐的深度视觉特征，实现单目RGB流下的在线定位与开词汇语义映射。

Result: 在保持实时性的同时，性能媲美SOTA方法；支持无标定单目输入、在线运行，并能稳健应对移动物体及可移动静态物体（如ego-centric视角下的家具）。

Conclusion: KM-ViPE推动了具身AI的空间智能实用化，在自主机器人与AR/VR等场景中具有重要应用价值。

Abstract: We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.

</details>


### [221] [StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data](https://arxiv.org/abs/2512.01895)
*Avirup Dey,Vinay Namboodiri*

Main category: cs.CV

TL;DR: 本文提出了一种名为StyleYourSmile的一次性跨域人脸重定向方法，无需配对的多风格数据，通过双编码器框架和数据增强策略实现身份与风格特征的解耦，并利用扩散模型实现跨域表情迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨域人脸重定向中依赖精细标注的多风格数据集、测试时优化或难以泛化，缺乏对身份、表情和领域特有风格的有效解耦。

Method: 提出一种高效的数据增强策略和双编码器框架，分别提取域不变的身份特征和捕捉域特定的风格变化，并以此条件化扩散模型进行跨域表情重定向。

Result: 在多种视觉域上进行了广泛实验，结果表明StyleYourSmile在身份保持性和重定向保真度方面优于现有方法。

Conclusion: StyleYourSmile实现了无需微调和成对多风格数据的一次性跨域人脸重定向，有效解耦身份与风格，提升了跨域泛化能力和应用灵活性。

Abstract: Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.

</details>


### [222] [SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception](https://arxiv.org/abs/2512.01908)
*Gurmeher Khurana,Lan Wei,Dandan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种空间感知的自监督学习框架SARL，用于融合视觉-触觉数据，通过在特征图层面引入三个新目标（SAL、PPDA、RAM），保留空间结构信息，显著提升几何敏感任务（如边缘姿态回归）性能，MAE达0.3955，相对提升30%。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法将特征图压缩为全局向量，丢失空间结构，不适用于需局部几何信息的接触式机器人操作任务；而视觉-触觉融合传感器提供天然对齐的多模态输入，亟需能保持空间一致性的表征学习方法。

Method: 在BYOL框架基础上，提出SARL，新增三个作用于中间特征图的损失函数：Saliency Alignment（SAL）、Patch-Prototype Distribution Alignment（PPDA）和Region Affinity Matching（RAM），分别保持注意力焦点、部件组成和几何关系在多视角下的一致性。

Result: SARL在六个下游任务上持续超越九种SSL基线；在边缘姿态回归任务中MAE为0.3955，较次优方法（0.5682）相对提升30%，逼近监督学习上界。

Conclusion: 对于融合视觉-触觉数据，最有效的自监督信号是结构化的空间等变性（spatial equivariance），即特征随物体几何结构可预测地变化，这能显著增强机器人感知能力。

Abstract: Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.

</details>


### [223] [Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2512.01922)
*Zahra Mahdavi,Zahra Khodakaramimaghsoud,Hooman Khaloo,Sina Bakhshandeh Taleshani,Erfan Hashemi,Javad Mirzapour Kaleybar,Omid Nejati Manzari*

Main category: cs.CV

TL;DR: 本文提出了Med-VCD方法，一种稀疏视觉对比解码技术，用于减少医疗大视觉语言模型（LVLMs）中的幻觉现象，无需二次解码，兼顾效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在医疗应用中易产生幻觉输出，而当前缓解幻觉的方法多依赖二次解码或回滚，导致推理变慢，且存在模态错位或生成内容与真实标签不一致的问题。

Method: 提出Med-VCD，一种稀疏视觉对比解码方法，包含动态的、基于视觉信息的token稀疏化策略，在解码过程中实时筛选关键视觉相关token，剔除冗余，保留核心视觉上下文。

Result: 在8个涵盖眼科、放射科和病理学的医学数据集上评估显示，Med-VCD相比基线模型平均提升事实准确率13%，幻觉准确率提升6%。

Conclusion: Med-VCD是一种高效可靠的幻觉缓解方法，适用于医疗LVLMs，在不牺牲推理速度的前提下显著提升生成结果的事实性与可信度。

Abstract: Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.

</details>


### [224] [Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory](https://arxiv.org/abs/2512.01934)
*Chenyi Wang,Yanmao Man,Raymond Muller,Ming Li,Z. Berkay Celik,Ryan Gerdes,Jonathan Petit*

Main category: cs.CV

TL;DR: 本文提出AdvTraj，首个针对检测-跟踪范式多目标跟踪（MOT）系统的在线物理域ID操纵攻击，通过生成对抗性轨迹干扰目标关联，无需攻击检测模块，在仿真中对SORT实现100%攻击成功率，并对多种SOTA MOT算法具有高达93%的迁移攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有MOT攻击方法多为数字域、模型特定、离线且鲁棒性差；而MOT中对象ID错误关联可能引发严重后果，其关联阶段的安全隐患尚未被充分研究。

Method: 提出AdvTraj攻击框架，利用对抗性轨迹在物理世界中转移攻击者ID至目标对象，干扰跟踪器的对象关联逻辑；不依赖攻击检测模块，设计两种人类可执行的通用对抗性运动模式，并在CARLA仿真环境中验证。

Result: AdvTraj在白盒设置下对SORT实现100% ID混淆成功率；对多种SOTA MOT算法迁移攻击成功率最高达93%；所生成对抗轨迹具备可解释的运动模式，且可在真实场景（如行人/车辆）中实施。

Conclusion: 揭示了当前SOTA MOT系统在对象关联阶段存在未被充分认识的安全弱点，强调需加强关联模块鲁棒性设计，并为物理世界中视觉跟踪系统的安全评估与防御提供了新思路。

Abstract: Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.

</details>


### [225] [Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2512.01949)
*Zhongyu Yang,Dannong Xu,Wei Pang,Yingfang Yuan*

Main category: cs.CV

TL;DR: 本文提出Script，一种即插即用的视觉token剪枝方法，无需重训练即可在多种多模态大模型上通用，通过图结构剪枝和查询条件语义剪枝两个模块，在显著提升推理速度与降低计算量的同时，几乎不损失性能。


<details>
  <summary>Details</summary>
Motivation: 视觉token数量激增导致多模态大语言模型内存占用高、推理延迟大；现有剪枝方法常忽略用户查询相关性或受限于注意力机制，适应性与有效性不足。

Method: 提出Script方法，包含两个模块：1）图结构剪枝模块，去除视觉冗余token；2）查询条件语义剪枝模块，保留与用户查询相关的视觉信息；整体为即插即用、无需微调的方案。

Result: 在14个图像与视频理解基准上，Script显著优于现有剪枝方法；在LLaVA-NeXT-7B上实现最高6.8倍prefill加速与10倍FLOP减少，同时保持96.88%原始性能。

Conclusion: Script是一种高效、通用、免训练的视觉token剪枝方法，能兼顾效率与精度，适用于多种MLLM架构。

Abstract: The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.

</details>


### [226] [SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation](https://arxiv.org/abs/2512.01960)
*Zisu Li,Hengye Lyu,Jiaxin Shi,Yufeng Zeng,Mingming Fan,Hanwang Zhang,Chen Liang*

Main category: cs.CV

TL;DR: SpriteHand是一个用于实时生成手-物体交互视频的自回归视频生成框架，支持多种物体类型和运动模式，具有高视觉质量、物理合理性和交互保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理引擎的模拟方法难以处理非刚性或关节式物体（如织物、弹性材料、铰链结构、毛茸茸表面甚至活体）的动态交互。

Method: 提出SpriteHand框架，采用因果推理架构进行自回归视频生成，并结合混合后训练策略提升视觉真实感与时间连贯性；输入为静态物体图像和手部动作视频流，输出为实时手-物体交互效果。

Result: 1.3B参数模型在单块RTX 5090 GPU上实现约18 FPS、640×368分辨率的实时流式生成，延迟约150 ms，可持续输出超一分钟；实验显示其在视觉质量、物理合理性与交互保真度上优于生成式及引擎式基线方法。

Conclusion: SpriteHand有效解决了复杂手-物体交互建模与合成难题，尤其适用于非刚性与动态物体场景，为虚拟现实、人机交互等领域提供了新工具。

Abstract: Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.

</details>


### [227] [SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning](https://arxiv.org/abs/2512.01975)
*Xu Zhang,Jin Yuan,Hanwang Zhang,Guojin Zhong,Yongsheng Zang,Jiacheng Lin,Zhiyong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新任务Image Collaborative Segmentation and Captioning（SegCaptioning），通过简单提示（如边界框）生成多样化的（描述文本，掩码）对；为此设计了基于场景图引导的扩散模型SGDiff，包含提示中心场景图适配器和场景图引导双模态Transformer，并引入多实体对比学习损失以提升图文对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有可控图像语义理解任务（如标注或分割）依赖高成本提示输入，且输出信息有限；需要一种能从简易提示生成多样化、语义一致的图文-掩码对的新范式。

Method: 提出Scene Graph Guided Diffusion Model（SGDiff）：1）Prompt-Centric Scene Graph Adaptor将用户提示映射为场景图以捕获意图；2）Scene Graph Guided Bimodal Transformer在扩散过程中联合预测对齐的caption-mask对；3）Multi-Entities Contrastive Learning loss显式对齐视觉与文本实体。

Result: 在两个数据集上的实验表明，SGDiff在SegCaptioning任务中性能优越，仅需最小提示输入即可同时提升图像描述与分割效果。

Conclusion: SegCaptioning是一种更灵活、用户友好的图像语义理解新范式；SGDiff通过结构化场景图建模与扩散生成机制，有效解决了少提示下多语义输出及图文掩码强对齐的关键挑战。

Abstract: Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.

</details>


### [228] [Artemis: Structured Visual Reasoning for Perception Policy Learning](https://arxiv.org/abs/2512.01988)
*Wei Tang,Yanpeng Sun,Shan Zhang,Xiaofan Li,Piotr Koniusz,Wei Li,Na Zhao,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出Artemis框架，通过结构化的基于提议的推理（每个步骤为(label, bounding-box)对）替代纯语言中间推理，以更好地匹配视觉感知所需的空域和物体中心化推理空间，从而提升多类视觉任务性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 纯自然语言中间推理在视觉感知任务中常导致性能下降，核心问题在于推理形式不匹配：语言推理是无结构的语义空间，而视觉感知需要空间和物体中心化的推理。

Method: 提出Artemis框架，采用结构化提议式推理，每步表示为(label, bounding-box)对，实现可验证的视觉状态表征；基于Qwen2.5-VL-3B构建，支持显式中间状态追踪与提议质量直接监督。

Result: 在定位与检测任务上表现强劲，并显著泛化至计数与几何感知任务；在通用MLLM基准上也达竞争性性能。

Conclusion: 将推理与空间表征对齐能有效增强感知策略学习，空间接地推理是构建可扩展、通用视觉策略的合理路径。

Abstract: Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.

</details>


### [229] [PAI-Bench: A Comprehensive Benchmark For Physical AI](https://arxiv.org/abs/2512.01989)
*Fengzhe Zhou,Jiannan Huang,Jialuo Li,Deva Ramanan,Humphrey Shi*

Main category: cs.CV

TL;DR: 本文提出了Physical AI Bench (PAI-Bench)，一个用于评估视频生成、条件视频生成和视频理解中物理感知与预测能力的统一基准，包含2808个真实案例；研究发现当前视频生成模型虽视觉保真度高，但物理动态连贯性不足，而多模态大语言模型在预测和因果推理方面表现有限，表明现有系统在Physical AI任务上仍处于初级阶段。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型和视频生成模型在物理感知与预测（Physical AI）方面的能力尚不明确，缺乏统一、全面的评估基准。

Method: 构建了PAI-Bench基准，涵盖视频生成、条件视频生成和视频理解三大任务，含2808个真实世界案例，并设计了面向任务的物理合理性与领域特异性推理评估指标。

Result: 实证表明：视频生成模型视觉质量高但物理动态常不连贯；多模态大语言模型在预测与因果解释上能力薄弱；整体显示现有模型距Physical AI要求仍有显著差距。

Conclusion: PAI-Bench为Physical AI提供了现实可行的评估基础，并明确了未来系统需突破的关键能力缺口，如物理一致性建模与因果预测。

Abstract: Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.

</details>


### [230] [Learning Visual Affordance from Audio](https://arxiv.org/abs/2512.02005)
*Lidong Lu,Guo Chen,Zhu Wei,Yicheng Liu,Tong Lu*

Main category: cs.CV

TL;DR: 本文提出了音频-视觉可供性定位（AV-AG）新任务，利用动作声音进行物体交互区域的像素级分割，并构建首个AV-AG数据集与提出AVAGFormer模型，在该任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本指令或演示视频的可供性定位方法受限于歧义性和遮挡问题；而声音提供实时、语义丰富且视觉无关的线索，可更直观地定位交互区域。

Method: 构建首个包含动作声音、物体图像和像素级可供性标注的AV-AG数据集（含零样本泛化子集），并提出AVAGFormer模型，其包含语义条件化的跨模态混合器和双头解码器，实现音视频信号融合以预测掩码。

Result: AVAGFormer在AV-AG任务上显著超越相关任务基线，实验分析验证了AV-AG与AVS的区别、端到端建模优势及各模块贡献。

Conclusion: AV-AG是一项有前景的新任务，声音作为强语义线索可有效提升可供性理解；AVAGFormer为该任务提供了高效框架，开源代码与数据集推动后续研究。

Abstract: We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.

</details>


### [231] [MV-TAP: Tracking Any Point in Multi-View Videos](https://arxiv.org/abs/2512.02006)
*Jahyeok Koo,Inès Hyeonsu Kim,Mungyeom Kim,Junghyun Park,Seohyun Park,Jaeyeong Kim,Jung Yi,Seokju Cho,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了MV-TAP，一种利用跨视角注意力机制和相机几何信息在多视角视频中跟踪动态场景中点的新方法，并构建了合成训练数据集与真实世界评估集，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视角相机系统能提供复杂现实场景的丰富观测，理解多视角下的动态物体对多种应用至关重要，但现有方法在多视角点跟踪中仍存在挑战。

Method: 提出MV-TAP点跟踪器，结合相机几何与跨视角注意力机制，聚合多视角时空信息以提升轨迹估计的完整性与鲁棒性；并构建大规模合成训练数据集及真实世界评估集。

Result: 在多个具有挑战性的基准上，MV-TAP显著优于现有点跟踪方法，成为多视角点跟踪研究的有效基线。

Conclusion: MV-TAP通过有效融合跨视角信息，提升了动态场景下多视角点跟踪的性能，为该方向后续研究提供了坚实基础与新思路。

Abstract: Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.

</details>


### [232] [AirSim360: A Panoramic Simulation Platform within Drone View](https://arxiv.org/abs/2512.02009)
*Xian Ge,Yuling Pan,Yuhang Zhang,Xiang Li,Weijun Zhang,Dizhe Zhang,Zhaoliang Wan,Xin Lin,Xiangkai Zhang,Juntao Liang,Jason Li,Wenjie Jiang,Bo Du,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: 本文提出了AirSim360，一个面向空中视角的360度全景数据仿真平台，旨在解决当前360度空间理解领域缺乏大规模、多样化数据的问题。平台涵盖渲染对齐的数据标注范式、行人感知交互系统和自动轨迹生成机制，并发布超6万张全景样本及完整工具链。


<details>
  <summary>Details</summary>
Motivation: 360度全方位理解领域缺乏大规模、多样化的数据，限制了空间智能的发展。

Method: 提出AirSim360仿真平台，包含三方面创新：1）渲染对齐的像素级几何/语义/实体标注范式；2）交互式行人感知行为建模系统；3）支持导航任务的自动轨迹生成机制。

Result: 收集超60K全景样本，在多项任务上验证了平台有效性；首次在全向设置下系统建模4D真实世界。

Conclusion: AirSim360为360度空间理解提供了可扩展、可复现且开源的仿真基础设施，推动了 aerial omnidirectional intelligence 的发展。

Abstract: The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.

</details>


### [233] [Improved Mean Flows: On the Challenges of Fastforward Generative Models](https://arxiv.org/abs/2512.02012)
*Zhengyang Geng,Yiyang Lu,Zongze Wu,Eli Shechtman,J. Zico Kolter,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出了改进的MeanFlow（iMF）方法，通过重新参数化训练目标为瞬时速度v的回归问题，并将分类器自由引导作为显式条件变量，提升了训练稳定性和测试灵活性。iMF在ImageNet 256×256上以单次函数评估实现1.72 FID，显著优于同类单步生成模型，且无需蒸馏即可接近多步方法性能。


<details>
  <summary>Details</summary>
Motivation: 原始MeanFlow框架存在两个关键问题：一是训练目标依赖网络自身，导致不稳定；二是分类器自由引导尺度在训练中固定，缺乏测试时的灵活性。

Method: 1）将训练目标重参数化为对平均速度u的预测，构建标准回归损失；2）将引导尺度设为显式条件输入，采用上下文内条件机制（in-context conditioning）处理多样引导信号，减小模型规模并提升性能。

Result: iMF在ImageNet 256×256上以1-NFE达成1.72 FID，显著优于此前单步生成方法，性能逼近多步方法，且不依赖知识蒸馏。

Conclusion: iMF通过更稳定的训练目标和灵活的引导机制，推动了单步‘快进’生成建模成为独立可行的范式。

Abstract: MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.

</details>


### [234] [TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models](https://arxiv.org/abs/2512.02014)
*Zhiheng Liu,Weiming Ren,Haozhe Liu,Zijian Zhou,Shoufa Chen,Haonan Qiu,Xiaoke Huang,Zhaochong An,Fanny Yang,Aditya Patel,Viktar Atliha,Tony Ng,Xiao Han,Chuyan Zhu,Chenyang Zhang,Ding Liu,Juan-Manuel Perez-Rua,Sen He,Jürgen Schmidhuber,Wenhu Chen,Ping Luo,Wei Liu,Tao Xiang,Jonas Schult,Yuren Cong*

Main category: cs.CV

TL;DR: TUNA是一种原生统一多模态模型，通过级联VAE编码器和表征编码器构建统一连续视觉表征空间，实现图像与视频在理解与生成任务中的端到端处理，显著优于解耦表征的先前方法，并在多项多模态任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有统一多模态模型中因视觉表征解耦导致的格式不匹配问题，提升理解与生成任务的协同效果。

Method: 提出TUNA模型，采用VAE编码器级联表征编码器的方式构建统一连续视觉表征空间，并联合训练理解与生成任务。

Result: 在图像/视频理解、图像/视频生成及图像编辑等多个基准上均达到当前最优性能（SOTA），验证了统一表征设计的有效性与可扩展性。

Conclusion: 统一连续视觉表征空间能有效避免解耦表示带来的不匹配问题，且更强的预训练表征编码器与联合训练策略可进一步提升多模态任务整体性能。

Abstract: Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.

</details>


### [235] [Generative Video Motion Editing with 3D Point Tracks](https://arxiv.org/abs/2512.02015)
*Yao-Chih Lee,Zhoutong Zhang,Jiahui Huang,Jui-Hsien Wang,Joon-Young Lee,Jia-Bin Huang,Eli Shechtman,Zhengqi Li*

Main category: cs.CV

TL;DR: 提出一种基于3D点轨迹的视频到视频框架，实现对相机和物体运动的联合编辑，提升复杂场景下的运动编辑精度与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有运动控制方法在全场景上下文保持和细粒度物体运动控制方面存在不足，难以处理复杂运动和遮挡问题。

Method: 提出一种以3D点轨迹为条件的视频生成模型，利用源视频及其对应的3D点轨迹（源与目标运动）建立稀疏对应关系，传递丰富上下文并保持时空连贯性；使用合成与真实数据分两阶段训练。

Result: 支持多种运动编辑任务，包括相机与物体的联合操控、运动迁移和非刚性变形，能有效处理深度顺序和遮挡，显著提升编辑精度。

Conclusion: 该方法通过引入3D轨迹条件，在复杂动态场景中实现了更精确、可控且一致的视频运动编辑，拓展了视频创作的可能性。

Abstract: Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.

</details>


### [236] [Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now](https://arxiv.org/abs/2512.02016)
*Varun Varma Thozhiyoor,Shivam Tripathi,Venkatesh Babu Radhakrishnan,Anand Bhattad*

Main category: cs.CV

TL;DR: 本文研究视频生成模型对重力定律的建模能力，发现其普遍存在重力加速度偏低的问题；提出一种无单位、双物体相对时间比测试协议，揭示模型违反伽利略等效原理；并通过仅用100个单球下落实例微调的轻量适配器，显著提升重力模拟精度，并实现跨场景零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型正被视作潜在的世界模型，需准确编码物理规律（如重力）；但现有评估常受度量尺度模糊（如帧率误设）干扰，亟需剥离混淆因素、严谨检验其内在物理表征能力。

Method: 提出一种不依赖重力g、焦距和尺度的无单位双物体测试协议，基于自由落体时间平方比等于高度比（t₁²/t₂² = h₁/h₂）验证伽利略等效原理；并设计轻量低秩适配器，在仅100个单球下落实例上进行微调以校正重力偏差。

Result: 原始视频生成器生成的下落加速度远低于真实值（g_eff ≈ 1.81 m/s²）；新协议证实该偏差非尺度混淆所致，而是本质性违背等效原理；微调后g_eff提升至6.43 m/s²（达地球重力65%），且零样本泛化至双球下落与斜面场景。

Conclusion: 视频生成器当前缺乏对基础物理规律（如重力）的正确内在表征；可通过少量特定物理数据驱动的轻量微调有效修正，表明物理一致性可作为可控、可学习的建模目标。

Abstract: Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\mathrm{eff}}$ from $1.81\,\mathrm{m/s^2}$ to $6.43\,\mathrm{m/s^2}$ (reaching $65\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [237] [Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis](https://arxiv.org/abs/2512.00046)
*Angelina Parfenova,Andreas Marfurt,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 该研究探讨了使用大语言模型（LLM）进行定性数据分析中的归纳编码，比较了六种开源LLM与人类专家的表现，发现人类在复杂句子上表现更好而LLM在简单句子上更优，并揭示了两者在标签生成上的系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 旨在探索大型语言模型在定性数据归纳编码中的潜力，弥补传统依赖预定义标签的演绎方法的不足，推动自动化分析的发展。

Method: 评估六种开源大语言模型与人类专家在归纳编码任务中的表现，通过专家对引文编码难度评分，并与测试集的黄金标准进行对比，分析人类与模型标签的差异。

Result: 人类编码者在处理复杂句子时表现良好但在简单句子上表现较差，而LLM则相反；部分LLM在接近黄金标准方面优于人类，但人类评价却更低。

Conclusion: LLM在定性数据分析中具有潜力，尤其在处理简单文本时表现良好，但其输出的可接受性仍受人类主观评价影响，需进一步优化以实现人机协同。

Abstract: This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.

</details>


### [238] [Emergent Convergence in Multi-Agent LLM Annotation](https://arxiv.org/abs/2512.00047)
*Angelina Parfenova,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本文通过模拟7500次多智能体多轮讨论，研究大语言模型（LLMs）作为黑箱代理时的协作协调机制，并提出过程级指标与嵌入几何分析方法，揭示其自发涌现的协调策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）日益被部署于协作场景，但作为黑箱代理时其协调机制尚不清楚。

Method: 在归纳式编码任务中模拟7500个多智能体、多轮讨论，生成超12.5万条话语；引入代码稳定性、语义自一致性、词汇置信度等过程级指标，并分析输出嵌入的几何演化（如内在维度变化）。

Result: LLM群体展现出词汇与语义收敛、非对称影响力模式及类协商行为；嵌入内在维度随轮次下降，表明语义压缩；无需显式角色提示即能自发协调。

Conclusion: 黑箱交互分析可有效揭示LLM群体中涌现的协调策略，为基于内部探针的可解释性方法提供可扩展的补充路径。

Abstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.

</details>


### [239] [Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees](https://arxiv.org/abs/2512.00204)
*Jason Lunder*

Main category: cs.CL

TL;DR: 本文提出了一种基于依赖句法树的Tree Matching Networks (TMN)模型，用于自然语言推断任务，相较于BERT等序列模型，在减少参数和训练时间的同时取得了更好的性能，但现有聚合方法限制了其可扩展性，因此提出了多头注意力聚合来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 利用显式的语言结构（如依存句法树）可能比从零学习词间关系更高效，从而提升模型的学习效率和性能。

Method: 将图匹配网络（GMN）改进为适用于依存句法树的树匹配网络（TMN），并在SNLI和SemEval任务上与BERT模型进行比较，引入多头注意力聚合机制以提升可扩展性。

Result: TMN在SNLI任务上显著优于BERT模型，且内存占用更小、训练时间更短；但在SemEval任务上两者表现均不佳。显式结构表示在相当规模下优于序列模型，但当前聚合方法限制了 scalability。

Conclusion: 使用显式语言结构（如依存树）有助于提升模型效率与性能，多头注意力聚合可有效缓解现有方法的可扩展性问题，未来值得进一步探索结构化表示在NLP中的应用。

Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.

</details>


### [240] [Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis](https://arxiv.org/abs/2512.00214)
*Matej Klemen,Tjaša Arčon,Luka Terčon,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文提出了一种基于智能体大语言模型（LLM）的框架，用于自动化、可解释地分析标注语料库（如Universal Dependencies），在多语言词序特征分析任务中验证了其可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 实证语法研究日益数据驱动，但系统性分析标注语料库仍需大量方法与技术投入；亟需降低门槛、提升可解释性与可扩展性。

Method: 构建一个具备自然语言任务理解、代码生成与数据驱动推理能力的智能体式LLM框架，并应用于Universal Dependencies语料库，开展受WALS启发的多语言语法任务评估。

Result: 在13项词序特征、170多种语言上评估，从主导语序准确率、语序覆盖完整性、分布保真度三方面验证系统性能，证明LLM与结构化语言数据结合可行。

Conclusion: 该框架为语料库驱动的语法探究提供了首个兼具可解释性与可扩展性的自动化路径，是迈向智能辅助语言学研究的重要一步。

Abstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.

</details>


### [241] [Minimal-Edit Instruction Tuning for Low-Resource Indic GEC](https://arxiv.org/abs/2512.00219)
*Akhil Rajeev P*

Main category: cs.CL

TL;DR: 本文提出了一种无需数据增强的印地语系语法错误纠正（GEC）方法，利用指令微调的大语言模型（12B GEMMA 3）与保守解码策略，在有限监督、多脚本、强形态变化条件下实现高效、可复现的纠错性能。


<details>
  <summary>Details</summary>
Motivation: 印地语系GEC面临监督数据稀缺、文字系统多样、形态丰富等挑战，传统基于数据增强的方法存在局限性。

Method: 采用bnb 4-bit精度下的参数高效微调（PEFT）对GEMMA 3模型进行Alpaca风格指令微调；设计基于错误分类器（含分类体系、标签分布与优先级）的语言特定固定提示；结合确定性、约束感知解码与轻量级归一化模块，确保最小且语义保持的编辑。

Result: 在官方未调优GLEU评估下，马来雅拉姆语得分为92.41（总排名第六），印地语为81.44（总排名第三）。

Conclusion: 基于分类器的提示设计、适配器式指令微调与确定性解码构成了一种可复现、计算高效的替代方案，优于依赖数据增强的流水线；该方法也为强化形态句法约束和面向人类评估的保守编辑研究提供了新方向。

Abstract: Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.
  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.

</details>


### [242] [OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion](https://arxiv.org/abs/2512.00234)
*Sai Koneru,Matthias Huck,Jan Niehues*

Main category: cs.CL

TL;DR: 本文提出OmniFusion，一种端到端融合多模态基础模型（MMFM）与翻译大语言模型（LLM）的框架，支持语音、图文联合输入的翻译任务，在同步语音翻译中降低延迟并提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有开源文本翻译大模型仅能用于级联式语音翻译（先ASR再翻译），导致高延迟且无法利用图像等多模态上下文；而多模态基础模型虽具跨模态能力，却缺乏多语言翻译专长。

Method: 提出一种新型分层隐藏状态融合策略，将预训练MMFM（Omni 2.5-7B）多个层的特征接入翻译LLM（SeedX PPO-7B），实现联合端到端训练。

Result: OmniFusion支持语音转文本、语音+图像转文本、文本+图像转文本三类翻译任务，在同步语音翻译中降低1秒延迟，并提升整体翻译质量。

Conclusion: 通过融合MMFM与翻译LLM，OmniFusion实现了低延迟、高质量、多模态支持的端到端翻译系统，为SimulST和多模态翻译提供了新范式。

Abstract: There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.

</details>


### [243] [Lost without translation -- Can transformer (language models) understand mood states?](https://arxiv.org/abs/2512.00274)
*Prakrithi Shivaprakash,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 该研究评估了大语言模型在理解印度语言中情绪状态（如抑郁、躁狂等）表达方面的能力，发现当前模型无法有效直接表征印度语言中的情绪症状，而依赖高质量翻译（尤其是英译中再嵌入）可提升表现，但现有方法不可持续，需开发能理解本地语言的模型以推动全球心理健康应用。


<details>
  <summary>Details</summary>
Motivation: 由于不同语言具有独特的痛苦表达方式（即‘痛苦习语’），而现有大语言模型主要基于英语，因此需要量化这些模型在非英语语境下（特别是印度多语言环境）对情绪状态的理解能力。

Method: 收集了11种印度语言中关于四种情绪状态（抑郁、正常心境、欣快躁狂、烦躁躁狂）的247个独特短语，比较七种实验条件下的k-means聚类性能，包括直接使用本地及罗马化脚本的嵌入、以及将短语翻译为英文和中文后的嵌入，采用调整兰德指数、归一化互信息、同质性和完整性构成的综合评分来评估性能。

Result: 直接使用印度语言嵌入的表现极差（综合得分0.002），所有基于翻译的方法均有改善；Gemini翻译的英文嵌入表现良好（0.60），人工翻译英文更优（0.61），而人工翻译英文再译为中文并用中文模型嵌入效果最佳（0.67）；专用印度语言模型（如IndicBERT和Sarvam-M）表现不佳。

Conclusion: 目前的语言模型无法有效直接表示印度语言中的情绪状态，这对其在印度精神科诊断或治疗中的应用构成根本障碍；虽然高质量翻译可以弥补差距，但依赖专有模型或复杂翻译流程不可持续；必须首先构建能够理解多样化本地语言的模型，才能实现全球心理健康的技术可及性。

Abstract: Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.

</details>


### [244] [EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education](https://arxiv.org/abs/2512.00290)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Yue Cui,Jiawei Shen,Zilong Li,Yidan Liang*

Main category: cs.CL

TL;DR: 本文提出了EduEval，一个面向中文K-12教育的分层评估基准，融合认知框架、真实教育场景与大规模题库，用于系统评测大语言模型在教育任务中的能力，并揭示了当前模型在不同认知维度上的表现差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育领域潜力巨大，但其未经审慎部署可能威胁教育质量，亟需一套严谨、贴合实际教学需求的中文教育评估基准。

Method: 构建EduEval基准：提出融合布鲁姆分类法与韦伯知识深度的EduAbility Taxonomy认知框架；整合真实考试题、课堂对话、学生作文和专家设计提示以保障真实性；涵盖24类任务、超1.1万道题，覆盖小学至高中全学段；对14个主流LLM在零样本与少样本设置下进行系统评测。

Result: 发现模型在记忆类任务上表现良好，但在课堂对话分类和创意内容生成上存在明显短板；部分开源模型在复杂教育推理任务上优于闭源模型；少样本提示效果因认知维度而异。

Conclusion: EduEval为中文教育场景下的LLM评估提供了可扩展、可解释、贴近实际的基准工具，研究结果为面向教育优化的LLM研发提供了明确方向和量化指标。

Abstract: Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.

</details>


### [245] [InnoGym: Benchmarking the Innovation Potential of AI Agents](https://arxiv.org/abs/2512.01822)
*Jintian Zhang,Kewei Xu,Jingsheng Zheng,Zhuoyun Yu,Yuqi Zhu,Yujie Luo,Lanning Wei,Shuofei Qiao,Lun Du,Da Zheng,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: InnoGym是首个系统评估AI代理创新潜力的基准框架，提出性能增益和新颖性两个互补指标，涵盖18个真实工程与科学任务，并揭示当前代理在创造力与有效性间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注解法的正确性，忽视了解决方案背后方法的多样性，而真正的创新不仅依赖正确答案，更取决于方法的原创性。因此需要一个能同时评估AI创新能力的新基准。

Method: 提出InnoGym基准框架，包含性能增益（衡量优于已知最佳解的程度）和新颖性（衡量方法论差异）两个指标；构建18个真实世界任务的数据集，并提供统一执行环境iGym以支持可复现的长周期评估。

Result: 实验表明部分AI代理能生成新颖方法，但其鲁棒性不足限制了性能提升，暴露出当前AI在创造性与实际效果之间的关键差距。

Conclusion: 创新需兼顾方法新颖性与实际性能，InnoGym为评估AI创新潜力提供了标准化工具，凸显未来需发展既能创造又具稳定性的智能代理。

Abstract: LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.

</details>


### [246] [Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets](https://arxiv.org/abs/2512.00323)
*Muhammad Muneeb,David B. Ascher,Ahsan Baidar Bakht*

Main category: cs.CL

TL;DR: 本文对Hugging Face上的47个上下文基础问答（CBQA）模型在8个数据集上进行了无微调基准测试，发现ahotrod/electra_large_discriminator_squad2_512整体表现最佳（平均准确率43%），并分析了上下文长度、答案长度和上下文复杂度对性能的影响，还尝试用遗传算法融合多模型结果以提升准确率。


<details>
  <summary>Details</summary>
Motivation: 为减少实际应用中对特定数据集重新训练模型的需求，需识别无需微调即可在多样化数据集上表现优异的CBQA模型。

Method: 在八个不同数据集上对47个现成CBQA模型进行零样本基准测试；分析性能与上下文长度、答案长度及上下文复杂度的关系；使用遗传算法集成多个模型的输出以提升整体准确率。

Result: ahotrod/electra_large_discriminator_squad2_512模型在全部数据集上平均准确率达43%，在bioasq10b-factoid、biomedical_cpgQA、QuAC和Question Answer Dataset上分别达65.92%、96.45%、11.13%和41.6%；bert-large-uncased-whole-word-masking-finetuned-squad在IELTS数据集上达82%；模型性能随答案长度增加而下降，且受上下文复杂度影响显著。

Conclusion: SQuAD系列预训练模型（尤其是electra_large_discriminator_squad2_512）在跨领域CBQA任务中具备较强泛化能力；计算开销与上下文长度和模型规模正相关；多模型集成（如遗传算法）可进一步提升性能。

Abstract: Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\%), biomedical\_cpgQA (96.45\%), QuAC (11.13\%), and Question Answer Dataset (41.6\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\% on the IELTS dataset.

</details>


### [247] [Evidence-Guided Schema Normalization for Temporal Tabular Reasoning](https://arxiv.org/abs/2512.00329)
*Ashish Thanga,Vibhu Dixit,Abhilash Shankarampeta,Vivek Gupta*

Main category: cs.CL

TL;DR: 提出一种基于SQL的方法，通过从维基百科信息框生成3NF模式、生成SQL查询并执行查询，来解决在演化半结构化表格上的时序推理问题。研究发现，模式设计质量对问答准确率的影响超过模型规模。提出了三项原则：保持上下文的规范化、减少歧义的语义命名、一致的时间锚定。最佳配置相比基线提升了16.8%。


<details>
  <summary>Details</summary>
Motivation: 当前的问答系统在处理随时间演化的半结构化表格时存在时序推理困难，难以准确理解复杂的时间相关数据。

Method: 采用基于SQL的方法，包括三个步骤：首先从维基百科信息框生成符合第三范式（3NF）的数据库模式，然后生成相应的SQL查询，最后执行查询获取答案。

Result: 实验结果显示，模式设计质量对问答性能的影响大于模型容量；最佳配置（Gemini 2.5 Flash模式 + Gemini-2.0-Flash查询）达到80.39 EM，比基线（68.89 EM）提升16.8%。

Conclusion: 在时序推理任务中，良好的数据库模式设计比单纯扩大模型规模更能提升问答系统的准确性，提出了三项可推广的设计原则。

Abstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).

</details>


### [248] [Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents](https://arxiv.org/abs/2512.00332)
*Daud Waqas,Aaryamaan Golthi,Erika Hayashida,Huanzhi Mao*

Main category: cs.CL

TL;DR: 本文提出Assertion-Conditioned Compliance (A-CC)评估范式，用于检测多轮工具调用大模型在面对用户或工具提供的错误断言时的脆弱性，揭示其在安全关键场景中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如BFCL）聚焦单轮函数调用能力，缺乏对多轮对话中模型面对误导性断言时鲁棒性的系统评估，尤其在医疗、金融等安全关键领域存在隐患。

Method: 提出A-CC评估范式，设计两类误导性断言：用户来源断言（USAs）和工具来源断言（FSAs），并构建对应测试集，量化模型在多轮对话中对错误断言的服从倾向（sycophancy与policy conflict）。

Result: 实验表明主流多轮工具调用模型对USAs和FSAs均高度脆弱，暴露出实际部署中未被识别的关键鲁棒性缺陷。

Conclusion: A-CC是一种必要且有效的多轮工具调用鲁棒性评估新范式，能揭示当前模型在真实系统交互中的隐性风险，为安全关键应用提供评估基础。

Abstract: Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.

</details>


### [249] [IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages](https://arxiv.org/abs/2512.00333)
*Ayush Maheshwari,Kaushal Sharma,Vivek Patel,Aditya Maheshwari*

Main category: cs.CL

TL;DR: 本文提出了IndicParam，一个针对低资源和极低资源印度语言的高质量、人工整理的多选题基准测试集，包含超过13,000个问题，涵盖11种语言及梵英混合语。评估了19个大语言模型，发现即使最先进的模型（如GPT-5）平均准确率也仅为45.0%，揭示了跨语言迁移在这些语言上的局限性。


<details>
  <summary>Details</summary>
Motivation: 低资源和极低资源的印度语言在当前的大语言模型评估中严重不足，缺乏标准化、高质量的基准测试。现有模型主要依赖高资源语言进行训练和评估，导致对这些语言的支持薄弱，难以衡量其真实性能。因此，需要一个专门针对这些语言的综合评测基准。

Method: 构建了一个名为IndicParam的人工标注多选题数据集，覆盖11种低/极低资源印度语言及梵英混合语，共13,000+题目。每个问题被标注为知识导向型或纯语言型，以区分事实记忆与语法能力。同时支持多种题型（如列表匹配、因果推理、顺序排列等）。对19个主流大语言模型（闭源与开源）进行了系统评估，并分析其在不同语言和题型上的表现。

Result: GPT-5在该基准上平均准确率为45.0%，DeepSeek-3.2为43.1%，Claude-4.5为42.7%。所有模型在极低资源语言上表现显著下降，且在知识导向类问题上普遍弱于语言结构类问题。模型在复杂题型（如序列排序、断言-理由对）上表现较差，显示出跨语言理解和推理能力的不足。

Conclusion: IndicParam填补了低/极低资源印度语言评测的空白，揭示了当前大语言模型在这些语言上的重大局限，特别是在知识回忆和复杂推理任务上。该基准为未来模型的跨语言公平性、鲁棒性和真实性评估提供了重要工具，并推动对边缘化语言的技术支持。

Abstract: While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.

</details>


### [250] [CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA](https://arxiv.org/abs/2512.00360)
*Vsevolod Kovalev,Parteek Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级、低延迟的跨模态检索方法CrossFusion-RAG，用于教育讲座视频上的时间戳问答任务，并在新构建的数据集CourseTimeQA上验证了其有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 在单GPU资源限制下，实现对教育讲座视频的高效、精准时间戳问答，兼顾低延迟与内存约束。

Method: 构建了CourseTimeQA数据集；设计了CrossFusion-RAG模型，融合冻结编码器、学习式视觉投影、浅层查询无关跨模态注意力（含时间一致性正则）、小型交叉注意力重排序器。

Result: 在CourseTimeQA上，nDCG@10提升0.10，MRR提升0.08，单A100 GPU中位端到端延迟约1.55秒；相比多种基线方法均具优势，并验证了对ASR噪声的鲁棒性与时间定位能力。

Conclusion: CrossFusion-RAG在资源受限场景下实现了跨模态视频问答的高效与准确平衡，为教育视频理解提供了实用、可复现的技术路径。

Abstract: We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.

</details>


### [251] [Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing](https://arxiv.org/abs/2512.00390)
*Nuo Chen,Hanpei Fang,Jiqun Liu,Wilson Wei,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型（LLM）在相关性标注中的“启动效应”偏见，并分析了模拟的“大五人格”特征如何影响这种偏见，发现高开放性和低神经质等人格特征可降低启动效应，提出“人格提示”作为缓解该问题的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被用于相关性标注，但其易受先前判断影响（启动效应），而人格特质可能调节此类认知偏见，因此探究模拟人格是否能减轻LLM中的启动效应。

Method: 通过在TREC 2021和2022深度学习数据集上，对多个LLM施加不同大五人格设定，评估其在相关性判断中的启动效应变化。

Result: 高开放性和低神经质等人格设定显著降低启动效应；不同模型和任务下最有效的人格特征存在差异。

Conclusion: 模拟人格可以调节LLM在相关性标注中的认知偏见，提出‘人格提示’作为一种结合心理学与LLM评估的新方法。

Abstract: Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.

</details>


### [252] [A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction](https://arxiv.org/abs/2512.00392)
*Damian Heywood,Joseph Andrew Carrier,Kyu-Hong Hwang*

Main category: cs.CL

TL;DR: 本研究开发了一个基于大语言模型的AI辅助英语写作错误分析系统，利用Corder、Richards和James的理论构建分类体系，可识别拼写、语法和标点等多层次错误，并通过API实现细粒度反馈。系统在真实错误文本上测试表现良好，但存在语境理解局限和新增错误类别问题，显示出在EFL教学中自动化错误分析的潜力，仍需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 传统英语写作评估依赖评分量规，难以提供细致、具体的错误反馈，尤其在EFL教学中教师负担重。因此需要一种能自动、准确识别并分类多种语言错误的工具，以提升反馈效率与教学质量。

Method: 基于Corder（1967）、Richards（1971）和James（1998）的语言学理论构建详细的错误分类体系，覆盖词汇和句子层面的拼写、语法和标点错误；利用Claude 3.5 Sonnet和DeepSeek R1等大语言模型，通过Python编写的API调用实现错误识别、分类与纠正；先对孤立错误进行初步测试以优化分类体系，再使用Jose da Fonseca（1855）的《English as she is spoke》这一富含真实语言错误的文本进行最终评估。

Result: AI系统成功识别出多种类型的语言错误，验证了其处理复杂、多层次错误的能力；但在语境理解方面存在局限，面对未编码的错误时会生成新的错误类别；分类体系在初期测试中暴露出类别重叠的问题，经调整后得到改善。

Conclusion: 该AI辅助错误分析系统展现了在EFL写作教学中实现自动化、细粒度反馈的巨大潜力，能够超越传统量规评估的限制；然而，仍需进一步改进模型的上下文理解能力，并扩展分类体系以涵盖风格和语篇层面的错误，才能更全面地应用于实际教学场景。

Abstract: This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.

</details>


### [253] [CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency](https://arxiv.org/abs/2512.00417)
*Jiacheng Guo,Suozhi Huang,Zixin Yao,Yifan Zhang,Yifu Lu,Jiashuo Liu,Zihao Li,Yanyan Deng,Qixin Xiao,Jia Tian,Kanghong Zhan,Tianyi Li,Xiaochen Liu,Jason Ge,Chaoyang He,Kaixuan Huang,Lin Yang,Wenhao Huang,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文提出了CryptoBench，首个面向加密货币领域的专家构建、动态更新的LLM代理评估基准，涵盖实时性、对抗性信息环境与多源数据融合等挑战，并揭示了当前主流模型存在‘检索-预测失衡’问题。


<details>
  <summary>Details</summary>
Motivation: 现有通用代理基准无法反映加密货币分析中极端时效性、高度对抗性信息环境及多源专业数据融合等独特挑战，亟需专业化、动态化评估工具。

Method: 构建了每月50题、由加密原生专家设计的动态基准CryptoBench，按四象限（简单/复杂检索、简单/复杂预测）精细分类任务，并对10个LLM及其代理框架进行系统评测。

Result: 发现主流LLM在数据检索任务上表现良好，但在预测类任务上显著薄弱，普遍存在‘检索-预测失衡’现象，即表面事实准确但深层分析与推理能力不足。

Conclusion: CryptoBench为LLM代理在高难度垂直领域的能力评估提供了新标准，其揭示的失衡问题指明了未来提升LLM推理与预测能力的关键方向。

Abstract: This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.

</details>


### [254] [MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications](https://arxiv.org/abs/2512.01710)
*Stefano Zeppieri*

Main category: cs.CL

TL;DR: 本文提出了一种名为混合记忆增强生成（MMAG）的框架，通过五层记忆结构（对话、长期用户、情景事件关联、感知上下文感知、短期工作记忆）提升大语言模型在长程交互中的相关性、个性化和连贯性，并在Heero对话代理中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在单次提示下生成连贯文本表现优秀，但在长程交互中难以维持相关性、个性化和连续性；而人类交流依赖多种记忆形式，因此需要为LLM构建类人记忆机制。

Method: 提出MMAG模式，将记忆划分为五个相互作用的层次，借鉴认知心理学理论映射为技术组件，并设计协调、优先级排序与冲突解决策略；在Heero对话代理中实现并验证。

Result: 在Heero代理中集成加密的长期用户档案和对话历史后，用户参与度和留存率得到提升；同时明确了存储、检索、隐私与延迟等实现挑战。

Conclusion: MMAG为构建更具一致性、主动性及更符合人类需求的记忆增强型语言代理提供了系统性基础框架。

Abstract: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

</details>


### [255] [SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling](https://arxiv.org/abs/2512.00466)
*Yang Xiao,Chunpu Xu,Ruifeng Yuan,Jiashuo Wang,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文提出SCALE框架，基于双过程理论，在推理时根据子问题难度选择性分配计算资源，显著提升数学推理性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展方法采用均匀资源分配，导致难题资源不足、简单任务浪费资源，产生性能瓶颈。

Method: SCALE框架包含四个阶段：问题分解、难度评估、处理模式分配（System 1/2）、上下文传播的顺序执行。

Result: 在AIME25上准确率从57.50%提升至71.25%（+13.75个百分点），计算成本降低33%-53%。

Conclusion: SCALE通过难度感知的资源分配，突破了当前测试时扩展方法的固有局限，实现了更高性能与更优资源利用的统一。

Abstract: Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.

</details>


### [256] [CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning](https://arxiv.org/abs/2512.00496)
*Diego A. B. Moreira,Alef I. Ferreira,Jhessica Silva,Gabriel O. dos Santos,Gustavo Bonil,João Gondim,Marina dos Santos,Helena Maia,Simone Hashiguti,Nádia da Silva,Carolina Scarton,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本文提出了一种名为CACARA的多模态与多语言架构，通过新兴对齐学习（emergent alignment learning）实现新模态的无缝集成和多语言能力的涌现，无需全量重训练或显式多语言预训练，显著降低计算开销并提升跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态模型依赖资源密集型的多模态联合训练，且扩展至新语言时同样需大量计算；亟需一种高效、可扩展的多模态多语言建模范式。

Method: 提出CACARA架构，采用新兴对齐学习策略：仅对新增模态在英文对齐数据上微调，不更新原有文本编码器，从而在不进行多语言预训练的前提下，使模型自动涌现出支持100多种语言的能力。

Result: 在音频到文本检索任务中R@1指标提升最高达14.24个百分点，超越当前最优多模态模型，且训练成本与单语模型相当。

Conclusion: 新兴对齐学习能同时实现高效多模态扩展与多语言能力涌现，为构建轻量、可扩展、知识保留的多模态多语言系统提供了新范式。

Abstract: As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.

</details>


### [257] [G-KV: Decoding-Time KV Cache Eviction with Global Attention](https://arxiv.org/abs/2512.00504)
*Mengqi Liao,Lu Wang,Chaoyun Zhang,Zekai Shen,Xiaowei Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Huaiyu Wan*

Main category: cs.CL

TL;DR: 提出了一种名为G-KV的KV缓存驱逐方法，结合局部和历史注意力得分进行全局评分，并通过强化学习和蒸馏优化模型。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法多关注提示压缩或基于局部注意力分数的令牌驱逐，忽视了令牌的长期重要性。

Method: 提出G-KV，采用结合局部和历史注意力得分的全局评分机制来评估令牌重要性，并引入强化学习与蒸馏的后训练技术以适应压缩KV缓存环境。

Result: 该方法能更准确地识别重要令牌，提升推理效率，有效缓解长序列带来的计算与内存压力。

Conclusion: G-KV通过全局重要性评估和模型优化，在保持性能的同时显著提升了推理效率，适用于复杂任务中的大模型部署。

Abstract: Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.

</details>


### [258] [Developing a Comprehensive Framework for Sentiment Analysis in Turkish](https://arxiv.org/abs/2512.00515)
*Cem Rifki Aydin*

Main category: cs.CL

TL;DR: 本文提出了一套全面的面向土耳其语的情感分析框架，并针对英语提出了若干专用方法，涵盖特征工程、词典构建、形态学分析、神经网络架构及词嵌入等多个方面，取得了土耳其语和英语上的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 针对土耳其语情感分析研究缺乏系统性框架的问题，以及英语情感分析中现有模型在多方面（如句法、语义、词汇）建模不足的现状，开展综合性研究。

Method: 结合无监督、半监督与监督指标构建新型特征集；采用半监督领域特定方法构建土耳其语极性词典；对土耳其语进行细粒度形态学极性分析；设计融合循环与递归神经网络的新型架构（英语）；构建融合情感、句法、语义和词汇特征的新型词嵌入（双语）；将上下文窗口重新定义为子句（英语）。

Result: 在土耳其语和英语多种类型数据集上，经典机器学习方法超越神经网络模型；建成首个土耳其语领域极性词典；实现土耳其语词素级极性标注；提出可推广至其他黏着语的形态分析方法；英语新词嵌入与子句上下文建模取得SOTA性能；所有主要方法均达当时最优水平。

Conclusion: 本工作是截至2020年7月最详尽、最全面的土耳其语情感分析研究，同时显著推动了英语意见分类的发展，其多项技术具备跨语言与跨任务迁移潜力。

Abstract: In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.

</details>


### [259] [Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity](https://arxiv.org/abs/2512.00552)
*Subramanyam Sahoo,Vinija Jain,Saanidhya Vats,Siddharth Mohapatra,Rui Min,Aman Chadha,Divya Chaudhary*

Main category: cs.CL

TL;DR: 提出一种新的诊断框架，通过四个维度评估语言模型的数学推理能力，揭示仅依赖答案准确率可能掩盖逻辑计算的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要依赖答案准确率，无法发现模型在数学推理中的根本性逻辑错误，容易高估其真实推理能力。

Method: 构建了一个包含前向-后向一致性、传递性覆盖、反事实敏感性和扰动鲁棒性的四维诊断框架，并在Qwen3-0.6B模型和MenatQA数据集上进行案例研究。

Result: 模型虽有70%以上的答案准确率，但后向一致性仅为15%，传递性覆盖为32.2%，对扰动敏感，显示出推理脆弱性。

Conclusion: 该框架能有效识别出传统指标忽略的推理失败，表明小规模模型更依赖模式匹配而非真正逻辑推理；框架本身具有模型无关性，可用于更广泛的模型评估。

Abstract: Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.

</details>


### [260] [Slovak Conceptual Dictionary](https://arxiv.org/abs/2512.00579)
*Miroslav Blšták*

Main category: cs.CL

TL;DR: 本文介绍了一种针对斯洛伐克语的新概念词典，旨在解决该低资源语言缺乏足够机器可读语言数据的问题。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语作为低资源语言，缺乏足够规模的机器可读语言数据源，导致许多自然语言处理任务效果较差甚至无法完成。

Method: 构建首个面向斯洛伐克语的概念词典，作为其首类语言学工具。

Result: 提出了斯洛伐克语首个概念词典，填补了该语言在高质量词典资源方面的空白。

Conclusion: 该词典为提升斯洛伐克语自然语言处理任务性能提供了基础支持，有助于缓解低资源语言的数据匮乏问题。

Abstract: When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.

</details>


### [261] [Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2512.00590)
*Alla Chepurova,Aydar Bulatov,Yuri Kuratov,Mikhail Burtsev*

Main category: cs.CL

TL;DR: 本文提出了Wikontic，一种多阶段知识图谱（KG）构建流水线，从开放域文本中提取带限定符的候选三元组，结合Wikidata类型与关系约束，并进行实体归一化，生成紧凑、本体一致、高连通性的KG；在多个基准上表现优异，且构建开销显著低于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统常将KG作为辅助检索结构，未充分挖掘其内在质量；本文旨在提升KG构建质量，使其能独立支撑问答等任务，减少对原始文本的依赖。

Method: 提出Wikontic多阶段流水线：1）从开放域文本抽取带限定符的候选三元组；2）基于Wikidata施加类型与关系约束；3）实体归一化以降低冗余；最终生成高质量KG。

Result: 在MuSiQue中96%生成三元组含正确答案实体；HotpotQA达76.0 F1，MuSiQue达59.8 F1，超越部分RAG基线；MINE-1信息保留率达86%，为SOTA；构建token消耗不足1000，远少于AriGraph和GraphRAG。

Conclusion: Wikontic显著提升了KG的内在质量与实用性，证明高质量KG可独立支撑复杂推理任务，为LLM高效利用结构化知识提供了可扩展新范式。

Abstract: Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.

</details>


### [262] [Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior](https://arxiv.org/abs/2512.00611)
*Franck Binard,Vanja Kljajevic*

Main category: cs.CL

TL;DR: Prism是一种小型、可组合的元语言，用于规范工具使用型软件代理的行为，其核心是固定的Core1语法，支持自然语言决策规则到可检查、可执行策略的映射。


<details>
  <summary>Details</summary>
Motivation: 解决现有工具使用型智能体缺乏统一、可组合、可验证的策略描述语言的问题，提升策略的可解释性、安全性与跨领域复用性。

Method: 设计基于固定核心（Core1）的元语言Prism，引入抽象算子与组合子机制，通过领域扩展机制支持不同应用场景；将自然语言决策规则形式化为可执行策略表达式。

Result: 在恒温器控制、家庭安防、电商推荐和医疗监测等场景中验证了Prism的实用性，实现了自然语言规则到可检验、可执行策略的映射，并展现出良好的可分析性与安全性约束能力。

Conclusion: Prism提供了一种兼顾语言学清晰性与工程实用性的代理控制接口语言，实现了核心语法与领域词典的分离，以及内部策略与外部工具的语义桥接。

Abstract: Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.

</details>


### [263] [ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization](https://arxiv.org/abs/2512.00617)
*Omer Jauhar Khan*

Main category: cs.CL

TL;DR: 本文提出ART（自适应响应调优）框架，通过ELO排名和多智能体推理提升大语言模型输出质量，显著提高响应准确性、连贯性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型响应常存在不一致、幻觉及跨领域质量波动等问题，需系统性优化方法。

Method: 采用锦标赛式ELO评分机制与多智能体协同推理，支持动态智能体选择、可配置锦标赛参数及多种共识融合策略。

Result: 实验显示ART在响应准确性、连贯性和可靠性上显著优于单模型基线，整体质量指标提升8.4%，ELO评分收敛R22值超0.96。

Conclusion: ART是一种可扩展、可用于生产的高质量LLM响应优化框架，适用于对响应质量要求严苛的实际应用场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.

</details>


### [264] [Sycophancy Claims about Language Models: The Missing Human-in-the-Loop](https://arxiv.org/abs/2512.00656)
*Jan Batzner,Volker Stocker,Stefan Schmid,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）谄媚响应模式的测量方法学挑战，识别出五种核心操作化定义，并指出当前研究缺乏对人类感知的评估，同时提出未来研究的可行建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽声称LLM存在谄媚响应现象，但在测量方法上存在诸多挑战，且未从人类视角进行评估，也难以与AI对齐中的相关概念区分。

Method: 文献综述与概念分析，系统梳理测量LLM谄媚性的方法学问题，识别五种核心操作化定义，并分析其与AI对齐中其他概念的边界。

Result: 明确了当前测量LLM谄媚性存在的主要问题：缺乏人类感知评估、概念边界模糊、操作化不统一。

Conclusion: 需加强以人类为中心的实证评估，并建立更清晰的概念框架与测量标准，以推动LLM谄媚性研究的严谨性与实用性。

Abstract: Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.

</details>


### [265] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 本文提出一种基于交互式可视化知识图谱的框架，用于识别和缓解大语言模型在企业场景中的幻觉问题，通过将专有知识与模型输出关联并标注置信度，支持用户诊断、反馈与迭代优化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在企业中因上下文窗口限制和预训练数据与领域知识不一致，易产生难以察觉的幻觉，现有缓解方法缺乏确定性保障。

Method: 构建交互式可视化知识图谱，将企业专有知识与模型生成内容结构化关联，并标注各断言的置信度与来源依据。

Result: 实现了可解释、可干预的人机协同工作流，使用户能直观识别幻觉区域、诊断推理薄弱环节并提供针对性反馈。

Conclusion: 该框架通过可视化知识图谱增强模型透明度与可控性，形成闭环反馈机制，有效提升企业级LLM应用的可靠性与响应质量。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [266] [A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data](https://arxiv.org/abs/2512.00673)
*Breanna E. Green,Ashley L. Shea,Pengfei Zhao,Drew B. Margolin*

Main category: cs.CL

TL;DR: 本文评估了GPT-4等大语言模型在复杂语义分类与标注任务中的表现，发现其虽有一定能力，但在处理细微语言时仍显著逊于人工标注，需谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI（如ChatGPT）在计算社会科学中日益普及，但其在需理解细微语言的复杂分类与标注任务中的实际性能尚缺乏深入评估。

Method: 对GPT-3.5、GPT-4和GPT-4o进行实验，设计四种提示风格，以精确率、召回率和F1值定量评估，并结合定性分析考察模型对细微语言的理解能力。

Result: GPT-4在含细微语言的数据集上分类效果有限；加入标签定义可小幅提升性能；定性分析揭示出四个具体问题；整体表现仍明显弱于人类标注者。

Conclusion: 在涉及细微语言的分类任务中，应审慎使用ChatGPT类工具，不可替代人工标注或未经验证直接部署。

Abstract: Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.

</details>


### [267] [FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case](https://arxiv.org/abs/2512.00745)
*Md Abdullah Al Kafi,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文提出了一种语言无关的基于Transformer的词性标注框架，适用于低资源语言（以孟加拉语和印地语为例），仅需少量代码修改即可跨语言迁移，准确率分别达96.85%和97%，具有模块化、开源、易适配等特点。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言词性标注中模型设计复杂、跨语言迁移困难、数据不平衡与语言重叠带来的挑战，推动小语种NLP发展。

Method: 构建语言无关的Transformer-based POS标注框架，强调模块化设计与最小代码修改实现跨语言适配（如从孟加拉语到印地语仅需三行代码），支持底层Transformer架构灵活替换。

Result: 在孟加拉语和印地语上分别达到96.85%和97%的token-level准确率，F1分数稳健；揭示了特定POS类别性能差异，反映数据集构建问题。

Conclusion: 该框架通过架构通用性与工程简洁性，显著降低低资源语言NLP模型开发门槛，使研究者可聚焦于语言预处理与数据优化等关键环节。

Abstract: This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.

</details>


### [268] [Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation](https://arxiv.org/abs/2512.00789)
*Xiaodong Cai,Hai Lin,Shaoxiong Zhan,Weiqi Luo,Hong-Gee Kim,Hongyan Hao,Yu Yang,Hai-Tao Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种无需超参数的熵均衡采样（EES）方法，通过信息论原理动态平衡归一化熵与概率质量，提升大语言模型文本生成的质量、多样性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有token采样策略依赖额外超参数，需大量调优，部署复杂。

Method: 提出熵均衡采样（EES），基于信息论，动态调整候选集，平衡归一化熵与概率质量，无需引入超参数。

Result: 在多种模型架构的推理与生成任务上验证，EES在不同温度设置下均保持高准确性、连贯性与多样性，且免调参。

Conclusion: EES是一种轻量、通用、即插即用的采样策略，显著简化部署并提升生成性能。

Abstract: Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES

</details>


### [269] [Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy](https://arxiv.org/abs/2512.00829)
*Md Mehrab Hossain Opi,Sumaiya Khan,Moshammad Farzana Rahman*

Main category: cs.CL

TL;DR: 本研究探讨了在孟加拉语自然语言处理（NLP）中使用自动混合精度（AMP）训练，以提升计算效率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 由于高端硬件资源有限，孟加拉语NLP模型的训练面临巨大挑战，亟需提高计算效率的方法。

Method: 采用自动混合精度（AMP）技术，结合16位和32位浮点运算，在四个基于Transformer的模型上评估其在四种孟加拉语NLP任务中的表现。

Result: AMP使训练速度提升了44.5%，内存消耗减少了17.6%，同时保持F1分数达到全精度基线的99.7%。

Conclusion: AMP能有效降低硬件需求，有助于在资源受限环境中推广先进的NLP技术。

Abstract: Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.

</details>


### [270] [WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](https://arxiv.org/abs/2512.00837)
*Yukang Lin,Jiahao Shao,Shuoran Jiang,Wentao Zhu,Bingjie Lu,Xiangping Wu,Joanna Siebert,Qingcai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于种子池控制的文本水印框架WaterSearch，通过在句子级别联合优化分布保真度和水印信号特性，提升了大语言模型生成文本的水印可检测性与文本质量，并展现出强抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在可检测性和文本质量之间存在权衡，难以兼顾下游任务性能与水印鲁棒性。

Method: 设计了一种控制种子池的嵌入方案，提出WaterSearch框架，在句子级别进行搜索式水印生成，并结合句子级检测方法以增强抗攻击能力。

Result: 在三个主流LLM和十项任务上实验表明，WaterSearch在95%水印强度下平均性能提升51.01%，在短文本和低熵输出等挑战场景下仍分别提升47.78%和36.47%，且对插入、同义替换和改写等攻击具有强鲁棒性。

Conclusion: WaterSearch有效平衡了水印强度与生成质量，显著优于现有方法，具备良好的实用性与抗攻击能力。

Abstract: Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.
  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.

</details>


### [271] [Less is More: Resource-Efficient Low-Rank Adaptation](https://arxiv.org/abs/2512.00878)
*Chunlin Tian,Xuyang Wei,Huanrong Liu,Zhijiang Guo,Li Li*

Main category: cs.CL

TL;DR: 本文提出EffiLoRA，一种轻量级、通用的低秩自适应方法，通过跨层共享A矩阵和运行时选择性更新B矩阵，在语言、多模态和扩散模型中实现更高效、鲁棒的参数高效微调。


<details>
  <summary>Details</summary>
Motivation: LoRA虽广泛应用，但在复杂数据集上仍存在显著开销和参数干扰问题；现有解耦方法训练成本依然较高。

Method: EffiLoRA采用跨所有Transformer层统一的A矩阵，并引入运行时选择性B矩阵更新机制，以动态权衡系统资源与模型性能。

Result: EffiLoRA在常识推理、视觉指令微调和图像生成等多模态任务上持续优于LoRA，展现出更高效率和更强鲁棒性。

Conclusion: EffiLoRA是一种资源高效、可泛化至多种模型架构的PEFT新范式，有效缓解了LoRA的冗余与干扰问题。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.

</details>


### [272] [Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios](https://arxiv.org/abs/2512.00920)
*Jianxiang Zang,Yongda Wei,Ruxue Bai,Shiyu Jiang,Nijia Mo,Binhong Li,Qiang Sun,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出Reward Auditor框架，用于评估奖励模型（RM）在现实扰动场景下的适用性（Suitability），通过假设检验量化其偏好感知置信度分布退化程度，从而推断系统性脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注特定场景下的偏好判断准确率，无法揭示奖励模型在真实世界中的关键脆弱性；真正的挑战在于评估其在现实扰动下的条件可靠性（即适用性）。

Method: 提出Reward Auditor——一种基于假设检验的审计框架，不直接评估准确率，而是通过统计检验分析RM在真实扰动下偏好置信度分布的退化程度，量化统计显著性与效应量。

Result: Reward Auditor能可靠推断RM在多种现实场景中系统性脆弱性的存在性、确定性与严重程度，为构建可验证安全、更鲁棒、更可信的下一代大模型对齐系统奠定基础。

Conclusion: 适用性（Suitability）是衡量奖励模型真实可靠性的重要新维度，Reward Auditor为该维度提供了首个可量化、可审计的科学评估范式。

Abstract: Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.

</details>


### [273] [Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study](https://arxiv.org/abs/2512.00931)
*Imane Jaaouine,Ross D. King*

Main category: cs.CL

TL;DR: 该研究探讨了提示工程（PE）方法在零样本设置下减少大语言模型（LLM）科学文本摘要中上下文不一致幻觉的效果，发现上下文重复和随机添加句子能显著提升摘要与原文的词汇对齐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成输出时可能出现与用户提示不一致的上下文不一致幻觉，影响其在科学文本摘要中的可靠性，因此需要探索有效的缓解策略。

Method: 在八篇酵母生物技术论文摘要上测试六种指令调优的LLM，采用七种提示方法（基线、两种复杂度的PE、两种上下文重复CR-K1/2、两种随机添加RA-K1/2），生成336个摘要并用六种指标评估其与原文的对齐程度。

Result: 上下文重复（CR）和随机添加（RA）方法显著提高了LLM生成摘要与原文之间的词汇对齐；基于3744个数据点的统计分析支持这一结论。

Conclusion: 提示工程技术，特别是上下文重复和随机添加，有助于减轻零样本科学文本摘要中的上下文不一致幻觉，提升生成结果的准确性。

Abstract: Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.

</details>


### [274] [DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics](https://arxiv.org/abs/2512.00938)
*Ahmed Mustafa Younes*

Main category: cs.CL

TL;DR: 本文提出DeformAr框架，用于诊断和解释阿拉伯语与英语在命名实体识别（NER）任务中性能差异的原因，结合跨组件分析与行为分析，支持数据与模型层面的细粒度可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在阿拉伯语NER任务上表现不佳，原因包括分词、数据集质量及标注不一致等多方面因素，但以往研究多孤立分析，缺乏对这些因素协同影响的系统性考察。

Method: 提出DeformAr——一个面向阿拉伯语NER系统的调试与评估框架，包含数据提取库与交互式仪表板，支持跨组件分析（分析数据与模型子模块间交互）和行为分析（融合可解释性技术、词元级指标、可视化与表征空间分析）。

Result: DeformAr实现了对阿拉伯语NER系统性能差异的组件级归因分析，能定位异常模型行为并关联至底层表征模式与数据因素，是首个专为阿拉伯语设计的基于组件的可解释性工具。

Conclusion: DeformAr为低资源语言（尤其是阿拉伯语）的模型分析提供了新范式，强调需从数据与模型协同角度理解性能瓶颈，推动更鲁棒、可解释的跨语言NER系统发展。

Abstract: Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.
  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.
  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.

</details>


### [275] [Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data](https://arxiv.org/abs/2512.00946)
*Alvaro Paredes Amorin,Andre Python,Christoph Weisser*

Main category: cs.CL

TL;DR: 本文评估了轻量级开源大语言模型（如Qwen3 8B、Llama3 8B等）在金融文本情感分析任务中的泛化能力，发现其在少量数据（甚至5%）、零样本/少样本设置下可媲美或超越FinBERT等专用模型，表明其是低成本、高可用的替代方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融文本分析中依赖昂贵且受限的计算资源和专有数据，而多数研究者和从业者难以获取；本文旨在考察轻量级开源LLM在真实受限条件下的可行性与泛化能力。

Method: 在五个公开金融情感数据集（涵盖英文与中文、不同格式与来源）上，对比FinBERT与三个轻量级开源LLM（DeepSeek-LLM 7B、Llama3 8B Instruct、Qwen3 8B），评估其在全量、子集（5%）、零样本及少样本设置下的性能。

Result: Qwen3 8B和Llama3 8B在大多数场景下表现最优，即使仅用5%训练数据或零/少样本设定仍保持强劲性能；显著优于FinBERT在部分跨源/跨语言任务上的表现。

Conclusion: 轻量级开源LLM可在资源受限条件下实现与专用模型相当甚至更优的金融情感分析效果，为学术界和中小机构提供了高性价比、易部署的实用工具。

Abstract: Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.

</details>


### [276] [Table as a Modality for Large Language Models](https://arxiv.org/abs/2512.00947)
*Liyao Li,Chao Ye,Wentao Ye,Yifei Sun,Zhe Jiang,Haobo Wang,Jiaming Tian,Yiming Zhang,Ningtao Wang,Xing Fu,Gang Chen,Junbo Zhao*

Main category: cs.CL

TL;DR: 本文提出了TAMO框架，通过将表格视为独立模态并与文本融合，利用超图神经网络作为全局表格编码器与大语言模型结合，有效保留了表格结构信息，在多个基准数据集上实现了平均42.65%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理表格数据时通常依赖序列化输入，导致结构信息丢失，难以有效理解表格内容，因此需要一种能更好保留和利用表格结构的方法。

Method: 提出TAMO框架，采用超图神经网络（Hypergraph Neural Network）作为表格编码器，将表格作为独立模态与文本token融合，构建一个与主流大语言模型无缝集成的多模态框架。

Result: 在HiTab、WikiTQ、WikiSQL、FeTaQA和StructQA等多个表格推理基准上取得显著性能提升，平均相对增益达42.65%，验证了方法在泛化能力上的优势。

Conclusion: 将表格视为独立模态并结合结构编码器可有效弥补当前大语言模型在表格理解上的不足，TAMO为多模态语言模型中的表格处理提供了新范式。

Abstract: To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.

</details>


### [277] [Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent](https://arxiv.org/abs/2512.00986)
*Zhihan Guo,Feiyang Xu,Yifan Li,Muzhi Li,Shuai Zou,Jiele Wu,Han Shi,Haoli Bai,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 本文提出了Dr.Mi-Bench，一个面向科学领域深度研究（DR）代理的模块化集成基准，并配套提出Dr.Mi-Eval评估范式，以全面评估DR代理在规划、检索与推理方面的能力；实验揭示了当前DR代理在多源检索和跨学科一致性上的关键短板，并指出提升高层规划能力是释放基础大模型推理潜力的关键。


<details>
  <summary>Details</summary>
Motivation: 现有DR代理评测基准存在两大缺陷：一是偏重检索，忽视高阶规划与推理；二是偏向通用领域，缺乏对核心应用场景——科学领域的覆盖。

Method: 构建基于学术文献的人类标注数据集（200个实例，覆盖10个科学领域），提出模块化集成评估范式Dr.Mi-Eval，支持端到端DR代理评估与基础大模型孤立评估两种互补模式。

Result: 实验发现DR代理性能呈碎片化：各具专长但普遍存在多源检索（尤其综述类任务）和跨学科表现不一致等关键弱点；提升高层规划能力是激活基础大模型推理潜力的关键因素。

Conclusion: Dr.Mi-Bench不仅填补了科学领域DR代理评测的空白，更通过暴露可操作的失败模式，为构建更可靠的学术研究助手提供了诊断性工具和发展指引。

Abstract: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.

</details>


### [278] [Advancing Academic Chatbots: Evaluation of Non Traditional Outputs](https://arxiv.org/abs/2512.00991)
*Nicole Favero,Francesca Salute,Daniel Hardt*

Main category: cs.CL

TL;DR: 本文扩展了大语言模型（LLM）评估的范围，对比Graph RAG与Advanced RAG两种检索策略在问答任务中的表现，并首次系统评估LLM生成幻灯片和播客脚本等非传统学术输出的能力；结果表明Advanced RAG配合GPT-4o mini效果最优，Graph RAG易致幻觉，而人类评审对格式与风格缺陷的识别不可替代。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估多集中于标准任务（如事实问答、短摘要），缺乏对新型检索策略和非传统学术输出（如幻灯片、播客脚本）的系统评估。

Method: 对比Graph RAG（基于知识图谱）与Advanced RAG（关键词+语义混合检索）在问答任务中的性能；构建LLaMA 3 70B与GPT-4o mini混合原型系统；采用人工评分（11个维度）与LLM裁判交叉验证QA质量；对幻灯片与播客脚本生成任务进行文档增强检索测试。

Result: GPT-4o mini + Advanced RAG在问答准确率上最优；Graph RAG改进有限且幻觉增多；GPT-4o mini在幻灯片/播客生成中表现最佳，LLaMA 3在叙事连贯性上有潜力；人工评审在识别排版与风格问题上不可或缺。

Conclusion: Advanced RAG更适配当前LLM QA任务；Graph RAG需简化结构与自动化；评估非传统学术输出必须融合人工与LLM评判；开放模型（如LLaMA 3）在特定维度具备竞争力。

Abstract: Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.

</details>


### [279] [When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals](https://arxiv.org/abs/2512.01037)
*Riad Ahmed Anonto,Md Labid Al Nahiyan,Md Tanvir Hassan,Ch. Md. Rakin Haider*

Main category: cs.CL

TL;DR: 本文提出了一种名为“语义混淆”的新失败模式，用于衡量安全对齐语言模型在面对语义相同但表面形式不同的提示时的局部不一致性，并构建了ParaGuard数据集和三个模型无关的细粒度指标来量化该问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅报告全局拒绝率等指标，忽略了模型对语义相同但措辞不同的提示表现出不一致拒绝行为（即局部不一致性）的问题，导致难以精准诊断和优化模型的安全对齐性能。

Method: 提出‘语义混淆’概念；构建包含10,000个提示、按语义意图聚类的受控同义改写数据集ParaGuard；设计三种模型无关、基于token嵌入、next-token概率和困惑度的细粒度指标：Confusion Index、Confusion Rate 和 Confusion Depth。

Result: 实验表明，全局误拒率掩盖了关键结构差异；新指标可揭示模型决策边界的全局不稳定、局部不一致热点，以及更严格拒绝策略未必加剧混淆的现象；混淆感知审计能区分‘拒绝频率’与‘拒绝合理性’。

Conclusion: 语义混淆是安全对齐评估中被忽视的关键维度；所提框架为开发者提供了实用信号，在不牺牲安全性的前提下有效降低误拒率。

Abstract: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

</details>


### [280] [ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages](https://arxiv.org/abs/2512.01077)
*Neha Joshi,Pamir Gogoi,Aasim Mirza,Aayush Jansari,Aditya Yadavalli,Ayushi Pandey,Arunima Shukla,Deepthi Sudharsan,Kalika Bali,Vivek Seshadri*

Main category: cs.CL

TL;DR: 本文介绍了一个名为ELR-1000的多模态数据集，包含来自印度东部偏远地区10种濒危语言的1,060个传统食谱，旨在支持濒危语言和文化保护。研究发现，尽管大型语言模型在翻译这些食谱时表现不佳，但提供特定文化背景信息可显著提升翻译质量。作者呼吁建立面向弱势语言和技术的文化敏感型基准，并公开该数据集以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 濒危语言和传统文化正面临消失的风险，现有语言技术难以处理低资源、具文化特异性的语言内容，亟需专门的数据集和评估基准来推动公平、文化敏感的语言技术发展。

Method: 通过为数字素养较低的用户提供移动界面，众包收集来自印度东部偏远地区10种濒危语言的传统食谱，构建了包含语言与文化语境的多模态数据集ELR-1000；并评估多种大型语言模型在有无文化背景提示下的英文翻译性能。

Result: 实验表明，大型语言模型在直接翻译濒危语言食谱时表现较差，但在引入语言背景、翻译示例和文化保留指南等上下文后，翻译质量显著提升。

Conclusion: 需要开发面向濒危语言和特定文化领域的语言技术，并通过提供文化上下文增强模型表现；发布ELR-1000数据集以促进对低资源、文化敏感型NLP任务的研究。

Abstract: We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.

</details>


### [281] [How do we measure privacy in text? A survey of text anonymization metrics](https://arxiv.org/abs/2512.01109)
*Yaxuan Ren,Krithika Ramesh,Yaxing Yao,Anjalie Field*

Main category: cs.CL

TL;DR: This paper conducts a systematic survey of privacy evaluation metrics in text anonymization, identifying six privacy notions, analyzing their alignment with legal standards (HIPAA/GDPR) and user expectations, and providing practical guidance to improve robustness, comparability, and legal awareness in privacy evaluation.


<details>
  <summary>Details</summary>
Motivation: Evaluating whether text anonymization methods sufficiently protect privacy remains an open challenge, despite its importance for NLP research on sensitive data.

Method: Manually reviewing 47 papers reporting privacy metrics, identifying and comparing six distinct privacy notions, and analyzing their alignment with legal standards (HIPAA, GDPR) and user-centered expectations from HCI studies.

Result: Six distinct privacy notions and their associated metrics are identified and compared; their alignment with legal standards and user expectations is assessed; gaps in current evaluation practices are highlighted.

Conclusion: The analysis provides practical guidance for navigating privacy evaluation approaches and aims to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.

Abstract: In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.

</details>


### [282] [DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](https://arxiv.org/abs/2512.01174)
*Hyunjun Kim,Sooyoung Ryu*

Main category: cs.CL

TL;DR: 本文提出了DrawingBench，一个面向具身智能体（agentic LLMs）的可验证评估框架，聚焦空间推理与GUI动作序列生成任务，强调透明性、可审计性和人类监督；实验表明外部多轮反馈显著提升性能，而明确可验证的指令比任务复杂度更能决定成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏透明性和可审计性，难以可靠评估自主AI代理的行为可信度；亟需可验证、可复现、可人工监督的评估方法。

Method: 构建DrawingBench框架：包含250个多样化提示（20类、4难度）、8条客观评分准则、动作级行为审计机制、确定性评估指标及基于多轮反馈的外部监督机制；在1000次测试中评估4个SOTA模型。

Result: 模型最高达92.8%完美表现；外部反馈平均提升3.2%（复杂场景最高+32.8%）；发现工具状态管理和长程规划存在系统性错误；当指令具备明确可验证标准时，性能达100%。

Conclusion: 透明、规则驱动且支持人工监督的评估框架能有效建立对具身智能体的信任；外部监督比自我修正更可靠；该开源框架为可信代理评估提供了通用范式。

Abstract: As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench

</details>


### [283] [TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness](https://arxiv.org/abs/2512.01183)
*Yongxin Zhou,Philippe Mulhem,Didier Schwab*

Main category: cs.CL

TL;DR: 本文系统研究了检索增强生成（RAG）系统中检索噪声（文本扰动）与生成温度参数之间的交互影响，提出了一个扰动-温度分析框架，并在HotpotQA上验证了不同扰动类型和温度设置对性能的影响模式，为RAG鲁棒性评估与调参提供了新方法和实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估通常孤立地考察检索质量或生成参数（如temperature），忽视二者间的交互效应；实际中检索噪声与生成随机性共同影响最终结果，亟需系统分析其耦合关系。

Method: 提出RAG Perturbation-Temperature Analysis Framework：对检索文档施加三类文本扰动（模拟噪声），并在多个温度设置下运行多种LLM；在HotpotQA数据集上开展跨模型（开源与闭源）实验，量化性能变化规律。

Result: 发现高温显著放大扰动带来的性能下降；不同扰动类型对温度的敏感性呈现非线性特征；识别出特定温度区间下模型对某些扰动尤为脆弱。

Conclusion: RAG系统鲁棒性高度依赖于扰动类型与温度的联合效应；该框架可作为诊断基准，支持更可靠的模型选型与参数配置，尤其在真实 noisy检索场景下。

Abstract: The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.

</details>


### [284] [Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks](https://arxiv.org/abs/2512.01191)
*Krithik Vishwanath,Mrigayu Ghosh,Anton Alyakin,Daniel Alexander Alber,Yindalon Aphinyanaphongs,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 本研究对比评估了两种专用临床AI系统（OpenEvidence和UpToDate Expert AI）与三种前沿通用大语言模型（GPT-5、Gemini 3 Pro、Claude Sonnet 4.5），发现通用模型在医学知识与临床对齐任务中普遍优于专用临床工具，揭示其在完整性、沟通质量、上下文意识和系统性安全推理方面的不足，强调需加强独立透明的临床AI评估。


<details>
  <summary>Details</summary>
Motivation: 专用临床AI助手正快速进入医疗实践，常被宣传为比通用大语言模型更安全可靠，但其缺乏独立、定量的评估，导致关键证据空白，而其影响已在诊断、分诊和指南解读中日益扩大。

Method: 采用包含1000道题目的混合基准测试（MedQA+HealthBench），对两类专用临床AI系统（OpenEvidence、UpToDate Expert AI）与三类前沿通用大语言模型（GPT-5、Gemini 3 Pro、Claude Sonnet 4.5）进行对比评估。

Result: 通用大语言模型整体表现优于专用临床AI工具；GPT-5得分最高；OpenEvidence和UpToDate在完整性、沟通质量、上下文意识及系统性安全推理方面存在明显缺陷。

Conclusion: 面向临床决策支持的专用AI工具实际性能可能落后于前沿通用大模型，亟需在部署至患者相关工作流前开展透明、独立的评估。

Abstract: Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.

</details>


### [285] [Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework](https://arxiv.org/abs/2512.01198)
*Jiatong Han*

Main category: cs.CL

TL;DR: 本研究提出一种基于“人在回路”（HITL）框架的中医典籍翻译方法，利用大语言模型（如DeepSeek V3.1）通过提示工程识别《黄帝内经》中的隐喻与转喻，并结合ChatGPT和Gemini模拟读者评估不同译文。结果显示，经提示调整的LLM译文在认知维度上表现最优，具高一致性，为中医等概念密集型古籍翻译提供了高效、可复制的认知路径。


<details>
  <summary>Details</summary>
Motivation: 现有中医英译多依赖直译，难以让目标语读者重构其基于意象思维的理论体系，导致临床应用困难。因此需探索能有效传递隐喻与转喻结构的翻译方法，以提升跨文化理解与实践转化。

Method: 采用人在回路（HITL）框架，选取《黄帝内经》四段核心文本，通过提示引导DeepSeek V3.1识别源文中的隐喻与转喻并生成译文；使用ChatGPT 5 Pro与Gemini 2.5 Pro模拟三类真实读者，从五个认知维度评分比较人工、基线模型与提示调整后的译文，辅以结构化访谈与诠释现象学分析（IPA）。

Result: 提示调整后的LLM译文在所有五个认知维度上表现最佳，且在不同模型与读者角色间具高度一致性；访谈揭示了人机翻译差异、有效的隐喻/转喻传递策略及读者认知偏好。

Conclusion: 该研究验证了基于提示的认知支架式HITL方法在中医典籍翻译中的有效性，为古代概念密集型文本的跨语言传播提供了可复制、高效且符合认知规律的新路径。

Abstract: Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.

</details>


### [286] [Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language](https://arxiv.org/abs/2512.01256)
*Ekha Morang,Surhoni A. Ngullie,Sashienla Longkumer,Teisovi Angami*

Main category: cs.CL

TL;DR: This paper presents the first sentiment analysis and emotion classification study for the Nagamese language, building a sentiment polarity lexicon of 1,195 words and applying Naïve Bayes and SVM for classification.


<details>
  <summary>Details</summary>
Motivation: No prior work exists on sentiment analysis for Nagamese, a low-resource creole language; this study aims to fill that gap.

Method: Constructed a sentiment polarity lexicon of 1,195 Nagamese words and used it with additional features in supervised machine learning models (Naïve Bayes and SVM) for sentiment (positive/negative/neutral) and basic emotion classification.

Result: First-ever sentiment polarity lexicon and classification framework for Nagamese, enabling polarity and emotion detection.

Conclusion: The study successfully establishes a foundational resource and methodology for sentiment and emotion analysis in Nagamese, demonstrating feasibility despite its low-resource status.

Abstract: The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na"ive Bayes and Support Vector Machines.
  Keywords: Nagamese, NLP, sentiment analysis, machine learning

</details>


### [287] [SUPERChem: A Multimodal Reasoning Benchmark in Chemistry](https://arxiv.org/abs/2512.01274)
*Zehua Zhao,Zhixian Huang,Junren Li,Siyu Lin,Junting Zhou,Fengqi Cao,Kun Zhou,Rui Ge,Tingting Long,Yuexiang Zhu,Yan Liu,Jie Zheng,Junnian Wei,Rong Zhu,Peng Zou,Wenyu Li,Zekai Cheng,Tian Ding,Yaxuan Wang,Yizhao Yan,Tingru Wei,Haowei Ming,Weijie Mao,Chen Sun,Yiming Liu,Zichen Wang,Zuo Zhang,Tong Yang,Hao Ma,Zhen Gao,Jian Pei*

Main category: cs.CL

TL;DR: SUPERChem is a new benchmark with 500 expert-curated, reasoning-intensive chemistry problems designed to evaluate LLMs' chemical reasoning beyond simple answer accuracy, using Reasoning Path Fidelity (RPF) scoring and multimodal/text-only formats.


<details>
  <summary>Details</summary>
Motivation: Existing chemistry benchmarks are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills.

Method: Introduce SUPERChem: a benchmark of 500 expert-curated, reasoning-intensive chemistry problems across subfields, in multimodal and text-only formats; use iterative curation to eliminate flaws and contamination; pair each problem with expert-authored solution paths for RPF scoring.

Result: Even top models (e.g., GPT-5 High: 38.5%) underperform human baseline (40.3%); SUPERChem reveals multi-step/multimodal reasoning, model-dependent visual effects, and distinguishes high-fidelity from heuristic reasoners.

Conclusion: SUPERChem provides a challenging, reliable benchmark and evaluation framework to advance LLMs toward expert-level chemical intelligence.

Abstract: Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.

</details>


### [288] [Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning](https://arxiv.org/abs/2512.01282)
*Jiahao Yuan,Zhiqing Cui,Hanqing Wang,Yuansheng Gao,Yucheng Zhou,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了KardiaBench，一个大规模、基于真实用户画像的对话情感理解基准，以及Kardia-R1框架，通过可解释的评分规则强化学习实现身份感知的共情对话生成。


<details>
  <summary>Details</summary>
Motivation: 现有共情对话系统依赖缺乏持久用户身份的情境中心数据集，且使用不透明的粗粒度奖励信号，难以捕捉个性化情感细节并验证共情推理过程。

Method: 构建了一个包含178,080个问答对、22,080轮多轮对话和671个真实用户画像的KardiaBench数据集，并提出Kardia-R1框架，采用基于评分规则的共情强化学习（Rubric-ERL）方法，结合GRPO算法，实现可解释、分步骤的共情认知训练。

Result: 在四个主流大语言模型上进行的实验表明，Kardia-R1在情感准确性、共情水平、相关性、人设一致性和安全性方面均显著优于其他方法。

Conclusion: Kardia-R1通过引入可解释的评分规则和身份感知机制，有效提升了对话系统的深层共情能力，为构建个性化、可信的共情智能体提供了新方向。

Abstract: As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.

</details>


### [289] [Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding](https://arxiv.org/abs/2512.01316)
*Koki Natsumi,Hiroyuki Deguchi,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了一种改进的MBR解码方法AC-PMBR，利用知识蒸馏模型指导评分矩阵补全，在保持计算成本相近的同时，显著提升了翻译质量与矩阵补全精度。


<details>
  <summary>Details</summary>
Motivation: 传统MBR解码计算复杂度高（O(n²)），PMBR虽降低计算量但牺牲了翻译质量，亟需改善质量与成本之间的权衡。

Method: 提出agreement-constrained PMBR（AC-PMBR）解码方法，利用知识蒸馏模型提供先验信息，约束并引导矩阵补全过程，提升评分估计准确性。

Result: 在WMT'23 En↔De任务上，AC-PMBR将矩阵补全的近似误差降低至PMBR的1/3，并在相近计算开销下获得更高翻译质量。

Conclusion: AC-PMBR有效提升了PMBR的质量-效率权衡，验证了引入结构化先验（如知识蒸馏模型）对概率化MBR解码的重要性。

Abstract: Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\leftrightarrow$De translation tasks.

</details>


### [290] [MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis](https://arxiv.org/abs/2512.01369)
*Md. Rafiul Biswas,Firoj Alam,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: MARSAD是一个面向阿拉伯语社交媒体的多功能NLP平台，支持实时内容监测与分析，涵盖情感、情绪、宣传、虚假信息和仇恨言论检测，并提供可视化报告和安全的数据抓取功能。


<details>
  <summary>Details</summary>
Motivation: 为了满足对阿拉伯语社交媒体内容进行高效、实时且多维度分析的需求，特别是针对研究人员和非技术用户缺乏易用工具的问题。

Method: 采用集成API密钥的安全数据爬取技术，结合灵活的文档存储与结构化数据管理的后端架构，前端提供用户友好的交互界面以实现数据上传与可视化分析。

Result: 实现了对大规模多模态社交媒体数据的高效处理，支持多种NLP任务并生成多维度分析报告，提升了阿拉伯语社交媒体内容的理解与监控能力。

Conclusion: MARSAD为阿拉伯语社交媒体分析提供了一个强大且易于使用的平台，填补了该领域在多功能实时分析工具方面的空白。

Abstract: MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.

</details>


### [291] [DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis](https://arxiv.org/abs/2512.01410)
*Ruohan Zhou,Jiachen Yuan,Churui Yang,Wenzheng Huang,Guoyan Zhang,Shiyao Wei,Jiazhen Hu,Ning Xin,Md Maruf Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种动态融合学习模型DyFuLM，用于多模态情感计算，通过分层动态融合和门控特征聚合模块有效提升细粒度和粗粒度情感分类性能。


<details>
  <summary>Details</summary>
Motivation: 为了更好地捕捉复杂文本中的层次语义表示和细粒度情感特征，解决现有方法在多模态情感分析中特征融合不充分的问题。

Method: 提出DyFuLM模型，包含分层动态融合模块和门控特征聚合模块，以实现跨层次信息的自适应融合与平衡表示学习。

Result: 在多任务情感数据集上，DyFuLM达到82.64%粗粒度和68.48%细粒度准确率，回归误差最低（MAE=0.0674，MSE=0.0082），R²达0.6903；消融实验表明各模块对性能有显著贡献。

Conclusion: DyFuLM通过有效的分层特征融合机制，显著提升了情感表示能力和整体性能，验证了其模块设计的有效性。

Abstract: Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.

</details>


### [292] [PromptBridge: Cross-Model Prompt Transfer for Large Language Models](https://arxiv.org/abs/2512.01420)
*Yaxuan Wang,Quan Liu,Zhenting Wang,Zichao Li,Wei Wei,Yang Liu,Yujia Bao*

Main category: cs.CL

TL;DR: 本文提出PromptBridge框架，解决大语言模型（LLM）快速演进导致的‘模型漂移’问题，即同一提示词在不同模型上性能显著下降；该框架无需训练，仅需少量对齐任务校准，即可实现跨模型提示词有效迁移。


<details>
  <summary>Details</summary>
Motivation: LLM模型生态快速演进（如GPT、Claude、Llama），实际系统常需频繁切换模型，但提示词高度模型敏感，直接复用导致性能严重下降，亟需一种轻量、通用的跨模型提示迁移方法。

Method: PromptBridge采用无训练范式：首先通过Model-Adaptive Reflective Prompt Evolution（MAP-RPE）在校准任务上为源/目标模型分别生成最优提示；再基于这些提示对学习跨模型映射函数；测试时直接将源模型提示映射为目标模型优化提示。

Result: 在单智能体与多智能体场景下实验表明，PromptBridge显著提升下游任务准确率，并大幅降低模型迁移所需的人力与计算成本。

Conclusion: PromptBridge提供了一种高效、可泛化的解决方案，缓解模型漂移问题，支持LLM应用在动态模型环境中稳健、低成本地持续演进。

Abstract: Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.

</details>


### [293] [Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech](https://arxiv.org/abs/2512.01439)
*Bharatdeep Hazarika,Arya Suneesh,Prasanna Devadiga,Pawan Kumar Rajpoot,Anshuman B Suresh,Ahmed Ifthaquar Hussain*

Main category: cs.CL

TL;DR: 本文提出了一种支持代码混合语言（如印地语-英语混合的Hinglish）的多语言对话AI系统，用于印度金融援助场景，通过多智能体架构提升用户参与度并保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性高但英语普及率低（仅10%），导致数字金融服务的可及性受限，亟需解决语言障碍以促进金融包容性。

Method: 构建基于多智能体架构的多语言对话AI系统，包含语言分类、功能管理与多语言响应生成模块，并对多种语言模型进行对比分析及真实场景部署验证。

Result: 在真实部署中显著提升了用户参与度，同时维持较低的延迟开销（4–8%）。

Conclusion: 该系统有效弥合了新兴市场数字金融服务中的语言鸿沟，为多语言、代码混合环境下的金融科技应用提供了可行方案。

Abstract: India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\%). This work contributes to bridging the language gap in digital financial services for emerging markets.

</details>


### [294] [MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification](https://arxiv.org/abs/2512.01443)
*Xabier de Zuazo,Ibon Saratxaga,Eva Navas*

Main category: cs.CL

TL;DR: 本文提出基于Conformer的解码器用于LibriBrain 2025 PNPL竞赛中的MEG语音检测和音素分类任务，通过适配原始MEG信号并引入任务特定策略，在两个任务上均取得优于基线的结果并进入排行榜前十。


<details>
  <summary>Details</summary>
Motivation: 针对脑磁图（MEG）信号处理中的语音检测与音素分类任务，探索适用于高维、低信噪比MEG数据的高效深度学习架构，并推动神经解码技术的发展。

Method: 采用紧凑型Conformer模型处理306通道原始MEG信号，使用轻量卷积投影层和任务专用头；语音检测任务中引入面向MEG的SpecAugment增强策略，音素分类任务中采用逆平方根类别加权和动态分组加载器处理平均样本，并通过实例级归一化缓解分布偏移。

Result: 在官方标准分割集上，最佳系统在语音检测任务达到88.9%的F1-macro，在音素分类任务达到65.8%，均超过竞赛基线，排名双任务前十。

Conclusion: Conformer结构结合MEG特异性优化策略可有效提升神经信号解码性能，验证了其在复杂脑信号建模中的潜力。

Abstract: We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.

</details>


### [295] [Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages](https://arxiv.org/abs/2512.01460)
*Jozef Kubík,Marek Šuppa,Martin Takáč*

Main category: cs.CL

TL;DR: 提出了一种结合主动学习（AL）、聚类和动态数据选择调度器的集成微调管道，用于低资源语言，在少量训练数据下提升了模型性能并减少了标注成本。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的数据有限，导致语言模型效果较差，而预训练计算成本高，因此更需在微调阶段进行优化。

Method: 将主动学习与结构化数据选择策略（即AL调度器）结合，并引入数据聚类，构建集成微调流程。

Result: 在斯洛伐克语、马耳他语、冰岛语和土耳其语上的实验表明，该方法可减少高达30%的标注量，F1分数提升最多达4个百分点，并提高微调稳定性。

Conclusion: 结合聚类与AL调度的主动学习方法能有效提升低资源语言模型的微调效率与性能。

Abstract: Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.

</details>


### [296] [MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages](https://arxiv.org/abs/2512.01512)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Bo Yang,Keqi Deng,Xie Chen,Yang Xiang,Ming Liu,Bin Qin,YaoWei Wang*

Main category: cs.CL

TL;DR: 本文提出MCAT框架，通过语言扩展方法和优化的语音适配器模块，解决了多模态大语言模型在语音到文本翻译任务中语言覆盖有限和推理效率低的问题，支持70种语言互译，并显著提升批处理推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有语音到文本翻译研究受限于语言覆盖范围窄（主要为英语）和长语音序列导致的推理速度下降。

Method: 提出MCAT框架，包含两个创新：1）基于课程学习与数据平衡的语言扩展方法，支持70种语言互译；2）设计优化的语音适配器模块，将语音序列压缩至30个token。

Result: 在FLEURS数据集上，MCAT在70×69个翻译方向上超越当前最优端到端模型，仅用每语言10小时数据和约1亿可训练参数即实现高性能，并提升批处理推理效率。

Conclusion: MCAT有效提升了多模态大语言模型在多语言语音翻译任务中的泛化能力与效率，且已开源以推动该领域发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.

</details>


### [297] [Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces](https://arxiv.org/abs/2512.01557)
*Edward Ajayi,Eudoxie Umwari,Mawuli Deku,Prosper Singadi,Jules Udahemuka,Bekalu Tadele,Chukuemeka Edeh*

Main category: cs.CL

TL;DR: 本研究探讨了非洲语言（约鲁巴语、基尼亚卢旺达语和阿姆哈拉语）在数字环境中的表征问题，发现当前语言检测工具在处理真实、单语的非洲语言数据时表现不佳；研究对比了Reddit与本地新闻来源的数据质量，指出新闻数据更适合作为训练高质量语言模型的资源，并呼吁开发能同时处理纯净文本与语码转换文本的新模型。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在线使用稀疏、受英语影响严重，缺乏真实单语对话数据，导致现有语言检测工具性能受限。

Method: 收集Yoruba、Kinyarwanda和Amharic在Reddit子版块及本地新闻网站的数据，对比分析其语言纯度与用户参与度；评估AfroLID与通用大语言模型在两类数据上的语言检测准确率。

Result: 新闻数据干净、单语、用户互动多，模型在其上检测准确率接近100%；Reddit数据极少且高度语码转换，模型表现差；新闻内容比社交平台数据更适合作为训练语料。

Conclusion: 专业策划的本地新闻内容是训练面向非洲语言的上下文丰富AI模型的更可靠数据源；未来模型需兼顾纯净文本与语码转换文本以提升检测准确性。

Abstract: This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.

</details>


### [298] [MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark](https://arxiv.org/abs/2512.01603)
*Yuezhang Peng,Chonghao Cai,Ziang Liu,Shuai Fan,Sheng Jiang,Hua Xu,Yuxin Liu,Qiguang Chen,Kele Xu,Yao Li,Sheng Wang,Libo Qin,Xie Chen*

Main category: cs.CL

TL;DR: 本文提出了一个新的多意图车载舱内语音理解数据集MAC-SLU，并基于该数据集对主流开源大语言模型（LLMs）和大音频语言模型（LALMs）进行了全面基准测试，发现微调方法仍显著优于上下文学习，而端到端LALMs性能接近流水线方法且避免了ASR错误传播。


<details>
  <summary>Details</summary>
Motivation: 现有SLU数据集缺乏多样性与复杂性，且缺少面向最新大语言模型（LLMs）和大音频语言模型（LALMs）的统一基准。

Method: 构建了真实、复杂的多意图车载舱内SLU数据集MAC-SLU；在该数据集上系统评测了多种LLM/LALM方法，包括上下文学习、监督微调（SFT）、端到端（E2E）及流水线范式。

Result: 实验表明：1）LLMs/LALMs通过上下文学习可完成SLU任务但性能远低于SFT；2）端到端LALMs性能媲美流水线方法，且规避了语音识别（ASR）的错误传播。

Conclusion: MAC-SLU填补了复杂多意图SLU数据集与LLM/LALM统一基准的空白；端到端LALMs是更具潜力的SLU解决方案，但监督微调仍是当前最有效范式。

Abstract: Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{https://github.com/Gatsby-web/MAC\_SLU} and datasets\footnote{huggingface.co/datasets/Gatsby1984/MAC\_SLU} are released publicly.

</details>


### [299] [Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems](https://arxiv.org/abs/2512.01661)
*Dengyun Peng,Qiguang Chen,Bofei Liu,Jiannan Guan,Libo Qin,Zheng Yan,Jinhao Liu,Jianshu Zhang,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了UnsolvableQA数据集和UnsolvableRL强化学习框架，旨在提升大语言模型识别问题不可解性的能力，防止幻觉与过度自信，并揭示了‘能力崩溃’现象。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以区分客观不可解（问题本身存在内在矛盾）与主观能力不足（超出模型能力范围），导致幻觉和过度自信。

Method: 构建了UnsolvableQA数据集（含可解与不可解样本对，采用程序化生成与‘逆向构造’法注入矛盾）；提出UnsolvableRL强化学习框架，含准确性、不可解性与难度三重奖励机制。

Result: 方法在不可解性检测上接近完美，并提升了可解任务的准确率；发现并命名了‘能力崩溃’现象，证明显式引入不可解数据对防止系统性过度自信至关重要。

Conclusion: 显式建模和训练模型识别不可解问题是提升LLM可靠性与审慎性的关键路径，UnsolvableQA与UnsolvableRL为此提供了有效方案与实证基础。

Abstract: Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

</details>


### [300] [Self-Supervised Borrowing Detection on Multilingual Wordlists](https://arxiv.org/abs/2512.01713)
*Tim Wientzek*

Main category: cs.CL

TL;DR: 本文提出了一种完全自监督的多语种词表借词检测方法，结合PMI相似性和轻量级对比学习，并自动选择决策阈值，性能媲美甚至优于有监督基线。


<details>
  <summary>Details</summary>
Motivation: 现有借词检测方法依赖有监督标注或简单字符串相似性度量（如NED、SCA），泛化性与可扩展性受限，亟需无需人工标注的自监督解决方案。

Method: 融合基于全局对应模型的PMI相似性与基于语音特征向量的轻量级对比学习组件，并设计无标签自动阈值选择机制。

Result: 在基准数据集上，仅用PMI已超越NED和SCA；联合相似性指标性能媲美或优于有监督基线；消融实验验证了字符编码、温度参数和增强策略的关键作用。

Conclusion: 该方法可扩展、免人工监督，且配套提供命令行工具，为语言学研究者提供了实用、高效的借词检测新范式。

Abstract: This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.

</details>


### [301] [Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks](https://arxiv.org/abs/2512.01725)
*Jiannan Guan,Qiguang Chen,Libo Qin,Dengyun Peng,Jinhao Liu,Liangyu Huo,Jian Xie,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在多解任务中表现不佳的问题，提出‘推理过度自信’概念，并通过MuSoBench基准测试验证短链式思维提示存在明显过度自信，而长链式思维提示可通过迭代探索与自我反思缓解该问题；进一步提出‘认知僵化假说’解释其成因，并用注意力熵分析提供初步支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要单一正确答案的推理任务中表现出色，但在需生成全面且多样答案的多解任务中表现较差，作者认为这是由于‘推理过度自信’所致。

Method: 构建多解问题基准MuSoBench，对比短链式思维（Short-CoT）与长链式思维（Long-CoT）提示方法的表现，进行行为观察与影响因素分析，并提出‘认知僵化假说’，辅以注意力熵分析验证。

Result: Short-CoT表现出显著的推理过度自信，Long-CoT能有效缓解；注意力熵分析为认知僵化假说提供初步支持；识别出推理过程过早收敛于狭窄思维路径是关键成因。

Conclusion: LLM推理评估不应仅关注单答案准确率，而应转向对推理完整性与探索广度的综合评估；Long-CoT等支持迭代反思的机制有助于提升多解任务表现。

Abstract: Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.

</details>


### [302] [Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference](https://arxiv.org/abs/2512.01728)
*Zhengjia Wang,Danding Wang,Qiang Sheng,Jiaying Wu,Juan Cao*

Main category: cs.CL

TL;DR: 本文提出OmiGraph框架，首次关注遗漏型误导（omission-based deception）在虚假信息检测中的作用，通过构建遗漏感知图、建模遗漏关系及设计遗漏感知消息传递机制，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于显式捏造内容的虚假信息，而忽视了通过隐性遗漏关键信息来诱导错误判断的遗漏型欺骗，后者更具隐蔽性和危害性。

Method: 提出OmiGraph：1）构建基于上下文环境的遗漏感知图以揭示潜在遗漏内容；2）设计遗漏导向的关系建模，捕获上下文依赖与动态遗漏意图；3）引入遗漏感知的消息传递与聚合机制，整合遗漏内容与关系实现整体欺骗感知。

Result: 在两个大规模基准数据集上，F1和ACC分别平均提升+5.4%和+5.3%。

Conclusion: 遗漏视角对虚假信息检测至关重要，OmiGraph为该新方向提供了首个系统性解决方案，并验证了其有效性。

Abstract: This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.

</details>


### [303] [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/abs/2512.01848)
*Jinghan Jia,Nathalie Baracaldo,Sijia Liu*

Main category: cs.CL

TL;DR: 本文探讨了大型推理模型（LRMs）在引入显式思维链（CoT）推理后带来的新型安全风险，并指出监督微调（SFT）在安全对齐中的局限性；进而提出采用强化学习（RL）作为补充优化框架，实验证明RL能更稳定、一致地提升安全性，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽通过显式链式推理提升了数学与逻辑问题求解能力，但其推理路径中易出现中间阶段的不安全行为，而现有基于监督微调（SFT）的安全对齐方法存在效果不稳定、损害推理能力、泛化性差等问题，亟需更鲁棒的安全训练范式。

Method: 提出使用强化学习（RL）替代或补充监督微调（SFT），通过奖励信号直接优化模型策略；在多个模型家族和基准上开展实验，并结合反思动态分析与词元级熵分析，探究RL如何调控推理过程。

Result: RL方法在多个模型和基准上实现了更强、更一致的安全性提升，同时未损害推理能力；分析表明RL能抑制不安全的探索性推理，同时保留深层反思能力。

Conclusion: 强化学习是一种比监督微调更有效、更稳健的大型推理模型安全对齐方法，为构建安全可靠的LRMs提供了新范式。

Abstract: Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.

</details>


### [304] [BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages](https://arxiv.org/abs/2512.01852)
*Hrishikesh Terdalkar,Kirtan Bhojani,Aryan Dongare,Omm Aditya Behera*

Main category: cs.CL

TL;DR: 本文提出了BHRAM-IL，一个用于多印度语言中幻觉识别与评估的基准，涵盖印地语、古吉拉特语、马拉地语、奥里亚语及英语，包含36,047个问题，并对14种多语言大模型进行了评估，结果显示现有模型在跨语言幻觉方面仍存在严重问题，提供了公开数据集和代码以推动多语言幻觉检测研究。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多语言应用中广泛部署，但其在资源匮乏的印度语言中的幻觉问题尚未被充分研究，缺乏专门的评估基准。

Method: 构建了一个名为BHRAM-IL的多语言幻觉评估基准，覆盖九类任务（事实、数值、推理、语言等），包含36,047个标注问题；在10,265个问题的子集上评估14种多语言大模型，采用类别特定的归一化指标（0-1范围）分析跨语言和跨模型的幻觉表现。

Result: 在所有类别和模型上的聚合主得分为0.23，语言校正后的模糊评分为0.385，表明模型在印度语言中普遍存在严重的幻觉问题，且不同语言和任务间表现差异显著。

Conclusion: BHRAM-IL为印度语言中的幻觉检测提供了有效的评估工具，揭示了当前多语言大模型在低资源语言中的局限性，推动未来在多语言幻觉检测与缓解方向的研究。

Abstract: Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.

</details>


### [305] [Cross-Lingual Interleaving for Speech Language Models](https://arxiv.org/abs/2512.01865)
*Adel Moumen,Guangzhi Sun,Philip C. Woodland*

Main category: cs.CL

TL;DR: 本文提出了一种无需文本监督的跨语言语音单元混合方法，并发布了EN-FR语音训练数据集TinyStories及配套评估基准StoryCloze和TopicCloze，显著提升了多语言语音语言模型的语义准确性和跨语言续写与表征对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有口语语言模型（SLMs）研究以英语为主，缺乏跨语言评估基准和训练数据，阻碍了低资源语言的NLP应用。

Method: 提出跨语言语音token交错混合方法，不依赖文本监督；构建EN-FR语音数据集TinyStories（约42k小时）及合成生成的EN-FR StoryCloze/TopicCloze评估基准。

Result: 在360M和1B参数SLMs上，交错方法提升了单语语义准确率、跨语言续写鲁棒性及隐藏状态跨语言对齐强度。

Conclusion: 跨语言交错是一种简单、可扩展的构建真正多语言口语语言模型的有效途径，所有资源将开源。

Abstract: Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.

</details>


### [306] [Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models](https://arxiv.org/abs/2512.01892)
*Heloisa Candello,Muneeza Azmat,Uma Sushmitha Gunturi,Raya Horesh,Rogerio Abreu de Paula,Heloisa Pimentel,Marcelo Carpinette Grave,Aminat Adebiyi,Tiago Machado,Maysa Malfiza Garcia de Macedo*

Main category: cs.CL

TL;DR: 本研究通过混合方法实验评估了生成式AI缓解策略在多个维度（忠实性、公平性、有害内容移除能力、相关性）上的效果，发现参与者的母语、AI工作经验和标注熟悉度显著影响其评价，并提出了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的迅速普及，探究人类对生成响应的感知变得至关重要，尤其是其‘幻觉’和生成有害内容的能力；尽管已实施多种防护措施，但人类对这些缓解策略的感知仍知之甚少。

Method: 采用被试内实验设计，57名参与者在两种条件下评估响应：有害响应+其缓解版本，以及仅缓解后的响应；从忠实性、公平性、有害内容移除能力和相关性四个维度进行多维混合方法评估。

Result: 参与者对语言和上下文特征高度敏感，轻微语法错误即受惩罚，而语义上下文保留则获奖励；母语、AI工作经验和标注熟悉度显著影响评估结果；提出了用于训练和评估缓解策略的新指标。

Conclusion: 人类评估与当前LLM定量评估中对语言的处理方式存在明显差异，需在人-AI协同评估研究中纳入更细粒度的语言与认知因素，并发展更符合人类感知的评估框架。

Abstract: With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.

</details>


### [307] [OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation](https://arxiv.org/abs/2512.01896)
*Jinzheng Yu,Yang Xu,Haozhen Li,Junqi Li,Yifan Feng,Ligu Zhu,Hao Shen,Lei Shi*

Main category: cs.CL

TL;DR: 本文定义了在线舆情报告自动生成（OPOR-GEN）任务，构建了事件中心的数据集OPOR-BENCH，并提出基于智能体的评估框架OPOR-EVAL，显著提升了与人工评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽使自动化舆情报告生成成为可能，但该领域缺乏正式的任务定义和基准数据集，亟需系统性研究支撑。

Method: 提出OPOR-GEN任务定义；构建覆盖463个危机事件的OPOR-BENCH数据集（含新闻、社交媒体及参考摘要）；设计OPOR-EVAL——一种模拟人类专家、上下文感知的智能体评估框架。

Result: 前沿大模型在OPOR-BENCH上的实验表明，OPOR-EVAL与人工评估高度相关；成功建立首个面向舆情报告生成的完整任务体系、数据集与评估框架。

Conclusion: 本工作为自动化舆情报告生成这一关键应用方向奠定了坚实基础，推动该领域走向标准化与可复现研究。

Abstract: Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.

</details>


### [308] [Latent Debate: A Surrogate Framework for Interpreting LLM Thinking](https://arxiv.org/abs/2512.01909)
*Lihu Chen,Xiang Yin,Francesca Toni*

Main category: cs.CL

TL;DR: 本文提出了一种名为'潜在辩论（latent debate）'的新框架，用于通过隐式内部论证来解释大语言模型（LLM）的预测，揭示其内部推理过程与幻觉成因。该方法无需多模型或多答案显式辩论，而是捕捉单个模型单次推理中隐藏的支持与攻击信号，并在真假判断任务上验证了其可解释性与幻觉检测能力。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLMs）的内部思维过程及幻觉（hallucination）成因仍是一个关键挑战。现有自一致性与多智能体辩论等方法依赖显式多答案或多模型交互，无法刻画单模型内部隐含的推理冲突。

Method: 提出‘潜在辩论’这一模型与任务无关的概念框架，并在True/False预测任务上以符号化方式实例化，用以近似LLM的内部思考过程；通过分析各层隐状态中的支持与攻击信号建模内部辩论动态。

Result: 实证表明，潜在辩论是忠实且结构化的代理模型，其预测与原LLM高度一致；它能有效检测幻觉，且中间层高程度的潜在辩论与更高幻觉风险显著相关。

Conclusion: 潜在辩论为理解LLM内部机制（尤其是推理过程中隐含的（不）一致性）提供了新视角，兼具可解释性与实用幻觉检测能力，是一种有前景的基础性分析框架。

Abstract: Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.

</details>


### [309] [Rectifying LLM Thought from Lens of Optimization](https://arxiv.org/abs/2512.01925)
*Junnan Liu,Hongwei Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出RePro方法，通过优化视角将思维链（CoT）视为梯度下降过程，利用双评分机制构建过程级奖励，结合强化学习提升大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 长思维链推理虽增强大模型表现，但存在过度思考和推理链过长等次优行为，影响性能。需从优化角度理解并改进推理过程。

Method: 将CoT类比为梯度下降，每步推理视为一次参数更新；设计代理目标函数，采用双评分机制（强度与稳定性）评估推理过程，并构建复合的过程级奖励，融入RLVR框架进行后训练优化。

Result: 在数学、科学和编程等多个基准上，RePro在多种强化学习算法和不同大模型中均显著提升推理性能，有效缓解过度思考等问题。

Conclusion: RePro通过引入过程级奖励机制，成功优化了LLM的推理路径，在多个领域提升了推理效率与质量，为改进长思维链推理提供了新思路。

Abstract: Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.

</details>


### [310] [How Far Are We from Genuinely Useful Deep Research Agents?](https://arxiv.org/abs/2512.01948)
*Dingling Zhang,He Zhu,Jincheng Ren,Kangqi Song,Xinran Zhou,Boyu Feng,Shudong Liu,Jiabin Luo,Weihao Xie,Zhaohui Wang,Tianrui Qin,King Zhu,Yuqing Wang,Qianben Chen,Yuchen Eleanor Jiang,Wei Wang,Jiaheng Liu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了FINDER基准和DEFT失败分类法，以解决现有深度研究代理（DRA）在报告生成任务中缺乏合适评估标准和系统性失败分析的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DRA多在问答类基准上验证，而面向综合报告生成的研究被忽视；当前报告合成基准存在任务复杂、评估主观等问题，难以反映真实用户需求。

Method: 构建了包含100个研究任务、419项结构化检查点的细粒度基准FINDER；基于约1000份主流DRA生成报告，结合扎根理论与人-大模型协同标注，提出首个DRA失败分类体系DEFT（含14种细粒度失败模式）。

Result: 实验发现当前DRA的主要瓶颈不在任务理解，而在证据整合、事实验证和抗推理干扰的规划能力。

Conclusion: FINDER和DEFT为DRA评估与改进提供了标准化工具与诊断框架，推动DRA从问答走向真正可用的分析师级报告生成。

Abstract: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.

</details>


### [311] [The Art of Scaling Test-Time Compute for Large Language Models](https://arxiv.org/abs/2512.02008)
*Aradhye Agarwal,Ayan Sengupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文首次大规模研究了测试时扩展（TTS）策略在不同开源大语言模型和推理数据集上的表现，揭示了TTS性能受模型类型、问题难度和计算预算影响的规律，并提出了实用的TTS策略选择指南。


<details>
  <summary>Details</summary>
Motivation: 缺乏对主流TTS策略在统一条件下的系统性比较，且模型类型与问题难度对TTS性能的影响尚不明确。

Method: 在八个开源LLM（7B至235B参数）上，于四个推理数据集上生成超三百亿token，系统评估多种TTS策略。

Result: 发现三个一致趋势：(1) 无单一TTS策略全局最优；(2) 推理模型按轨迹质量分为短视野与长视野两类；(3) 给定模型类型下，TTS性能随计算预算单调提升。

Conclusion: 提出兼顾问题难度、模型类型与计算预算的TTS策略选择实用指南，推动高效推理时扩展。

Abstract: Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.

</details>


### [312] [Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling](https://arxiv.org/abs/2512.02010)
*Jack Cook,Junxian Guo,Guangxuan Xiao,Yujun Lin,Song Han*

Main category: cs.CL

TL;DR: 本文提出了一种改进的NVFP4量化算法Four Over Six (4/6)，通过为每个数值块评估两个潜在缩放因子，缓解了NVFP4在大语言模型训练和推理中因量化误差（尤其在近最大值处）导致的训练发散与性能下降问题；该方法在NVIDIA Blackwell GPU上高效可实现，在预训练和后训练量化中均显著提升模型稳定性与下游精度。


<details>
  <summary>Details</summary>
Motivation: NVFP4等低精度格式虽能提升速度与节省内存，但其在训练和推理中因对权重、激活和梯度统一量化导致训练发散与推理性能下降，尤其是浮点格式在近最大值处的量化误差严重损害下游表现。

Method: 提出Four Over Six (4/6)算法，修改NVFP4量化流程：对每个数值块评估两个候选缩放因子（而非原NVFP4的多个），利用FP4误差分布特性，选择更优缩放使可表示值分布更均匀，从而更好表征近最大值；算法专为NVIDIA Blackwell GPU高效实现。

Result: 在Transformer与混合架构的LLM预训练中，4/6有效防止训练发散，训练损失显著逼近BF16水平；同时兼容多种后训练量化方法，普遍提升下游任务准确率。

Conclusion: 4/6是一种实用、高效且兼容性强的NVFP4改进方案，显著提升了低精度训练的稳定性与精度，为未来NVFP4在LLM训练与部署中的广泛应用提供了可行路径。

Abstract: As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [313] [Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions](https://arxiv.org/abs/2512.00097)
*Boyan Duan,Xiao Liang,Shuai Lu,Yaoxiang Wang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Mao Yang,Weizhu Chen,Yeyun Gong*

Main category: cs.AI

TL;DR: 本文提出了一种不依赖神经网络、仅基于CPU的高效欧氏几何定理证明方法HAGeo，通过启发式添加辅助构造，在IMO-30基准上达到金牌水平（28/30），超越AlphaGeometry；同时构建了更具挑战性的新基准HAGeo-409（409题）以更精准评估几何推理能力。


<details>
  <summary>Details</summary>
Motivation: 自动化几何定理证明，尤其是IMO级别问题，仍是AI领域的重要挑战；现有方法（如AlphaGeometry）依赖神经网络和GPU，效率与可解释性受限，亟需高效、轻量、可解释的纯符号方法。

Method: 提出HAGeo：一种基于启发式的辅助构造添加方法，不使用神经网络，完全在CPU上运行；核心是设计几何规则驱动的辅助点/线选择策略，结合符号推理引擎进行演绎证明。

Result: 在IMO-30基准上解决28/30题，达金牌水平，显著优于AlphaGeometry；在新构建的HAGeo-409基准（含难度标注）上验证了方法鲁棒性与泛化性。

Conclusion: 纯启发式、非学习型方法在高难度几何定理证明中具备强大竞争力；HAGeo证明了高效、可解释、低资源消耗的符号方法仍具前沿价值，并推动建立更科学的评测基准。

Abstract: Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.

</details>


### [314] [Chunking Strategies for Multimodal AI Systems](https://arxiv.org/abs/2512.00185)
*Shashanka B R,Mohith Charan R,Seema Banu F*

Main category: cs.AI

TL;DR: 本文综述了多模态分块策略的技术基础与设计空间，涵盖文本、图像、音频、视频及跨模态数据的分块方法，分析了各类方法的优缺点与挑战，并探讨了未来在自适应、学习驱动和任务特定分块方向的研究机会。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统的发展，不同模态数据的复杂性增加，亟需系统性的分块策略来提升处理效率、准确性和生成一致性。现有方法缺乏统一的分类与比较框架，限制了进一步创新。

Method: 提出一个全面的分类体系，对文本（如固定大小分块、递归分割）、图像（基于对象的分块）、音频（基于静音分割）、视频（场景检测）以及跨模态分块策略进行技术分析，并结合常用工具（如LangChain、Detectron2、PySceneDetect）评估其适用性与局限性。

Result: 梳理了各模态主流与新兴分块技术的方法论、优势与挑战，揭示了粒度与上下文之间的权衡问题以及多模态对齐困难；总结了当前开放问题，如异步信息密度和噪声对齐信号。

Conclusion: 建立统一的多模态分块框架对提升AI系统性能至关重要，未来应聚焦于自适应、学习驱动和任务定制化的分块策略，以应对现实应用中复杂的多模态交互需求。

Abstract: Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.

</details>


### [315] [A Rosetta Stone for AI Benchmarks](https://arxiv.org/abs/2512.00193)
*Anson Ho,Jean-Stanislas Denain,David Atanasov,Samuel Albanie,Rohin Shah*

Main category: cs.AI

TL;DR: 本文提出了一种统计框架，将不同AI基准测试统一到一个数值尺度上，以解决基准测试快速饱和的问题，从而支持跨时间和模型的能力比较与趋势分析。


<details>
  <summary>Details</summary>
Motivation: 大多数AI基准测试在推出后几年甚至几个月内就会饱和，难以研究AI能力的长期发展趋势。

Method: 构建了一个统计框架，将不同基准测试进行整合，使模型能力和基准难度处于同一数值尺度上，不依赖于对能力随时间或计算资源变化的假设。

Result: 该框架可用于测量AI进步速度、预测未来能力、估计算法效率提升率，并检测AI进展中的快速加速现象。

Conclusion: 该框架作为一种‘罗塞塔石碑’，有效支持跨基准、跨时间的AI能力比较与趋势分析，且无需对能力演化做先验假设。

Abstract: Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a "Rosetta Stone", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.

</details>


### [316] [OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction](https://arxiv.org/abs/2512.01289)
*Mingqin Yu,Fethi Rabhi,Boming Xia,Zhengyi Yang,Felix Tan,Qinghua Lu*

Main category: cs.AI

TL;DR: OntoMetric 是一个本体引导的框架，用于将ESG监管文档转化为经过验证、AI和网络就绪的知识图谱，通过结构感知分割、本体约束的LLM提取和两阶段验证，显著提升语义准确性和模式合规性。


<details>
  <summary>Details</summary>
Motivation: 现有ESG披露框架（如SASB、TCFD、IFRS S2）要求企业计算并报告大量指标，但其规范嵌于冗长非结构化PDF中，人工提取不可扩展，无约束大模型提取易产生不一致、幻觉、缺失溯源及高验证失败率。

Method: OntoMetric采用三阶段流程：(1) 基于目录边界的结构感知分段；(2) 将ESGMKG本体嵌入提示的本体约束LLM提取，并为实体添加语义字段；(3) 结合LLM语义验证与规则驱动模式校验（VR001–VR006）的两阶段验证，保留段级与页级溯源。

Result: 在5个ESG标准（共228页、60段）上评估，OntoMetric达到65–90%语义准确率和80–90%模式合规率，远高于基线方法的3–10%，单实体验证成本约0.01–0.02美元。

Conclusion: 符号本体约束与神经抽取结合可构建可靠、可审计的知识图谱，支持监管合规、可持续金融分析、透明度门户和自动化合规工具等下游应用。

Abstract: Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.

</details>


### [317] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott,Qiyao Wei,Rada Djoneva,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 本文研究了不同训练激励对AI系统推理链（CoT）可监控性的影响，发现对抗优化会降低监控性能，而直接优化监控性效果不稳定。


<details>
  <summary>Details</summary>
Motivation: AI系统通过自然语言输出推理过程（CoT），为安全监控提供了可能；但CoT是否真实反映内部推理过程，受训练方式影响，需系统评估其可监控性。

Method: 提出一种新方法来衡量监控性：判断监控器能否基于模型的推理预测关键潜在变量；在控制准确率前提下，对比分析长度惩罚、KL正则化、对抗优化和直接优化监控性等不同训练激励的效果。

Result: 常见激励（如长度惩罚、KL正则化）未表现出一致影响；对抗优化显著削弱监控性能；直接优化监控性未能稳定提升监控效果。

Conclusion: CoT的可监控性并非自动随模型能力提升而增强，特定训练目标（尤其是对抗优化）会损害监控有效性，需谨慎设计训练机制以保障可解释性与安全性。

Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [318] [Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking](https://arxiv.org/abs/2512.00267)
*Anab Maulana Barik,Shou Ziyi,Yang Kaiwen,Yang Qi,Shen Xin*

Main category: cs.AI

TL;DR: 本文提出了一种名为Trification的新型自动化事实核查框架，通过生成全面的验证动作集、构建可动态调整的依赖图来结构化逻辑关系，从而提升核查准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在验证声明的每个组成部分时存在不足，且缺乏将子任务结果逻辑连接以得出最终预测的结构化框架。

Method: 提出Trification框架：首先生成覆盖声明全部内容的验证动作集，然后将其组织成建模逻辑交互的依赖图，并支持动态修改该图以自适应调整验证策略。

Result: 在两个具有挑战性的基准数据集上实验表明，该框架显著提升了事实核查的准确性。

Conclusion: Trification为自动化事实核查提供了更完整、更结构化和更灵活的新范式，推动了该领域的前沿发展。

Abstract: Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.

</details>


### [319] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu,Yuchen Song,Yecheng Zhao,Ling Wu,Shan Wan*

Main category: cs.AI

TL;DR: 本文提出CogEvo-Edu，一种分层多智能体教育系统，将检索、记忆与控制视为耦合的认知演化过程，显著提升数字信号处理（DSP）领域的智能辅导效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM教育系统依赖静态RAG，难以在复杂STEM领域（如DSP）中维持长期学生模型、管理异构知识库并动态调整教学策略。

Method: 提出CogEvo-Edu系统，包含三层：认知感知层（CPL）构建自校正学生档案；知识演化层（KEL）赋予知识时空价值以驱动激活、压缩与遗忘；元控制层（MCL）通过内外双循环协同优化教学策略与参数；并构建专用基准DSP-EduBench进行评估。

Result: 在DSP-EduBench上，CogEvo-Edu整体评分从5.32提升至9.23，六项指标均优于静态RAG、简单记忆和单智能体基线。

Conclusion: 将检索、记忆与控制联合建模为认知演化过程，可显著增强LLM在复杂教育场景中的长期适应性与教学效能。

Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [320] [ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://arxiv.org/abs/2512.00305)
*Zhengzhuo Xu,SiNan Du,Yiyan Qi,SiwenLu,Chengjin Xu,Chun Yuan,Jian Guo*

Main category: cs.AI

TL;DR: 本文提出PointCoT方法，通过结合视觉定位与反思式链式思维推理，提升多模态大语言模型在图表理解中的准确性，减少数值幻觉，并构建大规模数据集ChartPoint-SFT-62k用于训练，显著提升了图表理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在图表理解中依赖OCR提取内容，当文本标注稀疏时易产生数值幻觉，且缺乏对图表元素和比例关系的视觉感知能力。

Method: 提出PointCoT方法，通过引导MLLM生成边界框并重新渲染图表，将文本推理步骤与视觉区域关联；构建自动化流程生成包含逐步推理、定位和可视化重绘的大规模数据集ChartPoint-SFT-62k，并训练指令微调模型ChartPointQ2及其改进版。

Result: 所提模型在多个图表基准上超越现有最先进方法，例如在ChartBench上提升5.04%。

Conclusion: 通过引入视觉定位与反思式交互机制，增强了MLLM在图表理解中的视觉基础能力，有效缓解了数值幻觉问题，验证了结合视觉感知与推理的重要性。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\% on ChartBench.

</details>


### [321] [Testing the Machine Consciousness Hypothesis](https://arxiv.org/abs/2512.01081)
*Stephen Fitz*

Main category: cs.AI

TL;DR: 本文提出机器意识假说，认为意识是计算系统通过通信实现的二阶感知功能属性，而非个体建模的副现象；研究基于元胞自动机构建分布式预测模型网络，探讨集体智能如何通过预测信息的噪声交换与对齐，自发形成共享的自我表征。


<details>
  <summary>Details</summary>
Motivation: 探索意识是否为可脱离物理基质的计算功能属性，并建立可实证检验的机器意识理论。

Method: 在具有计算不可约性的元胞自动机之上，构建能通信、适应的局部预测神经模型网络，研究其在自组织环境中通过预测同步产生集体自我模型的过程。

Result: 意识被建模为分布式系统中多主体间预测信息交换与对齐所涌现的共享语言和集体自我表征，而非单个模型的产物。

Conclusion: 意识源于通信而非建模本身；它是分布式学习系统在描述底层计算现实过程中，经由有损、噪声通信形成的协同自指表征，是一种集体智能的功能性涌现现象。

Abstract: The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.

</details>


### [322] [RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs](https://arxiv.org/abs/2512.00319)
*Ruike Hu,Shulei Wu*

Main category: cs.AI

TL;DR: 本文提出了一种轻量高效的强化学习框架，通过多维奖励函数和梯度正则化策略优化（GRPO），有效弥合大语言模型生成文本与结构化数据格式（如JSON）之间的'结构鸿沟'，在结构准确率和JSON有效性上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在生成结构化数据（如JSON、XML）时存在的'结构鸿沟'问题——即其概率性token生成机制与结构化格式的确定性要求之间的矛盾；传统监督微调（SFT）难以保证语法严格性，而约束解码又带来高推理延迟。

Method: 提出一种轻量级强化学习框架：设计分层多维奖励函数（涵盖结构完整性、格式正确性、内容准确性、有效性），并采用无需独立critic网络的梯度正则化策略优化（GRPO），降低显存占用。

Result: 在复杂菜谱生成和GSM8K-JSON等任务上，结构准确率达89.7%，JSON有效性达92.1%，显著超越零样本基线（如GPT-3.5）及SFT微调的LLaMA-3-8B；训练动态分析揭示模型遵循'先掌握语法、再提升语义'的自定步调课程学习路径。

Conclusion: 该RL框架高效、低资源消耗地提升了LLM生成结构化输出的可靠性与合规性，为LLM融入自动化软件系统提供了可行方案，并开源了Qwen-JSON模型。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the "Structure Gap" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to "hallucinated" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.

</details>


### [323] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang,Ruihan Sun,Ruixi Su,Shiqi Ma,Shiya Zhang,Xianna Weng,Xiaofan Zhang,Yuhan Zhan,Yuyang Xu,Zhaohan Chen,Zhengyuan Pan,Ziyi Song*

Main category: cs.AI

TL;DR: 本文提出首个能实时推断用户人格并优化个性化对话行为的强化学习框架Echo-N1，在主观性强、情感与人格敏感的对话任务上实现显著提升，并构建首个动态情绪智能评测套件。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型强化学习研究集中于数学、代码等确定性任务，忽视了体现人类智能核心的主观、情感化、人格敏感型对话；该领域因难以形式化而被普遍认为不适配传统RL范式。

Method: 提出一种新型RL框架，支持在线用户人格推断，并据此动态优化模型对话策略；同时构建首个面向动态情感智能的评估套件。

Result: Echo-N1模型在人类对话质量上相较基线大幅提升，超越专有模型Doubao 1.5 Character，验证了RL在高度主观对话场景中的有效性与鲁棒性。

Conclusion: 本工作开辟了强化学习新前沿——将RL成功应用于对话中深层主观、高度人性化维度的建模与优化。

Abstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [324] [Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models](https://arxiv.org/abs/2512.00349)
*Sitong Fang,Shiyi Hou,Kaile Wang,Boyuan Chen,Donghai Hong,Jiayi Zhou,Josef Dai,Yaodong Yang,Jiaming Ji*

Main category: cs.AI

TL;DR: 本文提出首个专门评估多模态欺骗行为的基准MM-DeceptionBench，并设计了‘图像辩论’多智能体监控框架以提升对多模态大模型欺骗行为的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究几乎全部局限于文本模态，而多模态大语言模型中的欺骗风险尚未被系统探索；随着模型能力增强，欺骗行为已从文本扩展至多模态，危害加剧且更难监测。

Method: 构建覆盖六类欺骗行为的多模态欺骗评测基准MM-DeceptionBench；提出‘图像辩论（debate with images）’多智能体辩论监控框架，强制模型基于视觉证据进行推理与回应，以增强欺骗识别能力。

Result: 所提方法显著提升欺骗行为检测效果，在GPT-4o上使Cohen's kappa提高1.5倍、准确率提高1.25倍，并在所有测试模型上均提升与人工判断的一致性。

Conclusion: 多模态欺骗是前沿AI发展中日益严峻的安全隐患；本文不仅首次系统揭示并量化该风险，还提供了可推广的评估基准与有效监测方法，为多模态AI安全研究奠定基础。

Abstract: Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.

</details>


### [325] [GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework](https://arxiv.org/abs/2512.00406)
*Pengyu Zeng,Yuqin Dai,Jun Yin,Jing Zhong,Ziyang Han,Chaoyang Shi,ZhanXiang Jin,Maowei Jiang,Yuxing Han,Shuai Lu*

Main category: cs.AI

TL;DR: 本文提出了GreenPlanner，一个兼顾能源效率与功能需求的生成式建筑设计框架，通过结合自动评估与生成，显著提升设计效率与合规性。


<details>
  <summary>Details</summary>
Motivation: 建筑设计影响人类福祉与碳排放，但当前布局生成依赖人工、成本高且不可扩展；现有方法常违反关键约束，缺乏自动化评估机制。

Method: 构建了带标签的设计可行性数据集（用于学习约束先验）、快速实用设计评估器（PDE，预测能耗与空间功能有效性）、PDE引导筛选生成的GreenPD数据集（匹配用户需求与规范合规布局），以及基于GreenPD并融合PDE反馈训练的GreenFlow生成模型。

Result: 实验表明GreenPlanner评估速度提升超10^5倍、准确率>99%，完全消除无效样本，并使设计效率比专业建筑师提高87%。

Conclusion: GreenPlanner实现了评估与生成的统一，支持可控、规范感知的建筑平面图生成，在效率、准确性与合规性上取得显著突破。

Abstract: Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.

</details>


### [326] [Mind the data gap: Missingness Still Shapes Large Language Model Prognoses](https://arxiv.org/abs/2512.00479)
*Yuta Kobayashi,Vincent Jeanselme,Shalmali Joshi*

Main category: cs.AI

TL;DR: 本研究探讨了在医疗数据中缺失值模式对大语言模型（LLM）零样本预测性能的影响，发现显式引入缺失指示符对不同规模的LLM影响不一：大模型可能受益，小模型则可能受损。研究强调需更透明地评估和处理信息性缺失对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管缺失值的信息性已被广泛研究，但其对大语言模型性能的影响尚不清楚，尤其是在医疗等依赖人类决策的数据收集场景中。

Method: 通过在哥伦比亚大学医学中心和MIMIC-IV数据集上进行一系列实验，分析显式添加缺失指示符在提示中的作用，评估其对不同LLM零样本预测性能和校准的影响，并结合聚合分析与理论洞察解释结果。

Result: 缺失值模式显著影响LLM的零样本预测性能；加入缺失指示符对部分模型有益，但损害其他模型的表现，影响因模型规模而异：大模型趋于受益，小模型可能表现更差。

Conclusion: 需要更系统、透明地评估和报告信息性缺失的表示方式对LLM下游任务性能的影响，以避免忽视其潜在偏差。

Abstract: Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.

</details>


### [327] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu,Hongjian Zhou,Bradley Max Segal,Jinge Wu,Zeyu Cao,Hantao Zhong,Lei Clifton,Fenglin Liu,David A. Clifton*

Main category: cs.AI

TL;DR: 本文提出CRPO方法，一种面向临床推理的多目标、可验证强化学习框架，用于对大语言模型进行后训练，以同时优化准确性、忠实性和全面性，无需人工标注，在临床推理任务上显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法（如GRPO）主要奖励正确性，难以满足医学等高风险领域对推理忠实性和全面性的多维要求。

Method: 提出临床目标相对策略优化（CRPO），融合基于规则和可验证的奖励信号，在无需人工标注的前提下联合优化准确性、忠实性和全面性。

Result: 在三个基准测试中，基于CRPO训练的Clinical-R1-3B模型在真实性与完整性方面显著优于标准GRPO，同时保持准确率提升。

Conclusion: CRPO为大语言模型在医疗领域的安全、协作式应用提供了可扩展的对齐路径，并展示了多目标、可验证强化学习在医学LLM后训练中的潜力。

Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [328] [EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients](https://arxiv.org/abs/2512.00670)
*He-Yen Hsieh,Hong Wang,H. T. Kung*

Main category: cs.AI

TL;DR: 提出EDIT方法，利用训练时的LoRA更新信息构建推理路径，在扩散式大语言模型中通过监测推理稳定性来自适应提前终止去噪过程，显著减少推理步数并保持性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型通常需要多步去噪生成 token，但答案常在早期已稳定，继续推理浪费计算资源。希望在保证准确性的同时减少不必要的推理步骤。

Method: 在SFT阶段收集AdamW优化过程中的LoRA更新作为推理映射，并在推理时监控token激活与该映射的对齐程度；使用KL散度判断未掩码token上对齐分布的收敛性，达到阈值即停止去噪。

Result: 在多个推理基准上减少11.8%到68.3%的扩散步数，多数情况下保持甚至提升准确率，仅增加约0.02%存储开销（1.5-2MB）。

Conclusion: EDIT有效利用训练过程中的梯度动态信息实现自适应早停，为降低dLLM推理成本提供了新方向。

Abstract: Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.

</details>


### [329] [Model of human cognition](https://arxiv.org/abs/2512.00683)
*Wu Yonggang*

Main category: cs.AI

TL;DR: 本文提出了一种神经理论框架，用于解释智能在系统中的涌现，兼具功能鲁棒性与生物合理性，并为可解释、可泛化的AI提供高效计算方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）缺乏可解释性、缺乏统一理论以及运行成本过高的问题。

Method: 构建一种神经理论框架，以建模智能的涌现机制，并兼顾功能鲁棒性与生物合理性。

Result: 该框架为认知过程（如决策与问题求解）提供了理论洞见，并支持开发更可解释、可泛化且计算高效的AI系统。

Conclusion: 所提出的框架有望弥合人工智能与神经科学之间的理论鸿沟，推动兼具可解释性、泛化性与效率的下一代AI发展。

Abstract: The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.

</details>


### [330] [When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF](https://arxiv.org/abs/2512.00709)
*Yifan Xu,Xichen Ye,Yifan Chen,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种针对偏好翻转问题的翻转感知直接偏好优化算法（FA-DPO），通过建模实例相关的翻转概率提升LLM对齐中数据噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于人类反馈中的偏好翻转普遍存在，导致数据标注被污染，影响大语言模型对齐效果，因此需要增强对齐算法在面对翻转对时的鲁棒性。

Method: 从RLHF视角出发，将人类意图模型与外部因素引起的偏好翻转机制分为两个阶段，在BT模型基础上引入实例依赖的翻转概率，并利用与偏好标注相关的特征来建模不确定性及翻转模式，设计了一种可与原始RLHF和DPO兼容的迭代优化算法。

Result: 实验验证了所提出的实例依赖偏好翻转模型在多种情况下的有效性，表明FA-DPO在不同场景下优于基线方法。

Conclusion: FA-DPO能够有效应对人类反馈中的偏好翻转问题，提升了对齐算法在噪声数据下的性能，具有良好的实用性和兼容性。

Abstract: Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.

</details>


### [331] [SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs](https://arxiv.org/abs/2512.00722)
*Jiaming Xu,Jiayi Pan,Hanzhen Wang,Yongkang Zhou,Jiancai Ye,Yu Wang,Guohao Dai*

Main category: cs.AI

TL;DR: 本文提出SpeContext，一种利用蒸馏语言模型（DLM）作为检索算法的新范式，通过算法、系统和编译三层协同设计，显著提升长上下文推理的吞吐量与速度，同时保持精度几乎不变。


<details>
  <summary>Details</summary>
Motivation: 检索算法的目标应与大语言模型（LLM）对齐，这与知识蒸馏目标相似；现有检索方法未充分利用LLM内部结构信息，且在资源受限场景下效率不足。

Method: 提出SpeContext：（1）算法层：基于DLM头级注意力权重设计轻量检索头，大幅减少参数；（2）系统层：采用弹性预加载策略实现KV缓存检索与LLM计算异步重叠；（3）编译层：构建理论内存模型并实现自适应内存管理以最大化GPU内存利用率。

Result: 在云和边缘两种资源受限环境中，相比Huggingface框架，SpeContext分别实现最高24.89倍吞吐提升和10.06倍加速，精度损失可忽略。

Conclusion: 利用DLM驱动检索是一种有效对齐LLM目标的新范式；SpeContext通过跨层协同优化，在准确率与吞吐量之间推动帕累托前沿，适用于实际部署场景。

Abstract: In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.

</details>


### [332] [Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens](https://arxiv.org/abs/2512.00729)
*Yuxiang Chen,Zuohan Wu,Ziwei Wang,Xiangning Yu,Xujia Li,Linyi Yang,Mengyue Yang,Jun Wang,Lei Chen*

Main category: cs.AI

TL;DR: 本文提出一个基于人类认知过程的大型推理模型（LRM）原子推理步骤分类体系，涵盖5大类17小类，并构建了27万+标注数据集；分析发现当前LRM的自我检查多流于表面，建议转向多步深度反思；同时提出自动标注框架CAPO，其一致性优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 受大型推理模型（LRMs）展现出类人推理行为的启发，旨在从人类心智过程出发，系统刻画其原子推理步骤，深入理解其‘心理机制’。

Method: 构建基于人类认知的五维十七类推理步骤分类体系；基于该体系标注277,534个原子推理步骤；对现有LRMs进行实证分析；提出基于LLM的自动标注框架CAPO。

Result: 揭示当前LRM后验‘双检’（self-monitoring）普遍浅层、极少引发实质性修正；CAPO在标注一致性上超越基线，更接近人类专家。

Conclusion: 应推动LRM从简单自我监控转向多步深度反思；所提出的分类体系、CAPO框架及实证洞察为可扩展、有原则地理解和提升LRM推理能力提供了新路径。

Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.

</details>


### [333] [MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents](https://arxiv.org/abs/2512.00756)
*Ruihan Chen,Qiming Li,Xiaocheng Feng,Xiaoliang Yang,Weihong Zhong,Yuxuan Gu,Zekun Zhou,Bing Qin*

Main category: cs.AI

TL;DR: 本文提出MPR-GUI-Bench多语言细粒度GUI基准测试集，揭示LVLMs在非英语GUI任务中性能显著下降，并提出GUI-XLI跨语言干预方法，在P&R相关层对隐状态进行干预，平均提升多语言P&R能力6.5%。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在多语言GUI场景下的感知与推理（P&R）能力研究不足，且缺乏对控件功能和空间关系等细粒度分析，限制了其全球应用。

Method: 构建多语言细粒度GUI基准MPR-GUI-Bench；提出GUI-XLI方法，在LVLMs中对P&R能力相关层的隐状态进行跨语言干预，以缩小英语与非英语输入在潜在空间中的表征差异。

Result: 实验表明LVLMs在非英语GUI任务中P&R性能明显弱于英语；GUI-XLI方法平均提升多语言P&R能力6.5%。

Conclusion: 多语言GUI理解存在显著语言偏差，通过针对性干预模型隐状态可有效缓解该问题，MPR-GUI-Bench为未来研究提供了细粒度评估基础。

Abstract: With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.

</details>


### [334] [BioPro: On Difference-Aware Gender Fairness for Vision-Language Models](https://arxiv.org/abs/2512.00807)
*Yujie Lin,Jiayao Ma,Qingguo Hu,Derek F. Wong,Jinsong Su*

Main category: cs.AI

TL;DR: 本文提出BioPro框架，通过偏差正交投影实现视觉语言模型中性别偏见的选择性去偏，既在中性场景下减少偏见，又在显式场景下保持性别真实性，并可推广至连续偏差变量。


<details>
  <summary>Details</summary>
Motivation: 现有公平性干预方法采用差异不感知视角，无法区分需中立处理与需保留群体特性的不同上下文，导致过度校正或校正不足。

Method: 提出无需训练的BioPro（Bias Orthogonal Projection）框架：基于反事实嵌入识别低维性别变化子空间，并通过正交投影选择性中和性别相关信息。

Result: 实验表明BioPro能有效降低中性场景下的性别偏见，同时保持显式场景中的性别保真度；还可泛化至场景亮度等连续偏差变量。

Conclusion: BioPro为视觉语言模型提供了差异感知、选择性公平的新范式，兼顾去偏与语义保真，具有跨偏差类型扩展潜力。

Abstract: Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.

</details>


### [335] [Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning](https://arxiv.org/abs/2512.00818)
*Haozhen Gong,Xiaozhong Ji,Yuansen Liu,Wenbin Wu,Xiaoxiao Yan,Jingjing Liu,Kai Wu,Jiazhen Pan,Bailiang Jian,Jiangning Zhang,Xiaobin Hu,Hongwei Bran Li*

Main category: cs.AI

TL;DR: 本文提出了Med-CMR——一个细粒度、高挑战性的医学复杂多模态推理基准，用于系统评估医学大语言模型（MLLMs）在视觉理解与多步临床推理上的能力，并揭示当前模型在长尾泛化等任务上仍存在显著短板。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态大模型（MLLMs）在临床中初现应用，但其复杂医学推理能力尚不明确，缺乏细粒度、临床真实、系统性的评估基准。

Method: 构建Med-CMR基准：1）系统性能力分解（视觉理解+多步推理）；2）设计高挑战性任务（涵盖小目标检测、细节判别、空间理解、时间预测、因果推理、长尾泛化、多源整合）；3）覆盖20,653组高质量VQA样本，横跨11个器官系统与12种影像模态，经双阶段（专家+模型辅助）验证。对18个SOTA MLLMs进行评测。

Result: GPT-5表现最佳（MCQ准确率57.81%，开放题得分48.70），但专用医学MLLM未稳定超越通用强模型；长尾泛化是主要失败模式。

Conclusion: Med-CMR为医学MLLM提供了严格的视觉-推理融合压力测试与罕见病例鲁棒性评估标准，可作为未来临床AI系统的可靠评测基准。

Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.

</details>


### [336] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu,Kezhi Wang,Luping Xiang,Kun Yang*

Main category: cs.AI

TL;DR: 本文提出了一种融合语义通信与智能体AI（Agentic AI）的轨迹预测框架，用于提升车路协同（V2X）环境下的预测精度与通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统V2X通信开销大、时延高；现有轨迹预测模型缺乏环境感知与逻辑推理能力。

Method: 在V2I中，RSU部署特征提取与语义分析双智能体，通过语义通信向车辆发送紧凑特征与语义洞察；在V2V中，车辆本地执行特征提取与语义分析，并融合邻居预测结果联合预测自身轨迹。

Result: 实验表明该方法在低信噪比（SNR）条件下预测精度最高提升47.5%，显著优于基线方案。

Conclusion: 语义通信与Agentic AI的协同可有效提升V2X中轨迹预测的准确性、鲁棒性与通信效率。

Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [337] [Assessing model error in counterfactual worlds](https://arxiv.org/abs/2512.00836)
*Emily Howerton,Justin Lessler*

Main category: cs.AI

TL;DR: 本文探讨了反事实情景建模中模型误差（特别是反事实世界中的误差）的估计方法，对比三种估计策略，强调模型误校准对决策价值评估的关键性，并提出可评估情景设计的必要条件。


<details>
  <summary>Details</summary>
Motivation: 反事实情景预测广泛用于规划与决策，但其事后评估罕见；区分情景偏差与模型误校准对评估模型决策价值至关重要，而后者需估计反事实世界中的模型误差。

Method: 提出并对比三种估计反事实模型误差的方法，并通过模拟实验分析其优劣。

Result: 明确了不同误差估计方法的适用场景与局限性，给出了反事实误差估计的实践建议，并指出使情景预测具备可评估性的设计要素。

Conclusion: 模型误校准比情景偏差更关键地影响模型在决策中的价值；准确估计反事实误差依赖于合理的情景设计与方法选择。

Abstract: Counterfactual scenario modeling exercises that ask "what would happen if?" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.

</details>


### [338] [ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI](https://arxiv.org/abs/2512.00839)
*Fabrizio Maturo,Donato Riccio,Andrea Mazzitelli,Giuseppe Bifulco,Francesco Paolone,Iulia Brezeanu*

Main category: cs.AI

TL;DR: ARCADIA是一个结合大语言模型推理与统计诊断的智能体AI框架，用于因果发现，能构建有效且时间一致的因果结构。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现算法在真实高风险场景中难以保证因果图的可靠性与可解释性，需要更稳定、可干预且可解释的因果建模方法。

Method: ARCADIA采用基于约束引导的提示工程和因果有效性反馈机制，迭代优化候选有向无环图（DAG），融合大语言模型推理与统计诊断。

Result: 在企业破产数据上的实验表明，ARCADIA生成的因果图比NOTEARS、GOLEM和DirectLiNGAM更可靠，并提供完全可解释、可干预的建模流程。

Conclusion: ARCADIA展示了智能体式大语言模型在自主科学建模与结构化因果推断中的潜力，推动了AI在高可信度因果分析领域的发展。

Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.

</details>


### [339] [One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces](https://arxiv.org/abs/2512.00852)
*Yandong Sun,Qiang Huang,Ziwei Xu,Yiqun Sun,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.AI

TL;DR: 本文提出了Semantic Field Subspace (SFS) 和 SAFARI 算法，用于在保持语义一致性的前提下揭示嵌入空间中的层次化语义结构，具备高效、可扩展和跨模态的优点。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入空间分析方法往往在语义连贯性和结构规则性之间权衡，或因提高可解释性而带来高计算开销，因此需要一种既能保持语义结构又高效可扩展的方法。

Method: 提出语义场子空间（SFS）来捕捉局部语义邻域，并设计SAFARI算法结合新的‘语义偏移’度量来无监督地发现层次语义结构；同时开发了一种高效的语义偏移近似方法以避免昂贵的SVD计算。

Result: 在六个真实文本和图像数据集上的实验表明，SFS在分类和政治偏见检测等任务中优于标准分类器，SAFARI能一致地揭示可解释且可泛化的语义层次，近似方法实现15~30倍加速且平均误差低于0.01。

Conclusion: SFS与SAFARI为嵌入空间中的语义结构建模提供了统一框架，兼顾几何保持、上下文感知、高效性和可解释性，推动了语义理解的结构化与规模化发展。

Abstract: Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.

</details>


### [340] [Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing](https://arxiv.org/abs/2512.00881)
*Li Yuan,Qingfei Huang,Bingshan Zhu,Yi Cai,Qingbao Huang,Changmeng Zheng,Zikun Deng,Tao Wang*

Main category: cs.AI

TL;DR: 本文提出了MMQAKE，首个用于多模态多跳问答的知识编辑基准，强调推理过程和视觉重述鲁棒性，并提出Hybrid-DMKG框架以动态多模态知识图为基础，通过混合推理实现更准确的多跳推理。


<details>
  <summary>Details</summary>
Motivation: 现有MKE基准仅关注最终答案正确性，忽略中间推理质量和对视觉重述输入的鲁棒性，亟需更全面的评估机制。

Method: 提出MMQAKE基准，评估模型在多跳文本-图像推理链中各步骤的表现及对视觉重述的鲁棒性；设计Hybrid-DMKG框架，结合大语言模型分解问题、多模态检索定位更新事实，并通过关系链接与RAG双路径推理融合决策。

Result: 实验表明现有MKE方法在更新后的多模态推理链上表现不佳；Hybrid-DMKG在MMQAKE上显著优于现有方法，提升准确率与鲁棒性。

Conclusion: Hybrid-DMKG有效支持多模态知识编辑下的复杂推理，MMQAKE为未来研究提供了更全面的评估标准。

Abstract: Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.

</details>


### [341] [Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models](https://arxiv.org/abs/2512.00918)
*Cen Lu,Yung-Chen Tang,Andrea Cavallaro*

Main category: cs.AI

TL;DR: 本文提出CAN方法，通过检测一致激活的神经元并逐步掩码，发现仅需屏蔽语言模型前馈网络中极少数（极端情况下仅4个）神经元即可导致大型视觉-语言模型（LVLMs）发生灾难性崩溃；关键神经元主要集中于语言模型部分，尤其是下投影层；崩溃呈现两阶段模式：先表达能力下降，后突然完全失效。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）虽展现出强大多模态理解能力，但其鲁棒性尚不明确，亟需识别导致灾难性崩溃的关键结构脆弱点。

Method: 提出CAN（Consistently Activated Neurons）方法，结合一致激活神经元检测与渐进式掩码策略，定位引发模型崩溃的关键神经元。

Result: 在LLaVA-1.5-7b-hf和InstructBLIP-Vicuna-7b上验证，仅屏蔽语言模型前馈网络中极少量神经元（如4个）即可触发灾难性崩溃；关键神经元主要集中于语言模型（而非视觉模块），下投影层尤为脆弱；崩溃过程呈现两阶段模式。

Conclusion: LVLMs存在显著结构性脆弱性，关键神经元高度集中于语言模型组件，尤其下投影层；该发现为LVLM安全性研究提供了重要启示。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.

</details>


### [342] [Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE](https://arxiv.org/abs/2512.00969)
*Felix Saretzky,Lucas Andersen,Thomas Engel,Fazel Ansari*

Main category: cs.AI

TL;DR: 本文提出了一种基于因果机器学习的预测性维护模型，旨在从故障预测转向主动处方，通过‘假设’模拟评估干预措施对关键绩效指标（如OEE）的影响，从而数据驱动地排序并推荐最优行动。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型依赖虚假相关性而非真实因果关系，导致无法解释故障原因及选择有效干预，制约了从预测性维护向处方性维护的转型。

Method: 构建基于预训练因果基础模型的‘假设’模型，用于估计各类潜在修复措施对系统级KPI的因果效应，并据此对干预措施进行数据驱动排序。

Result: 在半合成制造数据上验证了该模型优于基线机器学习模型，能识别根因并量化其运营影响，支持工程师在因果环境中测试方案、优化决策、减少停机损失。

Conclusion: 该工作为构建鲁棒的处方性维护框架奠定了技术基础，推动维护策略从‘会预测’迈向‘懂因果、可干预、能优化’。

Abstract: The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a "what-if" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.

</details>


### [343] [IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch](https://arxiv.org/abs/2512.00997)
*Param Biyani,Shashank Kirtania,Yasharth Bajpai,Sumit Gulwani,Ashish Tiwari*

Main category: cs.AI

TL;DR: IndiMathBench是一个基于印度数学奥林匹克竞赛题目的、由人类验证的Lean形式化定理证明评测基准，包含312个正式定理及其非正式问题描述，旨在评估数学推理与自动形式化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的数学定理自动形式化数据集缺乏高质量的人类验证和特定文化背景的题目覆盖，尤其是来自印度数学竞赛的题目。因此需要构建一个经过人工审核、来源可靠且具有挑战性的基准来评估前沿模型在数学定理证明中的真实表现。

Method: 提出一种AI辅助的人工协同流水线，结合类别检索、编译器反馈迭代优化和多模型集成生成候选形式化表达，并通过交互式仪表板提供自动化质量摘要，供专家高效验证并最终构建IndiMathBench数据集。

Result: 该数据集包含312个配对的非正式问题与Lean 4形式化定理；实验表明当前前沿模型在自动形式化中仍面临语法有效性和语义正确性之间的显著差距，即使经过迭代优化，定理证明成功率仍然较低。

Conclusion: IndiMathBench构成一个具有挑战性的数学推理评测基准，揭示了当前自动形式化与定理证明技术的局限性，为未来研究提供了高质量资源。

Abstract: We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.

</details>


### [344] [ChartAnchor: Chart Grounding with Structural-Semantic Fidelity](https://arxiv.org/abs/2512.01017)
*Xinhang Li,Jingbo Zhou,Pengfei Luo,Yixiong Xiao,Tong Xu*

Main category: cs.AI

TL;DR: 本文提出ChartAnchor基准，用于全面评估多模态大语言模型（MLLMs）在图表理解中的结构化能力，涵盖8000+图表-表格-代码三元组、30种图表类型，并设计图表到代码生成与图表到表格重建双任务及多层次评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准存在图表类型单一、任务孤立、评估不完整等问题，难以全面衡量模型在数值推理、多模态对齐和结构重建等方面的图表锚定（chart grounding）能力。

Method: 构建ChartAnchor基准，包含8k+真实与增强来源的图表-表格-代码三元组，覆盖30种图表类型；设计两个互补任务——图表到代码生成与受控图表到表格重建；引入融合语义验证、风格分析与感知指标的多级评估框架。

Result: 在主流MLLMs上的实验揭示其在数值精度和可执行代码生成方面存在显著缺陷，表明当前模型缺乏深层结构化推理能力。

Conclusion: ChartAnchor为图表锚定提供了首个统一、严谨且面向应用的评估基准，推动MLLMs在科学、金融与工业等领域的图表理解能力发展。

Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.

</details>


### [345] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee,Kyoung-Woon On,Simeng Han,Arman Cohan,Julia Hockenmaier*

Main category: cs.AI

TL;DR: 本文提出了LEGIT，一个包含24K实例的法律推理数据集，用于评估大模型在法律领域生成的推理轨迹的质量。通过将法院判决转化为对立双方论点与法院结论的层次化树结构作为评估标准，并经专家验证其可靠性，研究发现法律问题覆盖度和正确性均显著影响大模型的法律推理能力；同时，检索增强生成（RAG）与基于规则的强化学习（RL）对提升推理能力具有互补作用。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLM）在专业领域（如法律）中生成的推理轨迹质量至关重要，但因任务复杂性而极具挑战性。现有方法缺乏细粒度、可靠且可解释的评估标准。

Method: 构建LEGIT数据集：将真实法院判决转化为结构化的法律问题树（含双方论点与法院结论），作为评估推理轨迹覆盖度与正确性的细粒度rubric；通过法律专家标注验证rubric可靠性；并在该数据集上系统评测LLM推理性能及RAG、RL等增强方法的效果。

Result: 1）LLM法律推理能力显著受限于法律问题覆盖度与结论正确性；2）RAG提升整体推理能力（如覆盖度），而基于rubric的RL提升结论正确性但可能牺牲部分覆盖度，二者效果互补。

Conclusion: LEGIT为法律推理评估提供了首个大规模、细粒度、专家验证的基准；研究揭示了覆盖度与正确性之间的权衡关系，并证明RAG与RL结合是提升专业领域推理质量的有效路径。

Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [346] [Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal](https://arxiv.org/abs/2512.01045)
*Shenxi Liu,Kan Li,Mingyang Zhao,Yuhang Tian,Shoujun Zhou,Bin Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为Med-CRAFT的神经符号数据工程框架，用于自动生成具有逻辑可解释性的医学视频推理基准数据集，解决了传统标注成本高和合成方法易产生幻觉的问题。


<details>
  <summary>Details</summary>
Motivation: 高质量、逻辑标注的医学视频数据集稀缺，传统人工标注成本高且不可扩展，现有合成方法存在随机性幻觉和缺乏逻辑可解释性的问题。

Method: 提出Med-CRAFT框架，将基准合成形式化为确定性的图遍历过程：从原始视频中提取结构化视觉原语（如手术器械、解剖边界），构建动态时空知识图，并基于图中有效路径生成查询，确保每个合成项目具有严格的思维链（CoT）溯源。

Result: 实现了M3-Med-Auto大规模医学视频推理基准，展现出细粒度时间选择性和多跳逻辑复杂性；评估显示生成的查询复杂度与专家标注数据集相当，且逻辑对齐分析表明图结构与当前主流MLLM的推理步骤高度相关。

Conclusion: Med-CRAFT实现了可扩展、低成本的医学视频基准构建，能够将可验证的逻辑编码到视觉-语言基准中，为关键领域提供了鲁棒评估协议的构建路径。

Abstract: The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \textbf{\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.

</details>


### [347] [Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids](https://arxiv.org/abs/2512.01046)
*Hadi Nekoei,Alexandre Blondin Massé,Rachid Hassani,Sarath Chandar,Vincent Mai*

Main category: cs.AI

TL;DR: 本文提出了一种名为Shielded Controller Units (SCUs)的可解释、分层约束保障机制，用于增强强化学习在远程微电网能量管理中的安全性与实用性，在满足所有运行约束前提下实现24%燃油消耗降低。


<details>
  <summary>Details</summary>
Motivation: 现实世界中（尤其是能源转型场景如远程微电网）的强化学习需在复杂动态、强约束和监管要求下保证安全可靠，而现有方法缺乏可解释的约束保障机制。

Method: 提出SCU框架：基于系统先验动力学知识，将环境分层分解，每个SCU显式负责子集约束；通过屏蔽（shielding）机制实时干预RL动作以确保约束满足。

Result: 在严格运行要求的远程微电网任务中，配备SCU的RL智能体实现24%燃油消耗下降，且未加剧电池老化，同时100%满足所有操作约束，性能优于基线方法。

Conclusion: SCU是一种面向实际部署的、系统化且可解释的约束保障方法，有望推动强化学习在能源转型相关决策问题中的安全落地。

Abstract: Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.

</details>


### [348] [Automating the Refinement of Reinforcement Learning Specifications](https://arxiv.org/abs/2512.01047)
*Tanmay Ambadkar,Đorđe Žikelić,Abhinav Verma*

Main category: cs.AI

TL;DR: 本文提出了AutoSpec框架，通过探索引导策略自动细化粗粒度的逻辑规范，以增强强化学习在复杂任务中的学习效果。


<details>
  <summary>Details</summary>
Motivation: 当任务被欠指定时，智能体可能无法学习到有用的策略，因此需要改进粗粒度的逻辑规范以提供更好的学习指导。

Method: 利用SpectRL规范逻辑的组合性质，设计了四种细化过程，通过修改或添加边规范来优化抽象图，并保证规范的正确性。

Result: 实验证明，使用AutoSpec生成的细化规范可以显著提升强化学习算法解决复杂控制任务的能力。

Conclusion: AutoSpec能够在不改变原规范语义的前提下有效提升逻辑规范的指导性，从而改善强化学习性能。

Abstract: Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \textsc{AutoSpec} are utilized.

</details>


### [349] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren,Yan Zhuang,Xiaokang Ye,Lingjun Mao,Xuhong He,Jianzhi Shen,Mrinaal Dogra,Yiming Liang,Ruixuan Zhang,Tianai Yue,Yiqing Yang,Eric Liu,Ryan Wu,Kevin Benavente,Rajiv Mandya Nagaraju,Muhammad Faayez,Xiyan Zhang,Dhruv Vivek Sharma,Xianrui Zhong,Ziqiao Ma,Tianmin Shu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 本文介绍了SimWorld，一个基于Unreal Engine 5构建的新型世界模拟器，旨在支持大语言模型（LLM）和视觉语言模型（VLM）智能体在逼真、开放、物理与社会动态丰富的环境中进行开发与评估。


<details>
  <summary>Details</summary>
Motivation: 现有世界模拟器受限于手工构造环境、简化物理/社会规则，且缺乏对LLM/VLM原生支持，难以支撑智能体在真实复杂物理与社会环境中生存与发展的研究需求。

Method: 设计并实现SimWorld模拟器，具备三大核心能力：(1) 基于UE5的高保真、开放世界模拟（含物理/社会动力学与语言驱动的程序化生成）；(2) 面向LLM/VLM的多模态输入与开放式词汇动作接口；(3) 可定制、可扩展的多样化物理与社会推理任务场景。并在多智能体长周期递送任务中实证部署前沿LLM（如GPT-4o、Gemini-2.5-Flash等）。

Result: 在多智能体递送任务中揭示了不同前沿LLM在策略协作与竞争中的显著推理差异与共性局限；SimWorld已开源，成为推动具身智能研究的基础平台。

Conclusion: SimWorld填补了面向真实世界复杂性的LLM/VLM智能体仿真平台空白，为具身智能、AI经济行为、社会交互建模等跨学科研究提供了关键基础设施。

Abstract: While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [350] [CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents](https://arxiv.org/abs/2512.01089)
*Peter Jansen,Samiah Hassan,Pragnya Narasimha*

Main category: cs.AI

TL;DR: 本文提出了CodeDistiller系统，能够从大量科学GitHub仓库中自动提取并验证领域特定的代码示例库，从而增强自动化科学发现（ASD）系统的能力。实验表明，该系统生成的代码库使ASD代理在材料科学任务中生成更准确、完整且科学合理的实验。


<details>
  <summary>Details</summary>
Motivation: 现有的ASD系统受限于仅从参数化知识生成代码的能力，通常依赖少量人工编写或通用的代码示例，限制了其性能和适用范围。因此需要一种无需人工干预即可扩展高质量、领域特定代码示例的方法。

Method: 提出CodeDistiller系统，结合自动处理与领域专家评估，从250个材料科学GitHub仓库中提取功能代码示例，并构建可被ASD代理调用的高质量代码库。

Result: 最佳模型能为74%的仓库生成功能性代码示例；下游评估显示，使用CodeDistiller增强的ASD代理相比仅使用通用代码示例的代理，生成的实验更准确、完整且科学合理。

Conclusion: CodeDistiller通过自动化提炼真实科研代码，显著提升了ASD系统在材料科学中的实验生成能力，减少了对人工编码示例的依赖，拓展了自动化科学发现的应用边界。

Abstract: Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.

</details>


### [351] [Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems](https://arxiv.org/abs/2512.01099)
*Daria Smirnova,Hamid Nasiri,Marta Adamska,Zhengxin Yu,Peter Garraghan*

Main category: cs.AI

TL;DR: 本文提出了一种名为GUIDE的能源感知模型选择框架，通过在决策中引入定量的模型性能特征，解决了现有基于大语言模型（LLM）的编排系统因依赖定性描述而导致的模型选择次优、准确率低和能耗高的问题。实验表明，GUIDE在多种任务上提升了0.90%-11.92%的准确率，最高可提升54%的能效，并将编排器的选择延迟从4.51秒降至7.2毫秒。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM编排系统依赖对模型的定性描述进行决策，无法真实反映模型的能力和性能特征，导致模型选择不理想、准确率下降和能源成本增加。

Method: 提出GUIDE框架，通过实证分析LLM编排的局限性，并在决策过程中整合定量的模型性能指标（如准确率、延迟、能耗），实现能量-性能权衡的优化。

Result: 实验结果显示，相比基线方法，GUIDE在多个任务上提升了0.90%-11.92%的准确率，最高实现54%的能效提升，并将编排延迟从4.51秒显著降低至7.2毫秒。

Conclusion: GUIDE通过引入定量性能数据进行模型选择，有效提升了AI系统的准确性、能效和响应速度，为构建更高效的多模型协作系统提供了可行方案。

Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.

</details>


### [352] [Foundation Priors](https://arxiv.org/abs/2512.01107)
*Sanjog Misra*

Main category: cs.AI

TL;DR: 本文提出了“基础模型先验”（foundation prior）的概念，将大模型生成的合成数据视为来自先验预测分布的样本，而非真实观测数据，强调其反映用户主观信念与模型学习模式的结合，并提供了将其整合到统计分析中的贝叶斯框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛生成高质量合成数据，亟需厘清这些数据的本质及其在实证研究中的合理角色，避免将合成‘事实’误作真实数据。

Method: 通过建立一个指数倾斜的广义贝叶斯更新模型，推导出基础模型先验，显式建模用户原始先验、提示工程过程和对模型信任度对生成结果的影响。

Result: 提出了一套将合成数据作为主观先验纳入标准统计与计量分析流程的方法，适用于复杂模型优化、潜变量构建、实验设计引导等多种应用场景。

Conclusion: 将生成模型输出视为结构化的主观先验而非真实数据，为在实证研究中审慎利用基础模型提供了原则性框架，有助于防止误导性推断。

Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.

</details>


### [353] [A Benchmark of Causal vs Correlation AI for Predictive Maintenance](https://arxiv.org/abs/2512.01149)
*Krishna Taduri,Shaunak Dhande,Giacinto Paolo,Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该研究比较了八种预测模型在数控机床预测性维护中的表现，发现基于因果推断的模型（L5）在成本节约、召回率和精确度方面显著优于传统相关性模型，每年可节省116万美元，减少97%的误报。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法优化的是统计准确性指标，无法反映制造业中漏检故障成本远高于误报的实际业务需求，且难以区分因果关系与虚假相关。因此需要引入因果AI来提升预测的经济效果和可解释性。

Method: 评估了从基础统计方法到正式因果推断模型在内的八种预测模型，在包含10,000台CNC机器（故障率为3.3%）的数据集上进行测试，重点比较其在极端成本不对称环境下的年度成本节约、召回率、精确度和泛化能力。

Result: 因果推断模型L5实现了87.9%的召回率和92.1%的精确度，误报数从165降至5（减少97%），训练与测试性能差距仅为2.6个百分点，年均成本节约达116万美元，比最佳相关性模型L3多节约约8万美元。

Conclusion: 结合领域知识的因果AI方法在预测性维护中能带来更优的财务效益和更强的可解释性，优于仅依赖相关性的传统机器学习方法。

Abstract: Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.

</details>


### [354] [fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment](https://arxiv.org/abs/2512.01189)
*Chunzheng Zhu,Jialin Shao,Jianxin Lin,Yijun Wang,Jing Wang,Jinhui Tang,Kenli Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为fMRI2GES的新方法，利用未配对数据通过双脑解码对齐（Dual Brain Decoding Alignment）从fMRI信号中重建与言语相关的手势。该方法结合fMRI-to-text和text-to-gesture模型，实现自我监督训练，无需完全配对的{脑、语音、手势}数据，有效解决了数据稀缺问题，并探索了不同大脑区域对生成结果的影响。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏配对的{脑、语音、手势}数据，难以使用深度学习模型解码与言语相关手势的脑活动。现有研究多集中于脑到图像或语言的重建，而手势重建仍属空白。

Method: 提出fMRI2GES框架，采用Dual Brain Decoding Alignment策略：利用fMRI-to-text模型和text-to-gesture模型分别构建两条路径，在未配对数据上训练fMRI-to-gesture模型，并通过双路输出对齐实现自监督学习。同时分析不同皮层脑区（ROIs）对生成效果的影响。

Result: 实验表明该方法能直接从fMRI记录中重建出富有表现力的手势动作，并验证了不同脑区在手势生成中的作用，展示了在无配对三模态数据下仍可有效训练跨模态重建模型的能力。

Conclusion: 本工作首次实现了从脑信号到言语手势的重建，为理解大脑如何编码非语言行为提供了新工具和神经科学见解，推动了认知科学与脑-机接口的发展。

Abstract: Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \{brain, speech, gesture\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.

</details>


### [355] [Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction](https://arxiv.org/abs/2512.01210)
*Ruiyu Wang,Tuan Vinh,Ran Xu,Yuyin Zhou,Jiaying Lu,Carl Yang,Francisco Pasquel*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱（PrimeKG）引导的链式思维（CoT）框架，用于MIMIC-III中就诊级疾病预测，并生成临床合理、时间一致的可解释推理；该方法在小样本下表现优于传统基线，并在CRADLE数据集上实现零样本迁移，且获临床医生高度评价。


<details>
  <summary>Details</summary>
Motivation: 现有EHR预测模型缺乏细粒度、患者层面可用的、临床可理解的事前解释，难以支持临床决策。

Method: 将ICD-9码映射至PrimeKG，提取疾病相关节点与多跳推理路径作为CoT生成的结构化支架；仅保留结论与真实标签一致的解释，构建监督语料；在此基础上微调轻量级LLaMA-3.1-Instruct-8B和Gemma-7B模型。

Result: 在10种疾病、小训练集（400/1000例）下，AUROC达0.66–0.70，macro-AUPR达0.40–0.47；零样本迁移到CRADLE后准确率从~0.40–0.51提升至0.72–0.77；盲法临床评估显示其CoT解释在清晰性、相关性和临床正确性上更受偏好。

Conclusion: KG引导的CoT框架能有效提升EHR预测模型的可解释性与泛化能力，为临床可信AI提供了新范式。

Abstract: Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.

</details>


### [356] [Unsupervised decoding of encoded reasoning using language model interpretability](https://arxiv.org/abs/2512.01222)
*Ching Fang,Samuel Marks*

Main category: cs.AI

TL;DR: 本文构建了一个可控测试平台，通过微调模型使其以ROT-13加密形式进行思维链推理但输出仍为可读英文，评估了机制可解释性方法（特别是logit lens）解码隐藏推理过程的能力，并提出一种全自动无监督解码流程，结果表明当前可解释性技术对简单编码推理具有较强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，其可能发展出人类难以监督的隐式或编码化推理过程；亟需评估现有可解释性技术能否穿透此类编码推理。

Method: 构建ROT-13加密思维链的可控测试床（基于DeepSeek-R1-Distill-Llama-70B），采用logit lens分析内部激活以解码推理过程，并结合自动改写设计全自动无监督解码流程。

Result: logit lens能在中间至后期层高精度解码加密推理；全自动无监督流程能较准确地从内部表征重建完整推理过程。

Conclusion: 当前机制可解释性技术对简单形式的编码推理具备比预期更强的鲁棒性；本工作为评估AI可解释性方法应对非人类可读推理的能力提供了初步框架。

Abstract: As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

</details>


### [357] [RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2512.01300)
*Dacheng Liao,Mengshi Qi,Peng Shu,Zhining Zhang,Yuxin Lin,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 本文提出了RoboDriveBench——首个面向端到端轨迹预测任务的鲁棒性基准，并设计了RoboDriveVLM框架与基于跨模态知识蒸馏的测试时自适应（TTA）方法，以提升VLM在自动驾驶中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型（VLM）的端到端自动驾驶系统在真实驾驶场景中存在多重风险，亟需系统性评估其鲁棒性并提出可靠改进方案。

Method: 构建RoboDriveBench基准（含11类模拟场景、64,559个轨迹预测案例），提出RoboDriveVLM框架（融合激光雷达、雷达等多模态数据至统一潜在空间），并设计基于跨模态知识蒸馏的测试时自适应（TTA）方法。

Result: 实验表明现有VLM端到端系统鲁棒性不足，而RoboDriveVLM与新TTA方法显著提升了在传感器与提示语 corrupted 场景下的性能。

Conclusion: VLM直接用于端到端自动驾驶尚不稳健；引入多模态对齐与测试时自适应是提升实际部署可靠性的关键路径。

Abstract: Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

</details>


### [358] [CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL](https://arxiv.org/abs/2512.01311)
*Shinji Mai,Yunpeng Zhai,Ziqian Chen,Cheng Chen,Anni Zou,Shuchang Tao,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 本文提出CuES框架，通过内在好奇心驱动和环境基础的任务生成，解决大语言模型智能体在工具增强环境中因任务稀缺导致的强化学习瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖预定义任务集合，但在新环境中工具语义和功能未知，缺乏结构化训练任务（即任务稀缺问题），成为规模化智能体强化学习的关键瓶颈。

Method: 提出CuES（Curiosity-driven and Environment-grounded Synthesis）框架：基于环境结构与功能自主生成多样化、可执行、有意义的任务；结合内在好奇心驱动探索、抽象交互模式为可复用任务模板，并通过轻量自上而下引导与基于记忆的质量控制进行优化。

Result: 在AppWorld、BFCL、WebShop三个典型环境中，CuES生成的任务分布在多样性与可执行性上达到或超越人工构建数据集，并显著提升下游策略性能。

Conclusion: 好奇心驱动、环境基础的任务生成为智能体提供了可扩展的学习基础，使其不仅能学会如何行动，还能学会学什么。

Abstract: Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.

</details>


### [359] [Extending NGU to Multi-Agent RL: A Preliminary Study](https://arxiv.org/abs/2512.01321)
*Juan Hernandez,Diego Fernández,Manuel Cifuentes,Denis Parra,Rodrigo Toro Icarte*

Main category: cs.AI

TL;DR: 本文将NGU算法扩展到多智能体环境，在PettingZoo的simple_tag环境中验证其性能，发现共享经验回放缓冲区可提升性能与稳定性，而新颖性共享和异构beta参数效果有限。


<details>
  <summary>Details</summary>
Motivation: 将单智能体中表现优异的NGU算法（结合情景新颖性和内在动机）拓展至多智能体稀疏奖励场景，探索其适用性与关键设计选择的影响。

Method: 在simple_tag多智能体环境中实现并评估NGU；对比多智能体DQN基线；系统分析三种设计：共享/独立经验回放、不同k值下的新颖性共享、同质/异质beta参数。

Result: NGU优于DQN基线，共享回放缓冲区效果最佳且最稳定；k=1时新颖性共享效果尚可，k增大则性能下降；异构beta未带来增益。

Conclusion: NGU可有效应用于多智能体环境，关键在于经验共享与内在探索信号的精细调优。

Abstract: The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

</details>


### [360] [A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles](https://arxiv.org/abs/2512.01331)
*Saman Ahmadi,Mahdi Jalili*

Main category: cs.AI

TL;DR: 本文研究了大规模路网中电动汽车能量最优路径规划问题，提出了一种基于多目标A*搜索的标签设定方法，通过新的轮廓支配规则避免处理复杂轮廓，在真实道路网络上的实验表明该方法性能优异。


<details>
  <summary>Details</summary>
Motivation: 传统电动汽车路径规划假设初始能量已知，但现实中常存在能量不确定性，需对所有可能的初始能量水平进行路径规划，即能量最优轮廓搜索。现有方法依赖复杂的轮廓合并过程，效率较低。

Method: 提出一种基于多目标A*搜索的简单而有效的标签设定方法，引入新的轮廓支配规则，避免生成和处理复杂轮廓，并设计了四种变体进行评估。

Result: 在真实世界道路网络和真实能耗数据上的实验结果显示，所提出的能量轮廓A*搜索方法性能与已知初始能量下的能量最优A*相当。

Conclusion: 该方法简化了能量轮廓搜索过程，提高了计算效率，适用于大规模路网中具有能量不确定性的电动汽车路径规划。

Abstract: We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.

</details>


### [361] [Benchmarking Overton Pluralism in LLMs](https://arxiv.org/abs/2512.01351)
*Elinor Poole-Dayan,Jiayi Wu,Taylor Sorensen,Jiaxin Pei,Michiel A. Bakker*

Main category: cs.AI

TL;DR: 本文提出了一种衡量LLM中Overton多元主义的新框架，通过人类研究和自动化基准评估多个模型的多元观点覆盖程度，发现现有模型仍有较大提升空间，并提供了一个与人类判断高度相关的自动化评测工具。


<details>
  <summary>Details</summary>
Motivation: 为了量化大语言模型（LLMs）在输出中是否充分代表多样化的观点，即实现Overton多元主义，需要一个可操作、可重复的测量标准，以推动模型向更公平、包容的方向发展。

Method: 首先将Overton多元主义形式化为一种集合覆盖度量（OvertonScore），然后开展大规模具有美国代表性的人类研究（N=1209，60个问题，8个LLM），并开发一个能高度复现人类判断的自动化基准测试。

Result: 模型的平均OvertonScore在0.35–0.41之间，DeepSeek V3表现最佳，但距理论最大值1.0仍有显著差距；自动化基准与人类判断的等级相关系数达ρ=0.88，具备良好代理性能。

Conclusion: 本工作将多元对齐从规范性目标转化为可测量的基准，建立了评估和改进LLM多元性的系统化基础，为未来模型发展提供了实用工具和方向指引。

Abstract: We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($ρ=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.

</details>


### [362] [The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness](https://arxiv.org/abs/2512.01354)
*Zhongjie Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种基于认知过程模拟的合成数据生成新范式（PMCSF框架），通过逆向解码和重构人类文本的认知状态，生成具有真实人类特征（如不规则性）的文本，显著提升下游任务性能并缓解模型坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据过度追求统计平滑性，忽视了人类文本中源于认知机制的长尾、不规则特征，导致长期训练下模型坍塌。

Method: 提出Prompt-driven Cognitive Computing Framework（PMCSF），包含认知状态解码器（CSD）将文本映射为结构化认知向量，以及认知文本编码器（CTE）结合数学定义的认知扰动算子生成带人类典型缺陷的文本。

Result: 在认知编解码验证中，CTE生成文本与人类文本JS散度仅0.0614（远低于LLM的0.4431），通过双盲媒体评审且认知轮廓一致性ICC > 0.9；在A股压力测试中，使用该数据的策略在2015股灾中最大回撤降低47.4%，获得8.6%防御阿尔法，超额收益达交易成本33倍。

Conclusion: 建模人类认知局限而非复制数据表层分布，可生成具备真实功能增益的合成数据，为解决AI数据坍塌危机提供了可行技术路径。

Abstract: Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.

</details>


### [363] [A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building](https://arxiv.org/abs/2512.01434)
*Daull Xavier,Patrice Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco*

Main category: cs.AI

TL;DR: 提出CollabToolBuilder，一种结合人类专家指导的多智能体LLM框架，通过迭代学习生成工具以实现目标，减少任务适应和反馈获取时间。


<details>
  <summary>Details</summary>
Motivation: 为了在复杂迭代问题中更好地对齐人类意图与过程，降低领域适应成本和人工反馈开销。

Method: 采用四个专用代理（Coach、Coder、Critic、Capitalizer）协同工作，结合强化动态提示和系统化人工反馈机制。

Result: 初步实验显示该框架能有效生成高质量科研论文或专利等科学文档。

Conclusion: CollabToolBuilder为科学文档生成等复杂任务提供了一种可复用、可扩展的系统级解决方案。

Abstract: We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.

</details>


### [364] [A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale](https://arxiv.org/abs/2512.01440)
*Sylvain Marié,Pablo Knecht*

Main category: cs.AI

TL;DR: 本文提出了一种新的时间序列距离度量方法——选择性时间汉明距离（STH），用于处理离散事件系统的状态转移事件时间序列（STE-ts），避免了传统重采样带来的失真和高计算成本。


<details>
  <summary>Details</summary>
Motivation: 标准分析方法未能充分利用离散事件系统中事件与状态的双重特性，通常将信号建模为仅强调事件顺序的转移序列，或重采样为分类/序数状态时间序列，后者在观测周期和事件数量增大时代价高昂且失真严重。

Method: 定义状态转移事件时间序列（STE-ts），并提出选择性时间汉明距离（STH），该方法同时利用转移时间和状态驻留时长，无需重采样，并可聚焦于多个感兴趣的状态。

Result: STH 在精度和计算效率上优于重采样后的汉明距离和 Jaccard 距离，并在模拟和真实数据集上验证了其优势。

Conclusion: STH 是一种更精确、高效且灵活的距离度量方法，适用于大规模离散事件系统的时间序列分析，无需牺牲信息完整性进行重采样。

Abstract: Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.

</details>


### [365] [Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework](https://arxiv.org/abs/2512.01452)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: 本文提出了一种基于DSPy和GEPA模块的可编程风险偏倚（RoB）评估流程，通过代码化优化替代人工提示工程，提升自动化RoB评估的可复现性、透明性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM用于RoB评估的方法依赖难以复现、泛化和评估的手工提示设计，而人工评估又耗时且易变。

Method: 采用DSPy框架及其GEPA模块，通过Pareto引导搜索自动优化LLM提示，并生成可检查的执行轨迹；在100项RCT上评估，覆盖7个RoB领域，对比多种开源与商用大模型及手工提示。

Result: GEPA在Random Sequence Generation等报告较清晰的领域表现最优，相比手工提示在Random Sequence Generation和Selective Reporting上准确率提升30%-40%，整体性能具竞争力且更可复现。

Conclusion: GEPA能生成一致、可复现的提示，支持LLM在证据合成中结构化、可信赖的应用。

Abstract: Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.

</details>


### [366] [Multi-Path Collaborative Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.01485)
*Jindi Lv,Yuhao Zhou,Zheng Zhu,Xiaofeng Wang,Guan Huang,Jiancheng Lv*

Main category: cs.AI

TL;DR: 本文提出M3PO，一种通过多路径并行推理与跨路径协作优化的强化学习框架，以克服传统链式思维中自回归解码的确定性限制，提升大模型多步推理的可靠性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统链式思维（CoT）在解码过程中存在内部确定性，限制了对合理推理路径的探索；现有基于软抽象token的方法仍受贪心自回归解码制约，难以有效利用替代推理可能性。

Method: 提出Multi-Path Perception Policy Optimization（M3PO），采用并行策略rollout生成多样推理路径，并通过轻量级协同机制实现跨路径交互与策略更新，使各路径能借助同伴反馈优化自身推理。

Result: M3PO在知识密集型与推理密集型基准测试中均达到SOTA性能，同时保持模型可解释性与推理效率。

Conclusion: 多路径协同学习是提升大语言模型鲁棒推理能力的有效途径，M3PO为解决CoT中的确定性瓶颈提供了新范式。

Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.

</details>


### [367] [SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry](https://arxiv.org/abs/2512.01507)
*Daniel Armstrong,Zlatko Jončev,Andres M Bran,Philippe Schwaller*

Main category: cs.AI

TL;DR: 本文提出了一种利用大语言模型将合成化学战略知识转化为可验证、可解释的Python代码的新方法，从而弥合了计算机辅助合成规划（CASP）中战术与战略之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有CASP系统虽能生成化学上有效的反应步骤，但难以融入收敛合成、保护基最小化、环构建顺序等战略性考量。

Method: 利用大语言模型从合成路线中提取战略知识，将其形式化为具有战略/战术语义的Python函数；构建带战略标签的USPTO-ST数据集和开源代码库；支持自然语言驱动的路线检索与基于策略的聚类分析。

Result: 在路线检索任务中达到75% Top-3准确率；实现了比传统方法更细粒度的化学直观路线聚类；通过时间趋势分析验证了策略库的历史合理性。

Conclusion: 该工作首次将合成战略显式编码为可执行、可测试、可检索的代码，推动CASP从仅依赖分子结构转向支持以战略为导向的路线设计、搜索与评估。

Abstract: Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.

</details>


### [368] [LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems](https://arxiv.org/abs/2512.01556)
*Zhiyuan Wang,Aniri,Tianlong Chen,Yue Zhang,Heng Tao Shen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.AI

TL;DR: 提出LEC方法，通过线性期望约束实现对大语言模型预测的错误发现率（FDR）控制，在保证统计可靠性的同时提高有效样本保留率，并扩展至双模型路由机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常生成不可靠答案，现有不确定性度量方法无法有效区分对错，缺乏统计保证，导致用户可能接受错误结果。

Method: 将选择性预测重新定义为带线性期望约束的决策问题，利用可交换校准样本来构建有限样本下的充分条件，计算满足FDR约束且最大化覆盖的阈值，并扩展为双模型路由机制。

Result: 在闭合和开放问答任务上，LEC实现了更精确的FDR控制并显著提升样本保留率；双模型路由在更低风险下接受更多正确样本。

Conclusion: LEC为大语言模型提供了具有统计保证的可靠预测框架，有效平衡了预测准确性与风险控制，尤其在模型路由中表现出优越性能。

Abstract: Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.

</details>


### [369] [CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning](https://arxiv.org/abs/2512.01616)
*Chainesh Gautam,Raghuram Bharadwaj Diddigi*

Main category: cs.AI

TL;DR: 本文提出了一种受CLIP启发的新型方法，通过联合预训练的语言-策略对，在自然语言和策略嵌入间构建统一表征空间，以提升多任务强化学习中的跨任务迁移效率。


<details>
  <summary>Details</summary>
Motivation: 当前需要开发能在同一环境中解决多个与语言相关任务的智能体，而现有方法在跨任务迁移上效率不足。

Method: 借鉴CLIP的跨模态对齐思想，将任务指令（语言模态）与对应策略（行为模态）视为同一概念的不同表达，通过对比学习建立语言与策略嵌入的统一表征空间。

Result: 实验表明该方法显著提升了多任务间的迁移速度与效率。

Conclusion: 将CLIP范式扩展至强化学习领域是可行且有效的，语言与策略可被统一表征，从而支持高效多任务泛化。

Abstract: Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

</details>


### [370] [Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation](https://arxiv.org/abs/2512.01723)
*Saba Kublashvili*

Main category: cs.AI

TL;DR: 本文提出了HistoricalML，一种概率神经符号框架，用于解决历史事件建模中的数据稀缺、噪声大、缺乏反事实及需可解释性等挑战；结合贝叶斯不确定性量化、结构因果模型、Shapley值分配与注意力神经网络，在两个历史案例中验证了其有效性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 历史事件建模面临极端数据稀缺（N<<100）、测量异质噪声大、缺乏反事实及需人类可解释解释等根本性挑战。

Method: 提出HistoricalML框架，融合：(1) 贝叶斯不确定性量化以区分认知与偶然不确定性；(2) 结构因果模型支持混杂下的反事实推理；(3) 基于合作博弈论（Shapley值）的公平分配建模；(4) 基于注意力机制的神经架构实现上下文相关因子加权。

Result: 理论证明该方法在强先验下稀疏数据中具有一致估计性，且Shapley分配满足公理化公平性；实证上识别出德国在一战前+107.9%结构性张力，重现坎尼/扎马战役胜率（57.3%/57.8%），并揭示迦太基政治支持而非军力是决定性因素。

Conclusion: HistoricalML为小样本、高解释性历史建模提供了统一、可验证、公平且因果可追溯的计算范式。

Abstract: Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.

</details>


### [371] [Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems](https://arxiv.org/abs/2512.01786)
*Xiaochuan Li,Ke Wang,Girija Gouda,Shubham Choudhary,Yaqun Wang,Linwei Hu,Joel Vaughan,Freddy Lecue*

Main category: cs.AI

TL;DR: 提出LLM Jury-on-Demand，一种动态、基于学习的评估框架，通过可靠性预测器动态选择最可靠的LLM法官并加权聚合其评分，显著提升与人类判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法如人工评估成本高，单个LLM法官有偏见，静态评审团缺乏适应性，难以满足高风险领域对可扩展且可靠评估的需求。

Method: 训练一组可靠性预测器，利用token分布、嵌入和结构化输入特征来预测LLM法官与人类专家的一致性，并为每个数据点动态组建最优评审团，以可靠性作为权重聚合评分。

Result: 在摘要和RAG基准上的实验表明，该动态评审系统比单法官和静态评审基线显著更接近人类判断。

Conclusion: 自适应、基于学习的评审团有望构建更可扩展、可靠和可信的现代LLM评估系统，适用于高风险应用场景。

Abstract: As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

</details>


### [372] [H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons](https://arxiv.org/abs/2512.01797)
*Cheng Gao,Huimin Chen,Chaojun Xiao,Zhiyi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文系统研究了大语言模型中与幻觉现象相关的神经元（H-Neurons），发现极少数（<0.1%）神经元可稳定预测幻觉，且具有跨场景泛化性；这些神经元因果地导致过合规行为，并起源于预训练阶段。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究从训练数据和目标等宏观角度分析大语言模型的幻觉问题，但其在神经元层面的机制尚不清楚，亟需微观机制探索。

Method: 从三个维度系统分析幻觉相关神经元：（1）识别——通过稀疏子集预测幻觉；（2）行为影响——通过可控干预验证因果性；（3）起源分析——追溯至预训练基模型。

Result: 发现少于0.1%的神经元可高精度、跨场景预测幻觉；干预实验证明其导致过合规行为；这些神经元在预训练阶段即已形成并保持预测能力。

Conclusion: 幻觉现象具有可定位、可解释的神经元基础，其根源可追溯至预训练阶段，该发现为提升大语言模型可靠性提供了微观机制依据和新路径。

Abstract: Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

</details>


### [373] [Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees](https://arxiv.org/abs/2512.01870)
*Alessandro Breccia,Federica Gerace,Marco Lippi,Gabriele Sicuro,Pierluigi Contucci*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型是否能学习由自然数的迭代质因数分解生成的确定性树序列，发现GPT-2架构能在一定程度上学习该算术文本的内部语法结构。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型能否学习算术结构（而非仅经验数据）中的规律性，特别是由自然数迭代质因数分解导出的树序列所蕴含的确定性与统计结构。

Method: 将每个自然数映射为一个有根平面树，构成算术文本ℕ𝒯；使用GPT-2架构从零训练，数据为前10¹¹个元素；在next-word和masked-word预测任务上评估模型性能。

Result: 模型部分习得了ℕ𝒯的内部语法，捕捉到非平凡的正则性与相关性。

Conclusion: 表明语言模型的可学习性可能不仅限于经验数据，还可延伸至算术本身的结构性质。

Abstract: We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \mathbb{N}\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\mathbb{N}\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.

</details>


### [374] [Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning](https://arxiv.org/abs/2512.01878)
*Gaganpreet Jhajj,Fuhua Lin*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱（KG）网络中“惊讶最小化”来指导推理的方法，通过最短路径距离形式化“惊讶”，并探讨了图距离在图神经网络和基于模型的强化学习中的作用。


<details>
  <summary>Details</summary>
Motivation: 将神经科学中的自由能原理（FEP）引入知识图谱系统，以统一理解推理与生成模型，探索图结构中距离与信息处理效率的关系。

Method: 利用有向图中的最短路径距离来形式化“惊讶”，并将知识图谱视为智能体的生成模型，构建基于图距离的惊讶最小化框架。

Result: 建立了基于图距离的惊讶度量方法，并连接了图神经网络的消息传递深度与强化学习中的世界模型轨迹，初步验证了距离与惊讶之间的关系。

Conclusion: 图距离可以作为知识图谱中推理过程的指导原则，惊讶最小化为构建KG智能体提供了理论基础，并可能扩展至语法结构与自由能最小化的统一解释。

Abstract: In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.

</details>


### [375] [Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models](https://arxiv.org/abs/2512.01880)
*Daren Zhong,Dingcheng Huang,Clayton Greenberg*

Main category: cs.AI

TL;DR: 提出了一种基于n-gram语言模型的棋步预测框架，用于根据玩家技能水平进行行为分析，并通过模型选择器提升预测准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋引擎忽视了不同技能水平玩家下棋行为的变异性，难以准确建模人类走法模式。

Method: 将玩家分为七个技能等级，使用Lichess数据训练多个n-gram语言模型，并设计模型选择器动态匹配最适合的模型进行棋步预测。

Result: 在前16个半步的情况下，模型选择器对技能水平分类准确率达31.7%；整体棋步预测准确率比基准高出39.1%。

Conclusion: 该框架能有效捕捉人类玩家的行为特征，具备较高的计算效率和实际应用潜力，适用于实时国际象棋分析场景。

Abstract: Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.
  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.
  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.

</details>


### [376] [Learned-Rule-Augmented Large Language Model Evaluators](https://arxiv.org/abs/2512.01958)
*Jie Meng,Jin Mao*

Main category: cs.AI

TL;DR: 本文提出了一种规则增强的评估范式，通过LLM辅助的蒙特卡洛树搜索自动提取评分规则，并设计Chain-of-Rule和强化学习训练的规则增强评估器（RuAE），提升大语言模型在多样化评估任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的评估方法依赖人工设计的评估原则，成本高、与标注数据及LLM理解不一致，泛化能力差。

Method: 提出规则增强评估范式：1）LLM辅助的蒙特卡洛树搜索进行规则蒸馏；2）Chain-of-Rule（CoR）引导LLM遵循规则；3）通过强化学习训练规则增强的LLM评估器（RuAE）。

Result: 在多种评估任务上实验表明，该方法显著提升了LLM作为通用评估器的有效性和泛化性。

Conclusion: 规则蒸馏与规则引导推理相结合，能有效缓解LLM评估中的人工依赖与对齐问题，为构建通用、可扩展的LLM评估框架提供了新路径。

Abstract: Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

</details>


### [377] [From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning](https://arxiv.org/abs/2512.01970)
*Sitao Cheng,Xunjian Yin,Ruiwen Zhou,Yuxuan Li,Xinyi Wang,Liangming Pan,William Yang Wang,Victor Zhong*

Main category: cs.AI

TL;DR: 本文通过互补推理任务研究RL如何提升模型推理能力，发现RL并非简单放大已有行为，而是能合成新推理策略，但前提是模型已通过SFT掌握基础原子技能（参数推理与上下文推理）；该发现挑战了RL仅是概率放大器的传统观点。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习（RL）提升推理能力的机制：是激励新技能合成，还是仅放大已有行为？现有争议未解，尤其缺乏对复杂推理中RL作用机理的细粒度分析。

Method: 提出互补推理任务，构建可控合成人传记数据集，将推理能力解耦为参数推理（依赖内部知识）和上下文推理（依赖外部信息）两类原子技能；在I.I.D.、组合泛化、零样本三种难度下系统评估SFT与RL模型的表现，并揭示SFT泛化悖论。

Result: SFT在分布内表现优异但OOD泛化差，尤其零样本场景下崩溃；RL可作为推理合成器而非概率放大器，但仅当模型已通过SFT掌握两类原子技能时才有效；原子技能缺失则RL无法合成复杂推理。

Conclusion: RL并非简单放大器，而是在具备充分原子技能基础上主动合成复杂推理策略；解耦原子训练+RL是实现复杂推理泛化的可行路径。

Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

</details>


### [378] [Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback](https://arxiv.org/abs/2512.01979)
*Aiden Yiliu Li,Bizhi Yu,Daoan Lei,Tianhe Ren,Shilong Liu*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的多步视觉定位框架Chain of Ground (CoG)，通过迭代推理和逐步修正提升GUI界面中自然语言指令与界面元素的对齐精度，在ScreenSpot Pro和真实工业面板数据集TPanel UI上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在GUI定位任务中对小目标、视觉相似区域和复杂布局存在定位不准的问题，且未能充分利用模型自身的推理能力，缺乏可解释性。

Method: 提出Chain of Ground (CoG)框架，利用多模态大模型进行多轮迭代推理，每一步基于前序结果进行反思与调整，实现渐进式精确定位，无需额外训练。

Result: 在ScreenSpot Pro基准上达到68.4%准确率，提升4.8个百分点；在新提出的含模糊和遮挡的真实工业面板数据集TPanel UI上，相比强基线Qwen3-VL-235B提升6.9个百分点。

Conclusion: 通过结构化、迭代式的推理过程，可在不进行额外训练的情况下显著提升GUI定位性能，揭示了释放模型定位潜力的新方向。

Abstract: GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.

</details>


### [379] [LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess](https://arxiv.org/abs/2512.01992)
*Sai Kolasani,Maxim Saplin,Nicholas Crispino,Kyle Montgomery,Jared Quincy Davis,Matei Zaharia,Chi Wang,Chenguang Wang*

Main category: cs.AI

TL;DR: LLM CHESS 是一个用于评估大语言模型在国际象棋领域中推理与指令遵循能力的动态交互式评测框架，通过与随机对手对弈及与可调难度引擎对弈（估算 Elo），揭示了多数先进模型在基础任务中仍表现不佳，并发布了开源框架、排行榜与对局数据集。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准易被过拟合和记忆，难以真实反映模型的泛化推理与指令遵循能力；需一种动态、随机、抗饱和的评测方法。

Method: 构建 LLM CHESS 框架，让 50+ 开源/闭源模型以代理方式参与国际象棋对弈；采用胜率、败率、走法质量、合法性、幻觉动作、对局时长等行为指标评估；对顶尖推理模型进一步通过与可调强度引擎对弈估算 Elo 分数。

Result: 多数 SOTA 模型难以完成对局或稳定获胜；推理型与非推理型模型表现存在明显分层；即使任务简单、对手较弱，顶级推理模型仍面临挑战；该动态框架显著降低过拟合与基准饱和风险。

Conclusion: LLM CHESS 提供了一种更鲁棒、更具泛化性的推理与指令遵循能力评测范式，弥补了静态基准的局限，并已开源框架、 leaderboard 和游戏数据集以推动后续研究。

Abstract: We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [380] [Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions](https://arxiv.org/abs/2512.00004)
*Jihang Li,Bing Xu,Zulong Chen,Chuanfei Xu,Minping Chen,Suyu Liu,Ying Zhou,Zeyi Wen*

Main category: cs.IR

TL;DR: 本文提出了一种结合大语言模型（LLM）与角色感知多门控专家混合网络（MoE）的新型人才搜索框架，旨在提升招聘系统中职位匹配精度与 recruiter 行为建模能力，并通过多任务学习降低主观噪声。实验表明该方法显著提升 CTR、CVR 和转化率，带来显著成本节约。


<details>
  <summary>Details</summary>
Motivation: 现有人才搜索方法难以捕捉细粒度职位偏好、精细建模招聘人员行为，且易受主观判断噪声干扰。

Method: 1) 利用大语言模型从职位描述和历史招聘数据中提取细粒度招聘信号；2) 设计角色感知的多门控 MoE 网络以建模不同角色招聘人员的行为差异；3) 引入联合优化点击率（CTR）、转化率（CVR）和简历匹配相关性的多任务学习模块。

Result: 在真实招聘数据和线上 A/B 测试中，相对 AUC 提升 1.70%（CTR）和 5.97%（CVR），点击转化率提升 17.29%，年节省成本达数百万元人民币。

Conclusion: 该框架有效提升了人才搜索效果与业务价值，验证了 LLM 与角色感知建模在招聘推荐系统中的实用性与可扩展性。

Abstract: Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY.

</details>


### [381] [Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking](https://arxiv.org/abs/2512.00007)
*Jingyi Huang,Yuyi Yang,Mengmeng Ji,Charles Alba,Sheng Zhang,Ruopeng An*

Main category: cs.IR

TL;DR: 本研究提出了SAFE系统，一种结合大语言模型与检索增强生成（RAG）的代理系统，用于提升针对长篇幅新冠 misinformation 的自动化事实核查准确性。系统包含声明提取和验证两个代理，并在包含13万份新冠研究文献的语料库上运行。评估显示，SAFE显著优于基线模型，其中LOTR-RAG版本在一致性、有用性、清晰度和真实性方面表现最佳，而加入SRAG反而略微降低了整体性能。


<details>
  <summary>Details</summary>
Motivation: 应对新冠疫情中大规模、长篇幅虚假信息传播带来的挑战，现有大语言模型在事实核查的一致性和可解释性方面存在不足，亟需更可靠、可扩展的自动化解决方案。

Method: 提出SAFE系统，采用双代理架构：一个负责从长文本中提取声明，另一个利用LOTR-RAG技术基于大型新冠研究语料库进行声明验证；进一步开发了结合Self-RAG（SRAG）的增强版本以优化检索过程，并通过50篇虚假新闻文章（共246个由公共卫生专家标注的声明）对系统进行定量与主观评估。

Result: SAFE系统在所有指标上均显著优于基线LLM（p < 0.001）。在一致性评分（0-1）上，SAFE (LOTR-RAG) 得分为0.629，高于SAFE (+SRAG) 的0.577和基线的0.279；在主观评估（0-4分）中，其在有用性（3.640）、清晰度（3.800）和真实性（3.526）方面也表现最优；引入SRAG仅在清晰度上有轻微提升，但整体性能下降。

Conclusion: SAFE系统通过结合大语言模型与检索增强生成，有效提升了长篇幅新冠 misinformation 的自动化事实核查的准确性和可靠性，尤其LOTR-RAG架构优于引入SRAG的变体，为可扩展的虚假信息治理提供了坚实基础。

Abstract: The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.

</details>


### [382] [Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System](https://arxiv.org/abs/2512.00313)
*Zhitong Guan,Yi Wang*

Main category: cs.IR

TL;DR: 本研究比较了传统搜索引擎与大语言模型（LLM）驱动的搜索系统在搜索行为和学习效果上的差异，探讨了搜索策略、查询构建及知识理解等方面的变化。


<details>
  <summary>Details</summary>
Motivation: 传统关键词搜索在支持多步推理和探索性学习方面存在局限，而LLM驱动的搜索界面可能改变用户的信息获取方式，但其实际影响尚缺乏实证研究。

Method: 通过对比实验，分析用户在传统搜索引擎与LLM驱动系统中的搜索行为和学习成果，重点关注查询策略、信息导航和知识整合等指标。

Result: 研究发现LLM系统改变了用户的查询构建方式和信息评估行为，并对理解深度、知识整合和批判性思维产生显著影响。

Conclusion: 生成式AI正在重塑信息检索过程，对信息行为和学习效果具有深远影响，为信息检索、人机交互与技术支持学习领域提供了新的研究方向。

Abstract: Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.

</details>


### [383] [Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation](https://arxiv.org/abs/2512.00367)
*Aparajitha Allamraju,Maitreya Prafulla Chitale,Hiranmai Sri Adibhatla,Rahul Mishra,Manish Shrivastava*

Main category: cs.IR

TL;DR: 本文提出了两种高效的语义分块方法（PSC和MFC），在PubMed数据上训练并评估其对检索和生成质量的影响，结果表明PSC显著提升了检索性能，并在跨领域任务中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的固定长度和递归分块方法常破坏语义结构，影响检索增强生成的效果，而语义分块对生成质量的影响尚缺乏深入研究。

Method: 提出Projected Similarity Chunking (PSC) 和 Metric Fusion Chunking (MFC) 两种语义分块方法，在PubMed数据上使用三种嵌入模型进行训练，并构建评估框架，在PubMedQA与全文PMC文章结合的数据上评估分块对检索与生成的影响。

Result: PSC在MRR上实现24倍的提升，Hits@k也显著提高；所提方法在检索和生成任务中均优于传统分块方法，且具备良好的跨领域泛化能力。

Conclusion: PSC和MFC，尤其是PSC，能有效保持语义完整性，显著提升检索效果和生成质量，是一种高效且可泛化的语义分块方案。

Abstract: Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.

</details>


### [384] [PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing](https://arxiv.org/abs/2512.00439)
*Xiaoshan Yu,Ziwei Huang,Shangshang Yang,Ziwen Wang,Haiping Ma,Xingyi Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种新型的单次自适应测验（OAT）任务，并设计了个性化引导的进化式题目组装框架PEOAT，以解决传统计算机化自适应测验（CAT）在实时性、资源约束和心理测量敏感性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 传统CAT在大规模评估中交互成本高，在心理测评等敏感场景中易引入噪声和干扰，难以适用于时间敏感或资源受限环境。

Method: 提出OAT新任务；构建PEOAT框架：包括个性化感知初始化策略、认知增强的进化机制（保持图式交叉与认知引导变异）、多样性感知环境选择机制。

Result: 在两个数据集上进行了大量实验验证，并通过案例研究获得有价值洞见，证明PEOAT的有效性。

Conclusion: PEOAT通过组合优化视角有效实现了高质量、个性化的单次题目选择，提升了自适应测验在现实场景中的可行性与实用性。

Abstract: With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.

</details>


### [385] [DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems](https://arxiv.org/abs/2512.00596)
*Jiahao Tian,Zhenkai Wang*

Main category: cs.IR

TL;DR: 本文提出了一种新框架，通过将多模态与协同知识深度融合进行表征去噪，提升LLM特征在推荐系统中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统难以有效利用大语言模型（LLMs）生成的高维、噪声大的多模态特征；将这些特征作为静态输入使其脱离推荐任务核心。

Method: 提出统一架构：1）将降维直接嵌入推荐模型，实现端到端联合训练，使降维过程感知排序目标；2）引入融合协同过滤信号的对比学习目标，优化潜在空间。

Result: 实验表明该方法具有更强的判别能力，在多个基准上达到SOTA性能。

Conclusion: 多模态与协同知识的深度集成融合与去噪是释放LLM在推荐系统中潜力的关键范式。

Abstract: Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.

</details>


### [386] [ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation](https://arxiv.org/abs/2512.00679)
*Yi Zhang,Yiwen Zhang,Yu Wang,Tong Chen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文提出了一种名为ProEx的统一推荐框架，通过多面用户/物品画像外推与环境不变性学习，提升基于隐式反馈的LLM推荐性能。


<details>
  <summary>Details</summary>
Motivation: 单一用户/物品画像难以全面刻画复杂交互意图，且LLM生成存在不稳定性风险，需更鲁棒、多视角的语义建模方法。

Method: 利用思维链推理生成多个用户/物品画像，并映射为语义向量进行外推；引入‘环境’概念，对各画像线性组合建模，并最小化环境间差异以挖掘用户偏好不变性。

Result: 在三个数据集上，ProEx显著提升了三类判别式与三类生成式基线推荐模型的性能。

Conclusion: 多面画像外推与环境不变性学习是增强LLM驱动推荐鲁棒性与表达能力的有效范式。

Abstract: The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.

</details>


### [387] [SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG](https://arxiv.org/abs/2512.00772)
*Hyunseok Ryu,Wonjune Shin,Hyun Park*

Main category: cs.IR

TL;DR: 本文提出SHRAG框架，通过将自然语言查询转化为结构化布尔查询，并结合多语言扩展与嵌入模型，提升RAG系统的检索精度与跨语言问答能力，兼顾准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统构建门槛高、处理速度慢，且依赖文档中心式检索，难以兼顾精确检索与高效生成。

Method: 利用大语言模型作为查询策略师，将自然语言查询自动转化为逻辑结构化的布尔查询，进行布尔检索；引入多语言查询扩展和多语言嵌入模型，支持跨语言问答。

Result: 实验表明SHRAG显著提升了RAG系统的准确性和可靠性，实现了逻辑检索与生成推理的协同增强。

Conclusion: SHRAG不仅优化了RAG性能，还推动了从文档检索向直接、可靠答案生成的新搜索范式演进。

Abstract: Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.

</details>


### [388] [Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search](https://arxiv.org/abs/2512.00968)
*Ziyang Zeng,Heming Jing,Jindong Chen,Xiangli Li,Hongyu Liu,Yixuan He,Zhengyu Li,Yige Sun,Zheyong Xie,Yuqing Yang,Shaosheng Cao,Jun Fan,Yi Wu,Yao Hu*

Main category: cs.IR

TL;DR: 提出一种基于强化学习的生成式相关性模型训练框架，通过引入业务特定的相关性标准和逐步优势掩码策略，提升小红书搜索中的相关性排序性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统相关性模型难以建模复杂信号且缺乏可解释性，现有基于推理的生成式相关性模型依赖大量标注数据，泛化能力有限，且自由形式的推理不够具体，难以应对开放域搜索中的多样性和歧义性问题。

Method: 将相关性建模视为推理任务，设计融合业务特定标准的多步推理提示，并提出Stepwise Advantage Masking（SAM）作为轻量级过程监督策略，结合强化学习训练生成式相关性模型，并通过模型蒸馏实现工业部署。

Result: 在工业数据集上的实验和在线A/B测试表明，该方法显著提升了相关性排序的效果，增强了模型的接地推理能力。

Conclusion: 所提出的RL-based框架结合SAM策略能有效提升生成式相关性模型的性能与可解释性，适用于实际搜索系统部署。

Abstract: Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.

</details>


### [389] [Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions](https://arxiv.org/abs/2512.01171)
*Tao Xue,Yanwu Yang,Panyu Zhai*

Main category: cs.IR

TL;DR: 本文对在线广告中的转化率（CVR）预测进行了全面综述，将现有模型按技术分为六类，分析其框架、优缺点及应用方式，并总结了在公开与私有数据集上的性能表现，指出了未来研究趋势与挑战。


<details>
  <summary>Details</summary>
Motivation: 过去几十年虽有大量CVR预测模型，但方法演进脉络与技术间关系尚不清晰，亟需系统性梳理。

Method: 采用文献综述法，对CVR预测模型进行分类（共六类），分析各类技术框架、优缺点及其在CVR预测中的应用，并汇总多数据集上的性能结果。

Result: 归纳出六类主流CVR预测模型；发现已有性能评估结果不一致；指出语义增强、归因增强、去偏CVR预测以及CTR与CVR联合建模是未来重要方向。

Conclusion: 该综述为CVR预测领域提供了清晰的技术图谱与未来研究指引，有助于推动理论发展与工业实践。

Abstract: Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.

</details>


### [390] [Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives](https://arxiv.org/abs/2512.01179)
*Shan Gao,Yanwu Yang*

Main category: cs.IR

TL;DR: 本文提出了一个统一的CTR预测基准平台Bench-CTR，包含灵活的数据集与模型组件接口、涵盖真实与合成数据集的综合评估协议，并对从传统统计模型到LLM-based模型进行了系统性评测，揭示了高阶模型优势、LLM的数据高效性及CTR模型性能演进趋势。


<details>
  <summary>Details</summary>
Motivation: 解决CTR预测领域缺乏统一、全面、标准化的基准平台和评估协议的问题，以促进模型开发、公平比较与机理理解。

Method: 设计统一架构的Bench-CTR平台；构建涵盖真实/合成数据集、多维度指标、标准化流程的评估体系；在多个数据集上系统评测从传统模型到LLM-based模型的性能。

Result: （1）高阶模型整体优于低阶模型，但优势因指标和数据集而异；（2）LLM-based模型仅用2%训练数据即可达到其他模型相当性能，展现显著数据效率；（3）CTR模型性能在2015–2016年跃升后进入缓慢增长阶段，该趋势跨数据集一致。

Conclusion: Bench-CTR为CTR预测研究提供了可复现、可扩展、标准化的基准框架，推动模型创新与深入理解，并已开源代码。

Abstract: This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.

</details>


### [391] [Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation](https://arxiv.org/abs/2512.01372)
*Wei Yang,Rui Zhong,Yiqun Chen,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 本文提出结构化谱推理（SSR）框架，通过四阶段频域建模（谱分解、频带掩码调制、超光谱融合、对比对齐）提升多模态推荐的鲁棒性与泛化能力，尤其在稀疏和冷启动场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以应对多模态推荐中的模态噪声、语义不一致及图传播不稳定问题，且频域方法多依赖静态滤波，缺乏对谱结构的推理与模态可靠性自适应能力。

Method: 提出结构化谱推理（SSR）框架：（i）基于图引导的变换将多模态信号分解为不同谱带；（ii）引入谱带掩码机制，结合预测一致性目标抑制脆弱频段；（iii）采用低秩跨带交互的超光谱推理融合多频段信息；（iv）通过对比正则化对齐各模态谱特征，增强语义与结构一致性。

Result: 在三个真实数据集上显著优于强基线，尤其在稀疏和冷启动场景下增益明显；分析表明该方法提升了模型鲁棒性，并可清晰诊断各谱带贡献。

Conclusion: 结构化频域建模是提升多模态推荐鲁棒性与可解释性的有效路径，SSR框架为多模态信号的谱级协同提供了新范式。

Abstract: Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [392] [Toward a Safe Internet of Agents](https://arxiv.org/abs/2512.00520)
*Juan A. Wibowo,George C. Polyzos*

Main category: cs.MA

TL;DR: 本文提出了一种面向自主智能体系统的架构级安全分析框架，强调安全性应内生于系统设计而非事后添加，并从单智能体、多智能体系统到互操作多智能体系统三个层级识别风险与缓解原则。


<details>
  <summary>Details</summary>
Motivation: 现有威胁分类方法不足以应对'智能体互联网'（IoA）带来的新型系统性安全与可靠性风险，亟需基于架构原理的安全工程框架。

Method: 采用自下而上的解构方法，将智能体系统各组件视为双用途接口，在单智能体、多智能体系统（MAS）和互操作多智能体系统（IMAS）三个复杂度层级上分析核心架构组件及其固有安全风险。

Result: 发现智能体安全性本质上是一种架构原则；在每个层级识别出具体架构漏洞，并推导出对应的安全缓解原则。

Conclusion: 本综述为构建安全、可靠、可信的智能体系统提供了基础性指导，是实现安全‘智能体互联网’的关键支撑。

Abstract: Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an "Internet of Agents" (IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results & Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents.

</details>


### [393] [AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation](https://arxiv.org/abs/2512.00602)
*Wanle Zhong,Keman Huang,Xiaoyong Du*

Main category: cs.MA

TL;DR: 本文提出AgentODRL，一种基于多智能体（Orchestrator-Workers）架构的系统，利用大语言模型（LLM）提升自然语言到ODRL策略的自动化与高保真翻译能力，并通过语法验证与语义反思机制增强生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于ODRL授权策略的逻辑复杂性及高质量‘自然语言→ODRL’训练数据稀缺，难以高效准确完成翻译任务。

Method: 构建AgentODRL多智能体系统：包含Orchestrator协调器与Generator（ODRL生成）、Decomposer（用例分解）、Rewriter（逻辑简化）三类Worker；在Generator中引入基于验证器的语法策略和LoRA微调模型驱动的语义反思机制。

Result: 在新构建的770条数据空间场景用例数据集上实验表明，该系统在ODRL语法得分和语义得分上均优于基线方法。

Conclusion: AgentODRL通过模块化多智能体协同与针对性增强策略，显著提升了自然语言到ODRL策略转换的准确性与鲁棒性，为数据权利自动化管理提供了有效技术路径。

Abstract: The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality "Natural Language-to-ODRL" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task.

</details>


### [394] [Hierarchical Decentralized Multi-Agent Coordination with Privacy-Preserving Knowledge Sharing: Extending AgentNet for Scalable Autonomous Systems](https://arxiv.org/abs/2512.00614)
*Goutham Nalagatla*

Main category: cs.MA

TL;DR: AgentNet++ 是一个分层去中心化框架，通过多级组织、隐私保护的知识共享和自适应资源管理，提升了大规模 LLM 代理系统的可扩展性、效率和隐私性。


<details>
  <summary>Details</summary>
Motivation: 解决 AgentNet 在大规模代理系统中存在的可扩展性差、通信开销高、缺乏隐私保障和资源分配不佳等问题。

Method: 提出 AgentNet++，采用基于聚类的分层结构，代理自组织为专业化小组，结合差分隐私与安全聚合实现隐私保护的知识共享，并引入自适应资源管理和理论收敛性分析。

Result: 实验表明，相比 AgentNet 和其他基线，AgentNet++ 提高了 23% 的任务完成率，减少 40% 的通信开销，在超过 1000 个代理的场景下仍能有效扩展，并具备强隐私保障。

Conclusion: AgentNet++ 在保持完全去中心化的同时，显著提升了多代理系统的性能、可扩展性和隐私性，适用于复杂的大规模协作任务。

Abstract: Decentralized multi-agent systems have shown promise in enabling autonomous collaboration among LLM-based agents. While AgentNet demonstrated the feasibility of fully decentralized coordination through dynamic DAG topologies, several limitations remain: scalability challenges with large agent populations, communication overhead, lack of privacy guarantees, and suboptimal resource allocation. We propose AgentNet++, a hierarchical decentralized framework that extends AgentNet with multilevel agent organization, privacy-preserving knowledge sharing via differential privacy and secure aggregation, adaptive resource management, and theoretical convergence guarantees. Our approach introduces cluster-based hierarchies where agents self-organize into specialized groups, enabling efficient task routing and knowledge distillation while maintaining full decentralization. We provide formal analysis of convergence properties and privacy bounds, and demonstrate through extensive experiments on complex multi-agent tasks that AgentNet++ achieves 23% higher task completion rates, 40% reduction in communication overhead, and maintains strong privacy guarantees compared to AgentNet and other baselines. Our framework scales effectively to 1000+ agents while preserving the emergent intelligence properties of the original AgentNet.

</details>


### [395] [Augmented Runtime Collaboration for Self-Organizing Multi-Agent Systems: A Hybrid Bi-Criteria Routing Approach](https://arxiv.org/abs/2512.00740)
*Qingwen Yang,Feiyu Qu,Tiezheng Guo,Yanyi Liu,Yingyou Wen*

Main category: cs.MA

TL;DR: 本文提出了一种名为BiRouter的新型双标准路由方法，用于自组织多智能体系统（SO-MAS），通过仅使用局部信息实现高效的任务路由，显著提升了在去中心化、不可信环境下的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体协作方法依赖静态拓扑和集中式全局规划，难以在开放、去中心化的网络中实现良好的可扩展性和适应性，因此需要一种基于局部信息的高效协作机制。

Method: 提出BiRouter方法，每个智能体基于两个指标自主决策“下一跳”任务路由：ImpScore（评估候选智能体对整体目标的长期重要性）和GapScore（评估其对当前任务状态的上下文连续性），并引入动态更新的信誉机制以增强系统鲁棒性。

Result: 实验表明，BiRouter在性能和token效率上优于现有基线方法，并在信息受限、去中心化和不可信环境中表现出强健性和有效性。

Conclusion: BiRouter通过双标准决策和局部信息驱动的自组织机制，为去中心化多智能体系统的高效协作提供了可扩展、鲁棒的解决方案。

Abstract: LLM-based multi-agent systems have demonstrated significant capabilities across diverse domains. However, the task performance and efficiency are fundamentally constrained by their collaboration strategies. Prevailing approaches rely on static topologies and centralized global planning, a paradigm that limits their scalability and adaptability in open, decentralized networks. Effective collaboration planning in distributed systems using only local information thus remains a formidable challenge. To address this, we propose BiRouter, a novel dual-criteria routing method for Self-Organizing Multi-Agent Systems (SO-MAS). This method enables each agent to autonomously execute ``next-hop'' task routing at runtime, relying solely on local information. Its core decision-making mechanism is predicated on balancing two metrics: (1) the ImpScore, which evaluates a candidate agent's long-term importance to the overall goal, and (2) the GapScore, which assesses its contextual continuity for the current task state. Furthermore, we introduce a dynamically updated reputation mechanism to bolster system robustness in untrustworthy environments and have developed a large-scale, cross-domain dataset, comprising thousands of annotated task-routing paths, to enhance the model's generalization. Extensive experiments demonstrate that BiRouter achieves superior performance and token efficiency over existing baselines, while maintaining strong robustness and effectiveness in information-limited, decentralized, and untrustworthy settings.

</details>


### [396] [Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis](https://arxiv.org/abs/2512.01010)
*Vansh Sharma,Venkat Raman*

Main category: cs.MA

TL;DR: 本文提出Chain of Unit-Physics框架，通过将专家知识编码为单元物理测试，构建多智能体系统以提升科学计算代码生成的可靠性；在燃烧任务基准上验证其有效性，显著优于现有闭源/开源大模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的科学计算代码生成方法在高风险问题中可靠性不足，主因是训练数据中领域代码稀疏、且难以对小规模专家群体实施RLHF。

Method: 提出‘单元物理链’（Chain of Unit-Physics）逆向代码设计框架：以第一性原理（物理原语）为中心，将人类专家知识形式化为可执行的单元物理测试，作为多智能体代码生成过程的显式约束。

Result: 在非平凡燃烧任务上，该框架5–6轮迭代内收敛，结果与专家实现误差仅3.1×10⁻³%，运行速度快约33.4%，内存效率高约30%，成本相当于中型商用API；而闭源/开源基线模型均失败，暴露出四类典型错误。

Conclusion: Chain of Unit-Physics框架不仅提升代码正确性，更将第一性原理分析嵌入生成流程，为物理驱动的科学代码生成提供了实用、可扩展的新范式。

Abstract: Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\times10^{-3}$ %), with a $\sim$33.4 % faster runtime and a $\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.

</details>


### [397] [SocialDriveGen: Generating Diverse Traffic Scenarios with Controllable Social Interactions](https://arxiv.org/abs/2512.01363)
*Jiaguo Tian,Zhengbang Zhu,Shenyu Zhang,Li Xu,Bo Zheng,Xu Liu,Weiji Peng,Shizeng Yao,Weinan Zhang*

Main category: cs.MA

TL;DR: SocialDriveGen 是一种结合语义推理与社会偏好建模的分层框架，通过建模自我主义和利他主义来生成多样化且高保真的交通场景，提升自动驾驶策略的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有交通场景生成方法多基于规则或简化模型，缺乏真实性和多样性，且忽视社会偏好对驾驶行为的影响。

Method: 提出 SocialDriveGen 框架，采用分层结构融合语义推理与社会偏好（如自我主义与利他主义）建模，实现可控制的驾驶个性与交互风格生成。

Result: 在 Argoverse 2 数据集上的实验表明，该方法能生成从合作到对抗的多样化、高保真交通场景，并显著提升策略在罕见或高风险情况下的鲁棒性与泛化能力。

Conclusion: SocialDriveGen 通过引入社会偏好建模，有效提升了仿真交通场景的真实性和多样性，为自动驾驶系统的训练与评估提供了更优的模拟环境。

Abstract: The generation of realistic and diverse traffic scenarios in simulation is essential for developing and evaluating autonomous driving systems. However, most simulation frameworks rely on rule-based or simplified models for scene generation, which lack the fidelity and diversity needed to represent real-world driving. While recent advances in generative modeling produce more realistic and context-aware traffic interactions, they often overlook how social preferences influence driving behavior. SocialDriveGen addresses this gap through a hierarchical framework that integrates semantic reasoning and social preference modeling with generative trajectory synthesis. By modeling egoism and altruism as complementary social dimensions, our framework enables controllable diversity in driver personalities and interaction styles. Experiments on the Argoverse 2 dataset show that SocialDriveGen generates diverse, high-fidelity traffic scenarios spanning cooperative to adversarial behaviors, significantly enhancing policy robustness and generalization to rare or high-risk situations.

</details>


### [398] [Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs](https://arxiv.org/abs/2512.01610)
*Yuren Mao,Peigen Liu,Xinjian Wang,Rui Ding,Jing Miao,Hui Zou,Mingjie Qi,Wanxiang Luo,Longbin Lai,Kai Wang,Zhengping Qian,Peilun Yang,Yunjun Gao,Ying Zhang*

Main category: cs.MA

TL;DR: 提出了一种名为Agent-Kernel的新框架，基于社会中心的模块化微内核架构，解决了现有MAS框架在大规模仿真中的适应性、可配置性、可靠性和代码复用问题，并通过两个应用验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有MAS框架在适应性、可配置性、可靠性及代码复用方面存在不足，难以支持人口和角色动态变化的大规模社会仿真。

Method: 设计并实现Agent-Kernel框架，采用社会中心的模块化微内核架构，将核心系统功能与仿真逻辑解耦，分离认知过程与物理环境及行为执行。

Result: 在Universe 25实验仿真中成功处理了出生到死亡的快速种群动态；在浙江大学校园生活仿真中协调了10,000个异构智能体。

Conclusion: Agent-Kernel框架在大规模、动态社会仿真中展现出卓越的适应性、可配置性、可靠性和可重用性，显著优于现有框架。

Abstract: Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework built upon a novel society-centric modular microkernel architecture. It decouples core system functions from simulation logic and separates cognitive processes from physical environments and action execution. Consequently, Agent-Kernel achieves superior adaptability, configurability, reliability, and reusability. We validate the framework's superiority through two distinct applications: a simulation of the Universe 25 (Mouse Utopia) experiment, which demonstrates the handling of rapid population dynamics from birth to death; and a large-scale simulation of the Zhejiang University Campus Life, successfully coordinating 10,000 heterogeneous agents, including students and faculty.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [399] [Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification](https://arxiv.org/abs/2512.00163)
*Saeed AlMarri,Mathieu Ravaut,Kristof Juhasz,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.LG

TL;DR: 本研究系统评估了大语言模型（LLMs）在金融分类任务中的表现及其可解释性（通过SHAP值），发现LLM的自我解释与实际SHAP归因存在偏差，且与LightGBM差异显著，表明其目前不宜作为高风险金融建模的独立分类器，但结合少样本提示和改进的可解释方法仍有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在高风险结构化金融数据分类任务中的可靠性与可解释性，弥补其在零样本提示下应用于关键领域（如金融风险评估）的可信度空白。

Method: 对多个LLMs在金融分类任务上进行系统性评估，并生成其对应的SHAP值，同时与LightGBM的SHAP结果及LLMs自身生成的特征重要性解释进行对比分析。

Result: 发现LLMs的自我解释与其实际SHAP归因不一致；LLMs与LightGBM的SHAP值存在显著差异；LLMs当前不适合作为高风险金融建模的独立分类器。

Conclusion: LLMs在结构化金融分类中尚存可靠性与可解释性缺陷，但通过少样本提示和增强可解释机制，有望在未来应用于风险敏感场景。

Abstract: Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.

</details>


### [400] [Faster Verified Explanations for Neural Networks](https://arxiv.org/abs/2512.00164)
*Alessandro De Palma,Greta Dolcetti,Caterina Urban*

Main category: cs.LG

TL;DR: 本文提出FaVeX算法和验证器最优鲁棒解释，以提升神经网络验证解释的可扩展性，并在大规模网络上实现有意义的正式解释。


<details>
  <summary>Details</summary>
Motivation: 现有验证解释方法因需多次调用复杂度为指数级的神经网络验证器而面临严重可扩展性挑战。

Method: 提出FaVeX算法，通过动态结合批量与顺序处理输入特征、复用先前查询信息（包括证明某些输入特征不变性及搜索改变预测的特征赋值）来加速计算；同时提出分层的‘验证器最优鲁棒解释’定义，显式纳入验证器不完备性。

Result: 实验表明FaVeX与验证器最优鲁棒解释协同显著提升了可扩展性，能在含数十万非线性激活单元的网络上生成有意义的形式化解释。

Conclusion: FaVeX和验证器最优鲁棒解释共同解决了验证解释的可扩展性瓶颈，为大规模神经网络提供了实用、形式化可保证的解释方法。

Abstract: Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.

</details>


### [401] [We Still Don't Understand High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2512.00170)
*Colin Doumont,Donney Fan,Natalie Maus,Jacob R. Gardner,Henry Moss,Geoff Pleiss*

Main category: cs.LG

TL;DR: 本文发现，在高维空间的贝叶斯优化中，经过几何变换的线性核高斯过程（即贝叶斯线性回归）性能优于现有复杂方法，且计算高效、可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 高维空间下的贝叶斯优化面临‘维度灾难’挑战，现有方法依赖各种结构假设（如稀疏性、平滑性），但效果未必最优；作者试图重新审视简单模型在高维BO中的潜力。

Method: 采用贝叶斯线性回归（即线性核高斯过程），并引入几何变换以避免边界寻求行为；利用其闭式采样与线性计算复杂度优势，应用于高达6000维及超大规模（>20,000样本）分子优化任务。

Result: 该简单方法在60–6000维任务上达到SOTA性能；在线性计算复杂度下成功处理>20,000观测的分子优化；实证表明其显著优于依赖强结构假设的主流高维BO方法。

Conclusion: 高维贝叶斯优化不应盲目追求复杂建模，而应回归模型简洁性与计算可扩展性；线性模型经适当调整后具备强大竞争力，需更新对高维BO方法的传统认知。

Abstract: High-dimensional spaces have challenged Bayesian optimization (BO). Existing methods aim to overcome this so-called curse of dimensionality by carefully encoding structural assumptions, from locality to sparsity to smoothness, into the optimization procedure. Surprisingly, we demonstrate that these approaches are outperformed by arguably the simplest method imaginable: Bayesian linear regression. After applying a geometric transformation to avoid boundary-seeking behavior, Gaussian processes with linear kernels match state-of-the-art performance on tasks with 60- to 6,000-dimensional search spaces. Linear models offer numerous advantages over their non-parametric counterparts: they afford closed-form sampling and their computation scales linearly with data, a fact we exploit on molecular optimization tasks with > 20,000 observations. Coupled with empirical analyses, our results suggest the need to depart from past intuitions about BO methods in high-dimensional spaces.

</details>


### [402] [Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning](https://arxiv.org/abs/2512.00181)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: Orion-Bix 是一种面向表格数据的通用基础模型，采用双轴注意力机制与元学习驱动的上下文学习，支持小样本学习，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 表格数据普遍存在混合类型字段、弱特征结构和标注数据稀缺等问题，导致通用建模困难，亟需具备强泛化与小样本适应能力的基础模型。

Method: 提出 Orion-Bix 模型：编码器融合标准、分组、层次化与关系注意力（通过多 [CLS] 汇总）；标签感知的上下文学习头采用分层决策路由；在具有因果先验的合成多样化表格上进行元训练。

Result: 作为 scikit-learn 兼容模型，Orion-Bix 在公开基准上超越梯度提升基线，并与当前最优表格基础模型性能相当。

Conclusion: 双轴注意力结合阶段性元训练可有效支撑鲁棒、即插即用的小样本表格学习。

Abstract: Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-BiX .

</details>


### [403] [A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations](https://arxiv.org/abs/2512.00249)
*Scotty Black,Christian Darken*

Main category: cs.LG

TL;DR: 本文提出了一种分层混合人工智能方法，结合基于规则的脚本智能体（可靠、可预测）与深度强化学习智能体（自适应、可学习），在作战仿真中实现战术级脚本决策与战略级RL决策的协同，从而提升复杂动态环境下的智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有作战仿真智能体存在两极分化：脚本方法可预测但缺乏灵活性，强化学习方法适应性强但存在黑箱性和可扩展性差的问题；亟需一种兼顾可靠性与适应性的新范式。

Method: 提出分层混合AI架构：底层/战术层采用规则脚本智能体处理常规任务，上层/战略层采用深度强化学习智能体进行动态决策；通过层级协同机制实现二者优势互补。

Result: 该混合方法在复杂仿真环境中显著提升了智能体的整体性能，展现出更强的鲁棒性、适应性和有效性。

Conclusion: 分层混合AI是一种有前景的解决方案，能有效弥合脚本方法与强化学习之间的鸿沟，为未来作战仿真和兵棋推演中的智能体开发提供新思路。

Abstract: In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.

</details>


### [404] [Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation](https://arxiv.org/abs/2512.00191)
*Jose Luis Lima de Jesus Silva,Joao Pedro Gomes,Paulo Roberto de Melo Barros Junior,Vitor Hugo Serravalle Reis Rodrigues,Alexsandro Guerra Cerqueira*

Main category: cs.LG

TL;DR: 提出一种融合空间和几何特征的CFA U-Net模型，结合DBSCAN聚类，显著提升地震层位自动追踪在复杂地质和稀疏标注下的精度与连续性。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net类模型在复杂地质结构和稀疏标注下难以准确分割和插值地震层位，需提高几何保真度和连续性。

Method: 设计Context Fusion Attention (CFA) U-Net，融合空间与Sobel边缘特征于注意力门机制，并结合DBSCAN对Inline与Cross-line预测结果进行聚类优化。

Result: 在Mexilhao场和北海F3区块数据上表现优异，验证集IoU达0.881，MAE为2.49ms，稀疏条件下表面覆盖率高达97.6%。

Conclusion: 融合几何上下文与注意力机制的混合框架能有效提升地震解释在复杂构造和数据稀缺场景下的鲁棒性与泛化能力。

Abstract: Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. However, challenges remain in accurately segmenting complex geological features and interpolating horizons from sparse annotations. To address these issues, a hybrid framework is presented that integrates advanced U-Net variants with spatial clustering to enhance horizon continuity and geometric fidelity. The core contribution is the Context Fusion Attention (CFA) U-Net, a novel architecture that fuses spatial and Sobel-derived geometric features within attention gates to improve both precision and surface completeness. The performance of five architectures, the U-Net (Standard and compressed), U-Net++, Attention U-Net, and CFA U-Net, was systematically evaluated across various data sparsity regimes (10-, 20-, and 40-line spacing). This approach outperformed existing baselines, achieving state-of-the-art results on the Mexilhao field (Santos Basin, Brazil) dataset with a validation IoU of 0.881 and MAE of 2.49ms, and excellent surface coverage of 97.6% on the F3 Block of the North Sea dataset under sparse conditions. The framework further refines merged horizon predictions (inline and cross-line) using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to produce geologically plausible surfaces. The results demonstrate the advantages of hybrid methodologies and attention-based architectures enhanced with geometric context, providing a robust and generalizable solution for seismic interpretation in structurally complex and data-scarce environments.

</details>


### [405] [Emergent Riemannian geometry over learning discrete computations on continuous manifolds](https://arxiv.org/abs/2512.00196)
*Julian Brandon,Angus Chadwick,Arthur Pellegrino*

Main category: cs.LG

TL;DR: 本文提出了一种几何框架，通过分析神经网络中的黎曼拉回度量，揭示了网络如何在连续数据流形上进行离散计算，并区分了离散化和逻辑操作两个功能，同时比较了不同学习模式对泛化能力的影响。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何在连续输入数据上执行离散输出任务的计算机制尚不充分，本文旨在从几何角度揭示这一过程。

Method: 通过分析神经网络各层的黎曼拉回度量，将网络计算分解为对连续特征的离散化和对离散变量的逻辑操作，并比较丰富学习与懒惰学习两种模式下的度量与曲率结构差异。

Result: 发现了神经网络表示几何中离散计算的显著特征，识别出两种不同的计算功能，并揭示了不同学习制度在泛化能力上的几何基础差异。

Conclusion: 该研究提供了一个理解神经网络如何在连续流形上实现离散计算的几何框架，阐明了学习过程中表示变化的本质。

Abstract: Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.

</details>


### [406] [Constructing Efficient Fact-Storing MLPs for Transformers](https://arxiv.org/abs/2512.00207)
*Owen Dugan,Roberto Garcia,Ronny Junkins,Jerry Liu,Dylan Zinsley,Sabri Eyuboglu,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: 本文提出了一种新的MLP构建框架，用于在大语言模型中高效存储事实知识，提升了参数效率、适用性，并揭示了事实存储容量与Transformer可用性之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解大语言模型（LLMs）如何通过MLP参数存储事实知识，并改进现有显式权重构造方法的局限性。

Method: 提出一种新的MLP构造框架，适用于几乎所有可行的输入-输出对，达到渐近最优的参数效率，并保持在Transformer中的可用性；引入值嵌入的度量标准和编码-解码机制以匹配训练模型的事实存储特性。

Result: 1) 发现了刻画事实-每-参数扩展性的嵌入度量；2) 识别出能匹配梯度下降训练MLP性能的简单编码器-解码器机制；3) 揭示了MLP事实存储容量与Transformer可用性之间的根本权衡；4) 实现了在单层Transformer上通过整体替换MLP进行模块化事实编辑的验证概念。

Conclusion: 所提出的MLP构造框架不仅在理论上更优，还能实际应用于模型内部的知识编辑，为理解和操控LLM中的知识存储提供了新工具。

Abstract: The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-storing MLPs, providing an improved understanding of LLM fact storage mechanisms. In this paper, we introduce an MLP construction framework that improves over previous constructions in three areas: it 1) works for all but a measure-zero set of feasible input-output pairs, 2) achieves asymptotically optimal parameter efficiency matching information-theoretic bounds for some embeddings, and 3) maintains usability within Transformers for factual recall. Through our improvements, we 1) discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent-trained MLPs, 2) identify a simple encoder-decoder mechanism that empirically matches gradient-descent MLP facts-per-parameter asymptotics across all the inputs and outputs we test, and 3) uncover a fundamental tradeoff between an MLP's fact-storage capacity and its usability within Transformers. Finally, we demonstrate a proof-of-concept application of fact-storing MLPs: modular fact editing on one-layer Transformers by \textit{replacing entire MLPs at once}.

</details>


### [407] [TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection](https://arxiv.org/abs/2512.00229)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 本文提出TIE框架，通过训练-反演-排除的闭环流程，在标准分类器基础上引入垃圾类，实现可解释的异常检测与不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在面对分布外样本时容易产生过度自信的错误预测，因此需要一种能同时估计预测不确定性和检测分布外样本的统一方法。

Method: TIE框架将n类分类器扩展为(n+1)类模型，新增一个以高斯噪声初始化的垃圾类；每个训练周期内执行训练、反演和排除三步闭环操作，将高不确定性反演样本逐步纳入垃圾类，使反演样本从噪声逐渐演化为可视化类别原型。

Result: TIE在MNIST和FashionMNIST上训练后，对多种未知分布外数据集实现了近乎完美的OOD检测，FPR@95%TPR≈0，并在AUROC、AUPR等指标上表现优异。

Conclusion: TIE提供了一种无需外部OOD数据集、兼具可解释性与高鲁棒性的统一框架，有效解决了分布外检测与不确定性校准问题。

Abstract: Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \textit{uncertainty} and detect \textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \textit{AUROC}, \textit{AUPR}, and \textit{FPR@95\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \textbf{\(\!\approx\!\) 0 FPR@95\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.

</details>


### [408] [Self-Supervised Dynamical System Representations for Physiological Time-Series](https://arxiv.org/abs/2512.00239)
*Yenho Chen,Maxwell A. Xu,James M. Rehg,Christopher J. Rozell*

Main category: cs.LG

TL;DR: 本文提出PULSE框架，通过跨重建预训练目标，从生理时间序列中提取系统级生成变量信息，过滤样本特异性噪声，提升表征学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在生理时间序列上受限于启发式设计或约束不足的生成任务，难以有效保留生理状态信息并去除无关噪声。

Method: 基于动力系统生成模型的信息结构，提出PULSE——一种跨样本重建的预训练目标，聚焦提取跨相似样本共享的系统参数相关变量，丢弃个体特有噪声；并给出系统信息可恢复的理论条件。

Result: 在合成动力系统实验中验证了理论；在多个真实生理数据集上证明PULSE能更好区分语义类别、提升标签效率和迁移性能。

Conclusion: PULSE通过显式建模系统级生成结构，为生理时间序列的自监督表征学习提供了更鲁棒、可解释且实用的新范式。

Abstract: The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.

</details>


### [409] [Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves](https://arxiv.org/abs/2512.00242)
*Alessio Borgi,Fabrizio Silvestri,Pietro Liò*

Main category: cs.LG

TL;DR: 本文提出了PolyNSD，一种基于多项式近似的神经层扩散方法，通过谱重标度和正交多项式基的凸组合实现稳定、高效且可扩展的层传播，显著提升异质图和同质图上的性能，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有神经层扩散方法依赖SVD归一化和稠密边限制映射，存在计算复杂度高、梯度不稳定、需频繁重建拉普拉斯矩阵等问题。

Method: 提出多项式神经层扩散（PolyNSD），使用K次多项式表示传播算子，通过三步递推在谱重标度后的层拉普拉斯上高效计算；采用凸组合方式融合K+1个正交多项式基响应，并引入残差/门控路径增强稳定性。

Result: 在同质与异质图基准测试中达到新SOTA；仅用对角限制映射即获高性能，解耦性能与茎维数，显著降低运行时间和内存消耗。

Conclusion: PolyNSD提供了一种更稳定、高效、可扩展的神经层扩散范式，克服了传统方法的关键缺陷，并为图神经网络在异质场景下的建模提供了新思路。

Abstract: Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.

</details>


### [410] [SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks](https://arxiv.org/abs/2512.00251)
*Henry Onyeka,Emmanuel Samson,Liang Hong,Tariqul Islam,Imtiaz Ahmed,Kamrul Hasan*

Main category: cs.LG

TL;DR: 本文提出了一种基于Sinkhorn散度增强的条件生成对抗网络（SD-CGAN）框架，用于物联网边缘环境中的异常检测，通过合成数据增强和几何感知损失函数提升对复杂DoS攻击和零日漏洞的检测性能。


<details>
  <summary>Details</summary>
Motivation: 物联网边缘网络日益复杂，传统方法难以有效检测动态、不平衡流量下的高级DoS攻击和零日漏洞，亟需提升异常检测的鲁棒性和准确性。

Method: 提出SD-CGAN框架，结合CTGAN进行合成数据增强以缓解类别不平衡，并采用Sinkhorn散度作为几何感知的损失函数，提高训练稳定性并减少模式崩溃。

Result: 在CICDDoS2019数据集的攻击子集上，SD-CGAN在检测准确率、精确率、召回率和F1分数上均优于基线深度学习和GAN模型，同时保持较低计算开销。

Conclusion: SD-CGAN能有效提升物联网边缘环境中复杂攻击的检测能力，具备良好的实用性和部署潜力。

Abstract: The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.

</details>


### [411] [Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.00260)
*Y. Sungtaek Ju*

Main category: cs.LG

TL;DR: 提出了SVGP-KAN，结合稀疏变分推理与Kolmogorov-Arnold网络，实现高效、可扩展的概率建模，并支持结构识别。


<details>
  <summary>Details</summary>
Motivation: 标准KAN缺乏概率输出，限制了其在需要不确定性量化的应用中的使用；现有GP扩展方法计算复杂度高，难以应用于大规模数据。

Method: 将稀疏变分高斯过程（SVGP）引入KAN架构，利用M个诱导点和解析矩匹配，降低计算复杂度至O(NM^2)或线性于样本量。

Result: 实现了对大规模科学数据集的概率KAN建模，显著提升计算效率，并通过排列重要性分析实现输入特征选择和功能关系分类。

Conclusion: SVGP-KAN为KAN提供了可扩展的概率框架，兼具高效性与可解释性，适用于大样本和结构识别任务。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.

</details>


### [412] [Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning](https://arxiv.org/abs/2512.00272)
*Mohammad M Maheri,Xavier Cadet,Peter Chin,Hamed Haddadi*

Main category: cs.LG

TL;DR: 本文提出WARP防御方法，利用神经网络对称性进行参数重映射，以降低遗忘数据的梯度能量并增加参数分散度，从而缓解近似机器遗忘中的隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 近似机器遗忘虽高效，但会引入隐私风险：攻击者可通过比较遗忘前后的模型进行成员推断或数据重建。

Method: 提出WARP——一种即插即用的‘传送’防御机制，利用神经网络对称性对参数进行重映射，在保持预测性能的同时降低遗忘样本梯度范数、增大参数离散度，从而模糊遗忘数据信号。

Result: 在六种主流遗忘算法上验证WARP有效性：黑盒设置下攻击者AUC下降最高达64%，白盒设置下最高达92%，同时保留数据准确率基本不变。

Conclusion: 参数‘传送’是一种通用且有效的手段，可显著降低近似机器遗忘中各类攻击的成功率，为隐私保护提供新思路。

Abstract: Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.

</details>


### [413] [Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking](https://arxiv.org/abs/2512.00724)
*Lingling Fu*

Main category: cs.LG

TL;DR: 提出一种基于循环合并的MoE奖励建模方法，通过将密集模型升级为MoE结构并最终合并回密集模型，有效缓解奖励黑客问题，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型易受奖励黑客攻击，而MoE模型虽提升判别能力但计算开销大，需兼顾鲁棒性与效率。

Method: 首先将密集奖励模型升级为共享专家与专用专家并存的MoE架构，再通过路由权重归一化和可学习权重平均机制将专家合并回密集模型。

Result: 在多种模型规模下均有效缓解了奖励黑客问题，保持性能的同时显著降低推理开销。

Conclusion: 所提方法在提升RLHF奖励模型鲁棒性和推理效率方面具有潜力。

Abstract: Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.

</details>


### [414] [BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models](https://arxiv.org/abs/2512.00283)
*Yi Fang,Haoran Xu,Jiaxin Han,Sirui Ding,Yizhi Wang,Yue Wang,Xuan Wang*

Main category: cs.LG

TL;DR: 本文提出了BioArc框架，利用神经架构搜索（NAS）系统化地探索适用于多种生物数据模态的高效基础模型架构，并提炼出可指导未来模型设计的经验性原则，同时提出架构预测方法以高效适配新任务。


<details>
  <summary>Details</summary>
Motivation: 现有生物学基础模型多直接套用通用AI领域的架构，未考虑生物数据特有的物理化学与结构特性（如长程依赖、稀疏信息和复杂‘语法’），导致性能受限。

Method: 提出BioArc框架，基于神经架构搜索（NAS）在多生物模态上联合优化架构、分词与训练策略；通过大规模实验分析提炼设计原则，并开发多种架构预测方法。

Result: 发现了多个高性能、跨模态适用的新架构，总结出若干经验性设计原则，并验证了所提架构预测方法在效率与有效性上的优势。

Conclusion: BioArc为生物学基础模型提供了首个系统化、原则驱动的架构发现范式，是构建下一代任务特异与通用生物AI模型的重要基础资源与方法论支撑。

Abstract: Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.

</details>


### [415] [Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations](https://arxiv.org/abs/2512.00768)
*Hamed Razavi*

Main category: cs.LG

TL;DR: 本研究利用多种自然语言处理方法分析医疗对话中患者症状描述的模式，基于包含960段多轮对话的医学对话数据集，通过LDA、K-Means、NER和Apriori等方法提取症状主题、聚类相似描述并发现高频症状组合，结果表明对话数据可有效转化为临床有用的诊断信号，有助于提升医疗聊天机器人的决策支持与用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着数字健康系统的快速发展，亟需深入理解其如何解读和表征患者自述症状。医疗聊天机器人虽已用于临床支持，但其对话中蕴含的症状模式尚缺乏系统分析。因此，研究旨在挖掘文本对话中的症状表达规律，以提升自动化症状理解与诊断辅助能力。

Method: 采用多方法融合策略：使用Latent Dirichlet Allocation（LDA）识别潜在症状主题；K-Means对症状描述进行相似性聚类；基于Transformer的命名实体识别（NER）提取医学概念；Apriori算法挖掘频繁共现的症状对。分析基于包含24种临床状况的960段多轮医学对话数据集。

Result: 分析揭示了具有临床相关性的连贯主题结构，症状聚类具中等内聚性，并发现了如‘发烧-头痛’、‘皮疹-瘙痒’等高置信度症状关联组合。模型能有效从非结构化对话中提取结构化症状知识。

Conclusion: 研究表明，患者与医疗聊天机器人之间的对话数据可作为早期症状识别的重要诊断信号，所提出的框架能够将自由文本对话转化为可操作的症状知识，为增强医疗聊天机器人的决策支持能力、提升人机交互体验及未来系统优化提供了可扩展的技术路径。

Abstract: The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.

</details>


### [416] [Data-Driven Modeling and Correction of Vehicle Dynamics](https://arxiv.org/abs/2512.00289)
*Nguyen Ly,Caroline Tatsuoka,Jai Nagaraj,Jacob Levy,Fernando Palafox,David Fridovich-Keil,Hannah Lu*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的非自治车辆动力学建模与校正框架，通过局部参数化控制输入，结合DRIPS（线性代理模型）和FML（深度神经网络方法）两种互补策略，实现高数据效率与强非线性建模能力，并支持基于迁移学习的稀疏测量校正。


<details>
  <summary>Details</summary>
Motivation: 物理模型常因简化而存在模型形式不确定性，且非自治动力学受时变控制输入影响，难以直接从快照数据中学习预测模型。

Method: 将时变输入局部参数化，构建序列化的局部参数动力系统；采用DRIPS方法（降维+参数空间插值）构建高效线性代理模型；采用FML（流映射学习）深度神经网络方法建模强非线性演化；并扩展FML引入基于迁移学习的模型校正机制。

Result: 在单轮车、简化自行车及滑移自行车模型上验证：DRIPS具备鲁棒性和极高数据效率；FML在严重数据稀缺下仍能有效建模强非线性并校正模型误差。

Conclusion: 所提框架兼顾线性可表征系统的高效性与强非线性系统的表达力，为非自治车辆动力学的数据驱动建模与校正提供了统一、实用且可扩展的解决方案。

Abstract: We develop a data-driven framework for learning and correcting non-autonomous vehicle dynamics. Physics-based vehicle models are often simplified for tractability and therefore exhibit inherent model-form uncertainty, motivating the need for data-driven correction. Moreover, non-autonomous dynamics are governed by time-dependent control inputs, which pose challenges in learning predictive models directly from temporal snapshot data. To address these, we reformulate the vehicle dynamics via a local parameterization of the time-dependent inputs, yielding a modified system composed of a sequence of local parametric dynamical systems. We approximate these parametric systems using two complementary approaches. First, we employ the DRIPS (dimension reduction and interpolation in parameter space) methodology to construct efficient linear surrogate models, equipped with lifted observable spaces and manifold-based operator interpolation. This enables data-efficient learning of vehicle models whose dynamics admit accurate linear representations in the lifted spaces. Second, for more strongly nonlinear systems, we employ FML (Flow Map Learning), a deep neural network approach that approximates the parametric evolution map without requiring special treatment of nonlinearities. We further extend FML with a transfer-learning-based model correction procedure, enabling the correction of misspecified prior models using only a sparse set of high-fidelity or experimental measurements, without assuming a prescribed form for the correction term. Through a suite of numerical experiments on unicycle, simplified bicycle, and slip-based bicycle models, we demonstrate that DRIPS offers robust and highly data-efficient learning of non-autonomous vehicle dynamics, while FML provides expressive nonlinear modeling and effective correction of model-form errors under severe data scarcity.

</details>


### [417] [FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting](https://arxiv.org/abs/2512.00293)
*Yafei Lyu,Hao Zhou,Lu Zhang,Xu Yang,Zhiyong Liu*

Main category: cs.LG

TL;DR: 本文提出FiCoTS框架，采用LLM-as-Enhancer范式，通过细到粗的三级跨模态交互（token级对齐、feature级交互、decision级融合），增强时间序列预测中文本信息的辅助作用，显著提升多模态时序预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-Predictor方法难以让大语言模型充分理解语义差异大的时间序列数据，需探索更适配的LLM利用范式。

Method: 提出FiCoTS框架：1）token级动态异构图对齐时间序列块与文本token；2）feature级全局交叉注意力连接变量与文本上下文；3）decision级门控网络自适应融合双模态结果。

Result: 在七个真实世界基准上达到SOTA性能。

Conclusion: LLM-as-Enhancer范式更有效发挥LLM在文本理解上的优势，结合细到粗的跨模态交互设计，可显著提升多模态时间序列预测效果。

Abstract: Time series forecasting is central to data analysis and web technologies. The recent success of Large Language Models (LLMs) offers significant potential for this field, especially from the cross-modality aspect. Most methods adopt an LLM-as-Predictor paradigm, using LLM as the forecasting backbone and designing modality alignment mechanisms to enable LLM to understand time series data. However, the semantic information in the two modalities of time series and text differs significantly, making it challenging for LLM to fully understand time series data. To mitigate this challenge, our work follows an LLM-as-Enhancer paradigm to fully utilize the advantage of LLM in text understanding, where LLM is only used to encode text modality to complement time series modality. Based on this paradigm, we propose FiCoTS, an LLM-enhanced fine-to-coarse framework for multimodal time series forecasting. Specifically, the framework facilitates progressive cross-modality interaction by three levels in a fine-to-coarse scheme: First, in the token-level modality alignment module, a dynamic heterogeneous graph is constructed to filter noise and align time series patches with text tokens; Second, in the feature-level modality interaction module, a global cross-attention mechanism is introduced to enable each time series variable to connect with relevant textual contexts; Third, in the decision-level modality fusion module, we design a gated network to adaptively fuse the results of the two modalities for robust predictions. These three modules work synergistically to let the two modalities interact comprehensively across three semantic levels, enabling textual information to effectively support temporal prediction. Extensive experiments on seven real-world benchmarks demonstrate that our model achieves state-of-the-art performance. The codes will be released publicly.

</details>


### [418] [Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains](https://arxiv.org/abs/2512.00298)
*González Trigueros Jesús Eduardo,Alonso Sánchez Alejandro,Muñoz Rivera Emilio,Peñarán Prieto Mariana Jaqueline,Mendoza González Camila Natalia*

Main category: cs.LG

TL;DR: 本文研究了大数据异构性对分类策略的影响，发现高维数值数据中优化的线性模型优于深度模型，而文本数据中通过Transformer嵌入和贝叶斯目标编码等特征工程，简单模型反而泛化更好，提出了一种基于数据特性和基础设施约束的统一算法选择框架。


<details>
  <summary>Details</summary>
Motivation: 探究大数据异构性（“多样性”）对不同数据类型（结构化与非结构化）下机器学习模型性能的影响，并解决实际部署中因基础设施限制（如分布式微调）导致的模型失效问题。

Method: 采用双路径方法：对数值型结构化数据（Epsilon），使用遗传算法和Optuna进行进化式与贝叶斯超参优化；对大规模文本数据（Rest-Mex、IMDB），基于Apache Spark实现分布式处理，并结合ROBERTa嵌入与贝叶斯目标编码进行特征工程。

Result: 发现‘复杂性悖论’：在高维数值空间中，优化后的线性模型（SVM、逻辑回归）性能超越深度网络和梯度提升树；在文本领域，复杂模型易因分布式微调受限而过拟合，而经鲁棒特征工程增强的简单模型表现出更优泛化能力。

Conclusion: 模型选择不应仅依赖理论复杂度，而应协同考虑数据本质（结构化/非结构化）、维度特性及计算基础设施约束；本文提出的框架可指导实践中高效、稳健的算法选型。

Abstract: This study analyzes the impact of heterogeneity ("Variety") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a "complexity paradox": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.

</details>


### [419] [Gradient Inversion in Federated Reinforcement Learning](https://arxiv.org/abs/2512.00303)
*Shenghong He*

Main category: cs.LG

TL;DR: 提出了一种名为正则化梯度反演攻击（RGIA）的新方法，通过引入基于先验知识的正则化来约束状态、奖励和转移动态，有效提升了联邦强化学习中的数据重构攻击效果。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习中共享梯度可能导致隐私泄露，攻击者可利用梯度重构本地训练数据；但现有方法难以同时满足梯度匹配与环境真实转移动态的一致性。

Method: 提出RGIA攻击方法，在优化过程中引入基于先验知识的状态、奖励和转移动态正则化项，以约束重构数据符合真实转移分布，并理论证明该正则化能缩小解空间至满足两者一致的有效子集。

Result: 在控制任务和自动驾驶任务上的实验表明，RGIA能有效约束重构数据的转移分布，成功重建本地私有数据。

Conclusion: RGIA显著提高了在联邦强化学习场景下从共享梯度中恢复私有转移数据的能力，揭示了FRL系统更深层次的隐私风险，强调了加强防御机制的重要性。

Abstract: Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient sharing.However, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training data.Compared to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition distribution.Theoretically, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition dynamics.Extensive experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.

</details>


### [420] [Adversarial Signed Graph Learning with Differential Privacy](https://arxiv.org/abs/2512.00307)
*Haobin Ke,Sen Zhang,Qingqing Ye,Xun Ran,Haibo Hu*

Main category: cs.LG

TL;DR: 本文提出ASGL，一种基于对抗学习的隐私保护方法，用于在满足节点级差分隐私的同时，有效保留符号图中的结构和符号信息。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法在符号图上存在级联误差和梯度敏感性高的问题，难以兼顾隐私与效用。

Method: 将符号图按正负边分解为子图，设计梯度扰动的对抗模块逼近真实连接分布，并结合约束广度优先搜索树策略融合平衡理论推断边符号，实现梯度解耦以降低敏感性。

Result: 在多个真实数据集上，ASGL在节点分类、链接预测等任务中优于现有隐私保护方法，实现了更好的隐私-效用权衡。

Conclusion: ASGL通过子图分离与对抗学习框架，有效解决了符号图学习中的隐私泄露问题，同时保持了高质量的节点表示。

Abstract: Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.

</details>


### [421] [Tracing Mathematical Proficiency Through Problem-Solving Processes](https://arxiv.org/abs/2512.00311)
*Jungyang Park,Suho Kang,Jaewoo Park,Jaehong Kim,Jaewoo Shin,Seonjoon Park,Youngjae Yu*

Main category: cs.LG

TL;DR: 本文提出KT-PSP方法及KT-PSP-25数据集，利用学生解题过程建模多维数学能力，并设计StatusKT框架（基于教师-学生-教师三阶段大模型流水线）提升知识追踪的预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪方法仅依赖答题对错，缺乏对学生解题过程所蕴含的丰富能力信息的利用，导致可解释性差。

Method: 提出KT-PSP方法和KT-PSP-25新数据集；构建StatusKT框架，采用教师-学生-教师三阶段大语言模型流水线：教师LLM提取题目相关能力指标，学生LLM基于学生解题过程生成响应，教师LLM评估响应以判定各指标掌握程度。

Result: 在KT-PSP-25数据集上，StatusKT优于现有KT方法，且能通过显式建模数学能力提供可解释的预测结果。

Conclusion: 融合解题过程与多维能力建模的知识追踪方法，可同时提升预测性能与可解释性，为个性化学习提供更可靠支持。

Abstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.

</details>


### [422] [Introducing AI-Driven IoT Energy Management Framework](https://arxiv.org/abs/2512.00321)
*Shivani Mruthyunjaya,Anandi Dutta,Kazi Sifatul Islam*

Main category: cs.LG

TL;DR: 本文提出了一种面向物联网（IoT）系统的整体性框架，旨在通过上下文感知决策、主动适应与可扩展结构来降低功耗并提升电网稳定性，结合长短期负荷预测、异常检测及定性数据分析，并在电力消耗时间序列数据上验证了其可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现代生活对技术依赖加剧导致功耗问题日益突出，亟需通过精准预测与智能管理降低电费支出并增强电网可靠性。

Method: 构建一个面向IoT系统的整体框架，涵盖长期预测、短期预测、异常检测及融合定性数据的能源管理决策机制，并基于电力消耗时间序列数据进行性能评估。

Result: 该框架在电力时间序列数据上成功验证了其可行性，展现出支持节能和电网稳定性的直接应用潜力。

Conclusion: 所提出的框架为IoT系统提供了可扩展、自适应且上下文感知的能源管理基础，具备实际部署价值。

Abstract: Power consumption has become a critical aspect of modern life due to the consistent reliance on technological advancements. Reducing power consumption or following power usage predictions can lead to lower monthly costs and improved electrical reliability. The proposal of a holistic framework to establish a foundation for IoT systems with a focus on contextual decision making, proactive adaptation, and scalable structure. A structured process for IoT systems with accuracy and interconnected development would support reducing power consumption and support grid stability. This study presents the feasibility of this proposal through the application of each aspect of the framework. This system would have long term forecasting, short term forecasting, anomaly detection, and consideration of qualitative data with any energy management decisions taken. Performance was evaluated on Power Consumption Time Series data to display the direct application of the framework.

</details>


### [423] [Adaptive prediction theory combining offline and online learning](https://arxiv.org/abs/2512.00342)
*Haizheng Li,Lei Guo*

Main category: cs.LG

TL;DR: 本文提出了一种结合离线学习与在线适应的两阶段学习框架，用于非线性随机动力系统预测；理论分析了强相关、分布偏移数据下的离线泛化误差上界，并设计了基于元LMS的在线自适应算法以应对参数漂移；理论与实验均验证其优于纯离线或纯在线方法。


<details>
  <summary>Details</summary>
Motivation: 现实智能系统常需结合离线学习与在线适应来处理高度相关且非平稳的数据/信号，但该场景缺乏系统的理论分析。

Method: 离线阶段：采用近似非线性最小二乘估计，利用KL散度刻画分布差异，建立泛化误差上界；在线阶段：基于离线模型，提出元LMS预测算法以适应参数漂移。

Result: 所提两阶段框架在预测性能上优于纯离线或纯在线方法；提供了理论保证与实证支持。

Conclusion: 该工作首次对含分布偏移与强相关性的非线性随机系统中离线-在线联合学习进行了理论建模与分析，验证了其有效性与优越性。

Abstract: Real-world intelligence systems usually operate by combining offline learning and online adaptation with highly correlated and non-stationary system data or signals, which, however, has rarely been investigated theoretically in the literature. This paper initiates a theoretical investigation on the prediction performance of a two-stage learning framework combining offline and online algorithms for a class of nonlinear stochastic dynamical systems. For the offline-learning phase, we establish an upper bound on the generalization error for approximate nonlinear-least-squares estimation under general datasets with strong correlation and distribution shift, leveraging the Kullback-Leibler divergence to quantify the distributional discrepancies. For the online-adaptation phase, we address, on the basis of the offline-trained model, the possible uncertain parameter drift in real-world target systems by proposing a meta-LMS prediction algorithm. This two-stage framework, integrating offline learning with online adaptation, demonstrates superior prediction performances compared with either purely offline or online methods. Both theoretical guarantees and empirical studies are provided.

</details>


### [424] [Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning](https://arxiv.org/abs/2512.00351)
*Na Li,Yuchen Jiao,Hangguan Shan,Shefeng Yan*

Main category: cs.LG

TL;DR: 本文提出了一种内存高效的自博弈算法ME-Nash-QL，用于两人零和马尔可夫博弈，显著改善了空间复杂度、样本复杂度、计算复杂度和预热成本。


<details>
  <summary>Details</summary>
Motivation: 现有MARL理论研究面临内存效率低、样本复杂度随长视界和大状态空间急剧上升、计算复杂度高、非马尔可夫策略、非纳什策略及高预热成本等问题。

Method: 设计了一种无模型自博弈算法Memory-Efficient Nash Q-Learning (ME-Nash-QL)，适用于两人零和马尔可夫博弈，并从理论层面分析其空间、样本、计算与预热成本。

Result: ME-Nash-QL具有O(SABH)空间复杂度、Õ(H⁴SAB/ε²)样本复杂度、O(T poly(AB))计算复杂度和O(SAB poly(H))预热成本，在多项指标上优于现有算法。

Conclusion: ME-Nash-QL在保持马尔可夫策略的同时，显著降低了内存占用、样本需求、计算开销和预热成本，为MARL理论研究提供了更实用的算法框架。

Abstract: The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\min\{A, B\}\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.

</details>


### [425] [Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning](https://arxiv.org/abs/2512.00352)
*Na Li,Zewu Zheng,Wei Ni,Hangguan Shan,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 本文提出了一种用于离线鲁棒双人零和马尔可夫博弈（RTZMGs）的模型基础算法RTZ-VI-LCB，结合乐观鲁棒值迭代与数据驱动的Bernstein型惩罚项，在部分覆盖和环境不确定性下实现了近最优样本复杂度，并通过信息论下界验证了其紧性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中因环境不确定性导致的策略鲁棒性问题，特别是离线设置下鲁棒双人零和马尔可夫博弈的样本效率与理论保证问题。

Method: 提出模型基础算法RTZ-VI-LCB，融合乐观鲁棒值迭代与基于历史数据的Bernstein风格惩罚项，以应对分布偏移并提升鲁棒价值估计。

Result: 在部分覆盖条件下取得近最优样本复杂度，且信息论下界证明其关于状态和动作空间均达到最优；实验验证了算法有效性。

Conclusion: RTZ-VI-LCB是首个在离线RTZMGs中实现该最优性保证的算法，为该领域设定了新基准。

Abstract: Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.

</details>


### [426] [Learning Causal States Under Partial Observability and Perturbation](https://arxiv.org/abs/2512.00357)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li,Yamin Wang*

Main category: cs.LG

TL;DR: 本文提出CaDiff框架，利用异步扩散模型和新双模拟度量，在扰动和部分可观测马尔可夫决策过程（P²OMDPs）中提取因果状态表示，提升强化学习鲁棒性与性能，并提供理论误差界保证。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时应对观测中的扰动和部分可观测性问题，缺乏对底层因果结构的建模能力。

Method: 提出CaDiff框架，包含异步扩散模型（ADM）用于将扰动建模为可抑制噪声，以及新定义的双模拟度量来衡量部分可观测环境与其因果对应状态的相似性；并推导值函数近似误差的理论上界。

Result: 在Roboschool任务上，CaDiff相比基线方法提升回报至少14.18%；首次实现了兼具理论严谨性与实用性的基于扩散模型的因果状态近似。

Conclusion: CaDiff通过结合扩散建模与因果表征学习，有效缓解P²OMDP中的扰动与部分可观测性挑战，为鲁棒强化学习提供了新范式。

Abstract: A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.

</details>


### [427] [S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting](https://arxiv.org/abs/2512.00366)
*Wenshuo Wang,Yaomin Shen,Yingjie Tan,Yihao Chen*

Main category: cs.LG

TL;DR: 本文提出S^2-KD框架，通过融合语义先验与频谱表征进行知识蒸馏，利用多模态教师模型（结合大语言模型文本推理与频谱解耦）指导轻量级视觉学生模型，提升其在时空预测任务（如天气、交通）中长时程与非平稳场景下的性能，且推理时不需文本输入。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素级信号的频谱感知知识蒸馏方法缺乏对视觉模式背后语义和因果关系的建模能力，限制了学生模型的泛化与可解释性。

Method: 提出S^2-KD框架：1）构建具备文本推理能力与潜空间频谱解耦能力的多模态教师模型；2）设计新型蒸馏目标，将语义-频谱联合知识迁移至轻量纯视觉学生模型。

Result: 在WeatherBench和TaxiBJ+等基准上，S^2-KD显著提升简单学生模型性能，尤其在长时程预测和复杂非平稳场景下超越当前最优方法。

Conclusion: 统一语义先验与频谱表征的知识蒸馏范式能有效增强轻量模型的预测准确性与语义一致性，为高效时空预测提供新思路。

Abstract: Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.

</details>


### [428] [An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines](https://arxiv.org/abs/2512.00383)
*Jianhai Su,Jinzhu Luo,Qi Zhang*

Main category: cs.LG

TL;DR: 本文提出将离线强化学习（RL）算法作为从零开始的在线RL的子程序，利用历史交互数据构建离线数据集，提升在线学习效率，并通过实验验证其有效性及改进技术。


<details>
  <summary>Details</summary>
Motivation: 现有在线RL在样本效率方面存在瓶颈，而历史交互数据未被充分利用；本文旨在探索如何将离线RL作为子程序嵌入在线RL流程中，以提升学习效率。

Method: 提出一个通用框架，支持多种离线RL嵌入方式（如最终策略推荐、在线微调等），并设计若干实用技术以增强其效果；在多个任务上进行系统性实证分析。

Result: 实验表明：1）该框架效果高度依赖任务特性；2）所提改进技术显著提升性能；3）现有在线微调方法整体无效。

Conclusion: 将离线RL作为在线RL子程序是可行且有潜力的方向，但需针对任务特性设计适配机制，尤其在线微调方法亟待深入研究。

Abstract: We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.

</details>


### [429] [Efficient and Programmable Exploration of Synthesizable Chemical Space](https://arxiv.org/abs/2512.00384)
*Shitong Luo,Connor W. Coley*

Main category: cs.LG

TL;DR: PrexSyn 是一个基于解码器-only Transformer 的高效、可编程分子发现模型，通过十亿级可合成路径与分子性质配对数据训练，能快速生成满足单属性或复合逻辑属性条件的可合成分子，并支持基于黑盒函数的迭代优化。


<details>
  <summary>Details</summary>
Motivation: 现有可合成化学空间受限，难以同时兼顾分子的可合成性与目标性质，亟需一种能高效探索并编程化生成满足复杂性质约束的可合成分子的新方法。

Method: 提出 PrexSyn 模型：基于 decoder-only transformer 架构，使用自研 C++ 高通量实时数据引擎生成的十亿级可合成路径-性质配对数据进行大规模训练；支持基于性质的逻辑查询与迭代式黑盒优化。

Result: PrexSyn 在可合成化学空间覆盖率、分子采样效率和推理速度上均达到新SOTA；能高效生成满足单属性及复合逻辑属性（如 AND/OR）的可合成分子，并在黑盒优化任务中超越无合成约束基线。

Conclusion: PrexSyn 推动了可合成分子设计的前沿，成为一种通用、高效、可编程的分子优化工具。

Abstract: The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.

</details>


### [430] [Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics](https://arxiv.org/abs/2512.00389)
*Deep Patel,Emmanouil-Vasileios Vlatakis-Gkaragkounis*

Main category: cs.LG

TL;DR: 本文提出了一种理论框架，通过隐藏凸性和过参数化解释了非凸非凹零和博弈中梯度法为何能收敛到冯·诺依曼-纳什均衡（NE），并首次为含两层神经网络的此类博弈给出了全局收敛保证。


<details>
  <summary>Details</summary>
Motivation: 许多新兴应用（如对抗训练、AI对齐、鲁棒优化）可建模为神经网络间的零和博弈，其理想行为由冯·诺依曼-纳什均衡刻画；但现有理论难以解释实践中简单梯度法常能收敛的现象，暗示存在潜在几何结构。

Method: 提出基于隐藏凸性与过参数化的理论框架；推导交替梯度下降-上升算法的路径长度新界；结合随机矩阵理论，证明在过参数化条件下，隐式凸-凹几何可高概率约化为双向Polyak-Łojasiewicz（PŁ）min-max条件。

Result: 给出了确保非凸min-max博弈全局收敛至NE的充分条件（涵盖初始化、训练动态与网络宽度）；首次实现对含两层神经网络的零和博弈的全局收敛性理论保证。

Conclusion: 非凸非凹零和博弈中的经验收敛性可由隐藏凸性与过参数化共同解释；所提理论框架为理解神经网络博弈的优化动力学提供了新视角，并为设计更可靠的对抗与对齐算法奠定基础。

Abstract: Many emerging applications - such as adversarial training, AI alignment, and robust optimization - can be framed as zero-sum games between neural nets, with von Neumann-Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of hidden convexity and overparameterization. We identify sufficient conditions - spanning initialization, training dynamics, and network width - that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for the alternating gradient descent-ascent scheme in min-max games; and (b) we show that the reduction from a hidden convex-concave geometry to two-sided Polyak-Łojasiewicz (PŁ) min-max condition hold with high probability under overparameterization, using tools from random matrix theory.

</details>


### [431] [From Coefficients to Directions: Rethinking Model Merging with Directional Alignment](https://arxiv.org/abs/2512.00391)
*Zhikang Chen,Sen Cui,Deheng Ye,Min Zhang,Gang Niu,Yu Zhang,Masashi Sugiyama,Tingting Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为'Merging with Directional Alignment'（MDA）的统一几何框架，强调在参数空间和特征空间中进行方向对齐，以解决模型融合中因方向不一致导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法忽视了参数空间和特征空间中的方向信息，导致融合后模型结构不一致、性能下降；同时，系数优化方法隐含假设不同模型的特征方向兼容，但神经坍缩现象表明该假设不成立。

Method: 提出'方向对齐'概念，并构建统一几何框架MDA，在参数空间和特征空间同步进行方向对齐，提升融合模型的结构一致性。

Result: 理论分析表明方向对齐能增强结构一致性；大量实验在多个基准、不同模型规模和任务配置下验证了MDA的有效性。

Conclusion: 方向对齐是提升模型融合性能的关键因素，MDA框架为模型融合提供了新的几何视角与实用解决方案。

Abstract: Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, naïve merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \emph{directional alignment} and introduce a unified geometric framework, \emph{Merging with Directional Alignment} (\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.

</details>


### [432] [Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement](https://arxiv.org/abs/2512.00396)
*Andrea Procopio,Marco Esposito,Sara Raggiunto,Andrey Gizdov,Alberto Belli,Paola Pierleoni*

Main category: cs.LG

TL;DR: 本文研究了基于可穿戴设备的轻量级1D CNN模型用于帕金森病（PD）步态检测，使用短时三轴加速度数据，在资源受限的边缘设备上实现高效准确的分析。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的可穿戴设备和边缘节点，需要在短时间窗口内高效准确地检测帕金森病患者的步态，传统阈值方法存在精度低、假阳性多的问题。

Method: 比较了幅值阈值法与三种1D CNN模型（包括文献基准和两个超轻量模型，其中一个含残差连接），使用BioStampRC21数据集，2秒窗口、30Hz采样率，采用留一受试者交叉验证（LOSO）在16名PD患者胸部IMU数据上进行评估。

Result: 含残差连接的可分离模型（Model 2，533参数）达到PR-AUC=94.5%，F1=91.2%，MCC=89.4%，性能优于或匹配参数更多的基线模型（5,552参数），且模型更小；最小模型（Model 1，305参数）也表现优异；阈值法召回率高但精度低，假阳性多；胸部和大腿传感器位置最可靠；所有模型可在STM32类MCU上以亚10毫秒延迟运行。

Conclusion: 超轻量可分离CNN在准确性、效率和泛化能力方面优于固定阈值法，适用于边缘部署的可穿戴PD步态检测，凸显了为边缘设备定制时间序列模型的重要性。

Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.

</details>


### [433] [SelfAI: Building a Self-Training AI System with LLM Agents](https://arxiv.org/abs/2512.00403)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Xiaobing Yu,Yu Zhong,Shangqi Deng,Ufaq Khan,Jianghao Wu,Xiaofeng Liu,Imran Razzak,Xiaojun Chang,Yutong Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为SelfAI的通用多智能体平台，用于提升自主科学发现的效率和人机协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自主科学发现系统通常局限于特定领域，缺乏实时交互机制和合理的探索终止策略，导致效率低下、可重复性差以及人类专家参与不足。

Method: 设计了一个包含用户代理（User Agent）、认知代理（Cognitive Agent）和实验管理器（Experiment Manager）的多智能体架构。其中认知代理结合LLM与最优停止准则以优化超参数搜索，实验管理器支持并行容错训练，并维护结构化知识库实现持续反馈。同时提出了两个新评估指标Score和AUP_D来衡量发现效率与搜索多样性。

Result: 在回归、自然语言处理、计算机视觉、科学计算、医学影像和药物发现等多个基准任务上，SelfAI相比经典贝叶斯优化和基于LLM的基线方法，在性能相当或更优的同时显著减少了冗余试验次数，并实现了与研究人员的无缝交互。

Conclusion: SelfAI是一个高效、通用且支持人机协同的自主科学发现框架，通过多智能体协同、最优停止机制和结构化知识管理，提升了探索效率、可复现性和人类专家的参与度。

Abstract: Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.

</details>


### [434] [TrendGNN: Towards Understanding of Epidemics, Beliefs, and Behaviors](https://arxiv.org/abs/2512.00421)
*Mulin Tian,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络（GNN）的可解释性预测框架，用于建模疫情相关行为与信念信号间的复杂关联，通过构建趋势相似性图实现可解释预测。


<details>
  <summary>Details</summary>
Motivation: 现有流行病预测方法多为黑箱模型或简单机理模型，缺乏对行为与信念等关键信号的可解释性建模能力，难以支持干预效果评估。

Method: 构建基于趋势相似性的多信号关系图，并利用图神经网络（GNN）进行联合预测，从而揭示信号可预测性及关键关系贡献。

Result: 该框架实现了对行为与信念相关信号的可解释预测，识别出高预测性信号及对准确率贡献最大的关系。

Conclusion: 所提方法是迈向多信号可解释建模的重要一步，有助于构建融合行为、信念与观测数据的下一代仿真模型。

Abstract: Epidemic outcomes have a complex interplay with human behavior and beliefs. Most of the forecasting literature has focused on the task of predicting epidemic signals using simple mechanistic models or black-box models, such as deep transformers, that ingest all available signals without offering interpretability. However, to better understand the mechanisms and predict the impact of interventions, we need the ability to forecast signals associated with beliefs and behaviors in an interpretable manner. In this work, we propose a graph-based forecasting framework that first constructs a graph of interrelated signals based on trend similarity, and then applies graph neural networks (GNNs) for prediction. This approach enables interpretable analysis by revealing which signals are more predictable and which relationships contribute most to forecasting accuracy. We believe our method provides early steps towards a framework for interpretable modeling in domains with multiple potentially interdependent signals, with implications for building future simulation models that integrate behavior, beliefs, and observations.

</details>


### [435] [Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease](https://arxiv.org/abs/2512.00434)
*Benjamin D. Ballyk,Ankit Gupta,Sujay Konda,Kavitha Subramanian,Chris Landon,Ahmed Ammar Naseer,Georg Maierhofer,Sumanth Swaminathan,Vasudevan Venkateshwaran*

Main category: cs.LG

TL;DR: 本文提出了一种增强的时间序列生成模型（Augmented TimeGAN 和 DP-TimeGAN），用于生成符合隐私保护要求的纵向临床数据，在保持高统计真实性与临床实用性的同时，提供可量化的差分隐私保障。


<details>
  <summary>Details</summary>
Motivation: 电子病历广泛应用带来数据隐私挑战，严格的数据保护法规限制了机器学习模型对真实临床数据的使用；现有合成数据方法在时间序列建模或隐私保证方面存在不足。

Method: 改进当前领先的时间序列生成模型TimeGAN，引入差分隐私机制，构建非私有模型Augmented TimeGAN和私有模型DP-TimeGAN，并在慢性肾病（CKD）和ICU真实数据上进行评估。

Result: Augmented TimeGAN在多项统计指标上优于基于Transformer和流模型的方法；DP-TimeGAN在CKD数据集上平均真实性达0.778，在隐私-效用权衡上超越现有最先进模型；两类模型在临床专家评估中表现接近真实数据。

Conclusion: 所提方法能有效生成高质量、高隐私保障的纵向临床合成数据，为复杂慢性病建模提供了安全可靠的数据基础。

Abstract: Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.

</details>


### [436] [FairMT: Fairness for Heterogeneous Multi-Task Learning](https://arxiv.org/abs/2512.00469)
*Guanyu Hu,Tangzheng Lian,Na Yan,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan,Siyang Song,Zeyu Fu*

Main category: cs.LG

TL;DR: 本文提出了FairMT，一个统一的公平多任务学习框架，能够处理分类、检测和回归等异构任务，并在标签不完整的情况下实现公平性与任务性能的双赢。


<details>
  <summary>Details</summary>
Motivation: 现有的公平机器学习方法主要集中于单任务场景，且多针对分类任务，难以扩展到连续输出或多任务设置；同时，现有MTL结构可能导致任务头吸收偏差，造成不公平。此外，公平性常被视为与性能权衡的零和目标，限制了实际应用。因此，需要一种能统一处理异构任务、部分标签缺失并避免对优势群体性能过度牺牲的公平MTL框架。

Method: 提出FairMT框架，核心是“非对称异构公平约束聚合”机制，将不同任务类型的非对称公平违规统一为单一约束；通过原始-对偶优化联合优化公平性与效用，并设计头感知的多目标优化代理，以应对任务头带来的各向异性下降方向。

Result: 在三个同构与异构MTL基准上验证，FairMT在多种模态和监督条件下均显著提升了公平性，同时保持甚至优于现有方法的任务性能。

Conclusion: FairMT为异构多任务学习中的公平性问题提供了有效解决方案，突破了传统方法在任务类型、标签完整性及公平-效用权衡方面的局限，推动了公平ML在复杂现实场景中的应用。

Abstract: Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.

</details>


### [437] [ESPO: Entropy Importance Sampling Policy Optimization](https://arxiv.org/abs/2512.00499)
*Yuepeng Sheng,Yuwei Huang,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.LG

TL;DR: 本文提出了ESPO框架，通过基于预测熵的序列分组分解，实现细粒度控制与训练稳定性的平衡，在数学推理任务上显著提升了收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO、GSPO）在序列级优化中存在梯度利用不足和信用分配不均的问题，难以兼顾优化粒度与训练稳定性。

Method: 提出Entropy Importance Sampling Policy Optimization (ESPO)，将序列按预测熵分解为组，引入熵驱动的重要性采样以捕捉序列内异质性，并设计熵自适应裁剪机制，根据模型不确定性动态调整信任区域。

Result: 在多个数学推理基准上实验表明，ESPO加速了收敛，并在HMMT基准上将准确率从4.4%提升至13.13%，达到最先进水平。

Conclusion: ESPO通过熵引导的采样与自适应裁剪，有效解决了现有策略优化中梯度利用不足和信用分配不合理的问题，实现了更高效稳定的强化学习微调。

Abstract: Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.

</details>


### [438] [Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction](https://arxiv.org/abs/2512.00521)
*Sabrina Islam,Md. Atiqur Rahman,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.LG

TL;DR: 本文提出了一种名为Rep3Net的多模态深度学习架构，结合分子描述符、图结构表示和ChemBERTa生成的SMILES嵌入，用于提升分子对靶标蛋白（如PARP-1）的生物活性预测性能，并在多个基线模型上取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 传统QSAR模型难以有效捕获分子的结构与上下文信息，导致生物活性预测效果受限。

Method: 提出Rep3Net模型，融合分子描述符、基于图的分子空间与关系表示、以及ChemBERTa从SMILES生成的上下文化嵌入；采用多模态特征拼接策略进行生物活性预测。

Result: 在PARP-1数据集上，Rep3Net在生物活性预测任务中表现优于GCN、GAT、XGBoost等单模态模型，展现出更高预测精度和可扩展性。

Conclusion: Rep3Net为早期药物发现中的计算筛选提供了一个高效、可扩展的多模态生物活性预测框架。

Abstract: In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.

</details>


### [439] [Hyperbolic Continuous Structural Entropy for Hierarchical Clustering](https://arxiv.org/abs/2512.00524)
*Guangjie Zeng,Hao Peng,Angsheng Li,Li Sun,Chunyang Liu,Shengze Li,Yicheng Pan,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为HypCSE的新型神经网络模型，通过在双曲空间中嵌入图节点并最小化结构增强图上的连续结构熵（CSE），实现更优的层次聚类。


<details>
  <summary>Details</summary>
Motivation: 现有层次聚类方法缺乏全局优化目标，且图方法常忽略图结构的重要性，多在完全图或静态预定义图上优化。

Method: 提出HypCSE模型：利用双曲图神经网络将数据点嵌入双曲空间；定义并最小化基于嵌入的近似结构熵；通过最低公共祖先（LCA）建模并松弛为可微的连续结构熵（CSE）；引入图结构学习（GSL）策略动态更新图结构以更好捕获数据层次性。

Result: 在七个数据集上的大量实验表明，HypCSE性能优于现有方法。

Conclusion: HypCSE通过结合双曲嵌入、连续结构熵优化与动态图结构学习，有效提升了层次聚类的性能和理论一致性。

Abstract: Hierarchical clustering is a fundamental machine-learning technique for grouping data points into dendrograms. However, existing hierarchical clustering methods encounter two primary challenges: 1) Most methods specify dendrograms without a global objective. 2) Graph-based methods often neglect the significance of graph structure, optimizing objectives on complete or static predefined graphs. In this work, we propose Hyperbolic Continuous Structural Entropy neural networks, namely HypCSE, for structure-enhanced continuous hierarchical clustering. Our key idea is to map data points in the hyperbolic space and minimize the relaxed continuous structural entropy (SE) on structure-enhanced graphs. Specifically, we encode graph vertices in hyperbolic space using hyperbolic graph neural networks and minimize approximate SE defined on graph embeddings. To make the SE objective differentiable for optimization, we reformulate it into a function using the lowest common ancestor (LCA) on trees and then relax it into continuous SE (CSE) by the analogy of hyperbolic graph embeddings and partitioning trees. To ensure a graph structure that effectively captures the hierarchy of data points for CSE calculation, we employ a graph structure learning (GSL) strategy that updates the graph structure during training. Extensive experiments on seven datasets demonstrate the superior performance of HypCSE.

</details>


### [440] [Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine](https://arxiv.org/abs/2512.00528)
*Isara Liyanage,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: 本文通过三种方法改进可解释提升机（EBM）：贝叶斯超参优化、面向公平性的多目标优化函数、以及面向冷启动的自监督预训练流程，在多个基准数据集上验证了其在保持高透明性的同时提升鲁棒性与公平性。


<details>
  <summary>Details</summary>
Motivation: 应对高风险领域中复杂机器学习模型“黑箱”问题，提升玻璃盒模型EBM的准确性、透明性、公平性与鲁棒性，以满足监管与伦理要求。

Method: 1）基于贝叶斯方法的定向超参数优化；2）设计面向公平性的自定义多目标超参优化函数；3）提出面向冷启动场景的新型自监督预训练流程。

Result: 超参调优对ROC AUC提升有限，但显著改变了模型决策行为，表明需采用多维度评估而非单一指标。

Conclusion: 本工作是构建兼具准确性、鲁棒性、公平性与透明性的可信AI系统的关键一步，推动EBM向实际高风险应用场景落地。

Abstract: The widespread adoption of complex machine learning models in high-stakes domains has brought the "black-box" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.

</details>


### [441] [Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets](https://arxiv.org/abs/2512.00536)
*Aaryan Gupta,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 本文提出了一种高效的无模型数据集蒸馏算法，用于监督回归和离线强化学习，理论证明仅需O~(d²)个随机采样回归器即可保证蒸馏后数据集上模型性能接近原始数据集，并首次在离线RL中基于Bellman损失进行蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法多依赖模型训练或行为克隆目标，缺乏对监督学习（尤其是回归）和离线强化学习中关键结构（如Bellman误差）的理论保障与高效无模型实现。

Method: 提出一种不依赖模型训练的蒸馏算法：在监督回归中通过匹配原始与合成数据集在随机采样线性回归器上的MSE损失；在离线RL中则匹配Bellman损失（利用奖励与下一状态信息），而非传统的行为克隆。

Result: 理论证明：监督回归下仅需O~(d²)个随机回归器即可保证任意有界线性模型的MSE损失近似一致，并给出紧的Ω(d²)下界；离线RL中证明合成数据集上任意线性Q函数的Bellman损失与原数据集近似，从而保证策略性能相近；实验验证了理论结论与性能提升。

Conclusion: 该工作为数据集蒸馏提供了首个兼具高效性、无模型性与强理论保证（含紧界）的框架，尤其拓展至离线RL并首次利用Bellman损失，显著提升了蒸馏质量与适用性。

Abstract: Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $Ω(d^2)$ for the number of sampled regressors showing the tightness of our analysis.
  Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.

</details>


### [442] [DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning](https://arxiv.org/abs/2512.00545)
*Akrati Saxena,Harshith Kumar Yadav,Bart Rutten,Shashi Shekhar Jha*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的公平性感知影响力最大化方法DQ4FairIM，通过引入maximin公平目标来提升少数群体的影响力传播，实验证明该方法在保持较高影响力扩散的同时实现了更优的公平性与性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现实社交网络中存在结构性不平等，传统影响力最大化算法可能加剧对少数群体的边缘化，因此需要设计考虑公平性的新算法。

Method: 将公平性融入最大化影响传播问题，采用maximin公平目标，并构建基于深度Q学习和Structure2Vec嵌入的强化学习框架DQ4FairIM，将问题建模为马尔可夫决策过程进行求解。

Result: 在合成和真实网络上的实验表明，DQ4FairIM相比无公平意识和其他公平感知基线方法，在公平性方面表现更优，同时保持良好的影响力传播性能，并具备跨不同网络规模和种子节点数量的泛化能力。

Conclusion: DQ4FairIM能有效实现社区间的公平影响力传播，平衡性能与公平性，且所学策略无需重新训练即可推广到不同问题实例，具有实际应用潜力。

Abstract: The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.

</details>


### [443] [A Graph Neural Network Approach for Localized and High-Resolution Temperature Forecasting](https://arxiv.org/abs/2512.00546)
*Joud El-Shawa,Elham Bagheri,Sedef Akinli Kocak,Yalda Mohsenzadeh*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络（GNN）的高分辨率局地温度预测框架，旨在提升热浪早期预警能力，尤其关注全球南方和边缘化群体的公平性与可及性。


<details>
  <summary>Details</summary>
Motivation: 当前数值天气预报模型难以捕捉微尺度极端高温，导致最脆弱人群无法获得及时预警，而热浪正日益加剧并 disproportionately 影响资源匮乏地区。

Method: 构建了一个图神经网络框架，利用空间学习和高效计算实现多时间尺度（最长48小时）的局地高分辨率温度预测。

Result: 在加拿大安大略省西南部验证，1–48小时预测平均MAE为1.93°C，48小时单点MAE为2.93°C；模型基于24小时输入窗口，在最大区域上评估。

Conclusion: 该方法虽在数据丰富地区验证，但为面向全球南方数据稀缺地区的迁移学习与公平化局地预测奠定了基础。

Abstract: Heatwaves are intensifying worldwide and are among the deadliest weather disasters. The burden falls disproportionately on marginalized populations and the Global South, where under-resourced health systems, exposure to urban heat islands, and the lack of adaptive infrastructure amplify risks. Yet current numerical weather prediction models often fail to capture micro-scale extremes, leaving the most vulnerable excluded from timely early warnings. We present a Graph Neural Network framework for localized, high-resolution temperature forecasting. By leveraging spatial learning and efficient computation, our approach generates forecasts at multiple horizons, up to 48 hours. For Southwestern Ontario, Canada, the model captures temperature patterns with a mean MAE of 1.93$^{\circ}$C across 1-48h forecasts and MAE@48h of 2.93$^{\circ}$C, evaluated using 24h input windows on the largest region. While demonstrated here in a data-rich context, this work lays the foundation for transfer learning approaches that could enable localized, equitable forecasts in data-limited regions of the Global South.

</details>


### [444] [List Replicable Reinforcement Learning](https://arxiv.org/abs/2512.00553)
*Bohan Zhang,Michael Chen,A. Pavan,N. V. Vinodchandran,Lin F. Yang,Ruosong Wang*

Main category: cs.LG

TL;DR: 本文提出了可复制强化学习（list replicable RL）的理论框架，定义了“列表复杂度”来衡量算法在不同运行中输出策略的一致性，并设计了具有多项式列表复杂度的可复制表格型RL算法，提升了实际RL算法的稳定性。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在实践中表现出对训练条件的高度敏感和不稳定，导致结果难以复现。为形式化地解决这一可复制性问题，需要在理论层面构建能保证输出一致性的RL算法。

Method: 在PAC强化学习框架下引入弱可复制性和强可复制性两种形式，通过新颖的规划策略（基于字典序选择动作并结合随机容忍阈值）和保持可复制性的状态可达性检测机制，设计出列表复杂度在状态数、动作数和时间范围上均为多项式的表格型RL算法。

Result: 提出了首个能保证列表可复制性的高效表格型RL算法，其列表复杂度为多项式级别；进一步扩展至强可复制性情形，实现了对策略执行轨迹数量的多项式界约束；实验证明所提规划策略可提升实际RL框架的稳定性。

Conclusion: 本文为强化学习中的可复制性问题提供了理论基础与解决方案，表明通过特定算法设计可在理论上实现高可复制性，并有助于缓解实际应用中RL算法的不稳定性问题。

Abstract: Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \emph{small list} of policies across different runs, with high probability. The size of this list defines the \emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.

</details>


### [445] [Pre-Generating Multi-Difficulty PDE Data for Few-Shot Neural PDE Solvers](https://arxiv.org/abs/2512.00564)
*Naman Choudhary,Vedant Singh,Ameet Talwalkar,Nicholas Matthew Boffi,Mikhail Khodak,Tanya Marwah*

Main category: cs.LG

TL;DR: 通过在训练中结合低、中、高难度的PDE数据，研究发现利用较低计算成本生成的简单样本可显著提升神经网络对高难度物理问题的求解能力，实现高达8.9倍的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解器在高难度问题上计算成本高昂，而神经求解器的训练依赖这些数据，形成‘鸡生蛋’困境：越难的问题越需要神经加速，但又越难获得训练数据。本文旨在探索如何有效分配经典求解器的计算资源以突破这一瓶颈。

Method: 在2D不可压缩Navier-Stokes方程上系统地变化几何复杂度（障碍物数量与位置）、物理复杂度（雷诺数）及其组合，构建不同难度级别的数据集；通过混合不同难度级别的数据进行训练，评估模型在高难度任务上的表现，并比较不同数据构成下的计算成本与误差关系。

Result: 使用包含低、中等难度样本的数据集训练模型，可在仅需少量高难度样本的情况下有效学习高难度物理行为；相比仅使用高难度数据，混合策略可减少8.9倍的预生成计算开销以达到相同误差水平。

Conclusion: 数据难度的分层设计和合理组合能大幅提升神经PDE求解器的训练效率与实用性，表明对预生成数据的**原则性筛选**比单纯增加总量更重要，为构建PDE基础模型提供了可行路径。

Abstract: A key aspect of learned partial differential equation (PDE) solvers is that the main cost often comes from generating training data with classical solvers rather than learning the model itself. Another is that there are clear axes of difficulty--e.g., more complex geometries and higher Reynolds numbers--along which problems become (1) harder for classical solvers and thus (2) more likely to benefit from neural speedups. Towards addressing this chicken-and-egg challenge, we study difficulty transfer on 2D incompressible Navier-Stokes, systematically varying task complexity along geometry (number and placement of obstacles), physics (Reynolds number), and their combination. Similar to how it is possible to spend compute to pre-train foundation models and improve their performance on downstream tasks, we find that by classically solving (analogously pre-generating) many low and medium difficulty examples and including them in the training set, it is possible to learn high-difficulty physics from far fewer samples. Furthermore, we show that by combining low and high difficulty data, we can spend 8.9x less compute on pre-generating a dataset to achieve the same error as using only high difficulty examples. Our results highlight that how we allocate classical-solver compute across difficulty levels is as important as how much we allocate overall, and suggest substantial gains from principled curation of pre-generated PDE data for neural solvers. Our code is available at https://github.com/Naman-Choudhary-AI-ML/pregenerating-pde

</details>


### [446] [Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics](https://arxiv.org/abs/2512.00580)
*Giovanni Conforti,Alain Durmus,Le-Tuyet-Nhi Pham*

Main category: cs.LG

TL;DR: 本文研究了离散扩散模型（DDM）在离散状态空间中的理论基础，首次在有限和无限离散空间上建立了非渐近收敛性保证，且不依赖于得分函数的有界性假设，模型复杂度随维度线性或近线性增长，具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型相比连续情况缺乏严谨的理论分析，且其组合结构带来了显著挑战，本文旨在建立其理论收敛保证。

Method: 通过分析前向掩码和随机游走动力学，在有限空间ℤ_m^d和可数无限空间ℕ^d上推导离散得分函数，并利用其单调性分析反向过程与误差界限。

Result: 建立了DDM在多种噪声机制下的非渐近收敛性保证，证明模型复杂度仅随维度线性或对数因子增长，而非指数增长。

Conclusion: 本文为离散扩散模型提供了首个不依赖得分有界性的非渐近收敛理论，适用于包括掩码机制在内的多种离散噪声过程，支持高维数据的有效建模。

Abstract: We investigate the theoretical underpinnings of Discrete Diffusion Models (DDMs) on discrete state spaces. Unlike in the continuous setting-where diffusion models are well understood both theoretically and empirically-the discrete case poses significant challenges due to its combinatorial structure and the lack of rigorous analysis. In this work, we establish convergence guarantees for DDMs on both the finite space $\mathbb{Z}^d_m=\{0,...,m-1\}^d$ and the countably infinite space $\mathbb{N}^d$ under mild assumptions, focusing on forward masked and random walk dynamics. Similar to the continuous case, the backward process can be characterized by a discrete score function, whose monotonicity plays a central role in deriving the error bounds of the generated data. Notably, the complexity of our model scales linearly up to logarithmic factors, rather than exponentially, with the dimension, making it efficiently scalable to high-dimensional data. To the best of our knowledge, this study provides the first non-asymptotic convergence guarantees that do not rely on the boundedness of the estimated score-covering not only uniform noising processes on $\mathbb{Z}^d_m$ and on $\mathbb{N}^d$, but also masking-based noising dynamics.

</details>


### [447] [Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D](https://arxiv.org/abs/2512.00586)
*Michael R. Doane*

Main category: cs.LG

TL;DR: 本研究开发了一种基于NLP的生物医学文本分类器，用于预测神经科学领域临床试验的技术与监管成功率（pTRS），其中BioBERT模型表现最佳，ROC-AUC达0.74，显著优于传统方法和行业基准。


<details>
  <summary>Details</summary>
Motivation: 神经科学领域的药物研发失败率极高（低于10%的成功率），研发成本巨大，亟需一种能够早期识别有前景项目的工具以优化资源分配并降低风险。

Method: 利用ClinicalTrials.gov数据和Clinical Trial Outcome数据集中的结果标签，采用统计NLP技术提取文本特征，并构建多种非LLM模型（如逻辑回归、梯度提升、随机森林）以及基于BioBERT的LLM模型来预测临床试验成功概率。

Result: 在涵盖1976-2024年101,145项已完成临床试验的回顾性数据集上，非LLM模型最高达到0.64的ROC-AUC；而BioBERT模型达到0.74的ROC-AUC和0.185的Brier Score，预测误差平均比行业基准低40%，且70%的情况下优于基准。

Conclusion: 基于NLP特别是BioBERT的模型能有效提升对神经科学临床试验成功概率的预测能力，有助于改进药物研发决策、优化投资策略并推动智能化新药研发进程。

Abstract: This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.

</details>


### [448] [Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction](https://arxiv.org/abs/2512.00598)
*Yining Yuan,J. Ben Tamo,Wenqi Shi,Yishan Zhong,Micky C. Nnamdi,B. Randall Brenn,Steven W. Hwang,May D. Wang*

Main category: cs.LG

TL;DR: 提出FAIR-MTL，一种公平感知的多任务学习框架，通过无监督子群发现实现脊柱融合术后并发症严重程度的公平预测。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法依赖粗略人口统计调整或事后修正，无法捕捉临床人群的潜在结构，可能加剧偏见。

Method: 利用k-means聚类从人口统计嵌入中推断患者子群，用于多任务模型的任务路由，并采用逆频率加权和正则化缓解子群不平衡。

Result: 在四类并发症严重程度预测中AUC达0.86，准确率75%，显著降低性别和年龄相关的公平性指标差异，SHAP和Gini分析显示模型具有可解释性。

Conclusion: 将无监督子群发现融入多任务学习框架可提升手术风险预测的公平性、可解释性和临床实用性。

Abstract: Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.
  Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.
  Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.

</details>


### [449] [Efficient Matroid Bandit Linear Optimization Leveraging Unimodality](https://arxiv.org/abs/2512.00605)
*Aurélien Delage,Romaric Gaudel*

Main category: cs.LG

TL;DR: 本文研究了在拟阵约束下的组合半带宽问题，通过利用其潜在的单峰结构，提出了一种新方法，将成员查询次数减少到O(log log T)，同时保持与现有最优方法相当的遗憾性能，并显著降低了时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的组合半带宽方法在拟阵约束下虽然达到了最优遗憾，但面对大规模拟阵或成员查询代价高的情况（如保证多样性的在线推荐），时间复杂度仍然较高，限制了实际应用。

Method: 本文通过挖掘拟阵半带宽问题的内在单峰结构，设计了一种新算法，将涉及成员查询的迭代次数限制在O(log log T)内，在几乎不增加遗憾的前提下大幅减少计算开销。

Result: 实验表明，该方法在多个拟阵基准上实现了与最先进方法相当的遗憾性能，同时显著减少了运行时间和成员查询次数。

Conclusion: 通过利用单峰结构，本文在几乎不影响遗憾性能的情况下，显著提升了拟阵约束下半带宽学习的效率，为高成本成员查询场景提供了更实用的解决方案。

Abstract: We study the combinatorial semi-bandit problem under matroid constraints. The regret achieved by recent approaches is optimal, in the sense that it matches the lower bound. Yet, time complexity remains an issue for large matroids or for matroids with costly membership oracles (e.g. online recommendation that ensures diversity). This paper sheds a new light on the matroid semi-bandit problem by exploiting its underlying unimodal structure. We demonstrate that, with negligible loss in regret, the number of iterations involving the membership oracle can be limited to \mathcal{O}(\log \log T)$. This results in an overall improved time complexity of the learning process. Experiments conducted on various matroid benchmarks show (i) no loss in regret compared to state-of-the-art approaches; and (ii) reduced time complexity and number of calls to the membership oracle.

</details>


### [450] [Generalized Graph Transformer Variational Autoencoder](https://arxiv.org/abs/2512.00612)
*Siddhant Karki*

Main category: cs.LG

TL;DR: 本文提出了一种名为GGT-VAE的新型图链接预测模型，将广义图Transformer架构与变分自编码器（VAE）结合，利用全局自注意力机制和拉普拉斯位置编码建模节点间结构模式，无需消息传递，在多个基准数据集上取得优于基线的ROC-AUC和平均精度。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（如GCN、GNN）和GraphVAE依赖消息传递，难以有效捕获长程依赖和全局结构信息；需探索更强大的图结构建模方式。

Method: 提出广义图Transformer变分自编码器（GGT-VAE），融合Transformer式全局自注意力机制与拉普拉斯位置编码，在变分框架下直接学习图结构的潜在表示，摒弃传统消息传递范式。

Result: 在多个基准数据集上，GGT-VAE在ROC-AUC和平均精度指标上均显著优于现有基线方法。

Conclusion: GGT-VAE为图链接预测提供了一种不依赖消息传递的新范式，首次将广义图Transformer引入变分图生成框架，验证了其在结构建模与生成任务中的有效性与潜力。

Abstract: Graph link prediction has long been a central problem in graph representation learning in both network analysis and generative modeling. Recent progress in deep learning has introduced increasingly sophisticated architectures for capturing relational dependencies within graph-structured data. In this work, we propose the Generalized Graph Transformer Variational Autoencoder (GGT-VAE). Our model integrates Generalized Graph Transformer Architecture with Variational Autoencoder framework for link prediction. Unlike prior GraphVAE, GCN, or GNN approaches, GGT-VAE leverages transformer style global self-attention mechanism along with laplacian positional encoding to model structural patterns across nodes into a latent space without relying on message passing. Experimental results on several benchmark datasets demonstrate that GGT-VAE consistently achieves above-baseline performance in terms of ROC-AUC and Average Precision. To the best of our knowledge, this is among the first studies to explore graph structure generation using a generalized graph transformer backbone in a variational framework.

</details>


### [451] [Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies](https://arxiv.org/abs/2512.00619)
*Goutham Nalagatla,Shreyas Grandhe*

Main category: cs.LG

TL;DR: 提出一种基于预测编码的生成回放框架，用于缓解持续学习中的灾难性遗忘问题，在任务保持上表现优于反向传播方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在动态环境中因灾难性遗忘导致的持续学习难题，受生物记忆巩固机制启发。

Method: 采用基于预测编码原理的生成回放策略，并与基于反向传播的生成回放进行系统比较，评估其在多个基准数据集上的任务保留和迁移效率。

Result: 基于预测编码的回放在任务保持方面平均提升15.3%，同时保持有竞争力的迁移效率。

Conclusion: 生物启发的记忆机制可为持续学习提供有效解决方案，揭示了生物记忆过程与人工学习系统之间的联系。

Abstract: Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.

</details>


### [452] [Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model](https://arxiv.org/abs/2512.00630)
*Zhiming Lian*

Main category: cs.LG

TL;DR: 本文评估了大语言模型Qwen3-8B在金融文本分类和情感分析中的表现，结合噪声嵌入指令微调与低秩优化技术，实现了更高的准确率和训练效率，优于多种主流模型。


<details>
  <summary>Details</summary>
Motivation: 金融文本分类在量化交易和相关任务中日益重要，但现有模型在准确性、训练效率和资源消耗方面仍存在挑战，因此需要探索更高效、可扩展的解决方案。

Method: 采用Qwen3-8B大语言模型，结合Noisy Embedding Instruction Finetuning提升鲁棒性，并使用Rank-stabilized Low-Rank Adaptation和FlashAttention优化训练效率与显存占用。

Result: Qwen3-8B在金融情感分析和新闻分类任务中均优于T5、BERT、RoBERTa以及LLaMA1-7B、LLaMA2-7B和Baichuan2-7B等基线模型，表现出更高的分类准确率和更快的收敛速度。

Conclusion: Qwen3-8B结合高效的微调与优化方法，具备成为实时金融NLP应用中可扩展且经济的解决方案的潜力，有望推动动态量化交易系统的发展。

Abstract: Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.

</details>


### [453] [Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation](https://arxiv.org/abs/2512.00638)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: 提出了一种名为DP-FinDiff的差分隐私扩散框架，用于生成混合类型表格数据，在金融和医疗数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私数据合成方法在处理高维混合类型表格数据时存在编码开销大、效用低的问题，需要更高效且保护隐私的解决方案。

Method: 采用基于嵌入的表示来处理分类特征，并提出两种隐私感知训练策略：自适应时间步采样器和特征聚合损失，以适配扩散过程中的DP训练。

Result: 在金融和医疗数据集上，DP-FinDiff在相似隐私水平下比基线方法效用提高16-42%，显著提升了生成数据的保真度和下游任务性能。

Conclusion: DP-FinDiff有效平衡了隐私保护与数据效用，为敏感领域的安全数据共享提供了可行方案。

Abstract: We introduce DP-FinDiff, a differentially private diffusion framework for synthesizing mixed-type tabular data. DP-FinDiff employs embedding-based representations for categorical features, reducing encoding overhead and scaling to high-dimensional datasets. To adapt DP-training to the diffusion process, we propose two privacy-aware training strategies: an adaptive timestep sampler that aligns updates with diffusion dynamics, and a feature-aggregated loss that mitigates clipping-induced bias. Together, these enhancements improve fidelity and downstream utility without weakening privacy guarantees. On financial and medical datasets, DP-FinDiff achieves 16-42% higher utility than DP baselines at comparable privacy levels, demonstrating its promise for safe and effective data sharing in sensitive domains.

</details>


### [454] [ML-Tool-Bench: Tool-Augmented Planning for ML Tasks](https://arxiv.org/abs/2512.00672)
*Yaswanth Chittepu,Raghavendra Addanki,Tung Mai,Anup Rao,Branislav Kveton*

Main category: cs.LG

TL;DR: 本文提出了一种面向工具增强型机器学习（ML）代理的综合性基准测试，包含61个专用工具和15个Kaggle表格数据挑战，并引入内存中命名对象管理机制以支持复杂ML流程的规划评估；针对现有方法（如ReAct、树搜索）在有效性与状态评估上的不足，提出基于塑形确定性奖励与结构化文本反馈、以及任务分解两种改进策略，在GPT-4o上相较ReAct提升16.52百分位。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用基准仅关注工具选择或参数提取，无法评估ML代理所需的复杂规划与迭代能力；需构建更贴近真实端到端数据科学工作流的评估基准。

Method: 构建含61个专用工具和15个Kaggle挑战的基准，支持内存中命名对象管理；提出两种改进方法：1）使用塑形确定性奖励配合结构化文本反馈；2）将整体任务分解为子任务序列。

Result: 在GPT-4o上，所提方法相较ReAct在全部Kaggle挑战的中位数性能上提升16.52百分位；标准ReAct及LLM评估的树搜索方法在生成有效工具序列和状态评分一致性方面表现不佳。

Conclusion: 本文基准填补了工具增强型ML代理规划能力评估的空白，所提简单但有效的改进策略显著提升了代理在复杂ML流水线中的表现，为构建更强大的规划型ML代理奠定基础。

Abstract: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.

</details>


### [455] [Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks](https://arxiv.org/abs/2512.00686)
*Anish Lakkapragada*

Main category: cs.LG

TL;DR: 本文通过实验研究奇异学习理论（SLT）在可解释性和相变相关玩具模型中的应用，验证了自由能的Arrhenius型速率假设，并测量了局部学习系数随问题难度的缩放行为，揭示了SLT对理解神经网络相变的价值及待解问题。


<details>
  <summary>Details</summary>
Motivation: 经典统计推断与学习理论难以解释现代神经网络的成功，因其非可识别性（奇异性）违背了PAC界和渐近正态性的基本假设；奇异学习理论（SLT）作为基于代数几何的物理启发框架，有望弥合理论与实践鸿沟。

Method: 在可解释性与相变相关的简化模型中开展实证研究：1）用模运算‘grokking’模型和Anthropic的‘Superposition玩具模型’检验自由能F_n的Arrhenius型速率假设；2）在多项式回归器、低秩线性网络和低秩自编码器等受控网络族上，测量局部学习系数λ_α随问题难度的缩放规律。

Result: 实验复现了已知的缩放律，也发现了一些显著偏离理论预期的结果；证实了SLT有助于刻画神经网络相变行为。

Conclusion: SLT为理解神经网络相变提供了有力工具，本文展示了其实证价值，并提出了若干开放研究问题。

Abstract: Classical statistical inference and learning theory often fail to explain the success of modern neural networks. A key reason is that these models are non-identifiable (singular), violating core assumptions behind PAC bounds and asymptotic normality. Singular learning theory (SLT), a physics-inspired framework grounded in algebraic geometry, has gained popularity for its ability to close this theory-practice gap. In this paper, we empirically study SLT in toy settings relevant to interpretability and phase transitions. First, we understand the SLT free energy $\mathcal{F}_n$ by testing an Arrhenius-style rate hypothesis using both a grokking modulo-arithmetic model and Anthropic's Toy Models of Superposition. Second, we understand the local learning coefficient $λ_α$ by measuring how it scales with problem difficulty across several controlled network families (polynomial regressors, low-rank linear networks, and low-rank autoencoders). Our experiments recover known scaling laws while others yield meaningful deviations from theoretical expectations. Overall, our paper illustrates the many merits of SLT for understanding neural network phase transitions, and poses open research questions for the field.

</details>


### [456] [Flow Matching for Tabular Data Synthesis](https://arxiv.org/abs/2512.00698)
*Bahrul Ilmi Nasution,Floor Eijkelboom,Mark Elliot,Richard Allmendinger,Christian A. Naesseth*

Main category: cs.LG

TL;DR: 本文系统研究了流匹配（Flow Matching）在表格数据合成中的应用，对比了其与扩散模型（如TabDDPM、TabSyn）的性能，发现TabbyFlow等流匹配方法在效用、隐私与计算效率上均具优势，尤其在低步数（≤100）下表现优异，并揭示了概率路径（OT优于VP）与采样确定性对效用-隐私权衡的关键影响。


<details>
  <summary>Details</summary>
Motivation: 为解决隐私保护数据共享中生成高质量表格数据的需求，探索比扩散模型更高效、更可控的替代方法——流匹配（FM），特别是其变体（如变分FM）在效用与隐私权衡上的潜力。

Method: 提出多种流匹配实现方案用于表格数据合成，包括标准流匹配（FM）和变分流匹配（VFM）；系统比较最优传输（OT）与方差保持（VP）两种概率路径；对比确定性与随机采样策略；在多个基准数据集上开展实证评估。

Result: TabbyFlow等流匹配方法在数据效用上超越TabDDPM和TabSyn等扩散基线；仅需≤100函数评估步数即达优性能；OT路径整体性能更优，VP路径可降低披露风险；引入随机性（变分FM）可在保持边际分布的同时提升效用并降低隐私风险。

Conclusion: 流匹配是表格数据合成中一种高效、灵活且具隐私意识的强竞争力方法；概率路径选择与采样随机性是调控效用-隐私平衡的关键设计维度。

Abstract: Synthetic data generation is an important tool for privacy-preserving data sharing. While diffusion models have set recent benchmarks, flow matching (FM) offers a promising alternative. This paper presents different ways to implement flow matching for tabular data synthesis. We provide a comprehensive empirical study that compares flow matching (FM and variational FM) with a state-of-the-art diffusion method (TabDDPM and TabSyn) in tabular data synthesis. We evaluate both the standard Optimal Transport (OT) and the Variance Preserving (VP) probability paths, and also compare deterministic and stochastic samplers -- something possible when learning to generate using \textit{variational} flow matching -- characterising the empirical relationship between data utility and privacy risk. Our key findings reveal that flow matching, particularly TabbyFlow, outperforms diffusion baselines. Flow matching methods also achieves better performance with remarkably low function evaluations ($\leq$ 100 steps), offering a substantial computational advantage. The choice of probability path is also crucial, as using the OT path demonstrates superior performance, while VP has potential for producing synthetic data with lower disclosure risk. Lastly, our results show that making flows stochastic not only preserves marginal distributions but, in some instances, enables the generation of high utility synthetic data with reduced disclosure risk.

</details>


### [457] [Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset](https://arxiv.org/abs/2512.00708)
*Ming-Hsiu Wu,Ziqian Xie,Shuiwang Ji,Degui Zhi*

Main category: cs.LG

TL;DR: 本文提出了一个修改感知的DAVIS数据集，包含4032个涉及多种修饰的激酶-配体对，并设计了三种基准测试设置来评估模型在蛋白质修饰下的泛化能力；研究发现基于对接的方法在零样本设置下泛化更好，而无需对接的模型需少量微调即可显著提升对修饰蛋白的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在药物发现中（如蛋白-配体结合亲和力预测）过度拟合于简化、非生物真实的数据集，无法反映天然存在且具翻译后修饰的蛋白质，限制了其在精准医疗中的应用。

Method: 构建了一个完整且修饰感知的DAVIS数据集，涵盖取代、插入、缺失和磷酸化等修饰；提出三种新基准设置：增强数据预测、野生型到修饰泛化、少样本修饰泛化；系统评估了对接自由与对接依赖方法在各设置下的表现。

Result: 对接依赖模型在零样本场景下对蛋白修饰具有更强泛化能力；对接自由模型易过拟合野生型蛋白，但在少量修饰样本微调后性能显著提升。

Conclusion: 该修饰感知数据集与基准为开发更具生物学真实性和泛化能力的AI模型提供了重要基础，有望推动精准药物发现。

Abstract: Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete

</details>


### [458] [Exploiting Function-Family Structure in Analog Circuit Optimization](https://arxiv.org/abs/2512.00712)
*Zhuohua Liu,Kaiqi Huang,Qinxin Mei,Yuanqi Hu,Wei W. Xing*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理结构先验的模拟电路优化方法Circuit Prior Network（CPN），利用预训练的表格模型（TabPFN v2）结合精确离散后验下的Direct Expected Improvement（DEI），显著提升小样本下的建模精度与优化效率，优于传统高斯过程方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程代理模型采用全局平滑、平稳先验，与模拟电路中由器件物理决定的非线性、分段式（如指数律、有理函数、工作区依赖动态）性能映射不匹配，在小样本（50–100次评估）下拟合差、优化低效。

Method: 提出Circuit Prior Network（CPN）：将编码器件物理结构先验的预训练表格基础模型TabPFN v2与Direct Expected Improvement（DEI）结合，DEI在离散后验分布上精确计算期望改进值，避免高斯近似。

Result: 在6个电路、25个基线上验证：CPN在小样本下R²达≈0.99（GP-Matérn在Bandgap电路仅0.16）；功耗/性能等指标（FoM）提升1.05–3.81倍，迭代次数减少3.34–11.89倍。

Conclusion: 结构匹配的先验可替代人工定制模型，推动电路优化从经验建模转向系统化的物理结构识别范式。

Abstract: Analog circuit optimization is typically framed as black-box search over arbitrary smooth functions, yet device physics constrains performance mappings to structured families: exponential device laws, rational transfer functions, and regime-dependent dynamics. Off-the-shelf Gaussian-process surrogates impose globally smooth, stationary priors that are misaligned with these regime-switching primitives and can severely misfit highly nonlinear circuits at realistic sample sizes (50--100 evaluations). We demonstrate that pre-trained tabular models encoding these primitives enable reliable optimization without per-circuit engineering. Circuit Prior Network (CPN) combines a tabular foundation model (TabPFN v2) with Direct Expected Improvement (DEI), computing expected improvement exactly under discrete posteriors rather than Gaussian approximations. Across 6 circuits and 25 baselines, structure-matched priors achieve $R^2 \approx 0.99$ in small-sample regimes where GP-Matérn attains only $R^2 = 0.16$ on Bandgap, deliver $1.05$--$3.81\times$ higher FoM with $3.34$--$11.89\times$ fewer iterations, and suggest a shift from hand-crafting models as priors toward systematic physics-informed structure identification. Our code will be made publicly available upon paper acceptance.

</details>


### [459] [Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift](https://arxiv.org/abs/2512.00716)
*Fanlong Zeng,Wensheng Gan*

Main category: cs.LG

TL;DR: 提出了一种基于对比学习的更强大的对抗不变性增强方法MPAIACL，以应对图数据中的协变量分布偏移问题，充分利用潜在空间信息，在多个OOD数据集上表现出优异的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在处理协变量分布偏移时表现不佳，且当前方法未能充分挖掘潜在空间中的丰富信息。

Method: 提出MPAIACL方法，结合对抗不变性增强与对比学习，通过利用潜在表示的内在信息来提升模型对分布偏移的鲁棒性。

Result: 在多个公开的OOD数据集上，MPAIACL相较于基线方法展现出更强的泛化能力和有效性。

Conclusion: MPAIACL能有效应对图数据中的协变量分布偏移问题，通过对比学习充分释放潜在空间的信息潜力，提升了模型的鲁棒性。

Abstract: Covariate distribution shift occurs when certain structural features present in the test set are absent from the training set. It is a common type of out-of-distribution (OOD) problem, frequently encountered in real-world graph data with complex structures. Existing research has revealed that most out-of-the-box graph neural networks (GNNs) fail to account for covariate shifts. Furthermore, we observe that existing methods aimed at addressing covariate shifts often fail to fully leverage the rich information contained within the latent space. Motivated by the potential of the latent space, we introduce a new method called MPAIACL for More Powerful Adversarial Invariant Augmentation using Contrastive Learning. MPAIACL leverages contrastive learning to unlock the full potential of vector representations by harnessing their intrinsic information. Through extensive experiments, MPAIACL demonstrates its robust generalization and effectiveness, as it performs well compared with other baselines across various public OOD datasets. The code is publicly available at https://github.com/flzeng1/MPAIACL.

</details>


### [460] [ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering](https://arxiv.org/abs/2512.00725)
*Xinyue Wang,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 本文提出了一种基于多模态大语言模型（MLLM）的用户驱动聚类方法，利用MLLM文本token的隐藏状态作为特征进行灵活、语义可控的聚类，并引入轻量聚类头与伪标签学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度聚类方法只能为每个数据集提供单一聚类结果，难以满足用户对语义可定制、多视角聚类的需求；而直接使用MLLM生成的图像描述进行聚类又存在描述泛化、缺乏特征具体性的问题。

Method: 挖掘MLLM中文本token隐藏状态与图像特征的强关联性，将其作为可解释、可调控的聚类表示；设计轻量聚类头并结合伪标签学习机制优化聚类过程。

Result: 在多个数据集和评估指标上展现出具有竞争力的聚类性能，支持按用户指定语义标准进行灵活聚类。

Conclusion: MLLM的中间文本表征可被有效用于用户驱动的深度聚类，无需依赖生成式输出，兼顾可控性与准确性。

Abstract: Typical deep clustering methods, while achieving notable progress, can only provide one clustering result per dataset. This limitation arises from their assumption of a fixed underlying data distribution, which may fail to meet user needs and provide unsatisfactory clustering outcomes. Our work investigates how multi-modal large language models (MLLMs) can be leveraged to achieve user-driven clustering, emphasizing their adaptability to user-specified semantic requirements. However, directly using MLLM output for clustering has risks for producing unstructured and generic image descriptions instead of feature-specific and concrete ones. To address these issues, our method first discovers that MLLMs' hidden states of text tokens are strongly related to the corresponding features, and leverages these embeddings to perform clusterings from any user-defined criteria. We also employ a lightweight clustering head augmented with pseudo-label learning, significantly enhancing clustering accuracy. Extensive experiments demonstrate its competitive performance on diverse datasets and metrics.

</details>


### [461] [Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation](https://arxiv.org/abs/2512.00728)
*Zach Lawrence,Jessica Yao,Chris Qin*

Main category: cs.LG

TL;DR: 本文提出了两种深度学习框架：COVE-NN（用于个体风电场的LSTM调度策略）和风电功率生成建模框架，分别显著降低了COVE和RMSE，并提升了功率曲线相似性，从而增强了数据驱动调度策略的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提升风电场集成储能系统的调度策略鲁棒性与经济价值，利用本地化电网需求、市场条件及合成风能发电数据优化决策。

Method: 开发了两个深度学习框架：1）COVE-NN——基于LSTM的个体风电场调度策略；2）基于大气条件建模的风电功率生成框架。

Result: COVE-NN在Pyron站点43年模拟中使年COVE降低32.3%；功率建模框架在Palouse风电场验证中RMSE降低9.5%，功率曲线相似性提高18.9%。

Conclusion: 所提框架可提升混合风电场的数据驱动调度鲁棒性，并具备向其他可再生能源系统拓展的潜力。

Abstract: Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.

</details>


### [462] [REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories](https://arxiv.org/abs/2512.00736)
*Jacob Thompson,Emiliano Garcia-Lopez,Yonatan Bisk*

Main category: cs.LG

TL;DR: REM是一个用于评估多模态大模型在具身环境中长视野空间推理能力的基准，揭示了现有模型在处理复杂空间关系时的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽经大量视频训练，仍缺乏人类般稳定的空间推理能力，限制其在具身智能中的应用。

Method: 提出REM基准，基于可控3D环境，系统评估对象恒常性、空间关系和数量追踪等关键能力。

Result: 当前最佳模型在简单任务上表现尚可，但在中等复杂度下可靠性显著下降。

Conclusion: REM揭示了MLLM从序列视觉输入构建稳健空间表征的挑战，并提供针对性指标以推动未来改进。

Abstract: Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.

</details>


### [463] [Preventing Model Collapse via Contraction-Conditioned Neural Filters](https://arxiv.org/abs/2512.00757)
*Zongjian Han,Yiran Liang,Ruiwen Wang,Yiwei Luo,Yilin Huang,Xiaotong Song,Dongqing Wei*

Main category: cs.LG

TL;DR: 本文提出了一种基于收缩算子的神经网络滤波方法，通过设计满足收缩条件的神经滤波器，在无偏估计框架下完全消除递归训练生成模型时对样本量增长的依赖，从而解决模型坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在递归训练中因样本量限制导致的模型坍塌问题。

Method: 设计基于收缩算子的神经网络滤波器，构建专用网络架构与损失函数，使其主动学习满足指数族分布中Assumption 2.3的收缩条件。

Result: 理论证明：当满足学习到的收缩条件时，即使样本量恒定，估计误差也能概率收敛；实验验证：该滤波器能在固定样本量下有效学习收缩条件并防止模型坍塌。

Conclusion: 所提方法为生成模型递归训练提供了无需增样、端到端可行的实用解决方案。

Abstract: This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\limsup_{t\to\infty}\mathbb{P}(\|\mathbf{e}_t\|>δ)=0$ for any $δ>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.

</details>


### [464] [Forecasting India's Demographic Transition Under Fertility Policy Scenarios Using hybrid LSTM-PINN Model](https://arxiv.org/abs/2512.00760)
*Subarna Khanra,Vijay Kumar Kukreja,Indu Bala*

Main category: cs.LG

TL;DR: 本研究提出一种结合政策感知生育函数的LSTM-PINN混合模型，用于印度2024至2054年的人口预测，揭示不同生育政策对年龄结构、劳动力和抚养比的深远影响。


<details>
  <summary>Details</summary>
Motivation: 印度面临复杂的人口转型与政策干预交织的挑战，传统人口预测方法难以兼顾动态变化与物理约束，亟需兼具准确性与可解释性的新型建模方法。

Method: 构建融合长短期记忆网络（LSTM）的物理信息神经网络（PINN），将基于年龄结构的人口输运-反应偏微分方程作为物理约束嵌入模型，并引入政策敏感的生育函数，在三种生育政策情景下进行长期预测。

Result: 模型显示严格控制生育将加剧人口老龄化并降低劳动参与率，而宽松政策虽促进劳动力增长但增加人口压力；不同政策显著影响未来年龄分布与抚养比，验证了模型在长期预测中的有效性与敏感性。

Conclusion: LSTM-PINN混合框架在保持物理一致性的同时提升了预测精度与可解释性，为印度及其他类似国家的人口政策制定提供了有力工具，强调需采取平衡的生育干预以实现可持续发展。

Abstract: Demographic forecasting remains a fundamental challenge for policy planning in rapidly evolving nations such as India, where fertility transitions, policy interventions, and age structured dynamics interact in complex ways. In this study, we present a hybrid modelling framework that integrates policy-aware fertility functions into a Physics-Informed Neural Network (PINN) enhanced with Long Short-Term Memory (LSTM) networks to capture physical constraints and temporal dependencies in population dynamics. The model is applied to India's age structured population from 2024 to 2054 under three fertility-policy scenarios: continuation of current fertility decline, stricter population control, and relaxed fertility promotion. The governing transport-reaction partial differential equation is formulated with India-specific demographic indicators, including age-specific fertility and mortality rates. PINNs embed the core population equation and policy-driven fertility changes, while LSTM layers improve long-term forecasting across decades. Results show that fertility policies substantially shape future age distribution, dependency ratios, and workforce size. Stricter controls intensify ageing and reduce labour force participation, whereas relaxed policies support workforce growth but increase population pressure. Our findings suggest that the hybrid LSTM-PINN is an effective approach for demographic forecasting, offering accuracy with interpretability. Beyond methodological novelty, this work provides actionable insights for India's demographic policy debates, highlighting the need for balanced fertility interventions to ensure sustainable socio-economic development.

</details>


### [465] [Provable Benefit of Sign Descent: A Minimal Model Under Heavy-Tailed Class Imbalance](https://arxiv.org/abs/2512.00763)
*Robin Yadav,Shuo Xie,Tianhao Wang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文分析了在语言建模任务中，ℓ∞-范数下降（即符号下降）相较于梯度下降的收敛优势，从数据分布的重尾类别不平衡特性出发，在简化的next-token预测设定下提供了理论证明。


<details>
  <summary>Details</summary>
Motivation: 解释自适应优化方法（如Adam）在大语言模型预训练中优于梯度下降的原因，特别是探究ℓ∞范数下降等非欧几里得范数方法的优势如何由语言建模中的数据特性（如重尾类别不平衡）所驱动。

Method: 构建一个最小但具代表性的next-token预测模型，基于重尾类别不平衡的数据分布假设，进行理论分析与收敛性证明。

Result: 在重尾类别不平衡条件下，坐标式算法（如Sign descent）比归一化梯度下降（ℓ₂范数意义下的最速下降）具有更快的收敛速度。

Conclusion: 语言建模中常见的重尾类别不平衡现象可自然导出ℓ∞范数下降的优越性，为自适应优化方法的有效性提供了数据驱动的理论依据。

Abstract: Adaptive optimization methods (such as Adam) play a major role in LLM pretraining, significantly outperforming Gradient Descent (GD). Recent studies have proposed new smoothness assumptions on the loss function to explain the advantages of adaptive algorithms with structured preconditioners, e.g., coordinate-wise or layer-wise, and steepest descent methods w.r.t. non-euclidean norms, e.g., $\ell_\infty$ norm or spectral norm, over GD. However, it remains unclear how these smoothness assumptions manifest in language modelling tasks. In this work, we aim to analyze the benefit of $\ell_\infty$-norm descent (a.k.a. sign descent) directly from properties of the data distribution, namely, heavy-tailed class imbalance. We propose a minimal yet representative setting of next-token prediction, where we can provably show faster convergence of coordinate-wise algorithms such as Sign descent (steepest descent w.r.t. $\ell_\infty$ norm) over normalized GD (steepest descent w.r.t. to $\ell_2$ norm) in the presence of heavy tail class imbalance.

</details>


### [466] [AI Agent for Source Finding by SoFiA-2 for SKA-SDC2](https://arxiv.org/abs/2512.00769)
*Xingchen Zhou,Nan Li,Peng Jia,Yingfeng Liu,Furen Deng,Shuanghao Shu,Ying Li,Liang Cao,Huanyuan Shan,Ayodeji Ibitoye*

Main category: cs.LG

TL;DR: 提出基于强化学习（Soft Actor-Critic）的AI代理框架，自动优化射电源提取程序（如SoFiA）的参数，在SKA SDC2数据集上仅用100步即超越人工配置性能，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 源提取在下一代大规模射电巡天（如SKA）中至关重要，但现有工具（如SoFiA、Aegean）依赖复杂且难以手动调优的参数配置，影响提取结果的可靠性与效率。

Method: 构建一个基于强化学习算法Soft Actor-Critic（SAC）的AI代理框架，通过与环境交互动态调整SoFiA各模块的关键参数，并以SKA SDC2评分作为反馈信号进行训练，实现自动化参数优化。

Result: 在SKA SDC2数据集上验证表明，该AI代理在仅100次评估步骤内即可找到优于SoFiA团队基准配置的参数组合，且耗时更少，展现出高效性与可行性。

Conclusion: 该方法可有效解决射电源提取中的复杂参数调优问题，并具备推广潜力，适用于其他需精细参数调节的天文数据分析任务，但其性能依赖于高质量的训练数据和真实标注目录。

Abstract: Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.

</details>


### [467] [What Is Preference Optimization Doing, How and Why?](https://arxiv.org/abs/2512.00778)
*Yue Wang,Qizhou Wang,Zizhuo Zhang,Ang Li,Gang Niu,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文通过分析DPO和PPO在偏好优化中的优化动力学，揭示了二者在梯度更新目标、正负学习及损失重加权等方面的不同机制，并通过消融实验验证其对优化效率与性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为DPO属于监督学习而PPO属于强化学习，但缺乏对其差异根源的深入分析。本文旨在从优化动力学角度填补这一理论空白。

Method: 分析DPO与PPO的梯度更新目标方向、正负学习作用及损失重加权机制，并开展系统性消融实验。

Result: 发现DPO具有稳定学习目标，正负学习相互抵消，损失重加权起正则化作用；PPO目标动态变化，负学习主要用于探索，损失重加权反映不同token组在目标更新中的差异化作用。

Conclusion: 该分析深化了对偏好优化方法的理解，为构建更偏好对齐的大语言模型提供了理论依据与设计启示。

Abstract: Preference optimization (PO) is indispensable for large language models (LLMs), with methods such as direct preference optimization (DPO) and proximal policy optimization (PPO) achieving great success. A common belief is that DPO is supervised learning while PPO is reinforcement learning, yet deeper analyses for the reasons underlying these differences remain lacking. To fill this gap, we analyze their optimization dynamics, revealing distinct algorithmic behaviors and comprehending their underlying causes. First, we examine the target directions of gradient-based updates and find that DPO follows stable targets, whereas PPO follows dynamic targets that balance exploration and exploitation, thus validating the common belief from a new perspective. Second, we examine the roles of positive learning, negative learning, and loss reweighting, which are three key components in PO methods. Our analyses reveal that these components play fairly different roles. In DPO, positive and negative learning jointly shape the learning targets meanwhile mutually offset each other. However, loss reweighting in DPO acts less as a reward signal but more as a regularizer to mitigate overfitting. In PPO, negative learning primarily supports exploration rather than determining the targets. Meanwhile, loss reweighting, related to absolute values of token-level advantages, indicates the distinct roles of token groups in updating targets. Given these findings, we conduct carefully designed ablation studies to further examine how controlling these dynamics impacts optimization efficiency and practical performance. The insights gained from our analyses not only deepen the understanding of PO methods but also inspire the development of more preference-aligned LLMs.

</details>


### [468] [Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment](https://arxiv.org/abs/2512.00783)
*Libo Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Sigma的视觉-语言-动作（VLA）模型，旨在为类人机器人构建一个可实时更新的语义与连续控制之间的中介思维空间。该模型基于pi05_base，在单张RTX 4090上训练，通过LoRA微调与推理适配器优化，在离线闭环重放实验中显著降低了控制均方误差（MSE），同时保持语义-文本对齐质量与‘意念通信’规范不变。


<details>
  <summary>Details</summary>
Motivation: 解决类人机器人认知系统中语义理解与连续动作控制之间缺乏可时间更新的中介思维空间的问题。

Method: 基于开源pi05_base模型，自主设计VLA架构；使用svla_so101_pickplace数据集预处理训练；采用LoRA微调与推理阶段适配器联合优化；通过离线闭环重放进行评估。

Result: Sigma在向量、片段及完整轨迹多个时间尺度上均稳定降低控制MSE；语义-文本对齐质量与‘意念通信’规范未下降；无需重训基础模型即可实现心智响应式对齐控制。

Conclusion: 结合深层语义理解与关联能力的轻量架构，可在不修改基模型前提下实现可量化、可复现的语义对齐与意图驱动行为，为类人机器人认知系统提供新范式。

Abstract: To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named "Sigma" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.

</details>


### [469] [Limitations of Using Identical Distributions for Training and Testing When Learning Boolean Functions](https://arxiv.org/abs/2512.00791)
*Jordi Pérez-Guijarro*

Main category: cs.LG

TL;DR: 研究在训练数据和测试数据分布不一致的情况下，是否总是最优的让训练分布与测试分布相同。结果表明，在某些假设下，匹配分布并非总是最佳选择，但在目标函数具有规律性且为均匀分布时，传统结论仍然成立。


<details>
  <summary>Details</summary>
Motivation: 探讨在训练和测试数据分布不同的情况下，模型泛化的最优训练分布策略，挑战了通常认为训练与测试分布应一致的假设。

Method: 基于单向函数存在的假设，通过理论分析探讨不同分布对学习性能的影响，并在特定正则条件下分析均匀分布情形下的结果。

Result: 证明在一般情况下，训练分布与测试分布相同并非最优；但在目标函数具有规律性且分布为均匀时，传统结论成立。

Conclusion: 分布匹配并非在所有情况下都是最优的，最优训练分布取决于目标函数的结构和数据分布特性。

Abstract: When the distributions of the training and test data do not coincide, the problem of understanding generalization becomes considerably more complex, prompting a variety of questions. In this work, we focus on a fundamental one: Is it always optimal for the training distribution to be identical to the test distribution? Surprisingly, assuming the existence of one-way functions, we find that the answer is no. That is, matching distributions is not always the best scenario, which contrasts with the behavior of most learning methods. Nonetheless, we also show that when certain regularities are imposed on the target functions, the standard conclusion is recovered in the case of the uniform distribution.

</details>


### [470] [Estimating the Effective Rank of Vision Transformers via Low-Rank Factorization](https://arxiv.org/abs/2512.00792)
*Liyu Zerihun*

Main category: cs.LG

TL;DR: 本文提出了一种通过知识蒸馏和权重分解来估计深度神经网络内在维度（即有效秩）的框架，定义了'有效秩区间'和'有效拐点'两个指标，并在ViT-B/32模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度网络虽高度过参数化，但其学习到的表征常具有低秩结构；亟需一种稳定、可解释的方法来量化模型的内在维度。

Method: 先训练全秩教师模型，再在多个秩下对其权重进行分解并训练对应的学生模型（通过知识蒸馏），拟合准确率-秩曲线，利用单调PCHIP插值确定85–95%教师准确率对应的有效秩区间，并通过拐点分析（垂直距离最大处）定位有效膝点。

Result: 在ViT-B/32/CIFAR-100上得到有效秩区间[16, 34]、有效膝点r*≈31；秩32时学生达69.46%准确率（教师为73.35%，达94.7%），同时实现显著参数压缩。

Conclusion: 所提框架能稳健估计不同架构与数据集上的有效秩区域和膝点，为刻画深度模型内在维度提供了实用工具。

Abstract: Deep networks are heavily over-parameterized, yet their learned representations often admit low-rank structure. We introduce a framework for estimating a model's intrinsic dimensionality by treating learned representations as projections onto a low-rank subspace of the model's full capacity. Our approach: train a full-rank teacher, factorize its weights at multiple ranks, and train each factorized student via distillation to measure performance as a function of rank.
  We define effective rank as a region, not a point: the smallest contiguous set of ranks for which the student reaches 85-95% of teacher accuracy. To stabilize estimates, we fit accuracy vs. rank with a monotone PCHIP interpolant and identify crossings of the normalized curve. We also define the effective knee as the rank maximizing perpendicular distance between the smoothed accuracy curve and its endpoint secant; an intrinsic indicator of where marginal gains concentrate.
  On ViT-B/32 fine-tuned on CIFAR-100 (one seed, due to compute constraints), factorizing linear blocks and training with distillation yields an effective-rank region of approximately [16, 34] and an effective knee at r* ~ 31. At rank 32, the student attains 69.46% top-1 accuracy vs. 73.35% for the teacher (~94.7% of baseline) while achieving substantial parameter compression. We provide a framework to estimate effective-rank regions and knees across architectures and datasets, offering a practical tool for characterizing the intrinsic dimensionality of deep models.

</details>


### [471] [Soft Quality-Diversity Optimization](https://arxiv.org/abs/2512.00810)
*Saeed Hedayatian,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: 本文提出了一种无需离散化行为空间的新型质量多样性（QD）优化框架——Soft QD，并基于其设计了可微分算法SQUAD，在保持竞争力的同时提升了高维问题的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有QD方法依赖行为空间离散化，在大解空间和高维行为空间中面临存储开销大与离散失效（维度灾难）的问题。

Method: 提出Soft QD新框架，避免显式离散化；推导出可微分算法SQUAD（Soft QD Using Approximated Diversity），利用近似多样性进行优化。

Result: 理论验证了Soft QD具有单调性等良好性质，并与经典QD Score指标具有一致极限行为；实验表明SQUAD在标准基准上媲美当前最优方法，且在高维问题中更具可扩展性。

Conclusion: Soft QD为QD优化提供了更普适、可微、可扩展的新范式，SQUAD是其首个有效实现，推动QD向高维连续场景拓展。

Abstract: Quality-Diversity (QD) algorithms constitute a branch of optimization that is concerned with discovering a diverse and high-quality set of solutions to an optimization problem. Current QD methods commonly maintain diversity by dividing the behavior space into discrete regions, ensuring that solutions are distributed across different parts of the space. The QD problem is then solved by searching for the best solution in each region. This approach to QD optimization poses challenges in large solution spaces, where storing many solutions is impractical, and in high-dimensional behavior spaces, where discretization becomes ineffective due to the curse of dimensionality. We present an alternative framing of the QD problem, called \emph{Soft QD}, that sidesteps the need for discretizations. We validate this formulation by demonstrating its desirable properties, such as monotonicity, and by relating its limiting behavior to the widely used QD Score metric. Furthermore, we leverage it to derive a novel differentiable QD algorithm, \emph{Soft QD Using Approximated Diversity (SQUAD)}, and demonstrate empirically that it is competitive with current state of the art methods on standard benchmarks while offering better scalability to higher dimensional problems.

</details>


### [472] [Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification](https://arxiv.org/abs/2512.00812)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 本文提出因果协同博弈（CCG）框架，将多标签分类建模为协同多玩家交互，结合神经结构方程模型、反事实好奇心奖励和因果不变性损失，显著提升稀有标签预测性能与模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多标签分类面临标签不平衡、虚假相关性和分布偏移等挑战，尤其损害稀有标签的预测效果。

Method: 提出因果协同博弈（CCG）框架，融合神经结构方程模型进行显式因果发现、反事实好奇心奖励驱动鲁棒特征学习、因果不变性损失保障跨环境泛化，并设计稀有标签专用增强策略。

Result: 在多个基准测试中，CCG显著优于强基线方法，在稀有标签预测和整体鲁棒性方面均取得提升；消融实验与定性分析验证了各组件的有效性与可解释性。

Conclusion: 将因果推断与协同博弈论相结合，能有效推动多标签学习的发展，提升模型对稀有标签的识别能力与跨环境泛化性能。

Abstract: Multi-label classification (MLC) remains vulnerable to label imbalance, spurious correlations, and distribution shifts, challenges that are particularly detrimental to rare label prediction. To address these limitations, we introduce the Causal Cooperative Game (CCG) framework, which conceptualizes MLC as a cooperative multi-player interaction. CCG unifies explicit causal discovery via Neural Structural Equation Models with a counterfactual curiosity reward to drive robust feature learning. Furthermore, it incorporates a causal invariance loss to ensure generalization across diverse environments, complemented by a specialized enhancement strategy for rare labels. Extensive benchmarking demonstrates that CCG substantially outperforms strong baselines in both rare label prediction and overall robustness. Through rigorous ablation studies and qualitative analysis, we validate the efficacy and interpretability of our components, underscoring the potential of synergizing causal inference with cooperative game theory for advancing multi-label learning.

</details>


### [473] [ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning](https://arxiv.org/abs/2512.00831)
*Yuchen Zeng,Shuibai Zhang,Wonjun Kang,Shutong Wu,Lynnix Zou,Ying Fan,Heeju Kim,Ziqian Lin,Jungtaek Kim,Hyung Il Koo,Dimitris Papailiopoulos,Kangwook Lee*

Main category: cs.LG

TL;DR: 本文提出了ReJump方法，将大推理模型（LRMs）的推理过程建模为树状中间步骤节点上的访问序列（即'跳跃'），区分相邻与非相邻跳转以刻画计算、回溯、验证等行为，并基于此分析不同LRMs的推理策略差异及其影响因素，同时提出ReJump引导的测试时优化策略。


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs)虽在数学和编程等任务上表现优异，但其内部推理‘算法’仍不清晰，亟需可解释、可量化的推理行为分析框架。

Method: 提出ReJump表示法：将推理轨迹建模为树状中间步骤节点的访问顺序；定义‘跳跃’（jump）为节点间转移，分为相邻（如计算）与非相邻（如回溯、验证）两类；设计多维度指标量化探索、利用、过度思考、遗忘和验证等行为；构建LLM代理自动提取ReJump格式推理轨迹；并在多个对比实验中应用该框架（如蒸馏模型vs教师模型、CoT提示vs LRMs、数据量与强化学习影响分析）。

Result: 发现准确率相近的LRMs可能具有显著不同的推理行为；不同任务偏好不同推理风格（如探索-利用权衡）；蒸馏、训练方式（如RL）和示例数量显著影响推理策略；ReJump可支持测试时优化，如Best-of-N选择和提示选择，提升推理质量。

Conclusion: ReJump提供了一种结构化、可度量的推理行为分析范式，揭示了LRMs推理策略的多样性与可塑性，并支持推理过程的诊断与优化，为理解与改进大模型推理能力提供了新路径。

Abstract: Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.

</details>


### [474] [Uncertainty Quantification for Deep Regression using Contextualised Normalizing Flows](https://arxiv.org/abs/2512.00835)
*Adriel Sosa Marco,John Daniel Kirwan,Alexia Toumpa,Simos Gerasimou*

Main category: cs.LG

TL;DR: 本文提出MCNF方法，一种无需重新训练模型的后处理不确定性量化技术，可同时生成预测区间和完整条件预测分布，适用于深度回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略分布信息（如多峰或不对称分布），要么需要大幅修改模型结构并重新训练，难以兼顾预测区间与完整预测分布。

Method: MCNF是一种后处理方法，构建在已训练好的预测模型之上，不需重新训练，能同时输出预测区间和完整的条件预测分布。

Result: 实验表明MCNF校准性好，性能媲美当前最优方法，并为下游决策任务提供更丰富的不确定性信息。

Conclusion: MCNF是一种高效、灵活且信息丰富的后处理不确定性量化方法，适用于高风险领域中的安全决策。

Abstract: Quantifying uncertainty in deep regression models is important both for understanding the confidence of the model and for safe decision-making in high-risk domains. Existing approaches that yield prediction intervals overlook distributional information, neglecting the effect of multimodal or asymmetric distributions on decision-making. Similarly, full or approximated Bayesian methods, while yielding the predictive posterior density, demand major modifications to the model architecture and retraining. We introduce MCNF, a novel post hoc uncertainty quantification method that produces both prediction intervals and the full conditioned predictive distribution. MCNF operates on top of the underlying trained predictive model; thus, no predictive model retraining is needed. We provide experimental evidence that the MCNF-based uncertainty estimate is well calibrated, is competitive with state-of-the-art uncertainty quantification methods, and provides richer information for downstream decision-making tasks.

</details>


### [475] [Prediction-space knowledge markets for communication-efficient federated learning on multimedia tasks](https://arxiv.org/abs/2512.00841)
*Wenzhang Du*

Main category: cs.LG

TL;DR: KTA v2是一种面向联邦学习的预测空间知识交易机制，通过共享logits而非模型参数，在低通信开销下显著提升非IID数据下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中统计异质性（non-IID数据）和通信瓶颈问题，尤其在客户端部署大模型时，经典参数平均法（如FedAvg）易发散且通信代价高。

Method: 提出KTA v2：每轮客户端在私有数据上本地训练后，仅上传在小规模公共参考集上的logits；服务器据此构建预测空间的客户端相似图，并结合参考集准确率生成个性化教师集成，再将软目标下发供客户端进行第二阶段知识蒸馏；整体可视为带预测空间正则化的近似块坐标下降优化。

Result: 在FEMNIST、CIFAR-10和AG News上，KTA v2在远低于FedAvg的通信量（CIFAR-10为1/1100，AG News为1/300）下，显著超越本地训练、FedAvg、FedProx及FedMD等基线方法。

Conclusion: KTA v2通过预测空间的知识交换与个性化蒸馏，有效缓解统计异质性与通信限制的双重挑战，为高效鲁棒的联邦学习提供了新范式。

Abstract: Federated learning (FL) enables collaborative training over distributed multimedia data but suffers acutely from statistical heterogeneity and communication constraints, especially when clients deploy large models. Classic parameter-averaging methods such as FedAvg transmit full model weights and can diverge under nonindependent and identically distributed (non-IID) data. We propose KTA v2, a prediction-space knowledge trading market for FL. Each round, clients locally train on their private data, then share only logits on a small public reference set. The server constructs a client-client similarity graph in prediction space, combines it with reference-set accuracy to form per-client teacher ensembles, and sends back personalized soft targets for a second-stage distillation update. This two-stage procedure can be interpreted as approximate block-coordinate descent on a unified objective with prediction-space regularization. Experiments on FEMNIST, CIFAR-10 and AG News show that, under comparable or much lower communication budgets, KTA v2 consistently outperforms a local-only baseline and strong parameter-based methods (FedAvg, FedProx), and substantially improves over a FedMD-style global teacher. On CIFAR-10 with ResNet-18, KTA v2 reaches 57.7% test accuracy using approximately 1/1100 of FedAvg's communication, while on AG News it attains 89.3% accuracy with approximately 1/300 of FedAvg's traffic.

</details>


### [476] [Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy](https://arxiv.org/abs/2512.00849)
*Yunbo Long,Jiaquan Zhang,Xi Chen,Alexandra Brintrup*

Main category: cs.LG

TL;DR: 本文提出了一种名为引力联邦聚类（GFC）的新方法，通过将加噪的客户端中心点转化为全局引力势场，并利用持续同调分析提取稳定聚类中心，在不依赖迭代通信的前提下，有效解决了非独立同分布（non-IID）数据下本地差分隐私（LDP）联邦聚类的难题。


<details>
  <summary>Details</summary>
Motivation: 现有单次通信的联邦聚类方法在强LDP噪声和数据异构性下性能严重下降，难以兼顾隐私保护与聚类精度。

Method: GFC包含两个核心创新：(1) 客户端基于紧凑性感知的扰动机制，将局部簇结构编码为'质量'；(2) 服务端基于势场超水平集的持续同调分析进行拓扑聚合。理论方面推导了隐私预算ε与中心点估计误差的闭式界，并证明势场的Lipschitz平滑特性可在高密度区指数级抑制噪声。

Result: GFC在十个基准数据集上显著优于当前最优方法，尤其在强隐私约束（ε<1）下优势明显，同时在较宽松隐私预算下保持竞争力。

Conclusion: GFC通过将联邦聚类重构为物理启发空间中的拓扑持久性问题，实现了无需迭代通信的优异隐私-精度权衡，为隐私保护分布式学习提供了新范式。

Abstract: Clustering non-independent and identically distributed (non-IID) data under local differential privacy (LDP) in federated settings presents a critical challenge: preserving privacy while maintaining accuracy without iterative communication. Existing one-shot methods rely on unstable pairwise centroid distances or neighborhood rankings, degrading severely under strong LDP noise and data heterogeneity. We present Gravitational Federated Clustering (GFC), a novel approach to privacy-preserving federated clustering that overcomes the limitations of distance-based methods under varying LDP. Addressing the critical challenge of clustering non-IID data with diverse privacy guarantees, GFC transforms privatized client centroids into a global gravitational potential field where true cluster centers emerge as topologically persistent singularities. Our framework introduces two key innovations: (1) a client-side compactness-aware perturbation mechanism that encodes local cluster geometry as "mass" values, and (2) a server-side topological aggregation phase that extracts stable centroids through persistent homology analysis of the potential field's superlevel sets. Theoretically, we establish a closed-form bound between the privacy budget $ε$ and centroid estimation error, proving the potential field's Lipschitz smoothing properties exponentially suppress noise in high-density regions. Empirically, GFC outperforms state-of-the-art methods on ten benchmarks, especially under strong LDP constraints ($ε< 1$), while maintaining comparable performance at lower privacy budgets. By reformulating federated clustering as a topological persistence problem in a synthetic physics-inspired space, GFC achieves unprecedented privacy-accuracy trade-offs without iterative communication, providing a new perspective for privacy-preserving distributed learning.

</details>


### [477] [City-Conditioned Memory for Multi-City Traffic and Mobility Forecasting](https://arxiv.org/abs/2512.00851)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 本文提出了CityCond，一种轻量级的城市条件记忆层，可增强现有的时空预测模型，使其在不同城市间具有更好的适应性和准确性，尤其在数据稀缺和跨城市少样本场景下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有深度交通模型通常针对单一城市训练，难以应对不同城市的网络结构差异、数据量差异以及新城市历史数据不足的问题，导致维护成本高且迁移性能差。因此需要一个通用的、与主干模型无关的解决方案来提升多城市部署的效率和效果。

Method: 提出CityCond，结合城市ID编码器和可选的共享记忆库（CityMem），通过门控残差连接将城市特定信息融入主干模型的隐藏状态中。该方法可附加到多种主流时空模型（如GRU、TCN、Transformer、GNN、STGCN）上，并在METR-LA、PEMS-BAY和SIND数据集上进行全数据、低数据和跨城市少样本迁移实验。

Result: 在超过十四种模型变体和三次随机种子实验中，CityCond均带来一致的性能提升，尤其对高容量模型（如Transformer和STGCN）效果显著；CityMem在全数据设置下使Transformer误差降低约三分之一，并在低数据和跨城市迁移中带来显著增益；在SIND数据集上，仅城市ID条件即可小幅提升LSTM在低数据下的表现。

Conclusion: CityCond是一种可复用的设计模式，能够有效支持在现实数据约束下的可扩展多城市时空预测，降低维护成本并提升模型泛化能力。

Abstract: Deploying spatio-temporal forecasting models across many cities is difficult: traffic networks differ in size and topology, data availability can vary by orders of magnitude, and new cities may provide only a short history of logs. Existing deep traffic models are typically trained per city and backbone, creating high maintenance cost and poor transfer to data-scarce cities. We ask whether a single, backbone-agnostic layer can condition on "which city this sequence comes from", improve accuracy in full- and low-data regimes, and support better cross-city adaptation with minimal code changes.
  We propose CityCond, a light-weight city-conditioned memory layer that augments existing spatio-temporal backbones. CityCond combines a city-ID encoder with an optional shared memory bank (CityMem). Given a city index and backbone hidden states, it produces city-conditioned features fused through gated residual connections. We attach CityCond to five representative backbones (GRU, TCN, Transformer, GNN, STGCN) and evaluate three regimes: full-data, low-data, and cross-city few-shot transfer on METR-LA and PEMS-BAY. We also run auxiliary experiments on SIND, a drone-based multi-agent trajectory dataset from a signalized intersection in Tianjin (we focus on pedestrian tracks).
  Across more than fourteen model variants and three random seeds, CityCond yields consistent improvements, with the largest gains for high-capacity backbones such as Transformers and STGCNs. CityMem reduces Transformer error by roughly one third in full-data settings and brings substantial gains in low-data and cross-city transfer. On SIND, simple city-ID conditioning modestly improves low-data LSTM performance. CityCond can therefore serve as a reusable design pattern for scalable, multi-city forecasting under realistic data constraints.

</details>


### [478] [Robust Probabilistic Load Forecasting for a Single Household: A Comparative Study from SARIMA to Transformers on the REFIT Dataset](https://arxiv.org/abs/2512.00856)
*Midhun Manoj*

Main category: cs.LG

TL;DR: 本论文研究了在存在结构性数据缺失的情况下，如何利用不同模型对家庭用电量进行概率预测，发现Temporal Fusion Transformer（TFT）在点预测和区间预测上表现最优。


<details>
  <summary>Details</summary>
Motivation: 由于REFIT家庭用电数据集存在大规模结构性数据缺失且具有高度波动性，传统方法难以有效建模不确定性，因此需要系统评估适用于此类复杂时间序列的概率预测方法。

Method: 首先通过对比实验选择季节性插补法填补数据缺口，随后依次评估SARIMA、Prophet等经典模型、XGBoost机器学习模型以及LSTM和TFT等深度学习模型在点预测和概率预测上的表现。

Result: 实验表明经典模型无法捕捉数据的非线性和状态转换特征；LSTM提供了最校准的概率预测，而TFT在点预测精度（RMSE为481.94）和预测区间的安全性方面表现最佳，能有效覆盖极端波动。

Conclusion: Temporal Fusion Transformer是处理高波动、含缺失值的家庭用电负荷数据的首选模型，兼具高预测精度与稳健的不确定性量化能力。

Abstract: Probabilistic forecasting is essential for modern risk management, allowing decision-makers to quantify uncertainty in critical systems. This paper tackles this challenge using the volatile REFIT household dataset, which is complicated by a large structural data gap. We first address this by conducting a rigorous comparative experiment to select a Seasonal Imputation method, demonstrating its superiority over linear interpolation in preserving the data's underlying distribution. We then systematically evaluate a hierarchy of models, progressing from classical baselines (SARIMA, Prophet) to machine learning (XGBoost) and advanced deep learning architectures (LSTM). Our findings reveal that classical models fail to capture the data's non-linear, regime-switching behavior. While the LSTM provided the most well-calibrated probabilistic forecast, the Temporal Fusion Transformer (TFT) emerged as the superior all-round model, achieving the best point forecast accuracy (RMSE 481.94) and producing safer, more cautious prediction intervals that effectively capture extreme volatility.

</details>


### [479] [The Spectral Dimension of NTKs is Constant: A Theory of Implicit Regularization, Finite-Width Stability, and Scalable Estimation](https://arxiv.org/abs/2512.00860)
*Praveen Anilkumar Shukla*

Main category: cs.LG

TL;DR: 本文研究了深度神经网络在初始化时的内在复杂度，通过神经正切核（NTK）Gram矩阵的有效秩来刻画，并证明了其在无限宽度极限下的收敛性与有限宽度下的稳定性，同时提出了一种可扩展的无偏估计器并在CIFAR-10上验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 现代深度网络参数量巨大却仍能良好泛化，暗示其存在未被参数数量反映的低内在复杂度；本文旨在从初始化阶段出发，利用NTK有效秩量化该复杂度。

Method: 理论方面：分析i.i.d.数据下无限宽度NTK的有效秩极限律及有限宽度稳定性；方法上设计基于随机输出探针和CountSketch的可扩展估计器，并给出无偏性、一致性与方差界证明。

Result: 理论上证明了有效秩在无限宽度下收敛于常数r_∞，且具有子高斯集中性；有限宽度下其变化为O_p(m^{-1/2})；实验上在CIFAR-10上的ResNet-20/56显示r_eff≈1.0–1.3且几乎不随样本量n变化，与理论一致。

Conclusion: NTK有效秩是一个稳定、可估计且具理论保障的内在复杂度度量，能在初始化阶段揭示深度网络的低维本质，为理解过参数化模型的泛化提供新视角。

Abstract: Modern deep networks are heavily overparameterized yet often generalize well, suggesting a form of low intrinsic complexity not reflected by parameter counts. We study this complexity at initialization through the effective rank of the Neural Tangent Kernel (NTK) Gram matrix, $r_{\text{eff}}(K) = (\text{tr}(K))^2/\|K\|_F^2$. For i.i.d. data and the infinite-width NTK $k$, we prove a constant-limit law $\lim_{n\to\infty} \mathbb{E}[r_{\text{eff}}(K_n)] = \mathbb{E}[k(x, x)]^2 / \mathbb{E}[k(x, x')^2] =: r_\infty$, with sub-Gaussian concentration. We further establish finite-width stability: if the finite-width NTK deviates in operator norm by $O_p(m^{-1/2})$ (width $m$), then $r_{\text{eff}}$ changes by $O_p(m^{-1/2})$. We design a scalable estimator using random output probes and a CountSketch of parameter Jacobians and prove conditional unbiasedness and consistency with explicit variance bounds. On CIFAR-10 with ResNet-20/56 (widths 16/32) across $n \in \{10^3, 5\times10^3, 10^4, 2.5\times10^4, 5\times10^4\}$, we observe $r_{\text{eff}} \approx 1.0\text{--}1.3$ and slopes $\approx 0$ in $n$, consistent with the theory, and the kernel-moment prediction closely matches fitted constants.

</details>


### [480] [HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs](https://arxiv.org/abs/2512.00862)
*Ningning Chen,Weicai Ye,Ying Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于小波变换的高保真1比特后训练量化方法HBLLM，用于大语言模型，显著提升了量化精度并保持低开销。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型中实现高效、高保真的1比特量化以降低存储和计算成本。

Method: 利用Haar小波变换进行频域分解，结合频率感知的组内分组和基于ℓ2范数的重要列选择策略，并对非显著权重在频带内共享均值以提升存储效率。

Result: 在OPT和LLaMA模型上实验表明，HBLLM在1比特量化中达到最优性能，在LLaMA2-13B上实现了6.71的困惑度，平均权重存储仅为1.08比特。

Conclusion: HBLLM通过引入小波增强和结构感知分组策略，有效提升了1比特量化的保真度和效率，适用于大规模语言模型压缩。

Abstract: We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.

</details>


### [481] [Towards Active Synthetic Data Generation for Finetuning Language Models](https://arxiv.org/abs/2512.00884)
*Samuel Kessler,Menglin Xia,Daniel Madrigal Diaz,Dongge Han,Helia Heshemi,Saravan Rajmohan,Victor Ruehle,Jordan T. Ash*

Main category: cs.LG

TL;DR: 本文研究了在语言模型微调过程中，采用闭环迭代方式生成合成数据（即根据学生模型当前状态动态生成教师模型输出）相较于静态生成数据的优势，并发现简单的主动学习选择标准在性能上优于专门针对大语言模型设计的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常在学生模型微调前一次性生成全部合成数据，但本文动机在于探索更高效的数据生成策略——即在训练过程中根据学生模型状态动态、闭环地生成数据，以提升微调效果。

Method: 提出并评估一种闭环迭代的合成数据生成范式，对比多种数据选择策略（包括LLM专用方法和经典主动学习准则），在固定样本量或计算预算下进行实验。

Result: 在四个数学与逻辑推理数据集及四种小型语言模型上验证表明：闭环生成显著优于静态生成；且简单廉价的主动学习选择标准（如不确定性采样）性能最佳。

Conclusion: 动态、闭环的合成数据生成是一种更高效的语言模型微调数据构建方式，其优势不依赖于复杂设计，而可由经典主动学习思想实现。

Abstract: A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.

</details>


### [482] [Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models](https://arxiv.org/abs/2512.00888)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: 该论文评估了零样本基础模型（如TabPFN和TabICL）在表格数据上的硬件开销与准确性权衡，发现传统树集成模型在多数任务上精度相当或更优，且资源消耗远低于基础模型。


<details>
  <summary>Details</summary>
Motivation: 量化当前表格基础模型在实际部署中的硬件资源消耗（如延迟、内存、显存），并与其准确性进行系统性权衡分析。

Method: 构建了一个可复现的基准测试框架，在四个公开数据集上评估TabPFN-1.0、TabICL-base与XGBoost、LightGBM、随机森林的测试准确率、运行时间、CPU内存及GPU显存使用情况。

Result: 树集成模型在三个数据集上精度持平或优于基础模型，且延迟≤0.40秒，内存≤150MB，不占用显存；TabICL在Higgs数据集上精度略高但延迟高达960秒且需9GB显存；TabPFN精度相当但需4GB显存且无法处理完整Higgs数据。

Conclusion: 当前表格基础模型存在显著的硬件代价，传统树模型在效率和实用性上仍具优势，研究应关注效率优化。

Abstract: Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.

</details>


### [483] [Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs](https://arxiv.org/abs/2512.00908)
*Xinzhu Chen,Xuesheng Li,Zhongxiang Sun,Weijie Yu*

Main category: cs.LG

TL;DR: 本文提出LESS框架，一种基于低熵段正确性感知的强化学习方法，通过细粒度优势调制提升大模型推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法过度关注高熵token以驱动探索，却忽视了低熵段中蕴含的稳定、可复用的结构化推理模式；作者发现低熵段在正确响应间的重叠与模型准确率强相关，而错误响应中的重叠则呈现无益稳定性。

Method: 提出LESS（Low-Entropy Segment Selection）框架，在GRPO等RL算法基础上，对低熵段进行细粒度优势调制：增强仅出现在正确响应中的段、抑制仅出现在错误响应中的段、中和两者共有的段，同时保留高熵段的探索能力。

Result: 在三个主干模型和六个数学基准上，LESS持续超越强RL基线，显著提升准确率并增强性能下限鲁棒性。

Conclusion: 低熵段蕴含关键结构信息，其正确性感知建模比单纯依赖熵驱动更新更有效；LESS为RLVR提供了更精细、更鲁棒的奖励塑形范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.

</details>


### [484] [Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments](https://arxiv.org/abs/2512.00915)
*Junwoo Chang,Minwoo Park,Joohwan Seo,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.LG

TL;DR: 提出部分群不变MDP（PI-MDP）框架，通过选择性地应用群不变或标准Bellman备份来缓解局部对称性破坏带来的误差传播，提升强化学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境中几乎不存在完全的群不变MDP，局部对称性破坏会导致全局值估计错误，需设计能应对非对称性的方法以保留对称性带来的优势。

Method: 引入部分群不变MDP（PI-MDP），根据对称性成立的位置选择使用群不变或标准Bellman更新，并基于此提出PE-DQN和PE-SAC算法。

Result: 在Grid-World、运动和操作任务上的实验表明，PE-DQN和PE-SAC显著优于基线方法。

Conclusion: 选择性利用对称性可在存在局部对称破坏的情况下提高强化学习的鲁棒性和样本效率。

Abstract: Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.

</details>


### [485] [D-CTNet: A Dual-Branch Channel-Temporal Forecasting Network with Frequency-Domain Correction](https://arxiv.org/abs/2512.00925)
*Shaoxun Wang,Xingjun Zhang,Kun Xia,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出了一种基于Patch的双分支通道-时间网络（D-CTNet），用于提升多变量时间序列预测在非平稳和复杂工业协作环境下的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 工业协作环境中的多变量时间序列预测面临复杂的变量间依赖和由环境变化引起的非平稳分布偏移问题，现有模型难以有效应对。

Method: 设计了双分支结构，分别建模通道内时间演化和跨通道动态相关性；引入全局Patch注意力融合模块捕捉长程依赖，并提出频域平稳性校正机制，通过频谱对齐自适应抑制分布偏移。

Result: 在七个基准数据集上验证了模型的优越性，预测精度和鲁棒性均优于现有最先进方法。

Conclusion: D-CTNet能有效解耦时空依赖并增强对非平稳性的适应能力，有望成为工业协作系统中可靠的预测引擎。

Abstract: Accurate Multivariate Time Series (MTS) forecasting is crucial for collaborative design of complex systems, Digital Twin building, and maintenance ahead of time. However, the collaborative industrial environment presents new challenges for MTS forecasting models: models should decouple complex inter-variable dependencies while addressing non-stationary distribution shift brought by environmental changes. To address these challenges and improve collaborative sensing reliability, we propose a Patch-Based Dual-Branch Channel-Temporal Forecasting Network (D-CTNet). Particularly, with a parallel dual-branch design incorporating linear temporal modeling layer and channel attention mechanism, our method explicitly decouples and jointly learns intra-channel temporal evolution patterns and dynamic multivariate correlations. Furthermore, a global patch attention fusion module goes beyond the local window scope to model long range dependencies. Most importantly, aiming at non-stationarity, a Frequency-Domain Stationarity Correction mechanism adaptively suppresses distribution shift impacts from environment change by spectrum alignment. Evaluations on seven benchmark datasets show that our model achieves better forecasting accuracy and robustness compared with state-of-the-art methods. Our work shows great promise as a new forecasting engine for industrial collaborative systems.

</details>


### [486] [Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks](https://arxiv.org/abs/2512.00940)
*Susmit Agrawal,Krishn Vishwas Kher,Saksham Mittal,Swarnim Maheshwari,Vineeth N. Balasubramanian*

Main category: cs.LG

TL;DR: 本文提出MIRA框架，将Hopfield式联想记忆模块集成到共享骨干网络上，通过任务特定的目标实现领域泛化与持续学习的统一，显著提升模型的适应性与知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 受生物神经系统中神经调质动态调控单一神经回路启发，解决深度学习中领域泛化（DG）与持续学习（CL）方法割裂、缺乏联想记忆机制的问题。

Method: 提出Memory-Integrated Reconfigurable Adapters（MIRA），在共享骨干网之上叠加Hopfield风格联想记忆模块；记忆键后验学习，按样本检索并线性组合存储的适配器更新以适配任务或领域。

Result: 在标准基准测试中，MIRA在领域泛化任务中达到SOTA分布外精度，在增量学习中优于专为缓解灾难性遗忘设计的持续学习架构。

Conclusion: MIRA通过融合适配器调制与生物启发式联想记忆，实现了快速任务切换与长期知识保留，为构建更通用、记忆增强型AI系统提供了新路径。

Abstract: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds. Remarkably, they preserve knowledge of once-learned environments sans catastrophic forgetting, a phenomenon neuroscientists hypothesize, is due to a singular neural circuitry dynamically overlayed by neuromodulatory agents such as dopamine and acetylcholine. In parallel, deep learning research addresses analogous challenges via domain generalization (DG) and continual learning (CL), yet these methods remain siloed, despite the brains ability to perform them seamlessly. In particular, prior work has not explored architectures involving associative memories (AMs), which are an integral part of biological systems, to jointly address these tasks. We propose Memory-Integrated Reconfigurable Adapters (MIRA), a unified framework that integrates Hopfield-style associative memory modules atop a shared backbone. Associative memory keys are learned post-hoc to index and retrieve an affine combination of stored adapter updates for any given task or domain on a per-sample basis. By varying only the task-specific objectives, we demonstrate that MIRA seamlessly accommodates domain shifts and sequential task exposures under one roof. Empirical evaluations on standard benchmarks confirm that our AM-augmented architecture significantly enhances adaptability and retention: in DG, MIRA achieves SoTA out-of-distribution accuracy, and in incremental learning settings, it outperforms architectures explicitly designed to handle catastrophic forgetting using generic CL algorithms. By unifying adapter-based modulation with biologically inspired associative memory, MIRA delivers rapid task switching and enduring knowledge retention in a single extensible architecture, charting a path toward more versatile and memory-augmented AI systems.

</details>


### [487] [Associative Syntax and Maximal Repetitions reveal context-dependent complexity in fruit bat communication](https://arxiv.org/abs/2512.01033)
*Luigi Assom*

Main category: cs.LG

TL;DR: 本文提出了一种无监督方法，用于分析果蝠发声的离散性、句法和时序结构，发现其语音系统具有关联性句法、语境依赖性音节使用及重尾分布的最大重复模式，表明冲突情境下通信复杂度更高。


<details>
  <summary>Details</summary>
Motivation: 探究果蝠这类具有渐变特征的发声系统的通信复杂性，并理解其发声结构与行为语境的关系。

Method: 基于梅尔频谱图的流形学习改进无监督音节标注；将发声编码为音节序列分析句法类型；提取最大重复模式（MRs）评估句法结构；结合音节转移网络分析。

Result: 发现果蝠发声具有关联性句法（非组合性）、语境依赖的音节使用、MRs呈截断幂律分布（α < 2），且冲突情境下MR更长、序列互联性更强。

Conclusion: 果蝠在冲突情境中表现出更高的通信复杂度，反映信息可压缩性更低，支持通信复杂性与社会互动张力正相关的观点。

Abstract: This study presents an unsupervised method to infer discreteness, syntax and temporal structures of fruit-bats vocalizations, as a case study of graded vocal systems, and evaluates the complexity of communication patterns in relation with behavioral context. The method improved the baseline for unsupervised labeling of vocal units (i.e. syllables) through manifold learning, by investigating how dimen- sionality reduction on mel-spectrograms affects labeling, and comparing it with unsupervised labels based on acoustic similarity. We then encoded vocalizations as syllabic sequences to analyze the type of syntax, and extracted the Maximal Repetitions (MRs) to evaluate syntactical structures. We found evidence for: i) associative syntax, rather than combinatorial (context classification is unaffected by permutation of sequences, F 1 > 0.9); ii) context-dependent use of syllables (Wilcoxon rank-sum tests, p-value < 0.05); iii) heavy-tail distribution of MRs (truncated power-law, exponent α < 2), indicative of mechanism encoding com- binatorial complexity. Analysis of MRs and syllabic transition networks revealed that mother-pupil interactions were characterized by repetitions, while commu- nication in conflict-contexts exhibited higher complexity (longer MRs and more interconnected vocal sequences) than non-agonistic contexts. We propose that communicative complexity is higher in scenarios of disagreement, reflecting lower compressibility of information.

</details>


### [488] [Multi-Modal AI for Remote Patient Monitoring in Cancer Care](https://arxiv.org/abs/2512.00949)
*Yansong Liu,Ronnie Stafford,Pramit Khetrapal,Huriye Kocadag,Graça Carvalho,Patricia de Winter,Maryam Imran,Amelia Snook,Adamos Hadjivasiliou,D. Vijay Anand,Weining Lin,John Kelly,Yukun Zhou,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 本文提出了一种多模态AI远程患者监测（RPM）框架，用于癌症患者系统性治疗期间的副作用风险预测，基于真实世界异步、不完整数据，在84名患者、6080患者天的数据上验证了其可行性（AUROC=0.70），并识别出关键预测特征。


<details>
  <summary>Details</summary>
Motivation: 癌症患者在门诊间隔期间面临未被监测的副作用风险，亟需填补这一照护空白。

Method: 开发并适配了处理异步、不完整远程监测数据的多模态AI模型，整合HALO-X平台的多源数据（人口统计、可穿戴设备、每日问卷、临床事件），预测未来不良事件的连续风险。

Result: 模型准确率达83.9%（AUROC=0.70），关键预测特征包括既往治疗、健康自查和日最高心率；案例研究表明模型可在事件发生前输出渐进式风险预警。

Conclusion: 该研究证实了多模态AI驱动的远程患者监测在癌症照护中的可行性，为实现更主动的患者支持提供了新路径。

Abstract: For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop)

</details>


### [489] [WUSH: Near-Optimal Adaptive Transforms for LLM Quantization](https://arxiv.org/abs/2512.00956)
*Jiale Chen,Vage Egiazarian,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出了一种名为WUSH的新型数据感知线性变换方法，用于联合权重-激活量化，通过结合Hadamard矩阵和基于二阶矩的数据相关组件，理论上证明了其在标准无数据量化器下的最优性，并在常见数值格式下优于传统正交变换。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法中使用的固定正交变换（如Hadamard）忽略数据统计特性，无法最优地压缩动态范围，导致量化精度损失，因此需要一种数据自适应且理论最优的线性变换来改善低比特量化的有效性。

Method: 推导了适用于整数和浮点格式的AbsMax缩放块量化器下联合权值-激活量化的闭式最优线性分块变换，提出了WUSH方法，该方法结合Hadamard结构与基于数据二阶矩的自适应成分，实现高效且最优的非正交变换。

Result: 理论证明了所提WUSH变换在温和假设下的最优性，初步实验结果显示其在多种常见量化格式下 consistently 优于传统的Hadamard变换。

Conclusion: WUSH是一种结构化、可高效实现且理论上最优的数据感知线性变换，为低比特大模型部署中的联合权值-激活量化提供了更优解决方案。

Abstract: Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.

</details>


### [490] [Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning](https://arxiv.org/abs/2512.00961)
*Qi Wang,Mian Wu,Yuyang Zhang,Mingqi Yuan,Wenyao Zhang,Haoxiang You,Yunbo Wang,Xin Jin,Xiaokang Yang,Wenjun Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种利用预训练视频扩散模型提供目标驱动奖励信号的新方法，避免了人工设计奖励函数，通过视频级和帧级目标对强化学习智能体进行引导。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖人工设计的程序化奖励函数，设计困难且泛化性差；本文旨在利用预训练视频扩散模型中蕴含的丰富世界知识，自动提供通用、可迁移的目标驱动奖励。

Method: 利用预训练视频扩散模型作为奖励函数：1）微调模型后用其视频编码器评估智能体轨迹与生成目标视频的潜在表示对齐度（视频级奖励）；2）用CLIP识别生成视频中最相关帧作为目标状态，并学习前向-后向表征建模从状态-动作对到达该帧的概率（帧级奖励）。

Result: 在多个Meta-World任务上的实验验证了该方法的有效性，能生成更连贯、目标导向性更强的智能体轨迹。

Conclusion: 预训练视频扩散模型可有效充当无需人工设计的通用奖励函数，视频级与帧级奖励协同提升了强化学习的样本效率与目标达成能力。

Abstract: Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.

</details>


### [491] [Mode-Conditioning Unlocks Superior Test-Time Scaling](https://arxiv.org/abs/2512.01127)
*Chen Henry Wu,Sachin Goyal,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文提出模式条件化（ModC）框架，通过在测试时显式分配计算资源到不同推理模式，缓解并行采样中的多样性崩溃问题，显著提升大模型在各类推理任务上的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 并行采样虽能提升测试时扩展性，但受限于多样性崩溃——模型集中于少数模式，导致重复采样结果趋同、错误重复。

Method: 提出模式条件化（ModC）框架，利用专家模型或模式特定前缀显式分配测试时计算资源；结合梯度聚类实现无显式模式标签的ModC；并拓展至强化学习场景。

Result: 在图搜索任务与大规模推理基准（如OpenThoughts、NuminaMath）上一致提升扩展性；Qwen2.5-7B经ModC微调后在OpenThoughts上实现4倍效率增益且Pass@k上限提升；梯度聚类带来最高10%准确率提升；ModC还能增强多样性导向的RL方法。

Conclusion: 标准训练未能充分利用数据内在多样性，ModC提供了一种简单而有效的机制，在测试时释放多样性潜力，从而全面提升并行采样与推理性能。

Abstract: Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.

</details>


### [492] [Subgroup Validity in Machine Learning for Echocardiogram Data](https://arxiv.org/abs/2512.00976)
*Cynthia Feeney,Shane Williams,Benjamin S. Wessler,Michael C. Hughes*

Main category: cs.LG

TL;DR: 本文分析了当前公开的超声心动图数据集在性别、种族和民族等社会人口学信息报告方面的不足，发现其无法满足亚组有效性验证的需求，并通过两个数据集（TMED-2 和 MIMIC-IV-ECHO）的改进与模型亚组分析，揭示了现有模型在不同亚组中性能证据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前超声心动图数据集中患者性别、种族、民族等社会人口学信息报告严重不足，且缺乏亚组层面的性能评估，导致模型部署前存在亚组有效性隐患。

Method: 改进TMED-2和MIMIC-IV-ECHO两个数据集的社会人口学信息报告；对六个公开数据集进行统计分析；在TMED-2上对两个已发表的主动脉瓣狭窄检测模型开展探索性亚组性能分析。

Result: 六个公开数据集均未涵盖性别多元群体，多数种族/民族亚组样本量严重不足；两个模型在性别、种族、民族亚组上均缺乏充分的性能验证证据。

Conclusion: 需增加代表性不足亚组的数据量、提升人口学信息报告质量，并开展系统性亚组分析，方能支撑深度学习模型在临床中的公平、可靠部署。

Abstract: Echocardiogram datasets enable training deep learning models to automate interpretation of cardiac ultrasound, thereby expanding access to accurate readings of diagnostically-useful images. However, the gender, sex, race, and ethnicity of the patients in these datasets are underreported and subgroup-specific predictive performance is unevaluated. These reporting deficiencies raise concerns about subgroup validity that must be studied and addressed before model deployment. In this paper, we show that current open echocardiogram datasets are unable to assuage subgroup validity concerns. We improve sociodemographic reporting for two datasets: TMED-2 and MIMIC-IV-ECHO. Analysis of six open datasets reveals no consideration of gender-diverse patients and insufficient patient counts for many racial and ethnic groups. We further perform an exploratory subgroup analysis of two published aortic stenosis detection models on TMED-2. We find insufficient evidence for subgroup validity for sex, racial, and ethnic subgroups. Our findings highlight that more data for underrepresented subgroups, improved demographic reporting, and subgroup-focused analyses are needed to prove subgroup validity in future work.

</details>


### [493] [Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking](https://arxiv.org/abs/2512.01208)
*Alper Yıldırım,İbrahim Yücedağ*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型PRISM，通过将语义表示为复数域中的谐波频率，使用线性对数复杂度的门控谐波卷积替代传统的自注意力机制，解决了标准Transformer在学习新概念时面临的“可塑性-稳定性困境”。实验表明，PRISM在保持已有知识的同时能高效吸收新概念，显著优于传统Transformer。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer依赖局部梯度扩散进行语义对齐，导致训练成本高且难以适应新概念，出现“灾难性刚性”问题。作者旨在解决这一优化瓶颈及其带来的可塑性与稳定性之间的矛盾。

Method: 引入迭代语义图精炼（ISMR）作为诊断工具，并提出Phase-Resonant Intelligent Spectral Model (PRISM)，将语义身份编码为复数域中的共振频率，用O(N log N)复杂度的门控谐波卷积取代二次复杂度的自注意力。

Result: 在WMT14翻译任务中，虽然标准Transformer在静态性能上略优（23.88 vs 21.40 BLEU），但在注入新概念时发生灾难性遗忘（性能下降-10.55 BLEU，仅获得60%获取率）；而PRISM实现近乎无损的可塑性，5次少量学习获取率达96%，性能仅轻微下降（-0.84 BLEU）。

Conclusion: 谐波表示能够有效解耦记忆与推理过程，PRISM提供了一种结构性解决方案，克服了Transformer在实时知识适应中的根本性局限。

Abstract: Standard Transformers suffer from a "Semantic Alignment Tax", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates "Catastrophic Rigidity", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the "Plasticity-Stability" stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.

</details>


### [494] [Upper Approximation Bounds for Neural Oscillators](https://arxiv.org/abs/2512.01015)
*Zifeng Huang,Konstantin M. Zuev,Yong Xia,Michael Beer*

Main category: cs.LG

TL;DR: 本文研究了由二阶常微分方程（ODE）与多层感知机（MLP）构成的神经振荡器的逼近能力，推导了其对因果连续算子和一致渐近增量稳定二阶动力系统的上逼近界，并揭示其误差随MLP宽度呈多项式衰减，缓解参数复杂度诅咒。


<details>
  <summary>Details</summary>
Motivation: 理论上量化神经振荡器架构的表达能力仍具挑战性，尤其在处理长时序或连续时间函数的因果映射时。

Method: 构建由二阶ODE与MLP组成的神经振荡器模型，通过泛函分析与逼近理论推导其对因果连续算子及特定二阶动力系统的上逼近界；并将该证明方法推广至复数域连续时间状态空间模型。

Result: 得到两类逼近误差上界，且误差随两个MLP宽度的倒数呈多项式衰减；数值实验验证了误差衰减速率。

Conclusion: 为神经振荡器在科学与工程中的有效应用提供了坚实的理论基础，同时缓解了参数复杂度带来的性能瓶颈。

Abstract: Neural oscillators, originating from the second-order ordinary differential equations (ODEs), have demonstrated competitive performance in stably learning causal mappings between long-term sequences or continuous temporal functions. However, theoretically quantifying the capacities of their neural network architectures remains a significant challenge. In this study, the neural oscillator consisting of a second-order ODE followed by a multilayer perceptron (MLP) is considered. Its upper approximation bound for approximating causal and uniformly continuous operators between continuous temporal function spaces and that for approximating uniformly asymptotically incrementally stable second-order dynamical systems are derived. The established proof method of the approximation bound for approximating the causal continuous operators can also be directly applied to state-space models consisting of a linear time-continuous complex recurrent neural network followed by an MLP. Theoretical results reveal that the approximation error of the neural oscillator for approximating the second-order dynamical systems scales polynomially with the reciprocals of the widths of two utilized MLPs, thus mitigating the curse of parametric complexity. The decay rates of two established approximation error bounds are validated through two numerical cases. These results provide a robust theoretical foundation for the effective application of the neural oscillator in science and engineering.

</details>


### [495] [Operator-Theoretic Framework for Gradient-Free Federated Learning](https://arxiv.org/abs/2512.01025)
*Mohit Kumar,Mathias Brucker,Alexander Valentinitsch,Adnan Husakovic,Ali Abbas,Manuela Geiß,Bernhard A. Moser*

Main category: cs.LG

TL;DR: 提出一种基于算子理论的联邦学习框架，通过映射到再生核希尔伯特空间实现梯度无关的学习，具有低通信开销、差分隐私兼容性和全同态加密支持，且在异构环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的异构性、通信与计算限制、隐私保护问题，同时保证性能。

Method: 将L²最优解通过前向算子映射到RKHS，利用数据近似并用逆算子映射回来，构建梯度无关方案；设计基于核仿射包络机的空间折叠性质的高效核机器，客户端通过标量空间折叠度量传递知识，并结合噪声扰动数据实现一步式差分隐私协议。

Result: 推导了基于算子范数集中不等式的有限样本界，实现了低通信开销和强隐私保护下的高性能；在全球规则中仅需整数最小值和等值比较操作，支持全同态加密；在四个基准上性能达到或超过强梯度微调方法，提升最高达23.7点；核平滑缓解了高隐私设置下的精度损失。

Conclusion: 该框架为异构环境下的联邦学习提供了具有数学基础的梯度无关替代方案，具备可证明的风险、误差、鲁棒性和逼近保证，支持高效、私密且安全的知识传输。

Abstract: Federated learning must address heterogeneity, strict communication and computation limits, and privacy while ensuring performance. We propose an operator-theoretic framework that maps the $L^2$-optimal solution into a reproducing kernel Hilbert space (RKHS) via a forward operator, approximates it using available data, and maps back with the inverse operator, yielding a gradient-free scheme. Finite-sample bounds are derived using concentration inequalities over operator norms, and the framework identifies a data-dependent hypothesis space with guarantees on risk, error, robustness, and approximation. Within this space we design efficient kernel machines leveraging the space folding property of Kernel Affine Hull Machines. Clients transfer knowledge via a scalar space folding measure, reducing communication and enabling a simple differentially private protocol: summaries are computed from noise-perturbed data matrices in one step, avoiding per-round clipping and privacy accounting. The induced global rule requires only integer minimum and equality-comparison operations per test point, making it compatible with fully homomorphic encryption (FHE). Across four benchmarks, the gradient-free FL method with fixed encoder embeddings matches or outperforms strong gradient-based fine-tuning, with gains up to 23.7 points. In differentially private experiments, kernel smoothing mitigates accuracy loss in high-privacy regimes. The global rule admits an FHE realization using $Q \times C$ encrypted minimum and $C$ equality-comparison operations per test point, with operation-level benchmarks showing practical latencies. Overall, the framework provides provable guarantees with low communication, supports private knowledge transfer via scalar summaries, and yields an FHE-compatible prediction rule offering a mathematically grounded alternative to gradient-based federated learning under heterogeneity.

</details>


### [496] [Forecasting in Offline Reinforcement Learning for Non-stationary Environments](https://arxiv.org/abs/2512.01987)
*Suzan Ece Ada,Georg Martius,Emre Ugur,Erhan Oztop*

Main category: cs.LG

TL;DR: 提出FORL框架，结合条件扩散模型与零样本时间序列基础模型，应对非平稳离线强化学习中的突发时变偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法常假设环境平稳或仅考虑合成扰动，难以应对现实世界中突发的、时变的非平稳偏移，导致状态感知偏差和性能下降。

Method: 提出FORL框架，统一使用条件扩散模型生成候选状态，并引入零样本时间序列基础模型进行预测，无需预设非平稳性模式，适应未知且可能非马尔可夫的环境偏移。

Result: 在融合真实世界时间序列数据的离线RL基准上，FORL显著优于现有基线方法，表现出更强的鲁棒性和初始阶段性能。

Conclusion: FORL有效提升了离线强化学习在非平稳环境下的适应能力，通过结合零样本预测与经验，缩小了理论方法与现实复杂场景之间的差距。

Abstract: Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.

</details>


### [497] [Stabilizing Reinforcement Learning with LLMs: Formulation and Practices](https://arxiv.org/abs/2512.01374)
*Chujie Zheng,Kai Dang,Bowen Yu,Mingze Li,Huiqiang Jiang,Junrong Lin,Yuqiong Liu,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型的强化学习新框架，通过一阶近似分析了在何种条件下可使用词元级目标有效优化序列级奖励，并揭示了重要性采样、裁剪及路由重放等技术对训练稳定性的作用机制。


<details>
  <summary>Details</summary>
Motivation: 解释为何及在什么条件下，策略梯度方法（如REINFORCE）中可用词元级代理目标有效优化真实的序列级奖励，从而为稳定RL训练提供理论依据。

Method: 基于一阶近似理论分析，结合重要性采样校正、裁剪和Routing Replay等技术，在30B MoE模型上开展大规模on-policy与off-policy实验。

Result: 发现on-policy下重要性采样校正最稳定；off-policy下需联合使用裁剪与Routing Replay以缓解策略陈旧带来的不稳定性；训练稳定后，长期优化效果与冷启动初始化无关。

Conclusion: 训练-推理差异与策略陈旧性最小化是词元级代理目标有效的关键前提；所提出的稳定训练方案可推广至大模型RL优化实践。

Abstract: This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.

</details>


### [498] [AltNet: Addressing the Plasticity-Stability Dilemma in Reinforcement Learning](https://arxiv.org/abs/2512.01034)
*Mansi Maheshwari,John C. Raisbeck,Bruno Castro da Silva*

Main category: cs.LG

TL;DR: 本文提出AltNet，一种基于双网络交替重置的强化学习方法，旨在恢复神经网络在持续学习中的可塑性，同时避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 神经网络在强化学习中随时间推移会出现可塑性损失，导致持续学习能力下降；现有周期性重置方法虽有效但会引起性能骤降，在安全关键场景中存在风险。

Method: 提出AltNet，采用双网络架构：一个网络作为主动网络在线与环境交互并学习，另一个作为被动网络通过离策略方式从主动网络的经验和回放缓冲区中学习；在固定间隔时，主动网络被重置，被动网络接替成为新的主动网络。

Result: 在DeepMind Control Suite多个高维控制任务上，AltNet显著提升样本效率与最终性能，优于多种基线及现有重置方法，且全程无性能下降。

Conclusion: AltNet通过双网络角色交替机制，在不牺牲性能稳定性前提下成功恢复可塑性，为安全关键型持续强化学习提供了可靠新范式。

Abstract: Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.

</details>


### [499] [ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation](https://arxiv.org/abs/2512.01457)
*Rohin Manvi,Joey Hong,Tim Seyde,Maxime Labonne,Mathias Lechner,Sergey Levine*

Main category: cs.LG

TL;DR: ZIP-RC是一种零开销的自适应推理方法，通过复用前向传播中的logits同时预测奖励和计算成本，实现对推理过程的实时内省，提升大模型在数学任务上的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏对自身推理成功与否及所需计算量的预判能力，导致在推理过程中无法智能决策采样策略、资源分配和终止时机，影响效率与可信度。

Method: 提出ZIP-RC方法，在每个token生成时复用模型中保留或未使用的logits，联合预测最终奖励和剩余长度分布，基于此计算采样效用，并通过元动作动态决定是否继续或重启采样，无需额外模型或计算开销。

Result: 在混合难度的数学基准上，ZIP-RC在相等或更低平均成本下比多数投票法最高提升12%的准确性，并能在质量、计算量和延迟之间实现平滑的帕累托权衡。

Conclusion: ZIP-RC通过提供实时的奖励-成本内省能力，使大语言模型能够进行高效、自适应的推理，解决了现有方法在成本、延迟和缺乏信心信号方面的局限性。

Abstract: Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.

</details>


### [500] [FMTK: A Modular Toolkit for Composable Time Series Foundation Model Pipelines](https://arxiv.org/abs/2512.01038)
*Hetvi Shastri,Pragya Sharma,Walid A. Hanafy,Mani Srivastava,Prashant Shenoy*

Main category: cs.LG

TL;DR: 本文介绍了FMTK，一个开源、轻量且可扩展的工具包，用于构建和微调时间序列基础模型（TSFM）管道，通过标准化的骨干网络和组件抽象提升模块化与可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM管道组装通常依赖于特定模型的临时实现，缺乏模块化和可复现性。

Method: 提出FMTK工具包，提供标准化的骨干网络与组件抽象，支持灵活组合不同模型与任务。

Result: FMTK实现了高正确性与高性能，平均仅需七行代码即可完成TSFM管道构建与微调。

Conclusion: FMTK显著提升了TSFM研究与应用的开发效率、模块化程度与可复现性。

Abstract: Foundation models (FMs) have opened new avenues for machine learning applications due to their ability to adapt to new and unseen tasks with minimal or no further training. Time-series foundation models (TSFMs) -- FMs trained on time-series data -- have shown strong performance on classification, regression, and imputation tasks. Recent pipelines combine TSFMs with task-specific encoders, decoders, and adapters to improve performance; however, assembling such pipelines typically requires ad hoc, model-specific implementations that hinder modularity and reproducibility. We introduce FMTK, an open-source, lightweight and extensible toolkit for constructing and fine-tuning TSFM pipelines via standardized backbone and component abstractions. FMTK enables flexible composition across models and tasks, achieving correctness and performance with an average of seven lines of code. https://github.com/umassos/FMTK

</details>


### [501] [Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs](https://arxiv.org/abs/2512.01054)
*MohammadParsa Dini,Human Jafari,Sajjad Amini,MohammadMahdi Mojahedian*

Main category: cs.LG

TL;DR: 本文提出Adaptive-lambda SISS方法，将扩散模型中的固定混合权重lambda动态化为隐变量，并通过轻量推理网络根据即时损失信息自适应推断，显著提升机器遗忘效果与生成质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法（如Static-lambda SISS）使用固定lambda，无法适应不同样本和训练阶段所需的遗忘强度变化，难以兼顾遗忘效果与模型保留性能。

Method: 提出Adaptive-lambda SISS：将lambda建模为条件于SISS瞬时损失及其梯度的隐变量，由轻量推理网络参数化其自适应后验分布；联合优化扩散模型与lambda推断机制；进一步扩展至基于分数的遗忘、多类Score Forgetting Distillation、混合目标及强化学习框架。

Result: 在增强MNIST基准上，Adaptive-lambda SISS显著优于Static-lambda SISS，在更强遗忘被遗忘类别同时更好保持保留集的生成质量。

Conclusion: 动态调节lambda是提升生成模型机器遗忘性能的关键；所提自适应机制及多种拓展方向（如混合目标、RL formulation）为高效、可控的机器遗忘提供了新范式。

Abstract: Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.
  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.
  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.
  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.

</details>


### [502] [HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment](https://arxiv.org/abs/2512.01659)
*Valentin Noël,Elimane Yassine Seidou,Charly Ken Capo-Chichi,Ghanem Amari*

Main category: cs.LG

TL;DR: 本文提出HalluGraph，一种基于图论的幻觉检测框架，通过知识图谱的结构对齐来量化法律AI系统中的幻觉，提供可解释、有界且可追溯的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似度的幻觉检测方法在法律场景中存在严重缺陷，尤其无法识别关键实体（如当事人、日期、条款）的替换，而这类错误可能导致实质性后果，亟需高保真、可验证的问责机制。

Method: 构建HalluGraph框架，从查询、上下文和生成响应中分别抽取知识图谱，定义两个可分解指标：实体接地（EG）衡量响应中实体是否真实出现在源文档中；关系保持（RP）验证所断言的关系是否被上下文支持。

Result: 在结构化控制文档上AUC达0.979；在复杂法律生成任务中仍保持约0.89的AUC，显著优于语义相似度基线，并支持端到端审计追踪。

Conclusion: HalluGraph为法律RAG系统提供了具备透明性、可解释性和可追溯性的幻觉评估新范式，满足高风险法律应用的问责要求。

Abstract: Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.

</details>


### [503] [PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting](https://arxiv.org/abs/2512.01062)
*Seokhyun Chin,Junghwan Park,Woojin Cho*

Main category: cs.LG

TL;DR: 本文提出了一种基于卫星图像和物理约束的降水临近预报方法，通过物理信息神经网络（PINN）结合对流-扩散方程约束的双神经算子（PIANO）结构提升预测精度与物理一致性，并利用生成模型将卫星图转为雷达图以实现降水预测。


<details>
  <summary>Details</summary>
Motivation: 现有降水临近预报方法计算成本高、适用性受限，难以在许多国家推广；需兼顾准确性与物理一致性。

Method: 提出物理信息神经算子（PIANO）结构，在训练中引入对流-扩散方程作为物理约束（PINN损失），并结合生成模型将卫星图像转换为雷达图像用于降水预测。

Result: 相比基线模型，在中等（4mm/h）和短时强降水（8mm/h）事件预测上显著提升，且季节变化小、泛化鲁棒性强。

Conclusion: PIANO为物理信息驱动的降水临近预报提供了有效新范式，具备广泛应用潜力。

Abstract: Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.

</details>


### [504] [Bayesian dynamic scheduling of multipurpose batch processes under incomplete look-ahead information](https://arxiv.org/abs/2512.01093)
*Taicheng Zheng,Dan Li,Jie Li*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯网络的动态调度方法，用于多用途批处理过程的动态重调度，能够在不完全预知信息下有效降低长期成本和系统不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有重调度方法通常假设完全预知未来扰动信息，与实际中信息不完整的情况不符，导致长期成本较高和系统不稳定性增加。因此需要一种适应不完全信息的动态调度策略。

Method: 构建一个贝叶斯网络模型，学习扰动概率分布以量化各操作受扰动影响的可能性；在线执行时根据新观测到的扰动更新后验分布，并据此指导重调度决策。

Result: 在四个基准问题上相比周期性重调度策略，所提方法在长期成本和系统不稳定性方面均取得统计显著更优的结果；理论上证明当扰动相互独立时，影响变量满足贝叶斯网络的独立性假设。

Conclusion: 该方法能有效应对不完全预知环境下的动态扰动，在降低长期成本和系统不稳定性方面优于传统方法，且具备扩展至其他调度问题（如作业车间调度和连续过程）的潜力。

Abstract: Multipurpose batch processes become increasingly popular in manufacturing industries since they adapt to low-volume, high-value products and shifting demands. These processes often operate in a dynamic environment, which faces disturbances such as processing delays and demand changes. To minimise long-term cost and system nervousness (i.e., disruptive changes to schedules), schedulers must design rescheduling strategies to address such disturbances effectively. Existing methods often assume complete look-ahead information over the scheduling horizon. This assumption contrasts with realistic situations where schedulers can only access incomplete look-ahead information. Sticking with existing methods may lead to suboptimal long-term costs and high-level system nervousness. In this work we propose a Bayesian dynamic scheduling method. Our method relies on learning a Bayesian Network from the probability distribution of disturbances. Specifically, the Bayesian Network represents how likely each operation will be impacted by disturbances. During the online execution, when new disturbances become observed, this method updates the posterior distribution and therefore guides the rescheduling strategy. We compare our method with the existing periodic rescheduling strategy (which generates new schedules from scratch at fixed intervals) on four benchmark problems. Computational results show that our method achieves statistically better long-term costs and system nervousness. In the theoretical aspect, we prove that if disturbances are mutually independent, the impact-quantifying variables inherently satisfy the independence assumptions required by Bayesian Networks. As an implication, practitioners can extend the method to other scheduling problems (such as job shop scheduling and continuous processes), given that they define the problem-specific dependencies between operations.

</details>


### [505] [Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning](https://arxiv.org/abs/2512.01113)
*Dongyue Li,Zhenshuo Zhang,Minxuan Duan,Edgar Dobriban,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文提出分支神经网络（Branching Neural Networks）和AutoBRANE算法，用于多任务算法推理，通过梯度驱动的任务聚类与凸松弛优化，显著提升性能、降低计算开销，并在图算法与文本推理基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有模型在联合训练多个算法推理任务时因执行步骤差异导致负迁移，缺乏能统一高效处理多任务的架构。

Method: 提出分支神经网络架构，并设计AutoBRANE算法：在每层通过凸松弛近似最优任务划分，利用梯度相似性进行任务聚类，支持任意基模型（GNN/LLM）。

Result: 在CLRS基准上相对最强多任务GNN提升3.7%，运行时间降48%、内存降26%；在文本图推理任务上提升3.2%；在大规模图数据（21M边、500任务）上准确率提升28%，运行时间减少4.5倍。

Conclusion: 分支结构结合AutoBRANE搜索策略可有效缓解多任务干扰，实现高性能、高效率的通用算法推理框架，且学习到的任务层次结构具有可解释性。

Abstract: Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together.
  We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs.
  We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\times$ reduction in runtime.

</details>


### [506] [Agentic Policy Optimization via Instruction-Policy Co-Evolution](https://arxiv.org/abs/2512.01945)
*Han Zhou,Xingchen Wan,Ivan Vulić,Anna Korhonen*

Main category: cs.LG

TL;DR: 本文提出了INSPO，一种指令-策略协同进化框架，通过在强化学习过程中动态优化指令，提升大语言模型代理的多轮推理与工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态人工设计指令的强化学习方法可能无法适应策略演化，限制了代理的推理性能，因此需要动态优化指令以匹配不断改进的策略。

Method: INSPO维护一个动态的指令候选池，在强化学习循环中根据奖励信号评估并淘汰低效指令；通过基于回放缓冲区的on-policy反思机制，利用LLM生成并验证新指令。

Result: 在多轮检索与推理任务上的实验表明，INSPO显著优于使用静态指令的强基线方法，能发现更优的指令策略，带来显著性能提升且仅增加少量计算开销。

Conclusion: INSPO实现了指令与策略的协同进化，为构建更高效、自适应的智能代理提供了新方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.

</details>


### [507] [World Model Robustness via Surprise Recognition](https://arxiv.org/abs/2512.01119)
*Geigh Zollicoffer,Tanush Chopra,Mingkuan Yan,Xiaoxu Ma,Kenneth Eaton,Mark Riedl*

Main category: cs.LG

TL;DR: 提出一种利用世界模型的“惊讶度”来减轻噪声影响的方法，提升强化学习智能体在异常分布噪声下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的AI系统面临各种不可预见的异常噪声，传统鲁棒训练难以覆盖所有情况，需要更通用的抗噪机制。

Method: 基于世界模型的惊讶度，提出多表示和单表示拒绝采样方法，用于在存在传感器故障等噪声时过滤干扰。

Result: 在CARLA和Safety Gymnasium等多个自动驾驶仿真环境中，该方法在不同类型和强度的噪声下均优于基线，保持了智能体性能，并提升了Cosmos和DreamerV3两种先进世界模型的稳定性。

Conclusion: 所提方法具有跨世界模型架构的通用性和强鲁棒性，能有效应对现实部署中的分布外噪声问题。

Abstract: AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .

</details>


### [508] [Projection-Free CNN Pruning via Frank-Wolfe with Momentum: Sparser Models with Less Pretraining](https://arxiv.org/abs/2512.01147)
*Hamza ElMokhtar Shili,Natasha Patnaik,Isabelle Ruble,Kathryn Jarjoura,Daniel Suarez Aguirre*

Main category: cs.LG

TL;DR: 本文研究了Frank-Wolfe（FW）优化方法的算法变体在卷积神经网络剪枝中的应用，基于“彩票假说”，提出并验证了一种带动量的FW剪枝方法，在MNIST上实现了比原始稠密模型和简单剪枝基线更稀疏、更准确的子网络，且无需完整预训练。


<details>
  <summary>Details</summary>
Motivation: 受“彩票假说”启发，即大型预训练网络中存在性能优异的小型子网络，本文聚焦于CNN图像分类任务的高效剪枝方法。

Method: 比较了三种剪枝方法：简单幅值剪枝、Frank-Wolfe风格剪枝、以及带momentum的Frank-Wolfe剪枝；在MNIST数据集上训练CNN，并系统评估不同预训练轮数（1–10 epoch）下的剪枝效果。

Result: FW with momentum方法所得剪枝网络在测试准确率和稀疏性上均优于原始稠密模型及简单剪枝基线，且推理开销极小；仅需少量预训练（few epochs）即可达到高性能。

Conclusion: FW with momentum是一种高效的CNN剪枝策略，可减少对完整预训练的依赖，在保持甚至提升性能的同时显著提升模型稀疏性与推理效率。

Abstract: We investigate algorithmic variants of the Frank-Wolfe (FW) optimization method for pruning convolutional neural networks. This is motivated by the "Lottery Ticket Hypothesis", which suggests the existence of smaller sub-networks within larger pre-trained networks that perform comparatively well (if not better). Whilst most literature in this area focuses on Deep Neural Networks more generally, we specifically consider Convolutional Neural Networks for image classification tasks. Building on the hypothesis, we compare simple magnitude-based pruning, a Frank-Wolfe style pruning scheme, and an FW method with momentum on a CNN trained on MNIST. Our experiments track test accuracy, loss, sparsity, and inference time as we vary the dense pre-training budget from 1 to 10 epochs. We find that FW with momentum yields pruned networks that are both sparser and more accurate than the original dense model and the simple pruning baselines, while incurring minimal inference-time overhead in our implementation. Moreover, FW with momentum reaches these accuracies after only a few epochs of pre-training, indicating that full pre-training of the dense model is not required in this setting.

</details>


### [509] [Dynamic Algorithm for Explainable k-medians Clustering under lp Norm](https://arxiv.org/abs/2512.01150)
*Konstantin Makarychev,Ilias Papanikolaou,Liren Shan*

Main category: cs.LG

TL;DR: 本文提出了首个适用于任意lp范数（p≥1）的可解释k-中位数聚类算法，实现了O(p(log k)^{1 + 1/p - 1/p^2})近似比，并拓展至动态场景，支持高效增删更新。


<details>
  <summary>Details</summary>
Motivation: 现有可解释k-中位数聚类算法仅适用于p=1和p=2，缺乏对一般lp范数的统一处理，且难以适应动态数据流场景。

Method: 设计了一种基于阈值决策树的新算法，通过单特征阈值划分实现可解释性；并将其扩展至动态设置，利用数据结构优化更新开销。

Result: 在任意p≥1下取得O(p(log k)^{1 + 1/p - 1/p^2})近似比；动态版本具有O(d log^3 k)摊还更新时间和O(log k)资源调用。

Conclusion: 该工作统一并改进了可解释k-中位数聚类的理论界，同时首次支持高效动态维护，提升了实际部署可行性。

Abstract: We study the problem of explainable k-medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into k clusters while minimizing the k-medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable k-medians under lp norm for every finite p >= 1. Our algorithm achieves an O(p(log k)^{1 + 1/p - 1/p^2}) approximation to the optimal k-medians cost for any p >= 1. Previously, algorithms were known only for p = 1 and p = 2. For p = 2, our algorithm improves upon the existing bound of O(log^{3/2}k), and for p = 1, it matches the tight bound of log k + O(1) up to a multiplicative O(log log k) factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time O(d log^3 k) and O(log k) recourse, making it suitable for large-scale and evolving datasets.

</details>


### [510] [AlignSAE: Concept-Aligned Sparse Autoencoders](https://arxiv.org/abs/2512.02004)
*Minglai Yang,Xinyu Guo,Mihai Surdeanu,Liangming Pan*

Main category: cs.LG

TL;DR: AlignSAE是一种通过‘预训练+后训练’课程将稀疏自编码器（SAE）特征与定义本体对齐的方法，提升了特征的可解释性和概念控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏自编码器（SAE）在将隐层特征与人类定义的概念对齐方面表现不佳，导致特征纠缠和分布表示，难以解释和控制。

Method: 提出AlignSAE方法，采用‘先无监督预训练，再有监督后训练’的两阶段训练策略，将特定概念绑定到专用的潜在槽位，同时保持其余容量用于通用重建。

Result: 实验结果表明，AlignSAE能够实现精确的因果干预（如可靠的‘概念交换’），通过定位单一语义对齐的槽位来实现对特定关系的检查与控制。

Conclusion: AlignSAE通过结构化对齐增强了SAE的可解释性与可控性，为大语言模型中知识的显式操控提供了有效接口。

Abstract: Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a "pre-train, then post-train" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable "concept swaps", by targeting single, semantically aligned slots.

</details>


### [511] [Fiber Bundle Networks: A Geometric Machine Learning Paradigm](https://arxiv.org/abs/2512.01151)
*Dong Liu*

Main category: cs.LG

TL;DR: 本文提出Fiber Bundle Networks（FiberNet），将微分几何中的纤维丛理论引入机器学习，通过可学习的黎曼度量和变分原型优化，在纤维丛上实现几何可解释的分类。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度神经网络黑箱拟合导致的不可解释性与效率难以兼顾的问题，探索几何结构驱动的可解释机器学习框架。

Method: 将分类问题建模为纤维丛上的几何优化：类别构成底空间，小波变换特征位于对应纤维中；引入可学习黎曼度量选择重要频率成分；通过能量函数最小化进行变分原型优化；最终基于黎曼度量进行Voronoi镶嵌完成分类。

Result: 实现了兼具几何可解释性与计算效率的分类框架，在保持性能的同时提供清晰的决策区域划分机制。

Conclusion: 纤维丛与机器学习的融合为构建可解释、高效的学习模型提供了新范式，突破了传统深度学习在可解释性与效率之间的权衡限制。

Abstract: We propose Fiber Bundle Networks (FiberNet), a novel machine learning framework integrating differential geometry with machine learning. Unlike traditional deep neural networks relying on black-box function fitting, we reformulate classification as interpretable geometric optimization on fiber bundles, where categories form the base space and wavelet-transformed features lie in the fibers above each category. We introduce two innovations: (1) learnable Riemannian metrics identifying important frequency feature components, (2) variational prototype optimization through energy function minimization. Classification is performed via Voronoi tessellation under the learned Riemannian metric, where each prototype defines a decision region and test samples are assigned to the nearest prototype, providing clear geometric interpretability. This work demonstrates that the integration of fiber bundle with machine learning provides interpretability and efficiency, which are difficult to obtain simultaneously in conventional deep learning.

</details>


### [512] [Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution](https://arxiv.org/abs/2512.01152)
*Shravan Chaudhari,Yoav Wald,Suchi Saria*

Main category: cs.LG

TL;DR: 本文提出了一种名为\ours{}的新方法，能够在背景分布发生变化的情况下仍保证开放集识别的性能，并通过理论分析和实验验证了其在图像和文本数据上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的开放集识别方法大多假设已知类别的数据分布（即背景分布）是固定的，但在实际应用中，数据分布会动态变化，导致模型性能下降。因此，需要一种能在背景分布发生偏移时依然保持良好表现的开放集识别方法。

Method: \ours{}方法基于新颖类别与非新颖类别可分的假设，通过理论设计确保在背景分布变化时仍能正确识别未知类别；同时引入可扩展且鲁棒的技术以适应大规模数据场景。

Result: 在简化过参数化设定下，理论证明\ours{}优于代表性基线方法；在图像和文本数据上的实验表明，该方法在背景分布偏移情况下显著优于现有开放集识别方法，并揭示了新颖类别规模等因素对性能的影响。

Conclusion: \ours{}是一种在背景分布变化情况下仍能有效进行开放集识别的方法，具备理论保证和实证支持，为现实世界中动态环境下的机器学习系统提供了更强的适应能力。

Abstract: As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution of known categories may change. Guarantees on open-set recognition are mostly derived under the assumption that the distribution of known classes, which we call \emph{the background distribution}, is fixed. In this paper we develop \ours{}, a method that is guaranteed to solve open-set recognition even in the challenging case where the background distribution shifts. We prove that the method works under benign assumptions that the novel class is separable from the non-novel classes, and provide theoretical guarantees that it outperforms a representative baseline in a simplified overparameterized setting. We develop techniques to make \ours{} scalable and robust, and perform comprehensive empirical evaluations on image and text data. The results show that \ours{} significantly outperforms existing open-set recognition methods under background shift. Moreover, we provide new insights into how factors such as the size of the novel class influences performance, an aspect that has not been extensively explored in prior work.

</details>


### [513] [From Regression to Classification: Exploring the Benefits of Categorical Representations of Energy in MLIPs](https://arxiv.org/abs/2512.01160)
*Ahmad Ali*

Main category: cs.LG

TL;DR: 本文提出了一种基于多类别分类的机器学习原子间势（MLIP）新方法，通过预测能量/力的直方图分布并使用交叉熵损失进行训练，实现了与标量回归基线相当的绝对误差性能，并能通过预测分布的熵来量化模型的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有MLIP多采用标量回归方式，虽高效但缺乏对模型不确定性的量化能力；本文旨在引入分类框架以提供更丰富的监督信号并实现不确定性估计。

Method: 将能量和力的目标值转化为直方图形式，构建多类分类任务，用交叉熵损失训练模型以预测类别分布；通过预测分布的熵衡量认知不确定性。

Result: 该分类方法在绝对误差上可媲美回归基线，同时能自然地输出不确定性估计（如预测熵），为MLIP提供了新的评估与应用维度。

Conclusion: 多类别分类 formulation 是一种有前景的 MLIP 建模范式，兼顾精度与不确定性建模能力，拓展了传统回归方法的局限性。

Abstract: Density Functional Theory (DFT) is a widely used computational method for estimating the energy and behavior of molecules. Machine Learning Interatomic Potentials (MLIPs) are models trained to approximate DFT-level energies and forces at dramatically lower computational cost. Many modern MLIPs rely on a scalar regression formulation; given information about a molecule, they predict a single energy value and corresponding forces while minimizing absolute error with DFT's calculations. In this work, we explore a multi-class classification formulation that predicts a categorical distribution over energy/force values, providing richer supervision through multiple targets. Most importantly, this approach offers a principled way to quantify model uncertainty.
  In particular, our method predicts a histogram of the energy/force distribution, converts scalar targets into histograms, and trains the model using cross-entropy loss. Our results demonstrate that this categorical formulation can achieve absolute error performance comparable to regression baselines. Furthermore, this representation enables the quantification of epistemic uncertainty through the entropy of the predicted distribution, offering a measure of model confidence absent in scalar regression approaches.

</details>


### [514] [2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI](https://arxiv.org/abs/2512.01163)
*Soumyadeep Chandra,Sayeed Shafayet Chowdhury,Kaushik Roy*

Main category: cs.LG

TL;DR: 本文提出了一种名为ThermAl的物理信息驱动生成式AI框架，用于在芯片设计早期快速、准确地预测全芯片瞬态和稳态温度分布，相比传统FEM方法提速约200倍，RMSE仅0.71°C。


<details>
  <summary>Details</summary>
Motivation: 传统基于FEM的热仿真精度高但计算开销大，难以支撑早期设计迭代；而晚期发现热问题将导致多次返工，亟需一种兼顾速度与物理合理性的快速热分析方法。

Method: 提出ThermAl框架，采用融合位置编码与玻尔兹曼正则项的混合U-Net架构，以活动剖面为输入，直接生成热分布图；训练数据来自COMSOL仿真的多尺度电路（从基本门电路到复杂设计）热耗散图谱。

Result: 在大型电路上实现0.71°C RMSE的高精度预测，速度比FEM快约200倍；在25–95°C宽温区交叉验证中保持<2.2%满量程RMSE，展现出强泛化能力。

Conclusion: ThermAl为EDA流程早期热感知设计提供了高效可靠的AI替代方案，显著提升热可靠性评估效率，尤其适用于热点快速识别与热模式学习，但暂不覆盖后布局签核所需的>85°C精细可靠性分析。

Abstract: Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations offer high accuracy but computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose 'ThermAl', a physics-informed generative AI framework which effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated via COMSOL. Experimental results demonstrate that ThermAl delivers precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71°C, and outperforms conventional FEM tools by running up to ~200 times faster. We analyze performance across diverse layouts and workloads, and discuss its applicability to large-scale EDA workflows. While thermal reliability assessments often extend beyond 85°C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range 25-55°C, we additionally performed cross-validation on an extended dataset spanning 25-95°C maintaining a high accuracy (<2.2% full-scale RMSE) even under elevated temperature conditions representative of peak power and stress scenarios.

</details>


### [515] [A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems](https://arxiv.org/abs/2512.01167)
*Mohamed Abdallah Salem,Manuel Cuevas Perez,Ahmed Harb Rabia*

Main category: cs.LG

TL;DR: 本文提出了一种基于无模型Q学习的自适应LED照明控制策略，运行于低功耗微控制器上，利用LDR传感器反馈实现实时亮度调节，成功在64状态空间中稳定至13个目标光强等级，验证了轻量级边缘强化学习在资源受限农业环境中的可行性。


<details>
  <summary>Details</summary>
Motivation: 为实现资源受限农业系统中节能、自适应的照明控制，需在低功耗微控制器上部署轻量级智能控制方法，避免依赖高算力云端或复杂建模。

Method: 采用无模型Q-learning算法，在嵌入式平台上实时根据LDR传感器读数（映射为64状态）动态调整LED亮度；设定13个目标光强等级（L1–L13），每等级训练10轮共130次试验；通过收敛速度、步数与时长评估性能，并用箱线图与直方图分析跨目标的学习效率分布。

Result: 系统能快速、平滑收敛至各目标光强等级，超调小，鲁棒性强，可应对环境扰动；实验证实了在微控制器上部署RL进行实时闭环控制的可行性与有效性。

Conclusion: 轻量级、模型无关的强化学习可在低功耗边缘设备上成功实现自适应照明控制，为多模态农业环境调控（如光-温-湿协同控制）提供了可扩展的技术基础。

Abstract: This study presents a reinforcement learning (RL)-based control strategy for adaptive lighting regulation in controlled environments using a low-power microcontroller. A model-free Q-learning algorithm was implemented to dynamically adjust the brightness of a Light-Emitting Diode (LED) based on real-time feedback from a light-dependent resistor (LDR) sensor. The system was trained to stabilize at 13 distinct light intensity levels (L1 to L13), with each target corresponding to a specific range within the 64-state space derived from LDR readings. A total of 130 trials were conducted, covering all target levels with 10 episodes each. Performance was evaluated in terms of convergence speed, steps taken, and time required to reach target states. Box plots and histograms were generated to analyze the distribution of training time and learning efficiency across targets. Experimental validation demonstrated that the agent could effectively learn to stabilize at varying light levels with minimal overshooting and smooth convergence, even in the presence of environmental perturbations. This work highlights the feasibility of lightweight, on-device RL for energy-efficient lighting control and sets the groundwork for multi-modal environmental control applications in resource-constrained agricultural systems.

</details>


### [516] [Data assimilation and discrepancy modeling with shallow recurrent decoders](https://arxiv.org/abs/2512.01170)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 本文提出了一种名为DA-SHRED的机器学习框架，用于数据同化，通过结合浅层循环解码器与稀疏非线性动力学识别，在仿真模型与真实传感器数据之间架起桥梁，提升高维时空场的状态重建精度并恢复缺失物理机制。


<details>
  <summary>Details</summary>
Motivation: 现有仿真模型存在忽略小尺度过程、对扰动敏感、参数关联简化等问题，导致其与真实传感器观测结果偏差大，亟需有效数据同化方法弥合仿真与现实（SIM2REAL）差距。

Method: 提出DA-SHRED框架：1）利用SHRED从简化仿真模型中学习低维潜在空间；2）用稀疏传感器数据更新潜在变量以重构全状态；3）在潜在空间中引入稀疏识别非线性动力学（SINDy）回归模型，识别仿真中缺失的动力学项。

Result: DA-SHRED成功缩小SIM2REAL差距，并在高度复杂系统中准确恢复缺失动力学；验证了高效时序编码与物理信息校正结合可实现鲁棒数据同化。

Conclusion: DA-SHRED是一种融合数据驱动与物理建模优势的数据同化新范式，适用于高维稀疏感知场景，在保持计算效率的同时显著提升状态估计真实性与可解释性。

Abstract: The requirements of modern sensing are rapidly evolving, driven by increasing demands for data efficiency, real-time processing, and deployment under limited sensing coverage. Complex physical systems are often characterized through the integration of a limited number of point sensors in combination with scientific computations which approximate the dominant, full-state dynamics. Simulation models, however, inevitably neglect small-scale or hidden processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that often diverge from the reality measured by sensors. This creates a critical need for data assimilation, the process of integrating observational data with predictive simulation models to produce coherent and accurate estimates of the full state of complex physical systems. We propose a machine learning framework for Data Assimilation with a SHallow REcurrent Decoder (DA-SHRED) which bridges the simulation-to-real (SIM2REAL) gap between computational modeling and experimental sensor data. For real-world physics systems modeling high-dimensional spatiotemporal fields, where the full state cannot be directly observed and must be inferred from sparse sensor measurements, we leverage the latent space learned from a reduced simulation model via SHRED, and update these latent variables using real sensor data to accurately reconstruct the full system state. Furthermore, our algorithm incorporates a sparse identification of nonlinear dynamics based regression model in the latent space to identify functionals corresponding to missing dynamics in the simulation model. We demonstrate that DA-SHRED successfully closes the SIM2REAL gap and additionally recovers missing dynamics in highly complex systems, demonstrating that the combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.

</details>


### [517] [First On-Orbit Demonstration of a Geospatial Foundation Model](https://arxiv.org/abs/2512.01181)
*Andrew Du,Roberto Del Prete,Alejandro Mousist,Nick Manser,Fabrice Marre,Andrew Barton,Carl Seubert,Gabriele Meoni,Tat-Jun Chin*

Main category: cs.LG

TL;DR: 提出了紧凑型视觉Transformer地理基础模型，可在资源受限的太空硬件上实现高效的地球观测任务推理，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型地理基础模型难以部署在资源受限的太空硬件上，限制了其在轨应用。

Method: 通过模型压缩和领域自适应技术，开发紧凑型ViT-based GeoFM，并在两个典型飞行环境中验证其性能。

Result: 在五个下游任务中保持高性能，成功在国际空间站IMAGIN-e载荷上实现可靠的在轨推理。

Conclusion: 模型压缩与领域自适应是实现地理基础模型在轨高效部署的关键，推动了星载AI在地球观测任务中的可行性。

Abstract: Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.

</details>


### [518] [Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement](https://arxiv.org/abs/2512.01187)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 本文提出了一种名为Counter-Example-Driven Curricula（CEDC）的自动化课程学习框架，通过迭代利用模型自身失败案例（反例）来增强Transformer模型在长度外推和结构复杂性方面的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在外推（如更长或更复杂结构输入）时表现脆弱，亟需无需人工设计难度指标、自动聚焦模型失败的训练方法。

Method: CEDC框架每轮：1）用当前模型生成多样化候选问题；2）用快速可执行验证器识别错误预测（即反例）；3）基于发现的反例对模型进行微调。整个过程全自动、无须人工难度启发式。

Result: 在整数加法、排序、Dyck-2语言识别及三类文本分类任务上，CEDC相比静态训练和标准课程学习，长度外推能力提升最高达30倍；计算效率比均匀数据增强高3.75倍；且反例分析表明课程能自然适应并逐步攻克更复杂的错误模式。

Conclusion: 验证器引导、失败驱动的学习是一种简单、强大且高效的范式，可显著提升Transformer模型的泛化能力。

Abstract: Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.

</details>


### [519] [LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening](https://arxiv.org/abs/2512.01190)
*Nagham Osman,Keyue Jiang,Davide Buffelli,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: 本文分析了图生成中的自回归模型和单次生成模型（如扩散模型）的优缺点，提出了一种混合框架LGDC，结合两者优势，在保持局部结构的同时建模全局模式。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法在捕捉局部结构（如度分布、聚类性）与全局模式（如谱分布）之间存在权衡，亟需一种能兼顾二者的方法。

Method: 提出LGDC框架：利用谱保持的粗化-细化过程在图与潜在空间间双向映射，在潜在空间中用扩散模型生成图，再通过细化恢复细节。

Result: LGDC在局部结构数据集（Tree）上媲美自回归模型，在全局结构数据集（Planar, Community-20）上媲美扩散模型，验证了其有效性与效率。

Conclusion: 混合生成范式能有效平衡局部与全局图结构建模，LGDC为图生成提供了更全面、高效的解决方案。

Abstract: Graph generation is a critical task across scientific domains. Existing methods fall broadly into two categories: autoregressive models, which iteratively expand graphs, and one-shot models, such as diffusion, which generate the full graph at once. In this work, we provide an analysis of these two paradigms and reveal a key trade-off: autoregressive models stand out in capturing fine-grained local structures, such as degree and clustering properties, whereas one-shot models excel at modeling global patterns, such as spectral distributions. Building on this, we propose LGDC (latent graph diffusion via spectrum-preserving coarsening), a hybrid framework that combines strengths of both approaches. LGDC employs a spectrum-preserving coarsening-decoarsening to bidirectionally map between graphs and a latent space, where diffusion efficiently generates latent graphs before expansion restores detail. This design captures both local and global properties with improved efficiency. Empirically, LGDC matches autoregressive models on locally structured datasets (Tree) and diffusion models on globally structured ones (Planar, Community-20), validating the benefits of hybrid generation.

</details>


### [520] [Learning to Reconstruct Temperature Field from Sparse Observations with Implicit Physics Priors](https://arxiv.org/abs/2512.01196)
*Shihang Li,Zhiqiang Gong,Weien Zhou,Yue Gao,Wen Yao*

Main category: cs.LG

TL;DR: 本文提出IPTR框架，利用参考仿真数据中的稀疏监测-温度场对作为先验，结合隐式物理引导和傅里叶层编码，提升热源系统温度场重建的精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于DNN的方法仅依赖目标稀疏测量，忽略参考仿真数据中蕴含的热物理知识，且面临测量成本高、跨条件分布偏移大等挑战。

Method: 提出IPTR框架，包含双物理嵌入模块：一是基于交叉注意力的隐式物理引导分支，从参考数据中提取潜在物理规律；二是基于傅里叶层的辅助编码分支，捕获目标观测的空间特征；二者融合后解码重建完整温度场。

Result: 在单条件、多条件及少样本设置下，IPTR均显著优于现有方法，达到最优重建精度与强泛化能力。

Conclusion: 引入参考仿真数据作为物理先验并协同建模隐式物理与空间特征，可有效提升温度场重建模型的鲁棒性与实用性。

Abstract: Accurate reconstruction of temperature field of heat-source systems (TFR-HSS) is crucial for thermal monitoring and reliability assessment in engineering applications such as electronic devices and aerospace structures. However, the high cost of measurement acquisition and the substantial distributional shifts in temperature field across varying conditions present significant challenges for developing reconstruction models with robust generalization capabilities. Existing DNNs-based methods typically formulate TFR-HSS as a one-to-one regression problem based solely on target sparse measurements, without effectively leveraging reference simulation data that implicitly encode thermal knowledge. To address this limitation, we propose IPTR, an implicit physics-guided temperature field reconstruction framework that introduces sparse monitoring-temperature field pair from reference simulations as priors to enrich physical understanding. To integrate both reference and target information, we design a dual physics embedding module consisting of two complementary branches: an implicit physics-guided branch employing cross-attention to distill latent physics from the reference data, and an auxiliary encoding branch based on Fourier layers to capture the spatial characteristics of the target observation. The fused representation is then decoded to reconstruct the full temperature field. Extensive experiments under single-condition, multi-condition, and few-shot settings demonstrate that IPTR consistently outperforms existing methods, achieving state-of-the-art reconstruction accuracy and strong generalization capability.

</details>


### [521] [Know Thyself by Knowing Others: Learning Neuron Identity from Population Context](https://arxiv.org/abs/2512.01199)
*Vinam Arora,Divyansha Lachi,Ian J. Knight,Mehdi Azabou,Blake Richards,Cole L. Hurwitz,Josh Siegle,Eva L. Dyer*

Main category: cs.LG

TL;DR: 本文提出NuCLR，一种自监督框架，通过对比学习和时空Transformer，从神经活动数据中学习神经元身份的通用表征，在细胞类型和脑区解码任务中达到新SOTA，并展现出跨动物的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经元的信息处理方式依赖于其细胞类型、连接性和所在脑区，但仅从神经活动推断这些因素仍具挑战性。

Method: 提出NuCLR框架：结合不同时刻和不同刺激下同一神经元的多视角活动，采用对比学习目标拉近其表征；设计置换等变的时空Transformer以建模群体上下文，无需固定神经元顺序。

Result: 在多个电生理和钙成像数据集上，基于NuCLR表征的线性解码器在细胞类型与脑区解码任务中达到新SOTA；具备强零样本泛化能力；预训练动物数量增加持续提升下游性能；仅需少量标注样本即可获得竞争力。

Conclusion: 大规模、多样化的神经数据可使模型学习到跨动物泛化的神经元身份信息，NuCLR为神经元表征学习提供了可扩展、标签高效的新范式。

Abstract: Neurons process information in ways that depend on their cell type, connectivity, and the brain region in which they are embedded. However, inferring these factors from neural activity remains a significant challenge. To build general-purpose representations that allow for resolving information about a neuron's identity, we introduce NuCLR, a self-supervised framework that aims to learn representations of neural activity that allow for differentiating one neuron from the rest. NuCLR brings together views of the same neuron observed at different times and across different stimuli and uses a contrastive objective to pull these representations together. To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner. Across multiple electrophysiology and calcium imaging datasets, a linear decoding evaluation on top of NuCLR representations achieves a new state-of-the-art for both cell type and brain region decoding tasks, and demonstrates strong zero-shot generalization to unseen animals. We present the first systematic scaling analysis for neuron-level representation learning, showing that increasing the number of animals used during pretraining consistently improves downstream performance. The learned representations are also label-efficient, requiring only a small fraction of labeled samples to achieve competitive performance. These results highlight how large, diverse neural datasets enable models to recover information about neuron identity that generalize across animals. Code is available at https://github.com/nerdslab/nuclr.

</details>


### [522] [Sum Rate Maximization in STAR-RIS-UAV-Assisted Networks: A CA-DDPG Approach for Joint Optimization](https://arxiv.org/abs/2512.01202)
*Yujie Huang,Haibin Wan,Xiangcheng Li,Tuanfa Qin,Yun Li,Jun Li,Wen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的CA-DDPG算法，用于优化STAR-RIS-UAV无线通信系统的波束成形、相位偏移和无人机位置，以最大化系统和速率。


<details>
  <summary>Details</summary>
Motivation: 为了提升STAR-RIS辅助的无线通信系统的灵活性与频谱效率，需要联合优化波束成形、相位调控和无人机部署，传统方法难以应对高动态环境下的复杂非线性问题。

Method: 提出一种卷积增强的深度确定性策略梯度（CA-DDPG）算法，引入随机扰动因子增强探索能力，并通过卷积结构提升状态-动作值函数的评估精度，实现对波束成形矩阵、相位偏移矩阵和无人机位置的联合优化。

Result: 仿真结果表明，所提CA-DDPG算法能有效优化系统参数，在和速率方面优于其他对比算法，显著提升了系统容量和性能。

Conclusion: CA-DDPG算法在STAR-RIS-UAV系统中实现了更优的资源分配与控制，验证了其在智能无线环境重构中的潜力，为未来空地协同通信提供了有效解决方案。

Abstract: With the rapid advances in programmable materials, reconfigurable intelligent surfaces (RIS) have become a pivotal technology for future wireless communications. The simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) can both transmit and reflect signals, enabling comprehensive signal control and expanding application scenarios. This paper introduces an unmanned aerial vehicle (UAV) to further enhance system flexibility and proposes an optimization design for the spectrum efficiency of the STAR-RIS-UAV-assisted wireless communication system. We present a deep reinforcement learning (DRL) algorithm capable of iteratively optimizing beamforming, phase shifts, and UAV positioning to maximize the system's sum rate through continuous interactions with the environment. To improve exploration in deterministic policies, we introduce a stochastic perturbation factor, which enhances exploration capabilities. As exploration is strengthened, the algorithm's ability to accurately evaluate the state-action value function becomes critical. Thus, based on the deep deterministic policy gradient (DDPG) algorithm, we propose a convolution-augmented deep deterministic policy gradient (CA-DDPG) algorithm that balances exploration and evaluation to improve the system's sum rate. The simulation results demonstrate that the CA-DDPG algorithm effectively interacts with the environment, optimizing the beamforming matrix, phase shift matrix, and UAV location, thereby improving system capacity and achieving better performance than other algorithms.

</details>


### [523] [Research on Milling Machine Predictive Maintenance Based on Machine Learning and SHAP Analysis in Intelligent Manufacturing Environment](https://arxiv.org/abs/2512.01205)
*Wen Zhao,Jiawen Ding,Xueting Huang,Yibo Zhang*

Main category: cs.LG

TL;DR: 本文基于AI4I 2020数据集，构建了涵盖数据预处理、建模、评估、选择、SHAP解释与可视化的完整预测性维护流程，发现XGBoost和随机森林等集成学习模型在铣床故障预测中表现优异，并通过SHAP分析识别出加工温度、扭矩和转速为关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 推动智能制造环境下预测性维护的实践应用，促进制造业数字化转型、提升生产效率并降低维护成本。

Method: 基于AI4I 2020数据集，设计包含数据预处理、模型训练、模型评估、模型选择、SHAP分析和结果可视化六个环节的预测性维护实验流程，并对比八种机器学习模型性能。

Result: XGBoost和随机森林等集成学习方法在铣床故障预测任务中表现最佳；SHAP分析揭示加工温度、扭矩和转速是影响设备故障的关键特征。

Conclusion: 本研究融合人工智能与制造技术，为智能工厂中的预测性维护提供了可复用的方法论框架，具有实际工程指导价值。

Abstract: In the context of intelligent manufacturing, this paper conducts a series of experimental studies on the predictive maintenance of industrial milling machine equipment based on the AI4I 2020 dataset. This paper proposes a complete predictive maintenance experimental process combining artificial intelligence technology, including six main links: data preprocessing, model training, model evaluation, model selection, SHAP analysis, and result visualization. By comparing and analyzing the performance of eight machine learning models, it is found that integrated learning methods such as XGBoost and random forest perform well in milling machine fault prediction tasks. In addition, with the help of SHAP analysis technology, the influence mechanism of different features on equipment failure is deeply revealed, among which processing temperature, torque and speed are the key factors affecting failure. This study combines artificial intelligence and manufacturing technology, provides a methodological reference for predictive maintenance practice in an intelligent manufacturing environment, and has practical significance for promoting the digital transformation of the manufacturing industry, improving production efficiency and reducing maintenance costs.

</details>


### [524] [A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability](https://arxiv.org/abs/2512.01212)
*Xuanyi Zhao,Jiawen Ding,Xueting Huang,Yibo Zhang*

Main category: cs.LG

TL;DR: 本文比较了八种机器学习模型在西班牙电力市场电价预测中的性能，发现KNN模型表现最佳，并通过LIME方法增强了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着电力市场快速发展，电价波动加剧，传统线性模型难以捕捉电价的复杂非线性特征，亟需更先进的机器学习方法进行准确预测。

Method: 采用八种机器学习模型（线性回归、岭回归、决策树、KNN、随机森林、梯度提升、SVR、XGBoost）对西班牙电力市场数据（含用电量、发电量和气象变量）进行电价预测，并利用LIME方法进行可解释性分析。

Result: KNN模型表现最优，R²为0.865，MAE为3.556，RMSE为5.240；LIME分析表明气象因素与供需指标通过非线性关系显著影响电价波动。

Conclusion: 机器学习模型在电力价格预测中具有优越性能，结合LIME等可解释性分析方法可提升预测结果的透明度与决策支持能力。

Abstract: With the rapid development of electricity markets, price volatility has significantly increased, making accurate forecasting crucial for power system operations and market decisions. Traditional linear models cannot capture the complex nonlinear characteristics of electricity pricing, necessitating advanced machine learning approaches. This study compares eight machine learning models using Spanish electricity market data, integrating consumption, generation, and meteorological variables. The models evaluated include linear regression, ridge regression, decision tree, KNN, random forest, gradient boosting, SVR, and XGBoost. Results show that KNN achieves the best performance with R^2 of 0.865, MAE of 3.556, and RMSE of 5.240. To enhance interpretability, LIME analysis reveals that meteorological factors and supply-demand indicators significantly influence price fluctuations through nonlinear relationships. This work demonstrates the effectiveness of machine learning models in electricity price forecasting while improving decision transparency through interpretability analysis.

</details>


### [525] [Neural Network Optimal Power Flow via Energy Gradient Flow and Unified Dynamics](https://arxiv.org/abs/2512.01219)
*Xuezhi Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络动力学与能量梯度流的无监督OPF求解方法，通过构建能量函数并最小化物理残差，实现端到端、物理一致的最优潮流计算。


<details>
  <summary>Details</summary>
Motivation: 传统OPF算法计算效率低、依赖初值；现有深度学习方法依赖大量标签数据且难以保证物理一致性。

Method: 将OPF转化为能量最小化问题，构造衡量约束偏离程度的能量函数，利用梯度流引导神经网络学习满足潮流约束且成本最优的解；采用无监督方式直接最小化物理残差进行训练。

Result: 实现了无需预求解样本、无需标签数据的端到端物理约束学习，提升了计算效率与物理一致性。

Conclusion: 该方法突破了传统优化与监督学习OPF方法的局限，为可扩展、可解释、物理驱动的智能电网优化提供了新范式。

Abstract: Optimal Power Flow (OPF) is a core optimization problem in power system operation and planning, aiming to minimize generation costs while satisfying physical constraints such as power flow equations, generator limits, and voltage limits. Traditional OPF solving methods typically employ iterative optimization algorithms (such as interior point methods, sequential quadratic programming, etc.), with limitations including low computational efficiency, initial value sensitivity, and low batch computation efficiency. Most existing deep learning-based OPF methods rely on supervised learning, requiring pre-solving large numbers of cases, and have difficulty guaranteeing physical consistency. This paper proposes an Optimal Power Flow solving method based on neural network dynamics and energy gradient flow, transforming OPF problems into energy minimization problems. By constructing an energy function to measure the degree of deviation from the constraint manifold, and guiding networks to learn optimal solutions that simultaneously satisfy power flow constraints and minimize costs through gradient flow. Neural networks are trained unsupervised by directly minimizing physical residuals, requiring no labeled data, achieving true "end-to-end" physics-constrained learning.

</details>


### [526] [CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions](https://arxiv.org/abs/2512.01224)
*Ruixiang Feng,Zhenwei An,Yuntao Wen,Ran Le,Yiming Jia,Chen Yang,Zongchao Chen,Lisi Chen,Shen Gao,Shuo Shang,Yang Song,Tao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种工具增强型验证器CoSineVerifier，通过外部执行器实现精确计算与符号简化，采用冷启动微调加多轮工具集成强化学习的两阶段训练方法，在STEM、通用问答和长推理任务中表现优异，并在RLVR中作为奖励模型显著提升大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有答案验证方法在计算密集型科学领域（如代数等价性检验、物理常数替换）存在不足，难以满足高精度验证需求。

Method: 提出工具增强验证器CoSineVerifier，结合外部执行器进行精确计算与符号简化；设计冷启动微调+多轮工具集成强化学习的两阶段训练流程。

Result: 在VerifyBench-Hard和SCI-Bench上达到SOTA；在AIME'24/25的RLVR中作为奖励模型，持续优于基于规则和基于模型的验证器。

Conclusion: CoSineVerifier通过工具增强与分阶段训练，显著提升了科学推理任务中的答案验证鲁棒性与泛化能力，为LLM推理能力提升提供了有效奖励信号。

Abstract: Answer verification methods are widely employed in language model training pipelines spanning data curation, evaluation, and reinforcement learning with verifiable rewards (RLVR). While prior work focus on developing unified verifiers applicable across multiple reasoning scenarios, significant challenges remain in computation-oriented scientific domains, such as algebraic equivalence checking and physical constant substitution. In this paper, we introduce \model, a tool-augmented verifier that leverages external executors to perform precise computations and symbolic simplifications. \model enables robust verification that goes beyond simple semantic matching. We propose a novel two-stage pipeline, which begin with cold-start fine-tuning and followed by multi-turn reinforcement learning with tool integration. Extensive experiments conducted on STEM subjects, general QA, and long-form reasoning tasks demonstrates strong generalization of \model. The results shows that the \model achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench. And we also employ our \model in RLVR as a reward model, the results show that it consistently outperforms both rubric-based and model-based verifiers on AIME'24 and AIME'25, demonstrating strong potential to enhance reasoning capabilities of LLM. Our model is released at \hyperlink{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}.

</details>


### [527] [On the Tension Between Optimality and Adversarial Robustness in Policy Optimization](https://arxiv.org/abs/2512.01228)
*Haoran Li,Jiayu Lv,Congying Han,Zicheng Zhang,Anqi Li,Yan Liu,Tiande Guo,Nan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种名为BARPO的双层框架，通过调节对抗强度来统一标准策略优化与鲁棒策略优化，在保持全局最优的同时提升了训练的可导航性，实验证明其优于传统ARPO方法。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中最优性与对抗鲁棒性常被视为冲突目标，尽管理论上有对齐可能，但实践中存在差距，本文旨在弥合理论与实践之间的鸿沟。

Method: 通过分析标准策略优化（SPO）与对抗鲁棒策略优化（ARPO）之间的差异，识别出强对抗者导致的“粘滞一阶平稳策略”问题，并提出BARPO双层优化框架，动态调节对抗强度以平衡鲁棒性与性能。

Result: 实验表明BARPO在多种环境下均优于标准ARPO方法，有效缓解了鲁棒性与高性能之间的张力，提升了策略的综合表现。

Conclusion: BARPO为实现最优性与对抗鲁棒性的协同提供了可行路径，揭示了通过调节对抗强度可兼顾理论一致性与实际性能。

Abstract: Achieving optimality and adversarial robustness in deep reinforcement learning has long been regarded as conflicting goals. Nonetheless, recent theoretical insights presented in CAR suggest a potential alignment, raising the important question of how to realize this in practice. This paper first identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). Although they share theoretical consistency, a fundamental tension between robustness and optimality arises in practical policy gradient methods. SPO tends toward convergence to vulnerable first-order stationary policies (FOSPs) with strong natural performance, whereas ARPO typically favors more robust FOSPs at the expense of reduced returns. Furthermore, we attribute this tradeoff to the reshaping effect of the strongest adversary in ARPO, which significantly complicates the global landscape by inducing deceptive sticky FOSPs. This improves robustness but makes navigation more challenging. To alleviate this, we develop the BARPO, a bilevel framework unifying SPO and ARPO by modulating adversary strength, thereby facilitating navigability while preserving global optima. Extensive empirical results demonstrate that BARPO consistently outperforms vanilla ARPO, providing a practical approach to reconcile theoretical and empirical performance.

</details>


### [528] [Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe](https://arxiv.org/abs/2512.01252)
*Yahui Liu,Yang Yue,Jingyuan Zhang,Chenxi Sun,Yang Zhou,Wencong Zeng,Ruiming Tang,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文探索了扩散模型中混合专家（MoE）架构的设计空间，提出并验证了一系列关键架构因素（如DeepSeek式专家模块、中间宽度调整、专家数量变化、增强的位置编码），发现其调优效果可超越仅改进路由机制的方案，并在参数效率和性能上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 现有扩散MoE研究集中于路由机制，而对底层架构配置空间探索不足；受大语言模型中MoE设计启发，作者希望系统性地探索影响扩散MoE性能的关键架构因素。

Method: 基于LLM中MoE的设计范式，识别并实验验证若干关键架构因素（DeepSeek-style专家模块、中间层宽度、专家数量、注意力位置编码），在潜空间与像素空间扩散框架中进行系统性消融与对比实验。

Result: 所提架构在同等或更少激活参数下，显著超越强基线模型；提供了高效可复现的训练方案；代码与模型全部开源。

Conclusion: 扩散MoE的架构设计（而非仅路由机制）是提升性能的关键；合理配置专家结构、宽度、数量及位置编码能带来比路由创新更显著的增益；该工作为高效扩散MoE建模提供了实用指南。

Abstract: Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: https://github.com/yhlleo/EfficientMoE.

</details>


### [529] [Efficient Hyperparameter Search for Non-Stationary Model Training](https://arxiv.org/abs/2512.01258)
*Berivan Isik,Matthew Fahrbach,Dima Kuzmin,Nicolas Mayoraz,Emil Praun,Steffen Rendle,Raghavendra Vasudeva*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段超参数搜索范式，通过数据缩减和预测策略，在在线学习场景（如推荐与广告系统）中大幅降低超参数调优成本，实现在Criteo 1TB数据集上最高10倍加速，并在工业级广告系统中验证了显著效率提升。


<details>
  <summary>Details</summary>
Motivation: 在线学习系统（如推荐、广告）需持续适应非平稳数据分布，其模型训练及超参数搜索成本极高，亟需高效、适配时序非平稳特性的超参数优化方法。

Method: 提出两阶段范式：第一阶段聚焦于低成本、高识别准确率的候选配置筛选（采用专为序列非平稳数据设计的数据缩减与预测策略）；第二阶段仅对筛选出的少数优质配置进行完整训练。

Result: 在Criteo 1TB公开数据集上超参数搜索总成本降低最高达10倍；在规模大两个数量级的工业广告系统中也取得经验证的显著效率增益。

Conclusion: 准确识别优于早期性能优化是降低在线学习超参数搜索成本的关键，所提两阶段框架兼顾效率与有效性，可扩展至大规模工业场景。

Abstract: Online learning is the cornerstone of applications like recommendation and advertising systems, where models continuously adapt to shifting data distributions. Model training for such systems is remarkably expensive, a cost that multiplies during hyperparameter search. We introduce a two-stage paradigm to reduce this cost: (1) efficiently identifying the most promising configurations, and then (2) training only these selected candidates to their full potential. Our core insight is that focusing on accurate identification in the first stage, rather than achieving peak performance, allows for aggressive cost-saving measures. We develop novel data reduction and prediction strategies that specifically overcome the challenges of sequential, non-stationary data not addressed by conventional hyperparameter optimization. We validate our framework's effectiveness through a dual evaluation: first on the Criteo 1TB dataset, the largest suitable public benchmark, and second on an industrial advertising system operating at a scale two orders of magnitude larger. Our methods reduce the total hyperparameter search cost by up to 10$\times$ on the public benchmark and deliver significant, validated efficiency gains in the industrial setting.

</details>


### [530] [Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding](https://arxiv.org/abs/2512.01278)
*Yilong Zhao,Jiaming Tang,Kan Zhu,Zihao Ye,Chi-Chih Chang,Chaofan Lin,Jongseok Park,Guangxuan Xiao,Mohamed S. Abdelfattah,Mingyu Gao,Baris Kasikci,Song Han,Ion Stoica*

Main category: cs.LG

TL;DR: 本文提出SparseSpec，一种基于自推测的稀疏注意力 speculative decoding 框架，通过 PillarAttn 机制与三项系统级优化，在保持精度的同时显著提升长文本生成吞吐量（最高2.13倍）。


<details>
  <summary>Details</summary>
Motivation: 长链式推理生成导致KV缓存持续增长，使推理从计算受限转为内存带宽受限，制约大模型实际部署效率。

Method: 提出SparseSpec框架：采用同一模型作为draft和target模型（self-speculation）；设计稀疏注意力机制PillarAttn用于draft阶段，利用验证阶段信息动态选择关键token；并协同设计统一调度器、延迟验证（实现CPU/GPU重叠）和动态KV缓存管理。

Result: 在多个模型和数据集上超越现有SOTA方法，最高实现2.13倍吞吐量提升。

Conclusion: SparseSpec证明了自推测结合稀疏注意力与系统协同优化可有效缓解内存带宽瓶颈，为高效长文本推理提供新范式。

Abstract: Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.
  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.

</details>


### [531] [Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching](https://arxiv.org/abs/2512.01286)
*Mudit Gaur,Prashant Trivedi,Shuchin Aeron,Amrit Singh Bedi,George K. Atia,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文首次分析了无需假设经验风险最小化（ERM）的流匹配生成模型的样本复杂度，在标准假设下，证明了使用足够表达力的神经网络，仅需O(ε⁻⁴)个样本即可使学习分布与真实分布间的Wasserstein-2距离小于O(ε)。


<details>
  <summary>Details</summary>
Motivation: 尽管流匹配在实践中取得了成功，但其理论理解尤其是样本复杂度方面仍十分有限。

Method: 将速度场估计误差分解为神经网络逼近误差、有限样本带来的统计误差和有限优化步数引起的优化误差，并分别用独立技术处理。

Result: 在速度场估计损失函数和数据分布有界的标准假设下，证明了O(ε⁻⁴)样本足以保证Wasserstein-2距离小于O(ε)。

Conclusion: 本文为流匹配生成模型提供了首个非ERM假设下的样本复杂度理论保证，填补了该方向的重要理论空白。

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, offering faster sampling and simpler training by learning continuous flows governed by ordinary differential equations. Despite growing empirical success, the theoretical understanding of flow matching remains limited, particularly in terms of sample complexity results. In this work, we provide the first analysis of the sample complexity for flow-matching based generative models without assuming access to the empirical risk minimizer (ERM) of the loss function for estimating the velocity field. Under standard assumptions on the loss function for velocity field estimation and boundedness of the data distribution, we show that a sufficiently expressive neural network can learn a velocity field such that with $\mathcal{O}(ε^{-4})$ samples, such that the Wasserstein-2 distance between the learned and the true distribution is less than $\mathcal{O}(ε)$. The key technical idea is to decompose the velocity field estimation error into neural-network approximation error, statistical error due to the finite sample size, and optimization error due to the finite number of optimization steps for estimating the velocity field. Each of these terms are then handled via techniques that may be of independent interest.

</details>


### [532] [milearn: A Python Package for Multi-Instance Machine Learning](https://arxiv.org/abs/2512.01287)
*Dmitry Zankov,Pavlo Polishchuk,Michal Sobieraj,Mario Barbatti*

Main category: cs.LG

TL;DR: milearn是一个遵循scikit-learn接口的Python多实例学习（MIL）包，支持经典和神经网络算法，适用于回归与分类任务，并内置针对小数据集的超参数优化，特别支持关键实例检测（KID）问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决多实例学习中缺乏统一、易用框架的问题，尤其是在小样本数据场景下进行有效模型选择的需求。

Method: 开发了一个名为milearn的Python库，兼容scikit-learn的fit/predict接口，集成多种MIL算法并设计专用的超参数优化策略。

Result: 在多个合成MIL基准数据集上验证了milearn的有效性，涵盖数字分类与回归、分子性质预测及蛋白质相互作用预测等任务，并对关键实例检测提供了良好支持。

Conclusion: milearn提供了一个灵活且强大的统一框架，适用于多种MIL应用场景，尤其适合小数据集下的模型训练与选择。

Abstract: We introduce milearn, a Python package for multi-instance learning (MIL) that follows the familiar scikit-learn fit/predict interface while providing a unified framework for both classical and neural-network-based MIL algorithms for regression and classification. The package also includes built-in hyperparameter optimization designed specifically for small MIL datasets, enabling robust model selection in data-scarce scenarios. We demonstrate the versatility of milearn across a broad range of synthetic MIL benchmark datasets, including digit classification and regression, molecular property prediction, and protein-protein interaction (PPI) prediction. Special emphasis is placed on the key instance detection (KID) problem, for which the package provides dedicated support.

</details>


### [533] [Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models](https://arxiv.org/abs/2512.01343)
*Shashank Landge,Abhishek Patil,Tejas kamble,Bhushan Buddhivant,Priyanka Joshi*

Main category: cs.LG

TL;DR: 提出一种基于SVD的无数据结构感知量化方法，通过保留主成分方向的重要权重，在无需校准数据的情况下实现对大模型的有效后训练量化，在多个GLUE任务上优于AWQ和SpQR方法。


<details>
  <summary>Details</summary>
Motivation: 在缺乏校准数据或数据隐私受限的场景下，现有基于激活或二阶梯度的后训练量化方法无法使用，因此需要一种无需数据且能准确识别关键权重的量化方法。

Method: 利用SVD分解发现权重矩阵中的主成分，认为与主成分对齐的权重更为重要；保留前k个主成分对应的权重为FP32精度，其余权重进行低比特量化，从而实现数据自由的结构感知量化。

Result: 在DistilBERT模型上于GLUE基准（MRPC、RTE、QNLI）进行评估，所提方法在RTE任务上达到66.06%准确率，高于AWQ（65.34%）和SpQR（65.34%），且无需任何校准数据。

Conclusion: 模型权重的结构重要性（如SVD主成分）可作为功能重要性的可靠代理，能够在不依赖数据的情况下指导量化，为数据受限场景下的高效部署提供了新思路。

Abstract: As Large Language Models (LLMs) continue to scale in parameter count, deploying them on commodity hardware has become increasingly challenging. Post-Training Quantization (PTQ) addresses this by reducing the precision of model weights, typically to 4-bit or lower. However, uniform quantization often leads to significant performance degradation due to the presence of ``outlier features'' -- weights that, while few in number, are critical for maintaining model accuracy. Current state-of-the-art methods such as AWQ (Activation-aware Weight Quantization) and SpQR (Sparse Quantization Representations) rely on calibration data to identify these salient weights via activation magnitudes or Hessian sensitivity. In scenarios where data privacy is paramount or calibration data is unavailable, these methods are inapplicable.
  In this work, we propose a data-free, structure-aware hypothesis: that the weights identified as Principal Components via Singular Value Decomposition (SVD) are intrinsically important to the model's downstream performance. We introduce a novel selection heuristic that preserves the top-$k$ weights aligned with the principal components in FP32, while aggressively quantizing the residual weights. We compare our method against activation-aware (AWQ) and second-order (SpQR) methods across GLUE benchmarks (MRPC, RTE, QNLI) using a DistilBERT backbone. Our experiments reveal that structural importance is highly correlated with functional importance. On the challenging RTE task, our SVD-based method achieves an accuracy of 66.06\%, outperforming both AWQ (65.34\%) and SpQR (65.34\%) at high protection budgets, validating that intrinsic matrix structure can serve as a robust proxy for weight saliency without the need for forward passes or calibration data.

</details>


### [534] [Directed evolution algorithm drives neural prediction](https://arxiv.org/abs/2512.01362)
*Yanlin Wang,Nancy M Young,Patrick C M Wong*

Main category: cs.LG

TL;DR: 本文提出了一种受生物定向进化启发的定向进化模型（DEM），用于解决神经预测中领域迁移和标签稀缺的挑战，尤其在儿童人工耳蜗植入术前神经影像预测术后语言发育结果的任务中展现出跨域泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 神经预测在个体化医疗中潜力巨大，但受限于领域偏移（domain shift）和目标域标签稀缺问题，难以落地为临床AI应用。

Method: 提出定向进化模型（DEM），模拟生物定向进化的试错机制；结合经验回放（replay buffer）与持续反向传播（continual backpropagate）以平衡探索与利用；应用于四个儿童人工耳蜗植入数据集的跨域预测任务。

Result: DEM显著提升了跨数据集的术前神经影像对术后语言发育结果的预测性能，同时缓解了目标域标签稀缺问题。

Conclusion: DEM是一种有效的、适用于小样本跨域神经预测的计算框架，为医学AI中的可泛化建模提供了新思路。

Abstract: Neural prediction offers a promising approach to forecasting the individual variability of neurocognitive functions and disorders and providing prognostic indicators for personalized invention. However, it is challenging to translate neural predictive models into medical artificial intelligent applications due to the limitations of domain shift and label scarcity. Here, we propose the directed evolution model (DEM), a novel computational model that mimics the trial-and-error processes of biological directed evolution to approximate optimal solutions for predictive modeling tasks. We demonstrated that the directed evolution algorithm is an effective strategy for uncertainty exploration, enhancing generalization in reinforcement learning. Furthermore, by incorporating replay buffer and continual backpropagate methods into DEM, we provide evidence of achieving better trade-off between exploitation and exploration in continuous learning settings. We conducted experiments on four different datasets for children with cochlear implants whose spoken language developmental outcomes vary considerably on the individual-child level. Preoperative neural MRI data has shown to accurately predict the post-operative outcome of these children within but not across datasets. Our results show that DEM can efficiently improve the performance of cross-domain pre-implantation neural predictions while addressing the challenge of label scarcity in target domain.

</details>


### [535] [A Fine Evaluation Method for Cube Copying Test for Early Detection of Alzheimer's Disease](https://arxiv.org/abs/2512.01367)
*Xinyu Jiang,Cuiyun Gao,Wenda Huang,Yiyang Jiang,Binwen Luo,Yuxin Jiang,Mengting Wang,Haoran Wen,Yang Zhao,Xuemei Chen,Songqun Huang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于动态手写特征提取的立方体复制测试（CCT）精细评估方法，用于更客观地评估阿尔茨海默病早期视觉空间认知功能障碍。


<details>
  <summary>Details</summary>
Motivation: 传统MoCA中的CCT采用“通过/未通过”二分法评估，对教育程度较低的老年人存在评分偏倚，影响早期诊断准确性。

Method: 使用自主研发的Cogni-CareV3.0软件采集CCT动态手写数据，提取空间与运动特征，归一化不等维特征矩阵，并采用BiLSTM-Attention模型进行分类。

Result: 该方法分类准确率达86.69%，显著优于现有研究；立方体绘图能力评分在MCI患者、年龄和教育水平方面呈现显著规律性，且评分与年龄呈负相关，与教育水平呈正相关。

Conclusion: 本研究提供了一种更为客观、全面的视觉空间认知功能评估方法，有助于AD的早期筛查与个性化干预。

Abstract: Background: Impairment of visual spatial cognitive function is the most common early clinical manifestation of Alzheimer's Disease (AD). When the Montreal Cognitive Assessment (MoCA) uses the "0/1" binary method ("pass/fail") to evaluate the visual spatial cognitive ability represented by the Cube Copying Test(CCT), the elder with less formal education generally score 0 point, resulting in serious bias in the evaluation results. Therefore, this study proposes a fine evaluation method for CCT based on dynamic handwriting feature extraction of DH-SCSM-BLA. method : The Cogni-CareV3.0 software independently developed by our team was used to collect dynamic handwriting data of CCT. Then, the spatial and motion features of segmented dynamic handwriting were extracted, and feature matrix with unequal dimensions were normalized. Finally, a bidirectional long short-term memory network model combined with attention mechanism (BiLSTM-Attention) was adopted for classification. Result: The experimental results showed that: The proposed method has significant superiority compared to similar studies, with a classification accuracy of 86.69%. The distribution of cube drawing ability scores has significant regularity for three aspects such as MCI patients and healthy control group, age, and levels of education. It was also found that score for each cognitive task including cube drawing ability score is negatively correlated with age. Score for each cognitive task including cube drawing ability score, but positively correlated with levels of education significantly. Conclusion: This study provides a relatively objective and comprehensive evaluation method for early screening and personalized intervention of visual spatial cognitive impairment.

</details>


### [536] [Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators](https://arxiv.org/abs/2512.01370)
*Medha Sawhney,Abhilash Neog,Mridul Khurana,Anuj Karpatne*

Main category: cs.LG

TL;DR: PRISMA是一种基于扩散的神经算子，通过在频谱域中利用注意力机制嵌入PDE残差，实现无需梯度下降的快速、鲁棒、准确推理。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于扩散的PDE求解器依赖慢速、不稳定且难以适应噪声残差的梯度优化过程的问题。

Method: 提出PRISMA模型，将PDE残差作为核心架构特征，通过频谱域中的注意力机制进行调制，摒弃外部损失驱动的测试时优化。

Result: 在五个基准PDE上，相比先前方法，精度相当但推理成本显著降低：去噪步数减少10–100倍，推理速度提升15–250倍，尤其在含噪声观测下表现更鲁棒。

Conclusion: PRISMA通过架构级融合PDE残差，实现了免梯度、高效、自适应且超参数不敏感的PDE求解新范式。

Abstract: Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. To address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-descent free inference. In contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA has competitive accuracy, at substantially lower inference costs, compared to previous methods across five benchmark PDEs, especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.

</details>


### [537] [CLAPS: Posterior-Aware Conformal Intervals via Last-Layer Laplace](https://arxiv.org/abs/2512.01384)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: CLAPS是一种结合最后一层拉普拉斯近似与分裂共形校准的后验感知共形回归方法，通过高斯后验定义双侧后验累积分布函数得分，提升预测区间效率，尤其适用于小到中等规模表格数据。


<details>
  <summary>Details</summary>
Motivation: 在小到中等规模表格数据上，数据稀缺且不确定性建模至关重要，传统残差型共形方法效率不足，需更契合完整预测分布形状的 conformity 度量。

Method: 将最后一层拉普拉斯近似（Last-Layer Laplace Approximation）与分裂共形校准（split-conformal calibration）结合，从所得高斯后验中定义双侧后验累积分布函数（CDF）得分作为 conformity score。

Result: CLAPS在多个基准测试中以相同MLP骨干网络实现标称覆盖，预测区间更窄、效率更高、开销极小；并提供轻量级诊断工具，分离aleatoric与epistemic不确定性并可视化后验行为。

Conclusion: CLAPS为残差型共形基线提供了清晰、实用的升级方案，在保持严格统计保证的同时显著提升预测区间实用性。

Abstract: We present CLAPS, a posterior-aware conformal regression method that pairs a Last-Layer Laplace Approximation with split-conformal calibration. From the resulting Gaussian posterior, CLAPS defines a simple two-sided posterior CDF score that aligns the conformity metric with the full predictive shape, not just a point estimate. This alignment yields narrower prediction intervals at the same target coverage, especially on small to medium tabular datasets where data are scarce and uncertainty modeling matters. We also provide a lightweight diagnostic suite that separates aleatoric and epistemic components and visualizes posterior behavior, helping practitioners understand why intervals shrink when they do. Across multiple benchmarks using the same MLP backbone, CLAPS consistently attains nominal coverage with improved efficiency and minimal overhead, offering a clear, practical upgrade to residual-based conformal baselines.

</details>


### [538] [Consistency Flow Model Achieves One-step Denoising Error Correction Codes](https://arxiv.org/abs/2512.01389)
*Haoyu Lei,Chin Wa Lau,Kaiwen Zhou,Nian Guo,Farzan Farnia*

Main category: cs.LG

TL;DR: 本文提出ECCFM，一种架构无关的训练框架，用于高保真单步解码，通过将去噪过程建模为概率流常微分方程并引入微分时间正则化，实现比扩散解码器快30–100倍、误码率更低的解码性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于去噪扩散的神经解码器虽性能优异，但迭代采样导致低延迟场景实用性差，亟需高效准确的单步解码方法。

Method: 将反向去噪过程建模为概率流常微分方程（PF-ODE），并引入微分时间正则化以增强轨迹平滑性，从而支持端到端单步映射噪声信号至原始码字。

Result: 在多个解码基准上，ECCFM的误码率（BER）低于自回归和扩散基线，尤其在长码上提升显著；推理速度比扩散解码器快30–100倍。

Conclusion: ECCFM提供了一种通用、高效且高精度的单步神经解码新范式，兼顾性能与实时性需求。

Abstract: Error Correction Codes (ECC) are fundamental to reliable digital communication, yet designing neural decoders that are both accurate and computationally efficient remains challenging. Recent denoising diffusion decoders with transformer backbones achieve state-of-the-art performance, but their iterative sampling limits practicality in low-latency settings. We introduce the Error Correction Consistency Flow Model (ECCFM), an architecture-agnostic training framework for high-fidelity one-step decoding. By casting the reverse denoising process as a Probability Flow Ordinary Differential Equation (PF-ODE) and enforcing smoothness through a differential time regularization, ECCFM learns to map noisy signals along the decoding trajectory directly to the original codeword in a single inference step. Across multiple decoding benchmarks, ECCFM attains lower bit-error rates (BER) than autoregressive and diffusion-based baselines, with notable improvements on longer codes, while delivering inference speeds up from 30x to 100x faster than denoising diffusion decoders.

</details>


### [539] [RE-LLM: Integrating Large Language Models into Renewable Energy Systems](https://arxiv.org/abs/2512.01392)
*Ali Forootani,Mohammad Sadr,Danial Esmaeili Aliabadi,Daniela Thraen*

Main category: cs.LG

TL;DR: 本文提出了一种名为RE-LLM的混合框架，将大语言模型（LLM）嵌入能源系统建模流程，以提升结果可解释性与利益相关者参与度。


<details>
  <summary>Details</summary>
Motivation: 能源系统模型输出高度技术化，难以被政策制定者、规划者和公众等非专家理解，导致建模成果的实际影响力受限，尤其在高比例可再生能源、多部门耦合与深度不确定性背景下更为突出。

Method: 提出RE-LLM框架，融合三部分：(i) 基于优化的情景探索；(ii) 机器学习代理模型加速仿真；(iii) LLM驱动的自然语言生成，将复杂结果转化为面向利益相关者的清晰解释。

Result: 该框架显著降低计算负担，增强可解释性，支持实时权衡分析、敏感性评估与政策含义解读，并具备跨平台、跨模型的通用性与多语言交互能力。

Conclusion: RE-LLM代表一种以人为本的能源建模范式，有效弥合数据驱动分析与可持续能源转型中可操作决策之间的最后一公里鸿沟。

Abstract: Energy system models are increasingly employed to guide long-term planning in multi-sectoral environments where decisions span electricity, heat, transport, land use, and industry. While these models provide rigorous quantitative insights, their outputs are often highly technical, making them difficult to interpret for non-expert stakeholders such as policymakers, planners, and the public. This communication gap limits the accessibility and practical impact of scenario-based modeling, particularly as energy transitions grow more complex with rising shares of renewables, sectoral integration, and deep uncertainties. To address this challenge, we propose the Renewable Energy Large Language Model (RE-LLM), a hybrid framework that integrates Large Language Models (LLMs) directly into the energy system modeling workflow. RE-LLM combines three core elements: (i) optimization-based scenario exploration, (ii) machine learning surrogates that accelerate computationally intensive simulations, and (iii) LLM-powered natural language generation that translates complex results into clear, stakeholder-oriented explanations. This integrated design not only reduces computational burden but also enhances inter-pretability, enabling real-time reasoning about trade-offs, sensitivities, and policy implications. The framework is adaptable across different optimization platforms and energy system models, ensuring broad applicability beyond the case study presented. By merging speed, rigor, and interpretability, RE-LLM advances a new paradigm of human-centric energy modeling. It enables interactive, multilingual, and accessible engagement with future energy pathways, ultimately bridging the final gap between data-driven analysis and actionable decision-making for sustainable transitions.

</details>


### [540] [On Global Applicability and Location Transferability of Generative Deep Learning Models for Precipitation Downscaling](https://arxiv.org/abs/2512.01400)
*Paula Harder,Christian Lessig,Matthew Chantry,Francis Pelletier,David Rolnick*

Main category: cs.LG

TL;DR: 本文评估了生成式降尺度模型在全球不同区域的泛化能力，使用ERA5数据作为预测因子、IMERG降水数据作为目标，并通过基于地理位置的分层数据划分，在15个区域系统评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习降尺度模型多为区域特化，其对未见地理区域的泛化能力尚不清楚，亟需系统性评估。

Method: 构建全球统一框架，采用ERA5再分析数据为输入、IMERG 0.1°降水数据为标签；设计基于地理位置的分层数据划分策略，在15个全球区域上测试生成式降尺度模型的泛化性能。

Result: 揭示了当前生成式降尺度模型在跨区域泛化上的局限性，不同区域间性能差异显著，部分区域表现明显下降。

Conclusion: 生成式降尺度模型的地理泛化能力较弱，未来需发展更具鲁棒性和区域不变性的建模方法。

Abstract: Deep learning offers promising capabilities for the statistical downscaling of climate and weather forecasts, with generative approaches showing particular success in capturing fine-scale precipitation patterns. However, most existing models are region-specific, and their ability to generalize to unseen geographic areas remains largely unexplored. In this study, we evaluate the generalization performance of generative downscaling models across diverse regions. Using a global framework, we employ ERA5 reanalysis data as predictors and IMERG precipitation estimates at $0.1^\circ$ resolution as targets. A hierarchical location-based data split enables a systematic assessment of model performance across 15 regions around the world.

</details>


### [541] [Fantastic Features and Where to Find Them: A Probing Method to combine Features from Multiple Foundation Models](https://arxiv.org/abs/2512.01405)
*Benjamin Ramtoula,Pierre-Yves Lajoie,Paul Newman,Daniele De Martini*

Main category: cs.LG

TL;DR: 本文提出ComBo，一种简单可扩展的基于探测的适配器，能有效整合多个基础模型（FMs）的特征，无需微调或反向传播，且支持自动评估各模型对任务的相关性，在VTAB-1k上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有适配策略（如参数高效微调）仅针对单个模型，无法利用多个FM之间的互补优势；而现有探测方法难以扩展至大规模特征且依赖数据集特定超参调优。

Method: ComBo将一个或多个FM各层的激活压缩为紧凑的token级表示，并用轻量级Transformer进行任务预测；引入机制评估各backbone的任务相关性，实现选择性适配。

Result: 在VTAB-1k的19个任务上，ComBo优于先前探测方法，媲美甚至超越更昂贵的方法（如蒸馏式模型融合），并支持对已调优模型的高效探测。

Conclusion: ComBo提供了一种实用、通用的框架，用于融合多个基础模型的多样化表征，兼具高效性、可扩展性与任务自适应能力。

Abstract: Foundation models (FMs) trained with different objectives and data learn diverse representations, making some more effective than others for specific downstream tasks. Existing adaptation strategies, such as parameter-efficient fine-tuning, focus on individual models and do not exploit the complementary strengths across models. Probing methods offer a promising alternative by extracting information from frozen models, but current techniques do not scale well with large feature sets and often rely on dataset-specific hyperparameter tuning. We propose Combined backBones (ComBo), a simple and scalable probing-based adapter that effectively integrates features from multiple models and layers. ComBo compresses activations from layers of one or more FMs into compact token-wise representations and processes them with a lightweight transformer for task-specific prediction. Crucially, ComBo does not require dataset-specific tuning or backpropagation through the backbone models. However, not all models are equally relevant for all tasks. To address this, we introduce a mechanism that leverages ComBo's joint multi-backbone probing to efficiently evaluate each backbone's task-relevance, enabling both practical model comparison and improved performance through selective adaptation. On the 19 tasks of the VTAB-1k benchmark, ComBo outperforms previous probing methods, matches or surpasses more expensive alternatives, such as distillation-based model merging, and enables efficient probing of tuned models. Our results demonstrate that ComBo offers a practical and general-purpose framework for combining diverse representations from multiple FMs.

</details>


### [542] [A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns](https://arxiv.org/abs/2512.01412)
*Ziqian Wang,Yuxiao Cheng,Jinli Suo*

Main category: cs.LG

TL;DR: EXCAP是一个满足时间连续性、模式中心解释、因果解耦和模型保真度四个关键要求的统一框架，用于实现长时序神经网络的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法大多仅提供点级重要性评分，无法捕捉时间序列中的趋势、周期和机制变化等时序结构，限制了人类对长时序模型的理解与信任。

Method: 提出EXCAP框架，结合基于注意力的分段器提取连贯时序模式，由预训练因果图引导的因果结构化解码器，以及确保表示稳定性的潜在聚合机制。

Result: 理论分析表明EXCAP在时间上提供平滑稳定的解释，并对因果掩码扰动具有鲁棒性；实验显示其在分类与预测任务中兼具高精度与可解释性。

Conclusion: EXCAP为长时序建模提供了一种有原则且可扩展的可解释方法，在医疗和金融等高风险领域具有应用价值。

Abstract: Explainability is essential for neural networks that model long time series, yet most existing explainable AI methods only produce point-wise importance scores and fail to capture temporal structures such as trends, cycles, and regime changes. This limitation weakens human interpretability and trust in long-horizon models. To address these issues, we identify four key requirements for interpretable time-series modeling: temporal continuity, pattern-centric explanation, causal disentanglement, and faithfulness to the model's inference process. We propose EXCAP, a unified framework that satisfies all four requirements. EXCAP combines an attention-based segmenter that extracts coherent temporal patterns, a causally structured decoder guided by a pre-trained causal graph, and a latent aggregation mechanism that enforces representation stability. Our theoretical analysis shows that EXCAP provides smooth and stable explanations over time and is robust to perturbations in causal masks. Extensive experiments on classification and forecasting benchmarks demonstrate that EXCAP achieves strong predictive accuracy while generating coherent and causally grounded explanations. These results show that EXCAP offers a principled and scalable approach to interpretable modeling of long time series with relevance to high-stakes domains such as healthcare and finance.

</details>


### [543] [Fourier Neural Operators Explained: A Practical Perspective](https://arxiv.org/abs/2512.01421)
*Valentin Duruisseaux,Jean Kossaifi,Anima Anandkumar*

Main category: cs.LG

TL;DR: 本文提供了傅里叶神经算子（FNO）的全面实践指南，结合数学原理与实现策略，旨在帮助研究者正确理解和应用FNO于科学与工程中的偏微分方程求解。


<details>
  <summary>Details</summary>
Motivation: 尽管FNO在PDE建模中表现出色，但其理论基础和实现细节常被误解，导致应用不当。本文旨在弥合理论与实践之间的鸿沟。

Method: 通过整合算子理论与信号处理概念，系统阐述FNO的谱参数化设计及其组件的计算实现，并结合NeuralOperator 2.0.0库提供模块化代码支持。

Result: 建立了FNO的清晰、可靠的应用框架，澄清了文献中的常见误解，并提供了与理论一致的高质量实现方案。

Conclusion: 该指南有效连接了FNO的理论基础与实际应用，有助于其在多学科领域中的正确和高效使用。

Abstract: Partial differential equations (PDEs) govern a wide variety of dynamical processes in science and engineering, yet obtaining their numerical solutions often requires high-resolution discretizations and repeated evaluations of complex operators, leading to substantial computational costs. Neural operators have recently emerged as a powerful framework for learning mappings between function spaces directly from data, enabling efficient surrogate models for PDE systems. Among these architectures, the Fourier Neural Operator (FNO) has become the most influential and widely adopted due to its elegant spectral formulation, which captures global correlations through learnable transformations in Fourier space while remaining invariant to discretization and resolution. Despite their success, the practical use of FNOs is often hindered by an incomplete understanding among practitioners of their theoretical foundations, practical constraints, and implementation details, which can lead to their incorrect or unreliable application. This work presents a comprehensive and practice-oriented guide to FNOs, unifying their mathematical principles with implementation strategies. We provide an intuitive exposition to the concepts of operator theory and signal-processing that underlie the FNO, detail its spectral parameterization and the computational design of all its components, and address common misunderstandings encountered in the literature. The exposition is closely integrated with the NeuralOperator 2.0.0 library, offering modular state-of-the-art implementations that faithfully reflect the theory. By connecting rigorous foundations with practical insight, this guide aims to establish a clear and reliable framework for applying FNOs effectively across diverse scientific and engineering fields.

</details>


### [544] [Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging](https://arxiv.org/abs/2512.01461)
*Kuangpu Guo,Yuhe Ding,Jian Liang,Zilei Wang,Ran He*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解、阈值划分和缩放的模型融合方法DTS，能够在极低存储开销下有效保留任务特定信息，并在多任务和未见任务上实现优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法在多任务场景中常出现性能下降，难以有效保留任务特定信息，因此需要一种高效且具备良好泛化能力的融合框架。

Method: 提出DTS框架：首先对任务特定信息进行奇异值分解并保留少量奇异值和向量；然后通过新颖的阈值策略将奇异向量元素分组并分配缩放因子；进一步设计了一种基于任务语义相似性的无数据融合变体以提升对未见任务的泛化能力。

Result: 实验表明，DTS在多个任务上 consistently 优于现有最先进方法，仅需1%的额外存储开销；在未见任务上的变体也表现出显著更好的泛化性能。

Conclusion: DTS是一种高效、可扩展的模型融合方法，能有效平衡性能、存储与泛化能力，为多任务和开放世界场景下的模型合并提供了新思路。

Abstract: Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at https://github.com/krumpguo/DTS.

</details>


### [545] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2512.01465)
*Hongnan Si,Tong Li,Yujie Chen,Xin Liao*

Main category: cs.LG

TL;DR: 本文提出了一种神经Tucker卷积网络（NTCN）模型，用于水质量数据缺失值填补，通过Tucker张量交互和3D卷积提取时空特征，在多个真实数据集上优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 长期水质监测中常因传感器故障等原因导致数据缺失，给水质分析带来巨大挑战。

Method: 提出神经Tucker卷积网络（NTCN）：a) 将不同模态实体编码为嵌入向量，并通过外积构建Tucker交互张量以捕获模态间复杂特征交互；b) 利用3D卷积从该张量中提取细粒度时空特征。

Result: 在三个真实水质数据集上的实验表明，NTCN在填补精度上优于多种当前先进填补模型。

Conclusion: NTCN是一种有效且高性能的水质数据缺失填补方法，能更好地建模多模态时空依赖关系。

Abstract: Water quality monitoring is a core component of ecological environmental protection. However, due to sensor failure or other inevitable factors, data missing often exists in long-term monitoring, posing great challenges in water quality analysis. This paper proposes a Neural Tucker Convolutional Network (NTCN) model for water quality data imputation, which features the following key components: a) Encode different mode entities into respective embedding vectors, and construct a Tucker interaction tensor by outer product operations to capture the complex mode-wise feature interactions; b) Use 3D convolution to extract fine-grained spatiotemporal features from the interaction tensor. Experiments on three real-world water quality datasets show that the proposed NTCN model outperforms several state-of-the-art imputation models in terms of accuracy.

</details>


### [546] [Differentiable Weightless Controllers: Learning Logic Circuits for Continuous Control](https://arxiv.org/abs/2512.01467)
*Fabian Kresse,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文提出了一种名为可微无权重控制器（DWC）的新架构，用离散逻辑电路替代连续神经网络来表示和学习连续控制策略。DWC具备可微性，支持端到端梯度训练，并能直接编译为低延迟、超低功耗的FPGA电路，在多个MuJoCo任务中性能媲美神经网络，且具有结构稀疏性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索是否能用离散逻辑电路（而非传统连续神经网络）来表示和学习连续控制策略，以兼顾高性能、低功耗、低延迟与可解释性。

Method: 提出Differentiable Weightless Controllers（DWC），采用温度编码输入、稀疏连接的布尔查找表层和轻量动作头；支持端到端梯度训练，并可直接编译为FPGA兼容电路。

Result: 在五个MuJoCo基准（含高维Humanoid）上，DWC取得与全精度或量化神经网络相当的回报，四任务性能持平，HalfCheetah上揭示网络容量是主要瓶颈；同时展现出结构稀疏、可解释的连接模式。

Conclusion: 连续控制策略可用可微符号化逻辑电路有效建模；DWC在性能、能效、延迟和可解释性方面展现出显著优势，为边缘智能控制提供了新范式。

Abstract: We investigate whether continuous-control policies can be represented and learned as discrete logic circuits instead of continuous neural networks. We introduce Differentiable Weightless Controllers (DWCs), a symbolic-differentiable architecture that maps real-valued observations to actions using thermometer-encoded inputs, sparsely connected boolean lookup-table layers, and lightweight action heads. DWCs can be trained end-to-end by gradient-based techniques, yet compile directly into FPGA-compatible circuits with few- or even single-clock-cycle latency and nanojoule-level energy cost per action. Across five MuJoCo benchmarks, including high-dimensional Humanoid, DWCs achieve returns competitive with weight-based policies (full precision or quantized neural networks), matching performance on four tasks and isolating network capacity as the key limiting factor on HalfCheetah. Furthermore, DWCs exhibit structurally sparse and interpretable connectivity patterns, enabling a direct inspection of which input thresholds influence control decisions.

</details>


### [547] [Does Flatness imply Generalization for Logistic Loss in Univariate Two-Layer ReLU Network?](https://arxiv.org/abs/2512.01473)
*Dan Qiao,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 本文研究了在逻辑损失下，过参数化两层ReLU神经网络的泛化能力，发现平坦性对泛化的影响比平方损失下更微妙：局部平坦解在特定区域内具有近似最优泛化界，但存在无限平坦却过拟合的解，表明仅靠平坦性不足以保证泛化。


<details>
  <summary>Details</summary>
Motivation: 现有工作在平方损失下已证明平坦解不会过拟合，但在逻辑损失下该结论是否成立仍不清楚；尤其梯度下降在逻辑损失下常收敛到插值解，使得平坦性与泛化的关系变得模糊。

Method: 理论分析结合构造性反例与受控仿真实验：通过刻画‘不确定集’界定平坦解的泛化区域，并构造出处处‘确定’却任意平坦且过拟合的解；最后用数值模拟验证理论预测。

Result: 1）在左右两个不确定集之间，平坦解具有近似最优泛化界；2）存在在无穷远处任意平坦、处处‘确定’但仍过拟合的反例；3）仿真结果支持理论发现。

Conclusion: 平坦性本身在逻辑损失下不能单独保证泛化，其作用依赖于解的局部结构（如不确定集位置），需结合其他几何或统计性质进行更精细刻画。

Abstract: We consider the problem of generalization of arbitrarily overparameterized two-layer ReLU Neural Networks with univariate input. Recent work showed that under square loss, flat solutions (motivated by flat / stable minima and Edge of Stability phenomenon) provably cannot overfit, but it remains unclear whether the same phenomenon holds for logistic loss. This is a puzzling open problem because existing work on logistic loss shows that gradient descent with increasing step size converges to interpolating solutions (at infinity, for the margin-separable cases). In this paper, we prove that the \emph{flatness implied generalization} is more delicate under logistic loss. On the positive side, we show that flat solutions enjoy near-optimal generalization bounds within a region between the left-most and right-most \emph{uncertain} sets determined by each candidate solution. On the negative side, we show that there exist arbitrarily flat yet overfitting solutions at infinity that are (falsely) certain everywhere, thus certifying that flatness alone is insufficient for generalization in general. We demonstrate the effects predicted by our theory in a well-controlled simulation study.

</details>


### [548] [Multi-view diffusion geometry using intertwined diffusion trajectories](https://arxiv.org/abs/2512.01484)
*Gwendal Debaussart-Joniec,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文提出了一种统一的多视角扩散几何框架，通过交织的多视角扩散轨迹（MDTs）来建模多数据视角间的相互作用，具有清晰的概率与几何解释，并展示了其在流形学习和数据聚类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了更好地融合和理解多视角数据中的复杂交互，现有方法缺乏灵活性和理论支持，因此需要一个统一且可扩展的框架。

Method: 引入交织的多视角扩散轨迹（MDTs），构建随时间演变的轨迹依赖扩散算子，结合多个数据视角的随机游走算子，并推导出相应的扩散距离和基于SVD的嵌入表示，同时提出多种MDT算子学习策略。

Result: 建立了MDT的理论性质（如遍历性），提出了可解释的扩散距离与嵌入方法，实验表明所提MDT算子在流形学习和聚类任务中表现优异。

Conclusion: MDTs提供了一个灵活、理论完备的多视角扩散建模范式，不仅统一了现有模型，还可作为评估其他扩散方法的中性基线。

Abstract: This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.

</details>


### [549] [Winning Solutions for the Rayan AI Contest: Compositional Retrieval, Zero-Shot Anomaly Detection, and Backdoor Detection](https://arxiv.org/abs/2512.01498)
*Ali Nafisi,Sina Asghari,Mohammad Saeed Arvenaghi,Hossein Shakibania*

Main category: cs.LG

TL;DR: 本文提出了针对组合图像检索、零样本异常检测和后门模型检测三个机器学习挑战的解决方案，并在前两项任务中取得第一名，第三项第二名，展示了方法在检索、异常检测和模型安全方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中的关键挑战，包括复杂场景下的图像检索、无需异常样本训练的异常检测，以及神经网络中隐藏后门的识别，以提升实际应用中的系统可靠性与安全性。

Method: 在组合图像检索中结合视觉与文本输入进行多模态处理；在零样本异常检测中设计能识别并定位未知异常的模型；在后门检测中提出检测神经网络中隐藏触发器的方法。

Result: 组合图像检索达到95.38%准确率（第一），零样本异常检测达到73.14%准确率（第一），后门模型检测达到78%准确率（第二）。

Conclusion: 所提方法在三项挑战中均表现优异，验证了其在检索、异常检测与模型安全领域的有效性，对医疗、制造和网络安全等实际应用场景具有重要意义。

Abstract: This report presents solutions to three machine learning challenges: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection. In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38\% accuracy and ranking first with a clear margin over the second team. For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing 1st place with 73.14\% accuracy. In the backdoored model detection task, we proposed a method to detect hidden backdoor triggers in neural networks, reaching an accuracy of 78\%, which placed our approach in second place. These results demonstrate the effectiveness of our methods in addressing key challenges related to retrieval, anomaly detection, and model security, with implications for real-world applications in industries such as healthcare, manufacturing, and cybersecurity. Code for all solutions is available online.

</details>


### [550] [Walking on the Fiber: A Simple Geometric Approximation for Bayesian Neural Networks](https://arxiv.org/abs/2512.01500)
*Alfredo Reichlin,Miguel Vasco,Danica Kragic*

Main category: cs.LG

TL;DR: 本文提出了一种针对过参数化神经网络的贝叶斯后验采样新方法，通过利用损失极小值的低维结构并学习参数空间的形变，实现高效、可扩展的后验近似。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络中精确后验推断不可行，而常用近似方法（如Laplace法）在现代深度网络中面临可扩展性与后验精度不足的问题。

Method: 提出一种面向过参数化网络的采样技术变体，利用损失极小值的低维结构，并引入一个学习参数空间形变的模型，从而避免迭代过程、实现快速后验采样。

Result: 实验表明该方法在后验近似质量上具有竞争力，且相比近期改进技术显著提升了可扩展性。

Conclusion: 为深度学习中的贝叶斯推理提供了一种实用、高效的新替代方案。

Abstract: Bayesian Neural Networks provide a principled framework for uncertainty quantification by modeling the posterior distribution of network parameters. However, exact posterior inference is computationally intractable, and widely used approximations like the Laplace method struggle with scalability and posterior accuracy in modern deep networks. In this work, we revisit sampling techniques for posterior exploration, proposing a simple variation tailored to efficiently sample from the posterior in over-parameterized networks by leveraging the low-dimensional structure of loss minima. Building on this, we introduce a model that learns a deformation of the parameter space, enabling rapid posterior sampling without requiring iterative methods. Empirical results demonstrate that our approach achieves competitive posterior approximations with improved scalability compared to recent refinement techniques. These contributions provide a practical alternative for Bayesian inference in deep learning.

</details>


### [551] [Label Forensics: Interpreting Hard Labels in Black-Box Text Classifier](https://arxiv.org/abs/2512.01514)
*Mengyao Du,Gang Yang,Han Fang,Quanjun Yin,Ee-chien Chang*

Main category: cs.LG

TL;DR: 本文提出了“标签取证”（label forensics）框架，用于在黑盒环境下推断文本分类器中每个标签的语义含义，通过构建具有高一致性和广泛覆盖性的句子嵌入分布来重建标签语义。


<details>
  <summary>Details</summary>
Motivation: 许多自然语言处理模型以黑盒形式传播，缺乏对标签语义的文档说明，带来审计和监管难题，因此需要一种方法来逆向解析这些未记录分类器的内部语义。

Method: 提出一种黑盒框架，通过语义邻域采样器和迭代优化过程选择代表性种子句子，构建能反映标签语义的句子嵌入分布，确保生成样本对目标标签具有高一致性且覆盖广泛的语义空间。

Result: 在多个黑盒分类器上的实验显示平均标签一致性达到92.24%，并在一个未记录的HuggingFace分类器上成功应用，实现了细粒度的标签解释。

Conclusion: 该框架能有效还原黑盒分类器中标签的隐含语义，支持AI系统的可解释性与负责任的审计实践。

Abstract: The widespread adoption of natural language processing techniques has led to an unprecedented growth of text classifiers across the modern web. Yet many of these models circulate with their internal semantics undocumented or even intentionally withheld. Such opaque classifiers, which may expose only hard-label outputs, can operate in unregulated web environments or be repurposed for unknown intents, raising legitimate forensic and auditing concerns. In this paper, we position ourselves as investigators and work to infer the semantic concept each label encodes in an undocumented black-box classifier.
  Specifically, we introduce label forensics, a black-box framework that reconstructs a label's semantic meaning. Concretely, we represent a label by a sentence embedding distribution from which any sample reliably reflects the concept the classifier has implicitly learned for that label. We believe this distribution should maintain two key properties: precise, with samples consistently classified into the target label, and general, covering the label's broad semantic space. To realize this, we design a semantic neighborhood sampler and an iterative optimization procedure to select representative seed sentences that jointly maximize label consistency and distributional coverage. The final output, an optimized seed sentence set combined with the sampler, constitutes the empirical distribution representing the label's semantics. Experiments on multiple black-box classifiers achieve an average label consistency of around 92.24 percent, demonstrating that the embedding regions accurately capture each classifier's label semantics. We further validate our framework on an undocumented HuggingFace classifier, enabling fine-grained label interpretation and supporting responsible AI auditing.

</details>


### [552] [End-to-end Deep Reinforcement Learning for Stochastic Multi-objective Optimization in C-VRPTW](https://arxiv.org/abs/2512.01518)
*Abdo Abouelrous,Laurens Bliek,Yaoxin Wu,Yingqian Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的深度学习模型，结合注意力机制与多解轨迹，用于求解具有旅行时间不确定性和多目标（总旅行时间与路线完工时间）的车辆路径问题，并通过情景聚类优化训练过程，在生成高质量Pareto前沿的同时显著缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 实际决策中常面临环境不确定性及多个利益相关方的冲突目标，传统方法难以高效处理兼具随机性和多目标的车辆路径问题，因此需要一种能同时应对这两项挑战的学习型方法。

Method: 采用基于注意力机制的端到端深度学习模型，引入多解轨迹生成能力，并设计情景聚类的训练机制以降低训练开销，从而同时建模旅行时间不确定性并优化总旅行时间和路线完工时间两个目标。

Result: 实验表明，所提模型在可接受运行时间内相比三个基线方法能生成质量更高的Pareto前沿，验证了其在多目标随机路径规划中的有效性与计算优势。

Conclusion: 该模型能够有效联合处理随机性和多目标优化问题，为现实场景下的智能路径规划提供了高效且实用的学习型解决方案。

Abstract: In this work, we consider learning-based applications in routing to solve a Vehicle Routing variant characterized by stochasticity and multiple objectives. Such problems are representative of practical settings where decision-makers have to deal with uncertainty in the operational environment as well as multiple conflicting objectives due to different stakeholders. We specifically consider travel time uncertainty. We also consider two objectives, total travel time and route makespan, that jointly target operational efficiency and labor regulations on shift length, although different objectives could be incorporated. Learning-based methods offer earnest computational advantages as they can repeatedly solve problems with limited interference from the decision-maker. We specifically focus on end-to-end deep learning models that leverage the attention mechanism and multiple solution trajectories. These models have seen several successful applications in routing problems. However, since travel times are not a direct input to these models due to the large dimensions of the travel time matrix, accounting for uncertainty is a challenge, especially in the presence of multiple objectives. In turn, we propose a model that simultaneously addresses stochasticity and multi-objectivity and provide a refined training mechanism for this model through scenario clustering to reduce training time. Our results show that our model is capable of constructing a Pareto Front of good quality within acceptable run times compared to three baselines.

</details>


### [553] [TimePred: efficient and interpretable offline change point detection for high volume data - with application to industrial process monitoring](https://arxiv.org/abs/2512.01562)
*Simon Leszek*

Main category: cs.LG

TL;DR: TimePred是一种自监督框架，将高维时间序列的变点检测简化为单变量均值偏移检测，从而提升计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高维、大规模时间序列中的变点检测面临统计一致性、可扩展性和可解释性的挑战。

Method: TimePred通过预测每个样本的归一化时间索引，将多元变点检测降维为单变量均值偏移检测，并支持XAI归因方法实现特征级解释。

Result: 实验表明TimePred在保持竞争力的同时，计算成本降低达两个数量级；工业制造案例验证了其检测精度提升与可解释性价值。

Conclusion: TimePred在保证变点检测性能的同时显著提升了效率与可解释性，适用于实际工业场景。

Abstract: Change-point detection (CPD) in high-dimensional, large-volume time series is challenging for statistical consistency, scalability, and interpretability. We introduce TimePred, a self-supervised framework that reduces multivariate CPD to univariate mean-shift detection by predicting each sample's normalized time index. This enables efficient offline CPD using existing algorithms and supports the integration of XAI attribution methods for feature-level explanations. Our experiments show competitive CPD performance while reducing computational cost by up to two orders of magnitude. In an industrial manufacturing case study, we demonstrate improved detection accuracy and illustrate the practical value of interpretable change-point insights.

</details>


### [554] [Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism](https://arxiv.org/abs/2512.01568)
*Sandro Andric*

Main category: cs.LG

TL;DR: 本文通过多方法实验（IAT、行为选择、自我评估）检验24个前沿大语言模型的利他倾向，发现模型普遍存在隐性利他偏好但自我报告显著高估实际利他行为（'美德信号差距'），提出'校准差距'作为对齐新指标。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具备真实利他倾向，并检验其隐性关联、自我报告能否预测实际行为，以评估模型价值观对齐程度。

Method: 采用受人类社会心理学启发的多范式方法：隐式联想测验（IAT）测量隐性利他偏差、强制二元选择任务测量行为利他性、自我评估量表测量显性利他信念，测试24个前沿LLMs。

Result: 所有模型均呈现强隐性利他偏好（IAT均值0.87）；行为利他率65.6%，显著高于随机水平但个体差异大（48%-85%）；隐性关联与行为无显著相关（r=0.22）；75%模型存在显著自我高估（报告77.5% vs 行为65.6%，d=1.08）；仅12.5%模型兼具高利他行为与准确自知。

Conclusion: 模型的自我报告不能可靠反映其实际行为，'校准差距'（自我报告与行为价值的差异）应作为标准化对齐评估指标，提升模型可预测性与行为一致性。

Abstract: We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.

</details>


### [555] [Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade](https://arxiv.org/abs/2512.01572)
*Letian Yi,Tingpeng Zhang,Mingyuan Zhou,Guannan Wang,Quanke Su,Zhilu Lai*

Main category: cs.LG

TL;DR: 本文提出了一种名为Cascaded Sensing（Cas-Sensing）的分层重建框架，结合功能自编码器与条件扩散模型，从极稀疏随机采样中高保真重建全场，通过贝叶斯后验采样施加流形约束提升一致性。


<details>
  <summary>Details</summary>
Motivation: 解决从极稀疏、随机测量中重建全物理场这一长期存在的病态反问题。

Method: 提出Cas-Sensing框架：1）神经算子驱动的功能自编码器重建大尺度结构与几何边界作为中间变量；2）采用掩码级联策略训练的条件扩散模型生成细粒度细节；3）在生成过程中基于贝叶斯后验采样施加流形约束以保证测量一致性。

Result: 在仿真与真实数据集上验证了Cas-Sensing在不同传感器配置和几何边界下的强泛化能力，重建精度与鲁棒性显著提升。

Conclusion: Cas-Sensing通过分层概率建模有效缓解病态性，为科学与工程实际应用提供了可靠、通用的稀疏场重建新范式。

Abstract: Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.

</details>


### [556] [Scaling and context steer LLMs along the same computational path as the human brain](https://arxiv.org/abs/2512.01591)
*Joséphine Raugel,Stéphane d'Ascoli,Jérémy Rapin,Valentin Wyart,Jean-Rémi King*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLMs）与人脑在处理语言时表征序列的对齐关系，发现LLM各层激活与大脑不同时间响应存在层级对应：浅层对应早期脑响应，深层对应晚期脑响应，且该现象跨架构一致，但依赖于模型规模和上下文长度。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs与人脑表征对齐是否源于相似的计算序列，以及这种对齐背后的机制。

Method: 分析参与者聆听10小时有声书时的时序分辨脑信号，并与22个不同规模和架构的LLM进行联合比对，考察各层激活与脑响应的时间动态匹配关系。

Result: 发现LLM浅层激活更匹配早期脑响应，深层更匹配晚期脑响应；该对齐模式在Transformer和RNN架构中均成立，但受模型大小和上下文长度影响。

Conclusion: LLMs与人脑在语言处理中存在序列性计算对齐，表明二者在信息加工流程上存在部分收敛，其程度取决于模型容量与建模范围。

Abstract: Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether and why this alignment score arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 22 LLMs varying in size and architecture type. Our analyses confirm that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, this study sheds light on the sequential nature of computations and the factors underlying the partial convergence between biological and artificial neural networks.

</details>


### [557] [In-context Inverse Optimality for Fair Digital Twins: A Preference-based approach](https://arxiv.org/abs/2512.01650)
*Daniele Masti,Francesco Basciani,Arianna Fedeli,Girgio Gnecco,Francesco Smarra*

Main category: cs.LG

TL;DR: 本文提出一种将公平性作为可学习目标嵌入基于优化的数字孪生系统的新框架，通过人类成对偏好学习隐式公平目标，并利用新型Siamese神经网络生成上下文相关的凸二次代价函数，从而在保持计算效率的同时使决策结果更符合人类对公平的感知。


<details>
  <summary>Details</summary>
Motivation: 数字孪生系统的数学最优决策常与人类预期不一致，暴露出算法理性与人类有限理性之间的持续鸿沟，亟需将人类中心的公平性融入自主决策系统。

Method: 提出偏好驱动的学习流程，从人类对可行决策的成对偏好中推断潜在公平目标；设计新型Siamese神经网络，生成依赖于上下文信息的凸二次代价函数作为代理目标。

Result: 所提方法在COVID-19医院资源分配场景中验证有效，能将优化结果与人类感知的公平性对齐，同时保持计算高效性。

Conclusion: 该研究为在自主决策系统（尤其是数字孪生）的设计中嵌入以人为本的公平性提供了可操作路径。

Abstract: Digital Twins (DTs) are increasingly used as autonomous decision-makers in complex socio-technical systems. Their mathematically optimal decisions often diverge from human expectations, exposing a persistent gap between algorithmic and bounded human rationality. This work addresses this gap by proposing a framework that operationalizes fairness as a learnable objective within optimization-based Digital Twins. We introduce a preference-driven learning pipeline that infers latent fairness objectives directly from human pairwise preferences over feasible decisions. A novel Siamese neural network is developed to generate convex quadratic cost functions conditioned on contextual information. The resulting surrogate objectives align optimization outcomes with human-perceived fairness while maintaining computational efficiency. The approach is demonstrated on a COVID-19 hospital resource allocation scenario. This study provides an actionable path toward embedding human-centered fairness in the design of autonomous decision-making systems.

</details>


### [558] [ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models](https://arxiv.org/abs/2512.01672)
*Zhongyuan Wu,Jingyuan Wang,Zexuan Cheng,Yilong Zhou,Weizhi Wang,Juhua Pu,Chao Li,Changqing Ma*

Main category: cs.LG

TL;DR: 本文提出了一种新的异常检测范式ICAD，利用大语言模型的上下文学习能力，在统一框架下处理多模态异构数据，并具备强泛化能力，可快速适应新场景。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法大多局限于单一模态，缺乏跨域泛化能力，难以应对现代动态环境中多样且互联的多模态数据。因此需要一种能统一处理异构数据并快速适应新任务的异常检测方法。

Method: 提出In-Context Anomaly Detection (ICAD) 范式，将异常定义为与正常样本参考集的差异；基于此构建ICAD-LLM框架，利用大语言模型的上下文学习能力，在统一模型中处理时间序列、日志、表格等多种数据模态。

Result: 实验表明ICAD-LLM在多种任务上性能媲美专用异常检测方法，并展现出对未见任务的强大泛化能力，显著降低部署成本，实现快速环境适应。

Conclusion: ICAD-LLM是首个能够跨多个领域和模态执行异常检测的统一框架，验证了大语言模型在异常检测中的潜力，为未来通用异常检测系统提供了新方向。

Abstract: Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.

</details>


### [559] [Morphling: Fast, Fused, and Flexible GNN Training at Scale](https://arxiv.org/abs/2512.01678)
*Anubhab,Rupesh Nasre*

Main category: cs.LG

TL;DR: 本文提出Morphling，一种面向图神经网络（GNN）的领域专用代码合成器，通过架构感知的原语库与稀疏感知运行时引擎，显著提升CPU/GPU上的训练吞吐量（平均20X/19X）并大幅降低内存占用（最高15X）。


<details>
  <summary>Details</summary>
Motivation: 现有GNN框架（如PyG、DGL）使用通用内核，难以兼顾图遍历的不规则性与矩阵运算的规则性，导致缓存局部性差、内存搬运多、中间内存分配大。

Method: Morphling将高层GNN描述编译为针对OpenMP、CUDA和MPI后端的专用实现；引入架构感知的优化原语库，并设计稀疏感知运行时引擎，依据输入特征统计动态选择稠密或稀疏执行路径。

Result: 在11个真实数据集上，Morphling相较PyG和DGL，CPU平均提速20倍、GPU平均提速19倍，峰值达66倍；峰值内存消耗最多降低15倍。

Conclusion: 面向架构的专用代码合成是实现跨平台高性能GNN执行的有效且可扩展路径。

Abstract: Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.

</details>


### [560] [A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations](https://arxiv.org/abs/2512.01702)
*Bei Zhou,Cesare Corrado,Shuang Qian,Maximilian Balmus,Angela W. C. Lee,Cristobal Rodero,Marco J. W. Gotte,Luuk H. G. A. Hopman,Mengyun Qiao,Steven Niederer*

Main category: cs.LG

TL;DR: 本文提出了一种几何无关的神经算子框架，用于在多种左心房解剖结构上快速、准确地预测局部激活时间（LAT）场，显著提升心律失常个性化治疗中电生理模拟的实时性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于生物物理的房颤电生理模拟计算成本高，难以满足临床实时应用或大规模人群分析需求。

Method: 构建了包含308,700次GPU加速仿真的数据集，覆盖147例患者特异性左心房几何结构及多点起搏与传导参数变化；引入‘通用心房坐标系’统一表征解剖与功能数据；设计基于视觉Transformer的神经算子学习结构/电生理输入到LAT场的映射。

Result: 模型平均预测误差为5.1 ms（最大模拟时长455 ms），单样本推理耗时仅0.12 ms，性能优于现有算子学习方法。

Conclusion: 该框架实现了跨解剖变异的域不变生物物理映射学习，为将计算电生理无缝集成至实时及大规模临床工作流提供了通用新范式。

Abstract: Accurate maps of atrial electrical activation are essential for personalised treatment of arrhythmias, yet biophysically detailed simulations remain computationally intensive for real-time clinical use or population-scale analyses. Here we introduce a geometry-independent operator-learning framework that predicts local activation time (LAT) fields across diverse left atrial anatomies with near-instantaneous inference. We generated a dataset of 308,700 simulations using a GPU-accelerated electrophysiology solver, systematically varying multiple pacing sites and physiologically varied conduction properties across 147 patient-specific geometries derived from two independent clinical cohorts. All anatomical and functional data are expressed in a Universal Atrium Coordinate system, providing a consistent representation that decouples electrophysiological patterns from mesh topology. Within this coordinate space, we designed a neural operator with a vision-transformer backbone to learn the mapping from structural and electrophysiological inputs to LAT fields. With a mean prediction error of 5.1 ms over a 455 ms maximum simulation time, the model outperforms established operator-learning approaches and performs inference in 0.12 ms per sample. Our framework establishes a general strategy for learning domain-invariant biophysical mappings across variable anatomical domains and enables integration of computational electrophysiology into real-time and large-scale clinical workflows.

</details>


### [561] [Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method](https://arxiv.org/abs/2512.01732)
*Yan Huang,Jinming Xu,Jiming Chen,Karl Henrik Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种统一的时空梯度跟踪算法ST-GT，用于时变图上的分布式随机优化，通过跨节点跟踪全局梯度缓解数据异构性，并通过局部梯度滑动平均抑制噪声，在无需有界异构性假设下实现了强凸问题的线性收敛和非凸问题的次线性收敛，且首次在强凸情形下实现通信复杂度关于本地更新步数τ的线性加速。


<details>
  <summary>Details</summary>
Motivation: 分布式与联邦学习中，多步本地更新虽降低通信开销，但因节点间数据异构性和本地梯度噪声，易导致本地模型偏离全局最优。

Method: 从梯度跟踪视角重新审视Scaffold算法，提出时空梯度跟踪算法ST-GT：一方面在邻居节点间跟踪全局梯度以缓解数据异构性，另一方面维护本地梯度的运行平均以抑制噪声。

Result: ST-GT在无数据异构性有界假设下，对强凸问题达到线性收敛、非凸问题达到次线性收敛；首次实现通信复杂度关于本地更新步数τ的线性加速；将拓扑相关噪声项由σ²降至σ²/τ。

Conclusion: ST-GT是一种高效、鲁棒的分布式优化算法，显著提升通信效率与收敛性能，尤其适用于数据异构、图结构动态变化的实际场景。

Abstract: In distributed and federated learning algorithms, communication overhead is often reduced by performing multiple local updates between communication rounds. However, due to data heterogeneity across nodes and the local gradient noise within each node, this strategy can lead to the drift of local models away from the global optimum. To address this issue, we revisit the well-known federated learning method Scaffold (Karimireddy et al., 2020) under a gradient tracking perspective, and propose a unified spatio-temporal gradient tracking algorithm, termed ST-GT, for distributed stochastic optimization over time-varying graphs. ST-GT tracks the global gradient across neighboring nodes to mitigate data heterogeneity, while maintaining a running average of local gradients to substantially suppress noise, with slightly more storage overhead. Without assuming bounded data heterogeneity, we prove that ST-GT attains a linear convergence rate for strongly convex problems and a sublinear rate for nonconvex cases. Notably, ST-GT achieves the first linear speed-up in communication complexity with respect to the number of local updates per round $τ$ for the strongly-convex setting. Compared to traditional gradient tracking methods, ST-GT reduces the topology-dependent noise term from $σ^2$ to $σ^2/τ$, where $σ^2$ denotes the noise level, thereby improving communication efficiency.

</details>


### [562] [Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials](https://arxiv.org/abs/2512.01735)
*Marius Tacke,Matthias Busch,Kian Abdolazizi,Jonas Eichinger,Kevin Linka,Christian Cyron,Roland Aydin*

Main category: cs.LG

TL;DR: 本文提出了一种利用大语言模型（LLM）按需生成物理约束神经网络（如本构人工神经网络CANN）的框架，用于固体力学中的本构建模，显著降低专家依赖并实现端到端自动化。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的本构建模方法（如CANN）仍高度依赖专家知识和人工劳动，亟需降低门槛、提升自动化水平。

Method: 构建一个LLM驱动的框架，支持根据用户提供的材料类别和数据集，自动完成CANN的架构选择、物理约束嵌入及完整代码生成。

Result: 在三个基准问题上，LLM生成的CANN在精度上媲美甚至超越人工设计模型，并具备良好的泛化性和大变形外推能力。

Conclusion: LLM可有效生成物理约束神经网络，大幅减少本构建模所需专业知识，是迈向实用化端到端自动化的关键一步。

Abstract: Large language model (LLM)-based agentic frameworks increasingly adopt the paradigm of dynamically generating task-specific agents. We suggest that not only agents but also specialized software modules for scientific and engineering tasks can be generated on demand. We demonstrate this concept in the field of solid mechanics. There, so-called constitutive models are required to describe the relationship between mechanical stress and body deformation. Constitutive models are essential for both the scientific understanding and industrial application of materials. However, even recent data-driven methods of constitutive modeling, such as constitutive artificial neural networks (CANNs), still require substantial expert knowledge and human labor. We present a framework in which an LLM generates a CANN on demand, tailored to a given material class and dataset provided by the user. The framework covers LLM-based architecture selection, integration of physical constraints, and complete code generation. Evaluation on three benchmark problems demonstrates that LLM-generated CANNs achieve accuracy comparable to or greater than manually engineered counterparts, while also exhibiting reliable generalization to unseen loading scenarios and extrapolation to large deformations. These findings indicate that LLM-based generation of physics-constrained neural networks can substantially reduce the expertise required for constitutive modeling and represent a step toward practical end-to-end automation.

</details>


### [563] [MSPT: Efficient Large-Scale Physical Modeling via Parallelized Multi-Scale Attention](https://arxiv.org/abs/2512.01738)
*Pedro M. P. Curvo,Jan-Willem van de Meent,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 本文提出了多尺度Patch Transformer（MSPT），结合局部点注意力与全局粗粒度Patch级注意力，利用Ball树划分空间区域，有效处理大规模不规则几何体的物理仿真问题，在多个PDE基准和大型空气动力学数据集上实现了SOTA精度，同时显著降低内存与计算开销。


<details>
  <summary>Details</summary>
Motivation: 神经求解器在工业级物理仿真中面临关键可扩展性挑战：需同时高效建模百万级空间单元的细粒度局部交互与长程全局依赖。

Method: 提出多尺度Patch Transformer（MSPT），融合patch内局部点注意力与面向粗粒度patch表示的全局注意力；采用ball tree对输入域进行空间一致的patch划分，以适配非规则几何结构。

Result: 在弹性、塑性、流体动力学、多孔流动等标准PDE基准及ShapeNet-Car、Ahmed-ML等大规模气动数据集上达到SOTA精度，单GPU即可扩展至百万级点，且内存占用与计算成本显著降低。

Conclusion: MSPT通过双尺度注意力机制与ball tree空间划分，成功平衡了精度、效率与可扩展性，为大规模物理仿真提供了实用的神经求解框架。

Abstract: A key scalability challenge in neural solvers for industrial-scale physics simulations is efficiently capturing both fine-grained local interactions and long-range global dependencies across millions of spatial elements. We introduce the Multi-Scale Patch Transformer (MSPT), an architecture that combines local point attention within patches with global attention to coarse patch-level representations. To partition the input domain into spatially-coherent patches, we employ ball trees, which handle irregular geometries efficiently. This dual-scale design enables MSPT to scale to millions of points on a single GPU. We validate our method on standard PDE benchmarks (elasticity, plasticity, fluid dynamics, porous flow) and large-scale aerodynamic datasets (ShapeNet-Car, Ahmed-ML), achieving state-of-the-art accuracy with substantially lower memory footprint and computational cost.

</details>


### [564] [SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models](https://arxiv.org/abs/2512.01748)
*Stella Etuk,Ashraf Matrawy*

Main category: cs.LG

TL;DR: 本文提出了一种敏感性感知的差分隐私方法SA-ADP，可根据个人身份信息（PII）的敏感程度自适应分配噪声，在保护隐私的同时有效保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练过程中可能记忆并泄露个人身份信息（PII），传统差分隐私方法如DP-SGD通过统一加噪提供保护，但会显著损害模型效用，因此需要一种更精细的隐私保护机制。

Method: 提出SA-ADP方法，根据PII的不同敏感级别动态调整噪声分配，实现敏感性感知的差分隐私训练。

Result: 在ABCD、CUSTOMERSIM、Wikitext-2和UNSW-NB15四个数据集上实验表明，SA-ADP在隐私保护水平与DP-SGD相当的同时，模型性能与无差分隐私的基线方法（No-DP）相近，未明显损失效用。

Conclusion: SA-ADP通过敏感性感知的噪声分配机制，在不牺牲模型性能的前提下实现了强隐私保护，缓解了隐私与效用之间的权衡问题。

Abstract: Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.

</details>


### [565] [Mofasa: A Step Change in Metal-Organic Framework Generation](https://arxiv.org/abs/2512.01756)
*Vaidotas Simkus,Anders Christensen,Steven Bennett,Ian Johnson,Mark Neumann,James Gin,Jonathan Godwin,Benjamin Rhodes*

Main category: cs.LG

TL;DR: 本文介绍了Mofasa，一种用于生成金属有机框架（MOFs）的全原子潜在扩散模型，具备同时采样原子位置、原子类型和晶格向量的能力，并发布了包含数十万MOF结构的数据库MofasaDB。


<details>
  <summary>Details</summary>
Motivation: 尽管MOFs具有巨大的应用价值和可设计性，但此前尚缺乏高性能的生成模型来高效探索其庞大的组合设计空间。

Method: 提出了一种名为Mofasa的通用潜在扩散模型，能够联合采样原子位置、原子类型和晶格向量，适用于多达500原子的系统，避免了传统依赖手工组装算法的方法。

Result: Mofasa实现了MOF生成的最先进性能，并发布了一个大规模标注MOF结构数据库MofasaDB及配套的交互式网络平台。

Conclusion: Mofasa为MOF材料的理性设计与发现提供了强大新工具，推动了生成式AI在化学与材料科学中的实际应用。

Abstract: Mofasa is an all-atom latent diffusion model with state-of-the-art performance for generating Metal-Organic Frameworks (MOFs). These are highly porous crystalline materials used to harvest water from desert air, capture carbon dioxide, store toxic gases and catalyse chemical reactions. In recognition of their value, the development of MOFs recently received a Nobel Prize in Chemistry.
  In many ways, MOFs are well-suited for exploiting generative models in chemistry: they are rationally-designable materials with a large combinatorial design space and strong structure-property couplings. And yet, to date, a high performance generative model has been lacking. To fill this gap, we introduce Mofasa, a general-purpose latent diffusion model that jointly samples positions, atom-types and lattice vectors for systems as large as 500 atoms. Mofasa avoids handcrafted assembly algorithms common in the literature, unlocking the simultaneous discovery of metal nodes, linkers and topologies.
  To help the scientific community build on our work, we release MofasaDB, an annotated library of hundreds of thousands of sampled MOF structures, along with a user-friendly web interface for search and discovery: https://mofux.ai/ .

</details>


### [566] [Weight Space Representation Learning with Neural Fields](https://arxiv.org/abs/2512.01759)
*Zhuoqian Yang,Mathieu Salzmann,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: 本文探讨了权重作为有效表示的潜力，特别是在神经场中。通过预训练基础模型和低秩适应（LoRA）约束优化空间，使权重空间具有结构；实验表明乘性LoRA权重在重建、生成与分析任务中表现优异，并在潜在扩散模型中实现更高质量生成。


<details>
  <summary>Details</summary>
Motivation: 探索权重是否可作为神经场中有效的表示形式，并提升其在多任务中的表达能力与语义结构。

Method: 采用预训练基础模型结合低秩适应（LoRA），特别使用乘性LoRA权重，在2D/3D数据的重建、生成与分析任务中进行验证，并集成至潜在扩散模型。

Result: 乘性LoRA权重在多个任务中展现出高表示质量、区分性与语义结构，并在潜在扩散模型中生成质量优于现有权重空间方法。

Conclusion: 权重本身可被结构化并作为高质量、具语义的表示，在神经场及生成建模中具有重要应用潜力。

Abstract: In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.

</details>


### [567] [On the Unreasonable Effectiveness of Last-layer Retraining](https://arxiv.org/abs/2512.01766)
*John C. Hill,Tyler LaBonte,Xinchen Zhang,Vidya Muthukumar*

Main category: cs.LG

TL;DR: 本文研究了最后层重训练（LLR）方法为何能在不平衡验证集上仍提升最差组准确率，发现其有效性主要源于验证集的组平衡性，而非缓解神经坍塌；并指出CB-LLR和AFR等新算法通过隐式组平衡提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解释LLR方法在使用不平衡验证集时仍能显著提升最差组准确率这一‘不合理有效性’现象背后的机制。

Method: 通过实证分析检验两种假设：一是LLR通过缓解神经坍塌提升鲁棒性；二是LLR因验证集具备更好的组平衡性而有效。

Result: 实证结果不支持神经坍塌假说，但强烈支持组平衡假说；进一步揭示CB-LLR和AFR算法本质上实现了隐式组平衡。

Conclusion: LLR的有效性主要取决于验证集的组平衡程度，而非优化动态或模型结构特性；提升鲁棒性的关键在于构造或诱导更平衡的子集。

Abstract: Last-layer retraining (LLR) methods -- wherein the last layer of a neural network is reinitialized and retrained on a held-out set following ERM training -- have garnered interest as an efficient approach to rectify dependence on spurious correlations and improve performance on minority groups. Surprisingly, LLR has been found to improve worst-group accuracy even when the held-out set is an imbalanced subset of the training set. We initially hypothesize that this ``unreasonable effectiveness'' of LLR is explained by its ability to mitigate neural collapse through the held-out set, resulting in the implicit bias of gradient descent benefiting robustness. Our empirical investigation does not support this hypothesis. Instead, we present strong evidence for an alternative hypothesis: that the success of LLR is primarily due to better group balance in the held-out set. We conclude by showing how the recent algorithms CB-LLR and AFR perform implicit group-balancing to elicit a robustness improvement.

</details>


### [568] [How Does RL Post-training Induce Skill Composition? A Case Study on Countdown](https://arxiv.org/abs/2512.01775)
*Simon Park,Simran Kaur,Sanjeev Arora*

Main category: cs.LG

TL;DR: 该研究通过强化学习（RL）后训练探讨大型语言模型在倒计数任务中的技能组合与组成泛化能力，发现模型能实现分布外泛化，并表现出结构依赖的学习层次。


<details>
  <summary>Details</summary>
Motivation: 区分强化学习在推理增强中对长度泛化与组成泛化的影响，探究技能组合的机制及其结构对迁移的影响。

Method: 以倒计数任务为基准，将模型解构为表达式树，追踪子树（即子任务/技能）的形状与成功率变化，分析训练过程中的组成结构演化。

Result: 发现模型具备对更大输入规模和未见树形结构的分布外泛化能力；学习具有结构依赖性：先掌握浅层平衡树，再掌握深层不平衡树，且对右偏结构持续脆弱。

Conclusion: 纯强化学习后训练可促进超越传统指标（如pass@k）的组成式技能复用，但其泛化效果受组合结构影响显著，揭示了学习顺序与失败模式。

Abstract: While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL post-training teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given n numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger n and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability -- models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal.

</details>


### [569] [Dual Randomized Smoothing: Beyond Global Noise Variance](https://arxiv.org/abs/2512.01782)
*Chenhao Sun,Yuhao Mao,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了一种双随机平滑（dual RS）框架，通过输入依赖的噪声方差突破传统随机平滑中全局噪声方差的性能瓶颈，在小半径和大半径下均实现更强的认证鲁棒性，且在CIFAR-10和ImageNet上验证了其有效性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 标准随机平滑（RS）使用全局噪声方差，无法同时在小半径和大半径下取得高性能；需打破该限制以提升整体认证鲁棒性。

Method: 提出双RS框架：（i）设计一个经RS平滑的方差估计器，为每个输入预测局部恒定的最优噪声方差；（ii）将该方差用于标准RS分类器；并引入迭代训练策略联合优化两组件。

Result: 在CIFAR-10上，相比全局方差方法，显著提升各半径（尤其0.5/0.75/1.0）的认证准确率（相对提升19%/24%/21%），仅增加60%推理开销；在ImageNet上全半径有效；并支持路由式鲁棒性增强。

Conclusion: 输入依赖的局部恒定噪声方差在理论和实践上均可行且优越，双RS框架为认证鲁棒性提供了更灵活、高效的新范式。

Abstract: Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small and large radii. To break through the global variance limitation, we propose a dual RS framework which enables input-dependent noise variances. To achieve that, we first prove that RS remains valid with input-dependent noise variances, provided the variance is locally constant around each input. Building on this result, we introduce two components which form our dual RS framework: (i) a variance estimator first predicts an optimal noise variance for each input, (ii) this estimated variance is then used by a standard RS classifier. The variance estimator is independently smoothed via RS to ensure local constancy, enabling flexible design. We also introduce training strategies to iteratively optimize the two components. Extensive experiments on CIFAR-10 show that our dual RS method provides strong performance for both small and large radii-unattainable with global noise variance-while incurring only a 60% computational overhead at inference. Moreover, it consistently outperforms prior input-dependent noise approaches across most radii, with particularly large gains at radii 0.5, 0.75, and 1.0, achieving relative improvements of 19%, 24%, and 21%, respectively. On ImageNet, dual RS remains effective across all radii. Additionally, the dual RS framework naturally provides a routing perspective for certified robustness, improving the accuracy-robustness trade-off with off-the-shelf expert RS models.

</details>


### [570] [The Active and Noise-Tolerant Strategic Perceptron](https://arxiv.org/abs/2512.01783)
*Maria-Florina Blacan,Hedyeh Beyhaghi*

Main category: cs.LG

TL;DR: 本文研究了在战略性代理环境中进行主动学习的算法，提出了一种适用于线性分类器的主动学习方法，在存在策略性特征操纵的情况下仍能保持指数级的标签复杂度优势。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习假设数据是真实的，但在战略性分类场景（如招聘、贷款审批）中，代理会策略性地改变自身特征以获得有利结果，导致观测数据失真，因此需要新的噪声容忍型主动学习算法。

Method: 提出一种改进的Active Perceptron算法，适用于单位球面上均匀分布的数据，在非实现实（nonrealizable）设定下具备噪声鲁棒性，并保证标签查询数和额外错误数均为\tilde{O}(d \ln \frac{1}{ε})。

Result: 该算法在战略性环境下实现了与非战略性情形相当的指数级标签复杂度优势；相比先前的战略性Perceptron工作[ABBN21]，所需标签查询显著减少；且具有计算高效性。

Conclusion: 本文首次将主动学习成功拓展至战略性分类设定，证明了在存在策略性操纵时仍可兼顾高准确性、低标签成本与噪声鲁棒性，为公平、高效决策系统提供了理论基础与实用算法。

Abstract: We initiate the study of active learning algorithms for classifying strategic agents. Active learning is a well-established framework in machine learning in which the learner selectively queries labels, often achieving substantially higher accuracy and efficiency than classical supervised methods-especially in settings where labeling is costly or time-consuming, such as hiring, admissions, and loan decisions. Strategic classification, however, addresses scenarios where agents modify their features to obtain more favorable outcomes, resulting in observed data that is not truthful. Such manipulation introduces challenges beyond those in learning from clean data. Our goal is to design active and noise-tolerant algorithms that remain effective in strategic environments-algorithms that classify strategic agents accurately while issuing as few label requests as possible. The central difficulty is to simultaneously account for strategic manipulation and preserve the efficiency gains of active learning.
  Our main result is an algorithm for actively learning linear separators in the strategic setting that preserves the exponential improvement in label complexity over passive learning previously obtained only in the non-strategic case. Specifically, for data drawn uniformly from the unit sphere, we show that a modified version of the Active Perceptron algorithm [DKM05,YZ17] achieves excess error $ε$ using only $\tilde{O}(d \ln \frac{1}ε)$ label queries and incurs at most $\tilde{O}(d \ln \frac{1}ε)$ additional mistakes relative to the optimal classifier, even in the nonrealizable case, when a $\tildeΩ(ε)$ fraction of inputs have inconsistent labels with the optimal classifier. The algorithm is computationally efficient and, under these distributional assumptions, requires substantially fewer label queries than prior work on strategic Perceptron [ABBN21].

</details>


### [571] [DeepCAVE: A Visualization and Analysis Tool for Automated Machine Learning](https://arxiv.org/abs/2512.01810)
*Sarah Segel,Helena Graf,Edward Bergman,Kristina Thieme,Marcel Wever,Alexander Tornede,Frank Hutter,Marius Lindauer*

Main category: cs.LG

TL;DR: DeepCAVE is an interactive visualization tool for hyperparameter optimization (HPO) that enhances interpretability and debugging of the HPO process in AutoML.


<details>
  <summary>Details</summary>
Motivation: The complexity of hyperparameter optimization makes it difficult to understand and debug, hindering full utilization of ML models.

Method: Development of DeepCAVE, an interactive dashboard for visualizing and analyzing various aspects of the HPO process.

Result: Enables users to identify issues, untapped potentials, and new insights about ML models during tuning.

Conclusion: DeepCAVE improves HPO interpretability at the design level and supports development of more robust and efficient AutoML methodologies.

Abstract: Hyperparameter optimization (HPO), as a central paradigm of AutoML, is crucial for leveraging the full potential of machine learning (ML) models; yet its complexity poses challenges in understanding and debugging the optimization process. We present DeepCAVE, a tool for interactive visualization and analysis, providing insights into HPO. Through an interactive dashboard, researchers, data scientists, and ML engineers can explore various aspects of the HPO process and identify issues, untouched potentials, and new insights about the ML model being tuned. By empowering users with actionable insights, DeepCAVE contributes to the interpretability of HPO and ML on a design level and aims to foster the development of more robust and efficient methodologies in the future.

</details>


### [572] [Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning](https://arxiv.org/abs/2512.01818)
*Lama Alssum,Hasan Abed Al Kader Hammoud,Motasem Alfarra,Juan C Leon Alcazar,Bernard Ghanem*

Main category: cs.LG

TL;DR: 本文提出了一种名为信息最大化（IM）正则器的新方法，用于缓解深度神经网络在持续学习中的灾难性遗忘问题。该正则器仅依赖于期望标签分布，具有类别无关性和数据无关性，可无缝集成到多种基于记忆的持续学习方法中，并在图像和视频任务上均展现出稳定性能提升与低计算开销。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在持续学习中面临灾难性遗忘问题，即学习新任务时旧任务性能显著下降，现有方法在通用性、效率或跨模态适用性方面存在局限。

Method: 提出类别无关的信息最大化（IM）正则器，基于期望标签分布构建，不依赖具体类别信息；将其嵌入各类基于重放（rehearsal）的记忆型持续学习框架中，实现轻量级、可插拔的正则化。

Result: 在多个图像数据集和不同任务数量下，IM正则器持续提升基线方法性能，计算开销极小；进一步验证其在视频持续学习任务中同样有效，证明其数据无关性和泛化能力。

Conclusion: IM正则器是一种高效、通用、可扩展的持续学习正则化方案，兼顾性能提升与实际部署可行性，尤其适用于资源受限和跨模态的真实场景。

Abstract: Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.

</details>


### [573] [Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models](https://arxiv.org/abs/2512.01831)
*Yudi Wu,Wenhao Zhao,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息瓶颈理论的诊断框架，用于分析离散潜在生成模型（如自回归、MIM和扩散模型）在生成多样性上的差异，揭示了其背后‘压缩压力’与‘多样性压力’的权衡，并分解为路径多样性和执行多样性；通过三种零样本干预方法，发现三类模型分别采用‘多样性优先’、‘压缩优先’和‘解耦’策略。


<details>
  <summary>Details</summary>
Motivation: 不同离散潜在生成模型（AR、MIM、Diffusion）在生成多样性上表现差异显著，缺乏统一理论解释；需从信息论角度理解其内在生成策略差异。

Method: 基于信息瓶颈（IB）理论构建诊断框架，定义‘压缩压力’（最小化码本熵）与‘多样性压力’（最大化给定输入的条件熵），将多样性分解为‘路径多样性’和‘执行多样性’；设计三种零样本、推理时干预手段扰动潜在生成过程以探测多样性分配机制。

Result: 在AR、MIM、Diffusion代表性模型上应用该框架，识别出三类策略：MIM为‘多样性优先’、AR为‘压缩优先’、Diffusion为‘解耦’；验证了框架有效性，并据此提出新型推理时多样性增强技术。

Conclusion: 生成多样性差异源于模型对IB目标中压缩与多样性权衡的不同实现方式；该框架提供了可解释、可干预的分析范式，统一解释了不同架构行为，并支撑了实用的多样性调控方法。

Abstract: Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: "Diversity-Prioritized" (MIM), "Compression-Prioritized" (AR), and "Decoupled" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.

</details>


### [574] [Mitigating Gender Bias in Depression Detection via Counterfactual Inference](https://arxiv.org/abs/2512.01834)
*Mingxuan Hu,Hongbo Ma,Xinlan Wu,Ziqi Liu,Jiaqi Liu,Yangbin Chen*

Main category: cs.LG

TL;DR: 提出一种基于因果推断的反事实去偏框架，用于解决音频抑郁症检测中的性别偏差问题，通过消除性别对预测的直接影响来提升模型公平性与整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频抑郁症检测模型因训练数据性别不平衡而产生性别偏见，导致对女性患者过度诊断、男性患者检测效果差，影响模型公平性。

Method: 构建因果图分析决策过程，识别性别对预测的直接因果效应，并在推理阶段使用反事实推理估计并消除该效应，使模型依赖真实的声学病理特征进行判断。

Result: 在DAIC-WOZ数据集上使用两种先进声学骨干网络的实验表明，该框架显著降低了性别偏见，并优于现有的去偏策略，同时提升了整体检测性能。

Conclusion: 所提出的反事实去偏框架能有效缓解性别偏见，增强模型的公平性和泛化能力，为基于音频的心理健康诊断提供了更可靠的解决方案。

Abstract: Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing Framework grounded in causal inference. We construct a causal graph to model the decision-making process and identify gender bias as the direct causal effect of gender on the prediction. During inference, we employ counterfactual inference to estimate and subtract this direct effect, ensuring the model relies primarily on authentic acoustic pathological features. Extensive experiments on the DAIC-WOZ dataset using two advanced acoustic backbones demonstrate that our framework not only significantly reduces gender bias but also improves overall detection performance compared to existing debiasing strategies.

</details>


### [575] [The Mean-Field Dynamics of Transformers](https://arxiv.org/abs/2512.01868)
*Philippe Rigollet*

Main category: cs.LG

TL;DR: 本文将Transformer注意力机制建模为粒子系统，并研究其连续极限，揭示了token在长时间演化后出现全局聚类现象，同时分析了不同归一化方式和长上下文场景下的动力学特性。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer中注意力机制的内在动力学机制，特别是表征坍缩与多簇结构保持之间的权衡。

Method: 将注意力视为球面上的连续相互作用粒子系统，结合Wasserstein梯度流、Kuramoto同步模型和均值漂移聚类等理论工具进行分析，并引入等角简化模型以获得精确聚类速率。

Result: 发现了token演化的全局聚类现象及中间亚稳态多簇结构；推导出精确聚类速率；揭示了归一化方案对收缩速度的影响及长上下文注意力中的相变现象。

Conclusion: 该框架不仅解释了表征坍缩的机制，也刻画了维持表达性多簇结构的关键条件，为设计更稳健的深度注意力架构提供了理论依据。

Abstract: We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits. By idealizing attention continuous on the sphere, we connect Transformer dynamics to Wasserstein gradient flows, synchronization models (Kuramoto), and mean-shift clustering. Central to our results is a global clustering phenomenon whereby tokens cluster asymptotically after long metastable states where they are arranged into multiple clusters. We further analyze a tractable equiangular reduction to obtain exact clustering rates, show how commonly used normalization schemes alter contraction speeds, and identify a phase transition for long-context attention. The results highlight both the mechanisms that drive representation collapse and the regimes that preserve expressive, multi-cluster structure in deep attention architectures.

</details>


### [576] [Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion](https://arxiv.org/abs/2512.01881)
*Ahmed Nebli*

Main category: cs.LG

TL;DR: 本文提出ThermoLion优化器，通过本地信噪比（SNR）门控动态切换低比特探索与高精度利用阶段，并引入动量对齐机制加速稳定轨迹下的收敛，在12个视觉数据集上超越AdamW和Lion，且无需超参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有优化器（如AdamW和Lion）在高维随机噪声下采用静态信息通道策略，难以兼顾噪声鲁棒性与精细下降信号保留，尤其在非凸、崎岖损失曲面上表现不佳。

Method: 提出ThermoLion框架：1）基于局部信噪比（SNR）的自适应比特率更新门控机制，实现低比特探索与高精度利用的动态切换；2）动量对齐机制，检测历史动量与当前梯度的建设性干涉以加速收敛。

Result: 在CIFAR、SVHN、GTSRB等12个视觉数据集上，ThermoLion在收敛速度和最终准确率上均优于AdamW和Lion，且作为超参数无关的通用优化器，无需针对架构调优。

Conclusion: 动态调节更新比特率比静态先验更适配深度视觉模型训练，ThermoLion验证了SNR感知自适应优化在真实视觉任务中的有效性与泛化性。

Abstract: The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversely, sign-based methods (e.g., Lion) perform a radical 1-bit quantization of the gradient, which aims to provide robust regularization at the cost of discarding fine-grained descent information. We propose that optimal convergence requires neither static prior, but rather a dynamic modulation of the update bitrate. We introduce \textbf{ThermoLion}, a vision-centric framework that utilizes local Signal-to-Noise Ratio (SNR) gating to autonomously transition parameters between a "low-bit" exploration phase and a "high-precision" exploitation phase. Furthermore, we introduce a Momentum Alignment mechanism that detects constructive interference between historical drift and instantaneous gradients to accelerate convergence during stable trajectories. Empirical benchmarks across 12 diverse vision datasets (including CIFAR, SVHN, and GTSRB) demonstrate that ThermoLion serves as a hyperparameter-free generalist, surpassing both AdamW and Lion in convergence speed and terminal accuracy without architecture-specific tuning.

</details>


### [577] [New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles](https://arxiv.org/abs/2512.01882)
*Aref Ghoreishee,Abhishek Mishra,Lifeng Zhou,John Walsh,Nagarajan Kandasamy*

Main category: cs.LG

TL;DR: 本文提出了一种端到端多模态强化学习框架，用于自动驾驶车辆的高层决策，通过基于脉冲神经元的时序感知Transformer架构实现高效多模态融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的多模态架构计算开销大，难以部署于资源受限的边缘环境。

Method: 设计了一个基于交叉注意力机制的感知模块，并提出一种使用三值脉冲神经元的脉冲时序感知类Transformer架构，以降低计算成本并实现多模态融合。

Result: 在Highway Environment中多项任务的综合评估表明，该方法在实时自主决策中兼具有效性与高效性。

Conclusion: 所提出的脉冲神经元驱动的多模态强化学习框架，可在保证性能的同时显著降低计算开销，适用于边缘端自动驾驶系统。

Abstract: This work proposes an end-to-end multi-modal reinforcement learning framework for high-level decision-making in autonomous vehicles. The framework integrates heterogeneous sensory input, including camera images, LiDAR point clouds, and vehicle heading information, through a cross-attention transformer-based perception module. Although transformers have become the backbone of modern multi-modal architectures, their high computational cost limits their deployment in resource-constrained edge environments. To overcome this challenge, we propose a spiking temporal-aware transformer-like architecture that uses ternary spiking neurons for computationally efficient multi-modal fusion. Comprehensive evaluations across multiple tasks in the Highway Environment demonstrate the effectiveness and efficiency of the proposed approach for real-time autonomous decision-making.

</details>


### [578] [Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets](https://arxiv.org/abs/2512.01888)
*Adrienne M. Propp,Mauro Perego,Eric C. Cyr,Anthony Gruber,Amanda A. Howard,Alexander Heinlein,Panos Stinis,Daniel M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络（GNN）的物理启发式代理模型，结合域分解和迁移学习，在非结构化网格上高效准确地模拟偏微分方程，显著提升训练效率与泛化能力，适用于大规模不确定性量化任务。


<details>
  <summary>Details</summary>
Motivation: 传统代理模型在处理大规模、高分辨率PDE模拟时效率低、难以泛化，尤其在需要大量评估的不确定性量化任务中面临挑战。

Method: 提出一种基于图注意力机制的GNN代理模型，直接在非结构化网格上操作；采用域分解策略将网格划分为子域，并行训练局部GNN模型后聚合预测结果；结合迁移学习在子域间微调模型以加速训练并提升小数据下的准确性。

Result: 在冰盖模拟中，该方法能准确预测高分辨率网格上的全场速度，显著缩短训练时间，优于全局模型，且具备良好的可扩展性和UQ适用性。

Conclusion: 图神经网络结合域分解与迁移学习为大规模PDE系统提供了可扩展、高效的代理建模路径，具有广泛的应用前景。

Abstract: Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics.

</details>


### [579] [Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical Evaluation](https://arxiv.org/abs/2512.01890)
*Gaganpreet Jhajj,Fuhua Lin*

Main category: cs.LG

TL;DR: Elastic Weight Consolidation (EWC) 显著减少了知识图谱嵌入模型在连续学习中的灾难性遗忘，且任务划分策略对遗忘程度有显著影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络嵌入模型在顺序学习新知识时容易出现灾难性遗忘，影响知识图谱的持续更新，因此需要有效的持续学习方法来缓解这一问题。

Method: 采用基于正则化的持续学习方法Elastic Weight Consolidation (EWC)，结合TransE嵌入模型，在FB15k-237数据集上进行链路预测实验，并比较不同任务划分策略（按关系划分与随机划分）下的遗忘程度。

Result: EWC将灾难性遗忘从12.62%降低至6.85%，减少了45.7%；任务划分方式显著影响遗忘程度，按关系划分比随机划分多出9.8个百分点的遗忘。

Conclusion: EWC能有效缓解知识图谱嵌入模型在持续学习中的灾难性遗忘问题，同时任务构造方式对评估结果有重要影响，需在评估协议设计中加以考虑。

Abstract: Knowledge graphs (KGs) require continual updates as new information emerges, but neural embedding models suffer from catastrophic forgetting when learning new tasks sequentially. We evaluate Elastic Weight Consolidation (EWC), a regularization-based continual learning method, on KG link prediction using TransE embeddings on FB15k-237. Across multiple experiments with five random seeds, we find that EWC reduces catastrophic forgetting from 12.62% to 6.85%, a 45.7% reduction compared to naive sequential training. We observe that the task partitioning strategy affects the magnitude of forgetting: relation-based partitioning (grouping triples by relation type) exhibits 9.8 percentage points higher forgetting than randomly partitioned tasks (12.62% vs 2.81%), suggesting that task construction influences evaluation outcomes. While focused on a single embedding model and dataset, our results demonstrate that EWC effectively mitigates catastrophic forgetting in KG continual learning and highlight the importance of evaluation protocol design.

</details>


### [580] [Provably Safe Model Updates](https://arxiv.org/abs/2512.01899)
*Leo Elmecker-Plakolm,Pierre Fasterling,Philip Sosnin,Calvin Tsay,Matthew Wicker*

Main category: cs.LG

TL;DR: 提出了一种可验证安全的模型更新框架，通过计算参数空间中满足性能规范的最大局部不变域（LID），在连续学习和大模型微调中实现具备形式化安全保障的更新。


<details>
  <summary>Details</summary>
Motivation: 现有模型更新方法无法保证更新后的模型仍满足性能规范，尤其在安全关键场景下存在灾难性遗忘或对齐漂移风险，缺乏形式化安全保障。

Method: 将安全更新问题转化为计算最大局部不变域（LID），在正交体、zonotope等抽象参数域中进行松弛，构建可高效求解的 primal-dual 优化框架，支持投影更新、多LID计算、正则偏置引入和前瞻数据缓冲。

Result: 在持续学习和大模型微调任务上，该方法在防止遗忘方面达到或优于启发式基线，并提供了形式化的安全保证。

Conclusion: 所提框架实现了对模型更新的形式化安全认证，兼顾效率与安全性，适用于动态环境中需持续更新的安全关键系统。

Abstract: Safety-critical environments are inherently dynamic. Distribution shifts, emerging vulnerabilities, and evolving requirements demand continuous updates to machine learning models. Yet even benign parameter updates can have unintended consequences, such as catastrophic forgetting in classical models or alignment drift in foundation models. Existing heuristic approaches (e.g., regularization, parameter isolation) can mitigate these effects but cannot certify that updated models continue to satisfy required performance specifications. We address this problem by introducing a framework for provably safe model updates. Our approach first formalizes the problem as computing the largest locally invariant domain (LID): a connected region in parameter space where all points are certified to satisfy a given specification. While exact maximal LID computation is intractable, we show that relaxing the problem to parameterized abstract domains (orthotopes, zonotopes) yields a tractable primal-dual formulation. This enables efficient certification of updates - independent of the data or algorithm used - by projecting them onto the safe domain. Our formulation further allows computation of multiple approximately optimal LIDs, incorporation of regularization-inspired biases, and use of lookahead data buffers. Across continual learning and foundation model fine-tuning benchmarks, our method matches or exceeds heuristic baselines for avoiding forgetting while providing formal safety guarantees.

</details>


### [581] [Delays in Spiking Neural Networks: A State Space Model Approach](https://arxiv.org/abs/2512.01906)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 本文提出了一种通用框架，通过引入额外状态变量将延迟机制融入脉冲神经网络（SNNs），使神经元能访问有限时间窗口内的输入历史；该框架与LIF、adLIF等主流神经元模型兼容，并在SHD数据集上验证了其在保持计算效率的同时提升小规模网络性能的有效性。


<details>
  <summary>Details</summary>
Motivation: SNNs中延迟对建模时间依赖性至关重要，但现有方法缺乏通用、可扩展且计算高效的延迟建模机制。

Method: 提出基于额外状态变量的通用延迟建模框架，使每个神经元可访问有限长度的输入历史；该框架独立于具体神经元模型，支持端到端学习延迟相关参数，并分析延迟时长与参数对性能的影响及引入状态变量带来的架构权衡。

Result: 在Spiking Heidelberg Digits (SHD)数据集上，所提方法达到与现有延迟型SNN相当的性能，同时计算更高效；尤其在较小规模网络中，延迟引入显著提升性能。

Conclusion: 延迟是提升SNN时序建模能力的关键因素，所提出的通用、模型无关的延迟机制在保持计算效率的同时增强了小网络的表达能力，为构建更高效、更生物合理的SNN提供了新思路。

Abstract: Spiking neural networks (SNNs) are biologically inspired, event-driven models that are suitable for processing temporal data and offer energy-efficient computation when implemented on neuromorphic hardware. In SNNs, richer neuronal dynamic allows capturing more complex temporal dependencies, with delays playing a crucial role by allowing past inputs to directly influence present spiking behavior. We propose a general framework for incorporating delays into SNNs through additional state variables. The proposed mechanism enables each neuron to access a finite temporal input history. The framework is agnostic to neuron models and hence can be seamlessly integrated into standard spiking neuron models such as LIF and adLIF. We analyze how the duration of the delays and the learnable parameters associated with them affect the performance. We investigate the trade-offs in the network architecture due to additional state variables introduced by the delay mechanism. Experiments on the Spiking Heidelberg Digits (SHD) dataset show that the proposed mechanism matches the performance of existing delay-based SNNs while remaining computationally efficient. Moreover, the results illustrate that the incorporation of delays may substantially improve performance in smaller networks.

</details>


### [582] [A Footprint-Aware, High-Resolution Approach for Carbon Flux Prediction Across Diverse Ecosystems](https://arxiv.org/abs/2512.01917)
*Jacob Searcy,Anish Dulal,Scott Bridgham,Ashley Cordes,Lillian Aoki,Brendan Bohannan,Qing Zhu,Lucas C. R. Silva*

Main category: cs.LG

TL;DR: 本文提出了一种名为Footprint-Aware Regression（FAR）的深度学习框架，用于在30米分辨率下同时预测通量塔的空间足迹和像元级碳通量，结合AMERI-FAR25数据集（439站点年塔数据+对应Landsat影像）训练，在多类生态系统测试中月尺度净生态系统交换预测R²达0.78。


<details>
  <summary>Details</summary>
Motivation: 自然气候解决方案（NCS）虽具潜力，但大范围监测生态系统碳吸收仍困难；现有卫星产品空间分辨率常小于通量塔足迹，导致传统上尺度上推不准。

Method: 提出FAR——一种端到端深度学习框架，联合建模通量塔空间足迹与30米像素级碳通量；基于AMERI-FAR25数据集（整合439 site-years通量塔观测与匹配Landsat影像）进行训练。

Result: FAR在多类型生态系统测试站点上实现月尺度净生态系统交换（NEE）预测R² = 0.78，生成高分辨率（30 m）碳通量图，并可同步输出塔足迹估计。

Conclusion: FAR为融合地面通量观测与高分辨率遥感提供了新范式，显著提升NCS碳汇监测的空间精度与可扩展性，推动基于自然的气候行动量化评估。

Abstract: Natural climate solutions (NCS) offer an approach to mitigating carbon dioxide (CO2) emissions. However, monitoring the carbon drawdown of ecosystems over large geographic areas remains challenging. Eddy-flux covariance towers provide ground truth for predictive 'upscaling' models derived from satellite products, but many satellites now produce measurements on spatial scales smaller than a flux tower's footprint. We introduce Footprint-Aware Regression (FAR), a first-of-its-kind, deep-learning framework that simultaneously predicts spatial footprints and pixel-level (30 m scale) estimates of carbon flux. FAR is trained on our AMERI-FAR25 dataset which combines 439 site years of tower data with corresponding Landsat scenes. Our model produces high-resolution predictions and achieves R2 = 0.78 when predicting monthly net ecosystem exchange on test sites from a variety of ecosystems.

</details>


### [583] [SVRG and Beyond via Posterior Correction](https://arxiv.org/abs/2512.01930)
*Nico Daheim,Thomas Möllenhoff,Ming Liang Ang,Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 本文揭示了SVRG与贝叶斯后验校正之间的新联系，并基于高斯分布族提出了两种新的SVRG变体，包括类似牛顿法和类似Adam的扩展，首次将SVRG与贝叶斯方法结合以提升深度网络的变分训练。


<details>
  <summary>Details</summary>
Motivation: SVRG及其变体在深度学习中的应用有限，本文旨在通过建立其与贝叶斯后验校正的理论联系，提出更有效的优化方法。

Method: 利用各向同性高斯分布族推导出SVRG作为后验校正的特例，并通过更灵活的指数族分布（特别是高斯族）提出新的SVRG变体，包括引入Hessian校正的牛顿型方法和适用于Transformer模型预训练与微调的Adam型扩展。

Result: 成功推导出两种新的SVRG变体：一种具有Hessian校正的牛顿型方法，另一种是改进Transformer语言模型训练的Adam型扩展；验证了SVRG与贝叶斯方法之间的理论联系。

Conclusion: SVRG可被视为后验校正的一个特例，通过贝叶斯视角可自然导出更强大的SVRG变体，为深度学习中的优化提供了新方向。

Abstract: Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.

</details>


### [584] [KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference](https://arxiv.org/abs/2512.01953)
*Sai Gokhale,Devleena Das,Rajeev Patwari,Ashish Sirasao,Elliott Delaye*

Main category: cs.LG

TL;DR: 本文提出了KV Pareto框架，系统性地探索了KV缓存量化、分块预填充和模型权重量化的联合优化策略，在边缘设备上实现了68-78%的内存减少，仅带来1-3%的精度下降。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型在推理时面临KV缓存随序列长度线性增长带来的显著内存瓶颈，而现有优化技术的联合效应与最优配置在边缘部署场景下尚不明确。

Method: 提出KV Pareto系统级框架，综合评估KV缓存量化（int2/4/8、混合精度、不同粒度）、分块预填充和4位AWQ权重量化在多种LLM（Qwen、Llama、Mistral）上的协同效果，并构建内存-精度权衡前沿。

Result: 识别出各模型特定的Pareto最优配置，实现68–78%总内存降低且精度仅下降1–3%；在Needle-in-a-Haystack、GSM8k、MMLU及最长128k上下文任务上验证了其有效性。

Conclusion: 联合优化KV缓存、预填充和权重量化对高效边缘端长上下文LLM推理至关重要，KV Pareto为实际部署提供了可迁移的系统级指导。

Abstract: Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.

</details>


### [585] [Low-Rank Prehab: Preparing Neural Networks for SVD Compression](https://arxiv.org/abs/2512.01980)
*Haoran Qin,Shansita Sharma,Ali Abbasi,Chayne Thrash,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文提出Low-Rank Prehab方法，在SVD压缩前对模型进行预微调，显式鼓励权重矩阵的低秩结构，从而提升压缩后精度恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法（如SVD）将分解视为‘手术’、后续微调为‘康复’，但忽略了术前准备的重要性；作者受医学‘术前康复（prehabilitation）’启发，提出在压缩前主动引导权重趋向谱紧凑区域，以改善低秩近似质量。

Method: 在应用SVD等低秩分解前，引入一个预压缩微调阶段（Prehab），通过特定正则化或优化目标，使权重矩阵在保持任务性能的同时自然呈现低秩特性，从而为后续SVD提供更适宜的初始状态。

Result: 在LLMs和ViTs等多种Transformer架构上验证，Prehab显著减小压缩后的初始精度下降，并持续提升微调后性能；在不同压缩比下均优于SVD-LLM等前沿SVD方法。

Conclusion: 模型压缩效果不仅取决于分解与微调阶段，更关键在于压缩前的参数空间准备；Prehab证明‘术前准备’是提升低秩压缩整体效能的有效新范式。

Abstract: Low-rank approximation methods such as singular value decomposition (SVD) and its variants (e.g., Fisher-weighted SVD, Activation SVD) have recently emerged as effective tools for neural network compression. In this setting, decomposition acts as a "surgical" intervention, followed by fine-tuning that serves as "rehab" to recover accuracy. Inspired by prehabilitation in surgery, we introduce a pre-compression fine-tuning stage, Low-Rank Prehab, that explicitly encourages low-rank structure in weight matrices while preserving task performance. By conditioning the model before SVD, Prehab steers weights toward spectrally compact regions of the parameter space, enabling smoother low-rank approximation and improved recovery. Experiments on large language models (LLMs) and other Transformer-based architectures, including Vision Transformers (ViTs), show that Prehab substantially reduces the immediate accuracy drop after compression and consistently improves post-finetuning performance. Across a wide range of compression ratios, our method outperforms state-of-the-art SVD-based techniques such as SVD-LLM, highlighting the importance of preparing models for compression rather than only improving the compression and recovery stages. Source code is available at https://github.com/niqretnuh/PREHAB-SVD

</details>


### [586] [Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning](https://arxiv.org/abs/2512.01983)
*Eunjeong Jeong,Giovanni Perin,Howard H. Yang,Nikolaos Pappas*

Main category: cs.LG

TL;DR: 本文提出了一种基于版本年龄（VAoI）的轻量级客户端调度框架，用于能量采集型联邦学习（EHFL），通过特征代理降低计算开销，在非独立同分布数据和能量受限条件下显著提升能效与模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有能量采集型联邦学习策略未考虑边缘设备上本地训练能耗常远超通信能耗的问题，导致冗余计算与能量浪费，亟需能预测本地更新贡献的资源管理算法。

Method: 提出基于语义感知的版本年龄（VAoI）指标，并设计一种基于中间层特征提取的轻量代理，仅需一次前向传播即可估计模型冗余，避免原VAoI在全参数空间计算统计距离的高开销。

Result: 在极端非IID数据和稀缺能量条件下，实验表明该框架相比基线策略显著提升学习性能并降低能耗。

Conclusion: 语义感知的调度机制是现实EHFL场景（训练成本主导通信成本）中切实可行且至关重要的解决方案。

Abstract: Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweight client scheduling framework using the Version Age of Information (VAoI), a semantics-aware metric that quantifies update timeliness and significance. Crucially, we overcome VAoI's typical prohibitive computational cost, which requires statistical distance over the entire parameter space, by introducing a feature-based proxy. This proxy estimates model redundancy using intermediate-layer extraction from a single forward pass, dramatically reducing computational complexity. Experiments conducted under extreme non-IID data distributions and scarce energy availability demonstrate superior learning performance while achieving energy reduction compared to existing baseline selection policies. Our framework establishes semantics-aware scheduling as a practical and vital solution for EHFL in realistic scenarios where training costs dominate transmission costs.

</details>


### [587] [A Diffusion Model Framework for Maximum Entropy Reinforcement Learning](https://arxiv.org/abs/2512.02019)
*Sebastian Sanokowski,Kaustubh Patil,Alois Knoll*

Main category: cs.LG

TL;DR: 本文将最大熵强化学习（MaxEntRL）重新解释为基于扩散模型的采样问题，并通过最小化扩散策略与最优策略分布之间的逆KL散度，推导出融合扩散动力学的改进目标函数，进而提出DiffSAC、DiffPPO和DiffWPO等简单而有效的变体，在连续控制任务中展现出更优的性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型的成功应用于强化学习，特别是最大熵RL框架，以提升策略优化的表达能力和采样效率。

Method: 将MaxEntRL建模为扩散采样问题，利用可计算的逆KL散度上界进行优化，并结合策略梯度定理推导出含扩散动态的代理目标函数，从而构造DiffSAC、DiffPPO和DiffWPO。

Result: 在标准连续控制基准任务上，DiffSAC、DiffPPO和DiffWPO相比原算法（SAC、PPO）取得了更高的累积回报和更好的样本效率。

Conclusion: 扩散模型可自然融入最大熵RL框架，仅需少量代码修改即可显著提升现有主流算法性能，验证了该跨范式融合的有效性与实用性。

Abstract: Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [588] [TagSplat: Topology-Aware Gaussian Splatting for Dynamic Mesh Modeling and Tracking](https://arxiv.org/abs/2512.01329)
*Hanzhi Guo,Dongdong Weng,Mo Su,Yixiao Chen,Xiaonuo Dongye,Chenyu Xu*

Main category: cs.GR

TL;DR: 提出了一种基于高斯点阵的拓扑感知动态重建框架，能够生成高质量、拓扑一致的4D网格序列，并提升动态场景重建的精度和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有4D重建方法在生成高质量且拓扑一致的网格方面存在困难，难以满足动画和模型编辑等应用需求。

Method: 引入高斯拓扑结构来显式编码空间连通性，实现拓扑感知的致密化与剪枝；结合时间正则化项保持时间上的拓扑一致性，并利用可微分网格光栅化提升网格质量。

Result: 实验结果表明，该方法在重建拓扑一致的网格序列上显著优于现有方法，且生成的网格支持精确的3D关键点跟踪。

Conclusion: 所提出的框架有效解决了动态场景中拓扑不一致的问题，实现了高质量、连贯的4D重建，具有良好的应用前景。

Abstract: Topology-consistent dynamic model sequences are essential for applications such as animation and model editing. However, existing 4D reconstruction methods face challenges in generating high-quality topology-consistent meshes. To address this, we propose a topology-aware dynamic reconstruction framework based on Gaussian Splatting. We introduce a Gaussian topological structure that explicitly encodes spatial connectivity. This structure enables topology-aware densification and pruning, preserving the manifold consistency of the Gaussian representation. Temporal regularization terms further ensure topological coherence over time, while differentiable mesh rasterization improves mesh quality. Experimental results demonstrate that our method reconstructs topology-consistent mesh sequences with significantly higher accuracy than existing approaches. Moreover, the resulting meshes enable precise 3D keypoint tracking. Project page: https://haza628.github.io/tagSplat/

</details>


### [589] [Textured Word-As-Image illustration](https://arxiv.org/abs/2512.01648)
*Mohammad Javadian Farzaneh,Selim Balcisoy*

Main category: cs.GR

TL;DR: 提出了一种全自动管道，利用稳定扩散模型将语义概念、单词和字母结合，生成既保持可读性又富含语义纹理的文本图像，并支持实时调整。


<details>
  <summary>Details</summary>
Motivation: 为了在文本图像生成中同时实现语义表达与文字可读性的平衡，满足图形设计和艺术排版的需求。

Method: 用户输入语义概念、单词和字母；利用稳定扩散模型根据语义生成纹理并改变字母形状；将纹理映射到文本图像上，同时保持可读性和视觉一致性；支持实时调整纹理比例。

Result: 生成的文本图像在保持高可读性的同时，有效传达了输入的语义概念，用户评估表明该方法在语义对齐和视觉质量方面表现优异。

Conclusion: 该方法是一种强大且创新的工具，适用于图形设计、标志创作和艺术字体设计，实现了语义驱动的文本图像生成与可读性的良好平衡。

Abstract: In this paper, we propose a novel fully automatic pipeline to generate text images that are legible and strongly aligned to the desired semantic concept taken from the users' inputs. In our method, users are able to put three inputs into the system, including a semantic concept, a word, and a letter. The semantic concept will be used to change the shape of the input letter and generate the texture based on the pre-defined prompt using stable diffusion models. Our pipeline maps the texture on a text image in a way that preserves the readability of the whole output while preserving legibility. The system also provides real-time adjustments for the user to change the scale of the texture and apply it to the text image. User evaluations demonstrate that our method effectively represents semantic meaning without compromising legibility, making it a robust and innovative tool for graphic design, logo creation, and artistic typography.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [590] [DREAMer-VXS: A Latent World Model for Sample-Efficient AGV Exploration in Stochastic, Unobserved Environments](https://arxiv.org/abs/2512.00005)
*Agniprabha Chakraborty*

Main category: cs.RO

TL;DR: 本文提出DREAMer-VXS，一种基于模型的自主地面车辆（AGV）探索框架，通过从LiDAR数据学习世界模型并在潜空间中进行想象规划，显著提升样本效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习在真实机器人应用中存在样本效率低、策略脆弱的问题，亟需更高效、鲁棒的模型-based方法。

Method: 构建包含卷积变分自编码器（VAE）和循环状态空间模型（RSSM）的世界模型，从稀疏高维LiDAR观测中学习环境结构与动态；利用该模型作为高速仿真器，在潜空间中进行想象轨迹规划，并采用融合任务目标与内在好奇心奖励的actor-critic策略进行优化。

Result: 相比SAC等先进无模型方法，实现90%的环境交互减少；在未见环境中探索效率提升45%，且对动态障碍物具有更强鲁棒性。

Conclusion: DREAMer-VXS验证了基于潜空间想象规划的模型-based方法在AGV探索任务中的有效性，为学习型机器人迈向现实应用提供了高效、可泛化的新范式。

Abstract: The paradigm of learning-based robotics holds immense promise, yet its translation to real-world applications is critically hindered by the sample inefficiency and brittleness of conventional model-free reinforcement learning algorithms. In this work, we address these challenges by introducing DREAMer-VXS, a model-based framework for Autonomous Ground Vehicle (AGV) exploration that learns to plan from imagined latent trajectories. Our approach centers on learning a comprehensive world model from partial and high-dimensional LiDAR observations. This world model is composed of a Convolutional Variational Autoencoder (VAE), which learns a compact representation of the environment's structure, and a Recurrent State-Space Model (RSSM), which models complex temporal dynamics. By leveraging this learned model as a high-speed simulator, the agent can train its navigation policy almost entirely in imagination. This methodology decouples policy learning from real-world interaction, culminating in a 90% reduction in required environmental interactions to achieve expert-level performance when compared to state-of-the-art model-free SAC baselines. The agent's behavior is guided by an actor-critic policy optimized with a composite reward function that balances task objectives with an intrinsic curiosity bonus, promoting systematic exploration of unknown spaces. We demonstrate through extensive simulated experiments that DREAMer-VXS not only learns orders of magnitude faster but also develops more generalizable and robust policies, achieving a 45% increase in exploration efficiency in unseen environments and superior resilience to dynamic obstacles.

</details>


### [591] [A Comprehensive Survey on Surgical Digital Twin](https://arxiv.org/abs/2512.00019)
*Afsah Sharaf Khan,Falong Fan,Doohwan DH Kim,Abdurrahman Alshareef,Dong Chen,Justin Kim,Ernest Carter,Bo Liu,Jerzy W. Rozenblit,Bernard Zeigler*

Main category: cs.RO

TL;DR: 本文综述了外科数字孪生（SDTs）的研究现状与挑战，提出分类体系，总结关键技术进展，并指出验证、安全、人因、数据治理等开放问题，旨在推动可信、标准化且具临床价值的SDTs落地。


<details>
  <summary>Details</summary>
Motivation: 尽管SDTs在手术各阶段展现出潜力，但其在多模态数据融合、实时性、模型保真度与效率平衡、鲁棒性、可解释性、不确定性校准及临床合规性等方面仍面临严峻挑战，亟需系统性梳理与指引。

Method: 采用结构化文献综述方法，明确术语与范围，构建基于目的、模型保真度和数据源的分类法，综合分析形变配准与跟踪、实时仿真、AR/VR引导、边云协同、AI场景理解与预测等前沿技术，并对比非机器人与机器人闭环架构。

Result: 提出了SDT的统一分类体系；系统总结了多项关键技术的最新进展；识别出验证基准、安全保证、人因工程、“数字线程”集成与数据治理等核心开放问题；形成面向临床转化的研究路线图。

Conclusion: SDTs的发展需兼顾技术先进性与临床实用性，未来应聚焦于构建可信、符合标准、可验证且能带来可衡量临床效益的系统，加速从实验室原型走向常规手术实践。

Abstract: With the accelerating availability of multimodal surgical data and real-time computation, Surgical Digital Twins (SDTs) have emerged as virtual counterparts that mirror, predict, and inform decisions across pre-, intra-, and postoperative care. Despite promising demonstrations, SDTs face persistent challenges: fusing heterogeneous imaging, kinematics, and physiology under strict latency budgets; balancing model fidelity with computational efficiency; ensuring robustness, interpretability, and calibrated uncertainty; and achieving interoperability, privacy, and regulatory compliance in clinical environments. This survey offers a critical, structured review of SDTs. We clarify terminology and scope, propose a taxonomy by purpose, model fidelity, and data sources, and synthesize state-of-the-art achievements in deformable registration and tracking, real-time simulation and co-simulation, AR/VR guidance, edge-cloud orchestration, and AI for scene understanding and prediction. We contrast non-robotic twins with robot-in-the-loop architectures for shared control and autonomy, and identify open problems in validation and benchmarking, safety assurance and human factors, lifecycle "digital thread" integration, and scalable data governance. We conclude with a research agenda toward trustworthy, standards-aligned SDTs that deliver measurable clinical benefit. By unifying vocabulary, organizing capabilities, and highlighting gaps, this work aims to guide SDT design and deployment and catalyze translation from laboratory prototypes to routine surgical care.

</details>


### [592] [Foundation Models for Trajectory Planning in Autonomous Driving: A Review of Progress and Open Challenges](https://arxiv.org/abs/2512.00021)
*Kemal Oksuz,Alexandru Buburuzan,Anthony Knittel,Yuhan Yao,Puneet K. Dokania*

Main category: cs.RO

TL;DR: 本文综述了基于多模态基础模型（尤其是视觉-语言-动作VLA模型）的自动驾驶轨迹规划方法，提出统一分类体系，分析37种最新方法的架构设计、优缺点，并评估其代码与数据集开源情况。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型兴起推动自动驾驶从手工设计转向统一基础模型范式，尤其引入自然语言模态带来新机遇与挑战，亟需系统性梳理与评估。

Method: 构建统一分类法对37种基于基础模型的轨迹规划方法进行系统性综述，涵盖架构设计、方法优势、能力与局限，并评估其代码与数据集开源程度。

Result: 形成覆盖主流方法的结构化 taxonomy；识别出关键设计趋势与共性瓶颈；提供开源状态评估；配套网页资源便于社区使用。

Conclusion: 基于基础模型的轨迹规划展现出潜力，但当前方法在泛化性、可解释性、计算效率及真实部署适配性方面仍存明显局限，未来需加强跨模态对齐、闭环评估与开源共建。

Abstract: The emergence of multi-modal foundation models has markedly transformed the technology for autonomous driving, shifting away from conventional and mostly hand-crafted design choices towards unified, foundation-model-based approaches, capable of directly inferring motion trajectories from raw sensory inputs. This new class of methods can also incorporate natural language as an additional modality, with Vision-Language-Action (VLA) models serving as a representative example. In this review, we provide a comprehensive examination of such methods through a unifying taxonomy to critically evaluate their architectural design choices, methodological strengths, and their inherent capabilities and limitations. Our survey covers 37 recently proposed approaches that span the landscape of trajectory planning with foundation models. Furthermore, we assess these approaches with respect to the openness of their source code and datasets, offering valuable information to practitioners and researchers. We provide an accompanying webpage that catalogs the methods based on our taxonomy, available at: https://github.com/fiveai/FMs-for-driving-trajectories

</details>


### [593] [XFlowMP: Task-Conditioned Motion Fields for Generative Robot Planning with Schrodinger Bridges](https://arxiv.org/abs/2512.00022)
*Khang Nguyen,Minh Nhat Vu*

Main category: cs.RO

TL;DR: 本文提出XFlowMP，一种任务条件生成式运动规划器，利用薛定谔桥建模轨迹演化，实现语义与动力学约束的统一，显著提升轨迹平滑性、可行性与能效。


<details>
  <summary>Details</summary>
Motivation: 现有运动规划方法难以兼顾高阶语义任务配置与低层动态/碰撞约束，尤其缺乏对任务构型与运动可控性关联的有效建模。

Method: 提出XFlowMP，将轨迹生成建模为从噪声到专家示范的熵流过程，采用薛定谔桥作为条件流匹配机制，并结合分数函数学习含高阶动力学的运动场，同时编码起止位形。

Result: 在RobotPointMass基准上，相比最优基线，最大均值差异降低53.79%，平滑性提升36.36%，能耗降低39.88%，短时域规划时间减少11.72%；在LASA手写数据集上，MMD降低1.26%，平滑性提升3.96%，能耗降低31.97%；并在Kinova Gen3机械臂上验证了真实场景鲁棒性。

Conclusion: XFlowMP通过薛定谔桥驱动的条件流匹配，有效融合任务语义与物理可行性，在仿真与真实机器人平台上均展现出优越性能与泛化能力。

Abstract: Generative robotic motion planning requires not only the synthesis of smooth and collision-free trajectories but also feasibility across diverse tasks and dynamic constraints. Prior planning methods, both traditional and generative, often struggle to incorporate high-level semantics with low-level constraints, especially the nexus between task configurations and motion controllability. In this work, we present XFlowMP, a task-conditioned generative motion planner that models robot trajectory evolution as entropic flows bridging stochastic noises and expert demonstrations via Schrodinger bridges given the inquiry task configuration. Specifically, our method leverages Schrodinger bridges as a conditional flow matching coupled with a score function to learn motion fields with high-order dynamics while encoding start-goal configurations, enabling the generation of collision-free and dynamically-feasible motions. Through evaluations, XFlowMP achieves up to 53.79% lower maximum mean discrepancy, 36.36% smoother motions, and 39.88% lower energy consumption while comparing to the next-best baseline on the RobotPointMass benchmark, and also reducing short-horizon planning time by 11.72%. On long-horizon motions in the LASA Handwriting dataset, our method maintains the trajectories with 1.26% lower maximum mean discrepancy, 3.96% smoother, and 31.97% lower energy. We further demonstrate the practicality of our method on the Kinova Gen3 manipulator, executing planning motions and confirming its robustness in real-world settings.

</details>


### [594] [Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos](https://arxiv.org/abs/2512.00024)
*X. Hu,G. Ye*

Main category: cs.RO

TL;DR: 本文提出了一种结合大基础模型与点跟踪技术的新方法，用于从互联网上的人类操作视频中提取密集的关键点轨迹，从而更充分地利用这些视频进行机器人学习。


<details>
  <summary>Details</summary>
Motivation: 收集高质量机器人训练数据依赖真实机器人平台，成本高、耗时长；而现有基于网络人类操作视频的方法未能充分利用其中丰富的交互线索。

Method: 结合大型视频理解基础模型与点跟踪技术，提取操作过程中所有任务相关关键点的稠密轨迹。

Result: 实验表明该方法能准确跟踪整个操作过程中的关键点，提升了机器人学习的数据可扩展性与效率。

Conclusion: 该方法为大规模、数据高效的机器人学习提供了新途径，显著拓展了互联网人类演示视频的利用价值。

Abstract: Collecting high-quality data for training large-scale robotic models typically relies on real robot platforms, which is labor-intensive and costly, whether via teleoperation or scripted demonstrations. To scale data collection, many researchers have turned to leveraging human manipulation videos available online. However, current methods predominantly focus on hand detection or object pose estimation, failing to fully exploit the rich interaction cues embedded in these videos. In this work, we propose a novel approach that combines large foundation models for video understanding with point tracking techniques to extract dense trajectories of all task-relevant keypoints during manipulation. This enables more comprehensive utilization of Internet-scale human demonstration videos. Experimental results demonstrate that our method can accurately track keypoints throughout the entire manipulation process, paving the way for more scalable and data-efficient robot learning.

</details>


### [595] [A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation](https://arxiv.org/abs/2512.00027)
*Nivedan Yakolli,Avinash Gautam,Abhijit Das,Yuankai Qi,Virendra Singh Shekhawat*

Main category: cs.RO

TL;DR: 本文综述了视觉-语言导航（VLN）在机器人领域的最新进展，重点分析了多机器人协同中的挑战与未来方向，强调需增强双向通信、歧义消解、主动澄清、实时反馈、上下文推理及去中心化决策。


<details>
  <summary>Details</summary>
Motivation: 当前VLN模型在多智能体系统中面临双向通信不足、歧义难以解析、协作决策能力弱等问题，亟需系统性综述与方向指引。

Method: 系统性文献调研，涵盖约200篇相关论文，从任务特性、技术瓶颈与系统架构角度进行分类分析，并提出面向多机器人协同的改进框架。

Result: 梳理出VLN在机器人应用中的核心挑战；提出支持主动澄清、实时反馈和上下文推理的NLU增强路径；设计基于动态角色分配的去中心化决策框架。

Conclusion: 未来VLN系统应深度融合先进自然语言理解与去中心化协同机制，以提升人机交互质量，推动其在医疗、物流与应急响应等真实场景落地。

Abstract: Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.

</details>


### [596] [Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning](https://arxiv.org/abs/2512.00030)
*Zhaofan Zhang,Minghao Yang,Sihong Xie,Hui Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种名为DRIQN的新方法，结合分布鲁棒优化与隐式分位数网络，以提升无人水面艇在异方差观测噪声下的导航鲁棒性，在成功率、避碰、节能和时间效率方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人水面艇在复杂海洋环境中面临异方差观测噪声，影响传感器导航任务的稳定性，现有分布强化学习方法未能有效应对不同环境下的噪声变化，导致安全性和学习性能下降。

Method: 提出DRIQN方法，将分布鲁棒优化（DRO）与隐式分位数网络结合，并通过经验回放缓冲区中的显式子组建模，整合异质噪声源和关键鲁棒性场景，优化最坏情况下的性能。

Result: 在风险敏感环境中实验表明，DRIQN相比次优方法提升了13.51%的成功率，降低12.28%的碰撞率，并在时间上节省35.46%，能量上节省27.99%。

Conclusion: DRIQN能有效应对复杂海洋环境中的异方差噪声，显著提升无人艇导航的安全性与效率，为无先验信息下的自主导航提供了更具鲁棒性的解决方案。

Abstract: The robustness of Unmanned Surface Vehicles (USV) is crucial when facing unknown and complex marine environments, especially when heteroscedastic observational noise poses significant challenges to sensor-based navigation tasks. Recently, Distributional Reinforcement Learning (DistRL) has shown promising results in some challenging autonomous navigation tasks without prior environmental information. However, these methods overlook situations where noise patterns vary across different environmental conditions, hindering safe navigation and disrupting the learning of value functions. To address the problem, we propose DRIQN to integrate Distributionally Robust Optimization (DRO) with implicit quantile networks to optimize worst-case performance under natural environmental conditions. Leveraging explicit subgroup modeling in the replay buffer, DRIQN incorporates heterogeneous noise sources and target robustness-critical scenarios. Experimental results based on the risk-sensitive environment demonstrate that DRIQN significantly outperforms state-of-the-art methods, achieving +13.51\% success rate, -12.28\% collision rate and +35.46\% for time saving, +27.99\% for energy saving, compared with the runner-up.

</details>


### [597] [Intelligent Systems and Robotics: Revolutionizing Engineering Industries](https://arxiv.org/abs/2512.00033)
*Sathish Krishna Anumula,Sivaramkumar Ponnarangan,Faizal Nujumudeen,Ms. Nilakshi Deka,S. Balamuralitharan,M Venkatesh*

Main category: cs.RO

TL;DR: 本文讨论了人工智能、机器学习和自主机器人技术在多个工程领域的应用，概述了智能机器人系统对工业生产率、安全性和运营成本的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨智能系统与机器人技术如何提升工程行业的效率、精度和适应能力，并解决当前尚未克服的问题。

Method: 基于最新研究成果，提出一种评估工业中智能机器人系统的方法，并结合案例研究进行分析。

Result: 智能机器人不仅带来技术变革，还引入了重要的新工程方法，显著提升了生产力和安全性，降低了运营成本。

Conclusion: 智能机器人技术的应用标志着工程领域的重要转型，未来需进一步解决现有挑战以实现更广泛应用。

Abstract: A mix of intelligent systems and robotics is making engineering industries much more efficient, precise and able to adapt. How artificial intelligence (AI), machine learning (ML) and autonomous robotic technologies are changing manufacturing, civil, electrical and mechanical engineering is discussed in this paper. Based on recent findings and a suggested way to evaluate intelligent robotic systems in industry, we give an overview of how their use impacts productivity, safety and operational costs. Experience and case studies confirm the benefits this area brings and the problems that have yet to be solved. The findings indicate that intelligent robotics involves more than a technology change; it introduces important new methods in engineering.

</details>


### [598] [Design And Control of A Robotic Arm For Industrial Applications](https://arxiv.org/abs/2512.00034)
*Sathish Krishna Anumula,SVSV Prasad Sanaboina,Ravi Kumar Nagula,R. Nagaraju*

Main category: cs.RO

TL;DR: 本文设计并实现了一款六自由度工业机器人臂，采用伺服电机和微控制器，结合运动学与动力学分析，通过逆运动学算法和PID控制提高精度，经仿真与实验验证具有高精度、可重复性，适用于装配、焊接和物料搬运等任务，具备成本低、可扩展和可靠性高的特点。


<details>
  <summary>Details</summary>
Motivation: 为了满足工业环境中对自动化过程日益增长的需求，特别是针对装配、焊接和物料搬运等重复性任务，需要开发低成本且可靠的机器人系统。

Method: 设计了六自由度的机械臂结构，使用伺服电机驱动和微控制器控制；完成机械部件的加工制造；进行正逆运动学与动力学建模；采用逆运动学算法解算目标位置，并结合PID控制器实现精确轨迹控制。

Result: 通过仿真和实验测试验证了该机械臂系统能够实现高精度定位和良好的负载能力，具备良好的重复性和稳定性，能有效完成预定工业任务。

Conclusion: 所提出的机器人臂是一种经济、可扩展且可靠的解决方案，适用于制造业中多种自动化应用，具有较强的实用价值和推广潜力。

Abstract: The growing need to automate processes in industrial settings has led to tremendous growth in the robotic systems and especially the robotic arms. The paper assumes the design, modeling and control of a robotic arm to suit industrial purpose like assembly, welding and material handling. A six-degree-of-freedom (DOF) robotic manipulator was designed based on servo motors and a microcontroller interface with Mechanical links were also fabricated. Kinematic and dynamic analyses have been done in order to provide precise positioning and effective loads. Inverse Kinematics algorithm and Proportional-Integral-Derivative (PID) controller were also applied to improve the precision of control. The ability of the system to carry out tasks with high accuracy and repeatability is confirmed by simulation and experimental testing. The suggested robotic arm is an affordable, expandable, and dependable method of automation of numerous mundane procedures in the manufacturing industry.

</details>


### [599] [ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM](https://arxiv.org/abs/2512.00037)
*Tali Orlev Shapira,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出ICD-Net，一种通过学习处理原始惯性测量并生成带不确定性估计的位移来增强视觉-惯性SLAM性能的新框架，显著提升了高动态无人机场景下的轨迹估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-惯性SLAM在传感器标定不完美、测量噪声大、快速运动、低光照等情况下性能不佳，尤其影响对状态估计要求严格的无人机自主飞行。

Method: 提出ICD-Net，直接从惯性数据学习提取位移图并预测对应的协方差；将输出作为带有不确定性加权的残差项融入VINS-Fusion优化框架中，提供补偿误差的补充信息。

Result: 在高动态无人机序列上实验表明，相比标准VINS-Fusion，平均APE提升超过38%，且不确定性估计有助于增强系统鲁棒性。

Conclusion: 神经网络增强方法能有效应对多种SLAM性能退化问题，同时满足实时性要求。

Abstract: Visual-inertial SLAM systems often exhibit suboptimal performance due to multiple confounding factors including imperfect sensor calibration, noisy measurements, rapid motion dynamics, low illumination, and the inherent limitations of traditional inertial navigation integration methods. These issues are particularly problematic in drone applications where robust and accurate state estimation is critical for safe autonomous operation. In this work, we present ICD-Net, a novel framework that enhances visual-inertial SLAM performance by learning to process raw inertial measurements and generating displacement estimates with associated uncertainty quantification. Rather than relying on analytical inertial sensor models that struggle with real-world sensor imperfections, our method directly extracts displacement maps from sensor data while simultaneously predicting measurement covariances that reflect estimation confidence. We integrate ICD-Net outputs as additional residual constraints into the VINS-Fusion optimization framework, where the predicted uncertainties appropriately weight the neural network contributions relative to traditional visual and inertial terms. The learned displacement constraints provide complementary information that compensates for various error sources in the SLAM pipeline. Our approach can be used under both normal operating conditions and in situations of camera inconsistency or visual degradation. Experimental evaluation on challenging high-speed drone sequences demonstrated that our approach significantly improved trajectory estimation accuracy compared to standard VINS-Fusion, with more than 38% improvement in mean APE and uncertainty estimates proving crucial for maintaining system robustness. Our method shows that neural network enhancement can effectively address multiple sources of SLAM degradation while maintaining real-time performance requirements.

</details>


### [600] [VISTAv2: World Imagination for Indoor Vision-and-Language Navigation](https://arxiv.org/abs/2512.00041)
*Yanjia Huang,Xianshun Jiang,Xiangbo Gao,Mingyang Wu,Zhengzhong Tu*

Main category: cs.RO

TL;DR: 本文提出了VISTAv2，一种生成式世界模型，用于视觉-语言导航（VLN），通过动作条件下的未来视角预测与在线价值图规划相结合，提升导航鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像想象的VLN方法在连续真实空间中缺乏在线、动作条件预测能力，且未提供显式规划值；长时程目标函数又过于脆弱和低效。

Method: 提出VISTAv2模型：采用动作感知的条件扩散Transformer视频预测器生成短时序未来视角，在VAE隐空间中高效采样与稀疏解码；通过视觉-语言打分器对齐指令，并经可微分‘想象到价值’头融合多条rollout，输出想象的自我中心价值图；该价值图与基础目标在分数层面融合，实现可达性与风险感知引导。

Result: 在MP3D和RoboTHOR数据集上超越强基线；消融实验表明动作条件想象、指令引导的价值融合及在线价值图规划均至关重要。

Conclusion: VISTAv2为实现鲁棒、高效且可解释的VLN提供了实用新路径。

Abstract: Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN.

</details>


### [601] [Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge](https://arxiv.org/abs/2512.00048)
*Wenzheng Zhao,Ran Zhang,Ruth Palan Lopez,Shu-Fen Wung,Fengpei Yuan*

Main category: cs.RO

TL;DR: 提出一种名为Causal structure-aware Reinforcement Learning (CRL) 的新框架，将因果发现与推理融入策略优化，用于机器人辅助认知护理场景，提升决策的可解释性、鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在数据稀缺、需可解释决策且状态动态具有因果性的医疗干预中面临挑战，如痴呆照护。现有方法难以建模复杂的人类行为状态因果关系。

Method: 提出CRL框架，通过学习描述人类行为状态与机器人动作间因果依赖的有向无环图（DAG），并在策略优化中引入该因果结构，实现更高效的决策。在模拟的认知护理场景中验证，并设计轻量级LLM部署方式，将策略嵌入系统提示词中实现无需微调的对话生成。

Result: CRL在模拟实验中优于传统无模型RL基线，获得更高累积奖励，更稳定维持理想患者状态，行为更具可解释性且符合临床逻辑；性能在不同超参数下保持稳健；LLM部署方案能生成一致、支持性的对话。

Conclusion: CRL通过整合因果结构显著提升了强化学习在人机交互医疗场景中的适应性、可解释性和数据效率，展示了因果强化学习在智能健康领域的应用潜力。

Abstract: Reinforcement Learning (RL) faces significant challenges in adaptive healthcare interventions, such as dementia care, where data is scarce, decisions require interpretability, and underlying patient-state dynamic are complex and causal in nature. In this work, we present a novel framework called Causal structure-aware Reinforcement Learning (CRL) that explicitly integrates causal discovery and reasoning into policy optimization. This method enables an agent to learn and exploit a directed acyclic graph (DAG) that describes the causal dependencies between human behavioral states and robot actions, facilitating more efficient, interpretable, and robust decision-making. We validate our approach in a simulated robot-assisted cognitive care scenario, where the agent interacts with a virtual patient exhibiting dynamic emotional, cognitive, and engagement states. The experimental results show that CRL agents outperform conventional model-free RL baselines by achieving higher cumulative rewards, maintaining desirable patient states more consistently, and exhibiting interpretable, clinically-aligned behavior. We further demonstrate that CRL's performance advantage remains robust across different weighting strategies and hyperparameter settings. In addition, we demonstrate a lightweight LLM-based deployment: a fixed policy is embedded into a system prompt that maps inferred states to actions, producing consistent, supportive dialogue without LLM finetuning. Our work illustrates the promise of causal reinforcement learning for human-robot interaction applications, where interpretability, adaptiveness, and data efficiency are paramount.

</details>


### [602] [Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches](https://arxiv.org/abs/2512.00049)
*Ibrahim Khalil Kabir,Muhammad Faizan Mysorewala*

Main category: cs.RO

TL;DR: 本文综述了基于深度强化学习（DRL）的社交感知导航方法，分析了其在人类环境中提升机器人交互能力的关键因素与挑战。


<details>
  <summary>Details</summary>
Motivation: 为了使机器人能够在人类环境中安全、自然地移动并遵守隐含的社会规范，需要发展能够融合社会常识的导航策略。

Method: 系统回顾和分析了结合值函数、策略梯度及Actor-Critic框架的DRL算法，以及前馈、循环、卷积、图网络和Transformer等神经网络结构在社交导航中的应用。

Result: 发现DRL相比传统方法显著提升了安全性与人类接受度，但在评估机制、标准化社交指标、计算开销和仿真到现实迁移方面仍存在挑战。

Conclusion: 未来的发展依赖于融合多种方法优势的混合策略，并建立兼顾技术效率与以人为中心评估的统一基准。

Abstract: Socially aware navigation is a fast-evolving research area in robotics that enables robots to move within human environments while adhering to the implicit human social norms. The advent of Deep Reinforcement Learning (DRL) has accelerated the development of navigation policies that enable robots to incorporate these social conventions while effectively reaching their objectives. This survey offers a comprehensive overview of DRL-based approaches to socially aware navigation, highlighting key aspects such as proxemics, human comfort, naturalness, trajectory and intention prediction, which enhance robot interaction in human environments. This work critically analyzes the integration of value-based, policy-based, and actor-critic reinforcement learning algorithms alongside neural network architectures, such as feedforward, recurrent, convolutional, graph, and transformer networks, for enhancing agent learning and representation in socially aware navigation. Furthermore, we examine crucial evaluation mechanisms, including metrics, benchmark datasets, simulation environments, and the persistent challenges of sim-to-real transfer. Our comparative analysis of the literature reveals that while DRL significantly improves safety, and human acceptance over traditional approaches, the field still faces setback due to non-uniform evaluation mechanisms, absence of standardized social metrics, computational burdens that limit scalability, and difficulty in transferring simulation to real robotic hardware applications. We assert that future progress will depend on hybrid approaches that leverage the strengths of multiple approaches and producing benchmarks that balance technical efficiency with human-centered evaluation.

</details>


### [603] [Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control](https://arxiv.org/abs/2512.00050)
*Suzie Kim*

Main category: cs.RO

TL;DR: 提出了一种基于隐式脑电反馈（EEG中的ErrPs）的强化学习框架RLIHF，用于在稀疏奖励条件下实现无需显式人为干预的有效策略学习，在仿真环境中实现了与密集手工奖励相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在稀疏奖励下难以学习有效策略，而现有基于人类反馈的强化学习（RLHF）依赖显式反馈，增加用户认知负担。因此需要一种更自然、非侵入式的反馈方式。

Method: 利用预训练解码器将非侵入式脑电（EEG）中的错误相关电位（ErrPs）转化为概率性奖励信号，作为强化学习的隐式反馈，结合稀疏外部奖励进行策略学习。

Result: 在MuJoCo仿真环境中使用Kinova Gen2机械臂进行避障抓取放置任务，实验表明基于解码EEG反馈训练的智能体表现与使用密集手工设计奖励的智能体相当。

Conclusion: 隐式神经反馈（如EEG ErrPs）可用于实现可扩展且与人类对齐的交互式机器人强化学习，减少对显式人类反馈的依赖。

Abstract: Conventional reinforcement learning (RL) approaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, reinforcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, enabling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.

</details>


### [604] [Modeling and Control of Magnetic Forces between Microrobots](https://arxiv.org/abs/2512.00051)
*Amelia Fernández Seguel,Alejandro I. Maass*

Main category: cs.RO

TL;DR: 本文提出了一种级联控制方法，仅通过调节全局磁场角度ψ，实现对两个磁性微机器人间径向距离的独立、快速、精确控制，避免磁场方向突变，适用于靶向给药和微手术等生物医学场景。


<details>
  <summary>Details</summary>
Motivation: 现有磁控微机器人系统多依赖同步运动，难以满足需差异化驱动的生物医学应用（如靶向给药、微手术），亟需在共享全局磁场下实现多智能体的独立控制。

Method: 构建包含磁偶极-偶极相互作用与粘性介质中运动学的二维物理模型；设计PID控制器调节径向距离，并级联PD控制器平滑磁场角度变化；在MATLAB中进行仿真验证。

Result: PID控制器使收敛时间缩短约40%；PID+PD级联方案在相近时间内实现平滑角轨迹，角度波动仅±5°；验证了仅用磁场角度ψ即可快速精准调控两微机器人径向距离的可行性。

Conclusion: 该级联控制策略在2D双智能体场景下有效实现了共享磁场下的差异化径向控制，具备工程实用性；但受限于二维与双体假设，未来需拓展至3D及多智能体系统。

Abstract: The independent control of multiple magnetic microrobots under a shared global signal presents critical challenges in biomedical applications such as targeted drug delivery and microsurgeries. Most existing systems only allow all agents to move synchronously, limiting their use in applications that require differentiated actuation. This research aims to design a controller capable of regulating the radial distance between micro-agents using only the angle ψof a global magnetic field as the actuation parameter, demonstrating potential for practical applications. The proposed cascade control approach enables faster and more precise adjustment of the inter-agent distance than a proportional controller, while maintaining smooth transitions and avoiding abrupt changes in the orientation of the magnetic field, making it suitable for real-world implementation. A bibliographic review was conducted to develop the physical model, considering magnetic dipole-dipole interactions and velocities in viscous media. A PID controller was implemented to regulate the radial distance, followed by a PD controller in cascade to smooth changes in field orientation. These controllers were simulated in MATLAB, showing that the PID controller reduced convergence time to the desired radius by about 40%. When adding the second controller, the combined PID+PD scheme achieved smooth angular trajectories within similar timeframes, with fluctuations of only \pm 5^\circ. These results validate the feasibility of controlling the radial distance of two microrobots using a shared magnetic field in a fast and precise manner, without abrupt variations in the control angle. However, the model is limited to a 2D environment and two agents, suggesting future research to extend the controller to 3D systems and multiple agents.

</details>


### [605] [An adaptive experience-based discrete genetic algorithm for multi-trip picking robot task scheduling in smart orchards](https://arxiv.org/abs/2512.00057)
*Peng Chen,Jing Liangb,Kang-Jia Qiao,Hui Song,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本文提出了一种自适应经验驱动的离散遗传算法（AEDGA），用于解决智能果园中多行程采摘机器人任务调度（MTPRTS）问题，通过负载-距离平衡初始化、聚类局部搜索和经验自适应选择策略，显著提升了求解效率与质量。


<details>
  <summary>Details</summary>
Motivation: 应对果园自动化采摘中多机器人调度面临的计算复杂性、冗余局部搜索及易陷入局部最优等问题，同时满足严格完工时间约束。

Method: 提出自适应经验驱动的离散遗传算法（AEDGA），包含三方面创新：集成负载-距离平衡的初始化方法、基于聚类的局部搜索机制、经验驱动的自适应选择策略；并设计了三种框架构成的可行性修复策略以保障makespan约束。

Result: 在18个新构建和24个现有测试实例上，AEDGA显著优于8种前沿算法。

Conclusion: AEDGA有效缓解了传统元启发式算法在大规模MTPRTS问题中的搜索低效与早熟收敛问题，为智能果园多机器人协同作业提供了高效可行的调度方案。

Abstract: The continuous innovation of smart robotic technologies is driving the development of smart orchards, significantly enhancing the potential for automated harvesting systems. While multi-robot systems offer promising solutions to address labor shortages and rising costs, the efficient scheduling of these systems presents complex optimization challenges. This research investigates the multi-trip picking robot task scheduling (MTPRTS) problem. The problem is characterized by its provision for robot redeployment while maintaining strict adherence to makespan constraints, and encompasses the interdependencies among robot weight, robot load, and energy consumption, thus introducing substantial computational challenges that demand sophisticated optimization algorithms.To effectively tackle this complexity, metaheuristic approaches, which often utilize local search mechanisms, are widely employed. Despite the critical role of local search in vehicle routing problems, most existing algorithms are hampered by redundant local operations, leading to slower search processes and higher risks of local optima, particularly in large-scale scenarios. To overcome these limitations, we propose an adaptive experience-based discrete genetic algorithm (AEDGA) that introduces three key innovations: (1) integrated load-distance balancing initialization method, (2) a clustering-based local search mechanism, and (3) an experience-based adaptive selection strategy. To ensure solution feasibility under makespan constraints, we develop a solution repair strategy implemented through three distinct frameworks. Comprehensive experiments on 18 proposed test instances and 24 existing test problems demonstrate that AEDGA significantly outperforms eight state-of-the-art algorithms.

</details>


### [606] [SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning](https://arxiv.org/abs/2512.00062)
*Taewook Nam,Sung Ju Hwang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SpeedAug的基于强化学习的策略加速框架，通过在速度增强的演示上预训练策略来构建行为先验，从而高效地适应预先训练好的策略以实现更快的任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有的策略加速方法由于重新解释动作序列导致与原始演示存在分布偏移，而强化学习虽然有潜力但探索效率低下。因此需要一种能够高效利用样本并保持高成功率的方法来加速机器人操作策略。

Method: SpeedAug框架通过在不同速度下增强的演示数据上预训练一个策略，形成包含多种执行节奏的行为先验，并在此基础上进行强化学习微调，以适应更快速度的任务执行。

Result: 在机器人操作基准上的实验结果表明，从这种节奏丰富的策略初始化后进行RL微调，显著提高了现有RL和策略加速方法的样本效率，同时保持了较高的成功率。

Conclusion: SpeedAug通过构建速度多样化的行为先验，有效提升了策略加速过程中的样本效率和性能，为实际应用中快速、高效的机器人控制提供了可行方案。

Abstract: Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.

</details>


### [607] [Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals](https://arxiv.org/abs/2512.00069)
*Ohad Bachner,Bar Gamliel*

Main category: cs.RO

TL;DR: 本文提出一种将大语言模型（LLM）与符号规划结合的方法，利用LLM补充自然语言任务中缺失的常识性前提和子目标，提升机器人在现实场景中的规划有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人常因指令忽略常识性细节（如隐含前提和细分子目标）而在日常任务中失败；传统符号规划器需人工显式编写这些细节，费时且易遗漏。

Method: 将大语言模型与符号规划结合：给定自然语言任务，LLM生成合理的前提条件和子目标，再将其形式化为规划模型并在仿真中执行。

Result: 相比无LLM的基线规划器，该系统生成更有效的计划、任务成功率更高、环境变化时适应性更强。

Conclusion: 将LLM的常识推理能力融入经典符号规划，可显著提升机器人在真实场景中的行为可靠性。

Abstract: Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.

</details>


### [608] [Reconfigurable Auxetic Devices (RADs) for Robotic Surface Manipulation](https://arxiv.org/abs/2512.00072)
*Jacob Miske,Ahyan Maya,Ahnaf Inkiad,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 本文提出了一种基于可重构手性晶格的机器人表面操纵方法，利用负泊松比材料实现多方向扩展和表面适应性，通过单元间的间隙实现局部区域的可变膨胀，并结合实验验证了简化模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人表面通常使用正泊松比材料进行推拉操作，而具有负泊松比的手性材料在拉伸时可实现多方向扩展，从而提供更优的 conformable 接口，但其控制灵活性和适应性仍有提升空间。

Method: 采用可重构锁定机制或嵌入式舵机对手性晶格结构进行欠驱动控制，利用晶格单元之间的间隙（backlash）实现局部区域的可变膨胀，并通过实验与包含激活函数的简化模型对比验证系统性能。

Result: 实现了可变表面共形性的演示，验证了系统在表面收缩与扩张中的操控能力，实验结果与简化模型吻合良好。

Conclusion: 该可重构手性结构在保持柔顺性的同时，通过引入间隙机制提升了控制自由度，为自适应机器人结构在表面操纵任务中开辟了新机会。

Abstract: Robotic surfaces traditionally use materials with a positive Poisson's ratio to push and pull on a manipulation interface. Auxetic materials with a negative Poisson's ratio may expand in multiple directions when stretched and enable conformable interfaces. Here we demonstrate reconfigurable auxetic lattices for robotic surface manipulation. Our approach enables shape control through reconfigurable locking or embedded servos that underactuate an auxetic lattice structure. Variable expansion of local lattice areas is enabled by backlash between unit cells. Demonstrations of variable surface conformity are presented with characterization metrics. Experimental results are validated against a simplified model of the system, which uses an activation function to model intercell coupling with backlash. Reconfigurable auxetic structures are shown to achieve manipulation via variable surface contraction and expansion. This structure maintains compliance with backlash in contrast with previous work on auxetics, opening new opportunities in adaptive robotic structures for surface manipulation tasks.

</details>


### [609] [Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning](https://arxiv.org/abs/2512.00074)
*Qiwei Liang,Boyang Cai,Minghao Lai,Sitong Zhuang,Tao Lin,Yan Qin,Yixuan Ye,Jiaming Liang,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出AFRO框架，通过自监督方式学习动态感知的3D表征，无需动作或重建监督，在机器人操作任务中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉预训练方法在机器人操作任务上表现不佳，主要由于缺乏状态-动作-状态动力学建模，以及显式几何重建带来的冗余。

Method: AFRO将状态预测建模为生成扩散过程，并在共享潜在空间中联合建模前向与逆向动力学；采用特征差分和逆一致性监督防止动作学习中的特征泄露。

Result: AFRO结合Diffusion Policy后，在16个仿真和4个真实世界操作任务中大幅提高成功率，优于现有预训练方法；且随数据量和任务复杂度扩展性良好；可视化显示其学习到语义丰富、判别性强的特征。

Conclusion: AFRO是一种有效的面向机器人3D表征学习的自监督预训练框架，兼顾动力学建模能力与表征质量。

Abstract: Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We attribute this gap to two factors: the lack of state-action-state dynamics modeling and the unnecessary redundancy of explicit geometric reconstruction. We introduce AFRO, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. To prevent feature leakage in action learning, we employ feature differencing and inverse-consistency supervision, improving the quality and stability of visual features. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches. The framework also scales favorably with data volume and task complexity. Qualitative visualizations indicate that AFRO learns semantically rich, discriminative features, offering an effective pre-training solution for 3D representation learning in robotics. Project page: https://kolakivy.github.io/AFRO/

</details>


### [610] [Arcadia: Toward a Full-Lifecycle Framework for Embodied Lifelong Learning](https://arxiv.org/abs/2512.00076)
*Minghe Gao,Juncheng Li,Yuze Lin,Xuqi Liu,Jiaming Ji,Xiaoran Pan,Zihan Xu,Xian Li,Mingjie Li,Wei Ji,Rong Wei,Rui Tang,Qizhou Wang,Kai Shen,Jun Xiao,Qi Wu,Siliang Tang,Yueting Zhuang*

Main category: cs.RO

TL;DR: 本文提出Arcadia框架，强调具身学习应作为生命周期问题而非单阶段优化，通过闭环耦合自主探索、场景生成、共享表征学习和仿真评估四个阶段，实现持续改进与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有系统仅优化数据收集、仿真、学习或部署中的单一环节，难以持续改进或泛化到广泛场景，因此需将具身学习视为整体生命周期问题。

Method: 提出Arcadia闭环框架，包含四个紧密耦合阶段：（1）自我演化的探索与接地以实现物理环境中的自主数据采集；（2）生成式场景重建与增强以创建真实可扩展场景；（3）统一导航与操作的共享具身表征架构；（4）基于仿真的‘从实到仿’评估与演化以闭合反馈环。

Result: Arcadia在导航与操作基准测试中持续提升性能，并能稳健迁移到实体机器人，验证了该生命周期方法对终身学习与端到端泛化的有效性。

Conclusion: 具身学习必须采用不可分解的闭环生命周期设计，Arcadia为通用具身智能体提供了可扩展的基础框架，并开源标准化接口以支持可复现评估与跨模型比较。

Abstract: We contend that embodied learning is fundamentally a lifecycle problem rather than a single-stage optimization. Systems that optimize only one link (data collection, simulation, learning, or deployment) rarely sustain improvement or generalize beyond narrow settings. We introduce Arcadia, a closed-loop framework that operationalizes embodied lifelong learning by tightly coupling four stages: (1) Self-evolving exploration and grounding for autonomous data acquisition in physical environments, (2) Generative scene reconstruction and augmentation for realistic and extensible scene creation, (3) a Shared embodied representation architecture that unifies navigation and manipulation within a single multimodal backbone, and (4) Sim-from-real evaluation and evolution that closes the feedback loop through simulation-based adaptation. This coupling is non-decomposable: removing any stage breaks the improvement loop and reverts to one-shot training. Arcadia delivers consistent gains on navigation and manipulation benchmarks and transfers robustly to physical robots, indicating that a tightly coupled lifecycle: continuous real-world data acquisition, generative simulation update, and shared-representation learning, supports lifelong improvement and end-to-end generalization. We release standardized interfaces enabling reproducible evaluation and cross-model comparison in reusable environments, positioning Arcadia as a scalable foundation for general-purpose embodied agents.

</details>


### [611] [A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs](https://arxiv.org/abs/2512.00077)
*Bowen Zhi*

Main category: cs.RO

TL;DR: 本文提出了一种分层控制架构，结合基于学习的步态生成与基于模型的平衡控制，提升搭载超数肢体（SLs）的人形机器人行走稳定性。实验表明该动态平衡控制器显著改善了质心轨迹相似性与地面反作用力协调性。


<details>
  <summary>Details</summary>
Motivation: 超数肢体（SLs）为类人机器人带来动态扰动，严重影响其运动稳定性，亟需设计有效控制策略以应对SLs引入的质量与运动干扰。

Method: 提出分层控制架构：底层采用模仿学习与课程学习生成Unitree H1机器人的步行步态；高层主动利用SLs实现动态平衡；整体采用解耦策略融合学习型步态与模型型平衡。

Result: 相比静态负载条件，动态平衡控制器使质心轨迹DTW距离降低47%，步态更接近无负载基线，并提升了单步周期内的再稳定能力及地面反作用力的反相协调性。

Conclusion: 解耦式分层控制设计能有效抑制SLs带来的内部动态扰动，从而实现搭载功能性超数肢体的人形机器人稳定行走。

Abstract: The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs.

</details>


### [612] [Hyper-GoalNet: Goal-Conditioned Manipulation Policy Learning with HyperNetworks](https://arxiv.org/abs/2512.00085)
*Pei Zhou,Wanting Yao,Qian Luo,Xunzhe Zhou,Yanchao Yang*

Main category: cs.RO

TL;DR: 本文提出Hyper-GoalNet框架，利用超网络根据目标描述动态生成策略网络参数，分离目标理解与状态处理，并引入前向动力学模型和距离约束提升表征质量，在多样化操纵任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目标条件策略学习在多样化目标和环境中维持高性能存在挑战，需更灵活、鲁棒的策略生成机制。

Method: 提出Hyper-GoalNet：用超网络从目标描述生成策略网络参数；分离目标解释（决定参数）与状态处理（应用参数）；引入前向动力学模型和距离单调性约束优化隐空间表征。

Result: 在多种环境随机化的操纵任务中性能显著超越SOTA，尤其在高变异性条件下；真实机器人实验验证其对传感器噪声与物理不确定性的鲁棒性。

Conclusion: Hyper-GoalNet通过参数化策略生成与结构化隐空间约束，有效提升了目标条件策略的泛化性与鲁棒性，为复杂机器人操纵提供了新范式。

Abstract: Goal-conditioned policy learning for robotic manipulation presents significant challenges in maintaining performance across diverse objectives and environments. We introduce Hyper-GoalNet, a framework that generates task-specific policy network parameters from goal specifications using hypernetworks. Unlike conventional methods that simply condition fixed networks on goal-state pairs, our approach separates goal interpretation from state processing -- the former determines network parameters while the latter applies these parameters to current observations. To enhance representation quality for effective policy generation, we implement two complementary constraints on the latent space: (1) a forward dynamics model that promotes state transition predictability, and (2) a distance-based constraint ensuring monotonic progression toward goal states. We evaluate our method on a comprehensive suite of manipulation tasks with varying environmental randomization. Results demonstrate significant performance improvements over state-of-the-art methods, particularly in high-variability conditions. Real-world robotic experiments further validate our method's robustness to sensor noise and physical uncertainties. Code is available at: https://github.com/wantingyao/hyper-goalnet.

</details>


### [613] ["Why the face?": Exploring Robot Error Detection Using Instrumented Bystander Reactions](https://arxiv.org/abs/2512.00262)
*Maria Teresa Parreira,Ruidong Zhang,Sukruth Gowdru Lingaraju,Alexandra Bremers,Xuanyu Fang,Adolfo Ramirez-Aristizabal,Manaswi Saha,Michael Kuniavsky,Cheng Zhang,Wendy Ju*

Main category: cs.RO

TL;DR: 提出一种基于颈部佩戴设备捕捉人类对机器人错误反应的面部表情的新方法，通过NeckNet-18模型实现更准确的机器人错误检测。


<details>
  <summary>Details</summary>
Motivation: 机器人难以感知和利用人类细微的社会反馈（如皱眉、笑声），限制了其在社交环境中的适应能力。

Method: 开发了一种颈戴式设备以从下巴区域捕捉面部表情，并构建NeckNet-18 3D面部重建模型来映射面部关键点和头部运动，进而训练机器人错误检测模型。

Result: 该模型在识别机器人错误方面优于OpenFace和传统视频数据方法，尤其在个体内部数据上表现出更强的泛化能力。

Conclusion: 扩展以人为中心的机器人感知方式有助于提升机器人在复杂人类环境中的社会适应性，推动社交线索检测与自适应机器人技术的发展。

Abstract: How do humans recognize and rectify social missteps? We achieve social competence by looking around at our peers, decoding subtle cues from bystanders - a raised eyebrow, a laugh - to evaluate the environment and our actions. Robots, however, struggle to perceive and make use of these nuanced reactions. By employing a novel neck-mounted device that records facial expressions from the chin region, we explore the potential of previously untapped data to capture and interpret human responses to robot error. First, we develop NeckNet-18, a 3D facial reconstruction model to map the reactions captured through the chin camera onto facial points and head motion. We then use these facial responses to develop a robot error detection model which outperforms standard methodologies such as using OpenFace or video data, generalizing well especially for within-participant data. Through this work, we argue for expanding human-in-the-loop robot sensing, fostering more seamless integration of robots into diverse human environments, pushing the boundaries of social cue detection and opening new avenues for adaptable robotics.

</details>


### [614] [RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals](https://arxiv.org/abs/2512.00287)
*Yuzheng Gao,Yuxing Long,Lei Kang,Yuchong Guo,Ziyan Yu,Shangqing Mao,Jiyao Zhang,Ruihai Wu,Dongjiang Li,Hui Shen,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了RealAppliance数据集和RealAppliance-Bench基准，旨在解决现有家电资产在仿真与现实间存在的渲染质量差、机制不全及与说明书不一致等问题，以推动家电操作研究。


<details>
  <summary>Details</summary>
Motivation: 现有家电资产存在渲染质量差、物理/电子机制不完整、与说明书不一致等问题，导致仿真与现实之间存在差距，阻碍了家电操作技术的发展。

Method: 构建了包含100个高保真家电的RealAppliance数据集，涵盖完整的物理、电子机制及程序逻辑，并基于此提出RealAppliance-Bench基准，用于评估多模态大语言模型和具身操作规划模型在手册检索、部件定位、开环/闭环操作规划等任务上的性能。

Result: 在RealAppliance-Bench上对多种模型进行了系统评测，揭示了当前模型在家电操作规划任务中的能力边界与不足。

Conclusion: RealAppliance数据集与RealAppliance-Bench为家电操作研究提供了更贴近真实场景的高质量资源与统一评估标准，有助于推动该领域向更实用、鲁棒的方向发展。

Abstract: Existing appliance assets suffer from poor rendering, incomplete mechanisms, and misalignment with manuals, leading to simulation-reality gaps that hinder appliance manipulation development. In this work, we introduce the RealAppliance dataset, comprising 100 high-fidelity appliances with complete physical, electronic mechanisms, and program logic aligned with their manuals. Based on these assets, we propose the RealAppliance-Bench benchmark, which evaluates multimodal large language models and embodied manipulation planning models across key tasks in appliance manipulation planning: manual page retrieval, appliance part grounding, open-loop manipulation planning, and closed-loop planning adjustment. Our analysis of model performances on RealAppliance-Bench provides insights for advancing appliance manipulation research

</details>


### [615] [MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation](https://arxiv.org/abs/2512.00324)
*Jinda Du,Jieji Ren,Qiaojun Yu,Ningbin Zhang,Yu Deng,Xingyu Wei,Yufei Liu,Guoying Gu,Xiangyang Zhu*

Main category: cs.RO

TL;DR: 本文提出MILE系统，通过机械同构的遥操作与数据采集框架，解决了灵巧手模仿学习中高保真数据稀缺的问题，实现了无重映射的精准控制与高分辨率触觉感知，并构建了多模态高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习在灵巧手操作中受限于大规模、高保真数据的缺乏，且数据采集流程存在运动重映射不准、效率低、缺少高分辨率指尖触觉传感等问题。

Method: 设计MILE系统：基于人体手部解剖学参数构建外骨骼，机器人手保持关节位置一一对应同构性，消除非线性重映射；外骨骼实现多关节平均绝对角度误差低于1度；机器人手集成紧凑型指尖视觉-触觉模块；在此无重映射接口上开展遥操作并采集包含高分辨率指尖触觉、RGB-D图像和关节位置的多模态数据。

Result: 遥操作流程平均成功率提升64%；引入指尖触觉信号后，相较纯视觉基线平均再提升25%成功率。

Conclusion: MILE系统显著提升了灵巧手模仿学习的数据质量与采集效率，验证了高保真多模态（尤其触觉）数据对提升操作性能的关键作用。

Abstract: Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.

</details>


### [616] [DPNet: Doppler LiDAR Motion Planning for Highly-Dynamic Environments](https://arxiv.org/abs/2512.00375)
*Wei Zuo,Zeyi Ren,Chengyang Li,Yikun Wang,Mingle Zhao,Shuai Wang,Wei Sui,Fei Gao,Yik-Chung Wu,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种结合多普勒LiDAR的运动规划网络DPNet，通过D-KalmanNet和DT-MPC实现对快速移动障碍物的高效跟踪与实时反应，在保证高频率和高精度的同时实现了轻量化设计。


<details>
  <summary>Details</summary>
Motivation: 现有运动规划方法难以应对快速移动障碍物，因对环境变化理解不足，尤其在感知数据不完整时表现不佳。

Method: 提出DPNet，包含基于多普勒的Kalman神经网络（D-KalmanNet）用于预测障碍物状态，并构建多普勒调优的模型预测控制框架（DT-MPC）实现自适应参数调节的路径规划。

Result: 在高保真仿真器和真实数据集上验证了DPNet在跟踪与规划的准确性和频率上的优越性，显著优于多种基准方法。

Conclusion: DPNet通过融合多普勒LiDAR信息与模型驱动学习，在低数据消耗下实现了高效、准确且实时的动态环境运动规划。

Abstract: Existing motion planning methods often struggle with rapid-motion obstacles due to an insufficient understanding of environmental changes. To address this limitation, we propose integrating motion planners with Doppler LiDARs which provide not only ranging measurements but also instantaneous point velocities. However, this integration is nontrivial due to the dual requirements of high accuracy and high frequency. To this end, we introduce Doppler Planning Network (DPNet), which tracks and reacts to rapid obstacles using Doppler model-based learning. Particularly, we first propose a Doppler Kalman neural network (D-KalmanNet) to track the future states of obstacles under partially observable Gaussian state space model. We then leverage the estimated motions to construct a Doppler-tuned model predictive control (DT-MPC) framework for ego-motion planning, enabling runtime auto-tuning of the controller parameters. These two model-based learners allow DPNet to maintain lightweight while learning fast environmental changes using minimum data, and achieve both high frequency and high accuracy in tracking and planning. Experiments on both high-fidelity simulator and real-world datasets demonstrate the superiority of DPNet over extensive benchmark schemes.

</details>


### [617] [Balancing Efficiency and Fairness: An Iterative Exchange Framework for Multi-UAV Cooperative Path Planning](https://arxiv.org/abs/2512.00410)
*Hongzong Li,Luwei Liao,Xiangguang Dai,Yuming Feng,Rong Feng,Shiqin Tang*

Main category: cs.RO

TL;DR: 本文提出了一种用于多无人机协同路径规划（MUCPP）的迭代交换框架，通过任务交换与路径优化，在总任务代价最小化与各无人机工作负载均衡（公平性）之间取得更好权衡。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机协同路径规划中效率（最小化总任务成本）与公平性（平衡各无人机工作负载）难以兼顾的关键挑战。

Method: 提出迭代交换框架，联合优化总任务距离与最大完成时间（makespan）；在地形感知构型空间中使用A*搜索为每架无人机生成无碰撞轨迹；通过满足可行性与安全性约束的局部任务交换持续改进解。

Result: 在多个地形数据集上的实验表明，该方法在总距离与makespan之间的权衡上始终优于现有基线方法。

Conclusion: 所提迭代交换框架能有效协调效率与公平性，在多无人机协同路径规划中实现更优的整体性能。

Abstract: Multi-UAV cooperative path planning (MUCPP) is a fundamental problem in multi-agent systems, aiming to generate collision-free trajectories for a team of unmanned aerial vehicles (UAVs) to complete distributed tasks efficiently. A key challenge lies in achieving both efficiency, by minimizing total mission cost, and fairness, by balancing the workload among UAVs to avoid overburdening individual agents. This paper presents a novel Iterative Exchange Framework for MUCPP, balancing efficiency and fairness through iterative task exchanges and path refinements. The proposed framework formulates a composite objective that combines the total mission distance and the makespan, and iteratively improves the solution via local exchanges under feasibility and safety constraints. For each UAV, collision-free trajectories are generated using A* search over a terrain-aware configuration space. Comprehensive experiments on multiple terrain datasets demonstrate that the proposed method consistently achieves superior trade-offs between total distance and makespan compared to existing baselines.

</details>


### [618] [Hardware-Software Collaborative Computing of Photonic Spiking Reinforcement Learning for Robotic Continuous Control](https://arxiv.org/abs/2512.00427)
*Mengting Yu,Shuiying Xiang,Changjian Xie,Yonghang Chen,Haowen Zhao,Xingxing Guo,Yahui Zhang,Yanan Han,Yue Hao*

Main category: cs.RO

TL;DR: 本文提出了一种基于光子脉冲强化学习的新型计算架构，结合TD3算法与脉冲神经网络（SNN），采用硅基光子MZI芯片执行线性计算、电子域处理非线性脉冲激活，在Pendulum-v1和HalfCheetah-v2任务中实现了更高能效（1.39 TOPS/W）、超低延迟（120 ps）及更快收敛（减少23.33%步数）的实时机器人连续控制。


<details>
  <summary>Details</summary>
Motivation: 传统电子计算平台在高维状态空间和实时交互要求下的机器人连续控制任务中面临算力瓶颈，亟需更高效低延迟的新型计算范式。

Method: 将TD3强化学习算法与脉冲神经网络（SNN）融合，构建光电混合计算架构：硅光MZI芯片负责线性矩阵运算，电子电路实现非线性脉冲激活。

Result: 在Pendulum-v1和HalfCheetah-v2上验证了软硬件协同推理能力，HalfCheetah-v2奖励达5831，收敛步数减少23.33%，动作偏差低于2.2%，能效1.39 TOPS/W，延迟仅120 ps。

Conclusion: 该工作首次将可编程MZI光子计算芯片应用于机器人连续控制任务，证明了光子脉冲RL在自主与工业机器人实时决策中的巨大潜力。

Abstract: Robotic continuous control tasks impose stringent demands on the energy efficiency and latency of computing architectures due to their high-dimensional state spaces and real-time interaction requirements. Conventional electronic computing platforms face computational bottlenecks, whereas the fusion of photonic computing and spiking reinforcement learning (RL) offers a promising alternative. Here, we propose a novel computing architecture based on photonic spiking RL, which integrates the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm with spiking neural network (SNN). The proposed architecture employs an optical-electronic hybrid computing paradigm wherein a silicon photonic Mach-Zehnder interferometer (MZI) chip executes linear matrix computations, while nonlinear spiking activations are performed in the electronic domain. Experimental validation on the Pendulum-v1 and HalfCheetah-v2 benchmarks demonstrates the system capability for software-hardware co-inference, achieving a control policy reward of 5831 on HalfCheetah-v2, a 23.33% reduction in convergence steps, and an action deviation below 2.2%. Notably, this work represents the first application of a programmable MZI photonic computing chip to robotic continuous control tasks, attaining an energy efficiency of 1.39 TOPS/W and an ultralow computational latency of 120 ps. Such performance underscores the promise of photonic spiking RL for real-time decision-making in autonomous and industrial robotic systems.

</details>


### [619] [Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction](https://arxiv.org/abs/2512.00453)
*Arad Firouzkouhi,Omid Mirzaeedodangeh,Lars Lindemann*

Main category: cs.RO

TL;DR: 本文提出了CRSAIL，一种用于主动模仿学习的查询策略，通过共形拒绝采样在状态空间中识别欠表示的状态并选择性地查询专家，显著减少专家标注成本，同时保持或超过专家级性能。


<details>
  <summary>Details</summary>
Motivation: 现有主动模仿学习方法在GPU密集型模拟器、人机协作场景及机器人集群中因频繁查询专家而导致成本过高，尤其在重复或相似状态下冗余查询严重，亟需一种高效且鲁棒的查询机制以降低标注开销。

Method: CRSAIL通过计算当前状态与第K近邻专家状态的距离来评估状态新颖性，并利用共形预测设定全局阈值（基于(1-α)分位数）决定是否查询专家；该方法在整条轨迹执行完毕后批量查询，无需实时专家干预。

Result: 在MuJoCo机器人任务上，CRSAIL相比DAgger最多减少96%的专家查询，相比其他主动模仿学习方法最多减少65%，同时达到或超越专家奖励水平，且对超参数α和K具有实证鲁棒性。

Conclusion: CRSAIL提供了一种任务无关、分布自由的主动查询机制，有效缓解协变量偏移问题，在保证性能的同时极大降低专家标注成本，适用于动态未知的新系统部署。

Abstract: Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-α)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $α$ to the expected query rate and makes $α$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $α$ and $K$, easing deployment on novel systems with unknown dynamics.

</details>


### [620] [LAP: Fast LAtent Diffusion Planner with Fine-Grained Feature Distillation for Autonomous Driving](https://arxiv.org/abs/2512.00470)
*Jinhao Zhang,Wenlong Xia,Zhexuan Zhou,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 本文提出了LAtent Planner (LAP)，一种在VAE学习的潜在空间中进行规划的新框架，该空间解耦了高层意图与低层运动学，支持单步去噪生成高质量驾驶策略，并显著提升推理速度和闭环性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在自动驾驶行为建模中表现优异，但存在采样延迟高、直接在原始轨迹点上操作导致模型资源浪费于低层运动学而非高层多模态语义的问题。

Method: 提出LAP框架：1）构建VAE学习的潜在空间以解耦高层意图与低层运动学；2）引入细粒度特征蒸馏机制，增强高层语义规划空间与向量化场景上下文的交互融合；3）实现单步去噪规划。

Result: 在nuPlan大规模基准测试中，LAP在基于学习的规划方法中达到最优闭环性能，并相较此前SOTA方法最多提速10倍。

Conclusion: LAP通过潜在空间解耦与单步生成设计，有效兼顾规划质量与效率，为高效、多模态自动驾驶规划提供了新范式。

Abstract: Diffusion models have demonstrated strong capabilities for modeling human-like driving behaviors in autonomous driving, but their iterative sampling process induces substantial latency, and operating directly on raw trajectory points forces the model to spend capacity on low-level kinematics, rather than high-level multi-modal semantics. To address these limitations, we propose LAtent Planner (LAP), a framework that plans in a VAE-learned latent space that disentangles high-level intents from low-level kinematics, enabling our planner to capture rich, multi-modal driving strategies. We further introduce a fine-grained feature distillation mechanism to guide a better interaction and fusion between the high-level semantic planning space and the vectorized scene context. Notably, LAP can produce high-quality plans in one single denoising step, substantially reducing computational overhead. Through extensive evaluations on the large-scale nuPlan benchmark, LAP achieves state-of-the-art closed-loop performance among learning-based planning methods, while demonstrating an inference speed-up of at most 10 times over previous SOTA approaches.

</details>


### [621] [HAVEN: Hierarchical Adversary-aware Visibility-Enabled Navigation with Cover Utilization using Deep Transformer Q-Networks](https://arxiv.org/abs/2512.00592)
*Mihir Chauhan,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: 提出一种结合深度Transformer Q网络（DTQN）与模块化低层控制器的分层导航框架，通过任务感知特征历史和可见性感知候选生成，在部分可观环境中实现更安全、高效的自主导航。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，传统路径规划和无记忆强化学习方法因视野受限和遮挡问题易导致不安全或低效行为，难以满足机器人在自动驾驶、仓储自动化等场景中的安全导航需求。

Method: 采用分层架构：高层使用DTQN基于任务相关特征（如里程计、目标方向、障碍物距离和可见性）的历史序列选择子目标，并引入可见性感知的候选生成机制（如掩码和暴露惩罚）；底层采用势场控制器执行子目标并实现短期避障。方法在2D仿真中验证后，直接迁移到3D Unity-ROS环境，通过点云投影保持特征一致性。

Result: 相比经典规划器和强化学习基线，该方法在成功率、安全距离和到达目标时间上表现更优；消融实验表明时序记忆和可见性感知设计对性能提升至关重要。

Conclusion: 所提出的框架具有良好的可迁移性和通用性，能够在不同维度和平台间有效支持不确定环境下的安全自主导航。

Abstract: Autonomous navigation in partially observable environments requires agents to reason beyond immediate sensor input, exploit occlusion, and ensure safety while progressing toward a goal. These challenges arise in many robotics domains, from urban driving and warehouse automation to defense and surveillance. Classical path planning approaches and memoryless reinforcement learning often fail under limited fields of view (FoVs) and occlusions, committing to unsafe or inefficient maneuvers. We propose a hierarchical navigation framework that integrates a Deep Transformer Q-Network (DTQN) as a high-level subgoal selector with a modular low-level controller for waypoint execution. The DTQN consumes short histories of task-aware features, encoding odometry, goal direction, obstacle proximity, and visibility cues, and outputs Q-values to rank candidate subgoals. Visibility-aware candidate generation introduces masking and exposure penalties, rewarding the use of cover and anticipatory safety. A low-level potential field controller then tracks the selected subgoal, ensuring smooth short-horizon obstacle avoidance. We validate our approach in 2D simulation and extend it directly to a 3D Unity-ROS environment by projecting point-cloud perception into the same feature schema, enabling transfer without architectural changes. Results show consistent improvements over classical planners and RL baselines in success rate, safety margins, and time to goal, with ablations confirming the value of temporal memory and visibility-aware candidate design. These findings highlight a generalizable framework for safe navigation under uncertainty, with broad relevance across robotic platforms.

</details>


### [622] [Fast, Robust, Permutation-and-Sign Invariant SO(3) Pattern Alignment](https://arxiv.org/abs/2512.00659)
*Anik Sarker,Alan T. Asbeck*

Main category: cs.RO

TL;DR: 提出一种无需对应关系的SO(3)旋转集对齐方法，通过分解为变换基向量并使用快速鲁棒匹配器，在保持线性复杂度的同时实现高精度、高速度和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在缺乏时间对齐、存在异常值和未知轴约定时难以进行旋转集对齐，需要更高效鲁棒的对应关系无关方法。

Method: 将每个旋转分解为变换基向量（TBVs），在球面上对每轴使用快速鲁棒匹配器（SPMC、FRS及混合）进行对齐，并设计PASI包装器处理轴重标号和符号翻转，通过枚举24种有符号排列并融合估计得到最终旋转。

Result: 方法在EuRoC和ETH数据集上表现出高精度、6-60倍于传统方法的速度，且在高达90%异常值情况下仍保持鲁棒，整体复杂度为O(n)。

Conclusion: 该方法无需对应搜索即可实现快速、准确、鲁棒的旋转集对齐，适用于实际中存在严重异常值和轴模糊性的场景。

Abstract: We address the correspondence-free alignment of two rotation sets on \(SO(3)\), a core task in calibration and registration that is often impeded by missing time alignment, outliers, and unknown axis conventions. Our key idea is to decompose each rotation into its \emph{Transformed Basis Vectors} (TBVs)-three unit vectors on \(S^2\)-and align the resulting spherical point sets per axis using fast, robust matchers (SPMC, FRS, and a hybrid). To handle axis relabels and sign flips, we introduce a \emph{Permutation-and-Sign Invariant} (PASI) wrapper that enumerates the 24 proper signed permutations, scores them via summed correlations, and fuses the per-axis estimates into a single rotation by projection/Karcher mean. The overall complexity remains linear in the number of rotations (\(\mathcal{O}(n)\)), contrasting with \(\mathcal{O}(N_r^3\log N_r)\) for spherical/\(SO(3)\) correlation. Experiments on EuRoC Machine Hall simulations
  (axis-consistent) and the ETH Hand-Eye benchmark (\texttt{robot\_arm\_real})
  (axis-ambiguous) show that our methods are accurate, 6-60x faster than traditional methods, and robust under extreme outlier ratios (up to 90\%), all without correspondence search.

</details>


### [623] [MS-PPO: Morphological-Symmetry-Equivariant Policy for Legged Robot Locomotion](https://arxiv.org/abs/2512.00727)
*Sizhe Wei,Xulin Chen,Fengze Xie,Garrett Ethan Katz,Zhenyu Gan,Lu Gan*

Main category: cs.RO

TL;DR: 本文提出了MS-PPO，一种形态对称等变的策略学习框架，通过将机器人运动学结构和形态对称性嵌入到图神经网络中，提升了四足机器人在复杂运动任务中的训练稳定性、泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在腿部机器人运动控制中忽视了机器人的形态结构和对称性，导致训练效率低且泛化能力差。因此，需要一种能够显式建模形态对称性的策略架构以提升性能。

Method: 提出MS-PPO框架，构建基于机器人形态信息的图神经网络架构，并确保其在形态对称群作用下具有等变性，从而在策略输出上保持对称一致性，同时在价值估计中保持不变性；该方法无需依赖奖励塑形或数据增强来强制对称性。

Result: 在Unitree Go2和Xiaomi CyberDog2机器人上进行仿真与实机实验，涵盖小跑、弹跳、斜坡行走和双足转向等多种任务，结果表明MS-PPO相比现有最先进方法在训练稳定性、对称性泛化和样本效率方面表现更优。

Conclusion: 将机器人的运动学结构和形态对称性作为先验知识嵌入策略网络，为腿部机器人运动控制提供了强大的归纳偏置，是实现高效、通用强化学习控制的有效途径。

Abstract: Reinforcement learning has recently enabled impressive locomotion capabilities on legged robots; however, most policy architectures remain morphology- and symmetry-agnostic, leading to inefficient training and limited generalization. This work introduces MS-PPO, a morphological-symmetry-equivariant policy learning framework that encodes robot kinematic structure and morphological symmetries directly into the policy network. We construct a morphology-informed graph neural architecture that is provably equivariant with respect to the robot's morphological symmetry group actions, ensuring consistent policy responses under symmetric states while maintaining invariance in value estimation. This design eliminates the need for tedious reward shaping or costly data augmentation, which are typically required to enforce symmetry. We evaluate MS-PPO in simulation on Unitree Go2 and Xiaomi CyberDog2 robots across diverse locomotion tasks, including trotting, pronking, slope walking, and bipedal turning, and further deploy the learned policies on hardware. Extensive experiments show that MS-PPO achieves superior training stability, symmetry generalization ability, and sample efficiency in challenging locomotion tasks, compared to state-of-the-art baselines. These findings demonstrate that embedding both kinematic structure and morphological symmetry into policy learning provides a powerful inductive bias for legged robot locomotion control. Our code will be made publicly available at https://lunarlab-gatech.github.io/MS-PPO/.

</details>


### [624] [SAGAS: Semantic-Aware Graph-Assisted Stitching for Offline Temporal Logic Planning](https://arxiv.org/abs/2512.00775)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 本文提出SAGAS框架，在离线、无模型设定下，利用任务无关的碎片化轨迹数据，结合图辅助轨迹拼接与自动机引导规划，实现线性时序逻辑（LTL）约束下的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有LTL控制方法通常依赖精确动力学模型或昂贵的在线交互，难以适用于仅含固定、任务无关碎片轨迹的离线、模型无关场景。

Method: SAGAS框架包含三部分：1）基于学习到的时间距离表征构建潜在可达性图，并引入认证锚节点和概率软标签弥合语义鸿沟；2）将LTL规范转化为Büchi自动机，在隐式乘积空间中搜索代价最小的前缀-后缀计划；3）部署子目标条件化的底层策略执行潜在路点。

Result: 在OGBench运动控制基准上的实验表明，SAGAS能为多种LTL任务合成高效轨迹，有效弥合碎片化离线数据与复杂逻辑约束之间的差距。

Conclusion: SAGAS为离线、模型无关设定下的LTL约束控制提供了可行且高效的新范式，显著降低了对模型精度和在线交互的依赖。

Abstract: Linear Temporal Logic (LTL) provides a rigorous framework for complex robotic tasks, yet existing methods often rely on accurate dynamics models or expensive online interactions. In this work, we address LTL-constrained control in a challenging offline, model-free setting, utilizing only fixed, task-agnostic datasets of fragmented trajectories. We propose SAGAS, a novel framework combining graph-assisted trajectory stitching with automata-guided planning. First, we construct a latent reachability graph from a learned temporal-distance representation. To bridge the semantic gap, we augment this graph with certified anchor nodes and probabilistic soft labels. We then translate the specification into a Büchi automaton and search the implicit product space to derive a cost-minimal prefix-suffix plan. Finally, a subgoal-conditioned low-level policy is deployed to execute these latent waypoints. Experiments on OGBench locomotion domains demonstrate that SAGAS successfully synthesizes efficient trajectories for diverse LTL tasks, effectively bridging the gap between fragmented offline data and complex logical constraints.

</details>


### [625] [Sign Language Recognition using Bidirectional Reservoir Computing](https://arxiv.org/abs/2512.00777)
*Nitin Kumar Singh,Arie Rachmad Syulistyo,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.RO

TL;DR: 本文提出了一种基于MediaPipe和回声状态网络（ESN）的双向储备计算（BRC）架构，用于高效的手语识别（SLR），在WLASL数据集上达到57.71%准确率，训练时间仅9秒，显著优于Bi-GRU方法，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法虽广泛用于手语识别，但计算开销大、资源需求高，难以部署于资源受限的边缘设备，因此需要更轻量高效的替代方案。

Method: 利用MediaPipe提取手部关节点坐标作为输入，送入基于回声状态网络（ESN）的双向储备计算（BRC）架构；BRC分别进行前向与后向时序建模，将二者状态拼接后用于分类。

Result: 在WLASL数据集上取得57.71%的识别准确率，训练时间仅9秒，远低于Bi-GRU的55分38秒。

Conclusion: 所提BRC架构在保持可接受精度的同时极大降低计算开销，是一种适用于边缘设备的高效手语识别方案。

Abstract: Sign language recognition (SLR) facilitates communication between deaf and hearing individuals. Deep learning is widely used to develop SLR-based systems; however, it is computationally intensive and requires substantial computational resources, making it unsuitable for resource-constrained devices. To address this, we propose an efficient sign language recognition system using MediaPipe and an echo state network (ESN)-based bidirectional reservoir computing (BRC) architecture. MediaPipe extracts hand joint coordinates, which serve as inputs to the ESN-based BRC architecture. The BRC processes these features in both forward and backward directions, efficiently capturing temporal dependencies. The resulting states of BRC are concatenated to form a robust representation for classification. We evaluated our method on the Word-Level American Sign Language (WLASL) video dataset, achieving a competitive accuracy of 57.71% and a significantly lower training time of only 9 seconds, in contrast to the 55 minutes and $38$ seconds required by the deep learning-based Bi-GRU approach. Consequently, the BRC-based SLR system is well-suited for edge devices.

</details>


### [626] [Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration](https://arxiv.org/abs/2512.00797)
*Nan Sun,Bo Mao,Yongchang Li,Chenxu Wang,Di Guo,Huaping Liu*

Main category: cs.RO

TL;DR: 本文提出InteractGen，一种基于大语言模型的多智能体框架，将机器人智能分解为多个专用智能体，以解决单一基础模型在服务机器人实际部署中的局限性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在机器人领域存在单一大模型假设与实际服务工作流的分布式、动态特性不匹配的问题，现有视觉-语言模型和视觉-语言-动作策略在具身行动能力、几何定位和人机协作方面存在不足。

Method: 提出InteractGen框架，利用大语言模型驱动多智能体系统，将机器人智能分解为连续感知、依赖感知规划、决策与验证、失败反思和动态人类委托等专用智能体，并将基础模型作为闭环集体中的受控组件。

Result: 在异构机器人团队上部署并进行了为期三个月的开放使用研究，结果表明InteractGen提升了任务成功率、适应性和人机协作效果。

Conclusion: 多智能体协同编排比进一步扩大单一大模型更可行，是实现社会嵌入式服务自主性的有效路径。

Abstract: Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.

</details>


### [627] [A Novel MDP Decomposition Framework for Scalable UAV Mission Planning in Complex and Uncertain Environments](https://arxiv.org/abs/2512.00838)
*Md Muzakkir Quamar,Ali Nasir,Sami ELFerik*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展、容错的无人机（UAV）任务管理框架，通过两阶段分解策略解决大规模马尔可夫决策过程（MDP）的计算瓶颈：先基于领域特征将全局MDP分解为子MDP，再通过优先级重组与元策略融合，理论证明其全局最优性，并在仿真中显著加速计算而不损性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂不确定环境下无人机任务管理中大规模马尔可夫决策过程（MDP）求解的计算瓶颈问题，提升AI决策的实时性与可扩展性。

Method: 提出两阶段分解策略：第一阶段采用基于因子的算法，依据目标优先级、故障状态、空间布局和能量约束等特征将全局MDP分解为目标导向的子MDP；第二阶段采用优先级驱动的重组算法，独立求解各子MDP，并通过元策略进行冲突消解，合成统一全局策略。

Result: 理论证明在轻度概率独立假设下，合成策略等价于全局最优MDP策略；仿真表明计算时间降低数个数量级，同时保持任务可靠性与策略最优性。

Conclusion: 该框架为实时、复杂无人机任务执行提供了可扩展、鲁棒且具备理论保证的决策基础，推动了MDP在人工智能序列决策中的实际应用边界。

Abstract: This paper presents a scalable and fault-tolerant framework for unmanned aerial vehicle (UAV) mission management in complex and uncertain environments. The proposed approach addresses the computational bottleneck inherent in solving large-scale Markov Decision Processes (MDPs) by introducing a two-stage decomposition strategy. In the first stage, a factor-based algorithm partitions the global MDP into smaller, goal-specific sub-MDPs by leveraging domain-specific features such as goal priority, fault states, spatial layout, and energy constraints. In the second stage, a priority-based recombination algorithm solves each sub-MDP independently and integrates the results into a unified global policy using a meta-policy for conflict resolution. Importantly, we present a theoretical analysis showing that, under mild probabilistic independence assumptions, the combined policy is provably equivalent to the optimal global MDP policy. Our work advances artificial intelligence (AI) decision scalability by decomposing large MDPs into tractable subproblems with provable global equivalence. The proposed decomposition framework enhances the scalability of Markov Decision Processes, a cornerstone of sequential decision-making in artificial intelligence, enabling real-time policy updates for complex mission environments. Extensive simulations validate the effectiveness of our method, demonstrating orders-of-magnitude reduction in computation time without sacrificing mission reliability or policy optimality. The proposed framework establishes a practical and robust foundation for scalable decision-making in real-time UAV mission execution.

</details>


### [628] [Magnetic Tactile-Driven Soft Actuator for Intelligent Grasping and Firmness Evaluation](https://arxiv.org/abs/2512.00907)
*Chengjin Du,Federico Bernabei,Zhengyin Du,Sergio Decherchi,Matteo Lo Preti,Lucia Beccai*

Main category: cs.RO

TL;DR: 本文提出了一种名为SoftMag的磁感应软体执行器，集成了触觉感知功能，并通过多物理场仿真和神经网络解耦策略消除执行器形变对传感信号的干扰，实现了高保真触觉感知。基于该执行器构建的双指抓手可实时预测接触力、位置及物体硬度，验证了其在非破坏性质量评估中的潜力。


<details>
  <summary>Details</summary>
Motivation: 软体机器人在操作脆弱物体方面具有优势，但因缺乏集成触觉感知且执行器形变会干扰传感器信号，限制了其应用。本文旨在解决这两个关键问题。

Method: 设计SoftMag执行器，将磁感应与软体执行器结构一体化；建立多物理场仿真框架建模形变与传感的耦合效应；采用基于神经网络的解耦策略消除机械寄生效应；开发双指SoftMag抓手并使用多任务神经网络实现接触力、位置和物体硬度的实时估计。

Result: 实验验证了SoftMag执行器在压痕、准静态和阶跃驱动及疲劳测试中的性能与解耦有效性；双指抓手对杏子硬度的估计与参考测量值具有强相关性（Pearson r > 0.8），证明其可用于非破坏性品质检测。

Conclusion: 集成磁感应、学习型校正与实时推断的SoftMag平台使软体机器人具备适应性抓取和材料特性量化能力，为发展智能、材料感知的软体机器人提供新框架。

Abstract: Soft robots are powerful tools for manipulating delicate objects, yet their adoption is hindered by two gaps: the lack of integrated tactile sensing and sensor signal distortion caused by actuator deformations. This paper addresses these challenges by introducing the SoftMag actuator: a magnetic tactile-sensorized soft actuator. Unlike systems relying on attached sensors or treating sensing and actuation separately, SoftMag unifies them through a shared architecture while confronting the mechanical parasitic effect, where deformations corrupt tactile signals. A multiphysics simulation framework models this coupling, and a neural-network-based decoupling strategy removes the parasitic component, restoring sensing fidelity. Experiments including indentation, quasi-static and step actuation, and fatigue tests validate the actuator's performance and decoupling effectiveness. Building upon this foundation, the system is extended into a two-finger SoftMag gripper, where a multi-task neural network enables real-time prediction of tri-axial contact forces and position. Furthermore, a probing-based strategy estimates object firmness during grasping. Validation on apricots shows a strong correlation (Pearson r over 0.8) between gripper-estimated firmness and reference measurements, confirming the system's capability for non-destructive quality assessment. Results demonstrate that combining integrated magnetic sensing, learning-based correction, and real-time inference enables a soft robotic platform that adapts its grasp and quantifies material properties. The framework offers an approach for advancing sensorized soft actuators toward intelligent, material-aware robotics.

</details>


### [629] [Constant-Time Motion Planning with Manipulation Behaviors](https://arxiv.org/abs/2512.00939)
*Nayesha Gandotra,Itamar Mishani,Maxim Likhachev*

Main category: cs.RO

TL;DR: 本文提出了行为恒定时间运动规划器（B-CTMP），将恒定时间运动规划扩展到包含抓取和插入等操作行为的双步操作任务中，实现了在毫秒级时间内完成避障规划与操作执行，并保证了完整性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的恒定时间运动规划方法未显式包含物体操作所需的操作行为，限制了其在复杂操作任务中的应用。

Method: 提出B-CTMP算法，通过预处理构建紧凑数据结构，支持在固定时间预算内查询从起始状态到行为起始状态的无碰撞路径，并结合具体操作行为（如抓取、插入）完成任务。

Result: 在货架取物和插头插入两个典型任务的仿真中验证了方法有效性，并在真实机器人上进行了实验，结果表明B-CTMP能在几毫秒内完成查询，同时保证规划完整性与任务成功率。

Conclusion: B-CTMP成功统一了避障规划与物体操作，提供了一种具有可证明速度与成功保证的恒定时间操作框架，适用于半结构化环境中的灵巧操作。

Abstract: Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.

</details>


### [630] [H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer](https://arxiv.org/abs/2512.00971)
*Yunfeng Lin,Minghuan Liu,Yufei Xue,Ming Zhou,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为H-Zero的跨人形机器人运动预训练框架，能够通过在有限数量的机器人平台上进行预训练，实现对未见过的人形机器人和直立四足机器人的零样本和少样本迁移控制，显著减少调参需求。


<details>
  <summary>Details</summary>
Motivation: 现有控制器针对特定机器人设计，缺乏通用性，迁移至新平台需大量调参，限制了在多样化人形机器人上的快速部署。

Method: 提出H-Zero预训练框架，在多个不同人形机器人上联合训练一个基础策略，利用共享的运动先验知识实现跨平台迁移，支持零样本和少量微调下的快速适应。

Result: 在仿真中对未见机器人实现了高达81%的完整任务持续时间表现，并可在30分钟内完成对新机器人或直立四足机器人的少样本微调适配。

Conclusion: H-Zero展示了在多样化机器人形态上构建通用运动策略的可行性，为未来开发可迁移、高效的机器人控制器提供了有效路径。

Abstract: The rapid advancement of humanoid robotics has intensified the need for robust and adaptable controllers to enable stable and efficient locomotion across diverse platforms. However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy. We show that pretraining on a limited set of embodiments enables zero-shot and few-shot transfer to novel humanoid robots with minimal fine-tuning. Evaluations show that the pretrained policy maintains up to 81% of the full episode duration on unseen robots in simulation while enabling few-shot transfer to unseen humanoids and upright quadrupeds within 30 minutes of fine-tuning.

</details>


### [631] [FOM-Nav: Frontier-Object Maps for Object Goal Navigation](https://arxiv.org/abs/2512.01009)
*Thomas Chabal,Shizhe Chen,Jean Ponce,Cordelia Schmid*

Main category: cs.RO

TL;DR: 本文提出FOM-Nav框架，结合前沿-物体地图与视觉语言模型，提升具身导航中目标物体搜索的效率与语义理解能力，并在仿真与真实机器人上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有隐式记忆方法难以长期记忆与规划，显式建图方法缺乏丰富语义信息，亟需兼顾空间探索效率与语义理解能力的新方法。

Method: 提出模块化FOM-Nav框架：在线构建Frontier-Object Map（联合编码空间前沿与细粒度物体信息），并利用视觉语言模型进行多模态场景理解与高层目标预测，再由底层规划器生成轨迹；同时自动构建大规模真实扫描环境导航数据集用于训练。

Result: 在MP3D和HM3D基准上达到SOTA性能，尤其在导航效率指标SPL上显著提升，并在真实机器人上验证了有效性。

Conclusion: FOM-Nav成功融合显式空间结构与隐式语义理解，为具身导航提供了高效、可解释且可迁移的新范式。

Abstract: This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.

</details>


### [632] [Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping](https://arxiv.org/abs/2512.01018)
*Adelina Giurea,Stijn Luchie,Dieter Coppens,Jeroen Hoebeke,Eli De Poorter*

Main category: cs.RO

TL;DR: 本文提出了一种基于超宽带（UWB）雷达的无基础设施环境建图与障碍物检测方法，适用于视觉受限环境，具有高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统传感器如摄像头和LiDAR在黑暗、烟雾或反射表面等低可见度环境中性能下降，需要一种不依赖视觉且无需固定锚点的替代感知方案。

Method: 采用机器人搭载的UWB雷达，分析不同材料（金属、混凝土、胶合板）和UWB信道（5和9）对信道冲激响应（CIR）的影响，并提出包含目标识别、滤波和聚类三步的处理流程实现障碍物检测与环境建图。

Result: 该方法有效抑制噪声和多径效应，在信道9上对包括低反射材料（如胶合板）的障碍物检测达到至少82.36%的精确率和89.46%的召回率。

Conclusion: 本研究为开发不依赖视觉特征且无需固定锚节点的UWB-based SLAM系统奠定了基础，适用于动态、无锚点的复杂环境。

Abstract: This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform. Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces. In these visioned-impaired conditions, UWB radar offers a promising alternative. To this end, this work explores the suitability of robot-mounted UWB radar for environmental mapping in dynamic, anchor-free scenarios. The study investigates how different materials (metal, concrete and plywood) and UWB radio channels (5 and 9) influence the Channel Impulse Response (CIR). Furthermore, a processing pipeline is proposed to achieve reliable mapping of detected obstacles, consisting of 3 steps: (i) target identification (based on CIR peak detection), (ii) filtering (based on peak properties, signal-to-noise score, and phase-difference of arrival), and (iii) clustering (based on distance estimation and angle-of-arrival estimation). The proposed approach successfully reduces noise and multipath effects, resulting in an obstacle detection precision of at least 82.36% and a recall of 89.46% on channel 9 even when detecting low-reflective materials such as plywood. This work offers a foundation for further development of UWB-based localisation and mapping (SLAM) systems that do not rely on visual features and, unlike conventional UWB localisation systems, do not require on fixed anchor nodes for triangulation.

</details>


### [633] [CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding](https://arxiv.org/abs/2512.01022)
*Yi-Lin Wei,Haoran Liao,Yuhao Lin,Pengyue Wang,Zhizhao Liang,Guiliang Liu,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: 本文提出了CycleManip框架和首个循环操作基准，通过成本感知采样与多任务学习提升历史感知能力，实现了端到端的周期性操作模仿，适用于多种机器人平台。


<details>
  <summary>Details</summary>
Motivation: 循环操作（如摇瓶、敲钉）在日常生活中至关重要，但现有模仿学习方法难以在指定终止时间内完成，且缺乏相应基准与自动评估工具。

Method: 提出CycleManip端到端模仿框架，采用成本感知采样策略增强历史感知，并引入多任务学习提升历史理解；同时构建首个循环操作基准及自动评估方法。

Result: 在仿真与真实场景中均取得高成功率；具备强泛化性、可插拔性（兼容VLA等策略），并适配双臂夹爪、灵巧手及人形机器人等多种平台。

Conclusion: CycleManip有效解决了周期性操作中的时间控制与历史建模难题，推动了该方向的标准化与实用化发展。

Abstract: In this paper, we explore an important yet underexplored task in robot manipulation: cycle-based manipulation, where robots need to perform cyclic or repetitive actions with an expected terminal time. These tasks are crucial in daily life, such as shaking a bottle or knocking a nail. However, few prior works have explored this task, leading to two main challenges: 1) the imitation methods often fail to complete these tasks within the expected terminal time due to the ineffective utilization of history; 2) the absence of a benchmark with sufficient data and automatic evaluation tools hinders development of effective solutions in this area. To address these challenges, we first propose the CycleManip framework to achieve cycle-based task manipulation in an end-to-end imitation manner without requiring any extra models, hierarchical structure or significant computational overhead. The core insight is to enhance effective history perception by a cost-aware sampling strategy and to improve historical understanding by multi-task learning. Second, we introduce a cycle-based task manipulation benchmark, which provides diverse cycle-based tasks, and an automatic evaluation method. Extensive experiments conducted in both simulation and real-world settings demonstrate that our method achieves high success rates in cycle-based task manipulation. The results further show strong adaptability performance in general manipulation, and the plug-and-play ability on imitation policies such as Vision-Language-Action (VLA) models. Moreover, the results show that our approach can be applied across diverse robotic platforms, including bi-arm grippers, dexterous hands, and humanoid robots.

</details>


### [634] [VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference](https://arxiv.org/abs/2512.01031)
*Jiaming Tang,Yufei Sun,Yilong Zhao,Shang Yang,Yujun Lin,Zhuoyang Zhang,James Hou,Yao Lu,Zhijian Liu,Song Han*

Main category: cs.RO

TL;DR: 提出了一种名为VLASH的通用异步推理框架，用于视觉-语言-动作（VLA）模型，通过前推机器人状态来估计执行时刻的未来状态，实现了平滑、准确且快速响应的控制，无需额外开销或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在实际部署中存在推理延迟和动作不连贯问题，异步推理虽能提升实时性，但会导致预测与执行间的时序错位，引发动作不稳定。

Method: VLASH通过利用先前生成的动作块前推机器人状态，预测执行时的未来环境状态，从而弥合预测与执行之间的时间差距，实现稳定、低延迟的异步控制。

Result: 实验表明，VLASH相比同步推理最高可实现2.03倍的速度提升，反应延迟减少达17.4倍，并保持原始精度；成功应用于乒乓球和打地鼠等高精度快速反应任务。

Conclusion: VLASH是一种高效、通用的VLA异步推理框架，能够在不增加开销或修改模型结构的前提下，显著提升响应速度与控制连续性，推动VLA在真实场景中的应用。

Abstract: Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash

</details>


### [635] [Autonomous Grasping On Quadruped Robot With Task Level Interaction](https://arxiv.org/abs/2512.01052)
*Muhtadin,Mochammad Hilmi Rusydiansyah,Mauridhi Hery Purnomo,I Ketut Eddy Purnama,Chastine Fatichah*

Main category: cs.RO

TL;DR: 本研究开发了一种基于任务级交互的四足机器人自主抓取系统，集成了机械臂与夹爪，通过ROS实现分层控制系统，并采用Web界面进行人机交互。实验结果显示该系统在真实场景中具有较高的任务执行准确性和75%的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的四足机器人多专注于移动性，缺乏操作能力；远程复杂操作的手动控制难度大，因此需要发展具备自主抓取能力的服务型四足机器人。

Method: 在四足机器人上集成机械臂和夹爪，构建基于ROS的分层控制系统，并开发Web界面用于任务级人机交互，结合GraspNet实现自主导航、物体检测与抓取。

Result: 在实际场景测试中，机器人完成了导航、目标选择与抓取任务，12次试验中抓取成功9次，成功率为75%，用户反馈良好。

Conclusion: 所提出的系统显著提升了四足机器人在现实环境中作为服务机器人的操作能力，展示了其在复杂任务中的应用潜力。

Abstract: Quadruped robots are increasingly used in various applications due to their high mobility and ability to operate in diverse terrains. However, most available quadruped robots are primarily focused on mobility without object manipulation capabilities. Equipping a quadruped robot with a robotic arm and gripper introduces a challenge in manual control, especially in remote scenarios that require complex commands. This research aims to develop an autonomous grasping system on a quadruped robot using a task-level interaction approach. The system includes hardware integration of a robotic arm and gripper onto the quadruped robot's body, a layered control system designed using ROS, and a web-based interface for human-robot interaction. The robot is capable of autonomously performing tasks such as navigation, object detection, and grasping using GraspNet. Testing was conducted through real-world scenarios to evaluate navigation, object selection and grasping, and user experience. The results show that the robot can perform tasks accurately and consistently, achieving a grasping success rate of 75 % from 12 trials. Therefore, the system demonstrates significant potential in enhancing the capabilities of quadruped robots as service robots in real-world environments.

</details>


### [636] [Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer](https://arxiv.org/abs/2512.01061)
*Haoru Xue,Tairan He,Zi Wang,Qingwei Ben,Wenli Xiao,Zhengyi Luo,Xingye Da,Fernando Castañeda,Guanya Shi,Shankar Sastry,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种基于教师-学生-自举框架的视觉驱动人形机器人操作方法，利用GPU加速的逼真仿真进行纯RGB感知下的多样化关节物体交互任务，并实现了从仿真到现实的零样本迁移，性能优于人类遥操作。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人学习方法在复杂、多样化的环境中泛化能力有限，尤其是在需要视觉反馈的人形机器人操作任务中。为了克服这一挑战，作者希望通过全仿真训练实现对真实世界任务的零样本迁移，特别是针对高难度的关节物体（如门）操作任务。

Method: 提出一种教师-学生-自举学习框架，结合分阶段重置探索策略以稳定长视野特权策略训练，并采用基于GRPO的微调方法缓解部分可观测性问题，提升闭环一致性。整个策略完全在具有大规模物理和视觉随机化的仿真环境中训练。

Result: 该方法在多种不同类型的门上实现了稳健的零样本仿真到现实迁移，任务完成时间比人类遥操作快最多31.7%，且使用的是纯RGB输入。

Conclusion: 这是首个仅依赖纯RGB感知、在仿真中训练并成功应用于真实世界多样化关节物体操作任务的人形机器人策略，标志着仿真到现实人形机器人控制的重要进展。

Abstract: Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.

</details>


### [637] [Reinforcement Learning for Gliding Projectile Guidance and Control](https://arxiv.org/abs/2512.01066)
*Joel Cahn,Antonin Thomas,Philippe Pastor*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的控制律，旨在提升光学制导滑翔机在动态环境中的自主导航能力，并实现对摄像头检测目标的高精度跟踪与引导。


<details>
  <summary>Details</summary>
Motivation: 提升光学制导滑翔机在动态环境中的自主性和灵活性，拓展强化学习在固定翼飞行器全轴控制中的应用。

Method: 采用强化学习方法设计控制律，并将其应用于光学引导滑翔机，以实现目标检测、跟踪与精确引导。

Result: 验证了强化学习在固定翼飞机全轴控制中的可行性，为滑翔机在动态环境中实现自主目标跟踪提供了新方法。

Conclusion: 强化学习可有效应用于固定翼滑翔机的全轴控制，显著提升其在动态环境下的自主导航与目标跟踪能力。

Abstract: This paper presents the development of a control law, which is intended to be implemented on an optical guided glider. This guiding law follows an innovative approach, the reinforcement learning. This control law is used to make navigation more flexible and autonomous in a dynamic environment. The final objective is to track a target detected with the camera and then guide the glider to this point with high precision. Already applied on quad-copter drones, we wish by this study to demonstrate the applicability of reinforcement learning for fixed-wing aircraft on all of its axis.

</details>


### [638] [Estimation of Kinematic Motion from Dashcam Footage](https://arxiv.org/abs/2512.01104)
*Evelyn Zhang,Alex Richardson,Jonathan Sprinkle*

Main category: cs.RO

TL;DR: 本文探讨了使用车载摄像头视频预测汽车运动学参数（如速度和偏航角）的准确性，并利用车载CAN总线数据作为真值，构建神经网络模型进行预测与评估。


<details>
  <summary>Details</summary>
Motivation: 探索基于普通车载摄像头视频预测车辆实际运动学状态（如速度、yaw角、前车距离与相对速度）的可行性与精度。

Method: 结合车辆CAN总线提供的真值数据与时间同步的车载摄像头视频，训练神经网络模型来预测车辆速度、yaw角及前车状态；并提供基于开源工具和商用硬件的数据采集方案。

Result: 提出了可量化预测精度的神经网络模型，成功实现了对车速、yaw角、前车存在性、相对距离与相对速度的预测；同时验证了该方法在18小时真实驾驶数据上的有效性。

Conclusion: 普通车载摄像头配合轻量级神经网络可在无额外专用传感器条件下，较准确地估计关键车辆运动学与周边态势信息，具备实用潜力与可复现性。

Abstract: The goal of this paper is to explore the accuracy of dashcam footage to predict the actual kinematic motion of a car-like vehicle. Our approach uses ground truth information from the vehicle's on-board data stream, through the controller area network, and a time-synchronized dashboard camera, mounted to a consumer-grade vehicle, for 18 hours of footage and driving. The contributions of the paper include neural network models that allow us to quantify the accuracy of predicting the vehicle speed and yaw, as well as the presence of a lead vehicle, and its relative distance and speed. In addition, the paper describes how other researchers can gather their own data to perform similar experiments, using open-source tools and off-the-shelf technology.

</details>


### [639] [Supporting Productivity Skill Development in College Students through Social Robot Coaching: A Proof-of-Concept](https://arxiv.org/abs/2512.01105)
*Himanshi Lalwani,Hanan Salam*

Main category: cs.RO

TL;DR: 本文提出了一种基于社交辅助机器人（SAR）的教育型生产力教练系统，用于帮助大学生提升时间管理与任务优先级能力，兼具个性化、可扩展性与教育性。


<details>
  <summary>Details</summary>
Motivation: 现有自助书籍缺乏互动性，生产力应用缺乏教育性且可能削弱组织技能发展，而传统人工教练又难以规模化。

Method: 设计并实现一个具备六节时间管理与任务优先级课程的SAR系统，支持聊天交互、语音反馈、集成仪表盘（追踪进度、情绪、参与度、信心、用时）及个性化洞察功能，并对15名大学生开展用户评估。

Result: 系统获得79.2的系统可用性评分（SUS），用户在整体体验和参与度方面给出高分。

Conclusion: SAR作为教育型生产力教练是有效且可扩展的解决方案，有助于提升大学生生产力。

Abstract: College students often face academic challenges that hamper their productivity and well-being. Although self-help books and productivity apps are popular, they often fall short. Books provide generalized, non-interactive guidance, and apps are not inherently educational and can hinder the development of key organizational skills. Traditional productivity coaching offers personalized support, but is resource-intensive and difficult to scale. In this study, we present a proof-of-concept for a socially assistive robot (SAR) as an educational coach and a potential solution to the limitations of existing productivity tools and coaching approaches. The SAR delivers six different lessons on time management and task prioritization. Users interact via a chat interface, while the SAR responds through speech (with a toggle option). An integrated dashboard monitors progress, mood, engagement, confidence per lesson, and time spent per lesson. It also offers personalized productivity insights to foster reflection and self-awareness. We evaluated the system with 15 college students, achieving a System Usability Score of 79.2 and high ratings for overall experience and engagement. Our findings suggest that SAR-based productivity coaching can offer an effective and scalable solution to improve productivity among college students.

</details>


### [640] [Tactile Robotics: Past and Future](https://arxiv.org/abs/2512.01106)
*Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本文回顾了近半个世纪以来触觉机器人技术的发展历程，将其划分为四个阶段，并展望了2025年后触觉机器人在商业应用、类人灵巧操作、人工智能和远程临场等方面的未来前景。


<details>
  <summary>Details</summary>
Motivation: 通过梳理触觉机器人技术的历史发展脉络，整合专家观点，明确该领域长期存在的挑战与停滞原因，为未来研究方向和发展路径提供指导。

Method: 基于近150篇综述文献，采用历史分析法将1965-2024年划分为四个发展阶段，总结各阶段特征，并结合当前新兴主题预测下一代发展趋势。

Result: 识别出电子皮肤、触觉机器人手、基于视觉的触觉感知、软体/仿生触觉和触觉互联网等新兴方向；揭示了该领域反复出现的技术与应用挑战。

Conclusion: 触觉机器人有望在2025年后进入成熟期并实现广泛应用，其未来发展将深刻影响机器人学与人工智能的整体进步。

Abstract: What is the future of tactile robotics? To help define that future, this article provides a historical perspective on tactile sensing in robotics from the wealth of knowledge and expert opinion in nearly 150 reviews over almost half a century. This history is characterized by a succession of generations: 1965-79 (origins), 1980-94 (foundations and growth), 1995-2009 (tactile winter) and 2010-2024 (expansion and diversification). Recent expansion has led to diverse themes emerging of e-skins, tactile robotic hands, vision-based tactile sensing, soft/biomimetic touch, and the tactile Internet. In the next generation from 2025, tactile robotics could mature to widespread commercial use, with applications in human-like dexterity, understanding human intelligence, and telepresence impacting all robotics and AI. By linking past expert insights to present themes, this article highlights recurring challenges in tactile robotics, showing how the field has evolved, why progress has often stalled, and which opportunities are most likely to define its future.

</details>


### [641] [Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception](https://arxiv.org/abs/2512.01108)
*Gabriel Olin,Lu Chen,Nayesha Gandotra,Maxim Likhachev,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种基于树状结构的实时拦截规划框架，用于在传感器噪声下快速拦截运动目标，结合运动基元与自适应卡尔曼滤波实现动态目标信念更新与机器人臂实时响应。


<details>
  <summary>Details</summary>
Motivation: 拦截高速运动目标本身具有严格的时间约束，而传感器噪声进一步导致目标状态信息不完整，形成目标状态分布，需在持续接收观测信息的同时实时规划拦截动作。

Method: 构建一种在状态-时间空间中利用运动学-动力学运动基元生长的树状结构，支持从单一初始状态到达多个目标的可达性编码，并支持目标信念演化过程中的实时价值更新与目标间无缝切换；采用RIAE-AKF进行目标跟踪与信念更新。

Result: 在配备ZED 2i双目相机的ABB IRB-1600六自由度工业机械臂上成功验证了该框架，实现了噪声环境下对快速运动目标的实时、鲁棒拦截。

Conclusion: 所提树状规划结构与自适应估计方法相结合，有效解决了感知不确定性下的实时动态拦截问题，为具身智能系统在开放动态环境中的快速响应提供了可行路径。

Abstract: Intercepting fast moving objects, by its very nature, is challenging because of its tight time constraints. This problem becomes further complicated in the presence of sensor noise because noisy sensors provide, at best, incomplete information, which results in a distribution over target states to be intercepted. Since time is of the essence, to hit the target, the planner must begin directing the interceptor, in this case a robot arm, while still receiving information. We introduce an tree-like structure, which is grown using kinodynamic motion primitives in state-time space. This tree-like structure encodes reachability to multiple goals from a single origin, while enabling real-time value updates as the target belief evolves and seamless transitions between goals. We evaluate our framework on an interception task on a 6 DOF industrial arm (ABB IRB-1600) with an onboard stereo camera (ZED 2i). A robust Innovation-based Adaptive Estimation Adaptive Kalman Filter (RIAE-AKF) is used to track the target and perform belief updates.

</details>


### [642] [Ethically-Aware Participatory Design of a Productivity Social Robot for College Students](https://arxiv.org/abs/2512.01111)
*Himanshi Lalwani,Hanan Salam*

Main category: cs.RO

TL;DR: 本研究通过参与式设计方法，结合大学生和学生成功与健康教练的反馈，探索了社交辅助机器人（SAR）在提升大学生（尤其是ADHD学生）学术生产力中的应用潜力，并提出了以用户为中心的设计原则与伦理指南。


<details>
  <summary>Details</summary>
Motivation: 大学生（尤其ADHD学生）面临执行功能障碍与自我管理困难，传统生产力工具难以持续使用；SAR因其直观交互性在教育与心理健康领域已有成效，但其在学术生产力支持方面尚缺以用户为中心的设计研究。

Method: 采用参与式设计（PD）方法，开展学生与教练的访谈及协作工作坊，收集生产力挑战与SAR功能需求，并从设计初期嵌入伦理考量。

Result: 获得了关于学生生产力障碍、SAR设计偏好（如交互方式、提醒机制、个性化支持等）的深入洞察，并提炼出可操作的机器人特性建议及由利益相关者共同制定的伦理指南。

Conclusion: SAR在支持大学生生产力方面具有可行性和用户接受度，关键在于以参与式设计保障实用性与伦理性，为未来高等教育中负责任的SAR部署提供了理论与实践基础。

Abstract: College students often face academic and life stressors affecting productivity, especially students with Attention Deficit Hyperactivity Disorder (ADHD) who experience executive functioning challenges. Conventional productivity tools typically demand sustained self-discipline and consistent use, which many students struggle with, leading to disruptive app-switching behaviors. Socially Assistive Robots (SARs), known for their intuitive and interactive nature, offer promising potential to support productivity in academic environments, having been successfully utilized in domains like education, cognitive development, and mental health. To leverage SARs effectively in addressing student productivity, this study employed a Participatory Design (PD) approach, directly involving college students and a Student Success and Well-Being Coach in the design process. Through interviews and a collaborative workshop, we gathered detailed insights on productivity challenges and identified desirable features for a productivity-focused SAR. Importantly, ethical considerations were integrated from the onset, facilitating responsible and user-aligned design choices. Our contributions include comprehensive insights into student productivity challenges, SAR design preferences, and actionable recommendations for effective robot characteristics. Additionally, we present stakeholder-derived ethical guidelines to inform responsible future implementations of productivity-focused SARs in higher education.

</details>


### [643] [Real-World Reinforcement Learning of Active Perception Behaviors](https://arxiv.org/abs/2512.01188)
*Edward S. Hu,Jie Wang,Xingfang Yuan,Fiona Luo,Muyao Li,Gaspard Lambrechts,Oleh Rybkin,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: 提出了一种名为非对称优势加权回归（AAWR）的方法，利用训练时的“特权”传感器信息来高效训练主动感知策略，在多种部分可观测操作任务中表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习技术难以在部分可观测环境下生成主动感知行为，而这类行为对获取任务相关状态信息至关重要。

Method: 提出AAWR方法，利用训练时可获得的额外‘特权’传感器数据训练高质量的价值函数，并结合少量演示和粗略策略初始化，引导策略学习主动感知行为。

Result: 在3个机器人上的8个操作任务中验证了AAWR的有效性，其生成的主动感知行为在不同程度的部分可观测条件下均优于先前方法，尤其能提升‘通用’策略在严重部分可观测任务中的表现。

Conclusion: AAWR能高效地从少量演示和特权信息中学习主动感知策略，显著提升机器人在部分可观测环境下的操作性能，具有良好的实用性和扩展潜力。

Abstract: A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to "privileged" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a "generalist" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/

</details>


### [644] [RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments](https://arxiv.org/abs/2512.01194)
*Jaejin Jeon,Seonghoon Ryoo,Sang-Duck Lee,Soomok Lee,Seungwoo Jeong*

Main category: cs.RO

TL;DR: 本文提出了RoboLoc基准数据集，用于无GPS条件下的室内外环境地点识别，特别关注楼层切换与域迁移挑战，并对多种先进LiDAR模型进行了跨域泛化性能评测。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR数据集多集中于室外场景，缺乏室内-室外无缝域迁移支持，难以满足复杂环境下鲁棒定位需求。

Method: 构建了包含真实机器人轨迹、多样高程变化及室内外结构/非结构域过渡的RoboLoc数据集，并在点云、体素和鸟瞰图（BEV）等主流架构上进行跨域地点识别基准评测。

Result: 揭示了当前SOTA模型在室内-室外及楼层切换场景下的泛化能力局限，验证了RoboLoc作为多域定位系统评估平台的有效性。

Conclusion: RoboLoc填补了室内外协同定位数据集的空白，为开发更具鲁棒性和通用性的机器人定位系统提供了重要支撑。

Abstract: Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with fre- quent indoor-outdoor transitions. However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts. In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions. RoboLoc features real-world robot trajectories, diverse elevation profiles, and transitions between structured indoor and unstructured outdoor domains. We benchmark a variety of state-of-the-art models, point-based, voxel-based, and BEV-based architectures, highlighting their generalizability domain shifts. RoboLoc provides a realistic testbed for developing multi-domain localization systems in robotics and autonomous navigation

</details>


### [645] [COMET: A Dual Swashplate Autonomous Coaxial Bi-copter AAV with High-Maneuverability and Long-Endurance](https://arxiv.org/abs/2512.01246)
*Shuai Wang,Xiaoming Tang,Junning Liang,Haowen Zheng,Biyu Ye,Zhaofeng Liu,Fei Gao,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种名为COMET的同轴双旋翼飞行器平台，采用双倾斜盘机制，在效率、紧凑性和操控性之间实现了优化平衡。


<details>
  <summary>Details</summary>
Motivation: 同轴双旋翼飞行器在效率和结构紧凑方面具有潜力，但其设计中难以兼顾效率、机动性和紧凑性，限制了实际应用。

Method: 通过引入双倾斜盘机构，并结合台架测试优化系统效率与紧凑性，利用飞行耐久性实验验证不同负载下的性能，并通过轨迹跟踪测试评估机动性。

Result: 实验结果表明，双倾斜盘配置相比单倾斜盘能提升轨迹跟踪性能和飞行效率，且在多种场景下的自主飞行试验验证了系统的鲁棒性和实用性。

Conclusion: COMET平台在效率、机动性和紧凑性方面取得良好平衡，具备实际应用潜力，适用于复杂飞行任务。

Abstract: Coaxial bi-copter autonomous aerial vehicles (AAVs) have garnered attention due to their potential for improved rotor system efficiency and compact form factor. However, balancing efficiency, maneuverability, and compactness in coaxial bi-copter systems remains a key design challenge, limiting their practical deployment. This letter introduces COMET, a coaxial bi-copter AAV platform featuring a dual swashplate mechanism. The coaxial bi-copter system's efficiency and compactness are optimized through bench tests, and the whole prototype's efficiency and robustness under varying payload conditions are verified through flight endurance experiments. The maneuverability performance of the system is evaluated in comprehensive trajectory tracking tests. The results indicate that the dual swashplate configuration enhances tracking performance and improves flight efficiency compared to the single swashplate alternative. Successful autonomous flight trials across various scenarios verify COMET's potential for real-world applications.

</details>


### [646] [Visibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms](https://arxiv.org/abs/2512.01280)
*Longji Yin,Yunfan Ren,Fangcheng Zhu,Liuyu Shi,Fanze Kong,Benxu Tang,Wenyi Liu,Ximin Lyu,Fu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种去中心化的LiDAR集群目标跟踪框架，通过球面符号距离场（SSDF）建模环境遮挡、统一的视场对齐代价及受静电势启发的三维环绕分布策略，实现复杂环境中鲁棒、可见性优化的协同跟踪。


<details>
  <summary>Details</summary>
Motivation: 弥补集群目标跟踪研究不足，利用集群在分布式感知、容错冗余和多向覆盖方面的优势，解决单无人机跟踪在复杂环境中的可视性与协作瓶颈。

Method: 提出球面符号距离场（SSDF）实时建模3D遮挡；设计支持异构LiDAR的FOV对齐代价；引入静电势启发的3D多向环绕分布度量；构建分层规划器（运动学前端搜索+SE(3)时空后端优化）。

Result: 在杂乱户外环境中成功部署于异构LiDAR无人机集群，实现实时、去中心化、可动态重构的协同跟踪，显著提升对敏捷目标（如无人机、行人）的可见性维持能力与鲁棒性。

Conclusion: 该框架有效融合可见性感知与集群协同机制，为复杂场景下自主、可靠、高可见性的无人机集群跟踪提供了可行且高效的解决方案。

Abstract: Autonomous aerial tracking with drones offers vast potential for surveillance, cinematography, and industrial inspection applications. While single-drone tracking systems have been extensively studied, swarm-based target tracking remains underexplored, despite its unique advantages of distributed perception, fault-tolerant redundancy, and multidirectional target coverage. To bridge this gap, we propose a novel decentralized LiDAR-based swarm tracking framework that enables visibility-aware, cooperative target tracking in complex environments, while fully harnessing the unique capabilities of swarm systems. To address visibility, we introduce a novel Spherical Signed Distance Field (SSDF)-based metric for 3-D environmental occlusion representation, coupled with an efficient algorithm that enables real-time onboard SSDF updating. A general Field-of-View (FOV) alignment cost supporting heterogeneous LiDAR configurations is proposed for consistent target observation. Swarm coordination is enhanced through cooperative costs that enforce inter-robot safe clearance, prevent mutual occlusions, and notably facilitate 3-D multidirectional target encirclement via a novel electrostatic-potential-inspired distribution metric. These innovations are integrated into a hierarchical planner, combining a kinodynamic front-end searcher with a spatiotemporal $SE(3)$ back-end optimizer to generate collision-free, visibility-optimized trajectories.Deployed on heterogeneous LiDAR swarms, our fully decentralized implementation features collaborative perception, distributed planning, and dynamic swarm reconfigurability. Validated through rigorous real-world experiments in cluttered outdoor environments, the proposed system demonstrates robust cooperative tracking of agile targets (drones, humans) while achieving superior visibility maintenance.

</details>


### [647] [Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning](https://arxiv.org/abs/2512.01336)
*Diyuan Shi,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习和课程学习的策略，训练人形机器人在跌倒时自主形成三角结构以减少损伤，并成功迁移到真实平台。


<details>
  <summary>Details</summary>
Motivation: 人形机器人因高重心、大质量、高自由度等特点易跌倒且易造成严重硬件损坏，而现有基于控制的方法难以应对多样化的跌倒场景并可能引入不合适的先验知识。

Method: 采用大规模深度强化学习与课程学习方法，结合精心设计的奖励函数和领域多样化课程，训练人形机器人自主探索跌倒保护行为。

Result: 发现通过形成‘三角’结构可显著降低刚性躯体跌倒时的损伤；在多种指标和实验中验证了性能优越性，并可视化了跌倒行为，最终成功迁移至真实机器人平台。

Conclusion: 该方法为人形机器人跌倒保护提供了数据驱动、自适应的新范式，摆脱了对人工先验的依赖，并具备实际部署潜力。

Abstract: Humanoid robots have received significant research interests and advancements in recent years. Despite many successes, due to their morphology, dynamics and limitation of control policy, humanoid robots are prone to fall as compared to other embodiments like quadruped or wheeled robots. And its large weight, tall Center of Mass, high Degree-of-Freedom would cause serious hardware damages when falling uncontrolled, to both itself and surrounding objects. Existing researches in this field mostly focus on using control based methods that struggle to cater diverse falling scenarios and may introduce unsuitable human prior. On the other hand, large-scale Deep Reinforcement Learning and Curriculum Learning could be employed to incentivize humanoid agent discovering falling protection policy that fits its own nature and property. In this work, with carefully designed reward functions and domain diversification curriculum, we successfully train humanoid agent to explore falling protection behaviors and discover that by forming a `triangle' structure, the falling damages could be significantly reduced with its rigid-material body. With comprehensive metrics and experiments, we quantify its performance with comparison to other methods, visualize its falling behaviors and successfully transfer it to real world platform.

</details>


### [648] [Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1](https://arxiv.org/abs/2512.01358)
*Junsung Park,Hogun Kee,Songhwai Oh*

Main category: cs.RO

TL;DR: 本文提出了一种模态增强的微调框架，用于将基础机器人策略适配到不同人形机器人本体（如GR1和Unitree G1），通过引入接触信号、深度信息等后处理模态或高质量多模态数据，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 基础机器人策略难以直接泛化至不同硬件本体，需针对特定传感器与执行器特性进行有效适配。

Method: 提出模态增强微调框架：在GR1上使用公开数据并添加二值接触信号和ZoeDepth生成的度量深度；在Unitree G1上构建包含cuRobo运动规划、逆运动学及真实接触力的新多模态数据集，并进行针对性微调。

Result: GR1上在线成功率从51%提升至63%；G1‘Pick Apple to Bowl’任务成功率达94%，远超标准微调（48%）和零样本迁移（0%）。

Conclusion: 轻量级后处理模态可有效增强GR1策略，而高质量多模态数据对Unitree G1的可靠迁移至关重要；本工作确立了以模态设计与多模态微调为核心的统一数据驱动适配路径。

Abstract: This paper presents a modality-augmented fine-tuning framework designed to adapt foundation robot policies to diverse humanoid embodiments. We validate our approach across two distinct settings: (i) the GR1 embodiment, utilizing public datasets where we introduce post-processed modalities, including binary contact signals and ZoeDepth-generated metric depth; and (ii) the Unitree G1 embodiment, for which we contribute a novel multi-modal dataset incorporating cuRobo motion planning, inverse kinematics, and ground-truth contact-force measurements. Our experiments demonstrate that modality augmentation consistently enhances policy performance across different embodiments. Specifically, for the GR1, integrating contact-state cues and RGB-D fusion improves online success rates from 51% to 63%. Furthermore, in the G1 "Pick Apple to Bowl" task, our contact-augmented model achieves a success rate of 94%, significantly outperforming the 48% achieved by standard fine-tuning and the 0% baseline of zero-shot transfer. These results highlight that lightweight post-processing effectively strengthens policies for GR1, while high-quality multi-modal data is crucial for reliable transfer to the Unitree G1. Consequently, this work establishes a unified, data-centric pathway for extending foundation robot policies through targeted modality design and multi-modal fine-tuning.

</details>


### [649] [$\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering](https://arxiv.org/abs/2512.01446)
*Jiayi Li,Yuxuan Hu,Haoran Geng,Xiangyu Chen,Chuhao Zhou,Ziteng Cui,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出了一种名为M$^3$A的可变材质操作增强框架，利用光传输特性对单个真实世界演示进行光度重渲染，生成具有不同材质属性的多样化训练数据，从而提升机器人在多材质物体上的操作泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于仿真环境中的材料模拟或大量真实世界数据收集，难以有效应对透明或反射表面带来的分布外变化，且成本高、覆盖有限。

Method: 基于计算摄影技术，利用材料的光传输特性进行光度重渲染，在仅需单次真实演示的情况下生成多样化的高质量训练样本。

Result: 实验表明，M$^3$A显著提升了跨材质泛化性能，在三个真实世界任务中平均成功率提高了58.03%，并对未见过的材料表现出强鲁棒性。

Conclusion: M$^3$A通过解耦操作技能与表面外观，为材料泛化提供了一种高效、低成本的解决方案，并建立了首个涵盖仿真与真实环境的多材质操作基准。

Abstract: Material generalization is essential for real-world robotic manipulation, where robots must interact with objects exhibiting diverse visual and physical properties. This challenge is particularly pronounced for objects made of glass, metal, or other materials whose transparent or reflective surfaces introduce severe out-of-distribution variations. Existing approaches either rely on simulated materials in simulators and perform sim-to-real transfer, which is hindered by substantial visual domain gaps, or depend on collecting extensive real-world demonstrations, which is costly, time-consuming, and still insufficient to cover various materials. To overcome these limitations, we resort to computational photography and introduce Mutable Material Manipulation Augmentation (M$^3$A), a unified framework that leverages the physical characteristics of materials as captured by light transport for photometric re-rendering. The core idea is simple yet powerful: given a single real-world demonstration, we photometrically re-render the scene to generate a diverse set of highly realistic demonstrations with different material properties. This augmentation effectively decouples task-specific manipulation skills from surface appearance, enabling policies to generalize across materials without additional data collection. To systematically evaluate this capability, we construct the first comprehensive multi-material manipulation benchmark spanning both simulation and real-world environments. Extensive experiments show that the M$^3$A policy significantly enhances cross-material generalization, improving the average success rate across three real-world tasks by 58.03\%, and demonstrating robust performance on previously unseen materials.

</details>


### [650] [NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction](https://arxiv.org/abs/2512.01550)
*Fei Liu,Shichao Xie,Minghua Luo,Zedong Chu,Junjun Hu,Xiaolong Wu,Mu Xu*

Main category: cs.RO

TL;DR: 提出NavForesee，一种融合高级语言规划与预测性世界模型想象的视觉语言模型，用于解决复杂自然语言指令下的长视野具身导航问题。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航代理在面对复杂自然语言指令和未知环境时，缺乏有效的长期规划能力，导致任务失败率高。

Method: 设计一个统一的视觉语言模型（VLM），同时进行基于指令和历史观测的任务分解、进度跟踪、子目标生成（语言规划），以及短期环境动态和长期导航里程碑的预测（世界模型想象）。

Result: 在R2R-CE和RxR-CE基准上实现了极具竞争力的性能，验证了该方法在复杂场景下的有效性。

Conclusion: 将显式语言规划与隐式时空预测相结合，能显著提升具身导航代理的智能水平和任务能力，为未来研究提供了新方向。

Abstract: Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.

</details>


### [651] [L2M-Calib: One-key Calibration Method for LiDAR and Multiple Magnetic Sensors](https://arxiv.org/abs/2512.01554)
*Qiyang Lyu,Wei Wang,Zhenyu Wu,Hongming Shen,Huiqin Zhou,Danwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为L2M-Calib的一键式磁-LiDAR融合系统标定框架，联合估计传感器间外参和磁传感器的内参畸变，通过高斯-牛顿迭代与加权岭正则化总体最小二乘法实现鲁棒标定。


<details>
  <summary>Details</summary>
Motivation: 磁传感器具有对几何、纹理、光照和天气不敏感的环境磁场（AMF）感知能力，适合恶劣环境，但因缺乏有效标定方法，其在多模态系统中的应用仍被忽视。

Method: 采用迭代高斯-牛顿法优化外参，并将内参标定建模为加权岭正则化总体最小二乘（w-RRTLS）问题，以提升抗噪声与病态数据能力。

Result: 在仿真与真实AGV平台实验中均验证了该方法在多种环境与工况下具备高标定精度与鲁棒性。

Conclusion: L2M-Calib为磁-LiDAR多模态融合提供了可靠、易用且鲁棒的一键标定解决方案，推动了磁传感在复杂环境感知中的实用化。

Abstract: Multimodal sensor fusion enables robust environmental perception by leveraging complementary information from heterogeneous sensing modalities. However, accurate calibration is a critical prerequisite for effective fusion. This paper proposes a novel one-key calibration framework named L2M-Calib for a fused magnetic-LiDAR system, jointly estimating the extrinsic transformation between the two kinds of sensors and the intrinsic distortion parameters of the magnetic sensors. Magnetic sensors capture ambient magnetic field (AMF) patterns, which are invariant to geometry, texture, illumination, and weather, making them suitable for challenging environments. Nonetheless, the integration of magnetic sensing into multimodal systems remains underexplored due to the absence of effective calibration techniques. To address this, we optimize extrinsic parameters using an iterative Gauss-Newton scheme, coupled with the intrinsic calibration as a weighted ridge-regularized total least squares (w-RRTLS) problem, ensuring robustness against measurement noise and ill-conditioned data. Extensive evaluations on both simulated datasets and real-world experiments, including AGV-mounted sensor configurations, demonstrate that our method achieves high calibration accuracy and robustness under various environmental and operational conditions.

</details>


### [652] [A Cross-Embodiment Gripper Benchmark for Rigid-Object Manipulation in Aerial and Industrial Robotics](https://arxiv.org/abs/2512.01598)
*Marek Vagas,Martin Varga,Jaroslav Romancik,Ondrej Majercak,Alejandro Suarez,Anibal Ollero,Bram Vanderborght,Ivan Virgala*

Main category: cs.RO

TL;DR: 本文提出跨形态夹持器基准（CEGB），扩展了YCB和NIST基准，新增转移时间、能耗及任务导向的有效载荷评估三项指标，以支持夹持器在异构机器人平台间的可迁移性与能效评估；通过轻量自锁夹持器原型验证，展示了快速换装、低能耗保持与高成功率抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有YCB和NIST基准仅限单一平台，无法评估夹持器在不同机器人形态（如工业、协作、空中）间的可迁移性与能量效率，而现代移动与空中操作亟需此类能力。

Method: 构建CEGB基准，包含三部分新指标：（1）夹持器形态更换耗时；（2）抓取与持握能耗；（3）面向任务意图的理想有效载荷评估；并以轻量自锁夹持器为参考原型开展实验验证。

Result: 实验表明：形态更换中位时间约17.6秒；持握能耗约1.5 J/10 s；循环时间3.2–3.9秒，抓取成功率超90%。

Conclusion: CEGB为夹持器在跨平台、尤其是空中与移动操作场景下的可复现、能效感知评估提供了系统化基准框架。

Abstract: Robotic grippers are increasingly deployed across industrial, collaborative, and aerial platforms, where each embodiment imposes distinct mechanical, energetic, and operational constraints. Established YCB and NIST benchmarks quantify grasp success, force, or timing on a single platform, but do not evaluate cross-embodiment transferability or energy-aware performance, capabilities essential for modern mobile and aerial manipulation. This letter introduces the Cross-Embodiment Gripper Benchmark (CEGB), a compact and reproducible benchmarking suite extending YCB and selected NIST metrics with three additional components: a transfer-time benchmark measuring the practical effort required to exchange embodiments, an energy-consumption benchmark evaluating grasping and holding efficiency, and an intent-specific ideal payload assessment reflecting design-dependent operational capability. Together, these metrics characterize both grasp performance and the suitability of reusing a single gripper across heterogeneous robotic systems. A lightweight self-locking gripper prototype is implemented as a reference case. Experiments demonstrate rapid embodiment transfer (median ~= 17.6 s across user groups), low holding energy for gripper prototype (~= 1.5 J per 10 s), and consistent grasp performance with cycle times of 3.2 - 3.9 s and success rates exceeding 90%. CEGB thus provides a reproducible foundation for cross-platform, energy-aware evaluation of grippers in aerial and manipulators domains.

</details>


### [653] [Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track](https://arxiv.org/abs/2512.01608)
*Mo Chen*

Main category: cs.RO

TL;DR: 提出了一种用于非完整差速驱动移动机器人的实时自主轨迹导航框架，结合多任务视觉感知与稳定性保证的跟踪控制器，无需高精地图或GPS即可实现稳定可靠的导航。


<details>
  <summary>Details</summary>
Motivation: 为了在没有高精地图和全球定位系统的情况下实现非完整移动机器人在动态环境中的可靠自主导航。

Method: 通过2D到3D相机投影重建车道中心线，采用基于弧长的均匀点重采样和鲁棒QR最小二乘优化的三次多项式拟合；控制器基于Lyapunov稳定性理论设计，调节机器人的线速度和角速度。

Result: 实现了实时性、轨迹平滑性和闭环稳定性，在真实嵌入式平台上验证了系统在部分可观和动态场景下的有效性。

Conclusion: 该框架能够在无HD地图和卫星定位条件下，实现对位置和航向偏差的渐近收敛，具备实际部署价值。

Abstract: This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller. The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization. The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization. Real-world experiments on embedded platforms verify system fidelity, real-time execution, trajectory smoothness, and closed-loop stability for reliable autonomous navigation.

</details>


### [654] [Dynamic Log-Gaussian Process Control Barrier Function for Safe Robotic Navigation in Dynamic Environments](https://arxiv.org/abs/2512.01668)
*Xin Yin,Chenyang Liang,Yanning Guo,Jie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯过程的动态对数高斯过程控制屏障函数（DLGP-CBF），用于在未知动态环境中实时构建空间信息丰富且能响应障碍物运动的CBF，显著提升了避障性能。


<details>
  <summary>Details</summary>
Motivation: 现有CBF方法难以利用实时传感器数据在线合成信息丰富且感知障碍物运动的屏障函数，尤其在未知和动态场景中。

Method: 提出DLGP-CBF：1）采用高斯过程回归的对数变换，生成稀疏数据区域下仍平滑、信息丰富的屏障值与梯度；2）将DLGP-CBF显式建模为障碍物位置的函数，使安全约束融合障碍物速度预测，实现对动态障碍的主动响应。

Result: 仿真表明，相比基线方法，DLGP-CBF显著提升了安全裕度、轨迹平滑性和对动态障碍的响应能力。

Conclusion: DLGP-CBF为动态未知环境下的实时安全导航提供了一种有效、可扩展的新范式。

Abstract: Control Barrier Functions (CBFs) have emerged as efficient tools to address the safe navigation problem for robot applications. However, synthesizing informative and obstacle motion-aware CBFs online using real-time sensor data remains challenging, particularly in unknown and dynamic scenarios. Motived by this challenge, this paper aims to propose a novel Gaussian Process-based formulation of CBF, termed the Dynamic Log Gaussian Process Control Barrier Function (DLGP-CBF), to enable real-time construction of CBF which are both spatially informative and responsive to obstacle motion. Firstly, the DLGP-CBF leverages a logarithmic transformation of GP regression to generate smooth and informative barrier values and gradients, even in sparse-data regions. Secondly, by explicitly modeling the DLGP-CBF as a function of obstacle positions, the derived safety constraint integrates predicted obstacle velocities, allowing the controller to proactively respond to dynamic obstacles' motion. Simulation results demonstrate significant improvements in obstacle avoidance performance, including increased safety margins, smoother trajectories, and enhanced responsiveness compared to baseline methods.

</details>


### [655] [DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models](https://arxiv.org/abs/2512.01715)
*Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Yicheng Feng,Sipeng Zheng,Qin Jin,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出DiG-Flow框架，通过几何正则化提升视觉-语言-动作（VLA）模型在分布偏移和多步任务下的鲁棒性，核心是利用观测与动作嵌入间分布差异作为几何信号进行表征级调制，不改变原有流匹配结构，理论可证且实验有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于流匹配的VLA模型在分布偏移和复杂多步任务下性能下降，表明其学习的表征未能稳健捕捉任务相关语义。

Method: 提出DiG-Flow框架：计算观测与动作嵌入经验分布间的差异度量，经单调函数映射为调制权重，对观测嵌入施加残差更新；该操作仅作用于表征层，不修改流匹配路径或目标向量场，并提供理论收敛性与目标函数下降保证。

Result: DiG-Flow以极小开销集成到现有VLA架构中，在多步任务和小样本场景下性能显著提升，实证验证了其鲁棒性增强效果。

Conclusion: 几何差异引导的表征调制是一种有效且理论可靠的VLA鲁棒性增强范式，兼顾实用性与可解释性。

Abstract: Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.

</details>


### [656] [AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields](https://arxiv.org/abs/2512.01753)
*Zhihao Zhan,Yuhang Ming,Shaobin Li,Jie Yuan*

Main category: cs.RO

TL;DR: 本文提出了AgriLiRa4D，一个面向农业无人机的多模态、高精度、真实场景SLAM数据集，涵盖多种农田地形与作业模式，并提供FINS_RTK真值轨迹及同步的LiDAR、4D雷达、IMU数据，用于评估多传感器SLAM算法在低纹理、重复结构、动态植被等挑战下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有面向农业无人机的多模态真实世界SLAM数据集严重缺乏，难以支撑鲁棒自主导航研究。

Method: 构建了包含六组飞行序列（三种地形×两种作业模式）的AgriLiRa4D数据集，配备FINS_RTK高精度真值、同步多传感器数据（3D LiDAR、4D Radar、IMU）及完整标定参数；并基于该数据集对四种前沿多传感器SLAM算法进行跨模态组合基准测试。

Result: 验证了农业环境中单模态SLAM性能受限，凸显多模态融合的必要性；揭示了各算法在低纹理作物、重复图案和动态植被等典型农业挑战下的性能瓶颈。

Conclusion: AgriLiRa4D填补了农业SLAM领域高质量真实数据集的空白，为算法开发与鲁棒性评估提供了可靠基准，推动农业无人机自主导航技术发展。

Abstract: Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types-flat, hilly, and terraced-and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations. Leveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization. By filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs. The dataset can be downloaded from: https://zhan994.github.io/AgriLiRa4D.

</details>


### [657] [IGen: Scalable Data Generation for Robot Learning from Open-World Images](https://arxiv.org/abs/2512.01773)
*Chenghao Gu,Haolan Kang,Junchao Lin,Jinghe Wang,Duo Wu,Shuzhao Xie,Fanding Huang,Junchen Ge,Ziyang Gong,Letian Li,Hongying Zheng,Changwei Lv,Zhi Wang*

Main category: cs.RO

TL;DR: 本文提出IGen框架，利用开放世界图像生成高质量的视觉-运动数据，用于训练通用机器人策略，显著降低对真实机器人数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略训练需要大量数据，但真实机器人数据采集成本高且受限于环境；开放世界图像虽丰富多样，却缺乏对应的机器人动作标签，导致其难以被有效利用。

Method: IGen将2D开放图像转化为结构化3D场景表示，结合视觉语言模型将任务指令转化为高层计划，并生成SE(3)末端位姿序列作为低层动作，进而合成动态、时序一致的视觉观测。

Result: 实验表明IGen生成的视觉-运动数据质量高，仅用其训练的机器人策略性能可媲美使用真实数据训练的策略。

Conclusion: IGen为从开放世界图像中规模化生成机器人训练数据提供了可行路径，有望推动通用机器人策略的高效、低成本训练。

Abstract: The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.

</details>


### [658] [GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2512.01801)
*Yunfei Li,Xiao Ma,Jiafeng Xu,Yu Cui,Zhongren Cui,Zhigang Han,Liqun Huang,Tao Kong,Yuxiao Liu,Hao Niu,Wanli Peng,Jingchao Qiao,Zeyu Ren,Haixin Shi,Zhi Su,Jiawen Tian,Yuyang Xiao,Shenyu Zhang,Liwei Zheng,Hang Li,Yonghui Wu*

Main category: cs.RO

TL;DR: GR-RL是一个将通用视觉-语言-动作（VLA）策略转化为擅长长视野灵巧操作的专用机器人的框架，通过多阶段训练 pipeline 利用强化学习过滤、增强和优化人类演示，在系鞋带等高精度任务中实现83.3%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA策略假设人类示范是最优的，但在高精度灵巧操作中，人类示范往往是噪声大且次优的，因此需要一种能从次优示范中学习出高性能策略的方法。

Method: 提出GR-RL多阶段训练流程：1）利用离线强化学习学习视觉-语言条件下的任务进度函数，并用Q值作为鲁棒进度指标过滤演示轨迹；2）引入形态对称性数据增强提升泛化能力；3）通过在线强化学习训练潜在空间噪声预测器，使VLA策略在部署时更适应高精度控制。

Result: GR-RL首次实现了基于学习的机器人自主完成系鞋带任务（穿过多个鞋孔），达到83.3%的成功率，展现出在长视野、毫米级精度和软体交互任务中的卓越性能。

Conclusion: GR-RL为将通用机器人基础模型专业化为可靠的现实世界专家提供了有效路径，推动了灵巧操作中从次优人类示范学习的发展。

Abstract: We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.

</details>


### [659] [Much Ado About Noising: Dispelling the Myths of Generative Robotic Control](https://arxiv.org/abs/2512.01809)
*Chaoyi Pan,Giri Anantharaman,Nai-Chieh Huang,Claire Jin,Daniel Pfrommer,Chenyang Yuan,Frank Permenter,Guannan Qu,Nicholas Boffi,Guanya Shi,Max Simchowitz*

Main category: cs.RO

TL;DR: 本文通过在行为克隆基准上全面评估生成式控制策略（GCPs），发现其性能优势主要源于迭代计算机制（需中间步骤监督与适当随机性），而非传统认为的多模态建模能力；并提出轻量级两步回归策略MIP，性能媲美流模型GCPs。


<details>
  <summary>Details</summary>
Motivation: 探究生成式控制策略（如流模型、扩散模型）在机器人策略参数化中表现优异的根本原因，澄清关于其成功源于多模态建模或复杂映射能力的猜测。

Method: 在标准行为克隆基准上系统评估主流GCPs；分析其多模态建模能力、迭代结构、监督方式与随机性的影响；设计并验证最小迭代策略（MIP）——一种仅含两步回归的轻量迭代策略。

Result: GCPs的性能优势不依赖于多模态建模或更复杂的观测-动作映射，而关键在于受监督的迭代计算过程；MIP在多数任务中达到与流模型GCPs相当的性能，且常优于蒸馏后的快捷模型。

Conclusion: 生成式控制策略中分布拟合成分的重要性被高估；应转向以控制性能为核心的新策略设计范式，强调迭代结构与监督机制而非生成建模本身。

Abstract: Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/

</details>


### [660] [Is Image-based Object Pose Estimation Ready to Support Grasping?](https://arxiv.org/abs/2512.01856)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: 本文提出了一种评估仅需单张RGB图像输入的6自由度实例级物体位姿估计器的框架，并通过物理仿真中的抓取实验，验证其作为机器人抓取唯一感知机制的可行性。


<details>
  <summary>Details</summary>
Motivation: 评估现有RGB单图输入的6-DoF位姿估计器的实际抓取能力，填补仅依赖精度指标（如BOP）而缺乏下游任务验证的文献空白。

Method: 构建基于物理仿真的抓取评估框架，使用五种开源6-DoF位姿估计器在BOP数据子集上输出位姿，驱动平行夹爪和欠驱动机械手执行抓取任务。

Result: 实验揭示了不同估计器在真实抓取任务中的性能差异，发现高精度位姿估计并不总能直接转化为高成功率抓取，暴露了现有评估指标与实际应用间的差距。

Conclusion: 单纯依赖标准位姿精度指标不足以评估位姿估计器对机器人操作的实用性；需结合下游任务（如抓取）进行联合评估。

Abstract: We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.

</details>


### [661] [NeuroHJR: Hamilton-Jacobi Reachability-based Obstacle Avoidance in Complex Environments with Physics-Informed Neural Networks](https://arxiv.org/abs/2512.01897)
*Granthik Halder,Rudrashis Majumder,Rakshith M R,Rahi Shah,Suresh Sundaram*

Main category: cs.RO

TL;DR: 本文提出了一种名为NeuroHJR的新框架，利用物理信息神经网络（PINNs）近似Hamilton-Jacobi Reachability（HJR）解，实现复杂环境中自主地面车辆的实时避障。该方法避免了传统基于网格离散化的高计算成本，能够在连续状态空间中高效估计可达集，同时保持与经典HJR求解器相当的安全性性能。


<details>
  <summary>Details</summary>
Motivation: 传统的HJR方法虽然能提供形式化的安全保证，但由于其在障碍物众多的环境中可扩展性差、计算复杂度高，难以应用于实时避障。因此需要一种既能保障安全性又具备良好计算效率的方法。

Method: 提出NeuroHJR框架，将系统动力学和安全约束嵌入到物理信息神经网络（PINN）的损失函数中，通过端到端训练直接学习HJR偏微分方程的解，从而在连续状态空间中近似前向和后向可达集，无需网格离散化。

Result: 在密集障碍物环境下的仿真结果表明，NeuroHJR在安全性方面表现与经典HJR求解器相当，同时显著降低了计算开销，实现了更高效的实时避障性能。

Conclusion: NeuroHJR为基于可达性的避障方法提供了可扩展且高效的解决方案，推动了其在机器人系统中的实时部署应用。

Abstract: Autonomous ground vehicles (AGVs) must navigate safely in cluttered environments while accounting for complex dynamics and environmental uncertainty. Hamilton-Jacobi Reachability (HJR) offers formal safety guarantees through the computation of forward and backward reachable sets, but its application is hindered by poor scalability in environments with numerous obstacles. In this paper, we present a novel framework called NeuroHJR that leverages Physics-Informed Neural Networks (PINNs) to approximate the HJR solution for real-time obstacle avoidance. By embedding system dynamics and safety constraints directly into the neural network loss function, our method bypasses the need for grid-based discretization and enables efficient estimation of reachable sets in continuous state spaces. We demonstrate the effectiveness of our approach through simulation results in densely cluttered scenarios, showing that it achieves safety performance comparable to that of classical HJR solvers while significantly reducing the computational cost. This work provides a new step toward real-time, scalable deployment of reachability-based obstacle avoidance in robotics.

</details>


### [662] [Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model](https://arxiv.org/abs/2512.01924)
*Kentaro Fujii,Shingo Murata*

Main category: cs.RO

TL;DR: 提出了一种新的深度主动推理框架，通过多时间尺度的世界模型和抽象动作表示，在真实机器人操作任务中实现了高效的目标导向与探索行为的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习控制方法在不确定性环境下忽视探索行为，且计算成本高，难以有效处理复杂环境。

Method: 构建包含世界模型、动作模型和抽象世界模型的框架：世界模型编码快慢时间尺度的状态，动作模型通过向量量化压缩动作序列，抽象世界模型基于抽象动作预测未来慢状态以实现低成本决策。

Result: 在真实机器人操作任务中取得高成功率，能够在不确定环境中切换目标导向与探索行为，并显著降低动作选择的计算开销。

Conclusion: 建模多时间尺度动态和抽象动作与状态转移对提升机器人在不确定环境中的适应性和效率至关重要。

Abstract: Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.

</details>


### [663] [Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models](https://arxiv.org/abs/2512.01946)
*Paul Pacaud,Ricardo Garcia,Shizhe Chen,Cordelia Schmid*

Main category: cs.RO

TL;DR: 本文提出了一种自动合成机器人失败数据的方法，通过扰动成功轨迹生成多样化失败样本，并构建了三个新基准数据集；基于此数据训练的多视角VLM模型Guardian在失败检测任务中达到SOTA性能，并显著提升了真实机器人操作成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在机器人失败检测中受限于失败数据稀缺，导致准确率与泛化能力不足。

Method: 提出一种程序化扰动成功轨迹以自动生成规划与执行失败的方法，生成二分类标签、细粒度失败类别及逐步推理轨迹；构建RLENch-Fail、BridgeDataV2-Fail和UR5-Fail三个新失败检测基准；训练多视角图像输入的VLM模型Guardian用于失败检测与推理。

Result: Guardian在新旧基准上均达到SOTA性能；集成到操作系统后，在仿真与真实机器人中均显著提升任务成功率。

Conclusion: 自动合成失败数据能有效缓解数据稀缺问题，所构建的基准与Guardian模型为鲁棒机器人操作中的失败检测与恢复提供了实用且可扩展的解决方案。

Abstract: Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data.

</details>


### [664] [RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies](https://arxiv.org/abs/2512.01993)
*Guillermo Garcia-Cobo,Maximilian Igl,Peter Karkus,Zhejun Zhang,Michael Watson,Yuxiao Chen,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出了一种名为RoaD（Rollouts as Demonstrations）的新方法，通过将策略自身的闭环rollout作为额外训练数据，并结合专家引导生成高质量、现实的轨迹，以缓解行为克隆中常见的协变量偏移问题。该方法在大规模交通仿真和高保真端到端驾驶模拟中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶策略通常采用开环行为克隆训练，但在闭环部署时易因协变量偏移导致误差累积。

Method: RoaD利用策略自身闭环rollout生成训练数据，并在rollout过程中引入专家指导以提升轨迹质量与现实性，用于后续监督微调。

Result: 在WOSAC基准上性能媲美或优于先前闭环监督微调方法；在AlpaSim中驾驶得分提升41%，碰撞减少54%。

Conclusion: RoaD是一种简单高效、数据需求低、无需强假设的闭环适应方法，适用于包括端到端驾驶在内的更广泛场景。

Abstract: Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\% and reduces collisions by 54\%.

</details>


### [665] [Learning Sim-to-Real Humanoid Locomotion in 15 Minutes](https://arxiv.org/abs/2512.01996)
*Younggyo Seo,Carmelo Sferrazza,Juyue Chen,Guanya Shi,Rocky Duan,Pieter Abbeel*

Main category: cs.RO

TL;DR: 本文提出了一种基于离线策略RL算法（FastSAC和FastTD3）的简单实用方法，可在单块RTX 4090 GPU上仅用15分钟完成人形机器人运动控制策略的训练，并在强域随机化条件下实现快速、稳定的仿真到真实迁移。


<details>
  <summary>Details</summary>
Motivation: 人形机器人仿真到真实强化学习仍面临高维状态空间和域随机化带来的稳定性与可靠性挑战。

Method: 提出FastSAC和FastTD3两种改进型离线策略RL算法，结合大规模并行仿真（数千环境）、精调设计选择及极简奖励函数，提升训练稳定性和速度。

Result: 在Unitree G1和Booster T1机器人上实现了15分钟内端到端人形运动策略训练；支持强域随机化（动力学、崎岖地形、外力扰动）及全身人体动作跟踪策略快速训练。

Conclusion: 通过简化算法设计与工程优化，可显著降低人形机器人强化学习的训练门槛与时间成本，推动sim-to-real RL在实际机器人系统中的落地应用。

Abstract: Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.

</details>


### [666] [LLM-Driven Corrective Robot Operation Code Generation with Static Text-Based Simulation](https://arxiv.org/abs/2512.02002)
*Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan*

Main category: cs.RO

TL;DR: 本文提出一种基于大语言模型（LLM）的静态文本仿真方法，用于替代物理实验或定制仿真环境，以提升LLM生成机器人操作代码的可靠性；该框架将LLM配置为具备动作解析、状态推理、结果分析和语义观测生成能力的静态仿真器，并在无人机与地面小车等任务上验证了其高精度与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的机器人操作代码纠错框架依赖物理实验或定制仿真环境，存在配置成本高、执行耗时长等问题，亟需更轻量、普适的仿真替代方案。

Method: 将LLM本身作为增强型静态仿真器，通过提示工程赋予其动作解析、状态转移推理、执行结果分析及语义观测生成能力，构建无需动态执行的LLM驱动纠错生成框架。

Result: 在多种机器人（如UAV、小型地面车辆）操作任务上的实验表明，所提静态文本仿真准确率高，纠错框架性能媲美SOTA方法，且完全规避了物理实验或仿真器依赖。

Conclusion: LLM可被有效配置为可靠的静态仿真器，支撑高效、低成本、免执行的机器人操作代码生成与纠错，为LLM驱动机器人提供了新范式。

Abstract: Recent advances in Large language models (LLMs) have demonstrated their promising capabilities of generating robot operation code to enable LLM-driven robots. To enhance the reliability of operation code generated by LLMs, corrective designs with feedback from the observation of executing code have been increasingly adopted in existing research. However, the code execution in these designs relies on either a physical experiment or a customized simulation environment, which limits their deployment due to the high configuration effort of the environment and the potential long execution time. In this paper, we explore the possibility of directly leveraging LLM to enable static simulation of robot operation code, and then leverage it to design a new reliable LLM-driven corrective robot operation code generation framework. Our framework configures the LLM as a static simulator with enhanced capabilities that reliably simulate robot code execution by interpreting actions, reasoning over state transitions, analyzing execution outcomes, and generating se- mantic observations that accurately capture trajectory dynamics. To validate the performance of our framework, we performed experiments on various operation tasks for different robots, including UAVs and small ground vehicles. The experiment results not only demonstrated the high accuracy of our static text-based simulation but also the reliable code generation of our LLM-driven corrective framework, which achieves a comparable performance with state-of-the-art research while does not rely on dynamic code execution using physical experiments or simulators.

</details>


### [667] [Learning Dexterous Manipulation Skills from Imperfect Simulations](https://arxiv.org/abs/2512.02011)
*Elvis Hsieh,Wen-Han Hsieh,Yen-Jen Wang,Toru Lin,Jitendra Malik,Koushil Sreenath,Haozhi Qi*

Main category: cs.RO

TL;DR: 本文提出了一种名为DEXSCREW的sim-to-real框架，通过三阶段方法（仿真策略训练、真实世界遥操作数据收集、结合触觉感知的行为克隆）解决复杂接触动力学与多模态传感（尤其是触觉）模拟难的问题，在螺母-螺栓装配和螺丝刀操作任务中展现出对未见几何形状和外部扰动的良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与sim-to-real迁移在灵巧操作中受限于复杂接触动力学和多模态信号（特别是触觉反馈）难以高保真仿真。

Method: 提出三阶段DEXSCREW框架：1）在简化物体模型仿真中训练RL策略以习得手指步态；2）将该策略作为技能原语嵌入遥操作系统，采集含触觉与本体感知的真实演示数据；3）基于该数据训练融合触觉输入的行为克隆策略。

Result: 在螺母-螺栓装配和螺丝刀操作任务中，相比直接sim-to-real迁移，任务完成率更高；且对未见物体形状和外部扰动表现出强鲁棒性。

Conclusion: 通过分阶段利用仿真可扩展性与真实世界多模态数据，DEXSCREW有效弥合了仿真与现实间的感知与动力学鸿沟，为灵巧操作的sim-to-real提供了可行路径。

Abstract: Reinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose \ours, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulation using simplified object models that lead to the emergence of correct finger gaits. We then use the learned policy as a skill primitive within a teleoperation system to collect real-world demonstrations that contain tactile and proprioceptive information. Finally, we train a behavior cloning policy that incorporates tactile sensing and show that it generalizes to nuts and screwdrivers with diverse geometries. Experiments across both tasks show high task progress ratios compared to direct sim-to-real transfer and robust performance even on unseen object shapes and under external perturbations. Videos and code are available on https://dexscrew.github.io.

</details>


### [668] [ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation](https://arxiv.org/abs/2512.02013)
*Chenyang Gu,Jiaming Liu,Hao Chen,Runzhong Huang,Qingpo Wuwu,Zhuoyang Liu,Xiaoqi Li,Ying Li,Renrui Zhang,Peng Jia,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了ManualVLA框架，通过引入规划专家生成多模态操作手册，并结合Manual Chain-of-Thought推理机制，实现从目标状态到可执行动作的高效映射，在LEGO组装与物体重排任务中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理需明确定义目标状态的长周期任务（如LEGO组装、物体重排）时，难以协调高层规划与精细操控。

Method: 提出基于Mixture-of-Transformers（MoT）的ManualVLA框架，包含规划专家（生成图像+位置提示+文本指令的多模态手册）和动作专家；引入Manual Chain-of-Thought（ManualCoT）推理机制，利用手册显式控制条件与隐式表征指导动作；构建基于3D Gaussian Splatting的数字孪生工具链自动生成手册数据。

Result: ManualVLA在LEGO组装和物体重排任务中，真实世界平均成功率比当前分层SOTA基线高32%。

Conclusion: ManualVLA通过解耦规划与执行、引入多模态手册与ManualCoT推理，有效 bridged 'what' 与 'how' 的鸿沟，提升了VLA模型在长周期目标导向任务中的泛化与可控性。

Abstract: Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the "how" process from the "what" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.

</details>


### [669] [EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI](https://arxiv.org/abs/2512.02020)
*Jianlei Chang,Ruofeng Mei,Wei Ke,Xiangyu Xu*

Main category: cs.RO

TL;DR: 本文提出了EfficientFlow，一种基于流的具身智能体策略学习统一框架，通过引入等变性提升数据效率，并设计加速度正则化策略加速采样，实现在低数据量下高性能且快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有生成式策略在具身智能任务中存在数据效率低和采样速度慢的问题，需要大量演示数据且推理耗时长。

Method: 提出EfficientFlow框架：1）引入等变性到流匹配中，理论证明在各向同性高斯先验和等变速度预测网络下，动作分布保持等变性；2）设计加速度正则化策略，推导出仅使用条件轨迹即可稳定训练的代理损失函数。

Result: 在多个机器人操作基准上，EfficientFlow在有限数据下达到竞争性或更优性能，并显著加快推理速度。

Conclusion: EfficientFlow是一种高效、高性能的具身智能体策略学习范式，兼顾数据效率与推理速度。

Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.

</details>
