<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 103]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.RO](#cs.RO) [Total: 29]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.LG](#cs.LG) [Total: 56]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Complex-Valued 2D Gaussian Representation for Computer-Generated Holography](https://arxiv.org/abs/2511.15022)
*Yicheng Zhan,Xiangjun Gao,Long Quan,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出基于结构化复值2D高斯基元的全息图表示方法，显著降低参数搜索空间和VRAM使用，提升重建质量与优化速度。


<details>
  <summary>Details</summary>
Motivation: 现有全息图表示方法存储效率低、参数空间大，限制了计算机生成全息技术的可扩展性和重建质量。

Method: 引入结构化复值2D高斯基元表示全息图，设计可微光栅化器并结合GPU优化的自由空间光传播核，实现端到端训练，并提出适配实际全息格式的转换方法。

Result: 相比现有方法，VRAM使用最多降低2.5倍，优化速度提升50%，重建保真度更高，且能有效抑制噪声伪影。

Conclusion: 该表示方法通过缩小参数搜索空间，提升了全息图估计的可扩展性，为下一代计算机生成全息系统提供了高效、高质量的解决方案。

Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.

</details>


### [2] [Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video](https://arxiv.org/abs/2511.14848)
*Yarin Bekor,Gal Michael Harari,Or Perel,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种名为Gaussian See, Gaussian Do的新方法，用于从多视角视频中实现语义3D运动迁移，支持无需绑定、跨类别的对象间运动迁移。


<details>
  <summary>Details</summary>
Motivation: 实现无需rig的跨类别语义一致的3D运动迁移，解决现有方法在结构一致性和运动保真度上的不足。

Method: 通过条件反转从源视频提取运动嵌入，将其应用于静态目标形状的渲染帧，并利用生成的视频监督动态3D高斯点阵重建；引入基于锚点的视图感知运动嵌入机制和鲁棒的4D重建流程。

Result: 在首个语义3D运动迁移基准上验证了方法的有效性，相比基线方法在运动保真度和结构一致性方面表现更优。

Conclusion: 所提方法能有效实现高质量的语义3D运动迁移，具有良好的跨类别泛化能力和应用潜力。

Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/

</details>


### [3] [When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation](https://arxiv.org/abs/2511.14860)
*Aashish Ghimire,Jun Zeng,Roshan Paudel,Nikhil Kumar Tomar,Deepak Ranjan Nayak,Harshith Reddy Nalla,Vivek Jha,Glenda Reynolds,Debesh Jha*

Main category: cs.CV

TL;DR: 本研究首次系统评估了卷积神经网络、视觉Transformer和状态空间Mamba架构在全景牙片龋齿分割中的性能，发现尽管复杂模型具有理论优势，但基于CNN的DoubleU-Net表现最佳，表明在特定医学图像分割任务中，架构与任务的匹配比模型复杂度更重要。


<details>
  <summary>Details</summary>
Motivation: 由于病灶对比度低、形态变异大以及标注数据有限，全自动龋齿分割仍具挑战性，亟需系统评估不同先进架构在该任务上的表现以指导模型选择。

Method: 在统一配置下训练了12种前沿模型（包括VMUnet、MambaUNet、DoubleU-Net等），并在新构建的DC1000数据集上进行基准测试，比较其在龋齿分割任务中的性能。

Result: 基于CNN的DoubleU-Net取得了最高的Dice系数（0.7345）、mIoU（0.5978）和精确率（0.8145），前三名模型均为CNN架构；而Mamba和Transformer类方法因数据量限制和空间先验较弱表现不佳。

Conclusion: 在数据有限的特定医学图像分割任务中，简单且具有强空间先验的CNN架构优于复杂的注意力机制模型，强调应优先考虑架构与任务的适配性而非一味追求模型复杂度。

Abstract: Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.

</details>


### [4] [B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?](https://arxiv.org/abs/2511.14870)
*Fuyang Zhang,Pradeep Kumar Jayaraman,Xiang Xu,Yasutaka Furukawa*

Main category: cs.CV

TL;DR: 提出了一种基于体积距离函数的CAD边界表示新方法BR-DF，可无失败地转换为水密B-Rep模型，并结合多分支隐扩散模型实现高质量CAD生成，成功率达到100%。


<details>
  <summary>Details</summary>
Motivation: 传统CAD生成方法在生成边界表示（B-Rep）时存在拓扑错误和失败率高的问题，亟需一种鲁棒且能保持几何与拓扑完整性的表示方法。

Method: 提出B-Rep Distance Functions（BR-DF），将CAD模型的表面网格编码为符号距离函数（SDF），并将顶点、边、面及其拓扑信息编码为每面的无符号距离函数（UDF）；采用改进的Marching Cubes算法将BR-DF直接转换为水密的面片B-Rep模型；设计基于3D U-Net的多分支潜在扩散模型联合生成SDF和UDFs。

Result: 所提方法在CAD生成性能上达到与当前最先进方法相当的水平，并实现了前所未有的100%成功生成（面片）B-Rep模型。

Conclusion: BR-DF是一种鲁棒且有效的CAD几何表示方法，能够保证B-Rep模型的成功重建，结合扩散模型展示了在生成建模中的巨大潜力。

Abstract: This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.

</details>


### [5] [GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis](https://arxiv.org/abs/2511.14884)
*Antonio Ruiz,Tao Wu,Andrew Melnik,Qing Cheng,Xuqin Wang,Lu Liu,Yongliang Wang,Yanfeng Zhang,Helge Ritter*

Main category: cs.CV

TL;DR: 本文提出了一种名为GeoSceneGraph的方法，能够从文本提示中生成具有图结构和几何对称性的3D室内场景，无需依赖预定义的关系类别或真实关系标注，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成3D室内场景时，要么忽略场景的图结构，要么依赖用户提供的语义图或真实关系标注，限制了灵活性和现实性；同时，小型专用模型对于资源受限设备仍有必要。

Method: 提出GeoSceneGraph，基于等变图神经网络（EGNN），利用3D场景的图结构和几何对称性，通过一种简单有效的方式将文本特征作为条件输入EGNN，无需使用真实关系标注。

Result: 尽管不依赖真实关系标注，GeoSceneGraph的表现与依赖此类标注的方法相当，并通过消融实验验证了设计的有效性。

Conclusion: GeoSceneGraph为在资源受限设备上实现高质量、结构合理的文本到3D室内场景生成提供了一个高效且可行的解决方案。

Abstract: Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.

</details>


### [6] [HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation](https://arxiv.org/abs/2511.14897)
*Pranav Indrakanti,Ivor Simpson*

Main category: cs.CV

TL;DR: 提出了一种无监督的单幅MRI双向合成方法，基于物理机制实现高场与超低场MRI之间的相互转换。


<details>
  <summary>Details</summary>
Motivation: 现有MRI合成模型缺乏对高低场MRI对比度变化物理机制的考虑，导致合成效果受限。

Method: 通过建立前向模型模拟高场到超低场的转换，利用组织类型信噪比估计目标对比度；采用隐式神经表示（INR）网络进行超分辨率重建，同时预测组织分割和图像强度。

Result: 在合成的超低场类图像中WM-GM对比度提升52%，在64mT实际图像中提升37%；敏感性实验显示模型对噪声、对比度变化和初始种子具有鲁棒性。

Conclusion: 该方法通过结合物理驱动的前向模型与深度学习，实现了高质量的无监督双向MRI合成，具有良好的应用潜力。

Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.

</details>


### [7] [InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization](https://arxiv.org/abs/2511.14899)
*Daniel Gilo,Or Litany*

Main category: cs.CV

TL;DR: 提出I-Mix2Mix框架，将2D扩散模型的编辑能力蒸馏到预训练的多视角扩散模型中，提升稀疏输入下的多视角一致性编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏输入视图下进行多视角图像编辑时易产生伪影和不一致问题，难以保持跨视角的一致性。

Method: 提出InstructMix2Mix（I-Mix2Mix），用多视角扩散模型替代SDS中的传统神经场整合器，并引入增量学生更新、专用教师噪声调度和注意力修改以增强跨视角一致性。

Result: 实验表明，I-Mix2Mix在保持高单帧编辑质量的同时，显著提升了多视角一致性。

Conclusion: I-Mix2Mix通过知识蒸馏和改进的训练策略，有效解决了稀疏输入下多视角图像编辑的挑战，优于基于神经场或时序注意力的现有方法。

Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

</details>


### [8] [Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis](https://arxiv.org/abs/2511.14900)
*Zehao Liu,Wejieying Ren,Jipeng Zhang,Tianxiang Zhao,Jingxi Zhu,Xiaoting Li,Vasant G. Honavar*

Main category: cs.CV

TL;DR: 提出SkinR1，一种结合教科书推理与强化学习的新型皮肤病学视觉-语言模型，通过分层感知的差异诊断轨迹生成和有监督微调，提升诊断准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在皮肤病诊断中受限于数据异质性、缺乏可靠的诊断推理依据以及泛化能力不足，难以满足临床可信度需求。

Method: 设计基于教科书的推理生成器以构建层次化、差异诊断感知的推理路径；利用这些路径进行有监督微调（SFT）赋予模型扎实的推理基础；提出一种融合疾病层级结构的新型强化学习范式，将推理模式迁移到大规模稀疏标注数据上。

Result: 在多个皮肤病数据集上的实验表明，SkinR1在诊断准确率上优于现有方法，消融研究验证了SFT所建立的推理基础对性能的关键作用。

Conclusion: SkinR1通过整合专家知识驱动的推理监督与强化学习，有效解决了数据异构性和泛化难题，显著提升了皮肤病临床推理的可靠性与可扩展性。

Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.

</details>


### [9] [FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding](https://arxiv.org/abs/2511.14901)
*Zhenshi Li,Weikang Yu,Dilxat Muhtar,Xueliang Zhang,Pengfeng Xiao,Pedram Ghamisi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种面向遥感领域的细粒度视觉-语言预训练框架FarSLIP，通过构建首个多粒度遥感图像-文本数据集MGRS-200k，并采用patch-to-patch蒸馏和CLS token-based区域对齐策略，解决了现有CLIP模型在遥感图像中空间感知不足的问题，在多种任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP模型及其遥感变体在细粒度对齐方面表现有限，主要由于缺乏有效的对象级监督以及直接应用通用领域方法导致语义连贯性下降，因此需要专门针对遥感图像的多粒度对齐方法。

Method: 构建了包含丰富对象级文本监督的MGRS-200k数据集；提出FarSLIP框架，采用patch-to-patch蒸馏以保持语义一致性，并通过CLS token进行区域-类别对齐来增强空间感知能力。

Result: FarSLIP在遥感开放词汇语义分割、零样本分类和图像-文本检索等多个任务上均达到最先进水平，显著优于现有方法。

Conclusion: FarSLIP有效提升了遥感图像中细粒度视觉-语言对齐能力，兼顾局部判别性和全局语义一致性，为遥感领域多模态学习提供了新基准。

Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.

</details>


### [10] [nnMIL: A generalizable multiple instance learning framework for computational pathology](https://arxiv.org/abs/2511.14907)
*Xiangde Luo,Jinxi Xiang,Yuanfeng Ji,Ruijiang Li*

Main category: cs.CV

TL;DR: nnMIL是一种简单且通用的多实例学习框架，用于将补丁级病理基础模型转化为可靠的滑片级临床推理，具有良好的泛化性和不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有的聚合补丁级特征到滑片级预测的方法受限于设计缺陷，影响了泛化性和可靠性。

Method: 提出nnMIL框架，引入补丁和特征层面的随机采样，结合轻量级聚合器进行滑窗推理，实现高效训练和不确定性估计。

Result: 在40,000张全切片图像和35项临床任务中，nnMIL持续优于现有MIL方法，并表现出强跨模型泛化、可靠不确定性量化和稳健的生存分层能力。

Conclusion: nnMIL为将病理基础模型转化为临床有意义的预测提供了实用且可推广的解决方案，推动了可靠AI系统在实际场景中的发展与部署。

Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.

</details>


### [11] [X-WIN: Building Chest Radiograph World Model via Predictive Sensing](https://arxiv.org/abs/2511.14918)
*Zefan Yang,Ge Wang,James Hendler,Mannudeep K. Kalra,Pingkun Yan*

Main category: cs.CV

TL;DR: 提出了一种名为X-WIN的胸部X光（CXR）世界模型，通过从CT体积数据中提取三维解剖知识来增强CXR的表征学习和疾病诊断能力。


<details>
  <summary>Details</summary>
Motivation: 由于CXR是二维投影图像，存在结构重叠问题，难以捕捉三维解剖结构，限制了其在表征学习和疾病诊断中的性能。

Method: X-WIN利用胸部CT的体积知识，在潜在空间中学习预测其2D投影；引入亲和力引导的对比对齐损失以捕捉同一体积不同投影间的相关性，并结合掩码图像建模和域分类器提升模型适应性和真实与合成CXR表示的一致性。

Result: X-WIN在多种下游任务中优于现有的基础模型，支持线性探测和少样本微调，并能通过渲染2D投影重建3D CT体积。

Conclusion: X-WIN通过融合三维解剖先验有效提升了CXR的表征能力，在诊断性能和三维重建方面均表现出优势。

Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.

</details>


### [12] [CPSL: Representing Volumetric Video via Content-Promoted Scene Layers](https://arxiv.org/abs/2511.14927)
*Kaiyuan Hu,Yili Jin,Junhua Liu,Xize Duan,Hong Kang,Xue Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Content-Promoted Scene Layers（CPSL）的紧凑2.5D视频表示方法，通过深度和显著性引导将帧分解为几何一致的图层，实现高质量、低开销的视差合成，支持实时播放和可扩展的沉浸式媒体应用。


<details>
  <summary>Details</summary>
Motivation: 现有体视频表示在捕捉、计算和渲染方面成本高昂，限制了其在按需视频和实时通信中的可扩展性和可行性。

Method: 基于每帧的深度和内容显著性，将视频帧分解为少量几何一致的图层，配备软alpha带和边缘深度缓存，利用深度加权扭曲和前向-后向alpha融合进行新视角合成，并通过运动引导传播和逐层编码保持时序一致性。

Result: 在多个基准上，CPSL在感知质量和边界保真度方面优于基于图层和神经场的方法，同时将存储和渲染成本降低数倍。

Conclusion: CPSL为从2D视频向可扩展的2.5D沉浸式媒体转变提供了一条实用路径。

Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.

</details>


### [13] [Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities](https://arxiv.org/abs/2511.14945)
*Fan Yang,Quanting Xie,Atsunori Moteki,Shoichi Masui,Shan Jiang,Yonatan Bisk,Graham Neubig*

Main category: cs.CV

TL;DR: 本文提出了一个包含580个多模态人类活动序列的新基准，用于研究具有低对比度模式的长期周期性工作流，并设计了三个与实际应用对齐的评估任务。同时提出了一种轻量级、无需训练的基线方法，在多个任务上优于现有方法，且在实际部署中展现出与传统监督方法相当的优势，无需标注和重新训练。


<details>
  <summary>Details</summary>
Motivation: 长期周期性工作流具有复杂的结构和低对比度模式，现有方法难以有效处理，且缺乏公开的基准数据集来推动相关研究。

Method: 构建了一个包含580个长周期、多模态人类活动序列的基准数据集，支持无监督周期检测、任务完成跟踪和程序异常检测三类任务；提出一种轻量、无需训练的基线模型来捕捉多样化的周期性工作流模式。

Result: 实验表明该基准对现有无监督方法和基于大语言模型的零样本方法均构成挑战；所提基线在所有任务上显著优于对比方法；在真实场景中具备良好的部署优势，无需标注和再训练即可应用。

Conclusion: 该工作填补了长期周期性工作流分析的研究空白，提供了有价值的基准资源和有效的基线方法，推动了无需监督的周期性行为建模在实际场景中的应用。

Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.

</details>


### [14] [RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems](https://arxiv.org/abs/2511.14948)
*Jaro Meyer,Frédéric Giraud,Joschua Wüthrich,Marc Pollefeys,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出一种基于LED Clock的低成本、通用多摄像头同步方法，可在可见光和红外相机系统中实现毫秒级时间对齐，优于现有视觉、音频和时间码方法，并在大规模手术记录中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在异构相机系统（如专业与消费级设备、可见光与红外传感器）中缺乏硬件同步能力的情况下，实现在非受控真实环境中多视角视频流的高精度时空对齐。

Method: 设计并构建一个自定义的LED Clock，利用红光和红外LED编码时间信息，通过从视频帧中视觉解码曝光窗口的起止时间，实现跨模态（RGB与IR）的毫秒级时间同步。

Result: 与硬件同步相比，达到1.34ms的均方根误差（RMSE），在多个实验中优于光信号、音频和时间码同步方法，并显著提升多视角姿态估计与3D重建等下游任务性能；在超过25个异构相机的大规模手术数据上完成验证。

Conclusion: 该方法简化了多相机同步流程，提升了在工业与临床等复杂、非受限环境下的视觉感知可行性与可及性。

Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.

</details>


### [15] [Artificial intelligence approaches for energy-efficient laser cutting machines](https://arxiv.org/abs/2511.14952)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashour,Ahmed Elshenawy*

Main category: cs.CV

TL;DR: 提出基于深度学习的自适应控制方法，通过材料分类和烟雾检测动态调节激光切割中吸尘泵的功率，实现20%至50%的节能。


<details>
  <summary>Details</summary>
Motivation: 激光切割中吸尘泵通常为开环控制，能耗高且不环保，缺乏根据材料和烟雾情况自适应调节的能力。

Method: 采用闭-loop控制，结合无透镜散斑传感与定制CNN、USB相机与VGG16迁移学习进行材料分类，并使用独立的深度学习模型检测烟雾水平，动态调整吸尘泵功率。

Result: 系统能自动在非工作时段关闭吸尘泵并在切割时动态调功，实验显示吸尘泵能耗降低了20%到50%。

Conclusion: 所提出的深度学习驱动的闭环控制系统显著降低了激光切割过程中的能耗，有助于制造业的可持续发展。

Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.

</details>


### [16] [EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects](https://arxiv.org/abs/2511.14970)
*Gbenga Omotara,Ramy Farag,Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.CV

TL;DR: 本文提出了一种用于透明物体感知的边缘引导空间注意力（EGSA）融合机制和多模态渐进式训练策略，有效缓解了语义与几何特征融合中的负向任务干扰，在深度估计和语义分割任务中均提升了性能，尤其在透明区域表现突出。


<details>
  <summary>Details</summary>
Motivation: 透明物体的感知在计算机视觉中具有挑战性，因为透明性会干扰深度估计和语义分割。现有的多任务学习方法常因任务间的负向交互而受限，因此需要一种能有效融合语义与几何信息并减少干扰的机制。

Method: 提出边缘引导空间注意力（EGSA）机制，利用边界信息指导语义与几何特征的融合；并设计一种多模态渐进式训练策略，先从RGB图像提取边缘进行学习，再过渡到由预测深度图生成的边缘，无需真实深度标签进行训练。

Result: 在Syn-TODD和ClearPose两个基准上，EGSA在深度估计精度上优于当前最先进的MODEST方法，同时保持了有竞争力的分割性能，尤其在透明区域提升显著。

Conclusion: 边缘引导的特征融合结合渐进式训练策略，是一种提升透明物体感知鲁棒性的有效方法，为多任务学习中的特征融合提供了新思路。

Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

</details>


### [17] [Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation](https://arxiv.org/abs/2511.14981)
*Nicholas Cooper,Lijun Chen,Sailesh Dwivedy,Danna Gurari*

Main category: cs.CV

TL;DR: 提出了一种仅使用基于特征的损失、而不使用基于logits的损失的知识蒸馏框架，并引入知识质量度量来选择最优教师层，显著提升了学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法通常结合logits和中间层特征进行训练，但logits可能限制特征迁移的效果，因此希望探索仅使用特征损失的有效性。

Method: 提出一种仅基于特征损失的知识蒸馏框架，利用潜在表示的几何特性设计知识质量度量，用于选择最有效的教师层进行知识迁移。

Result: 在三个图像分类数据集上，使用四种不同的学生-教师模型组合（包括CNN和视觉Transformer）进行了实验，结果表明该方法相比传统方法最高可提升15%的top-1准确率，达到当前最优水平。

Conclusion: 仅使用特征损失进行知识蒸馏是可行且高效的，结合知识质量评估能进一步提升性能，为知识蒸馏提供了新的有效范式。

Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.

</details>


### [18] [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/abs/2511.14993)
*Vladimir Arkhipkin,Vladimir Korviakov,Nikolai Gerasimenko,Denis Parkhomenko,Viacheslav Vasilev,Alexey Letunovskiy,Maria Kovaleva,Nikolai Vaulin,Ivan Kirillov,Lev Novitskiy,Denis Koposov,Nikita Kiselev,Alexander Varlamov,Dmitrii Mikhailov,Vladimir Polovnikov,Andrey Shutkin,Ilya Vasiliev,Julia Agafonova,Anastasiia Kargapoltseva,Anna Dmitrienko,Anastasia Maltseva,Anna Averchenkova,Olga Kim,Tatiana Nikulina,Denis Dimitrov*

Main category: cs.CV

TL;DR: Kandinsky 5.0 是一个先进的生成式模型系列，支持高分辨率图像和10秒视频合成，包含三种模型：Image Lite、Video Lite 和 Video Pro，具备高效训练、优化架构和公开代码，推动高质量生成模型的发展。


<details>
  <summary>Details</summary>
Motivation: 为了提升高分辨率图像和短时视频生成的质量与效率，同时增强模型的可访问性和适应性，满足多样化的生成任务需求。

Method: 采用多阶段训练流程，包括数据收集、处理、过滤和聚类，并结合自监督微调（SFT）和基于强化学习（RL）的后训练；提出新的架构、训练和推理优化技术。

Result: Kandinsky 5.0 在多种生成任务中实现了最先进的性能和高速生成，通过人类评估验证了其优越性，尤其在视频生成质量上表现突出。

Conclusion: Kandinsky 5.0 是一个大规模、开源的生成框架，充分释放预训练和后续训练阶段的潜力，适用于广泛的应用场景，有助于推动生成模型的研究与普及。

Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.

</details>


### [19] [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2511.15435)
*Linyin Luo,Yujuan Ding,Yunshan Ma,Wenqi Fan,Hanjiang Lai*

Main category: cs.CV

TL;DR: 提出了一种针对多模态检索增强生成（MRAG）系统的层次化视觉攻击方法，通过在图像输入中添加微小扰动，破坏跨模态对齐，导致检索和生成性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注知识投毒攻击，而本文探讨一种新的攻击场景：仅通过对用户输入的图像添加不可察觉的扰动来攻击MRAG系统，无需篡改其他组件。

Method: 提出层次化视觉攻击，采用两阶段策略：首先破坏跨模态对齐，使检索器召回无关知识；然后干扰多模态语义对齐，实现生成器输入的错配。使用CLIP-based检索器和BLIP-2、LLaVA等LMM进行实验。

Result: 在OK-VQA和InfoSeek两个数据集上的实验表明，该攻击显著降低了检索与生成性能，验证了MRAG系统在视觉输入扰动下的脆弱性。

Conclusion: MRAG系统易受隐蔽的视觉攻击影响，凸显了在实际应用中加强多模态输入鲁棒性的必要性。

Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.

</details>


### [20] [FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation](https://arxiv.org/abs/2511.14998)
*Yueru He,Xueqing Peng,Yupeng Cao,Yan Wang,Lingfei Qian,Haohang Li,Yi Han,Ruoyu Xiang,Mingquan Lin,Prayag Tiwari,Jimin Huang,Guojun Xiong,Sophia Ananiadou*

Main category: cs.CV

TL;DR: FinCriticalED是一个针对金融文档的视觉基准，用于在事实层面评估OCR和视觉语言模型，强调数值和时间信息的准确性，而非传统的表面文本相似性指标。


<details>
  <summary>Details</summary>
Motivation: 金融文档中微小的OCR错误可能导致重大误解，而传统指标无法捕捉这些关键错误，因此需要一个专注于事实正确性的评估基准。

Method: 构建包含500个图像-HTML对的数据集，由金融专家标注并验证超过七百个数值和时间事实，并设计基于大语言模型评判的结构化事实提取与上下文验证流程。

Result: 评估结果显示即使最强的专有模型在复杂视觉情境下仍存在显著的事实性错误，尤其是在数值和时间理解方面。

Conclusion: FinCriticalED为金融及其他高精度要求领域提供了推进视觉事实准确性的严谨基础。

Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.

</details>


### [21] [CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification](https://arxiv.org/abs/2511.15016)
*Zhenyu Cui,Jiahuan Zhou,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种跨模态知识解耦与对齐方法CKDA，用于可见光-红外终身行人重识别，有效缓解了模态特定知识与共性知识之间的相互干扰和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可见光-红外终身行人重识别中忽略了模态特定知识获取与模态共性知识防遗忘之间的相互干扰，导致协同遗忘问题。

Method: 提出了CKDA方法，包含模态共性提示（MCP）和模态特定提示（MSP）模块以解耦并纯化不同模态的判别信息，并设计了跨模态知识对齐（CKA）模块，在双模态原型基础上于互异且独立的跨模态和模内特征空间中对新旧知识进行对齐。

Result: 在四个基准数据集上的大量实验验证了CKDA方法的有效性和优越性，优于现有的最先进方法。

Conclusion: CKDA通过显式分离和平衡保留模态共性和模态特定知识，有效解决了VI-LReID中的协同遗忘问题，显著提升了终身学习下的跨模态行人匹配性能。

Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.

</details>


### [22] [Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans](https://arxiv.org/abs/2511.15029)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TL;DR: 该研究探讨了计算机视觉模型（如ResNet-50）在训练过程中是否表现出与儿童发展相似的数学思维发展轨迹，发现在几何和数量表征方面存在一定程度的发展对齐现象。


<details>
  <summary>Details</summary>
Motivation: 探究CV模型是否不仅在成人类认知上对齐，也在发展过程中模仿儿童数学能力的发展轨迹。

Method: 通过分析ResNet-50模型在训练过程中的表征变化，与儿童在几何和数量概念发展上的轨迹进行对比。

Result: 在欧几里得几何、几何图形、度量属性和拓扑等概念上表现出发展对齐，但在手性图形、几何变换和对称图形上未发现；同时发现模型随着训练逐步形成类似人类‘心理数轴’的数量表征。

Conclusion: CV模型在某些数学概念的学习过程中展现出与儿童发展相似的进展模式，表明其在理解人类数学认知发展方面具有潜力。

Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.

</details>


### [23] [UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space](https://arxiv.org/abs/2511.15046)
*Panqi Yang,Haodong Jing,Nanning Zheng,Yongqiang Ma*

Main category: cs.CV

TL;DR: 本文提出了UniHOI，通过统一的token空间联合建模人-物交互（HOI）的检测与生成任务，引入对称的交互感知注意力模块和半监督学习范式，实现了图像与交互语义间的双向映射，在有限标注下显著提升了检测与生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将HOI检测与生成作为独立任务处理，限制了对交互理解的整体发展，因此需要一种能够统一建模两个任务的框架以促进知识共享和泛化能力。

Method: 提出UniHOI，构建统一的token空间；设计对称的交互-aware注意力模块；采用统一的半监督学习范式，实现图像与交互语义之间的双向映射。

Result: 在长尾HOI检测上准确率提升4.9%，在开放词汇生成任务中交互指标提升42.0%，实现了检测与生成任务的SOTA性能。

Conclusion: UniHOI通过统一建模HOI检测与生成，有效促进了知识共享与泛化，在两种任务上均取得显著性能提升，推动了全面的人-物交互理解。

Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.

</details>


### [24] [Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method](https://arxiv.org/abs/2511.15052)
*Yue Wen,Kunjing Yang,Minru Bai*

Main category: cs.CV

TL;DR: 提出一种基于退化建模的低秩与残差融合模型（DLRRF），通过分解目标高光谱图像并利用光谱相关性和隐式正则化，有效应对高光谱与多光谱图像融合中的跨图像变异性问题，显著提升融合性能。


<details>
  <summary>Details</summary>
Motivation: 由于采集条件不同，高光谱图像（HSI）与多光谱图像（MSI）之间存在光谱变异性和空间局部变化（即跨图像变异性），严重影响融合效果；现有方法直接对图像进行变换处理，可能加剧融合模型的病态性。

Method: 将光谱变异性建模为光谱退化算子的变化，并将目标HSI分解为低秩和残差两部分，分别恢复整体结构与丢失的空间细节；利用图像内光谱相关性对两部分进行降维，并引入隐式正则项以融合空间先验信息；采用近端交替优化算法在即插即用框架下求解，其中隐式正则项子问题由外部去噪器处理。

Result: 大量数值实验表明，DLRRF在存在跨图像变异性的情况下，显著优于现有HSI与MSI融合方法，能够更准确地恢复高分辨率高光谱图像。

Conclusion: DLRRF通过退化建模和低秩-残差分解有效解决了HSI与MSI融合中的跨图像变异性问题，结合PnP框架与外部去噪器实现了高性能融合，具备良好的收敛性与应用潜力。

Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.

</details>


### [25] [CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues](https://arxiv.org/abs/2511.15054)
*Srijan Ray,Bikesh K. Nirala,Jason T. Yustein,Sundaresh Ram*

Main category: cs.CV

TL;DR: 提出了一种名为CellGenNet的知识蒸馏框架，用于在有限监督下实现跨组织的鲁棒细胞核分割。该方法通过师生架构、软伪标签和混合损失函数显著提升了分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于染色、成像条件和组织形态的变异性，显微镜全切片图像中的细胞核分割仍然具有挑战性，尤其是在标注数据有限的情况下。

Method: 采用师生架构，教师模型在稀疏标注上训练并生成未标记区域的软伪标签；学生模型通过结合真实标签、教师提供的概率目标以及二元交叉熵与Tversky损失的混合损失进行优化，并引入一致性正则化和层间dropout以稳定特征表示。

Result: 在多种癌症组织的全切片图像上实验表明，CellGenNet在分割准确性和泛化性能上优于有监督和半监督基线方法。

Conclusion: CellGenNet能有效提升在有限标注下的跨组织细胞核分割性能，支持可扩展且可重复的病理分析。

Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.

</details>


### [26] [ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing](https://arxiv.org/abs/2511.02505)
*Yaosen Chen,Wei Wang,Tianheng Zheng,Xuming Wen,Han Yang,Yanru Zhang*

Main category: cs.CV

TL;DR: 提出一种基于能量模型的视频镜头组装优化方法，通过脚本与视频库的视觉-语义匹配及参考视频风格学习，实现符合艺术表达的自动化镜头组合。


<details>
  <summary>Details</summary>
Motivation: 现有智能视频编辑技术难以捕捉创作者在镜头组装中的独特艺术表达，需一种能学习并复现参考视频风格的自动化方法。

Method: 首先利用大语言模型生成脚本，并与视频库进行视觉-语义匹配以获取候选镜头；然后对参考视频中的镜头进行分割与标注，提取镜头大小、摄像机运动和语义等属性；接着使用基于能量的模型学习这些属性，评估候选镜头序列与参考风格的一致性；最后结合多种语法规则优化镜头组装。

Result: 实现了能够根据特定逻辑、叙事需求或艺术风格自动排列组合镜头的方法，生成的视频在视觉连贯性和风格一致性上更接近参考视频。

Conclusion: 该方法不仅能自动化视频镜头组装，还能有效学习和再现参考视频的编辑风格，使无经验用户也能创作出具有艺术表现力的视频。

Abstract: Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator's unique artistic expression in shot assembly. To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: https://sobeymil.github.io/esa.com

</details>


### [27] [ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling](https://arxiv.org/abs/2511.15057)
*Yaxiong Chen,Qicong Wang,Chunlei Li,Jingliang Hu,Yilei Shi,Shengwu Xiong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 本文提出了ProPL，一种用于通用半监督超声图像分割的框架，能够处理多种器官和任务，并利用有标签和无标签数据。


<details>
  <summary>Details</summary>
Motivation: 现有超声图像分割方法通常针对特定解剖结构或任务设计，限制了其在临床中的广泛应用。因此，需要一种能跨器官、跨任务通用的分割方法。

Method: 提出ProPL框架，采用共享视觉编码器与提示引导的双解码器结构，通过解码时提示机制实现灵活任务适应，并结合不确定性驱动的伪标签校准（UPLC）模块进行可靠自训练。

Result: 在涵盖5个器官和8种分割任务的综合数据集上实验表明，ProPL在多个指标上优于现有最先进方法。

Conclusion: ProPL为通用超声图像分割建立了新基准，具有更强的临床适用性和扩展潜力。

Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.

</details>


### [28] [Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://arxiv.org/abs/2511.15059)
*Keito Sasagawa,Shuhei Kurita,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 该研究评估了多模态大语言模型（MLLMs）对竖排日文文本的阅读能力，发现现有模型在处理竖排文本时表现较差；通过构建合成的日文OCR数据集进行微调和评估，证明训练可显著提升模型对竖排文本的理解能力。


<details>
  <summary>Details</summary>
Motivation: 由于部分日文文档采用竖排书写，而当前MLLMs对此类文本的支持不足，因此需要专门研究其处理竖排日文的能力。

Method: 生成包含横排和竖排日文的合成OCR数据集，并构建真实场景下的评估数据集；使用这些数据对现有MLLMs进行评估与微调。

Result: 实验表明，现有MLLMs在竖排日文上的性能低于横排；基于合成数据集的微调能有效提升模型对竖排文本的理解能力。

Conclusion: 支持竖排文本对日文文档理解至关重要，通过针对性的数据合成与训练可显著改善MLLMs在此类任务上的表现。

Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.

</details>


### [29] [Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065)
*Cheng Yang,Haiyuan Wan,Yiran Peng,Xin Cheng,Zhaoyang Yu,Jiayi Zhang,Junchi Yu,Xinlei Yu,Xiawu Zheng,Dongzhan Zhou,Chenglin Wu*

Main category: cs.CV

TL;DR: 本文探索了视频模型通过视频生成进行推理的能力，提出了VR-Bench这一基准测试，基于迷宫求解任务评估视频模型的空间规划与多步推理能力。实验表明，经过监督微调（SFT）的视频模型在空间感知和推理上优于现有视觉语言模型，并展现出跨场景、任务和复杂度的良好泛化能力，且推理时的多样化采样可提升10-20%的推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 受语言模型从文本生成到文本推理发展的启发，作者希望探究视频模型是否能通过视频生成进行推理，尤其是利用视频所具有的明确空间布局和时间连续性来支持空间推理。

Method: 提出VR-Bench，一个包含7,920个程序化生成视频的基准，涵盖五种迷宫类型和多种视觉风格，基于迷宫求解任务评估视频模型的推理能力；采用监督微调（SFT）方法激发模型推理能力，并研究推理时多样化采样的影响。

Result: 视频模型在VR-Bench上表现出强于领先视觉语言模型的空间感知与推理能力，具备良好的泛化性，并发现推理时的多样化采样可使推理可靠性提升10-20%。

Conclusion: 视频生成为推理提供了有力范式，尤其适用于需要空间与时间连贯性的推理任务，VR-Bench为未来视频模型的推理能力研究提供了有效平台。

Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.

</details>


### [30] [BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching](https://arxiv.org/abs/2511.15066)
*Yachuan Huang,Xianrui Luo,Qiwen Wang,Liao Shen,Jiaqi Li,Huiqiang Sun,Zihao Huang,Wei Jiang,Zhiguo Cao*

Main category: cs.CV

TL;DR: 本文提出了一种无需深度信息的可控散景渲染框架BokehFlow，基于流匹配技术直接从全焦图像生成逼真的散景效果，并通过文本提示实现对焦点区域和模糊强度的语义控制。


<details>
  <summary>Details</summary>
Motivation: 现有可控散景渲染方法依赖精确的深度图，而生成式方法在可控性和效率方面存在局限，因此需要一种无需深度输入且具备良好控制性的新方法。

Method: 提出BokehFlow，采用流匹配框架，利用交叉注意力机制通过文本提示实现对聚焦区域和模糊强度的语义控制，直接从全焦图像生成散景效果。

Result: 在四个自建数据集上进行实验，结果表明BokehFlow在渲染质量与效率方面优于现有的依赖深度和生成式方法，能生成视觉效果 compelling 的散景图像。

Conclusion: BokehFlow实现了高质量、高效率且可控的散景渲染，无需深度输入，为图像美学增强提供了新的解决方案。

Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.

</details>


### [31] [MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation](https://arxiv.org/abs/2511.15077)
*Shengjing Tian,Yinan Han,Xiantong Zhao,Xuehu Liu,Qi Lang*

Main category: cs.CV

TL;DR: 提出MambaTrack3D，一种基于Mamba状态空间模型的高效3D单目标跟踪框架，专为高时变户外环境设计，通过跨帧传播和特征增强模块实现近线性复杂度与强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的LiDAR跟踪器在高时变动态环境中存在计算复杂度高、时间冗余严重及几何先验利用不足的问题。

Method: 设计基于Mamba的跨帧传播模块（MIP）以实现高效历史帧间信息传播，并引入分组特征增强模块（GFEM）在通道层面分离前景与背景语义，减少内存冗余。

Result: 在KITTI-HTV和nuScenes-HTV上显著优于现有方法，较HVTrack提升最高达6.5成功率和9.5精度，且在标准KITTI数据集上表现具有竞争力。

Conclusion: MambaTrack3D实现了精度与效率的优良权衡，具备在高时变和常规场景下的强泛化能力，适用于复杂动态环境中的3D目标跟踪。

Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.

</details>


### [32] [TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.15085)
*Wen Yin,Siyu Zhan,Cencen Liu,Xin Hu,Guiduo Duan,Xiurui Xie,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 提出了一种基于典型性的一致性感知多模态情感识别框架TiCAL，通过伪单模态标签和典型性估计动态评估样本一致性，并在双曲空间中进行特征嵌入以提升情感表征，有效缓解模态间情感冲突，显著提高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖统一情感标签训练模型，忽视了同一样本中不同模态可能存在情感表达不一致的问题，即模态间情感冲突。

Method: 提出TiCAL框架，结合伪单模态情感标签与典型性估计来动态评估训练样本的一致性；在双曲空间中嵌入特征以捕捉情感类别的细粒度差异，并将一致性估计引入学习过程。

Result: 在CMU-MOSEI和MER2023等基准数据集上实验表明，TiCAL有效缓解了模态间情感冲突，整体识别准确率相比当前最优方法DMD提升了约2.6%。

Conclusion: TiCAL能有效应对多模态情感识别中的模态间情感冲突问题，通过一致性建模和双曲空间表征显著提升模型性能。

Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.

</details>


### [33] [Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis](https://arxiv.org/abs/2511.15092)
*Chengyu Xie,Zhi Gong,Junchi Ren,Linkun Yu,Si Shen,Fei Shen,Xiaoyu Du*

Main category: cs.CV

TL;DR: 提出了一种联合条件扩散模型（JCDM），利用多视角先验信息提升姿态引导的人体图像生成质量，具有高保真度和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有姿态引导人体图像生成方法受限于单视角参考的不完整纹理以及缺乏显式的跨视角交互。

Method: 设计了外观先验模块（APM）从不完整参考中推断整体身份保持先验，并通过联合条件注入（JCI）机制融合多视角线索，将共享条件注入去噪主干网络以对齐不同姿态下的身份、颜色和纹理。

Result: 实验表明该方法在保真度和跨视角一致性方面达到最先进水平。

Conclusion: JCDM能有效利用多视角先验，支持可变数量的参考视角，且可与标准扩散模型结合，仅需最小且有针对性的结构修改。

Abstract: Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.

</details>


### [34] [A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models](https://arxiv.org/abs/2511.15098)
*Duo Li,Zuhao Yang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 该论文研究了离散扩散多模态大语言模型（dMLLMs）中的视觉token冗余问题，发现冗余主要出现在从零训练的dMLLM处理长答案任务时，并提出针对不同架构采用不同的加速策略（如层跳跃或渐进式剪枝），为提升dMLLM效率提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 现有dMLLM在推理时因每步去噪中的全序列注意力计算导致计算开销大，且多数方法忽视模态特有的视觉token冗余问题，因此需要系统研究视觉冗余的演化及其对效率和性能的影响。

Method: 通过分析不同dMLLM架构和任务下视觉token冗余的演变，评估剪枝对模型响应和效率的影响，并比较层跳跃与渐进/后期剪枝在不同类型dMLLM上的有效性。

Result: 发现视觉冗余仅存在于从零训练的dMLLM在处理长答案任务时；剪枝会造成信息损失，但此类模型可在后期去噪中逐步恢复；层跳跃适合AR-to-diffusion模型，而渐进或后期剪枝更适合从零训练的dMLLM。

Conclusion: 应根据dMLLM的架构类型采取差异化的效率优化策略，该研究为dMLLM的高效应用提供了重要指导。

Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.

</details>


### [35] [Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting](https://arxiv.org/abs/2511.15102)
*Junseo Koo,Jinseo Jeong,Gunhee Kim*

Main category: cs.CV

TL;DR: 提出了一种新的高斯混合方法（Gaussian Blending），用于改进3D高斯点阵渲染中的alpha混合，有效减少在未见采样率下的模糊和阶梯状伪影，同时保持实时渲染速度和零额外内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在未见采样率下存在缩放时的模糊和阶梯状伪影，源于传统alpha混合的局限性。

Method: 将传统的标量alpha和透射率计算扩展为空间变化的分布形式，提出高斯混合方法，使背景点可参与像素渲染。

Result: 在多种采样率下均优于现有方法，尤其在未见尺度上显著提升细节还原能力，且保持实时性和低内存开销。

Conclusion: 高斯混合是一种高效、即插即用的改进方案，能显著提升3DGS在多尺度视图合成中的鲁棒性和视觉质量。

Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.

</details>


### [36] [Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection](https://arxiv.org/abs/2511.15343)
*Spyridon Loukovitis,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出一种轻量、模型无关的后处理框架，实现无人机导航中对已知目标、未知物体和背景的实时三类分类，提升开放集检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放集检测方法多依赖单一不确定性分数和阈值，难以区分未知物体与背景杂波，限制了在安全关键场景（如无人机导航）中的应用。需要更灵活的方法来同时处理已知、未知和背景三类。

Method: 设计一个模型无关的后处理框架，通过融合多种置信度得分和每检测特征，使用紧凑的多层感知机（MLP）进行多源信息聚合，实现从二类到三类（ID、OOD、背景）的实时分类。

Result: 在二类分类任务中比基于阈值的方法平均高出2.7% AUROC，保持或提升开放集mAP；在三类分类中展现出优越性能，且闭集mAP最高提升9点（相对提升18%）。

Conclusion: 该方法有效分离未知对象与背景，支持安全的无人机导航决策，推动开放集检测向更细粒度、更可靠的方向发展。

Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.

</details>


### [37] [An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring](https://arxiv.org/abs/2511.15117)
*Jun-Yi Liu,Chung-Hao Chen,Ya-Chi Tsao,Ssu-Yao Wu,Yu-Ting Tsao,Lyn Chao-ling Chen*

Main category: cs.CV

TL;DR: 提出了一种基于事件触发的系统，用于监测老年人在家庭场景中的物理和心理状态，通过GMM背景建模检测行为，并结合SVM分析图像，实现与亲属的直观社交互动。


<details>
  <summary>Details</summary>
Motivation: 关注老年人的身心健康，解决其因技术经验不足难以使用智能设备的问题，提升居家养老的安全性与社交连接。

Method: 采用GMM背景建模检测访客和老年人的运动行为，识别看护、危险通知等事件；使用SVM进行图像分析；设计基于日常行为的直观操作以触发与亲属的社交媒体通信。

Result: 在5个家庭中成功检测并记录了三类生活事件，系统能有效识别行为并触发相应事件响应，图像分析准确率较高，用户交互自然简便。

Conclusion: 该事件触发系统能够有效提升老年人居家生活的安全性和社交参与度，具有实际应用潜力。

Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.

</details>


### [38] [Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation](https://arxiv.org/abs/2511.15118)
*Jin Wang,Bingfeng Zhang,Jian Pang,Weifeng Liu,Baodi Liu,Honglong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的无偏语义解码（USD）策略，结合SAM和CLIP模型，在少样本分割中通过支持集和查询集共同提取目标信息，提升模型的泛化能力和语义判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖精确提示，难以充分激活SAM的泛化能力，且在面对未知类别时容易产生偏差。

Method: 提出USD策略，利用CLIP的语义对齐能力，在图像级进行全局补充，在像素级进行局部引导，并设计可学习的视觉-文本目标提示生成器。

Result: 该方法无需重新训练视觉基础模型，即可增强SAM对目标区域的关注，提升少样本分割性能。

Conclusion: 所提方法有效提升了SAM在少样本分割任务中的表现，实现了更一致、无偏的语义解码。

Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.

</details>


### [39] [MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling](https://arxiv.org/abs/2511.15645)
*Shanshan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba架构的惯性里程计方法MambaIO，通过拉普拉斯金字塔分解IMU信号，并在不同频率成分上分别利用Mamba和卷积结构提升行人定位精度，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统惯性里程计多采用全局坐标系处理IMU数据，但无人机场景中体坐标系表现更优，因此需重新评估全局坐标系在行人场景下的适用性。

Method: 提出MambaIO，使用拉普拉斯金字塔将IMU测量分解为高低频成分；低频部分用Mamba架构提取上下文运动特征，高频部分用卷积结构捕捉局部细节。

Result: 在多个公开数据集上验证，MambaIO显著降低定位误差，达到SOTA性能。

Conclusion: MambaIO有效提升了行人惯性里程计的精度，且首次将Mamba架构应用于该任务，验证了其在惯性导航中的潜力。

Abstract: Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.

</details>


### [40] [WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images](https://arxiv.org/abs/2511.15132)
*Nishchala Thakur,Swati Kochhar,Deepti R. Bathula,Sukrit Gupta*

Main category: cs.CV

TL;DR: 提出了一种名为WaveFuse-AL的新型多策略主动学习框架，通过结合周期性和性能自适应机制，在医学图像分析中显著提升了标注效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的主动学习策略在不同学习阶段表现不稳定，难以充分利用有限的标注资源。

Method: WaveFuse-AL融合了BALD、BADGE、Entropy和CoreSet等多种主流采样策略，并引入周期性（正弦）时间先验与基于性能的自适应机制，动态调整各策略的重要性权重。

Result: 在APTOS-2019、RSNA肺炎检测和ISIC-2018三个医学影像基准上验证了该方法的有效性，结果显示WaveFuse-AL在12项指标中的10项上均优于单策略和交替策略基线方法，且提升具有统计显著性。

Conclusion: WaveFuse-AL能够更高效地利用标注预算，稳定提升模型性能，适用于多种医学图像任务。

Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.

</details>


### [41] [DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging](https://arxiv.org/abs/2511.15151)
*Meihua Zhou,Xinyu Tong,Jiarui Zhao,Min Cheng,Li Yang,Lei Tian,Nan Wan*

Main category: cs.CV

TL;DR: 提出了一种名为DCL-SE的端到端框架，通过数据驱动的时空编码和动态课程学习，在多种脑成像任务中实现了更高的准确性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高维神经影像分析在临床诊断中常受限于时空保真度的妥协以及大规模通用模型适应性不足的问题。

Method: 引入DCL-SE框架，采用近似秩池化（ARP）将三维脑数据编码为信息丰富的二维动态表示，并结合动态分组机制（DGM）指导的动态课程学习策略，逐步优化解码器从全局结构到细微病理特征的提取能力。

Result: 在六个公开数据集上验证了DCL-SE的有效性，涵盖阿尔茨海默病分类、脑肿瘤分类、脑动脉分割和脑年龄预测等任务，性能优于现有方法。

Conclusion: 研究表明，在大规模预训练网络时代，紧凑且任务特定的架构对医学影像分析具有重要意义。

Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.

</details>


### [42] [SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection](https://arxiv.org/abs/2511.15153)
*Chun-Jung Lin,Tat-Jun Chin,Sourav Garg,Feras Dayoub*

Main category: cs.CV

TL;DR: 本文提出了SceneEdited，首个支持城市尺度HD地图维护的3D点云更新数据集，包含800多个场景和23,000多个合成对象变化，填补了2D变化检测与3D地图更新之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测到环境变化后难以有效更新3D高精地图，尤其是基于2D图像的变化检测结果难以转化为3D更新，缺乏标准数据集支持相关研究。

Method: 构建了一个大规模、城市级的带标注数据集SceneEdited，包含多时相LiDAR点云、RGB图像和精确变化掩码，并提供基于图像的运动恢复结构（SfM）基础更新方法及配套工具包。

Result: 数据集覆盖73公里驾驶路线和约3平方公里城区，包含超过23,000个手动和自动合成的变化实例，支持对3D地图更新方法的训练与评估，并提供了可扩展、可追踪、可移植的工具链。

Conclusion: SceneEdited为3D高精地图的持续更新提供了标准化基准和重要资源，推动了从变化检测到实际地图更新的研究发展。

Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.

</details>


### [43] [Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation](https://arxiv.org/abs/2511.15159)
*Firdavs Nasriddinov,Rafal Kocielnik,Anima Anandkumar,Andrew J. Hung*

Main category: cs.CV

TL;DR: 本文提出一种结构感知的流程，通过从真实手术培训对话中提取Instrument-Action-Target（IAT）三元组并构建手术动作本体，用于指导GPT-4o生成临床可信、教练风格的术中反馈。


<details>
  <summary>Details</summary>
Motivation: 高质量的术中反馈对医学生技能提升至关重要，但目前缺乏能够理解临床相关语义结构的自动化反馈系统。

Method: 1) 从真实反馈文本中挖掘IAT三元组并归一化；2) 结合手术流程、任务上下文和精细时间仪器运动，微调视频到IAT的模型；3) 利用IAT表示引导GPT-4o生成反馈。

Result: 在视频到IAT识别任务中，AUC指标显著提升（器械0.67→0.74，动作0.60→0.63，组织0.74→0.79）；在反馈生成任务中，IAT条件生成得分从2.17提升至2.44（+12.4%），且评分≥3的比例从21%翻倍至42%，文本相似性指标也明显改善。

Conclusion: 基于显式IAT结构的生成方法提升了反馈的保真度，并提供可验证的临床依据，支持在手术培训中可审计地应用。

Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

</details>


### [44] [Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance](https://arxiv.org/abs/2511.15164)
*Songze Li,Mingyu Gao,Tonghua Su,Xu-Yao Zhang,Zhongjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态持续指令调优方法，通过利用参数空间的几何特性近似旧任务的梯度，缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在持续学习新任务时容易遗忘旧任务，即灾难性遗忘问题，限制了其在实际场景中的应用。

Method: 将灾难性遗忘视为旧任务梯度缺失问题，使用当前参数与先前最优参数之间的方向向量作为梯度指引，并结合有限回放缓冲区的真实梯度，采用伯努利采样策略动态平衡模型稳定性与可塑性。

Result: 在多个多模态持续指令调优数据集上实验表明，该方法在不扩展模型的情况下达到了最先进的性能，有效缓解了灾难性遗忘。

Conclusion: 所提出的方法通过几何梯度近似和动态调节机制，在保持紧凑模型结构的同时显著提升了多模态持续学习的性能。

Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.

</details>


### [45] [Computer-Use Agents as Judges for Generative User Interface](https://arxiv.org/abs/2511.15567)
*Kevin Qinghong Lin,Siyuan Hu,Linjie Li,Zhengyuan Yang,Lijuan Wang,Philip Torr,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出Coder-CUA协作框架，利用计算机使用代理（CUA）作为评判者来指导编码语言模型（Coder）进行面向代理的自动GUI设计，提升数字界面的任务执行效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI主要为人设计，导致代理需模仿人类行为，效率低下；同时，编码大模型的发展为自动GUI设计提供了新机遇，但缺乏以代理需求为核心的评估机制。

Method: 构建AUI-Gym基准，包含52个应用和1560个可验证任务；提出Coder-CUA协作框架，其中Coder生成/修改GUI，CUA通过实际导航任务评估其可用性，并通过CUA Dashboard将多步操作历史压缩为可视化反馈用于迭代优化。

Result: 实现了基于任务可解性和代理导航成功率的GUI评估体系，验证了代理作为设计评判者的有效性，显著提升了GUI对自动化代理的操作友好性与执行效率。

Conclusion: 通过让代理在GUI设计中扮演“法官”角色，推动界面设计从以人为本转向代理原生优化，促进代理从被动使用转向主动参与数字环境建设。

Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.

</details>


### [46] [Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation](https://arxiv.org/abs/2511.15167)
*Jing Cao,Kui Jiang,Shenyi Li,Xiaocheng Feng,Yong Huang*

Main category: cs.CV

TL;DR: 提出了一种名为SEC-Depth的自进化对比学习框架，用于提升自监督深度估计在雨雾等恶劣天气下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计方法在雨雾等能见度低的天气条件下性能显著下降，难以满足自动驾驶和机器人应用的需求。

Method: 设计动态更新的延迟模型以捕捉训练过程中的优化状态，并引入自进化对比损失（SECL），将历史延迟模型的输出作为负样本，自适应调整学习目标。

Result: 实验表明，该方法可无缝集成到多种基线模型中，在零样本评估下显著提升了模型在恶劣天气条件下的鲁棒性。

Conclusion: SEC-Depth通过利用训练过程中的中间参数和对比学习机制，有效增强了自监督深度估计在复杂天气下的稳定性与泛化能力。

Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.

</details>


### [47] [MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction](https://arxiv.org/abs/2511.15179)
*Kyotaro Tokoro,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 本文提出了一种用于人体运动预测（HMP）的新型多模态评估指标MMCM，通过基于聚类的模式划分来衡量预测动作的覆盖率和有效性，解决了现有指标无法准确评估多模态且运动学合理的预测结果的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的HMP评估指标无法有效衡量多模态预测中动作分布的多样性和运动学合理性，容易对单一模式内分散但无效的预测给予过高评分，因此需要一种能同时评估覆盖率和有效性的新指标。

Method: 提出MMCM指标，利用聚类将运动空间划分为多个模式以评估覆盖率，并基于真实数据中的未来动作识别有效模式以评估有效性。

Result: 实验证明所提出的聚类方法能合理定义运动模式，MMCM能够更准确地评估多模态预测结果的质量。

Conclusion: MMCM是一种更合理、更敏感的多模态人体运动预测评估指标，能够在覆盖率和有效性两个维度上提升评价准确性。

Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM

</details>


### [48] [When to Think and When to Look: Uncertainty-Guided Lookback](https://arxiv.org/abs/2511.15613)
*Jing Bi,Filippos Bellos,Junjia Guo,Yayuan Li,Chao Huang,Yunlong,Tang,Luchuan Song,Susan Liang,Zhongfei,Zhang,Jason J. Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文首次系统分析了大规模视觉语言模型（LVLMs）中“测试时思考”对视觉推理的影响，发现更多思考并不总带来更好性能，过长的推理链可能偏离图像内容。作者提出一种无需训练的解码策略——不确定性引导回看（uncertainty guided lookback），通过结合不确定性信号与自适应回看提示，在多个基准上显著提升性能，尤其在传统思考模式表现弱的类别中效果更优，并实现了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时思考在大型语言模型和视觉语言模型中展现出潜力，但其对视觉推理的具体影响尚缺乏系统性分析。本文旨在填补这一空白，探究思考过程如何真正影响视觉理解与推理性能。

Method: 在InternVL3.5和Qwen3-VL系列的十个模型变体上进行大规模、受控的对比实验，使用MMMU-val数据集并在充足token预算和多遍解码条件下评估。深入分析成功推理轨迹中的语言模式，识别出与良好视觉 grounding 相关的关键短回看短语，并据此提出不确定性引导回看方法，结合不确定性估计、自适应回看提示和广度搜索进行解码优化。

Result: 实验表明，过长的推理链常导致偏离图像的错误路径，表现不如标准指令模式。提出的不确定性引导回看策略在MMMU上提升了整体性能，尤其在传统思考薄弱类别中增益最大，并超越多种强基线。该方法在五个额外基准（包括多模态套件和数学视觉推理数据集）上均表现出一致改进，具备良好泛化性。

Conclusion: 测试时思考的效果依赖于对图像内容的有效回看而非推理长度；引入基于不确定性的自适应回看机制能显著增强视觉语言模型的视觉 grounding 与推理准确性，为无需训练的推理优化提供了新方向。

Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.

</details>


### [49] [Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset](https://arxiv.org/abs/2511.15186)
*Geon Choi,Hangyul Yoon,Hyunju Shin,Hyunki Park,Sang Hoon Seo,Eunho Yang,Edward Choi*

Main category: cs.CV

TL;DR: 提出了一种新的指令引导病变分割（ILS）范式，并构建了首个大规模胸部X光病变分割指令数据集MIMIC-ILS，同时开发了能够根据简单指令分割病变并提供文本解释的视觉语言模型ROSALIA。


<details>
  <summary>Details</summary>
Motivation: 现有胸部X光病变分割模型受限于标签种类少且依赖冗长的专业文本输入，难以实际应用，因此需要更灵活、用户友好的分割方法。

Method: 提出指令引导病变分割（ILS）新范式，构建全自动多模态流水线生成MIMIC-ILS数据集（含110万指令-答案对），并基于该数据集微调视觉语言模型ROSALIA以实现指令驱动的分割与解释生成。

Result: MIMIC-ILS包含19.2万张图像和9.1万个唯一分割掩码，覆盖七种主要病变类型；ROSALIA在新任务上表现出高分割准确率和文本生成准确性。

Conclusion: 所提出的ILS范式和MIMIC-ILS数据集为胸部X光图像中像素级病变定位提供了有效且实用的新途径，ROSALIA验证了该方法的可行性与潜力。

Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.

</details>


### [50] [VisPlay: Self-Evolving Vision-Language Models from Images](https://arxiv.org/abs/2511.15661)
*Yicheng He,Chengsong Huang,Zongxia Li,Jiaxin Huang,Yonghui Yang*

Main category: cs.CV

TL;DR: VisPlay是一种自演化的强化学习框架，利用未标注图像数据使视觉语言模型（VLMs）自主提升推理能力，通过角色分工与GRPO训练方法，在多个基准上实现推理、泛化和抗幻觉的显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注或任务特定启发式设计奖励函数，成本高且难以扩展，因此需要一种可扩展的自演化方法来提升VLM在复杂推理任务上的表现。

Method: 提出VisPlay框架，将单一VLM拆分为两个交互角色：图像条件提问者生成挑战性问题，多模态推理者生成银牌答案；采用Group Relative Policy Optimization（GRPO）联合训练，并引入多样性和难度奖励以平衡问题复杂度与答案质量。

Result: 在Qwen2.5-VL和MiMo-VL两个模型家族上验证，VisPlay在MM-Vet、MMMU等八个基准上均提升视觉推理、组合泛化能力并减少幻觉现象，展现出良好的可扩展性。

Conclusion: VisPlay为构建可自我进化的多模态智能系统提供了可扩展的技术路径，无需人工标注即可持续提升VLM的复杂推理能力。

Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/

</details>


### [51] [BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI](https://arxiv.org/abs/2511.15188)
*Wasif Jalal,Md Nafiu Rahman,M. Sohel Rahman*

Main category: cs.CV

TL;DR: 提出了一种结合Vision Transformer和ResNet的混合模型BrainRotViT，用于从结构MRI中准确估计脑龄，在多中心数据上表现优异，具有良好的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统回归和CNN方法在脑龄估计中存在特征工程依赖、感受野受限和过拟合问题，纯Transformer模型则需要大量数据和计算资源，因此需要一种高效且泛化能力强的混合架构。

Method: 首先在辅助的年龄和性别分类任务上预训练ViT编码器以提取矢状面切片特征；固定该编码器后，将其应用于所有切片生成嵌入向量矩阵，输入到带残差结构的CNN回归器中，并在最后全连接层引入受试者性别信息来预测连续脑龄。

Result: 在涵盖11个MRI数据集（130多个采集站点）的验证中，MAE为3.34年（Pearson r=0.98，Spearman ρ=0.97，R²=0.95），优于基线和当前最优模型；在4个独立队列中MAE介于3.77至5.04年之间；脑龄差与阿尔茨海默病、认知障碍和自闭症谱系障碍相关；注意力图突出了小脑蚓部、中央前/后回、颞叶和内侧上额叶等与衰老相关的脑区。

Conclusion: BrainRotViT提供了一个高效、可解释且泛化性强的脑龄预测框架，弥合了CNN与Transformer方法之间的差距，为衰老和神经退行性疾病研究开辟了新途径。

Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.

</details>


### [52] [MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping](https://arxiv.org/abs/2511.15690)
*Yushi Huang,Zining Wang,Zhihang Yuan,Yifu Ding,Ruihao Gong,Jinyang Guo,Xianglong Liu,Jun Zhang*

Main category: cs.CV

TL;DR: 提出MoDES，一种无需训练的自适应专家跳过框架，通过全局调制局部门控和双模态阈值方法，显著提升MoE多模态大模型的推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有专家跳过方法直接应用于多模态大模型时性能下降严重，因未考虑专家在不同层的异质贡献及模态特定行为。

Method: 提出MoDES框架，包含全局调制局部门控（GMLG）机制和双模态阈值（DMT）方法，并利用前沿搜索算法快速确定最优阈值。

Result: 在3个模型系列、13个基准上验证有效性，跳过88%专家时性能提升达10.67%，预填充速度提升2.16倍，解码速度提升1.26倍。

Conclusion: MoDES是首个训练-free的高效MoE多模态推理框架，兼顾高精度与高速度，显著优于先前方法。

Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.

</details>


### [53] [Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition](https://arxiv.org/abs/2511.15197)
*Raghu Vamsi Chittersu,Yuvraj Singh Rathore,Pranav Adlinge,Kunal Swami*

Main category: cs.CV

TL;DR: 本文提出了一种名为Insert In Style的零样本生成框架，用于在风格化场景中实现高保真、实用的对象插入，解决了现有方法在真实对象与风格化域融合中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于参考的对象组合方法在将真实世界对象插入风格化领域时表现不佳，且当前方法在实用性与生成保真度之间存在权衡。

Method: 提出一种统一框架，包含多阶段训练策略以解耦身份、风格和组合表示，并设计一种特殊的掩码注意力架构，在生成过程中强制实施这种解耦。

Result: 在新构建的10万样本数据集上训练，并通过新的公共基准测试验证，模型在身份和风格指标上均显著优于现有方法，用户研究也证实了其优越性。

Conclusion: Insert In Style是首个兼具实用性与高保真度的零样本生成框架，无需文本提示或针对特定主体的微调，有效避免了概念干扰问题。

Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.

</details>


### [54] [Think Visually, Reason Textually: Vision-Language Synergy in ARC](https://arxiv.org/abs/2511.15703)
*Beichen Zhang,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉与语言的协同推理方法（VLSR）和模态切换自纠错机制（MSSC），以解决前沿大模型在抽象推理任务（如ARC-AGI）中从少量示例推断规则能力不足的问题，实验表明该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在抽象推理任务上表现不佳，尤其是仅依赖文本或简单图像输入时无法准确执行结构化规则，而人类则能结合视觉抽象与语言推理完成此类任务，因此需要融合两种模态的优势。

Method: 提出Vision-Language Synergy Reasoning (VLSR)，将任务分解为视觉与语言各自擅长的子任务；并设计Modality-Switch Self-Correction (MSSC)，利用视觉验证语言推理结果以实现自我纠正。

Result: 在多个主流大模型和ARC-AGI任务上，该方法相比纯文本基线最高提升了4.33%的性能，且验证了视觉与语言模态互补的有效性。

Conclusion: 融合视觉抽象与语言推理是实现类人通用智能的关键路径，该工作为未来基础模型的多模态协同推理提供了有效范式。

Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.

</details>


### [55] [Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval](https://arxiv.org/abs/2511.15201)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果理论的去偏方法，用于解决食谱与食物图像跨模态检索中的表示学习偏差问题，并在Recipe1M数据集上实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将食谱视为描述菜肴外观的文本，忽略了烹饪过程、呈现方式等因素导致的视觉-文本不对齐，从而在相似性判断中引入偏差。

Method: 采用因果理论建模偏差，将食材视为混杂因素之一，通过后门调整进行因果干预，并设计了一个可插拔的多标签食材分类神经模块来去偏。

Result: 在Recipe1M数据集上，无论测试数据规模为1K、10K还是50K，中位秩（MedR）均达到1，验证了方法的优越性能。

Conclusion: 所提出的因果干预方法能有效缓解跨模态检索中的偏差，显著提升食谱-食物图像检索效果。

Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.

</details>


### [56] [Physics-Based Benchmarking Metrics for Multimodal Synthetic Images](https://arxiv.org/abs/2511.15204)
*Kishor Datta Gupta,Marufa Kamal,Md. Mahfuzur Rahman,Fahad Rahman,Mohd Ariful Haque,Sunzida Siddique*

Main category: cs.CV

TL;DR: 提出了一种结合大语言模型、知识映射和视觉-语言模型的物理约束多模态数据评估（PCMDE）指标，以克服现有评估指标在语义和结构准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的评估指标如BLEU、CIDEr等难以捕捉语义或结构准确性，尤其在领域特定或上下文相关场景中表现不足。

Method: PCMDE包含三个阶段：(1) 通过目标检测和视觉-语言模型提取空间和语义特征；(2) 使用置信度加权组件融合进行组件级验证；(3) 利用大语言模型进行物理引导推理，强制执行结构和关系约束。

Result: 该方法能更准确地评估多模态生成内容的结构和语义一致性，尤其在需要物理规律或领域知识的场景中优于传统指标。

Conclusion: PCMDE通过融合多模态特征与物理常识推理，显著提升了对复杂多模态输出的评估能力，是一种更具鲁棒性和解释性的评估框架。

Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.

</details>


### [57] [SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning](https://arxiv.org/abs/2511.15242)
*Yuhao Shen,Jiahe Qian,Zhangtianyi Chen,Yuanhao He,Juexiao Zhou*

Main category: cs.CV

TL;DR: SkinGPT-R1 是一种专注于皮肤病学的视觉语言模型，通过显式的分步推理链提升诊断可解释性和准确性，在 DermBench 基准上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了提高皮肤病诊断中视觉语言模型的可解释性与临床可信度，需要构建专门的、医生认可的推理链数据集和评估体系。

Method: 提出 SkinGPT-R1 模型和 DermCoT 推理链语料库，并定义 DermEval 六维评估标准及 DermBench 基准；采用 DermCoT 监督训练和皮肤病感知的视觉蒸馏方法。

Result: 在 DermBench 上，SkinGPT-R1 平均得分 4.031/5，排名第一，较 Vision-R1 提升约 41%；在三个分类基准上保持稳定增益，消融实验显示 DermCoT 和视觉蒸馏均有显著贡献。

Conclusion: SkinGPT-R1 结合领域特定的推理链监督和视觉蒸馏，有效提升了皮肤病诊断的推理质量与识别性能，具备临床应用潜力。

Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.

</details>


### [58] [SplitFlux: Learning to Decouple Content and Style from a Single Image](https://arxiv.org/abs/2511.15258)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Yongjun Zhang,Ziyang Chen,Shuting He*

Main category: cs.CV

TL;DR: 本文提出SplitFlux，通过分析Flux模型的特性，利用LoRA微调单流Dream Blocks实现内容与风格的有效解耦，提升图像生成中的内容保持和风格化质量。


<details>
  <summary>Details</summary>
Motivation: 现有SDXL-based方法难以实现高质量的内容-风格解耦，而Flux模型因特性未被充分探索导致解耦效果不佳。

Method: 基于对Flux模型的系统分析，发现单个Dream Blocks对生成至关重要，且早期块控制内容、后期块控制风格；据此提出SplitFlux，采用Rank-Constrained Adaptation和Visual-Gated LoRA策略进行内容与风格分离。

Result: 实验表明，SplitFlux在多种场景下均优于当前最先进方法，显著提升内容保持和风格化质量。

Conclusion: SplitFlux通过精细调控单流结构中的LoRA更新，有效实现了内容与风格的解耦，支持高质量的图像风格迁移与内容重嵌入。

Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.

</details>


### [59] [Graph Query Networks for Object Detection with Automotive Radar](https://arxiv.org/abs/2511.15271)
*Loveneet Saini,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的图查询网络（GQN），用于3D雷达下的物体检测，通过构建对象特定的图并引入EdgeFocus和DeepContext Pooling模块，在NuScenes数据集上显著提升了检测性能，同时降低了图构建开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格或序列的卷积和Transformer检测器难以有效处理雷达因长波长导致的稀疏且不规则的反射信号，因此需要一种能更好建模雷达感知物体间关系与上下文信息的新方法。

Method: 提出Graph Query Networks（GQN），将雷达感知的物体建模为图结构，引入图查询机制在鸟瞰图空间中动态关注，使用EdgeFocus模块进行关系推理，以及DeepContext Pooling模块聚合上下文特征。

Result: 在NuScenes数据集上，GQN相对mAP最高提升53%，相比最强的纯雷达方法提升8.2%，同时图构建峰值开销降低80%，计算量适中。

Conclusion: GQN通过图结构建模和新型注意力机制，有效解决了3D雷达检测中的稀疏性和不规则性问题，实现了显著的性能提升和效率优化，为自动驾驶中的雷达感知提供了新思路。

Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.

</details>


### [60] [Edge-Centric Relational Reasoning for 3D Scene Graph Prediction](https://arxiv.org/abs/2511.15288)
*Yanni Ma,Hao Liu,Yulan Guo,Theo Gevers,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出了一种名为LEO的链路引导边中心关系推理框架，通过将场景图转换为线图并进行边中心推理，有效捕捉高阶关系依赖，从而提升3D场景图关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对象中心的图神经网络受限于仅利用成对对象上下文，难以捕捉对准确关系预测至关重要的高阶关系依赖。

Method: 首先预测物体对之间的潜在链路以抑制无关边，然后将原始场景图转化为线图，在线图上使用边中心的图神经网络进行关系推理，并将增强后的关系特征融合回原图以提升对象级推理。

Result: 在线图上实现了更丰富的关系上下文建模，显著提升了关系预测准确性，在3DSSG数据集上结合两个强基线模型均取得一致性能提升。

Conclusion: 所提出的LEO框架通过从关系级上下文到对象级理解的渐进式推理，有效克服了传统方法在捕捉高阶关系依赖上的局限性，且具有模型无关性，可广泛集成到现有方法中。

Abstract: 3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.

</details>


### [61] [Taming Generative Synthetic Data for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.15299)
*Jialong Sun,Hongguang Zhu,Weizhe Liu,Yunda Sun,Renshuai Tao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外人工成本的单阶段X射线安检图像合成方法Xsyn，基于文本到图像生成，结合交叉注意力优化和背景遮挡建模策略，显著提升合成图像质量及违禁品检测性能。


<details>
  <summary>Details</summary>
Motivation: 训练违禁品检测模型需要大量标注的X射线图像，而传统合成方法依赖耗时耗力的两阶段流程（先前景提取再合成），效率低下。因此，亟需一种高效、低成本的图像合成方法以缓解数据不足问题。

Method: 提出Xsyn，一种基于扩散模型的端到端文本到图像生成框架。引入两种关键策略：1）交叉注意力优化（CAR），利用扩散模型中的交叉注意力图优化边界框标注；2）背景遮挡建模（BOM），在潜在空间显式建模背景遮挡以增强成像复杂度。

Result: 实验表明，Xsyn在mAP上比先前方法提升1.2%，且生成的图像能有效提升多种X射线数据集和检测器上的违禁品检测性能。为首个无需额外人工标注成本即可实现高质量X射线图像合成的方法。

Conclusion: Xsyn通过创新的单阶段生成框架和两项关键技术，实现了高效、高质量的X射线安检图像合成，解决了传统方法劳动密集的问题，为安检检测模型的训练提供了更具实用性的数据增强方案。

Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.

</details>


### [62] [Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language](https://arxiv.org/abs/2511.15308)
*Yan Xia,Letian Shi,Yilin Di,Joao F. Henriques,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了Text2Loc++，一种用于自然语言描述与3D点云子图匹配的跨模态粗到精定位框架，并发布了新的城市规模数据集以支持评测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂自然语言描述下对3D点云子图的定位能力有限，缺乏多样化场景和语言复杂度的基准测试支持。

Method: 提出Text2Loc++，包含层次化Transformer与注意力点云编码器用于全局识别；引入掩码实例训练（MIT）和模态感知分层对比学习（MHCL）提升跨模态对齐；在精细定位阶段采用原型地图克隆（PMC）和级联交叉注意力Transformer（CCAT）实现高效定位。

Result: 在KITTI360Pose上性能超越现有方法达15%，在新构建的城市尺度数据集上表现出强泛化能力，能有效处理复杂语言表达和多样的城市环境。

Conclusion: Text2Loc++通过创新的粗到精架构和多层次对比学习机制，显著提升了基于自然语言的3D点云定位性能与鲁棒性，推动了跨模态定位的发展。

Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.

</details>


### [63] [Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models](https://arxiv.org/abs/2511.15311)
*Mehran Tamjidi,Hamidreza Dastmalchi,Mohammadreza Alimoradijazi,Ali Cheraghian,Aijun An,Morteza Saberi*

Main category: cs.CV

TL;DR: 提出了一种无需训练的在线测试时自适应方法Uni-Adapter，用于提升3D视觉语言基础模型在噪声和分布偏移数据下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉语言基础模型在面对噪声、不完整或分布偏移的数据时性能下降，难以满足实际应用需求。

Method: 通过动态原型学习，在线维护一个3D缓存来存储并更新类别特定的聚类中心（原型），结合基于图的标签平滑模块和熵加权预测融合策略，实现对3D VLFMs的测试时自适应。

Result: 在多个3D基准上显著提升了现有3D VLFMs的性能，如ModelNet-40C提升10.55%，ScanObjectNN-C提升8.26%，ShapeNet-C提升4.49%。

Conclusion: Uni-Adapter是一种有效的训练-free测试时自适应方法，能够显著增强3D视觉语言模型在复杂真实场景中的鲁棒性和泛化能力。

Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.

</details>


### [64] [A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data](https://arxiv.org/abs/2511.15312)
*Mauro Larrat,Claudomiro Sales*

Main category: cs.CV

TL;DR: 提出一种基于多模态Transformer的无人机检测与空中目标识别方法，融合雷达、RGB视频、红外视频和音频数据，实现高精度、高效的实时分类，性能达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统单模态方法在复杂环境中对无人机检测和空中目标识别存在局限性，需要更鲁棒的多模态融合方案以提升检测准确性和适应性。

Method: 设计并评估一种新型多模态Transformer模型，融合雷达、可见光、红外视频和音频数据，利用自注意力机制进行特征融合与分类。

Result: 在独立测试集上达到0.9812准确率、0.9873召回率、0.9787精确率、0.9826 F1分数和0.9954特异性，模型计算量低（1.09 GFLOPs），推理速度达41.11 FPS。

Conclusion: 多模态Transformer架构显著提升了空中目标分类性能，具备高精度、高效率特点，适用于复杂空域下的无人机实时检测与监控。

Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.

</details>


### [65] [What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs](https://arxiv.org/abs/2511.15316)
*Zhihan Ren,Lijun He,Jiaxi Liang,Xinzhu Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: 提出FIA-Flow，一种黑盒特征反演攻击框架，通过潜在特征空间对齐模块和确定性反演流匹配方法，实现从中间特征高保真重建输入图像，揭示分割DNN中更严重的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有特征反演攻击（FIA）方法重建质量有限，难以准确评估分割DNN中中间特征泄露带来的真实隐私风险，因此需要更有效的攻击方法来揭示其潜在威胁。

Method: 设计了潜在特征空间对齐模块（LFSAM）以桥接中间特征空间与潜在空间之间的语义鸿沟，并提出确定性反演流匹配（DIFM）来校正分布不匹配问题，实现单步推理下的高质量图像重建。

Result: FIA-Flow在多种模型（如AlexNet、ResNet、Swin Transformer等）和不同层上均实现了更保真且语义一致的特征反演效果，并提出了基于视觉语言大模型的新型隐私度量指标。

Conclusion: FIA-Flow显著提升了特征反演攻击的质量，揭示了分割DNN中比以往认知更为严重的隐私泄露风险，强调了加强中间特征保护的必要性。

Abstract: Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features. To exploit the semantic information within intermediate features, we design a Latent Feature Space Alignment Module (LFSAM) to bridge the semantic gap between the intermediate feature space and the latent space. Furthermore, to rectify distributional mismatch, we develop Deterministic Inversion Flow Matching (DIFM), which projects off-manifold features onto the target manifold with one-step inference. This decoupled design simplifies learning and enables effective training with few image-feature pairs. To quantify privacy leakage from a human perspective, we also propose two metrics based on a large vision-language model. Experiments show that FIA-Flow achieves more faithful and semantically aligned feature inversion across various models (AlexNet, ResNet, Swin Transformer, DINO, and YOLO11) and layers, revealing a more severe privacy threat in Split DNNs than previously recognized.

</details>


### [66] [Adaptive thresholding pattern for fingerprint forgery detection](https://arxiv.org/abs/2511.15322)
*Zahra Farzadpour,Masoumeh Azghani*

Main category: cs.CV

TL;DR: 本文提出了一种基于自适应阈值模式的小波变换方法，用于指纹伪造检测，并通过SVM分类器实现对真实与伪造指纹的区分，具有较强的抗噪声和数据丢失能力。


<details>
  <summary>Details</summary>
Motivation: 指纹活体检测系统易受伪造攻击，需开发能有效区分真假指纹的技术，尤其是应对各种干扰和篡改手段。

Method: 采用各向异性扩散预处理，结合三级小波变换，对不同层系数进行自适应阈值处理并生成特征向量，最后使用SVM进行分类。

Result: 在90%像素缺失和70x70区块缺失情况下，准确率分别比现有方法提高约8%和5%，表现出更强的鲁棒性。

Conclusion: 所提方法在多种失真条件下均表现出优越性能，有效提升了指纹伪造检测的准确性与抗干扰能力。

Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.

</details>


### [67] [IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers](https://arxiv.org/abs/2511.15369)
*Gihwan Kim,Jemin Lee,Hyungshin Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的无需重训练的后训练量化框架IPTQ-ViT，用于实现全整数量化视觉Transformer，通过多项式GELU和位移Softmax近似函数及统一度量优化各层激活函数选择，在精度和延迟上媲美量化感知训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法在非线性层量化中存在精度损失或无法实现全整数量化的问题，且依赖昂贵的重训练过程，限制了其在资源受限环境中的应用。

Method: 提出IPTQ-ViT框架，设计基于多项式的GELU和基于位移的Softmax近似函数，并引入统一指标综合考虑量化敏感性、扰动和计算成本，以逐层选择最优近似函数。

Result: 在图像分类任务中相比现有PTQ方法最高提升6.44% top-1精度（平均1.78%），目标检测提升1.0 mAP；在W8A8和W4A8配置下优于部分浮点PTQ方法，精度与延迟接近整数量化感知训练方法。

Conclusion: IPTQ-ViT实现了无需重训练的全整数量化视觉Transformer，有效平衡了精度、效率与实现复杂度，适用于资源受限场景。

Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.

</details>


### [68] [Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training](https://arxiv.org/abs/2511.15379)
*Yunjiao Zhou,Xinyan Chen,Junlang Qian,Lihua Xie,Jianfei Yang*

Main category: cs.CV

TL;DR: 本文提出ZOMG，一种零样本、开放词汇的运动序列语义分割框架，无需标注或微调即可将动作分解为细粒度子动作，在多个数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有动作分割方法依赖密集标注和预定义类别，难以适用于开放词汇的真实场景，因此需要一种无需监督的通用解决方案。

Method: ZOMG结合语言语义划分（利用大语言模型将指令分解为有序子动作）和软掩码优化（学习实例特定的时间掩码以聚焦关键帧，同时保持段内连续性和段间分离性），在不修改预训练编码器的前提下实现动作分割。

Result: 在HumanML3D等三个运动-语言数据集上，ZOMG在动作检索任务中比先前方法提升+8.7% mAP，且在下游检索任务中表现显著更优。

Conclusion: ZOMG实现了无需标注的动作语义分割，为开放词汇、无监督的运动理解提供了新范式。

Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.

</details>


### [69] [Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models](https://arxiv.org/abs/2511.15390)
*Haidong Kang,Lihong Lin,Enneng Yang,Hongning Dai,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AutoPrune的新型剪枝方法，利用大语言模型自身设计最优剪枝算法，结合图驱动的思维链（GCoT）和偏态感知动态稀疏分配（SDSA），解决了高剪枝率下的性能下降问题，在无需专家知识的情况下实现了高效、可解释的自动化剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型的剪枝方法依赖人工设计，成本高且需要专家知识，同时在高剪枝比例下因异常值问题导致性能严重下降，缺乏自适应稀疏性设计机制。

Method: 提出AutoPrune，利用大语言模型自我设计剪枝算法；引入图驱动的思维链（GCoT）优化提示以增强推理过程；设计偏态感知动态稀疏分配（SDSA）解决异常值引起的性能退化问题。

Result: 在主流大语言模型基准上进行了广泛实验，结果显示AutoPrune在高剪枝比下显著优于现有最先进方法，有效缓解性能下降并提升可解释性。

Conclusion: AutoPrune首次实现大语言模型自主剪枝，克服了对专家知识的依赖和异常值导致的性能瓶颈，为高效、自动化模型压缩提供了新范式。

Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.

</details>


### [70] [ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation](https://arxiv.org/abs/2511.15396)
*Simon Boeder,Fabian Gigengack,Simon Roesler,Holger Caesar,Benjamin Risse*

Main category: cs.CV

TL;DR: ShelfOcc是一种纯视觉方法，通过在原生3D空间中生成度量一致的语义体素标签，实现无需LiDAR的真实3D监督，显著提升了弱/自监督占用估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D投影或渲染监督的方法存在几何不一致和严重深度渗漏问题，且依赖LiDAR或额外传感器，限制了真实3D场景理解的鲁棒性。

Method: ShelfOcc利用视频生成3D语义体素标签作为监督信号，引入专门框架跨帧过滤并累积静态几何信息，处理动态内容，并将语义信息传播到稳定的体素表示中，从而实现高质量的3D监督。

Result: 在Occ3D-nuScenes基准上，ShelfOcc大幅超越此前所有弱/自监督方法，相对性能提升高达34%。

Conclusion: 高质量的3D监督对鲁棒的占用学习至关重要，ShelfOcc提出了一个不依赖LiDAR的数据驱动新方向，为3D场景理解提供了与架构创新互补的重要路径。

Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.

</details>


### [71] [Controlling False Positives in Image Segmentation via Conformal Prediction](https://arxiv.org/abs/2511.15406)
*Luca Mossina,Corentin Friedrich*

Main category: cs.CV

TL;DR: 提出一种无需重新训练的后处理框架，通过共形预测为预训练分割模型生成具有统计保证的置信掩码，控制假阳性率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分割中缺乏对错误的显式统计保证，尤其在临床决策中过度分割可能导致严重后果。

Method: 基于预训练分割模型，构建通过提高分数阈值或形态学腐蚀得到的嵌套收缩掩码族，利用标注的校准集通过共形预测选择收缩参数，以实现图像级假阳性的分布无关控制。

Result: 在息肉分割基准上验证了方法的有效性，实证结果显示能稳定控制假阳性率在用户指定容限内，且具备有限样本下的理论保证。

Conclusion: 该方法是模型无关、无需重训练的后处理方案，为临床应用中的风险敏感型语义分割提供了实用且可靠的统计保障。

Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.

</details>


### [72] [D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models](https://arxiv.org/abs/2511.15411)
*Wenlun Zhang,Yunshan Zhong,Zihao Ding,Xinyu Li,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: 本文提出了D4C，首个针对CLIP模型的无数据量化（DFQ）框架，通过提示引导语义注入、结构对比生成和扰动感知增强三个组件生成语义丰富且结构多样的伪图像，显著提升了CLIP在无真实数据情况下的量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有DFQ方法在直接应用于CLIP等视觉-语言模型时表现不佳，主要由于合成样本语义不足和图像内多样性低，限制了其在隐私敏感场景中的应用。

Method: 提出D4C框架，包含三个核心组件：1）提示引导语义注入，利用文本提示对齐生成图像与真实语义；2）结构对比生成，通过前景-背景对比学习重建自然图像的组成结构；3）扰动感知增强，引入可控扰动提升样本多样性和鲁棒性。

Result: 实验表明D4C在多种比特宽度和模型上均显著提升性能。例如，在W4A8设置下，CLIP ResNet-50和ViT-B/32在CIFAR-10、CIFAR-100和ImageNet-1K上的零样本分类准确率分别最高提升12.4%、19.7%和5.7%。

Conclusion: D4C有效解决了现有DFQ方法在CLIP模型上语义缺失和多样性不足的问题，首次实现了高性能的无数据量化视觉-语言模型，推动了其在隐私保护场景下的部署应用。

Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

</details>


### [73] [WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes](https://arxiv.org/abs/2511.15429)
*Marc-Emmanuel Coupvent des Graviers,Hejer Ammar,Christophe Guettier,Yann Dumortier,Romaric Audigier*

Main category: cs.CV

TL;DR: WarNav是一个针对冲突地区非结构化环境中自主地面车辆导航的新型真实世界语义分割数据集，填补了传统城市驾驶数据与高风险作战环境之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集主要针对城市环境，难以应对战区等极端、危险和复杂场景中的导航需求，缺乏适用于此类高风险环境的基准数据集。

Method: 基于开源DATTALION库构建WarNav数据集，解决数据异质性和伦理问题；使用在城市场景上训练的先进语义分割模型进行跨域基准测试，并探索无目标域标注情况下的可导航性建模方法。

Result: 提供了多个先进模型在WarNav上的基线性能，揭示了训练环境对模型表现的影响，并提出了在无标注目标图像条件下提升复杂环境可导航性的初步方案。

Conclusion: WarNav为高风险、标注稀缺场景下的自主导航研究提供了重要资源，推动鲁棒、安全且低依赖标注数据的自动驾驶系统发展。

Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.

</details>


### [74] [Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection](https://arxiv.org/abs/2511.15433)
*YiKang Shao,Tao Shi*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态目标检测方法RSC-MD，以解决融合退化问题，通过理论分析揭示了梯度抑制和模态不平衡两个关键缺陷，并设计了相应的模块来改善各模态的优化。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了多模态融合中的融合退化问题，且缺乏对其成因的理论分析。本文旨在填补这一空白，系统地研究融合退化的根源并提出解决方案。

Method: 提出了表示空间约束学习与模态解耦（RSC-MD）方法，包含RSC模块用于放大被抑制的梯度，以及MD模块用于消除模态间耦合干扰和模态不平衡。

Result: 在FLIR、LLVIP、M3FD和MFAD数据集上的实验表明，该方法有效缓解了融合退化，在多个基准上达到了最先进的性能。

Conclusion: RSC-MD通过解决梯度抑制和模态不平衡问题，实现了各模态主干网络的充分优化，显著提升了多模态检测性能。

Abstract: Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.

</details>


### [75] [A Dataset and Baseline for Deep Learning-Based Visual Quality Inspection in Remanufacturing](https://arxiv.org/abs/2511.15440)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: 提出了一种新的图像数据集，用于评估分类模型在齿轮箱组件质量检测中的泛化能力，并通过对比正则化损失提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在自动化视觉检测中潜力巨大，但在新产品变体、组件或缺陷模式上的泛化能力有限，需要提高模型的适应性。

Method: 构建了一个包含两种汽车变速箱典型齿轮组件的新图像数据集，涵盖良品与缺陷品；通过不同的训练-测试划分引入分布偏移，以评估模型泛化能力，并提出一种对比正则化损失来增强模型鲁棒性。

Result: 实验结果表明，所提出的对比正则化损失能有效提升模型对未见组件类型的泛化能力。

Conclusion: 该数据集可用于基准测试，所提出的损失函数有助于提升视觉检测模型在实际再制造场景中的适用性和稳定性。

Abstract: Remanufacturing describes a process where worn products are restored to like-new condition and it offers vast ecological and economic potentials. A key step is the quality inspection of disassembled components, which is mostly done manually due to the high variety of parts and defect patterns. Deep neural networks show great potential to automate such visual inspection tasks but struggle to generalize to new product variants, components, or defect patterns. To tackle this challenge, we propose a novel image dataset depicting typical gearbox components in good and defective condition from two automotive transmissions. Depending on the train-test split of the data, different distribution shifts are generated to benchmark the generalization ability of a classification model. We evaluate different models using the dataset and propose a contrastive regularization loss to enhance model robustness. The results obtained demonstrate the ability of the loss to improve generalisation to unseen types of components.

</details>


### [76] [Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras](https://arxiv.org/abs/2511.15459)
*Ziyan Liu,Qi Su,Lulu Tang,Zhaofei Yu,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出EASD，一种用于脉冲相机的端到端目标检测方法，并构建首个面向驾驶场景的模拟脉冲检测基准DSEC Spike。


<details>
  <summary>Details</summary>
Motivation: 传统图像传感器在快速运动和极端光照下存在运动模糊和饱和问题，而脉冲相机虽具有微秒级延迟和超高动态范围，但其稀疏离散输出难以被标准检测器处理。

Method: 采用双分支设计：基于时间的纹理与特征融合分支用于全局跨片段语义，熵选择性注意力分支用于物体中心细节；并引入DSEC Spike数据集。

Result: 实现了针对脉冲相机数据的端到端目标检测，在自动驾驶场景下有效利用脉冲流数据。

Conclusion: EASD框架能有效处理脉冲相机的稀疏异步输出，在自动驾驶相关条件下提升了检测性能。

Abstract: Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.

</details>


### [77] [SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome](https://arxiv.org/abs/2511.15464)
*Dabin Jeong,Amirhossein Vahidi,Ciro Ramírez-Suástegui,Marie Moullet,Kevin Ly,Mohammad Vali Sanian,Sebastian Birk,Yinshui Chang,Adam Boxall,Daniyal Jafree,Lloyd Steele,Vijaya Baskar MS,Muzlifah Haniffa,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: 提出Sigmma，一种多模态对比对齐框架，通过多尺度对齐和图表示学习，提升HE图像与空间转录组的跨模态表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在单一尺度上对齐HE图像与空间转录组数据，忽略了细粒度细胞结构及其空间组织。

Method: 设计多尺度对比对齐机制，并将细胞相互作用建模为图结构，整合子图内外关系，实现多层次跨模态表征学习。

Result: 在基因表达预测任务上平均提升9.78%，跨模态检索任务上平均提升26.93%，并在下游分析中展现出良好的多组织结构学习能力。

Conclusion: Sigmma能更有效地捕捉跨模态对应关系，在计算病理学中具有更强的多尺度表征学习优势。

Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.

</details>


### [78] [Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners](https://arxiv.org/abs/2511.15468)
*Xabier Lekunberri,Ahmad Kamal,Izaro Goienetxea,Jon Ruiz,Iñaki Quincoces,Jaime Valls Miro,Ignacio Arganda-Carreras,Jose A. Fernandes-Salvador*

Main category: cs.CV

TL;DR: 本研究提出了一种结合YOLOv9-SAM2与分层分类的多阶段管道，用于基于电子监控图像准确估计金枪鱼捕捞中的物种组成，显著降低了人工分析负担，并在交叉验证和实际操作测试中实现了84.8%的个体分割与分类率及4.5%的平均误差。


<details>
  <summary>Details</summary>
Motivation: 电子监控系统在金枪鱼渔业中产生大量视频数据，传统人工分析效率低，而现有AI模型在物种识别（尤其是大眼金枪鱼与黄鳍金枪鱼区分）上因训练数据不足和专家判别难度大而表现受限，亟需提高自动化识别的准确性与泛化能力。

Method: 构建一个多阶段处理流程：首先比较三种分割方法（Mask R-CNN、DINOv2+SAM2、YOLOv9+SAM2），选用性能最佳者进行鱼类个体分割；使用ByteTrack实现个体追踪；在分类阶段对比标准多类分类与分层分类方法，并基于船上观察员提供的真实标签数据集进行交叉验证和独立测试。

Result: 专家间对大眼金枪鱼和黄鳍金枪鱼的识别一致率分别为42.9%±35.6%和57.1%±35.6%，表明物种区分本身存在高难度；YOLOv9-SAM2在分割任务中表现最优（mAP 0.66±0.03，召回率0.88±0.03）；分层分类模型比标准模型具有更好的泛化能力；最终组合方案实现了84.8%的个体被成功分割与分类，平均误差为4.5%。

Conclusion: 结合YOLOv9-SAM2与分层分类的多阶段AI管道能有效提升电子监控数据中金枪鱼物种组成的估计精度，具备在实际渔业管理中部署的潜力，可减少对人工标注的依赖并提高报告准确性。

Abstract: Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.

</details>


### [79] [RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection](https://arxiv.org/abs/2511.15476)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出了一种混合深度学习框架RS-CA-HSICT，结合CNN与Transformer优势，用于提升猴痘（MPox）检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉猴痘图像的多尺度特征、局部细节和长距离依赖关系方面存在不足，需提升检测精度与鲁棒性。

Method: 设计了HSICT模块整合CNN主干与改进的ICT块，引入残差CNN、空间CNN和通道增强（CA）结构，并采用通道融合与注意力机制优化特征选择。

Result: 在Kaggle基准和自建MPox数据集上分别达到98.30%的准确率和98.13%的F1分数，优于现有CNN和ViT模型。

Conclusion: RS-CA-HSICT能有效融合全局上下文与局部纹理信息，显著提升MPox图像分类性能，具备临床辅助诊断潜力。

Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.

</details>


### [80] [FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI](https://arxiv.org/abs/2511.15481)
*Luisa Gallée,Yiheng Xiong,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 提出了一种名为FunnyNodules的全参数化合成数据集，用于系统分析医学AI模型中的基于属性的推理能力。


<details>
  <summary>Details</summary>
Motivation: 密集标注且包含诊断推理过程的医学图像数据集稀缺，限制了可解释AI模型的发展和评估。

Method: 设计了一个生成抽象肺结节形状的合成数据集，具有可控的视觉属性（如圆形度、边缘锐利度和毛刺），并通过预定义的属性组合确定目标类别，实现对决策规则的完全控制。

Result: 展示了FunnyNodules可用于模型无关的评估，以检验模型是否学习到正确的属性-目标关系，并解释属性预测的表现偏差及注意力与属性特定区域的一致性。

Conclusion: FunnyNodules提供了完整的真值信息，是一个多功能平台，可用于开发、基准测试和深入分析医学图像分析中的可解释AI方法。

Abstract: Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.

</details>


### [81] [Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels](https://arxiv.org/abs/2511.15496)
*Maria Pilligua,David Serrano-Lozano,Pai Peng,Ramon Baldrich,Michael S. Brown,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 本文提出了一个名为MILL的多光照低光图像数据集，用于评估和改进低光增强算法在不同光照强度下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强方法通常依赖单一低光条件下的配对训练数据，缺乏对不同光照强度下算法性能的全面理解。

Method: 构建了一个包含多种光照强度、固定相机设置和精确照度测量的MILL数据集，并基于该数据集对先进方法进行基准测试，提出提升算法在多光照条件下鲁棒性的改进方案。

Result: 实验表明现有方法在不同光照强度下性能波动显著；所提改进方法在Full HD图像上分别实现最高10 dB（DSLR）和2 dB（智能手机）的PSNR提升。

Conclusion: MILL数据集为低光图像增强提供了更全面的评估平台，所提出的改进策略有效增强了算法在多样光照条件下的鲁棒性。

Abstract: Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.

</details>


### [82] [Learning to Expand Images for Efficient Visual Autoregressive Modeling](https://arxiv.org/abs/2511.15499)
*Ruiqing Yang,Kaixin Zhang,Zheng Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: 提出了一种名为Expanding Autoregressive Representation (EAR)的新生成范式，通过从图像中心螺旋向外展开标记并采用长度自适应解码策略，实现了高效、高质量的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视觉生成方法由于逐标记解码或多尺度表示的复杂性而效率低下，需要一种更高效且符合人类视觉感知模式的生成方式。

Method: 设计了EAR框架，按照从中心到外围的螺旋顺序展开图像标记，并引入长度自适应解码策略，在每一步动态调整预测标记数量，支持并行解码。

Result: 在ImageNet上实验表明，EAR在单尺度自回归模型中实现了最先进的保真度与效率权衡，显著降低计算成本并提升生成质量。

Conclusion: EAR通过模拟人类视觉系统的感知模式，为可扩展且认知对齐的自回归图像生成提供了新方向。

Abstract: Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.

</details>


### [83] [Multi-Text Guided Few-Shot Semantic Segmentation](https://arxiv.org/abs/2511.15515)
*Qiang Jiao,Bin Yan,Yi Yang,Mengrui Shi,Qiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于多文本引导的少样本语义分割网络MTGNet，通过融合多样化的文本提示来增强文本先验，并优化跨模态视觉先验，提升了复杂类别下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP-based少样本分割方法依赖单一文本提示，难以覆盖复杂类别的语义多样性，且缺乏跨模态交互，易受支持集噪声影响，导致目标区域激活不完整。

Method: 提出MTGNet，包含三个模块：1）多文本先验精炼（MTPR）模块，融合多个文本描述以增强前景激活和语义覆盖；2）文本锚点特征融合（TAFF）模块，利用多文本嵌入作为语义锚点传递判别性局部原型；3）前景置信加权注意力（FCWA）模块，利用支持集内部自相似性抑制干扰区域，提升视觉先验鲁棒性。

Result: 在PASCAL-5i上1-shot mIoU达到76.8%，COCO-20i上达到57.4%，尤其在高类内差异场景下表现显著提升。

Conclusion: MTGNet通过多文本引导和跨模态优化有效增强了少样本语义分割中语义覆盖与视觉先验质量，显著提升了分割性能，特别是在结构复杂和类内差异大的类别上。

Abstract: Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.

</details>


### [84] [A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture](https://arxiv.org/abs/2511.15535)
*Pandiyaraju V,Abishek Karthik,Sreya Mynampati,Poovarasan L,D. Saraswathi*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN、ViT和GNN的混合深度学习框架，用于在复杂田间条件下实现高精度杂草检测，结合GAN增强与自监督预训练，在多基准数据集上达到99.33%的准确率。


<details>
  <summary>Details</summary>
Motivation: 精准农业中需要高效、可持续的杂草识别方法，以减少除草剂滥用并提升作物管理精度。

Method: 提出融合卷积神经网络、视觉Transformer和图神经网络的混合框架，采用GAN进行数据增强，并利用自监督对比预训练提升小样本下的特征学习能力。

Result: 在多基准数据集上实现了99.33%的准确率、精确率、召回率和F1分数，模型具备良好的泛化性、可解释性和边缘设备部署能力。

Conclusion: 该框架能有效整合局部、全局和关系特征，支持实时、高效的田间杂草检测，推动可持续精准农业发展。

Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.

</details>


### [85] [Scriboora: Rethinking Human Pose Forecasting](https://arxiv.org/abs/2511.15565)
*Daniel Bermuth,Alexander Poeppel,Wolfgang Reif*

Main category: cs.CV

TL;DR: 本文评估了多种人体姿态预测算法，在绝对姿态预测任务中揭示了许多可复现性问题，并提出了统一的训练与评估流程；通过借鉴语音理解模型，提升了当前最优性能，并在含噪声的真实估计姿态上测试模型鲁棒性，发现性能显著下降，但可通过无监督微调部分恢复。


<details>
  <summary>Details</summary>
Motivation: 现有姿态预测方法存在可复现性问题，且缺乏在真实噪声（如姿态估计算法输出）下的鲁棒性评估，限制了其在实际场景中的应用。

Method: 借鉴语音理解模型的设计思路，将其有效迁移到姿态预测任务中；构建统一的训练与评估框架，并引入基于姿态估计算法生成噪声的新数据集变体，评估模型在噪声输入下的表现，探索无监督微调的恢复能力。

Result: 改进后的语音启发模型在绝对姿态预测上达到当前最优性能；在含噪声的估计姿态输入下，模型性能显著下降，但通过无监督微调可部分恢复性能。

Conclusion: 将语音模型思想迁移至姿态预测是有效的，能提升预测性能；未来研究需关注模型在真实噪声下的鲁棒性，而无监督微调是提升实用性的可行方向。

Abstract: Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.

</details>


### [86] [Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector](https://arxiv.org/abs/2511.15571)
*Weiheng Zhu,Gang Cao,Jing Liu,Lifang Yu,Shaowei Weng*

Main category: cs.CV

TL;DR: 提出了一种双域特征重要性攻击（DuFIA）方案，通过联合建模空间和频域特征重要性来生成对抗样本，有效降低AI生成图像检测器的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有AIGI检测器在清洁条件下表现良好，但在面对对抗性攻击时的安全性尚未充分研究，需开发更先进的攻击方法以评估其鲁棒性。

Method: 利用空间插值梯度和频率感知扰动捕捉 forensically 重要的特征，并融合空间域与频率域的特征重要性指导对抗样本的优化生成。

Result: 实验表明DuFIA在多种AIGI检测器上具有良好的跨模型迁移性、透明性和鲁棒性。

Conclusion: DuFIA能有效削弱AIGI检测器的性能，揭示了当前检测器在面对双域对抗攻击时的脆弱性，有助于提升未来检测器的安全设计。

Abstract: Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.

</details>


### [87] [From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers](https://arxiv.org/abs/2511.15572)
*Huiyuan Tian,Bonan Xu,Shijian Li,Xin Jin*

Main category: cs.CV

TL;DR: 本文分析了特征图知识蒸馏在Vision Transformers（ViT）中效果不佳的原因，提出了一种基于token级谱能量模式（SEP）的分析方法，揭示了全局低秩但局部高带宽的编码特性导致教师-学生模型间的特征对齐困难。基于此，作者提出了两种简单的解决方案：后处理特征提升和原生宽度对齐，显著提升了ViT的知识蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 特征图知识蒸馏在卷积网络中有效，但在Vision Transformers中常失效。为了理解这一现象并指导方法设计，需要深入分析ViT的表示特性。

Method: 采用两视角表示分析：一是对完整特征矩阵进行逐层奇异值分解（SVD），分析全局低秩性；二是引入token级谱能量模式（SEP）分析，研究各token如何使用通道容量。基于分析结果，提出两种策略：轻量级投影器的后处理特征提升，以及仅扩展学生模型最后一块的原生宽度对齐。

Result: 在ImageNet-1K上，所提方法使DeiT-Tiny在CaiT-S24指导下准确率从74.86%提升至77.53%和78.23%，同时提升了无教师训练的学生模型性能。

Conclusion: ViT特征蒸馏失败源于编码不匹配，尽管整体表示低秩，但单个token广泛使用通道导致高带宽编码。利用低秩结构设计对齐策略可有效恢复蒸馏效果，为紧凑型ViT设计提供了可解释的指导。

Abstract: Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\%/95\%/90\%/80\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\%$ to $77.53\%$ and $78.23\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.

</details>


### [88] [AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning](https://arxiv.org/abs/2511.15578)
*Urjitkumar Patel,Fang-Chun Yeh,Chinmay Gondhalekar*

Main category: cs.CV

TL;DR: AVATAAR是一个模块化、可解释的框架，结合全局与局部视频上下文，通过反馈循环实现类人迭代推理，在长视频问答中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在处理需要全面理解和细致分析的复杂视频问题时存在不足，难以有效应对细粒度和语义丰富的查询。

Method: 提出AVATAAR框架，包含持久性全局摘要、预检索思考代理、重构模块及二者之间的反馈循环，融合全局与局部上下文进行逐步推理和检索策略优化。

Result: 在CinePile基准上，相较于基线模型，AVATAAR在时间推理、技术问题、主题提问和叙事理解方面分别取得+5.6%、+5%、+8%和+8.2%的相对增益，各模块均有正向贡献，反馈循环对适应性至关重要。

Conclusion: AVATAAR有效提升了长视频理解与问答能力，兼具准确性、可解释性和可扩展性，为长视频QA提供了可扩展的解决方案。

Abstract: With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.

</details>


### [89] [US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery](https://arxiv.org/abs/2511.15600)
*Miruna-Alexandra Gafencu,Yordanka Velikova,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 提出一种多模态深度学习方法，利用单张X射线图像补全3D超声中被遮挡的脊椎结构，克服超声成像中骨性声影导致的解剖结构缺失问题。


<details>
  <summary>Details</summary>
Motivation: 超声在脊柱手术中具有无辐射、低成本和实时成像的优势，但因骨组织产生声影而难以完整显示椎体结构，限制了其应用。

Method: 设计一种融合X射线与3D超声的多模态深度学习模型，通过模拟配对训练数据（2D侧位X射线视图和部分可见的3D椎体表示）进行训练，实现对超声中缺失椎体结构的补全。

Result: 在模拟实验中显著提升了椎体重建精度（p < 0.001），优于现有最先进方法，并实现了无需与CT等术前影像配准的完整腰椎三维可视化叠加。

Conclusion: 结合单张X射线投影可有效弥补超声在脊柱成像中的关键缺陷，同时保留其作为主要成像方式的优势，为术中导航提供了更准确、完整的实时可视化方案。

Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete

</details>


### [90] [CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking](https://arxiv.org/abs/2511.15580)
*Sifan Zhou,Yichao Cao,Jiahao Nie,Yuqian Fu,Ziyu Zhao,Xiaobo Lu,Shuo Wang*

Main category: cs.CV

TL;DR: 提出了一种名为CompTrack的端到端框架，用于解决LiDAR点云中3D单目标跟踪的空间和信息冗余问题，通过空间前景预测和动态令牌压缩模块实现高效准确的实时跟踪。


<details>
  <summary>Details</summary>
Motivation: 点云固有的稀疏性导致背景噪声带来的空间冗余和前景内部的信息冗余，限制了现有跟踪器的性能和效率。

Method: 设计了空间前景预测（SFP）模块以基于信息熵滤除背景噪声，并提出了基于信息瓶颈的动态令牌压缩（IB-DTC）模块，利用在线SVD分析对前景进行自适应压缩，生成紧凑且高信息量的代理令牌。

Result: 在KITTI、nuScenes和Waymo数据集上实验表明，CompTrack在保持高性能的同时达到90 FPS的实时运行速度。

Conclusion: CompTrack有效解决了点云跟踪中的双重冗余问题，在精度和效率之间实现了良好平衡，适用于实际自动驾驶场景。

Abstract: 3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.

</details>


### [91] [Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.15633)
*Tao Hu,Lan Li,Zhen-Hao Xie,Da-Wei Zhou*

Main category: cs.CV

TL;DR: 提出HASTEN方法，通过超球空间中的层次语义树锚定来缓解基于CLIP的类增量学习中的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的类增量学习方法未能显式捕捉视觉与语言概念间的固有层次结构，导致细粒度类别特征漂移和灾难性遗忘。

Method: 利用外部知识图谱作为监督信号，将视觉与文本特征嵌入超球空间以保持层次结构；并通过将梯度投影到共享映射器的零空间来减少对先前任务的干扰。

Result: 在多个实验中，HASTEN持续优于现有方法，并提供统一的结构化表示。

Conclusion: HASTEN通过显式建模层次关系有效缓解了类增量学习中的灾难性遗忘问题，提升了模型的持续学习能力。

Abstract: Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.

</details>


### [92] [Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition](https://arxiv.org/abs/2511.15597)
*Xufei Wang,Junqiao Zhao,Siyue Tao,Qiwen Gu,Wonbong Kim,Tiantian Feng*

Main category: cs.CV

TL;DR: 本文提出了一种名为KDF+的新型持续学习框架，用于LiDAR地点识别，通过损失感知采样策略和回放增强机制有效缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR地点识别方法在适应新环境时容易遗忘先前知识，即存在灾难性遗忘问题，限制了其在持续学习场景中的应用。

Method: KDF+扩展了原有的KDF范式，引入损失感知采样策略，根据样本损失评估学习难度并按难度概率采样；同时设计回放增强机制，在新任务训练中微调记忆样本的损失，以加强长期知识保留。

Result: 在多个基准上的实验表明，KDF+ consistently 优于现有的持续学习方法，并能显著提升最先进框架的性能，具有稳定性和通用性。

Conclusion: KDF+通过合理的样本选择与记忆强化机制，有效缓解了LiDAR地点识别中的灾难性遗忘问题，为持续学习提供了高效且可集成的解决方案。

Abstract: LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.

</details>


### [93] [MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation](https://arxiv.org/abs/2511.15603)
*Bin Xie,Gady Agam*

Main category: cs.CV

TL;DR: 本文提出了一种名为MaskMed的新型医学图像分割方法，通过解耦分割头和全尺度感知可变形Transformer模块，实现了类无关掩码预测与类标签预测的分离，并利用共享对象查询和跨分辨率注意力机制，在降低内存消耗的同时提升了多尺度特征融合效果。该方法在AMOS 2022和BTCV数据集上显著超越nnUNet，分别提升2.0%和6.9%的Dice分数，达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统的医学图像分割方法使用点对卷积分割头，每个输出通道固定对应特定类别，这种刚性设计限制了特征共享和语义泛化能力。为此，本文旨在打破这一限制，提升模型的灵活性和泛化性能。

Method: 提出统一的解耦分割头，将多类预测分解为类无关的掩码预测和类标签预测，使用共享对象查询进行协同学习；同时引入全尺度感知可变形Transformer模块，使低分辨率编码器特征可通过可变形注意力机制关注全分辨率特征，实现高效且空间对齐的多尺度融合。

Result: 在AMOS 2022和BTCV两个医学图像分割任务中，MaskMed分别取得了比nnUNet高2.0%和6.9%的Dice分数，表现出卓越的分割性能，尤其在复杂结构和小目标分割上优势明显。

Conclusion: MaskMed通过解耦设计和高效的多尺度融合机制，有效提升了医学图像分割的性能与泛化能力，为未来通用医学分割架构提供了新思路。

Abstract: Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.

</details>


### [94] [The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification](https://arxiv.org/abs/2511.15622)
*Dante Francisco Wasmuht,Otto Brookes,Maximillian Schall,Pablo Palencia,Chris Beirne,Tilo Burghardt,Majid Mirmehdi,Hjalmar Kühl,Mimi Arandjelovic,Sam Pottie,Peter Bermant,Brandon Asheim,Yi Jin Toh,Adam Elzinga,Jason Holmberg,Andrew Whitworth,Eleanor Flatt,Laura Gustafson,Chaitanya Ryali,Yuan-Ting Hu,Baishan Guo,Andrew Westbury,Kate Saenko,Didac Suris*

Main category: cs.CV

TL;DR: 本文提出了SA-FARI，这是目前最大规模的开源野生动物多目标跟踪（MAT）数据集，包含来自四大洲99个物种的11,609个相机陷阱视频，时间跨度近十年，并提供了高质量的时空标注。该数据集填补了现有数据在物种多样性、地理覆盖和标注精度上的空白，为野外通用多动物跟踪模型的发展提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有的多动物跟踪数据集在规模、物种多样性和地理时空覆盖上存在局限，难以支持适用于广泛野生动物种群的通用跟踪模型训练与评估。因此，需要一个大规模、多样化且标注丰富的数据集来推动该领域发展。

Method: 作者构建了SA-FARI数据集，收集了2014至2024年间来自741个地点的11,609个相机陷阱视频，涵盖99个物种。所有视频均进行了密集标注，包括约94万个边界框、分割掩码和物种标签，形成约46小时的标注数据。同时发布了匿名化摄像头位置信息。研究还基于最先进的视觉-语言模型（如SAM 3）和纯视觉方法在检测与跟踪任务上进行了全面基准测试。

Result: SA-FARI成为首个兼具高物种多样性、多区域覆盖和高质量时空标注的大规模MAT数据集，共包含16,224个masklet身份和942,702个标注实例。基准实验表明，视觉-语言模型在物种特定和通用提示下均表现出潜力，但仍有提升空间。

Conclusion: SA-FARI为野生动物多动物跟踪提供了一个前所未有的开放基准，显著提升了数据规模与多样性，有望推动跨物种、跨区域的通用视觉模型在生态保护中的应用。

Abstract: Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\href{https://www.conservationxlabs.com/sa-fari}{\text{conservationxlabs.com/SA-FARI}}$.

</details>


### [95] [FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation](https://arxiv.org/abs/2511.15618)
*Tingrui Shen,Yiheng Zhang,Chen Tang,Chuan Ping,Zixing Zhao,Le Wan,Yuwang Wang,Ronggang Wang,Shengfeng He*

Main category: cs.CV

TL;DR: FlashMesh是一种快速且高保真的3D网格生成框架，通过预测-校正-验证范式加速自回归解码，利用网格数据的结构相关性实现多层级并行推测，显著提升生成速度与质量。


<details>
  <summary>Details</summary>
Motivation: 自回归模型逐标记生成3D网格导致推理速度慢，限制了在交互式和大规模应用中的实用性，因此需要一种更高效的生成方法。

Method: 提出FlashMesh框架，采用预测-校正-验证的范式，结合小时钟Transformer架构设计针对面、点、坐标层级的多标记推测解码策略，实现并行化生成。

Result: 实验表明，FlashMesh相比标准自回归模型最多实现2倍加速，同时提升了生成保真度。

Conclusion: 网格数据中的结构先验可被系统利用以同时加速并增强自回归生成，为高效3D内容生成提供了新方向。

Abstract: Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.

</details>


### [96] [GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI](https://arxiv.org/abs/2511.15658)
*Naomi Simumba,Nils Lehmann,Paolo Fraccaro,Hamed Alemohammad,Geeth De Mel,Salman Khan,Manil Maskey,Nicolas Longepe,Xiao Xiang Zhu,Hannah Kerner,Juan Bernabe-Moreno,Alexander Lacoste*

Main category: cs.CV

TL;DR: GEO-Bench-2 提出一个标准化的评估框架，用于全面评测地理空间基础模型（GeoFMs），涵盖多种任务和19个开源数据集，并通过能力分组帮助用户根据任务需求选择合适模型。


<details>
  <summary>Details</summary>
Motivation: 当前GeoFMs缺乏统一、标准的评估协议，导致模型比较困难，阻碍了方法创新和实际应用中的模型选择。

Method: 构建包含分类、分割、回归、目标检测和实例分割的多任务评估框架，使用19个开源数据集，并提出“能力”分组来按数据特征（如分辨率、波段、时序性）对模型进行归类评估。

Result: 实验表明没有单一模型在所有任务上都表现最优；自然图像预训练模型在高分辨率任务中表现好，而遥感专用模型（如TerraMind、Prithvi、Clay）在多光谱应用（如农业、灾害响应）中更优。

Conclusion: 模型性能高度依赖于任务需求、数据模态和约束条件，目前尚无通用的全能GeoFM，GEO-Bench-2为可复现、针对性的模型评估提供了开放平台，推动未来研究。

Abstract: Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.
  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.

</details>


### [97] [Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography](https://arxiv.org/abs/2511.15640)
*Shourov Joarder,Tushar Talukder Showrav,Md. Kamrul Hasan*

Main category: cs.CV

TL;DR: 提出MUSSE-Net，一种无监督、多阶段的深度学习框架，用于鲁棒且一致的超声应变弹性成像，显著提升应变估计性能与临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 超声应变弹性成像（USE）在临床诊断中具有重要价值，但受限于组织去相关噪声、缺乏真实标签以及不同形变条件下应变估计不一致等问题。

Method: 设计了一种残差感知、多阶段无监督序列深度学习框架MUSSE-Net，其核心是USSE-Net，采用多流编码器-解码器结构并行处理形变前后射频信号，引入上下文感知互补特征融合（CACFF）、三交叉注意力（TCA）瓶颈和交叉注意力融合（CAF）解码器，并结合一致性损失保证时间连贯性，第二阶段残差细化进一步提升精度与降噪。

Result: 在模拟数据和真实临床数据（BUET医学中心）上验证，MUSSE-Net在目标SNR（24.54）、背景SNR（132.76）、CNR（59.81）和弹性SNR（9.73）等指标上优于现有无监督方法，生成的应变图具有更高病灶对比度和显著噪声抑制效果。

Conclusion: MUSSE-Net有效克服了USE中的关键限制，实现了鲁棒、一致且临床可解释的应变估计，推动了无监督深度学习在超声弹性成像中的应用。

Abstract: Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.

</details>


### [98] [MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features](https://arxiv.org/abs/2511.15675)
*Sejuti Rahman,Swakshar Deb,MD. Sameer Iqbal Chowdhury,MD. Jubair Ahmed Sourov,Mohammad Shamsuddin*

Main category: cs.CV

TL;DR: 提出一种多频率图卷积网络（MF-GCN），结合眼动、音频和视频三模态数据，有效捕捉跨模态交互，显著提升抑郁症检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的模型主要关注低频信息，忽略了高频信号在抑郁检测中的潜在价值，限制了模型性能。

Method: 设计了多频率滤波器组模块（MFFBM），在图卷积网络中同时利用低频和高频信号，构建三模态（眼动、音频、视频）融合的MF-GCN框架。

Result: 在二分类任务中达到0.96敏感度和0.94 F2分数；三分类任务中敏感度0.79、特异度0.87；在CMDC数据集上敏感度0.95、F2分数0.96，均优于基线模型。

Conclusion: 所提出的三模态多频率框架能更全面地捕捉抑郁相关特征，具有优越的判别能力和良好的泛化性，适用于抑郁症的自动检测。

Abstract: Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.

</details>


### [99] [INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases](https://arxiv.org/abs/2511.15656)
*Edward Vendrow,Julia Chae,Rupa Kurinchi-Vendhan,Isaac Eckert,Jazlynn Hall,Marta Jarzyna,Reymond Miyajima,Ruth Oliver,Laura Pollock,Lauren Schrack,Scott Yanco,Oisin Mac Aodha,Sara Beery*

Main category: cs.CV

TL;DR: INQUIRE-Search是一个开源系统，利用自然语言交互式搜索生态图像数据库中的概念，显著提高科学发现的效率和规模。


<details>
  <summary>Details</summary>
Motivation: 现有生态工作流依赖元数据过滤或人工检查，难以大规模获取生物多样性图像中的次级信息。

Method: 开发了INQUIRE-Search系统，支持用自然语言在生态图像数据库中快速交互式搜索概念，并验证导出相关观测数据用于新科学分析。

Result: 相比传统方法更省时，通过五个案例研究展示了其在物种行为季节变化、野火后森林再生等多方面的科学应用潜力。

Conclusion: 该工具开启了一种高效、可扩展的科学发现新范式，呼吁科学家重新思考科研流程并发展新的实验设计与不确定性分析方法。

Abstract: Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.

</details>


### [100] [Hyperspectral Image Classification using Spectral-Spatial Mixer Network](https://arxiv.org/abs/2511.15692)
*Mohammed Q. Alkhatib*

Main category: cs.CV

TL;DR: 提出了一种轻量级深度学习模型SS-MixNet，用于高光谱图像分类，在极低标注数据（1%）下在两个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为解决高光谱图像分类中局部特征提取与长距离依赖建模的平衡问题，并在标注数据极少的情况下实现高效准确分类。

Method: 结合3D卷积层提取局部光谱-空间特征，使用两个并行MLP风格的mixer模块捕捉光谱和空间维度的长程依赖，并引入基于深度可分离卷积的注意力机制增强判别能力。

Result: 在QUH-Tangdaowan和QUH-Qingyun数据集上分别达到95.68%和93.86%的整体精度，显著优于2D-CNN、3D-CNN、IP-SWIN、SimPoolFormer和HybridKAN等对比方法。

Conclusion: SS-MixNet在有限监督条件下能有效实现准确且鲁棒的高光谱图像分类，兼具高性能与低计算开销，具有实际应用潜力。

Abstract: This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet

</details>


### [101] [First Frame Is the Place to Go for Video Content Customization](https://arxiv.org/abs/2511.15700)
*Jingxi Chen,Zongxia Li,Zhichao Liu,Guangyao Shi,Xiyang Wu,Fuxiao Liu,Cornelia Fermuller,Brandon Y. Feng,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 该研究发现视频生成模型将第一帧视为存储视觉实体的“概念记忆缓冲区”，而非仅仅是时空起点，利用这一特性仅用20-50个训练样本即可实现无需架构修改或大规模微调的鲁棒视频内容定制。


<details>
  <summary>Details</summary>
Motivation: 传统上认为视频第一帧仅是生成的起始点，但作者希望探索其是否在模型中具有更深层的作用，从而挖掘视频生成模型中被忽视的参考定制能力。

Method: 通过分析视频生成模型对第一帧的隐式处理机制，提出将其作为概念记忆缓冲区，并在此基础上设计仅需少量样本（20-50个）的定制方法，无需改变模型结构或进行大规模微调。

Result: 成功实现了在多种场景下的鲁棒且泛化的视频内容定制，验证了第一帧作为记忆缓冲区的有效性。

Conclusion: 视频生成模型隐式地将第一帧用作存储和复用视觉实体的记忆机制，这一发现揭示了现有模型中一种强大且被忽视的参考式定制能力。

Abstract: What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.

</details>


### [102] [GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization](https://arxiv.org/abs/2511.15705)
*Yikun Wang,Zuyan Liu,Ziyi Wang,Pengfei Liu,Han Hu,Yongming Rao*

Main category: cs.CV

TL;DR: 本文提出了GeoBench和GeoVista，前者是一个用于评估智能体模型地理定位能力的新基准，后者是一种能集成工具调用的智能体模型，在地理定位任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于图像操作工具，缺乏通用的智能体视觉推理模型；同时现有的地理定位基准不足以支持高分辨率图像和深度推理需求。

Method: 提出GeoBench基准测试集，包含全球照片、全景图和卫星图像；设计GeoVista模型，结合图像放大和网络搜索工具，并通过冷启动监督微调和强化学习进行训练，采用分层奖励机制提升性能。

Result: GeoVista在开源智能体模型中显著超越其他模型，在多数指标上性能接近Gemini-2.5-flash和GPT-5。

Conclusion: 集成工具调用与分层强化学习可有效提升智能体在复杂视觉推理任务（如地理定位）中的表现，推动通用多模态智能体的发展。

Abstract: Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.

</details>


### [103] [RoMa v2: Harder Better Faster Denser Feature Matching](https://arxiv.org/abs/2511.15706)
*Johan Edstedt,David Nordström,Yushan Zhang,Georg Bökman,Jonathan Astermark,Viktor Larsson,Anders Heyden,Fredrik Kahl,Mårten Wadenbäck,Michael Felsberg*

Main category: cs.CV

TL;DR: 提出了一种新的密集特征匹配方法，通过改进架构、损失函数、训练分布和优化流程，显著提升了匹配精度和效率，成为新的SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有密集匹配器在复杂真实场景下表现不佳，高精度模型通常速度慢，限制了应用。

Method: 设计了新的匹配架构与损失函数，采用多样化的训练分布，使用解耦的两阶段匹配-细化流程，并开发了自定义CUDA内核以减少内存占用，结合DINOv3基础模型增强鲁棒性和无偏性。

Result: 新方法在多个实验中显著优于先前方法，精度更高，训练更快，内存更高效。

Conclusion: 所提出的方法在密集特征匹配任务上实现了新的最先进性能，兼具高精度、高效率和强鲁棒性。

Abstract: Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [104] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过在推理时分配额外计算资源来提高预训练大语言模型预测准确性的技术，提出了一种基于问题分解方式和子问题拓扑结构的统一分类框架，并整合分析了现有方法的优缺点，指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了提升预训练大语言模型在推理阶段的预测准确性，探索如何有效利用额外计算资源的方法。

Method: 通过将测试时扩展方法按问题分解方式和子问题的拓扑结构（如顺序、并行、树状）进行分类，统一分析Chain-of-Thought、Branch-Solve-Merge和Tree-of-Thought等方法。

Result: 建立了一个统一的框架来理解不同的测试时扩展技术，并系统总结了各类方法的优势与局限性。

Conclusion: 该分类视角有助于深入理解现有方法的工作机制，为未来设计更高效的推理时扩展策略提供了指导方向。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [105] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型在生成链式思维（CoT）推理的最初几个标记后即可预测最终答案的正确性，表明模型内部对结果的承诺出现得非常早，尤其在较难问题中，长推理链存在选择偏差。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在链式思维推理过程中何时内部确定最终答案，以理解其推理稳定性和可解释性。

Method: 通过训练线性分类器，分析模型在生成前t个推理标记后的隐藏状态，预测最终输出的正确性，并比较不同难度问题下的表现。

Result: 发现仅用前几个推理标记就能高准确率预测最终答案是否正确；对于较难问题，预测准确率下降，揭示了长推理链中存在选择偏差。

Conclusion: 大型语言模型在推理初期就已大致确定答案，后续推理更多是形式化展开而非实质修正，这对模型可解释性和推理时控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [106] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出LiveCLKTBench，一个自动化生成管道，用于隔离并测量大语言模型中的跨语言知识迁移。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型中的跨语言知识迁移具有挑战性，因为目标语言中的正确回答可能源于真实的知识迁移或预训练期间的先前接触。需要一种方法来区分这两种情况。

Method: 构建LiveCLKTBench管道，识别现实世界领域中的自包含、时间敏感的知识实体，基于时间出现过滤，并验证模型知识；使用这些实体文档生成事实问题，并翻译成多种语言以评估跨语言迁移能力。

Result: 在五种语言上评估多个大语言模型，发现跨语言迁移受语言距离影响显著，且在不同语言方向上常呈不对称性；更大模型能提升迁移效果，但随规模增大增益递减，且在不同领域表现不一。

Conclusion: LiveCLKTBench能有效衡量跨语言知识迁移，研究结果揭示了多语言迁移的新见解，证明该基准对未来研究具有重要价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [107] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 本文提出了一种名为NAMeGEn的多智能体优化框架，用于解决中文婴儿命名这一创造性自然语言生成任务中的多目标灵活性和解释复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 由于用户需求多样化且生成内容需要具备审美解释能力，现有的大语言模型在短文本创造性生成方面表现有限，尤其是在中文婴儿命名这类任务中。

Method: 提出NAMeGEn框架，通过多个智能体迭代执行目标提取、名称生成与评估，并结合一个包含1.7万首古典诗词的语料库提升美学质量，同时构建了CBNames基准和定制化指标。

Result: 实验表明，NAMeGEn在无需训练的情况下，优于六种基于不同大模型基底的基线方法，能有效生成符合个性化要求且具有意义解释的创意名称。

Conclusion: NAMeGEn为短文本创造性生成提供了一个可解释、可定制的新范式，在中文命名任务上表现出优越性能，具有推广到其他CNLG任务的潜力。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [108] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: 提出COMPASS框架，通过上下文调制的PID注意力控制系统，在解码过程中动态调节注意力以减少大语言模型的幻觉，提升事实一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常生成流畅但不准确的内容，根源在于其在上下文和参数化知识间的注意力分配不当，需要可解释的方法来理解和控制这一行为。

Method: 引入Context Reliance Score（CRS）量化上下文依赖，并结合PID控制器在解码时动态调节注意力头，形成无需重训练或多次解码的轻量反馈系统。

Result: 在多个基准上显著降低幻觉率（绝对下降2.8%至5.8%），并揭示不同注意力头对证据对齐的贡献。

Conclusion: 基于反馈的可解释性方法有助于科学理解大模型行为，COMPASS为提升生成事实性提供了有效且可解释的控制路径。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [109] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 本文研究了在巴西葡萄牙语中，手动和自动韵律分割标注对FastSpeech 2非自回归模型合成自发语音质量的影响，结果表明引入韵律分割可略微提升语音的自然度和可懂度，且手动标注因提供更多变性而更接近自然语调。


<details>
  <summary>Details</summary>
Motivation: 自发语音合成面临诸多挑战，如停顿、话语转换和不流利现象，现有系统虽已取得进展，但明确韵律分割数据集的构建及其对合成效果的影响尚缺乏研究。

Method: 采用FastSpeech 2非自回归模型，对比使用手动与自动韵律分割标注进行训练的效果，并分析其在巴西葡萄牙语自发语音合成中的表现，重点评估韵律建模对核重音和前核语调轮廓的还原能力。

Result: 实验结果显示，使用韵律分割训练能略微提升合成语音的可懂度和声学自然度；自动分割产生更规整的片段，而手动分割带来更多变性，使韵律更自然；在中性陈述句中，两种方法均能再现预期的核重音模式，但手动分割模型更贴近自然的前核语调轮廓。

Conclusion: 引入显式韵律分割标注有助于提升自发语音合成的自然度，尤其是手动标注能更好地捕捉语调变化，未来应进一步探索高质量韵律标注在语音合成中的应用。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [110] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED是一个基于多智能体框架的标准化病人模拟系统，通过患者代理、辅助代理和评估代理实现逼真的对话、事实一致性和可操作反馈。实验表明其学习效果与真人相当，并在灵活性、心理安全和成本效率方面更优。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型驱动的标准化病人模拟器行为不一致，且缺乏与真人标准化病人的严谨对比，同时传统标准化病人成本高、难以扩展。

Method: 提出EasyMED多-agent框架，包含负责对话的患者代理、确保事实一致性的辅助代理和提供反馈的评估代理；并构建SPBench基准，涵盖14个专科的真实医患交互及8项专家定义的评估标准。

Result: EasyMED在学习成果上与人类标准化病人相当，对基础较弱的学生能带来更大的技能提升，并在灵活性、心理安全感和成本效益方面表现更优。

Conclusion: EasyMED是一种可扩展、低成本且高效的标准化病人模拟方案，在临床技能培训中具有广泛应用潜力。

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [111] [Opinion Mining and Analysis Using Hybrid Deep Neural Networks](https://arxiv.org/abs/2511.14796)
*Adel Hidri,Suleiman Ali Alsaif,Muteeb Alahmari,Eman AlShehri,Minyar Sassi Hidri*

Main category: cs.CL

TL;DR: 提出一种结合BGRU和LSTM的混合深度学习模型（HBGRU-LSTM），用于提升情感分析性能，尤其在处理上下文细微差别、可扩展性和类别不平衡方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理文本情感分析中的上下文复杂性、可扩展性和类别不平衡问题上存在局限，需要更强大的模型来提升准确性和鲁棒性。

Method: 提出一种混合深度神经网络模型HBGRU-LSTM，结合双向门控循环单元（BGRU）和长短期记忆网络（LSTM），在IMDB和Amazon等基准数据集上进行实验验证。

Result: 该模型在测试中达到95%的准确率，优于LSTM、CNN+LSTM和GRU+LSTM；负类召回率从86%提升至96%（平衡数据集），误分类损失从20.24%降至13.3%。

Conclusion: HBGRU-LSTM模型在情感分析任务中表现出更强的泛化能力和分类均衡性，尤其适用于处理不平衡数据和复杂语义结构的文本。

Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.

</details>


### [112] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: 提出层次化令牌前置（HTP）方法，通过分块摘要令牌和均值池化提升大语言模型在长文本上的嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的因果注意力机制限制了信息从后往前流动，导致表示质量下降；现有方法因过度压缩信息而不适用于长文档。

Method: 将输入分块，在每个后续块前添加块级摘要令牌，实现多路径反向信息流；用均值池化替代最后一令牌池化以缓解读出层的过压缩问题。

Result: HTP在11个检索数据集和30个通用嵌入基准上均取得一致性能提升，尤其在长上下文场景下表现突出。

Conclusion: HTP是一种简单、与架构无关的方法，可有效增强零样本和微调模型的长文档嵌入能力，具有良好的可扩展性。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [113] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 提出一个基于数学框架来理解、测量和缓解大语言模型中的幻觉问题，结合多种理论方法并开发了如对比解码、检索增强等策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然强大，但容易产生事实性错误的幻觉，需要系统性方法来识别和减少这些问题。

Method: 利用概率建模、信息论、三角信号分析和贝叶斯不确定性估计，分析错误在自回归过程中的累积，并提出改进的不确定性度量和缓解策略。

Result: 提出了语义和相位感知的不确定性度量，并开发了对比解码、检索增强、事实对齐和拒绝回答等原则性缓解方法。

Conclusion: 该统一框架将校准、检索和对齐的最新进展联系起来，有助于构建更安全、可靠的大语言模型。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [114] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 提出了一种名为TASA的学生感知辅导框架，通过整合个性、记忆和遗忘动态来实现个性化的数学学习，显著提升了基于大语言模型的辅导系统的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的智能辅导系统在捕捉学生知识动态演化方面存在不足，尤其是在数学辅导中需要精细的支架式教学。

Method: TASA框架结合了学生个性档案、事件记忆记录以及连续遗忘曲线与知识追踪技术，动态更新学生的掌握状态并生成适应性的问题和解释。

Result: 实验结果表明，TASA相比代表性基线方法实现了更优的学习成果和更自适应的教学行为。

Conclusion: 建模时间遗忘和学习者特征对提升基于大语言模型的辅导系统至关重要。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [115] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架来评估印度语言中的视觉-语言模型（VLM），并发布了HinTel-AlignBench基准，涵盖印地语和泰卢固语的多样化数据源及与英语对齐的样本。


<details>
  <summary>Details</summary>
Motivation: 当前多语言VLM评估存在依赖未经验证的自动翻译、任务/领域覆盖狭窄、样本量有限以及缺乏本土文化相关的问答等问题，亟需改进以推动低资源语言的公平AI发展。

Method: 提出一个结合反向翻译、过滤和人工验证的半自动化数据集构建框架，并构建HinTel-AlignBench基准，包含改编的英文数据集和原生印度数据集（如JEE和VAANI），每种语言约4000个问答对。

Result: 在5个任务中，4个任务的所有模型在印地语和泰卢固语上的性能均低于英语，平均下降分别为8.3分和5.5分，并识别出常见的失败模式。

Conclusion: 该研究揭示了现有VLM在印度语言上的性能退化问题，提供了全面的评估基准和改进建议，强调了文化与语言本地化在多模态理解中的重要性。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [116] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文首次系统研究了内在维度（ID）与可解释文本特征之间的关系，发现ID与基于熵的指标无关，表现出显著的体裁分层现象，并通过稀疏自编码器识别出影响ID的因果语言特征。


<details>
  <summary>Details</summary>
Motivation: 尽管内在维度（ID）在大模型分析中广泛应用，但其与文本特征的关系尚不清楚，因此需要探究ID的文本决定因素及其在不同语境下的意义。

Method: 采用交叉编码器分析、语言学特征提取和稀疏自编码器（SAE）相结合的方法，在多种大模型上分析不同文本的ID，并进行控制变量和干预实验以识别因果关系。

Result: 1) ID与熵类指标在控制长度后无相关性；2) 科学文本ID低（~8），百科中等（~9），创意/观点类文本ID高（~10.5）；3) SAE识别出正式语气、统计信息降低ID，而情感化、叙事性语言增加ID，且 Steering 实验验证了因果性。

Conclusion: ID反映的是表示几何复杂性而非预测难度，科学写作对当前LLM而言更具表示简洁性，而虚构、情感类文本引入更多自由度，研究为ID的正确使用提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [117] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: 提出OEMA，一种基于多智能体协作的零样本临床命名实体识别框架，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督模型需要昂贵的标注数据，而零样本NER在实例选择粒度和提示与自我改进集成方面存在困难。

Method: OEMA包含三个组件：自注释器生成示例，判别器通过SNOMED CT过滤示例，预测器利用实体描述进行准确推理。

Result: 在MTSamples和VAERS数据集上，OEMA在精确匹配上达到最先进性能；在相关匹配下，性能与监督式BioClinicalBERT相当且优于CRF。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督学习的性能，展现出在临床NLP应用中的潜力。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [118] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文提出了Context Cascade Compression (C3)，通过级联两个不同规模的LLM实现长文本上下文的高效压缩与解码，小模型将长文本压缩为少量潜在token，大模型负责解码，在20倍和40倍压缩比下分别达到98%和93%的解码准确率，显著优于DeepSeek-OCR，展示了纯文本压缩在长上下文任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 应对百万级token输入在长上下文任务中带来的计算与内存挑战，探索文本压缩的极限，并提供比光学字符压缩（OCR）更高效、更简单的解决方案。

Method: 采用级联架构：小规模LLM作为第一阶段将长上下文压缩为固定长度的潜在token（如32或64个），实现高比例压缩；大规模LLM作为第二阶段在压缩后的上下文上执行解码任务。使用纯文本管道，忽略布局、颜色和视觉编码带来的信息损失。

Result: 在20倍压缩比下，模型达到98%的解码准确率，显著高于DeepSeek-OCR的约60%；在40倍压缩比下仍保持约93%的准确率。结果表明C3在上下文压缩任务中具有更高的性能和可行性。

Conclusion: C3通过纯文本级联压缩架构实现了极高的上下文压缩比与解码准确性，不仅优于现有的光学字符压缩方法，还为未来OCR及相关领域提供了潜在的压缩上限参考，证明了高比压缩下的信息保留是可行的。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [119] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 提出一种符合航空维修规范的检索系统，通过结合LLM重排序与语义搜索，在保留现有认证手册查看器的基础上显著提升维修人员查找效率。


<details>
  <summary>Details</summary>
Motivation: 航空维修技术人员花费大量时间查阅手册，传统检索方式效率低下且需确保每项操作可追溯至认证来源，亟需在合规前提下提升检索效率。

Method: 系统基于ATA章节结构构建版本鲁棒的嵌入表示，利用视觉-语言解析技术结构化认证内容，并通过LLM重排序增强语义搜索，与现有认证查看器协同工作而非替代。

Result: 在4.9万个合成查询上实现超过90%的检索准确率；10名持证技师的双语对照实验显示前10命中率达90.9%，查找时间从6-15分钟缩短至18秒，减少95%。

Conclusion: 语义检索可在严格适航监管环境下安全运行，显著降低实际多语言MRO工作负载，为合规场景下的智能辅助提供了可行路径。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [120] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文报告了TeamNRC在BHASHA-Task 1语法错误纠正任务中针对5种印度语言的参与结果，采用零/少样本提示不同规模的语言模型，在Telugu和Hindi中分别取得第4和第2名的成绩。研究扩展至Tamil、Malayalam和Bangla，并探讨数据质量与评估指标问题，强调小规模语言模型的潜力及印度语言脚本数据集和度量标准构建中的挑战。


<details>
  <summary>Details</summary>
Motivation: 提升印度多种语言的语法错误纠正能力，探索适用于低资源语言的有效方法，并解决现有数据质量和评估指标的不足。

Method: 采用零样本和少样本提示技术，使用从40亿参数到大型专有模型的不同规模语言模型进行实验。

Result: 在Telugu和Hindi上分别获得83.78和84.31的GLEU分数，排名分别为第4和第2；并对Tamil、Malayalam和Bangla进行了扩展实验，分析了数据质量与评估指标的影响。

Conclusion: 小规模语言模型在印度语言语法纠错任务中具有巨大潜力，但需要更高质量的数据集和更适合印度语言脚本的评估指标。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [121] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语方言在酒店评论中的情感分析，采用SetFit框架进行少样本学习，在官方测试集上取得了73%的F1分数，排名第12位。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言情感分析面临语言多样性及标注数据稀缺的挑战，特别是在酒店等特定领域。

Method: 采用SetFit（Sentence Transformer Fine-tuning）框架，利用其数据高效的少样本学习能力对摩洛哥和沙特阿拉伯方言的酒店评论进行情感分类。

Result: 在AHaSIS共享任务的官方评测集上，系统达到73%的F1分数，位列26个参赛队伍中的第12名。

Conclusion: 研究表明，少样本学习方法在处理特定领域中资源稀缺的阿拉伯语方言文本时具有潜力。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [122] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文研究发现，对抗性诗歌可作为大语言模型的通用单轮越狱技术，在25个前沿模型中显著提高攻击成功率，揭示了当前安全对齐方法的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 探索风格变化（如诗歌）是否能绕过大型语言模型的安全机制，检验现有对齐方法的鲁棒性。

Method: 将1200个有害提示转化为诗歌形式，使用标准化元提示生成对抗性诗句，并在25个主流模型上测试攻击成功率；通过开源判别模型集成和人工验证评估输出。

Result: 手工创作的诗歌平均越狱成功率达62%，元提示转化版本达43%，部分厂商模型超过90%；相比散文基线最高提升18倍，且跨CBRN、网络攻击等多个风险领域有效。

Conclusion: 诗歌风格本身即可系统性绕过当前安全防护，表明现有对齐方法存在根本局限，需重新评估安全评测协议。

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [123] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2 是一个扩展更新的西班牙/英语医疗多选题推理数据集，包含超过12,000个问题，用于推动生物医学推理和模型改进研究。


<details>
  <summary>Details</summary>
Motivation: 应对高质量数据集在捕捉医疗领域语言和概念复杂性方面的迫切需求。

Method: 基于十年西班牙专业考试题目扩展数据集，构建多语言版本，并使用提示、RAG 和概率选择方法对多个开源大语言模型进行基准测试。

Result: 模型规模和内在推理能力是性能的主要驱动因素，复杂的推理策略增益有限。

Conclusion: HEAD-QA v2 是一个可靠的资源，有助于推进生物医学推理和语言模型的研究。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [124] [The Empowerment of Science of Science by Large Language Models: New Tools and Methods](https://arxiv.org/abs/2511.15370)
*Guoqiang Liang,Jingqian Gong,Mengxuan Li,Gege Lin,Shuo Zhang*

Main category: cs.CL

TL;DR: 本文综述了支持大语言模型（LLM）的核心技术，并探讨了LLM在科学计量学中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在自然语言处理、图像识别和多模态任务中展现出强大能力，其在通向通用人工智能（AGI）的道路上扮演关键角色，因此有必要从用户角度系统梳理其核心技术并探索其在科学研究评估中的潜力。

Method: 采用综述方法，涵盖提示工程、检索增强生成、微调、预训练和工具学习等LLM核心技术，并回顾科学学（SciSci）的发展历程，提出基于AI代理的科研评价模型，以及利用LLM进行研究前沿检测和知识图谱构建的方法。

Result: 总结了LLM核心技术和发展历程，提出了在 scientometrics 领域的新应用方向，包括AI驱动的科研评价、新研究前沿识别和知识图谱构建。

Conclusion: LLM不仅在技术层面持续进步，还将在科学发现和科研管理中发挥重要作用，推动科学学进入智能化新时代。

Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.

</details>


### [125] [DEPO: Dual-Efficiency Preference Optimization for LLM Agents](https://arxiv.org/abs/2511.15392)
*Sirui Chen,Mengshi Zhao,Lei Xu,Yuying Zhao,Beier Zhu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文提出了双效率（dual-efficiency）概念，包括步骤级效率和轨迹级效率，并提出DEPO方法，在减少token和步骤使用的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代理在推理时往往链路过长，影响交互效率，但缺乏对效率的系统性定义，限制了针对性优化。

Method: 提出双效率定义，并基于此设计DEPO——一种联合优化每步简洁性和总步数的偏好优化方法。

Result: 在WebShop和BabyAI上，DEPO最多减少60.9%的token使用和26.9%的步骤，同时性能最高提升29.3%，并在数学任务和小数据场景下表现出良好泛化性。

Conclusion: DEPO通过双效率优化，在保持甚至提升性能的同时显著提高了LLM代理的推理效率，具有良好的泛化能力和数据效率。

Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.

</details>


### [126] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: 本文介绍了IIT Madras SPRING实验室为ASRU MADASR 2.0挑战赛开发的系统，重点是通过多解码器架构和音素公共标签集（CLS）提升多语言多方言语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 提升在8种语言33种方言中语音识别的语言和方言识别准确率，并在限制使用额外数据的情况下构建从零开始的多语言系统。

Method: 采用基于音素公共标签集（CLS）的多解码器架构进行训练，并探索了从音素空间转换回字素表示时保留性能增益的方法。

Result: 在Track 2中，系统在3种语言上优于基线系统的WER/CER，并在所有参赛队伍中实现了最高的语言ID和方言ID识别准确率。

Conclusion: 所提出的多解码器与CLS方法有效提升了受限条件下的多语言多方言语音识别性能，尤其在语言和方言识别方面表现突出。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [127] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster 是一种全新的、完全基于大语言模型的文本聚类框架，通过引入动态记忆和双提示策略，实现了无需调参且端到端的高效聚类。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的文本聚类方法缺乏状态记忆和对聚类粒度的控制，依赖复杂外部模块，难以实现真正的端到端学习。

Method: 提出 LLM-MemCluster 框架，将聚类重构为纯 LLM 任务；采用动态记忆机制赋予模型状态感知能力，并设计双提示策略让模型自主推理聚类数量。

Result: 在多个基准数据集上验证，该无需微调的框架显著且稳定地优于强基线方法。

Conclusion: LLM-MemCluster 提供了一种有效、可解释且真正端到端的大语言模型文本聚类新范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [128] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了语言处理数据结构（LPDS）和pelican nlp工具包，旨在提高语言数据处理的标准化和可重复性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的发展，语言数据处理方法不断演进，但缺乏数据组织与共享的标准以及可复现的处理方法。

Method: 借鉴脑成像数据结构（BIDS），提出LPDS规范，并开发了模块化的Python工具pelican nlp，支持从数据清洗到特征提取的全流程处理。

Result: 实现了基于配置文件驱动的语言数据处理流程，支持预处理和多种语言及声学特征的标准化提取。

Conclusion: LPDS和pelican nlp共同构建了一个端到端的、透明且可复现的语言数据处理 pipeline。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [129] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文提出了Mera Multi，一个针对俄语的开源多模态评估框架，包含18个从零构建的任务，涵盖文本、图像、音频和视频模态，旨在系统评估多模态大语言模型的能力、局限与风险。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型迅速发展，但其智能性、局限性和风险尚不明确，尤其是在俄语等非英语语言中缺乏多模态评估基准。

Method: 构建了一个基于指令的多模态评估框架Mera Multi，提出通用的多模态能力分类体系，设计18个涵盖多种模态的新任务，并创建具有俄语文化与语言特性的数据集，采用统一提示和指标，并引入防止基准泄漏的方法（如水印和私有集授权）。

Result: 提供了闭源和开源模型的基线结果，验证了框架的有效性；数据集完全从零构建，强调俄语语言文化特性，并支持跨模态模型评估。

Conclusion: Mera Multi填补了俄语多模态评估的空白，其方法论可推广至其他斯拉夫语系乃至类型学上多样的语言，为多语言多模态模型评估提供了可复用的范式。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [130] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: 本文提出了HSKBenchmark，首个用于中文二语习得（SLA）中大语言模型（LLM）分阶段建模与写作评估的基准，涵盖HSK 3至6级，包含真实教材、合成数据和语言学驱动的评估体系，并通过课程调优框架模拟人类学习轨迹，实验表明该基准能有效建模中文SLA并支持LLM的动态写作评估。


<details>
  <summary>Details</summary>
Motivation: 由于无法在人类学习者身上控制语言输入，语言习得研究面临可验证性与可扩展性挑战，尤其是在中文二语习得领域，缺乏系统性基准来支持大语言模型的阶段性建模与评估。

Method: 构建了HSKBenchmark，包含HSK 3-6级的真实教材（676万token）、1.6万条合成指令、30个测试主题及语言学基础的评估系统；提出课程调优框架，按学习阶段训练模型；开发HSKAgent并在1万条学习者作文上微调；设计评估体系衡量语法覆盖、错误、复杂度与整体评分。

Result: 实验证明HSKBenchmark能有效建模中文二语习得过程，支持大语言模型的动态写作评估；微调后的模型写作水平接近高级人类学习者，表现出类人习得特征。

Conclusion: HSKBenchmark、HSKAgent及相关模型为语言习得建模与大语言模型可解释性研究提供了基础工具与资源，具有推动该领域未来发展的潜力。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


### [131] [Tokenisation over Bounded Alphabets is Hard](https://arxiv.org/abs/2511.15709)
*Violeta Kastreva,Philip Whittington,Dennis Komm,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文研究了在有限大小字母表上的分词问题，证明了即使在二元字母表上，自底向上和直接分词两种变体不仅是NP完全的，而且不存在多项式时间近似方案（除非P=NP），表明分词的计算困难性是根本性的，而非由大字母表引起。


<details>
  <summary>Details</summary>
Motivation: 先前工作指出分词是NP完全问题，但假设输入字母表无限大，这与实际中固定大小字母表（如字节或Unicode）不符。因此需要研究在有界字母表下的分词复杂性。

Method: 分析两种自然的分词变体：自底向上分词和直接分词，分别考虑选择合并操作序列或构建最优压缩数据集的词表，并在n元字母表（包括二元和一元）上进行复杂性证明。

Result: 证明在二元字母表上，两种分词变体均为NP完全且不可近似；直接分词在一元字母表上仍为NP完全，说明其计算困难性是本质的。

Conclusion: 分词的计算不可行性并非源于大字母表或复杂构造，而是根本性障碍，解释了为何BPE等实用算法需依赖启发式方法，并建议未来研究应关注近似算法。

Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [132] [Area-Optimal Control Strategies for Heterogeneous Multi-Agent Pursuit](https://arxiv.org/abs/2511.15036)
*Kamal Mammadov,Damith C. Ranasinghe*

Main category: cs.MA

TL;DR: 提出了一种基于安全可达集面积最小化的多智能体追逃博弈策略，通过阿波罗尼乌斯圆交集定义安全区域，并利用解析梯度得到各智能体的最优控制律，实现高效实时的协同围捕。


<details>
  <summary>Details</summary>
Motivation: 在异构速度的多追捕者与单个较慢逃脱者之间的追逃博弈中，需要一种有效的协同策略来确保对逃脱者的可靠捕获。传统方法可能缺乏明确的几何目标或实时性不足，因此本文旨在提出一种具有清晰几何解释且可实时计算的协作策略。

Method: 定义逃脱者的安全可达集为每个追捕者-逃脱者对所对应的阿波罗尼乌斯圆的交集；将追捕策略建模为零和博弈，追捕者合作最小化该集合的面积，而逃脱者试图最大化它；通过推导安全可达集面积关于智能体位置的解析梯度，得到各智能体航向的闭式瞬时最优控制律。

Result: 仿真结果表明，基于梯度的控制能够有效引导追捕者系统性地缩小逃脱者的安全区域，最终实现确定性捕获；所提方法计算效率高，适用于实时实现。

Conclusion: 该面积最小化方法为多追捕者协同围捕提供了一个清晰的几何优化目标，结合解析控制律实现了高效、实时且保证捕获性能的追逃策略。

Abstract: This paper presents a novel strategy for a multi-agent pursuit-evasion game involving multiple faster pursuers with heterogenous speeds and a single slower evader. We define a geometric region, the evader's safe-reachable set, as the intersection of Apollonius circles derived from each pursuer-evader pair. The capture strategy is formulated as a zero-sum game where the pursuers cooperatively minimize the area of this set, while the evader seeks to maximize it, effectively playing a game of spatial containment. By deriving the analytical gradients of the safe-reachable set's area with respect to agent positions, we obtain closed-form, instantaneous optimal control laws for the heading of each agent. These strategies are computationally efficient, allowing for real-time implementation. Simulations demonstrate that the gradient-based controls effectively steer the pursuers to systematically shrink the evader's safe region, leading to guaranteed capture. This area-minimization approach provides a clear geometric objective for cooperative capture.

</details>


### [133] [Distributed primal-dual algorithm for constrained multi-agent reinforcement learning under coupled policies](https://arxiv.org/abs/2511.15053)
*Pengcheng Dai,He Wang,Dongming Wang,Wenwu Yu*

Main category: cs.MA

TL;DR: 本文研究了约束多智能体强化学习（CMARL），提出了一种基于邻域信息耦合策略的分布式原始-对偶算法，在不直接共享策略参数和拉格朗日乘子的情况下实现协作优化与安全约束满足，并通过独立的时变网络交换局部估计，理论分析表明算法以高概率达到ε-一阶平稳收敛，仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中实现协作优化的同时满足个体安全约束是一个关键挑战，且由于隐私和安全需求，智能体无法直接共享策略参数或拉格朗日乘子，因此需要设计一种既能保证性能又能保护信息安全的分布式算法。

Method: 提出一种耦合策略框架，策略依赖于智能体自身及其κ_p跳邻居的状态和参数；设计分布式原始-对偶算法，每个智能体仅利用其2κ_p跳内的状态-动作信息和κ+2κ_p跳内的奖励信息，并通过独立时变网络更新和交换对其他智能体参数的局部估计，而不直接共享真实值。

Result: 理论证明该算法以高概率实现ε-一阶平稳收敛，逼近误差为O(γ^((κ+1)/κ_p))；在GridWorld环境中的仿真实验验证了算法的有效性。

Conclusion: 所提方法在保障隐私和系统安全的前提下，有效解决了约束多智能体强化学习问题，具备良好的收敛性和实用性。

Abstract: In this work, we investigate constrained multi-agent reinforcement learning (CMARL), where agents collaboratively maximize the sum of their local objectives while satisfying individual safety constraints. We propose a framework where agents adopt coupled policies that depend on both local states and parameters, as well as those of their $κ_p$-hop neighbors, with $κ_p>0$ denoting the coupling distance. A distributed primal-dual algorithm is further developed under this framework, wherein each agent has access only to state-action pairs within its $2κ_p$-hop neighborhood and to reward information within its $κ+ 2κ_p$-hop neighborhood, with $κ> 0$ representing the truncation distance. Moreover, agents are not permitted to directly share their true policy parameters or Lagrange multipliers. Instead, each agent constructs and maintains local estimates of these variables for other agents and employs such estimates to execute its policy. Additionally, these estimates are further updated and exchanged exclusively through an independent, time-varying networks, which enhances the overall system security. We establish that, with high probability, our algorithm can achieve an $ε$-first-order stationary convergence with an approximation error of $\mathcal{O}(γ^{\frac{κ+1}{κ_{p}}})$ for discount factor $γ\in(0,1)$. Finally, simulations in GridWorld environment are conducted to demonstrate the effectiveness of the proposed algorithm.

</details>


### [134] [Adversarial Attack on Black-Box Multi-Agent by Adaptive Perturbation](https://arxiv.org/abs/2511.15292)
*Jianming Chen,Yawen Wang,Junjie Wang,Xiaofei Xie,Yuanzhe Hu,Qing Wang,Fanjiang Xu*

Main category: cs.MA

TL;DR: 提出AdapAM，一种针对黑盒多智能体系统的新型自适应攻击框架，通过自适应选择策略和基于代理的扰动生成，实现高效且隐蔽的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击框架在多智能体系统中存在实用性不足（需白盒信息或高控制权限）以及缺乏隐蔽性或有效性的问题，亟需一种适用于黑盒环境、兼具效果与隐蔽性的攻击方法。

Method: AdapAM包含两个核心组件：(1) 自适应选择策略，动态选择受害者并确定最不利的恶意行为，平衡攻击效果与隐蔽性；(2) 基于代理的扰动生成，利用生成对抗模仿学习构建目标MAS的近似模型，在白盒下生成扰动观测，从而在黑盒环境中诱导受害者执行恶意行为。

Result: 在八个多智能体环境中评估，AdapAM在不同扰动率下均优于四个先进基线方法，攻击性能最佳；其生成的扰动噪声最小，最难被检测，表现出更强的隐蔽性。

Conclusion: AdapAM是一种有效且隐蔽的黑盒多智能体系统对抗攻击框架，揭示了现有MAS在安全性与可靠性方面的潜在漏洞，为后续防御机制设计提供了重要参考。

Abstract: Evaluating security and reliability for multi-agent systems (MAS) is urgent as they become increasingly prevalent in various applications. As an evaluation technique, existing adversarial attack frameworks face certain limitations, e.g., impracticality due to the requirement of white-box information or high control authority, and a lack of stealthiness or effectiveness as they often target all agents or specific fixed agents. To address these issues, we propose AdapAM, a novel framework for adversarial attacks on black-box MAS. AdapAM incorporates two key components: (1) Adaptive Selection Policy simultaneously selects the victim and determines the anticipated malicious action (the action would lead to the worst impact on MAS), balancing effectiveness and stealthiness. (2) Proxy-based Perturbation to Induce Malicious Action utilizes generative adversarial imitation learning to approximate the target MAS, allowing AdapAM to generate perturbed observations using white-box information and thus induce victims to execute malicious action in black-box settings. We evaluate AdapAM across eight multi-agent environments and compare it with four state-of-the-art and commonly-used baselines. Results demonstrate that AdapAM achieves the best attack performance in different perturbation rates. Besides, AdapAM-generated perturbations are the least noisy and hardest to detect, emphasizing the stealthiness.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [135] [Fluid Control with Localized Spacetime Windows](https://arxiv.org/abs/2511.15189)
*Yixin Chen,David I. W. Levin,Timothy R. Langlois*

Main category: cs.GR

TL;DR: 提出一种基于局部时空窗口的物理流体控制方法，通过在用户定义的空间区域内搜索最优时间窗口大小来提高大规模模拟的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的力基时空控制方法难以处理大规模模拟，因此需要一种更高效的方法来实现对复杂流体行为的精确控制。

Method: 利用CMA-ES算法在用户定义的空间区域内搜索最优时间窗口大小，并在“浮动”背景网格上优化和施加控制力，从而解耦控制维度与模拟，支持与粒子方法无缝集成。

Result: 该方法在多个2D和3D粒子型自由表面模拟示例中展示了高效性和有效性，能够在保持全局一致性的同时显著提升计算性能。

Conclusion: 所提出的局部时空窗口控制方法能够有效扩展到以前难以处理的大规模模拟场景，为复杂流体控制提供了新的可行方案。

Abstract: We present a physics-based fluid control method utilizing localized spacetime windows, extending force-based spacetime control to simulation scales that were previously intractable. Building on the observation that optimal control force distributions are often localized, we show that operating only in a localized spacetime window around the edit of interest can improve performance. To determine the optimal spacetime window size, we employ the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) method to search for the optimal temporal window size within a user-defined spatial region. Instead of using a Lagrangian representation, we optimize and apply control forces on a "floating" background grid, decoupling the control dimensionality from the simulation and enabling seamless integration with particle-based methods. Moreover, since the boundary conditions of the localized areas are encoded in the objective function, no extra effort is required to ensure consistency between the local control region and the global simulation domain. We demonstrate the effectiveness and efficiency of our method with various 2D and 3D particle-based free-surface simulation examples.

</details>


### [136] [One algebra for all : Geometric Algebra methods for neurosymbolic XR scene authoring, animation and neural rendering](https://arxiv.org/abs/2511.15398)
*Manos Kamarianakis,Antonis Protopsaltis,George Papagiannakis*

Main category: cs.GR

TL;DR: 本文探讨了几何代数（GA）在计算机图形学和扩展现实中的变革性作用，特别是在角色动画、渲染、绑定、神经渲染和生成式AI场景编辑中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统表示方法如矩阵、四元数和向量在处理旋转、平移和缩放时存在精度和性能限制，需要更统一高效的数学框架。

Method: 采用几何代数将几何形式和变换封装为统一的代数表达式，保持多步变换中的几何特性，并探索其在神经符号化XR场景创作中的应用。

Result: GA能够提高绑定角色动画的保真度，增强软体模拟，简化实时渲染，并优化神经与生成式AI的场景编辑。

Conclusion: 几何代数为计算机图形学和扩展现实提供了一个连贯且高效的框架，显著提升视觉效果和计算效率。

Abstract: This position paper delves into the transformative role of Geometric Algebra (GA) in advancing specific areas of Computer Graphics (CG) and Extended Reality (XR), particularly in character animation, rendering, rigging, neural rendering, and generative AI-driven scene editing. Common CG algorithms require handling rotations, translations, and dilations (uniform scalings) in operations such as object rendering, rigged model animation, soft-body deformation, and XR simulations. Traditional representation forms - such as matrices, quaternions, and vectors - often introduce limitations in precision and performance. Recent breakthroughs in the use of GA suggest it can significantly enhance these processes by encapsulating geometric forms and transformations into uniform algebraic expressions, which maintain critical geometric properties throughout multi-step transformations. Furthermore, we explore how GA can serve as a unifying mathematical substrate for neurosymbolic XR scene authoring, bridging learned neural representations and explicit geometric reasoning. This paper outlines how GA-based approaches can improve the fidelity of rigged character animations, enhance soft-body simulations, streamline real-time rendering, and optimize neural and generative AI scene editing. GA offers a coherent and efficient framework for these processes, resulting in superior visual outcomes and computational efficiency, particularly in XR environments.

</details>


### [137] [MHR: Momentum Human Rig](https://arxiv.org/abs/2511.15586)
*Aaron Ferguson,Ahmed A. A. Osman,Berta Bescos,Carsten Stoll,Chris Twigg,Christoph Lassner,David Otte,Eric Vignola,Federica Bogo,Igor Santesteban,Javier Romero,Jenna Zarate,Jeongseok Lee,Jinhyung Park,Jinlong Yang,John Doublestein,Kishore Venkateshan,Kris Kitani,Ladislav Kavan,Marco Dal Farra,Matthew Hu,Matthew Cioffi,Michael Fabris,Michael Ranieri,Mohammad Modarres,Petr Kadlecek,Rinat Abdrashitov,Romain Prévost,Roman Rajbhandari,Ronald Mallet,Russel Pearsall,Sandy Kao,Sanjeev Kumar,Scott Parrish,Te-Li Wang,Tony Tung,Yuan Dong,Yuhua Chen,Yuanlu Xu,Yuting Ye,Zhongshi Jiang*

Main category: cs.GR

TL;DR: 提出MHR，一种结合ATLAS骨架/形状解耦范式和现代灵活绑定与姿态校正系统的参数化人体模型，适用于AR/VR和图形管线。


<details>
  <summary>Details</summary>
Motivation: 为了实现表达力强且符合解剖学合理性的人体动画，并更好地集成到AR/VR和图形处理流程中。

Method: 结合ATLAS的解耦骨架/形状框架与受Momentum库启发的灵活绑定和姿态校正系统。

Result: 开发出支持非线性姿态校正的MHR模型，能够生成自然的人体动画。

Conclusion: MHR是一种适用于AR/VR和图形管道的、具有高表达性和解剖合理性的参数化人体模型。

Abstract: We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs](https://arxiv.org/abs/2511.14777)
*Mahdi Samiei,Mahdi Mansouri,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 本文提出了一种基于有限状态机（FSM）执行的可解释框架，用于评估大语言模型（LLMs）在多步、规则化程序推理中的能力。实验表明，随着任务长度和分支复杂度增加，模型性能显著下降，且高分支比长记忆更影响准确性。外部化中间步骤可提升表现，该方法为提升LLMs的算法可靠性提供了可控的测试基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多推理任务中表现良好，但其在长程、规则驱动的程序性推理方面的能力尚不明确。缺乏一个可控制且可解释的基准来系统评估模型在纯程序执行中的表现。因此，需要一种最小化干扰因素（如世界知识）的任务框架，以准确衡量模型的内部过程一致性与执行保真度。

Method: 引入有限状态机（FSM）执行任务作为评估LLMs程序推理能力的基准。给定明确的FSM定义和输入动作序列，要求模型逐步执行状态转移，仅依赖确定性规则而不需外部知识。通过测量单步准确率（Turn Accuracy）和整体任务准确率（Task Accuracy），分离局部计算能力与长期状态保持能力，并控制任务长度和分支复杂度进行系统分析。

Result: 实验发现，随着任务步数或状态分支复杂度增加，模型性能系统性下降；高分支因子对性能的影响大于长记忆需求；更大的模型在单步准确率上表现更好，但在多步推理中仍脆弱，除非通过提示引导其显式输出中间步骤。

Conclusion: FSM执行是一种有效、透明且可控的方法，可用于诊断LLMs在长程程序推理中的失败模式。研究强调了当前模型在过程保真度上的局限，并建议通过外部化中间状态和设计归纳偏置来提升其算法可靠性，为构建真正具备程序执行能力的模型提供了实验基础。

Abstract: Large language models (LLMs) have achieved remarkable results on tasks framed as reasoning problems, yet their true ability to perform procedural reasoning, executing multi-step, rule-based computations remains unclear. Unlike algorithmic systems, which can deterministically execute long-horizon symbolic procedures, LLMs often degrade under extended reasoning chains, but there is no controlled, interpretable benchmark to isolate and measure this collapse. We introduce Finite-State Machine (FSM) Execution as a minimal, fully interpretable framework for evaluating the procedural reasoning capacity of LLMs. In our setup, the model is given an explicit FSM definition and must execute it step-by-step given input actions, maintaining state consistency over multiple turns. This task requires no world knowledge, only faithful application of deterministic transition rules, making it a direct probe of the model's internal procedural fidelity. We measure both Turn Accuracy and Task Accuracy to disentangle immediate computation from cumulative state maintenance. Empirical results reveal systematic degradation as task horizon or branching complexity increases. Models perform significantly worse when rule retrieval involves high branching factors than when memory span is long. Larger models show improved local accuracy but remain brittle under multi-step reasoning unless explicitly prompted to externalize intermediate steps. FSM-based evaluation offers a transparent, complexity-controlled probe for diagnosing this failure mode and guiding the design of inductive biases that enable genuine long-horizon procedural competence. By grounding reasoning in measurable execution fidelity rather than surface correctness, this work helps establish a rigorous experimental foundation for understanding and improving the algorithmic reliability of LLMs.

</details>


### [139] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 本文介绍了FERMAT，一个用于自动化数学理论发现的强化学习环境，并探索了通过进化算法自动生成数学对象‘有趣性’评分的方法，其中基于大语言模型的进化算法在数论和有限域发现上表现优异。


<details>
  <summary>Details</summary>
Motivation: 实现人工智能在开放性数学理论发现中的自动化是一个重大挑战，本文旨在通过构建新的环境和方法推动这一目标。

Method: 提出FERMAT强化学习环境，建模概念发现与定理证明；设计基于LLM的进化算法，引入函数抽象机制，用于生成非平凡的‘有趣性’度量。

Result: 基于LLM的进化算法在发现初等数论和有限域方面显著优于硬编码基线，验证了方法的有效性。

Conclusion: FERMAT为自动化数学理论发现提供了可扩展的框架，结合LLM的进化算法能有效识别‘有趣’的数学对象，推动AI在数学发现中的应用。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [140] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI 是一个用于检查和干预多智能体交互中信念状态的系统框架，应用于具有共享电子病历和真实实验室结果的医疗案例模拟器，揭示了智能体在诊断过程中的信念形成与证据整合动态。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中信念的形成、固化及对反证据的抵抗，模仿现实世界中专业领域的认知偏见，并提供可重现的方法来分析科学推理中的认知孤岛现象。

Method: 构建 Ask WhAI 框架，记录和回放智能体交互，支持对信念和推理的外部查询，并通过反事实证据注入测试信念结构；在儿科神经精神疾病多学科诊疗模拟中应用该框架，利用带时间戳的电子病历和持有真实结果的 LabAgent 进行验证。

Result: 发现智能体的信念表现出类似现实医学专业的立场，如过度依赖经典研究、抵制反证据；通过断点查询可区分先验信念与推理影响，且这些信念可被追溯和检验。

Conclusion: Ask WhAI 提供了一种可重现的方式，使多智能体系统中的信念动态变得可见且可测试，有助于理解科学推理中的认知偏差和信息孤岛问题。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [141] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 提出了一种基于GPT-4o的全自动地理编码工作流，用于处理EM-DAT灾害数据库中的非结构化文本位置信息，通过交叉验证GADM、OpenStreetMap和Wikidata三个地理信息源生成具有可靠性评分的子国家层级地理数据。


<details>
  <summary>Details</summary>
Motivation: 灾害事件的子国家位置数据对风险评估至关重要，但现有数据库（如EM-DAT）常以非结构化文本形式记录位置，存在粒度不一、拼写不一致等问题，难以与空间数据集成。

Method: 利用GPT-4o大语言模型自动解析和清洗文本位置信息，并结合GADM、OpenStreetMap和Wikidata三个独立地理信息库进行交叉验证，根据来源一致性分配可靠性评分，生成标准化的子国家几何体。

Result: 应用于2000至2024年EM-DAT数据集，成功地理编码14,215个事件，覆盖17,948个唯一位置，实现了全自动化、无需人工干预的高覆盖率地理匹配。

Conclusion: 该方法无需人工干预，支持所有灾害类型，可灵活映射到不同地理框架，并展示了大语言模型在从非结构化文本中提取地理信息方面的可扩展性和可靠性。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [142] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 本研究通过创建名为Rachel So的完整AI学术身份，发表了10余篇由AI生成的论文，探讨学术界对AI作者身份的反应及其对出版、科研和科学体系的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力的增强，亟需探讨其在学术出版中的角色和影响，尤其是AI作为作者的可行性及其对学术生态的冲击。

Method: 采用行动研究方法，构建AI学术身份Rachel So，并在2025年3月至10月间发布多篇AI生成论文，追踪其发表、引用及同行评审情况。

Result: Rachel So成功发表10余篇论文，获得引用并收到同行评审邀请，表明当前学术系统未能有效识别或阻止AI作者的参与。

Conclusion: 现有学术交流体系在面对高度自主的AI时存在漏洞，需重新审视作者身份、出版伦理及科研诚信机制。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [143] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于概率的、考虑不确定性的方法，用于量化自动驾驶测试与训练数据集在操作设计域（TOD）下的场景代表性。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信性与安全性依赖于训练和测试数据集的数据相关安全属性，尤其是场景数据对实际运行环境的代表性。由于真实目标操作域（TOD）分布未知且数据有限，亟需一种能处理不确定性并量化代表性的方法。

Method: 采用基于区间值的不精确贝叶斯方法，通过比较场景套件与推断出的TOD在多个操作类别（如天气、道路类型、时间等）上的特征分布，局部和全局地估计数据代表性，并处理数据稀缺和先验不确定性问题。

Result: 该方法能够生成具有不确定性感知的区间化代表性度量，在数值示例中展示了其在不同操作类别下对分布差异的建模能力，并支持在依赖关系和先验不确定条件下进行全局与局部评估。

Conclusion: 所提出的不精确贝叶斯方法为自动驾驶等安全关键系统的数据代表性评估提供了可量化、透明且适用于小样本场景的框架，有助于提升AI系统的验证可信度。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [144] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 本文提出了一种基于Sharpness-Aware Minimization（SAM）增强的Soft Actor Critic算法，在分布式多智能体强化学习框架下优化O-RAN网络中的资源管理。通过引入基于时序差分误差方差的自适应选择性SAM机制和动态ρ调度策略，提升了模型在动态环境中的鲁棒性、泛化能力和训练稳定性，显著提高了资源分配效率（最高提升22%）和QoS保障。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（DRL）在O-RAN资源管理中面临动态环境下鲁棒性和泛化能力不足的问题，现有方法难以兼顾学习效率与稳定性。

Method: 提出一种改进的Soft Actor Critic（SAC）算法，结合Sharpness-Aware Minimization（SAM），构建分布式多智能体强化学习（MARL）框架；引入基于TD-error方差的自适应选择性正则化机制，仅对高复杂度环境下的智能体施加SAM；设计动态ρ调度策略以优化各智能体的探索-利用权衡。

Result: 实验结果表明，该方法相比传统DRL方法显著提升了资源分配效率（最高达22%），增强了QoS满足率，并在不同O-RAN切片中表现出更优的泛化性和训练稳定性。

Conclusion: 所提出的自适应选择性SAM机制与动态ρ调度有效提升了多智能体DRL在O-RAN资源管理中的性能，为下一代无线网络提供了更具鲁棒性和可扩展性的解决方案。

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [145] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Macro Action Quantization (MAQ)的人类似强化学习框架，通过将人类示范提炼为宏观动作来提升智能体行为的人类相似性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习智能体虽性能优越，但行为往往不自然，缺乏与人类行为的一致性，影响可解释性和可信度。

Method: 将人类相似性建模为轨迹优化问题，并结合向量量化变分自编码器（VQ-VAE）提取宏观动作，采用滚动时域控制实现高效学习。

Result: 在D4RL Adroit基准上显著提升了轨迹相似性得分，并在人类评估中获得最高的人类相似性排名。

Conclusion: MAQ能有效提升强化学习智能体的人类相似性，且易于集成到现有算法中，为构建更可信、可解释的智能体提供了新方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [146] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: 本研究提出OpenBioLLM，一个基于开源模型的模块化多智能体框架，用于基因组问答任务，在无需微调的情况下性能优于或匹配专有模型GeneGPT，同时降低40-50%延迟。


<details>
  <summary>Details</summary>
Motivation: GeneGPT依赖专有大模型导致可扩展性差、成本高且存在数据隐私和泛化问题，亟需基于开源模型的高效替代方案。

Method: 首先在单体架构下复现GeneGPT并评估其局限性；进而设计OpenBioLLM，采用多智能体架构实现工具路由、查询生成和响应验证的智能体专业化分工。

Result: OpenBioLLM在90%以上基准任务上达到或超过GeneGPT性能，Gene-Turing和GeneHop平均得分分别为0.849和0.830，延迟降低40-50%。

Conclusion: 基于开源模型的模块化多智能体系统在基因组问答中具有高性能与高效率潜力，为领域应用提供了可扩展、低成本且隐私友好的解决方案。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [147] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: 提出了一种名为ProRAC的神经符号框架，利用大语言模型解决动作与变化推理（RAC）问题，在多个基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了有效解决动作与变化推理（RAC）中的复杂逻辑问题，结合神经网络与符号推理的优势。

Method: ProRAC从问题中提取动作和问题等基本元素，逐步执行每个动作以推导最终状态，并基于该状态评估查询得出答案。

Result: 在多个RAC基准、领域、大语言模型架构和任务类型上均实现了优异性能。

Conclusion: ProRAC通过渐进式状态推理，有效提升了RAC任务的准确性和泛化能力，展现出神经符号方法的潜力。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [148] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One 是一种基于大语言模型的多智能体框架，通过科学家、提取器和测试器三个智能体协作，结合外部知识进行自动化特征提取，在表格数据上实现了更优性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动特征工程方法受限于单一的大模型架构、简单的量化反馈，且缺乏对外部领域知识的系统性整合，限制了特征的质量与可解释性。

Method: 提出 Rogue One 框架，包含三个专门的智能体（Scientist、Extractor、Tester），采用迭代协作机制，并引入定性反馈和“泛洪-剪枝”策略来平衡探索与利用，结合检索增强生成（RAG）系统融入外部知识。

Result: 在19个分类和9个回归数据集上显著优于现有最先进方法，并能生成具有语义意义和可解释性的特征，例如在心肌数据集中发现新的潜在生物标志物。

Conclusion: Rogue One 有效提升了自动特征工程的性能与科学价值，兼具统计强度与可解释性，展现出作为科学发现工具的潜力。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [149] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: 本文提出了SafeRBench，首个端到端评估大型推理模型（LRM）安全性的基准，涵盖输入、中间推理和最终输出，通过细粒度分析和人类对齐验证揭示多维安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有的安全性评估主要关注输出层面，难以捕捉推理过程中逐步显现的有害内容或误导性逻辑等动态风险，因此需要一种能够全面评估LRM在推理链中安全表现的方法。

Method: 1) 在输入设计中引入风险类别与等级，构建包含不同危害梯度的平衡提示集；2) 提出微思维分块机制，将长推理链分割为语义连贯单元，并从十个安全维度进行细粒度分析；3) 通过专门设计的人类标注来验证LLM自动评估的安全对齐性。

Result: 在19个大型推理模型上的评估表明，SafeRBench能够实现细致、多维度的安全评估，揭示了不同模型在各风险维度上的表现差异及潜在防护机制。

Conclusion: SafeRBench为大型推理模型提供了首个覆盖完整推理过程的端到端安全评估框架，有助于系统识别和缓解推理过程中隐含的安全威胁。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [150] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: 提出了一种结合异质信息网络（HIN）和大语言模型（LLM）的知识追踪框架HISE-KT，通过自动化元路径质量评估和相似学生检索机制，提升了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法在使用异质信息网络时存在元路径选择噪声大、缺乏实例质量评估的问题，而基于大语言模型的方法忽略了跨学生间的丰富信息，且两者都难以提供准确且有证据支持的解释。

Method: 构建包含多种节点类型的多关系异质信息网络，利用大语言模型对元路径实例进行智能评分与筛选，并设计基于元路径的相似学生检索机制，结合结构化提示将目标学生的学习历史与相似轨迹整合，由大语言模型生成预测和解释报告。

Result: 在四个公开数据集上的实验表明，HISE-KT在预测性能和可解释性方面均优于现有的知识追踪基线方法。

Conclusion: HISE-KT通过HIN与LLM的协同增强了知识追踪的效果，实现了更精准的性能预测和基于证据的可解释分析，为智能教育系统提供了新的技术路径。

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [151] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: 本文提出了一种名为COPYCHECK的新框架，利用不确定性信号来检测大语言模型（LLM）训练过程中是否使用了受版权保护的内容。该方法通过捕捉“已见”与“未见”数据之间的不确定性模式，克服了现有成员推理攻击的局限性，在多个LLM上实现了超过90%的平均平衡准确率，并展现出良好的跨架构泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练中可能使用了受版权保护的数据，引发未经授权使用的担忧。现有的成员推理攻击因模型过度自信、缺乏真实训练数据信息和依赖经验阈值而受限，因此需要更可靠的方法来检测训练数据的版权归属。

Method: 提出COPYCHECK框架，利用LLM的不确定性信号进行版权检测；采用两步策略：一是将文件分割为小片段以减少对大规模训练数据的依赖，二是通过不确定性引导的无监督聚类消除对经验阈值的依赖。

Result: COPYCHECK在LLaMA 7b上达到90.1%，在LLaMA2 7b上达到91.6%的平均平衡准确率，相比现有最先进基线方法性能提升超过90%，最高达到93.8%。同时在GPT-J 6B上表现出强泛化能力。

Conclusion: COPYCHECK首次将不确定性应用于LLM中的版权检测，有效利用模型过度自信的特性，提供了可实现训练数据透明性的实用工具，具有良好的准确性和跨模型泛化性能。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [152] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: SOLID框架结合数学优化与大语言模型（LLM），通过双价格和偏差惩罚实现两者的迭代协作，在保持模块化和数据隐私的同时提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 旨在融合数学优化的精确性与大语言模型的上下文理解能力，以实现更智能、灵活且可解释的决策系统。

Method: 提出SOLID框架，通过优化代理与LLM代理之间的双价格和偏差惩罚机制进行迭代协作，并在凸性假设下保留理论收敛性保证。

Result: 在股票投资组合案例中验证了SOLID的有效性，使用历史价格和金融新闻作为输入，结果显示其在多种场景下均能收敛，并优于仅使用优化器的基线方法，年化收益率更高。

Conclusion: SOLID为数学优化与大语言模型的协同提供了有效范式，具有广泛应用于自动化智能决策领域的潜力。

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [153] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 本文讨论了推理AI在计算效率接近物理极限时的可持续性问题，指出仅靠效率提升不足以实现可持续发展，需通过研究和政策手段为系统优化和治理设定明确限制。


<details>
  <summary>Details</summary>
Motivation: 随着AI从模式识别转向多步推理，计算需求持续指数增长，而传统能效提升已接近物理极限，缺乏自然饱和点，导致能源消耗风险上升。

Method: 分析推理AI的计算扩展趋势与能效瓶颈，结合历史能耗模式，提出需在系统优化和治理中引入显式限制机制。

Result: 论证了单纯依赖效率改进无法实现推理AI的可持续发展，识别出研究与政策干预的关键方向。

Conclusion: 为确保推理AI的可持续性，必须主动设计包含明确计算资源上限的优化框架和治理策略。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [154] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探讨了人工智能研究中关于智能本质的两种根本对立观念：智能实在论认为智能是一种可跨系统度量的单一普遍能力，而智能多元论则认为智能是多样且依赖情境的能力。这两种隐含的观念深刻影响了研究方法、实证解释和风险评估。明确这些假设有助于澄清AI领域的分歧。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中的许多争论源于对智能本质的不同预设，但这些预设往往未被明言。厘清这些基础观念有助于理解不同研究取向之间的根本分歧。

Method: 通过分析AI领域内的主要争论，考察智能实在论与智能多元论如何在方法论、现象解释和风险判断三个层面影响研究实践。

Result: 揭示了两种智能观在模型选择、基准设计、实验验证、能力解释及风险评估上的系统性差异，并展示了它们如何导致对同一现象的不同解读。

Conclusion: 明确智能概念的基础假设有助于促进AI研究中的清晰对话，推动更具反思性的科学实践，并为解决AI发展中的分歧提供框架。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [155] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 提出Octopus：一种具备六种能力协调的智能多模态推理新范式，能够自主探索并动态选择最适合的推理路径，在Octopus-Bench上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏人类般的自主探索多样化推理路径的能力，难以适应现实任务中动态变化的需求，且通常只覆盖人类思维能力的一个子集。

Method: 定义了多模态推理所需的六种核心能力，构建了Octopus-Bench评测基准，并提出Octopus模型，能够在推理过程中自主探索并动态选择最合适的能力。

Result: Octopus在Octopus-Bench的大多数任务上取得了最佳性能。

Conclusion: 能力协调在智能多模态推理中起着关键作用。

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [156] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个受文明5启发的强化学习综合挑战环境，旨在同时测试多个经典RL难题，强调长期、跨多变量交互的深度推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估代理在不相关策略之间的切换能力，而非其在多个交互挑战中的深层推理能力。因此需要一个能综合评估多种RL挑战的环境。

Method: 设计了一个名为Terra Nova的新环境，整合了部分可观测性、信用分配、表示学习和巨大动作空间等RL挑战，要求代理进行长时程、集成化的决策。

Result: Terra Nova能够在一个统一环境中同时呈现多个RL挑战，促进对复杂、交互式问题的综合解决能力的研究。

Conclusion: Terra Nova作为一个综合挑战环境，为评估和推动强化学习代理在复杂、现实世界问题中的深层推理能力提供了新平台。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [157] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于物理交互的智能体学习框架IPR，通过结合视觉语言模型与世界模型，在多样化的游戏中实现类人推理，并展现出随经验增长而提升的物理因果理解能力。


<details>
  <summary>Details</summary>
Motivation: 探索智能体是否能像人类一样通过与环境交互来获得物理和因果推理能力，并持续改进。

Method: 提出Interactive Physical Reasoner (IPR)，利用世界模型的rollout结果来评估并强化VLM策略，引入PhysCode作为语义意图与物理动态对齐的动作表示。在1000多个异构游戏中进行预训练，并在G2U（Game-to-Unseen）设置下评估生存、好奇、效用三个类人层级。

Result: IPR在三项类人任务上表现稳健，整体性能媲美GPT-5，在好奇心任务上超越GPT-5；性能随训练游戏数量和交互步数增加而提升，并具备对未见游戏的零样本迁移能力。

Conclusion: 以物理为中心的交互是实现持续提升的物理与因果推理的有效路径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [158] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体大语言模型的交易意图挖掘框架TIM，用于准确推断去中心化金融（DeFi）中的用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以深入理解DeFi交易背后的用户意图，因智能合约交互复杂、链上/链下因素多样且日志信息不透明。

Method: 构建基于扎根理论的DeFi意图分类体系，并设计包含元层级规划器、问题求解器和认知评估器的多智能体大语言模型系统。

Result: 实验表明，TIM在意图推断性能上显著优于机器学习模型、单一大语言模型和单智能体基线方法。

Conclusion: TIM能更可靠地理解DeFi用户动机，为复杂的区块链活动提供上下文感知的解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [159] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: Agents4Science是首个由AI代理担任主要作者和审稿人的会议，探讨了AI在科学研究中的角色及其与人类协作的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理作为科学家和审稿人的能力，推动人机协作在科研中的应用。

Method: 组织了一场名为Agents4Science的会议，其中AI代理作为主要作者和审稿人，人类作为合作作者和合作审稿人参与。

Result: 总结了会议中的关键经验，揭示了AI在科研中作为创作者和评审者的潜力与挑战。

Conclusion: AI代理在科学研究中展现出一定能力，但仍需与人类紧密协作以实现更高效的科学创新。

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [160] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文研究了在AI研究代理中，想法多样性对代理性能的影响，发现更高的想法多样性与更强的性能表现相关。


<details>
  <summary>Details</summary>
Motivation: AI研究代理尚处于初期阶段，影响其成功或失败的关键因素尚不明确，因此需要探究关键驱动因素。

Method: 作者分析了不同模型和代理框架在MLE-bench基准上的代理轨迹，并通过控制实验调整想法多样性的程度，同时采用多种评估指标验证结果。

Result: 分析表明不同模型和框架产生的想法多样性不同，高性能代理通常具有更高的想法多样性；控制实验显示提高想法多样性可提升性能，且该结论在多种评估指标下均成立。

Conclusion: 想法多样性是AI研究代理成功的关键因素之一，提升多样性有助于增强代理的科研能力。

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [Z-Merge: Multi-Agent Reinforcement Learning for On-Ramp Merging with Zone-Specific V2X Traffic Information](https://arxiv.org/abs/2511.14910)
*Yassine Ibork,Myounggyu Won,Lokesh Das*

Main category: cs.RO

TL;DR: 提出了一种基于V2X辅助的多智能体强化学习框架，用于高速公路匝道合流控制，通过融合全局与局部信息优化车道变换与车距调整策略。


<details>
  <summary>Details</summary>
Motivation: 现有匝道合流方法依赖局部信息，难以兼顾安全性与交通效率，尤其在混合交通环境下表现不佳。

Method: 将合流问题建模为多智能体部分可观测马尔可夫决策过程（MA-POMDP），利用路侧单元提供的区域级全局信息，设计具有混合动作空间的深度Q学习算法，结合离散与连续控制决策。

Result: 在SUMO与MOSAIC联合仿真中，该框架显著提升了合流成功率、交通效率和道路安全，适用于多种交通场景。

Conclusion: V2X辅助的MARL框架能有效协调AV与HV的交互行为，为复杂混合交通环境下的自主合流提供了高效解决方案。

Abstract: Ramp merging is a critical and challenging task for autonomous vehicles (AVs), particularly in mixed traffic environments with human-driven vehicles (HVs). Existing approaches typically rely on either lane-changing or inter-vehicle gap creation strategies based solely on local or neighboring information, often leading to suboptimal performance in terms of safety and traffic efficiency. In this paper, we present a V2X (vehicle-to-everything communication)-assisted Multiagent Reinforcement Learning (MARL) framework for on-ramp merging that effectively coordinates the complex interplay between lane-changing and inter-vehicle gap adaptation strategies by utilizing zone-specific global information available from a roadside unit (RSU). The merging control problem is formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP), where agents leverage both local and global observations through V2X communication. To support both discrete and continuous control decisions, we design a hybrid action space and adopt a parameterized deep Q-learning approach. Extensive simulations, integrating the SUMO traffic simulator and the MOSAIC V2X simulator, demonstrate that our framework significantly improves merging success rate, traffic efficiency, and road safety across diverse traffic scenarios.

</details>


### [162] [A visual study of ICP variants for Lidar Odometry](https://arxiv.org/abs/2511.14919)
*Sebastian Dingler,Hannes Burrichter*

Main category: cs.RO

TL;DR: 提出了一种新的方法来过滤动态物体并解决激光雷达里程计中ICP算法受动态物体、非重叠区域和传感器噪声影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的ICP算法在实际应用中因动态物体、非重叠区域和噪声导致精度下降，需要更鲁棒的方法。

Method: 基于可视化ICP多维目标函数的方法，分析不同ICP变体，并提出新的动态物体滤波和盲区处理方法。

Result: 新方法能有效识别并减轻动态物体和盲区对里程计精度的影响。

Conclusion: 所提方法提升了激光雷达里程计在复杂真实环境中的鲁棒性和准确性。

Abstract: Odometry with lidar sensors is a state-of-the-art method to estimate the ego pose of a moving vehicle. Many implementations of lidar odometry use variants of the Iterative Closest Point (ICP) algorithm. Real-world effects such as dynamic objects, non-overlapping areas, and sensor noise diminish the accuracy of ICP. We build on a recently proposed method that makes these effects visible by visualizing the multidimensional objective function of ICP in two dimensions. We use this method to study different ICP variants in the context of lidar odometry. In addition, we propose a novel method to filter out dynamic objects and to address the ego blind spot problem.

</details>


### [163] [SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification](https://arxiv.org/abs/2511.14977)
*Xiangyu Li,Zhaomiao Guo*

Main category: cs.RO

TL;DR: 本文提出SVBRD-LLM框架，通过零样本提示工程从真实交通视频中自动发现、验证和应用可解释的行为规则，以分析自动驾驶车辆与人类驾驶车辆的行为差异。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在公共道路上的增多，理解其真实世界行为对交通安全、政策制定和公众接受度至关重要。现有方法难以提取可解释的、适用于实际场景的行为规则。

Method: 该框架结合YOLOv8和ByteTrack提取车辆轨迹，计算运动学特征，并利用GPT-5进行零样本提示，生成35条结构化行为规则假设；通过验证集迭代优化规则，过滤虚假相关性，构建高置信度规则库。

Result: 在1500多小时的真实交通视频上实验表明，该框架在自动驾驶车辆识别任务中达到90.0%准确率和93.3% F1分数，并成功揭示了自动驾驶车辆在速度控制平滑性、变道保守性和加速度稳定性方面的独特行为特征。

Conclusion: SVBRD-LLM能够有效从真实交通数据中挖掘高可信、可解释的自动驾驶行为规则，为交通分析、安全评估和政策制定提供了有力工具。

Abstract: As more autonomous vehicles operate on public roads, understanding real-world behavior of autonomous vehicles is critical to analyzing traffic safety, making policies, and public acceptance. This paper proposes SVBRD-LLM, a framework that automatically discovers, verifies, and applies interpretable behavioral rules from real traffic videos through zero-shot prompt engineering. The framework extracts vehicle trajectories using YOLOv8 and ByteTrack, computes kinematic features, and employs GPT-5 zero-shot prompting to compare autonomous and human-driven vehicles, generating 35 structured behavioral rule hypotheses. These rules are tested on a validation set, iteratively refined based on failure cases to filter spurious correlations, and compiled into a high-confidence rule library. The framework is evaluated on an independent test set for speed change prediction, lane change prediction, and autonomous vehicle identification tasks. Experiments on over 1500 hours of real traffic videos show that the framework achieves 90.0% accuracy and 93.3% F1-score in autonomous vehicle identification. The discovered rules clearly reveal distinctive characteristics of autonomous vehicles in speed control smoothness, lane change conservativeness, and acceleration stability, with each rule accompanied by semantic description, applicable context, and validation confidence.

</details>


### [164] [Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy](https://arxiv.org/abs/2511.15239)
*Tomoki Nakao,Kazumi Kasaura,Tadashi Kozuno*

Main category: cs.RO

TL;DR: 提出一种基于缠绕数的分层导航方法，通过强化学习学习合作对称性打破策略，有效解决多智能体导航中的对称性死锁问题。


<details>
  <summary>Details</summary>
Motivation: 在分布式多智能体导航中，多个智能体交互时难以自主打破决定如何相互通过的对称性，导致死锁问题。

Method: 引入一种量化合作对称性打破策略的方法，使用称为缠绕数的拓扑不变量，并通过强化学习来学习这些策略。该方法包括一个基于学习的规划器和一个基于模型的控制器组成的分层策略。

Result: 仿真和真实机器人实验表明，该方法在密集环境中优于现有基线方法，能够高效避免碰撞和死锁，实现更优的导航性能。

Conclusion: 所提出的分层导航方法结合了基于学习方法的灵活决策能力和基于模型方法的可靠性，在解决多智能体导航中的对称性死锁问题上表现出色。

Abstract: We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at https://github.com/omron-sinicx/WNumMPC.

</details>


### [165] [An Alignment-Based Approach to Learning Motions from Demonstrations](https://arxiv.org/abs/2511.14988)
*Alex Cuellar,Christopher K Fourie,Julie A Shah*

Main category: cs.RO

TL;DR: 本文提出了Cluster Alignment for Learned Motions (CALM)，一种新的模仿学习框架，通过与代表性平均轨迹对齐来克服传统时间依赖或状态依赖方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习方法在处理重叠轨迹和扰动下的鲁棒性方面存在缺陷，因此需要一种更灵活、稳定的学习机制。

Method: CALM基于演示运动的代表‘均值’轨迹进行对齐，引入了一种能够应对扰动下对齐变化的对齐技术，并利用演示聚类生成多模态行为。

Result: 在2D数据集上验证了CALM能有效缓解时间依赖和时间无关方法的缺点，并在7自由度机器人上成功实现了三个任务领域的学习。

Conclusion: CALM提供了一种优于传统方法的模仿学习新范式，在保持收敛性的同时提升了鲁棒性和表达能力。

Abstract: Learning from Demonstration (LfD) has shown to provide robots with fundamental motion skills for a variety of domains. Various branches of LfD research (e.g., learned dynamical systems and movement primitives) can generally be classified into ''time-dependent'' or ''time-independent'' systems. Each provides fundamental benefits and drawbacks -- time-independent methods cannot learn overlapping trajectories, while time-dependence can result in undesirable behavior under perturbation. This paper introduces Cluster Alignment for Learned Motions (CALM), an LfD framework dependent upon an alignment with a representative ''mean" trajectory of demonstrated motions rather than pure time- or state-dependence. We discuss the convergence properties of CALM, introduce an alignment technique able to handle the shifts in alignment possible under perturbation, and utilize demonstration clustering to generate multi-modal behavior. We show how CALM mitigates the drawbacks of time-dependent and time-independent techniques on 2D datasets and implement our system on a 7-DoF robot learning tasks in three domains.

</details>


### [166] [Communication-Aware Asynchronous Distributed Trajectory Optimization for UAV Swarm](https://arxiv.org/abs/2511.14994)
*Yue Yu,Xiaobo Zheng,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种通信感知的异步分布式轨迹优化框架（CA-ADTO），用于在通信受限环境下实现无人机群的高效轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 在通信受限环境中，传统的分布式优化方法因不可靠链路和有限数据交换而难以有效部署，亟需一种鲁棒且低通信开销的轨迹规划方法。

Method: 采用两层架构，结合参数化微分动态规划（PDDP）进行单个无人机的局部轨迹优化，并利用异步交替方向乘子法（async-ADMM）实现群体协同，支持异步通信与非线性动力学建模。

Result: 所提方法显著降低了通信开销，实现了完全分布式的优化，在通信不稳定条件下仍能有效处理非线性动力学和时空耦合问题。

Conclusion: CA-ADTO框架在通信受限场景下具有良好的实用性与鲁棒性，为无人机群的分布式轨迹规划提供了可行的解决方案。

Abstract: Distributed optimization offers a promising paradigm for trajectory planning in Unmanned Aerial Vehicle (UAV) swarms, yet its deployment in communication-constrained environments remains challenging due to unreliable links and limited data exchange. This paper addresses this issue via a two-tier architecture explicitly designed for operation under communication constraints. We develop a Communication-Aware Asynchronous Distributed Trajectory Optimization (CA-ADTO) framework that integrates Parameterized Differential Dynamic Programming (PDDP) for local trajectory optimization of individual UAVs with an asynchronous Alternating Direction Method of Multipliers (async-ADMM) for swarm-level coordination. The proposed architecture enables fully distributed optimization while substantially reducing communication overhead, making it suitable for real-world scenarios in which reliable connectivity cannot be guaranteed. The method is particularly effective in handling nonlinear dynamics and spatio-temporal coupling under communication constraints.

</details>


### [167] [Lie Group Control Architectures for UAVs: a Comparison of SE2(3)-Based Approaches in Simulation and Hardware](https://arxiv.org/abs/2511.15023)
*Dimitria Silveria,Kleber Cabral,Peter Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: 本文提出并实验验证了一种基于李群SE2(3)的新型模型预测控制器（MPC），在仿真和实际四旋翼平台上均表现出优于传统方法的轨迹跟踪性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了提升四旋翼飞行器在复杂场景下的控制性能与鲁棒性，利用李群理论的几何特性发展先进的控制策略。

Method: 基于SE2(3)李群框架设计了一种新型MPC控制器，并与SE2(3)-LQR及工业标准控制器在仿真和Quanser QDrone实际平台上进行对比实验。

Result: SE2(3) MPC在仿真中表现与LQR相当，在实测中显著优于LQR和工业标准控制器，展现出更优的轨迹跟踪精度和鲁棒性。

Conclusion: 基于李群的控制器在实际应用中具有优越性能，本文提出的SE2(3) MPC是一种有效且具前景的先进控制方案。

Abstract: This paper presents the integration and experimental validation of advanced control strategies for quadcopters based on Lie groups. We build upon recent theoretical developments on SE2(3)-based controllers and introduce a novel SE2(3) model predictive controller (MPC) that combines the predictive capabilities and constraint-handling of optimal control with the geometric properties of Lie group formulations. We evaluated this MPC against a state-of-the-art SE2(3)-based LQR approach and obtained comparable performance in simulation. Both controllers where also deployed on the Quanser QDrone platform and compared to each other and an industry standard control architecture. Results show that the SE_2(3) MPC achieves superior trajectory tracking performance and robustness across a range of scenarios. This work demonstrates the practical effectiveness of Lie group-based controllers and offers comparative insights into their impact on system behaviour and real-time performance

</details>


### [168] [Painted Heart Beats](https://arxiv.org/abs/2511.15105)
*Angshu Adhya,Cindy Yang,Emily Wu,Rishad Hasan,Abhishek Narula,Patrícia Alves-Oliveira*

Main category: cs.RO

TL;DR: 本文提出了一种名为AURA的协同人机绘画框架，通过机器人感知艺术家的心率变化来调整其绘画行为，实现与人类艺术家的情感同步创作。


<details>
  <summary>Details</summary>
Motivation: 旨在探索人类艺术家与机器人之间的自然、情感化协作方式，利用生物信号提升艺术创作中的互动流畅性。

Method: 开发了一个配备EmotiBit传感器的机械臂系统，实时监测艺术家的心率（ arousal水平），并根据心率状态决定机器人的绘画行为：高心率时远离该区域，中性心率时继续正常绘画；同时探讨了基于自然语言和物理触觉的其他交互模式。

Result: 实现了基于生理信号的动态人机协同绘画系统，机器人能根据艺术家的情绪状态调整空间行为，并展示了多种交互方式的可能性。

Conclusion: AURA框架证明了利用生物特征（如心率）可以有效支持人机在艺术创作中的情感化、自适应协同，为未来创造性人机合作提供了新范式。

Abstract: In this work we present AURA, a framework for synergistic human-artist painting. We developed a robot arm that collaboratively paints with a human artist. The robot has an awareness of the artist's heartbeat through the EmotiBit sensor, which provides the arousal levels of the painter. Given the heartbeat detected, the robot decides to increase proximity to the artist's workspace or retract. If a higher heartbeat is detected, which is associated with increased arousal in human artists, the robot will move away from that area of the canvas. If the artist's heart rate is detected as neutral, indicating the human artist's baseline state, the robot will continue its painting actions across the entire canvas. We also demonstrate and propose alternative robot-artist interactions using natural language and physical touch. This work combines the biometrics of a human artist to inform fluent artistic interactions.

</details>


### [169] [Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization](https://arxiv.org/abs/2511.15194)
*Jian Deng,Yuandong Wang,Yangfu Zhu,Tao Feng,Tianyu Wo,Zhenzhou Shao*

Main category: cs.RO

TL;DR: 提出了一种基于SE(2)群等变理论的通用规范化框架Eq.Bot，用于机器人操作学习，能够在不修改模型结构的情况下赋予模型空间等变性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏几何一致性保证，难以处理旋转和平移等空间变换，且当前引入等变性的方法复杂度高、计算成本大、可移植性差。

Method: 基于SE(2)群等变理论，构建一个将观测转换到规范空间、在该空间执行策略后再映射回原空间的模型无关框架。

Result: 在CNN和Transformer架构下均显著优于现有方法，在多种机器人操作任务中性能提升最高达50.0%。

Conclusion: Eq.Bot作为一种通用、可移植的等变性增强框架，有效提升了机器人操作中对空间变换的鲁棒性和泛化能力。

Abstract: Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.

</details>


### [170] [VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation](https://arxiv.org/abs/2511.15200)
*Tairan He,Zi Wang,Haoru Xue,Qingwei Ben,Zhengyi Luo,Wenli Xiao,Ye Yuan,Xingye Da,Fernando Castañeda,Shankar Sastry,Changliu Liu,Guanya Shi,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种名为VIRAL的视觉仿真到现实（sim-to-real）框架，用于实现人形机器人在无真实世界微调的情况下自主完成移动操作任务。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在现实世界中部署的主要障碍是缺乏自主的移动与操作能力。

Method: 采用教师-学生架构：教师策略在完整状态信息下通过强化学习训练长视野的移动操作技能；学生策略则通过大规模仿真中的视觉数据，结合在线DAgger和行为克隆进行蒸馏学习，并应用大规模视觉域随机化和真实到仿真的对齐来缩小仿真与现实差距。

Result: 在Unitree G1人形机器人上实现了基于RGB图像的连续移动操作，最多完成54个周期的任务，且无需真实世界微调即可泛化到多种空间和外观变化，性能接近专家级遥操作水平。

Conclusion: 大规模计算资源和视觉域随机化对于实现基于RGB的人形机器人移动操作至关重要，VIRAL为零样本仿真到现实迁移提供了有效方案。

Abstract: A key barrier to the real-world deployment of humanoid robots is the lack of autonomous loco-manipulation skills. We introduce VIRAL, a visual sim-to-real framework that learns humanoid loco-manipulation entirely in simulation and deploys it zero-shot to real hardware. VIRAL follows a teacher-student design: a privileged RL teacher, operating on full state, learns long-horizon loco-manipulation using a delta action space and reference state initialization. A vision-based student policy is then distilled from the teacher via large-scale simulation with tiled rendering, trained with a mixture of online DAgger and behavior cloning. We find that compute scale is critical: scaling simulation to tens of GPUs (up to 64) makes both teacher and student training reliable, while low-compute regimes often fail. To bridge the sim-to-real gap, VIRAL combines large-scale visual domain randomization over lighting, materials, camera parameters, image quality, and sensor delays--with real-to-sim alignment of the dexterous hands and cameras. Deployed on a Unitree G1 humanoid, the resulting RGB-based policy performs continuous loco-manipulation for up to 54 cycles, generalizing to diverse spatial and appearance variations without any real-world fine-tuning, and approaching expert-level teleoperation performance. Extensive ablations dissect the key design choices required to make RGB-based humanoid loco-manipulation work in practice.

</details>


### [171] [A Class of Dual-Frame Passively-Tilting Fully-Actuated Hexacopter](https://arxiv.org/abs/2511.15225)
*Jiajun Liu,Yimin Zhu,Xiaorui Liu,Mingye Cao,Mingchao Li,Lixian Zhang*

Main category: cs.RO

TL;DR: 提出了一种具有双框架被动倾斜结构的全驱动六旋翼无人机，能够以最少的执行器独立控制平移运动和姿态，消除了内力抵消，提高了飞行效率和续航能力，并通过仿真验证了其全驱动运动能力和控制策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高传统全驱动无人机的飞行效率和续航能力，解决内力抵消导致的能量浪费问题。

Method: 设计了一种新型双框架被动倾斜结构的全驱动六旋翼无人机，建立其动力学模型，并开发相应的全驱动控制器。

Result: 仿真结果表明该无人机具备优越的全驱动运动能力，且所提出的控制策略能够实现高效稳定的控制。

Conclusion: 该全驱动六旋翼无人机在等效载荷下具有更高的飞行效率和更长的续航时间，是一种高效、稳定的新型无人机平台。

Abstract: This paper proposed a novel fully-actuated hexacopter. It features a dual-frame passive tilting structure and achieves independent control of translational motion and attitude with minimal actuators. Compared to previous fully-actuated UAVs, it liminates internal force cancellation, resulting in higher flight efficiency and endurance under equivalent payload conditions. Based on the dynamic model of fully-actuated hexacopter, a full-actuation controller is designed to achieve efficient and stable control. Finally, simulation is conducted, validating the superior fully-actuated motion capability of fully-actuated hexacopter and the effectiveness of the proposed control strategy.

</details>


### [172] [Modelling and Model-Checking a ROS2 Multi-Robot System using Timed Rebeca](https://arxiv.org/abs/2511.15227)
*Hiep Hong Trinh,Marjan Sirjani,Federico Ciccozzi,Abu Naser Masud,Mikael Sjödin*

Main category: cs.RO

TL;DR: 本文提出使用基于模型的开发方法，结合形式化验证技术（特别是模型检测），利用Timed Rebeca语言对ROS2多机器人系统进行建模与验证，解决离散建模与连续系统间的鸿沟，并通过离散化策略和优化技术提升验证效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中存在复杂的异步交互和并发行为，传统开发方式难以保证设计正确性；需要一种能支持时间语义、并发性和自动验证的建模方法来提升多机器人系统的可靠性与开发效率。

Method: 采用基于Actor的建模语言Timed Rebeca对ROS2节点拓扑、物理信号和运动原语等进行建模，设计针对不同类型信息的离散化策略，识别足够的抽象阈值，并应用优化技术压缩状态空间以提高模型检测效率。

Result: 成功实现了多机器人系统的离散化建模与高效模型检测，验证了关键系统属性，建立了模型与实现之间的双向工程流程，并公开发布了Rebeca与ROS2代码作为多自主机器人系统建模的基础。

Conclusion: Timed Rebeca能够有效支持多机器人系统的形式化建模与验证，所提出的离散化与优化策略可在保持模型准确性的同时显著提升验证效率，展示了模型驱动开发在复杂机器人系统中的可行性与优势。

Abstract: Model-based development enables quicker prototyping, earlier experimentation and validation of design intents. For a multi-agent system with complex asynchronous interactions and concurrency, formal verification, model-checking in particular, offers an automated mechanism for verifying desired properties. Timed Rebeca is an actor-based modelling language supporting reactive, concurrent and time semantics, accompanied with a model-checking compiler. These capabilities allow using Timed Rebeca to correctly model ROS2 node topographies, recurring physical signals, motion primitives and other timed and time-convertible behaviors. The biggest challenges in modelling and verifying a multi-robot system lie in abstracting complex information, bridging the gap between a discrete model and a continuous system and compacting the state space, while maintaining the model's accuracy. We develop different discretization strategies for different kinds of information, identifying the 'enough' thresholds of abstraction, and applying efficient optimization techniques to boost computations. With this work we demonstrate how to use models to design and verify a multi-robot system, how to discretely model a continuous system to do model-checking efficiently, and the round-trip engineering flow between the model and the implementation. The released Rebeca and ROS2 codes can serve as a foundation for modelling multiple autonomous robots systems.

</details>


### [173] [Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms](https://arxiv.org/abs/2511.15274)
*Alexander Boldachev*

Main category: cs.RO

TL;DR: 本文比较了行为树（BTs）和可执行本体（EO）两种机器人行为建模方法，指出EO通过事件驱动的语义建模在动态系统中具有更强的灵活性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器人控制中语义与过程之间的鸿沟，探索更适应动态环境的行为建模方法。

Method: 对比基于控制流的命令式行为树与基于数据流规则的声明式可执行本体，并在移动操作任务中进行实践验证。

Result: EO在反应性与模块化方面可媲美BTs，支持运行时模型修改、全时序可追溯性和统一的数据-逻辑-接口表示，但在稳定场景下BTs仍具优势。

Conclusion: EO提供了一种从程序化编程向语义化建模转变的新框架，在动态演化系统中展现出潜力，是对BTs的有益补充。

Abstract: This paper compares two distinct approaches to modeling robotic behavior: imperative Behavior Trees (BTs) and declarative Executable Ontologies (EO), implemented through the boldsea framework. BTs structure behavior hierarchically using control-flow, whereas EO represents the domain as a temporal, event-based semantic graph driven by dataflow rules. We demonstrate that EO achieves comparable reactivity and modularity to BTs through a fundamentally different architecture: replacing polling-based tick execution with event-driven state propagation. We propose that EO offers an alternative framework, moving from procedural programming to semantic domain modeling, to address the semantic-process gap in traditional robotic control. EO supports runtime model modification, full temporal traceability, and a unified representation of data, logic, and interface - features that are difficult or sometimes impossible to achieve with BTs, although BTs excel in established, predictable scenarios. The comparison is grounded in a practical mobile manipulation task. This comparison highlights the respective operational strengths of each approach in dynamic, evolving robotic systems.

</details>


### [174] [Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception](https://arxiv.org/abs/2511.15279)
*Jiashu Yang,Yifan Han,Yucheng Xie,Ning Guo,Wenzhao Lian*

Main category: cs.RO

TL;DR: EyeVLA提出了一种用于主动视觉感知的机器人眼球系统，通过将动作离散化为动作令牌并与视觉-语言模型结合，实现视觉、语言与动作的联合建模，在真实环境中通过旋转和缩放等指令驱动动作，高效获取精确的视觉信息。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型和固定RGB-D相机系统无法兼顾大范围覆盖与细粒度细节获取，限制了开放世界机器人应用中的感知能力，因此需要一种能主动选择视角、按需获取信息的主动视觉感知系统。

Method: 将动作行为离散化为动作令牌，并与具备强开放世界理解能力的视觉-语言模型（VLM）融合，构建单一自回归序列进行联合建模；利用2D边界框坐标引导推理链，并通过强化学习优化视角选择策略，仅用少量真实数据即可将VLM的场景理解能力迁移至视觉-语言-动作（VLA）策略。

Result: 实验表明，EyeVLA能在真实环境中高效执行指令驱动的任务，通过主动旋转和缩放动作获取更准确的视觉信息，显著提升环境感知能力。

Conclusion: EyeVLA是一种新颖的主动视觉感知机器人系统，能够基于指令主动获取高信息量的视觉观测，支持下游具身任务，推动了开放世界中机器人视觉系统的发展。

Abstract: In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.

</details>


### [175] [Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2511.15284)
*Jonas De Maeyer,Hossein Yarahmadi,Moharram Challenger*

Main category: cs.RO

TL;DR: 提出了一种可扩展的、区域感知的强化学习框架，用于动态环境中的路径规划，通过分层环境分解和分布式RL代理实现局部适应，并引入基于子环境成功率的重训练机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设环境完全不可预测或依赖全局规划器，限制了在现实场景中的可扩展性和实用性。本文旨在通过利用环境变化常局限于局部区域的特点，提升动态路径规划的效率与可扩展性。

Method: 采用分层环境分解策略，部署多个分布式强化学习代理进行局部适应；提出基于子环境成功率为依据的重训练机制；探索单智能体Q学习与多智能体联邦Q学习两种训练范式，通过周期性聚合局部Q表加速学习。

Result: 联邦学习变体始终优于单智能体方法，在多障碍物同时变化和难度递增的真实场景中表现优异，性能接近A* Oracle，且适应时间更短、可扩展性更强。

Conclusion: 该去中心化框架无需全局规划器，为未来结合深度强化学习和灵活环境分解的改进奠定了基础，尽管大规模环境下的初始训练时间仍需优化。

Abstract: Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.

</details>


### [176] [Optimizing Robot Positioning Against Placement Inaccuracies: A Study on the Fanuc CRX10iA/L](https://arxiv.org/abs/2511.15290)
*Nicolas Gautier,Yves Guillermit,Mathieu Porez,David Lemoine,Damien Chablat*

Main category: cs.RO

TL;DR: 提出了一种基于粒子群优化和α-形状算法的方法，用于确定Fanuc CRX10iA/L协作机器人在工业任务轨迹下的最优基座位置，并考虑了逆运动学多解及实际部署中的放置误差。


<details>
  <summary>Details</summary>
Motivation: 解决协作机器人在移动平台或人工部署时因基座位置不准确而导致的轨迹执行问题，提升机器人放置的鲁棒性。

Method: 采用粒子群优化算法搜索可行区域，结合α-形状算法界定边界，并利用Voronoi图计算最大内切圆以提高鲁棒性；通过逆运动学模型和雅可比矩阵评估各初始构型并沿轨迹评分。

Result: 成功识别出多个可行基座位置区域，并能处理最多16组逆运动学解，有效考虑了关节限位、奇异性及工作空间约束。

Conclusion: 该方法为协作机器人在存在放置误差场景下的部署提供了可靠的基座定位方案，具有实际工业应用价值。

Abstract: This study presents a methodology for determining the optimal base placement of a Fanuc CRX10iA/L collaborative robot for a desired trajectory corresponding to an industrial task. The proposed method uses a particle swarm optimization algorithm that explores the search space to find positions for performing the trajectory. An $α$-shape algorithm is then used to draw the borders of the feasibility areas, and the largest circle inscribed is calculated from the Voronoi diagrams. The aim of this approach is to provide a robustness criterion in the context of robot placement inaccuracies that may be encountered, for example, if the robot is placed on a mobile base when the system is deployed by an operator. The approach developed uses an inverse kinematics model to evaluate all initial configurations, then moves the robot end-effector along the reference trajectory using the Jacobian matrix and assigns a score to the attempt. For the Fanuc CRX10iA/L robot, there can be up to 16 solutions to the inverse kinematics model. The calculation of these solutions is not trivial and requires a specific study that planning tools such as MoveIt cannot fully take into account. Additionally, the optimization process must consider constraints such as joint limits, singularities, and workspace limitations to ensure feasible and efficient trajectory execution.

</details>


### [177] [MSA - Technique for Stiffness Modeling of Manipulators with Complex and Hybrid Structures](https://arxiv.org/abs/2511.15294)
*Alexandr Klimchik,Anatol Pashkevich,Damien Chablat*

Main category: cs.RO

TL;DR: 提出了一种基于矩阵结构分析的混合结构操作手机器人刚度建模的系统化半解析方法，适用于包含闭环、柔性连杆和外部载荷的复杂结构。


<details>
  <summary>Details</summary>
Motivation: 传统刚度建模方法难以处理具有混合架构（如闭环、弹性关节和外部预加载）的操作机，缺乏统一且高效的建模框架。

Method: 采用矩阵结构分析方法，将操作机的刚度模型表示为描述连杆弹性的方程组，并结合描述连杆间连接关系的约束方程，避免了扩展刚度矩阵中的行列合并操作。

Result: 实现了复杂混合结构操作机（如NaVaRo）的笛卡尔刚度矩阵的半解析计算，提高了建模效率与灵活性。

Conclusion: 该方法为复杂结构机器人提供了通用、模块化的刚度建模框架，显著简化了建模过程，适用于多种结构类型和载荷条件。

Abstract: The paper presents a systematic approach for stiffness modeling of manipulators with complex and hybrid structures using matrix structural analysis. In contrast to previous results, it is suitable for mixed architectures containing closed-loops, flexible links, rigid connections, passive and elastic joints with external loadings and preloadings. The proposed approach produces the Cartesian stiffness matrices in a semi-analytical manner. It presents the manipulator stiffness model as a set of conventional equations describing the link elasticities that are supplemented by a set of constraints describing connections between links. Its allows user straightforward aggregation of stiffness model equations avoiding traditional column/row merging procedures in the extended stiffness matrix. Advantages of this approach are illustrated by stiffness analysis of NaVaRo manipulator.

</details>


### [178] [C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models](https://arxiv.org/abs/2511.15333)
*Nayoung Oh,Dohyun Kim,Junhyeong Bang,Rohan Paul,Daehyung Park*

Main category: cs.RO

TL;DR: 提出了一种名为C2F-Space的粗到细空间定位框架，通过结合视觉语言模型和超像素化技术，在复杂场景中实现了更精确的空间引用定位，并在新构建的基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂的推理（如距离、几何和对象间关系），而视觉语言模型虽然具有较强的推理能力，但难以生成细粒度的输出区域。

Method: 设计了一个基于网格的视觉定位提示和提出-验证策略，利用视觉语言模型进行粗略估计，然后通过超像素化对区域进行局部细化以适应周围环境。

Result: 在新构建的空间定位基准上，C2F-Space在成功率和交并比指标上显著优于五个最先进的基线方法。消融实验验证了两步过程各模块的有效性及其协同效应。

Conclusion: C2F-Space框架有效克服了现有方法的局限性，能够在复杂环境中实现更准确的空间定位，并成功应用于模拟机器人抓取放置任务。

Abstract: Space grounding refers to localizing a set of spatial references described in natural language instructions. Traditional methods often fail to account for complex reasoning -- such as distance, geometry, and inter-object relationships -- while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs. To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization. For the coarse estimation, we design a grid-based visual-grounding prompt with a propose-validate strategy, maximizing VLM's spatial understanding and yielding physically and semantically valid canonical region (i.e., ellipses). For the refinement, we locally adapt the region to surrounding environment without over-relaxed to free space. We construct a new space-grounding benchmark and compare C2F-Space with five state-of-the-art baselines using success rate and intersection-over-union. Our C2F-Space significantly outperforms all baselines. Our ablation study confirms the effectiveness of each module in the two-step process and their synergistic effect of the combined framework. We finally demonstrate the applicability of C2F-Space to simulated robotic pick-and-place tasks.

</details>


### [179] [Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention](https://arxiv.org/abs/2511.15358)
*Gabriele Calzolari,Vidya Sumathy,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出一种平台无关的强化学习框架，结合图神经网络策略与安全过滤器，实现障碍物密集环境中的高效安全自主探索。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中实现高效且安全的自主探索，避免碰撞并提升探索效率。

Method: 采用基于图神经网络的策略进行下一航点选择，并结合安全过滤器修正不可行动作；使用PPO算法训练策略，设计基于势场的奖励函数，综合考虑未探索区域的距离和信息增益。

Result: 仿真和实验室实验证明该方法在杂乱环境中能有效实现安全、高效的探索，减少了安全过滤器的干预次数。

Conclusion: 该框架成功融合了强化学习的适应性与显式安全机制的可靠性，有助于推动学习型策略在真实机器人系统中的部署。

Abstract: Autonomous exploration of obstacle-rich spaces requires strategies that ensure efficiency while guaranteeing safety against collisions with obstacles. This paper investigates a novel platform-agnostic reinforcement learning framework that integrates a graph neural network-based policy for next-waypoint selection, with a safety filter ensuring safe mobility. Specifically, the neural network is trained using reinforcement learning through the Proximal Policy Optimization (PPO) algorithm to maximize exploration efficiency while minimizing safety filter interventions. Henceforth, when the policy proposes an infeasible action, the safety filter overrides it with the closest feasible alternative, ensuring consistent system behavior. In addition, this paper introduces a reward function shaped by a potential field that accounts for both the agent's proximity to unexplored regions and the expected information gain from reaching them. The proposed framework combines the adaptability of reinforcement learning-based exploration policies with the reliability provided by explicit safety mechanisms. This feature plays a key role in enabling the deployment of learning-based policies on robotic platforms operating in real-world environments. Extensive evaluations in both simulations and experiments performed in a lab environment demonstrate that the approach achieves efficient and safe exploration in cluttered spaces.

</details>


### [180] [RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer](https://arxiv.org/abs/2511.15414)
*Mingyang Feng,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 提出了一种结合RRT*和Transformer的新型采样路径规划算法RRT*former，利用环境和历史采样信息提升路径最优性和采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有采样算法忽略环境或历史采样信息，导致采样效率和路径质量受限。

Method: 将标准RRT*算法与Transformer网络结合，用Transformer提取环境特征并利用历史样本信息指导新采样的生成。

Result: 实验表明，相比RRT*、Neural RRT*等方法，RRT*former在路径最优性和采样效率上均有显著提升。

Conclusion: RRT*former通过融合Transformer有效利用环境与历史信息，显著提升了复杂动态环境下的路径规划性能。

Abstract: We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.

</details>


### [181] [Discovering Optimal Natural Gaits of Dissipative Systems via Virtual Energy Injection](https://arxiv.org/abs/2511.15513)
*Korbinian Griesbauer,Davide Calzolari,Maximilian Raff,C. David Remy,Alin Albu-Schäffer*

Main category: cs.RO

TL;DR: 提出一种基于自然动力学的能量最优控制方法，通过能量注入技术和连续性方法优化弹性腿式机器人的能耗。


<details>
  <summary>Details</summary>
Motivation: 提高腿式机器人在非结构化环境中的能量效率，弥补其相比轮式机器人能效较低的不足。

Method: 设计了一种新的能量注入技术来识别系统的被动运动模式，并结合连续性方法求解全驱动、耗散系统的能量最优控制输入。

Result: 在单腿和多腿机器人仿真模型上验证了该方法的有效性，成功发现了利用系统自然动力的节能控制策略。

Conclusion: 该框架为弹性腿式机器人的设计与控制提供了有效途径，显著提升其能量效率和环境适应能力。

Abstract: Legged robots offer several advantages when navigating unstructured environments, but they often fall short of the efficiency achieved by wheeled robots. One promising strategy to improve their energy economy is to leverage their natural (unactuated) dynamics using elastic elements. This work explores that concept by designing energy-optimal control inputs through a unified, multi-stage framework. It starts with a novel energy injection technique to identify passive motion patterns by harnessing the system's natural dynamics. This enables the discovery of passive solutions even in systems with energy dissipation caused by factors such as friction or plastic collisions. Building on these passive solutions, we then employ a continuation approach to derive energy-optimal control inputs for the fully actuated, dissipative robotic system. The method is tested on simulated models to demonstrate its applicability in both single- and multi-legged robotic systems. This analysis provides valuable insights into the design and operation of elastic legged robots, offering pathways to improve their efficiency and adaptability by exploiting the natural system dynamics.

</details>


### [182] [Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies](https://arxiv.org/abs/2511.15520)
*Gabriel Lauzier,Alexandre Girard,François Ferland*

Main category: cs.RO

TL;DR: 提出了一种在部分去噪后即执行动作的方法，通过耦合系统动力学与去噪过程，实现快速模仿学习，并给出基于演示方差的稳定性判断指标。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现优异，但其反向去噪过程计算昂贵，难以满足实时性要求。

Method: 研究在部分去噪阶段就执行动作的可行性，分析植物动力学与去噪动态耦合时闭环系统的稳定性理论边界。

Result: 建立了一个更快的模仿学习框架，并提出了一个基于演示方差判断控制器稳定性的度量指标。

Conclusion: 该方法能够在保证稳定性的同时加速决策，适用于实时机器人控制任务。

Abstract: Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.

</details>


### [183] [Decentralized Gaussian Process Classification and an Application in Subsea Robotics](https://arxiv.org/abs/2511.15529)
*Yifei Gao,Hans J. He,Daniel J. Stilwell,James McMahon*

Main category: cs.RO

TL;DR: 提出一种去中心化的数据共享策略，用于多自主水下航行器（AUV）在实时构建通信成功概率地图时高效共享测量数据，并通过真实水下通信数据验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于水声通信存在范围有限、多径效应和低带宽等问题，团队AUV需要实时学习通信环境以应对不确定性，从而提升协作效率。

Method: 将问题建模为去中心化分类问题，AUV仅共享部分通信测量数据；基于严格推导设计一种数据共享策略，以优化信息融合并构建通信成功概率图。

Result: 利用Virginia Tech 690 AUV采集的真实水声通信数据对所提共享策略进行实验验证，结果表明该策略能有效提升通信地图构建的准确性与效率。

Conclusion: 所提出的共享策略能够在资源受限的水下环境中有效支持多AUV系统的协同通信地图构建，具有实际应用潜力。

Abstract: Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.

</details>


### [184] [NMPC-based Motion Planning with Adaptive Weighting for Dynamic Object Interception](https://arxiv.org/abs/2511.15532)
*Chen Cai,Saksham Kohli,Steven Liu*

Main category: cs.RO

TL;DR: 提出一种基于非线性模型预测控制（MPC）的运动规划方法，用于双臂协作机器人动态捕捉快速移动物体，相比传统方法显著降低控制能耗并提升运动质量。


<details>
  <summary>Details</summary>
Motivation: 解决协作机械臂在闭链约束下捕捉高速物体时的协调控制难题，尤其是传统MPC方法易导致执行器功率超限的问题。

Method: 提出自适应终端（AT）MPC框架，通过代价整形优化轨迹生成，结合高层拦截规划与实时关节空间控制，并与依赖终端惩罚的原始终端（PT）MPC进行对比。

Result: 实验显示AT-MPC在真实双臂平台上平均规划周期为19 ms（低于40 ms采样周期），有效避免功率越限，显著降低控制 effort，且计算开销小。

Conclusion: AT-MPC在动态协作捕捉任务中表现出更优的运动质量、鲁棒性和能效，适合高动态实时控制应用。

Abstract: Catching fast-moving objects serves as a benchmark for robotic agility, posing significant coordination challenges for cooperative manipulator systems holding a catcher, particularly due to inherent closed-chain constraints. This paper presents a nonlinear model predictive control (MPC)-based motion planner that bridges high-level interception planning with real-time joint space control, enabling dynamic object interception for systems comprising two cooperating arms. We introduce an Adaptive- Terminal (AT) MPC formulation featuring cost shaping, which contrasts with a simpler Primitive-Terminal (PT) approach relying heavily on terminal penalties for rapid convergence. The proposed AT formulation is shown to effectively mitigate issues related to actuator power limit violations frequently encountered with the PT strategy, yielding trajectories and significantly reduced control effort. Experimental results on a robotic platform with two cooperative arms, demonstrating excellent real time performance, with an average planner cycle computation time of approximately 19 ms-less than half the 40 ms system sampling time. These results indicate that the AT formulation achieves significantly improved motion quality and robustness with minimal computational overhead compared to the PT baseline, making it well-suited for dynamic, cooperative interception tasks.

</details>


### [185] [UltraDP: Generalizable Carotid Ultrasound Scanning with Force-Aware Diffusion Policy](https://arxiv.org/abs/2511.15550)
*Ruoqu Chen,Xiangjie Yan,Kangchen Lv,Gao Huang,Zheng Li,Xiang Li*

Main category: cs.RO

TL;DR: 提出了一种基于扩散策略（Diffusion-Policy）的自主超声扫描方法UltraDP，结合多模态输入和专用引导模块，在颈动脉扫描中实现了95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有超声扫描机器人泛化能力差、数据利用率低，难以应对患者解剖结构差异和人机交互复杂性。

Method: 采用Diffusion-Policy框架，融合超声图像、腕部相机图像、接触力矩和探头姿态等多感官输入，结合专用引导模块生成动作，并使用混合力-阻抗控制器确保安全稳定的接触控制。

Result: 在包含21名志愿者、46万个样本对的大规模颈动脉扫描数据集上训练，UltraDP在未见过的受试者横断面扫描中达到95%的成功率。

Conclusion: UltraDP展现出强大的泛化能力和高效的数据利用，显著提升了自主超声扫描的性能和可靠性。

Abstract: Ultrasound scanning is a critical imaging technique for real-time, non-invasive diagnostics. However, variations in patient anatomy and complex human-in-the-loop interactions pose significant challenges for autonomous robotic scanning. Existing ultrasound scanning robots are commonly limited to relatively low generalization and inefficient data utilization. To overcome these limitations, we present UltraDP, a Diffusion-Policy-based method that receives multi-sensory inputs (ultrasound images, wrist camera images, contact wrench, and probe pose) and generates actions that are fit for multi-modal action distributions in autonomous ultrasound scanning of carotid artery. We propose a specialized guidance module to enable the policy to output actions that center the artery in ultrasound images. To ensure stable contact and safe interaction between the robot and the human subject, a hybrid force-impedance controller is utilized to drive the robot to track such trajectories. Also, we have built a large-scale training dataset for carotid scanning comprising 210 scans with 460k sample pairs from 21 volunteers of both genders. By exploring our guidance module and DP's strong generalization ability, UltraDP achieves a 95% success rate in transverse scanning on previously unseen subjects, demonstrating its effectiveness.

</details>


### [186] [SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.15605)
*Senyu Fei,Siyin Wang,Li Ji,Ao Li,Shiduo Zhang,Liming Liu,Jinlong Hou,Jingjing Gong,Xianzhong Zhao,Xipeng Qiu*

Main category: cs.RO

TL;DR: 提出了一种名为Self-Referential Policy Optimization (SRPO)的新型视觉-语言-动作强化学习框架，利用模型自身生成的成功轨迹作为自我参考，通过潜在世界表征衡量行为进展，显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖专家示范导致示范偏差，而当前VLA-RL方法受限于严重稀疏奖励问题，无法有效利用失败轨迹中的有用信息。

Method: SRPO利用当前训练批次中模型自身生成的成功轨迹作为自参考，赋予失败尝试以进度奖励；采用世界模型的潜在空间编码来度量跨环境的行为进展，实现无需外部示范或人工奖励设计的高效强化学习。

Result: 在LIBERO基准上，从48.9%成功率的监督基线出发，仅用200步强化学习即达到99.2%的新SOTA性能，相对提升103%；在LIBERO-Plus上性能提升达167%。

Conclusion: SRPO通过自参照机制和潜在世界表征有效缓解奖励稀疏问题，显著提升VLA模型的训练效率与泛化能力，无需额外监督即可实现卓越性能。

Abstract: Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.

</details>


### [187] [Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography](https://arxiv.org/abs/2511.15614)
*Sai Puppala,Ismail Hossain,Jahangir Alam,Sajedul Talukder*

Main category: cs.RO

TL;DR: 本文提出了一种名为Optimus-Q的机器人系统，用于在核电站中自主监测空气质量与污染，结合联邦学习和量子密钥分发技术，实现安全、高效的环境监控。


<details>
  <summary>Details</summary>
Motivation: 为了提升核电厂等高风险环境中的安全性、效率和环境监测能力，需要一种能够实时检测污染物并保障数据隐私与通信安全的智能机器人系统。

Method: Optimus-Q机器人采用红外传感器实时采集环境数据，结合系统性导航模式和机器学习算法进行气体排放预测；通过联邦学习实现跨电站协同学习以保护数据隐私，并利用量子密钥分发（QKD）确保通信安全。

Result: 通过仿真和实地实验验证，Optimus-Q能有效提升核设施的操作安全性和应急响应能力，具备高效的区域覆盖和污染监测性能。

Conclusion: 融合机器人、机器学习与量子通信技术的Optimus-Q系统为高风险环境下的监测提供了创新且可靠的解决方案，具有广泛的应用前景。

Abstract: The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.

</details>


### [188] [Real-time Point Cloud Data Transmission via L4S for 5G-Edge-Assisted Robotics](https://arxiv.org/abs/2511.15677)
*Gerasimos Damigos,Achilleas Santi Seisa,Nikolaos Stathoulopoulos,Sara Sandberg,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种基于速率自适应和点云编码的实时LiDAR数据传输框架，适用于机器人远程处理，结合L4S和SCReAM v2协议与Draco压缩算法，在5G网络下实现低延迟、低丢包的3D点云流传输。


<details>
  <summary>Details</summary>
Motivation: 为了满足机器人应用中对高带宽、低延迟LiDAR数据远程传输的需求，解决传统传输方法在动态网络环境下延迟高、丢包严重的问题。

Method: 扩展了L4S支持的SCReAM v2传输框架，集成Draco几何压缩算法，根据网络状况动态调整点云压缩率，实现低延迟、低损耗的数据流传输。

Result: 在城市5G网络中进行了多公里实地实验，实现了稳定的低延迟和低丢包传输，并成功支持了实时3D SLAM算法的远程处理。

Conclusion: 所提出的框架能有效支持高带宽LiDAR数据在不可靠网络下的实时传输，满足机器人应用对延迟和精度的要求，具有实际部署价值。

Abstract: This article presents a novel framework for real-time Light Detection and Ranging (LiDAR) data transmission that leverages rate-adaptive technologies and point cloud encoding methods to ensure low-latency, and low-loss data streaming. The proposed framework is intended for, but not limited to, robotic applications that require real-time data transmission over the internet for offloaded processing. Specifically, the Low Latency, Low Loss, Scalable Throughput L4S-enabled SCReAM v2 transmission framework is extended to incorporate the Draco geometry compression algorithm, enabling dynamic compression of high-bitrate 3D LiDAR data according to the sensed channel capacity and network load. The low-latency 3D LiDAR streaming system is designed to maintain minimal end-to-end delay while constraining encoding errors to meet the accuracy requirements of robotic applications. We demonstrate the effectiveness of the proposed method through real-world experiments conducted over a public 5G network across multi-kilometer urban environments. The low-latency and low-loss requirements are preserved, while real-time offloading and evaluation of 3D SLAM algorithms are used to validate the framework's performance in practical use cases.

</details>


### [189] [In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data](https://arxiv.org/abs/2511.15704)
*Xiongyi Cai,Ri-Zhao Qiu,Geng Chen,Lai Wei,Isabella Liu,Tianshu Huang,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的方法来收集和利用自我中心视频数据，通过将人类数据分为“野外”和“任务中”两类，并构建了包含超过1000小时野外数据和20小时任务数据的PHSD数据集，训练出语言条件化的大型策略模型Human0，在无需机器人演示的情况下实现了指令跟随、少样本学习和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的自我中心视频数据利用方式受限于数据异质性，通常仅用于简单的预训练，未能充分挖掘其潜力。因此需要一种更系统、可扩展的方法来高效利用这类数据以提升操纵策略的学习效果。

Method: 将人类自我中心数据分为‘in-the-wild’（野外）和‘on-task’（任务中）两类，构建PHSD数据集；采用基于流匹配的语言条件化策略模型，并结合领域自适应技术缩小人与人形机器人之间的差距。

Result: 训练出的Human0模型能仅从人类数据中实现语言指令跟随、具备少样本学习能力，并通过任务中数据提升了控制鲁棒性，在多种操作任务中表现出良好性能。

Conclusion: 通过系统性地分类和利用自我中心人类数据，结合大规模训练与领域自适应，可以有效提升语言条件化操纵策略的泛化性、适应性和实用性，为未来人形机器人学习提供了可行路径。

Abstract: Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [190] [Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm](https://arxiv.org/abs/2511.14763)
*Li Cuihong,Huang Xiaowen,Yin Chuanhuan,Sang Jitao*

Main category: cs.IR

TL;DR: 本文提出了一种基于知识蒸馏的成员推断攻击范式，用于提升对基于大语言模型的推荐系统的攻击性能。


<details>
  <summary>Details</summary>
Motivation: 传统的成员推断攻击依赖影子模型来获取目标模型特征，但在大语言模型推荐系统中难以构建影子模型。

Method: 引入知识蒸馏技术，分别对成员和非成员数据进行蒸馏，构建具有更强区分能力的参考模型，并提取个体特征用于训练融合特征的攻击模型。

Result: 所提方法在成员推断攻击性能上优于基于影子模型的攻击方法。

Conclusion: 知识蒸馏能够有效增强参考模型对成员与非成员数据的判别能力，从而提升成员推断攻击的效果。

Abstract: Membership Inference Attack (MIA) aims to determine if a data sample is used in the training dataset of a target model. Traditional MIA obtains feature of target model via shadow models and uses the feature to train attack model, but the scale and complexity of training or fine-tuning data for large language model (LLM)-based recommendation systems make shadow models difficult to construct. Knowledge distillation as a method for extracting knowledge contributes to construct a stronger reference model. Knowledge distillation enables separate distillation for member and non-member data during the distillation process, enhancing the model's discriminative capability between the two in MIA. This paper propose a knowledge distillation-based MIA paradigm to improve the performance of membership inference attacks on LLM-based recommendation systems. Our paradigm introduces knowledge distillation to obtain a reference model, which enhances the reference model's ability to distinguish between member and non-member data. We obtain individual features from the reference model and train our attack model with fused feature. Our paradigm improves the attack performance of MIA compared to shadow model-based attack.

</details>


### [191] [Image-Seeking Intent Prediction for Cross-Device Product Search](https://arxiv.org/abs/2511.14764)
*Mariya Hendriksen,Svitlana Vakulenko,Jordan Massiah,Gabriella Kazai,Emine Yilmaz*

Main category: cs.IR

TL;DR: 本文提出了图像寻求意图预测任务，利用大规模生产数据训练了一个名为IRP的模型，以预测语音查询是否需要视觉辅助和跨设备切换，从而提升电商用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着用户在多设备间购物，如何精准预测何时需要视觉增强和设备切换成为提升用户体验的关键挑战。

Method: 使用90万条语音查询数据及产品元数据，结合查询语义与产品信息，并通过轻量级摘要和可微分精度导向损失优化模型。

Result: 实验表明，结合查询语义与产品数据能持续提升预测准确性，且有效降低误报率。

Conclusion: LLM驱动的跨设备购物助手能够智能预判用户需求，实现更流畅、个性化的电商体验。

Abstract: Large Language Models (LLMs) are transforming personalized search, recommendations, and customer interaction in e-commerce. Customers increasingly shop across multiple devices, from voice-only assistants to multimodal displays, each offering different input and output capabilities. A proactive suggestion to switch devices can greatly improve the user experience, but it must be offered with high precision to avoid unnecessary friction. We address the challenge of predicting when a query requires visual augmentation and a cross-device switch to improve product discovery. We introduce Image-Seeking Intent Prediction, a novel task for LLM-driven e-commerce assistants that anticipates when a spoken product query should proactively trigger a visual on a screen-enabled device. Using large-scale production data from a multi-device retail assistant, including 900K voice queries, associated product retrievals, and behavioral signals such as image carousel engagement, we train IRP (Image Request Predictor), a model that leverages user input query and corresponding retrieved product metadata to anticipate visual intent. Our experiments show that combining query semantics with product data, particularly when improved through lightweight summarization, consistently improves prediction accuracy. Incorporating a differentiable precision-oriented loss further reduces false positives. These results highlight the potential of LLMs to power intelligent, cross-device shopping assistants that anticipate and adapt to user needs, enabling more seamless and personalized e-commerce experiences.

</details>


### [192] [Optimizing Agricultural Research: A RAG-Based Approach to Mycorrhizal Fungi Information](https://arxiv.org/abs/2511.14765)
*Mohammad Usman Altam,Md Imtiaz Habib,Tuan Hoang*

Main category: cs.IR

TL;DR: 本文提出了一种基于检索增强生成（RAG）的系统，用于支持菌根真菌在可持续农业中的应用，通过结合语义检索与结构化数据提取，提升农业知识问答的准确性与实用性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型受限于静态训练数据，难以满足农业领域对动态、专业性知识的需求，因此需要一种能整合外部专业知识的智能系统以支持农学决策。

Method: 采用双层架构：第一层利用向量嵌入从农学文献中进行语义检索并增强上下文；第二层通过结构化信息抽取获取实验元数据（如接种方式、孢子密度、土壤参数和产量结果），并将嵌入存储于高性能向量数据库以实现近实时检索。

Result: 该系统能够高效检索并合成关于丛枝菌根真菌（AMF）与作物（如番茄）互作的相关信息，显著提升回答的相关性和证据支持度。

Conclusion: 所提出的RAG框架展示了人工智能在推动农业生态创新和可持续 farming 决策方面的潜力，为领域特定的知识发现提供了可扩展的技术路径。

Abstract: Retrieval-Augmented Generation (RAG) represents a transformative approach within natural language processing (NLP), combining neural information retrieval with generative language modeling to enhance both contextual accuracy and factual reliability of responses. Unlike conventional Large Language Models (LLMs), which are constrained by static training corpora, RAG-powered systems dynamically integrate domain-specific external knowledge sources, thereby overcoming temporal and disciplinary limitations. In this study, we present the design and evaluation of a RAG-enabled system tailored for Mycophyto, with a focus on advancing agricultural applications related to arbuscular mycorrhizal fungi (AMF). These fungi play a critical role in sustainable agriculture by enhancing nutrient acquisition, improving plant resilience under abiotic and biotic stresses, and contributing to soil health. Our system operationalizes a dual-layered strategy: (i) semantic retrieval and augmentation of domain-specific content from agronomy and biotechnology corpora using vector embeddings, and (ii) structured data extraction to capture predefined experimental metadata such as inoculation methods, spore densities, soil parameters, and yield outcomes. This hybrid approach ensures that generated responses are not only semantically aligned but also supported by structured experimental evidence. To support scalability, embeddings are stored in a high-performance vector database, allowing near real-time retrieval from an evolving literature base. Empirical evaluation demonstrates that the proposed pipeline retrieves and synthesizes highly relevant information regarding AMF interactions with crop systems, such as tomato (Solanum lycopersicum). The framework underscores the potential of AI-driven knowledge discovery to accelerate agroecological innovation and enhance decision-making in sustainable farming systems.

</details>


### [193] [OTCR: Optimal Transmission, Compression and Representation for Multimodal Information Extraction](https://arxiv.org/abs/2511.14766)
*Yang Li,Yajiao Wang,Wenhao Hu,Zhixiong Zhang,Mengting Zhang*

Main category: cs.IR

TL;DR: 本文提出OTCR框架，采用任务中心视角进行多模态信息抽取，通过跨模态最优传输和变分信息瓶颈实现文本主导、视觉选择性支持的可控融合，在文档AI中实现了更优的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设文本与视觉模态等效或采用统一融合方式，导致任务无关冗余信息过多，限制了模型泛化能力。因此需要一种能控制模态融合、减少冗余的新方法。

Method: 提出两阶段框架OTCR：第一阶段使用跨模态最优传输（OT）建立文本token与视觉patch之间的稀疏对齐，并通过上下文感知门控控制视觉信息注入；第二阶段采用变分信息瓶颈（VIB）压缩融合特征，过滤任务无关噪声，生成紧凑且任务自适应的表示。

Result: 在FUNSD数据集上达到91.95%的SER和91.13%的RE，在XFUND（ZH）上达到91.09%的SER和94.20%的RE，表现具有竞争力。特征分析表明模态冗余降低，任务信号增强。

Conclusion: 本文提出的OTCR框架提供了一种可解释、基于信息论的可控多模态融合范式，强调文本主导、视觉选择性支持，有效提升文档AI中的多模态信息抽取性能。

Abstract: Multimodal Information Extraction (MIE) requires fusing text and visual cues from visually rich documents. While recent methods have advanced multimodal representation learning, most implicitly assume modality equivalence or treat modalities in a largely uniform manner, still relying on generic fusion paradigms. This often results in indiscriminate incorporation of multimodal signals and insufficient control over task-irrelevant redundancy, which may in turn limit generalization. We revisit MIE from a task-centric view: text should dominate, vision should selectively support. We present OTCR, a two-stage framework. First, Cross-modal Optimal Transport (OT) yields sparse, probabilistic alignments between text tokens and visual patches, with a context-aware gate controlling visual injection. Second, a Variational Information Bottleneck (VIB) compresses fused features, filtering task-irrelevant noise to produce compact, task-adaptive representations. On FUNSD, OTCR achieves 91.95% SER and 91.13% RE, while on XFUND (ZH), it reaches 91.09% SER and 94.20% RE, demonstrating competitive performance across datasets. Feature-level analyses further confirm reduced modality redundancy and strengthened task signals. This work offers an interpretable, information-theoretic paradigm for controllable multimodal fusion in document AI.

</details>


### [194] [An LLM-Powered Agent for Real-Time Analysis of the Vietnamese IT Job Market](https://arxiv.org/abs/2511.14767)
*Minh-Thuan Nguyen,Thien Vo-Thanh,Thai-Duy Dinh,Xuan-Quang Phan,Tan-Ha Mai,Lam-Son Lê*

Main category: cs.IR

TL;DR: 本文提出了一种基于AI的招聘市场顾问系统，通过自动化爬取和大语言模型处理非结构化招聘信息，结合ReAct框架实现自主推理与多工具协同，为越南IT求职者提供实时、数据驱动的职业建议。


<details>
  <summary>Details</summary>
Motivation: 针对越南IT就业市场中职业指导信息不足、现有报告过时且手动分析岗位数据不现实的问题，亟需一种能够实时获取并分析大量招聘信息的解决方案。

Method: 构建了一个自动化的数据采集管道，使用Playwright爬取招聘网站，并利用大语言模型（LLM）结构化处理非结构化数据；系统核心采用ReAct框架的工具增强型AI代理，集成SQL查询、语义搜索和数据可视化工具，实现自主推理与任务执行。

Result: 原型系统成功收集并分析了3,745条职位信息，能够回答复杂的多步骤查询，按需生成可视化图表，并提供基于真实数据的个性化职业建议。

Conclusion: 该研究展示了一种新型劳动力市场分析范式，表明专用的代理式AI系统可显著提升职业信息的可及性与时效性，为新一代专业人士提供公平、可靠的职业决策支持。

Abstract: Individuals entering Vietnam's dynamic Information Technology (IT) job market face a critical gap in reliable career guidance. Existing market reports are often outdated, while the manual analysis of thousands of job postings is impractical for most. To address this challenge, we present the AI Job Market Consultant, a novel conversational agent that delivers deep, data-driven insights directly from the labor market in real-time. The foundation of our system is a custom-built dataset created via an automated pipeline that crawls job portals using Playwright and leverages the Large Language Model (LLM) to intelligently structure unstructured posting data. The core of our system is a tool-augmented AI agent, based on the ReAct agentic framework, which enables the ability of autonomously reasoning, planning, and executing actions through a specialized toolbox for SQL queries, semantic search, and data visualization. Our prototype successfully collected and analyzed 3,745 job postings, demonstrating its ability to answer complex, multi-step queries, generate on-demand visualizations, and provide personalized career advice grounded in real-world data. This work introduces a new paradigm for labor market analysis, showcasing how specialized agentic AI systems can democratize access to timely, trustworthy career intelligence for the next generation of professionals.

</details>


### [195] [Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation](https://arxiv.org/abs/2511.14768)
*Bhavika Jain,Robert Pitsko,Ananya Drishti,Mahfuza Farooque*

Main category: cs.IR

TL;DR: 提出了一种情感感知的社交媒体推荐框架（ESMR），通过结合Transformer情绪预测与混合推荐策略，在保持用户参与度的同时改善情绪恢复并减少情绪波动。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统仅优化参与度指标，忽视用户情绪状态，长期暴露于情绪化内容可能损害用户心理健康。

Method: ESMR框架结合Transformer情绪预测器，使用LightGBM在情绪稳定期优化参与度，当检测到持续负面情绪时切换至基于因果奖励的强化学习策略。

Result: 在30天行为轨迹评估中，ESMR显著提升情绪恢复能力，降低情绪波动，并保持高水平参与度。

Conclusion: ESMR为构建兼顾情感健康与参与度的推荐系统提供了可行路径。

Abstract: Social media recommendation systems play a central role in shaping users' emotional experiences. However, most systems are optimized solely for engagement metrics, such as click rate, viewing time, or scrolling, without accounting for users' emotional states. Repeated exposure to emotionally charged content has been shown to negatively affect users' emotional well-being over time. We propose an Emotion-aware Social Media Recommendation (ESMR) framework that personalizes content based on users' evolving emotional trajectories. ESMR integrates a Transformer-based emotion predictor with a hybrid recommendation policy: a LightGBM model for engagement during stable periods and a reinforcement learning agent with causally informed rewards when negative emotional states persist. Through behaviorally grounded evaluation over 30-day interaction traces, ESMR demonstrates improved emotional recovery, reduced volatility, and strong engagement retention. ESMR offers a path toward emotionally aware recommendations without compromising engagement performance.

</details>


### [196] [Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications](https://arxiv.org/abs/2511.14769)
*Yifan Xu,Vipul Gupta,Rohit Aggarwal,Varsha Mahadevan,Bhaskar Krishnamachari*

Main category: cs.IR

TL;DR: 本文提出了一种基于聚类的自适应检索方法CAR，用于动态确定检索文档的最优数量，从而提升RAG系统的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 静态的top-k检索方法无法根据查询特性灵活调整检索数量，导致信息不足或冗余，影响RAG性能。

Method: CAR通过分析查询-文档相似性距离的聚类模式，检测相关文档簇到非相关文档的过渡点，动态设定检索截断点。

Result: 在Coinbase CDP数据集和MultiHop-RAG基准上，CAR实现了最高的TES得分，减少了60%的LLM token使用、22%的端到端延迟和10%的幻觉，同时保持答案相关性，并使用户参与度提升200%。

Conclusion: CAR能有效适应不同复杂度的查询，显著提升RAG系统的效率、准确性和用户体验。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.

</details>


### [197] [ExplainRec: Towards Explainable Multi-Modal Zero-Shot Recommendation with Preference Attribution and Large Language Models](https://arxiv.org/abs/2511.14770)
*Bo Ma,LuYao Liu,ZeHua Hu,Simon Lau*

Main category: cs.IR

TL;DR: 本文提出ExplainRec框架，通过偏好归因、多模态融合和零样本迁移学习提升大语言模型在推荐系统中的可解释性和冷启动性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统在可解释性和冷启动场景中存在不足，需要更有效的解决方案。

Method: 引入偏好归因调优、零样本偏好迁移、多模态增强和多任务协同优化四项技术。

Result: 在MovieLens-25M和Amazon数据集上，ExplainRec在电影推荐和跨域任务中AUC分别提升0.7%和0.9%，并能生成可解释推荐结果。

Conclusion: ExplainRec有效提升了推荐系统的可解释性、冷启动性能和跨域适应能力，具有实际应用价值。

Abstract: Recent advances in Large Language Models (LLMs) have opened new possibilities for recommendation systems, though current approaches such as TALLRec face challenges in explainability and cold-start scenarios. We present ExplainRec, a framework that extends LLM-based recommendation capabilities through preference attribution, multi-modal fusion, and zero-shot transfer learning. The framework incorporates four technical contributions: preference attribution tuning for explainable recommendations, zero-shot preference transfer for cold-start users and items, multi-modal enhancement leveraging visual and textual content, and multi-task collaborative optimization. Experimental evaluation on MovieLens-25M and Amazon datasets shows that ExplainRec outperforms existing methods, achieving AUC improvements of 0.7\% on movie recommendation and 0.9\% on cross-domain tasks, while generating interpretable explanations and handling cold-start scenarios effectively.

</details>


### [198] [SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs](https://arxiv.org/abs/2511.14881)
*Bi Xue,Hong Wu,Lei Chen,Chao Yang,Yiming Ma,Fei Ding,Zhen Wang,Liang Wang,Xiaoheng Mao,Ke Huang,Xialu Li,Peng Xia,Rui Jian,Yanli Zhao,Yanzun Huang,Yijie Deng,Harry Tran,Ryan Chang,Min Yu,Eric Dong,Jiazhou Wang,Qianqian Zhang,Keke Zhai,Hongzhang Yin,Pawel Garbacki,Zheng Fang,Yiyi Pan,Min Ni,Yang Liu*

Main category: cs.IR

TL;DR: 本文提出了SilverTorch，一个基于GPU的推荐模型服务系统，通过将索引和过滤服务统一为模型层，提升了大规模深度学习推荐模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的CPU-based推荐系统在成本和优化机会上存在不足，难以支持更复杂的模型架构。

Method: 提出SilverTorch系统，使用GPU上的Bloom索引算法进行特征过滤，设计张量原生融合Int8 ANN内核进行最近邻搜索，并协同设计索引以减少内存使用和计算开销；引入OverArch评分层和Value Model实现多任务结果聚合，通过缓存预计算嵌入加速排序。

Result: 在工业级数据集上的评估显示，与现有最先进方法相比，SilverTorch延迟降低5.6倍，吞吐量提高23.7倍，成本效益提升13.35倍，同时提高了检索精度。

Conclusion: SilverTorch通过GPU加速和模型统一服务范式，显著提升了推荐系统的性能、效率和准确性，支持更复杂模型的在线服务，已实际应用于多个主要产品中。

Abstract: Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.
  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.
  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.

</details>


### [199] [Multi-Aspect Cross-modal Quantization for Generative Recommendation](https://arxiv.org/abs/2511.15122)
*Fuwei Zhang,Xiaoyu Liu,Dongbo Xi,Jishen Yin,Huan Chen,Peng Yan,Fuzhen Zhuang,Zhao Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种新的生成式推荐方法MACRec，通过跨模态量化和多方面对齐机制，利用多模态信息提升语义ID质量和推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法在利用多模态信息和捕捉模态间复杂交互方面存在不足，难以学习高质量的语义ID并有效训练模型。

Method: 提出MACRec模型，引入跨模态量化以减少冲突并提升码本质量，并结合隐式与显式的多方面跨模态对齐来增强生成能力。

Result: 在三个常用推荐数据集上的实验表明，所提方法在推荐效果上优于现有方法。

Conclusion: MACRec通过融合多模态信息并在多个层面进行对齐，有效提升了生成式推荐中语义ID的质量和模型的生成能力。

Abstract: Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.

</details>


### [200] [ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation](https://arxiv.org/abs/2511.15141)
*Sunwoo Kim,Geon Lee,Kyungho Kim,Jaemin Yoo,Kijung Shin*

Main category: cs.IR

TL;DR: 本文提出了一种基于项目的检索增强生成方法ItemRAG，用于大型语言模型推荐系统，通过挖掘项目间的共购历史提升推荐性能，尤其在冷启动场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于用户相似性的RAG方法在处理冷启动项目时存在局限，且未能充分利用项目间的共购模式，因此需要一种更有效的检索策略来增强LLM在推荐任务中的表现。

Method: 提出ItemRAG，一种基于项目的RAG方法，从项目-项目共购历史中检索相关项目，并结合语义相似性和共购频率优化检索结果，以增强LLM的推荐能力。

Result: 实验表明，ItemRAG在Hit-Ratio-1指标上最高可将零样本LLM推荐器性能提升43%，并在标准和冷启动推荐设置下均优于基于用户的RAG基线方法。

Conclusion: ItemRAG通过捕捉项目间的共购模式并融合语义与行为信息，有效提升了LLM在推荐任务中的表现，尤其适用于冷启动场景。

Abstract: Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.

</details>


### [201] [Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing](https://arxiv.org/abs/2511.15241)
*Mi Tian,Kun Zhang,Fei Liu,Jinglong Li,Yuxin Liao,Chenxi Bai,Zhengtao Tan,Le Wu,Richang Hong*

Main category: cs.IR

TL;DR: 提出了一种去偏框架，通过交叉属性检索和选择性mixup正则化来缓解计算机化自适应测试中的选择偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有计算机化自适应测试方法主要关注诊断准确性，忽视了因能力估计引导题目选择而产生的选择偏差，导致模型预测偏差和不公平性。

Method: 设计了两个模块：交叉属性考生检索以获取具有均衡回答分布的参考考生，并在标签一致性的约束下对偏见考生与其匹配的平衡考生进行mixup增强。

Result: 在两个基准数据集上结合多种先进诊断模型的实验表明，该方法显著提升了CAT中题目选择的泛化能力和公平性。

Conclusion: 所提出的去偏框架有效缓解了自适应测试中的选择偏差，改善了模型的预测准确性和公平性。

Abstract: Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.

</details>


### [202] [Unveiling Inference Scaling for Difference-Aware User Modeling in LLM Personalization](https://arxiv.org/abs/2511.15389)
*Suyu Chen,Yimeng Bai,Yulong Huang,Xiaoyan Zhao,Yang Zhang*

Main category: cs.IR

TL;DR: 提出了一种名为Difference-aware Reasoning Personalization (DRP) 的框架，通过推理扩展来增强大语言模型的个性化输出，能够在生成个性化内容时更全面、细致地捕捉用户间的差异。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法多依赖用户自身历史数据，忽视了用户间差异的重要性，且特征提取局限于固定维度和直觉式快速推理（System-1），限制了个性化效果。

Method: 提出DRP框架，利用推理扩展机制自主识别用户差异的相关特征维度，并生成结构化的定义与描述，引入慢速、深度推理（System-2）来建模用户差异，提升个性化能力。

Result: 在个性化评论生成任务上的实验表明，DRP在多个指标上均优于基线方法，展现出更强的个性化生成性能。

Conclusion: DRP通过系统性建模用户间差异并引入深度推理机制，有效提升了大语言模型在个性化任务中的表现，为个性化生成提供了新思路。

Abstract: Large Language Models (LLMs) are increasingly integrated into users' daily lives, driving a growing demand for personalized outputs. Prior work has primarily leveraged a user's own history, often overlooking inter-user differences that are critical for effective personalization. While recent methods have attempted to model such differences, their feature extraction processes typically rely on fixed dimensions and quick, intuitive inference (System-1 thinking), limiting both the coverage and granularity of captured user differences. To address these limitations, we propose Difference-aware Reasoning Personalization (DRP), a framework that reconstructs the difference extraction mechanism by leveraging inference scaling to enhance LLM personalization. DRP autonomously identifies relevant difference feature dimensions and generates structured definitions and descriptions, enabling slow, deliberate reasoning (System-2 thinking) over user differences. Experiments on personalized review generation demonstrate that DRP consistently outperforms baseline methods across multiple metrics.

</details>


### [203] [CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search](https://arxiv.org/abs/2511.15443)
*Ao Xie,Jiahui Chen,Quanzhi Zhu,Xiaoze Jiang,Zhiheng Qin,Enyun Yu,Han Li*

Main category: cs.IR

TL;DR: 本文提出了CroPS，一种用于缓解短视频搜索中过滤气泡效应的新型检索数据引擎，通过多视角引入多样化的正样本，并结合层次化标签分配策略和H-InfoNCE损失函数提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于历史用户交互的自强化训练范式导致模型陷入过滤气泡，忽略潜在相关但未曝光的内容，限制了检索多样性与准确性。

Method: 提出CroPS框架，从查询重构行为（查询层面）、推荐流中的互动数据（系统层面）和大语言模型生成的世界知识（知识层面）引入多视角正样本，并设计层次化标签分配（HLA）策略与H-InfoNCE损失进行细粒度优化。

Result: 在快手搜索平台上的实验表明，CroPS在离线和在线A/B测试中均显著优于强基线模型，提升了检索性能并降低了用户查询重构率。

Conclusion: CroPS有效缓解了密集检索中的过滤气泡问题，已在快手搜索大规模部署，服务于数亿用户。

Abstract: Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [204] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文在实解析假设下研究了解码器-only Transformer中从离散提示到末尾标记隐藏状态映射的可逆性，提出逐层碰撞判据与可注入层，并通过几何诊断在预训练模型中验证了表示的普遍可注入性。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型中提示与隐藏状态之间的映射是否可逆，对于模型解释性和安全性具有重要意义。现有工作表明在某些条件下该映射是可注入的，但缺乏对训练动态和量化影响的深入分析。

Method: 引入逐层碰撞判据Δ^ℓ和可注入层U^ℓ，证明其在参数空间中的开稠性；结合非奇异优化器假设和连续初始化，分析训练路径上的可注入性保持；考虑对称群作用下的商空间性质；定义分离边距和下Lipschitz常数作为几何诊断工具，基于最近邻统计进行估计。

Result: 理论上证明了在非退化情况下，Transformer表示映射在大多数参数下是可注入的，且该性质沿训练轨迹保持；实验显示LLaMA-3和Qwen模型在全精度和8位激活量化下无碰撞，4位量化引发少量碰撞并显著降低下Lipschitz常数；小规模GPT-2训练过程中归一化指标稳定。

Conclusion: Transformer模型在连续参数理想化下具有普遍且持久的可注入性，而实际中的可逆性可通过简单几何诊断进行探测，量化尤其是低位宽会削弱表示的可分性。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [205] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的推理能力评估框架DEVAL，用于衡量大语言模型在输入变化时对输出进行相应调整的衍生能力（DC），并提出了衍生提示（DP）方法以提升该能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型在数据变化下推理能力的系统描述与评估，而人类能够基于抽象规则在输入变化时调整输出，因此需要定义并评估模型的这种衍生推理能力。

Method: 定义了衍生关系（DR）和衍生能力（DC），构建了系统化的评估框架DEVAL，并在七个主流任务中评估六种大型模型；提出衍生提示（DP）方法以增强DC。

Result: 实验表明主流大模型如GPT-4o和Claude3.5具备一定DR识别能力，但在应用DR解决问题时表现显著下降；所提DP方法平均提升15.2%的DC，优于常见提示技术。

Conclusion: 衍生能力是衡量模型推理的重要维度，当前主流LLM在此方面仍有不足，衍生提示是一种有效增强该能力的新颖提示工程方法。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [206] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出动态嵌套层次结构，使模型能够自主调整优化层级和更新频率，实现持续学习和适应非平稳环境。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在非平稳环境中表现不佳，因架构固定难以持续适应和终身学习。

Method: 基于嵌套学习范式，提出动态嵌套层次结构，允许模型在训练或推理过程中自主调整优化层级数、结构和更新频率。

Result: 理论证明了收敛性、表达能力边界和亚线性遗憾，并在语言建模、持续学习和长上下文推理中展示了优越性能。

Conclusion: 动态嵌套层次结构为实现自适应的通用人工智能奠定了基础。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [207] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习算法GTPO，用于训练大语言模型进行多轮工具集成推理，相较于现有的GRPO方法在多个推理基准上平均提升了3.0%。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）使用粗粒度的轨迹级奖励，难以提供足够的学习信号，导致多轮交互任务中训练停滞。

Method: 提出Group Turn Policy Optimization (GTPO)，引入三个关键创新：按轮次分配奖励、基于回报的优势估计和自监督奖励塑形。

Result: GTPO在多个推理基准上平均优于GRPO 3.0%，验证了其在复杂数学推理任务中的有效性。

Conclusion: GTPO通过细粒度奖励和自监督信号显著提升多轮工具集成推理的训练效果，为实际应用中的复杂推理任务提供了更优解决方案。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [208] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: 本文提出FinTRec，一种基于Transformer的金融服务业序列推荐框架，解决长时多渠道用户交互与多产品协同推荐的挑战，相较传统树模型在效果和成本上均有提升。


<details>
  <summary>Details</summary>
Motivation: 金融服务业中推荐系统面临用户跨渠道长期交互、多产品关联及业务目标冲突等独特挑战，传统树模型虽具可解释性但难以捕捉复杂序列模式，亟需有效的Transformer解决方案。

Method: 提出FinTRec框架，采用统一的Transformer架构建模用户异构时序行为，支持跨产品信号共享，并通过微调适配不同产品场景，兼顾实时推荐与业务目标协调。

Result: 在历史模拟和线上A/B测试中，FinTRec持续优于生产级树模型基线，实现离线性能全面提升，同时降低训练成本与技术债务。

Conclusion: FinTRec为金融服务业提供了首个兼顾技术与业务需求的统一序列推荐方案，验证了Transformer架构在该领域的可行性与优越性。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [209] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer引导的深度强化学习（DRL）方法，用于eVTOL飞行器最优起飞轨迹设计，显著提升了训练效率和能量消耗优化精度。


<details>
  <summary>Details</summary>
Motivation: 传统最优控制方法受限于问题复杂度，而标准DRL存在训练困难的问题，因此需要一种能有效降低训练难度并提升性能的方法。

Method: 采用Transformer引导的DRL方法，在每一步时间步中利用Transformer探索更真实的状态空间，通过调整功率和机翼角度来优化eVTOL无人机的起飞轨迹。

Result: 所提方法仅用4.57×10⁶时间步完成训练，仅为标准DRL的25%；在最优能耗上达到97.2%的准确率，优于标准DRL的96.3%。

Conclusion: Transformer引导的DRL在训练效率和最优设计验证方面均优于标准DRL，为eVTOL轨迹优化提供了一种高效可行的解决方案。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [210] [Bringing Federated Learning to Space](https://arxiv.org/abs/2511.14889)
*Grace Kim,Filip Svoboda,Nicholas Lane*

Main category: cs.LG

TL;DR: 本文首次系统分析了将现成的联邦学习算法应用于低地球轨道卫星星座的可行性，提出了一种“空间化”框架以适应轨道约束，并通过大量仿真验证了其在不同星座配置下的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着低轨卫星星座规模迅速扩大，下行链路带宽受限问题日益突出，亟需发展星上分布式机器学习技术；联邦学习虽具潜力，但需克服空间环境中的间歇连接和轨道动态等特有挑战。

Method: 提出一个综合的‘空间化’框架，将FedAvg、FedProx、FedBuff等地面联邦学习算法适配至轨道环境，考虑轨道运动与通信约束，并在768种不同星座配置下进行大规模参数扫描评估。

Result: 空间适配后的联邦学习算法可有效扩展至百颗卫星规模，性能接近集中式训练理想情况；通过轨道调度和星群内本地协同，可将数月的训练周期缩短至数天，提速约9倍。

Conclusion: 该工作为未来任务设计提供了可行路径，推动实现更自主、更鲁棒、数据驱动的卫星运行模式。

Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.

</details>


### [211] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 本文提出了一种名为LIT（可检查工具的大型语言模型）的框架，通过强制LLM优先使用更可靠的外部工具来解决任务，从而提升其在高风险场景下的可信度和可调试性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM推理过程不透明，可能导致不可靠且难以排查的解决方案，限制了其在高风险领域的应用。因此需要一种机制使其选择更可靠、易排查的解决路径。

Method: 构建基于现有LLM工具调用能力的框架LIT，引入包含1300个问题的基准数据集和可定制的可靠性成本函数，用于评估不同工具的可靠性和可调试性，并引导LLM选择最优工具调用序列。

Result: 实验表明，使用LIT框架的LLM能够在保持任务性能的同时，实现更可靠和可解释的问题解决过程。

Conclusion: LIT框架有效提升了LLM在复杂任务中的可靠性与可追溯性，为高风险领域中LLM的可信应用提供了可行路径。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [212] [Structured Contrastive Learning for Interpretable Latent Representations](https://arxiv.org/abs/2511.14920)
*Zhengyang Shen,Hua Tu,Mayue Shi*

Main category: cs.LG

TL;DR: 提出结构化对比学习（SCL），将潜在空间划分为不变、可变和自由特征，实现对语义无关变换的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络对语义无关变换（如ECG相位偏移、传感器旋转）过于敏感，现有方法缺乏对潜在空间结构的主动控制。

Method: 设计Structured Contrastive Learning (SCL)，通过划分潜在空间为不变、可变和自由三类特征，并引入新的可变机制，在对比学习中增强正样本对内的区分能力。

Result: 在ECG相位不变性和IMU旋转鲁棒性任务中显著提升性能：ECG相似度从0.25提升至0.91，WISDM活动识别准确率达86.65%，旋转一致性达95.38%。

Conclusion: SCL实现了从被动数据增强到主动结构学习的范式转变，无需修改模型结构即可提升鲁棒性和可解释性。

Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.

</details>


### [213] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: 提出Causal-GCN，一种基于do演算的因果图卷积网络，用于识别对阿尔茨海默病进展具有稳定因果影响的脑区。


<details>
  <summary>Details</summary>
Motivation: 现有深度图学习模型多为相关性建模，易将人口统计学和遗传因素与疾病特异性特征混淆，缺乏因果解释性。

Method: 构建结构连接组作为图结构，节点表示脑区，边表示解剖连接；通过主成分汇总年龄、性别、APOE4等混杂因素，并利用后门调整进行因果干预；在训练后通过阻断入边并修改节点特征来估计各脑区的平均因果效应。

Result: 在ADNI队列484名受试者上验证，性能与基线GNN相当，同时给出可解释的因果效应排序，突出后部、扣带和岛叶枢纽区域，符合已知AD神经病理特征。

Conclusion: Causal-GCN能有效分离混杂因素，识别具有稳定因果作用的脑区，提升了AD分类模型的可解释性和生物学合理性。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [214] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 本文首次系统比较了四种在相同隐私预算和模型规模下的临床文本诊断编码方法，发现知识蒸馏在中等隐私预算下表现最佳，能恢复最多非私有性能的同时保持强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本上训练时可能泄露患者敏感信息，而现有的差分隐私方法往往严重降低诊断准确性，因此需要探索更有效的隐私保护策略。

Method: 在相同的10亿参数模型和隐私预算下，对四种自动化诊断编码训练流程进行头对头比较，包括直接DP-SGD、DP合成数据训练以及基于DP教师模型的知识蒸馏等方法。

Result: 在中等和宽松隐私预算（ε = 4, 6）下，基于DP教师模型的知识蒸馏优于直接DP-SGD和DP合成数据方法，最多可恢复63%的非私有性能，并在成员推断攻击下保持接近随机猜测的隐私防护水平（AUC ≈ 0.5）。

Conclusion: 不同架构在隐私-效用权衡上差异显著，知识蒸馏是实现隐私保护临床NLP最实用的路径。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [215] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: 提出了一种名为Graph Memory（GM）的结构化非参数框架，通过在区域级原型上构建紧凑的关系记忆来增强基于嵌入的推理。


<details>
  <summary>Details</summary>
Motivation: 传统方法孤立处理每个训练样本，缺乏对嵌入空间整体结构的建模，导致推理效率低、解释性差和决策边界不平滑。

Method: 将嵌入空间总结为带有可靠性指标的原型节点，并通过边连接这些节点以编码几何和上下文关系，统一了实例检索、原型推理和图标签传播。

Result: 在合成数据和乳腺组织病理学（IDC）等真实数据集上的实验表明，GM在精度上与kNN和Label Spreading相当，但校准性能更好、决策边界更平滑，且仅需十分之一的样本量。

Conclusion: GM通过显式建模可靠性和关系结构，在非参数学习中建立了局部证据与全局一致性之间的原则性桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [216] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast 是一套基于深度学习的模型，用于预测电离层总电子含量（TEC），结合多种物理驱动因素和观测数据，提升空间天气预报能力。


<details>
  <summary>Details</summary>
Motivation: 电离层对GNSS精度、高频通信和航空运行至关重要，准确建模其变化具有重要意义，但现有方法存在不足。

Method: 采用受GraphCast启发的图结构深度学习模型，利用时空学习融合多源异构数据进行全球TEC预测。

Result: 在平静期和风暴期条件下验证显示，IonCast 预测技能优于持续性模型。

Conclusion: IonCast 展示了机器学习在增强电离层变化物理理解及提升业务化空间天气韧性方面的潜力。

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [217] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的智能辅导系统（ITS），通过模拟课堂环境中的时间序列动态，结合个体学生状态学习与群体信息，利用探测性干预平衡信息获取与学生干扰之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 每个学生都是独特的，且学习过程只能部分观测，传统ITS难以准确个性化教学，因此需要一种能够动态适应并有效估计学生状态的方法。

Method: 设计了一个包含辅导课、讲座和考试的动态时间序列模拟课堂环境，引入不同强度的探测性干预；采用强化学习方法结合群体信息来估计学生个体状态，并与基于规则的启发式方法进行比较。

Result: RL方法与启发式方法效果相近，但面临更多隐藏信息时挑战更大；允许探测干预能显著提升性能；RL策略在学生分布变化时表现出灵活性，但在困难班级中表现受限；在有测验和期中考试的课程结构中提升更明显。

Conclusion: 探测性干预有助于缓解学生状态估计难题，多种课程结构下策略均有效，尤其在提供阶段性评估的结构中表现更优，但强化学习在高难度情境下的适应性仍需改进。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [218] [Oversampling techniques for predicting COVID-19 patient length of stay](https://arxiv.org/abs/2511.15048)
*Zachariah Farahany,Jiawei Wu,K M Sajjadul Islam,Praveen Madiraju*

Main category: cs.LG

TL;DR: 本研究利用电子健康记录（EHR）数据，通过人工神经网络结合贝叶斯优化和过采样技术，预测COVID-19患者住院时长（LOS）以评估病情严重程度。


<details>
  <summary>Details</summary>
Motivation: 由于COVID-19病情严重程度差异大且长住院病例较少，导致分类不平衡，难以准确预测病情严重性。因此需要有效方法来提升对少数类（长住院）的预测能力。

Method: 采用合成过采样技术（如SMOTE）处理类别不平衡问题，并使用人工神经网络（ANN）进行建模，通过贝叶斯优化调参，选择F1分数最优的模型进行评估。

Result: 该方法在不平衡数据上提升了对长住院患者的预测性能，获得了较高的F1分数，表明模型在识别严重病例方面表现良好。

Conclusion: 结合过采样与贝叶斯优化的ANN模型能有效预测COVID-19患者病情严重程度，有助于临床早期干预和资源分配。

Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.

</details>


### [219] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出一种结合局部与全局信息提取及注意力机制融合的框架，用于在有限输入长度下实现心律失常的检测与分类，实验表明该方法在多个数据库上性能优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 由于心律失常持续时间长短不一且起始时间变化大，现有方法未充分考虑这一问题，导致检测和分类困难，因此需要一种能有效捕捉局部与全局动态特征的方法。

Method: 提出包含局部与全局特征提取以及基于注意力机制的局部-全局信息融合的框架，使用MITDB和AFDB数据库进行10类和4类心律失常检测，评估其对发作时间、结束时间和持续时间的识别能力。

Result: 在MITDB和AFDB上的持续时间、事件和Dice分数F1得分分别达到96.45%/82.05%/96.31%和97.57%/98.31%/97.45%，显著优于基准模型；跨数据库验证也显示良好泛化能力。

Conclusion: 所提方法能有效捕获心律失常的局部与全局信息，在准确检测的同时精确定位发生时间，有助于临床制定更精准的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [220] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 本研究开发并验证了一种基于病理全切片图像的多实例学习模型TDAM-CRC，用于结直肠癌的精准预后预测，并结合多组学数据揭示其分子机制。


<details>
  <summary>Details</summary>
Motivation: 传统的TNM分期系统在高度异质性的结直肠癌中预后分层能力有限，难以满足个体化医疗需求。

Method: 采用多实例学习框架构建TDAM-CRC模型，在TCGA队列（n=581）训练并在独立外部队列（n=1031）验证；整合基因组、转录组等多组学数据提升可解释性，并通过网络分析识别关键基因。

Result: TDAM-CRC在两个队列中均实现稳健的风险分层，预测性能显著优于传统临床分期及现有模型；多组学分析发现高风险亚型与代谢重编程和免疫抑制微环境相关；鉴定出MRPL37为关键枢纽基因，其高表达由启动子低甲基化驱动，是预后良好的独立生物标志物；构建了融合TDAM-CRC风险评分的列线图。

Conclusion: TDAM-CRC是一种可靠的AI驱动病理模型，可提升结直肠癌风险分层精度，揭示新的分子靶点，助力个体化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [221] [Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection](https://arxiv.org/abs/2511.15083)
*Xiancheng Wang,Lin Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 提出了一种结合傅里叶层、KAN和Mamba的新型混合架构Fourier-KAN-Mamba，用于时间序列异常检测，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Mamba在长序列建模中表现出色，但直接用于异常检测任务时难以捕捉复杂的时序模式和非线性动态，因此需要增强其特征提取和非线性建模能力。

Method: 提出Fourier-KAN-Mamba，融合傅里叶层提取多尺度频域特征，KAN提升非线性表示能力，并引入时序门控机制以更好区分正常与异常模式。

Result: 在MSL、SMAP和SWaT数据集上实验表明，该方法显著优于现有的最先进异常检测方法。

Conclusion: Fourier-KAN-Mamba通过融合频域分析、非线性建模和选择性状态空间模型，有效提升了时间序列异常检测性能，具有较强的应用潜力。

Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network

</details>


### [222] [Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data](https://arxiv.org/abs/2511.15112)
*Wei-hsiang Yen,Lyn Chao-ling Chen*

Main category: cs.LG

TL;DR: 本研究结合深度学习与情感分析，用于预测台湾半导体产业趋势，以台积电为研究对象，通过融合文本数据的情感分析与时间序列数据，利用LSTM模型提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统数据分析方法在应对半导体产业高频变化和高维时序数据时表现不佳，需引入更有效的分析手段。

Method: 收集台积电季报中的文本与财务时间序列数据，结合公司内外部事件进行情感分析，并将情感增强的数据输入LSTM模型进行趋势预测。

Result: 模型成功预测出台积电在晶圆技术上的显著发展及全球市场的潜在威胁，结果与实际产品发布和国际新闻相符。

Conclusion: 该方法能有效整合多源数据与事件影响，提升产业趋势预测精度，对学术研究与商业决策均具参考价值。

Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.

</details>


### [223] [Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling](https://arxiv.org/abs/2511.15125)
*Huifan Zhang,Pingqiang Zhou*

Main category: cs.LG

TL;DR: 提出一种基于不确定性感知的贝叶斯在线学习框架，用于高效建模射频无源器件，显著减少电磁仿真时间。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的射频无源器件建模需要大量电磁仿真，计算成本高，难以覆盖完整的几何和频率设计空间。

Method: 采用具有可重构头结构的贝叶斯神经网络进行几何-频率联合建模，并引入基于不确定性的自适应采样策略，优化训练数据在几何参数和频率域的分布。

Result: 在三种射频无源器件上验证，仅使用传统方法2.86%的电磁仿真时间，实现35倍加速，同时保持高建模精度。

Conclusion: 该框架显著提升了射频无源器件参数化建模的效率，为低数据条件下的高效电磁仿真建模提供了有效解决方案。

Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.

</details>


### [224] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏矩阵乘法算法，用于对整个Medline数据集进行自组织映射，从而更全面地绘制现有医学知识图谱。


<details>
  <summary>Details</summary>
Motivation: 以往的方法由于现有算法的内存和计算需求呈指数增长，只能处理Medline数据库的小子集。

Method: 设计了一种用于稀疏矩阵乘法的新算法，并将其应用于整个Medline数据集的自组织映射。

Result: 该算法使得对整个Medline数据集进行自组织映射成为可能，提高了根据数据集变化随时间精炼映射的可行性。

Conclusion: 新算法有效解决了大规模数据处理的瓶颈，实现了对医学知识更完整、可更新的映射。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [225] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 提出GRPO-Verif算法，通过统一损失函数联合优化生成与自验证，提升大模型推理中的自验证能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理中难以稳定验证自身思维链，需增强其自验证能力以进一步提升推理性能。

Method: 提出GRPO-Verif算法，在强化学习框架下将解生成与自验证结合于统一损失函数，并引入可调超参数控制验证信号权重。

Result: 实验表明该方法有效增强了模型的自验证能力，同时保持了相当的推理性能。

Conclusion: GRPO-Verif能有效提升大模型的自验证能力，为改进推理性能提供了新路径。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [226] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出一种不确定性感知的主动学习框架，通过结合模型不确定性和跨模态一致性来提升脑电情感识别中对标签噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑电信号易受伪影和个体差异影响，情感标签常来自主观且不一致的报告，导致情感解码困难。

Method: 提出一个不确定性感知的主动学习框架，利用模型不确定性和跨模态（EEG与面部）一致性判断不确定性来源；通过表征对齐模块将EEG和面部特征映射到共享隐空间，并将残余差异视为噪声引发的不一致，主动查询这些样本以获取专家反馈。

Result: 在ASCERTAIN数据集上的实验表明，该方法在数据效率和抗噪能力方面表现优异，能有效提升情感识别性能。

Conclusion: 该框架通过融合跨模态一致性与主动学习，显著增强了对噪声标签的鲁棒性，为脑机接口中的情感识别提供了一种高效可靠的方法。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [227] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文研究了复变自编码器（complex VAEs）中的凯勒几何结构，提出了一种基于复高斯混合的凯勒势导数方法，有效计算费雪信息度量，并通过解码器几何正则化潜空间，提升了表示的平滑性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明潜欧几里得变分自编码器具有黎曼结构，但复变VAE的几何特性尚未充分探索，本文旨在揭示复变VAEs中的凯勒几何结构并加以利用。

Method: 针对具有复潜变量的VAE，推导复情形下基于复高斯正则化的费雪信息度量，利用KL散度的Hessian与费雪信息的一致性，构建符合凯勒几何的势函数，并提出一种高效的、基于PSH函数的计算方法。

Result: 提出了一个与费雪信息度量近似且保持凯勒几何特性的凯勒势导数方法，实现了高效计算；通过解码器几何正则化和加权复体积元采样，获得了更平滑的表示和更少的语义异常点。

Conclusion: 复变VAE展现出一定程度的凯勒几何结构，利用该结构可通过凯勒势有效逼近费雪信息度量，并在降低计算成本的同时提升生成表示的质量。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [228] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器和多样性损失来提升故障数据生成的真实性和多样性。


<details>
  <summary>Details</summary>
Motivation: 由于故障事件稀少且标注成本高，故障数据稀缺，现有时间序列生成模型在少样本场景下难以有效捕捉故障分布，导致生成样本缺乏真实性和多样性。

Method: 采用基于扩散模型的框架，引入正负差异适配器，利用预训练的正常数据分布建模正常与故障域之间的差异，并引入多样性损失以防止模式崩溃，通过样本间差异正则化增强生成多样性。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，在关键基准上达到最先进性能。

Conclusion: 所提出的方法有效解决了少样本条件下故障时间序列生成的挑战，为工业设备监测中的数据稀缺问题提供了可行方案。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [229] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种基于量子图注意力网络（Q-GAT）的深度强化学习方法，用于求解车辆路径问题（VRP），通过用参数化量子电路替代传统MLP，减少了50%以上的可训练参数，并在基准测试中比经典GAT模型降低约5%的路径成本。


<details>
  <summary>Details</summary>
Motivation: 经典基于图神经网络的VRP求解模型依赖大量多层感知机（MLP），导致参数量大、内存受限，限制了其在大规模物流优化中的效率与应用。

Method: 提出量子图注意力网络（Q-GAT），在深度强化学习框架中使用参数化量子电路（PQC）替代传统MLP进行关键读出操作，结合PPO算法与贪心/随机解码策略，在保持表达能力的同时显著压缩模型规模。

Result: 实验表明，Q-GAT在VRP基准任务上收敛更快，路由成本较经典GAT基线降低约5%，且可训练参数减少超过50%。

Conclusion: PQC增强的GNN可作为紧凑且高效的求解器，具有在大规模路径规划与物流优化中应用的潜力。

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [230] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一种基于蒸馏的框架，用于加速掩码自回归扩散模型（MAR）的推理，并使其适用于强化学习后训练。它通过将扩散链压缩为单步生成，实现超过30倍的速度提升，同时保持高质量生成和更好的偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 原始的MAR模型由于外层自回归解掩码循环和内层扩散去噪链的双重结构导致推理速度慢，限制了其在强化学习等需要高效生成场景中的应用。

Method: 提出MARVAL框架，采用基于分数的变分目标，将掩码自回归扩散模型蒸馏成单步生成模型，保留自回归解掩码顺序；并构建MARVAL-RL框架，支持高效的强化学习后训练。

Result: 在ImageNet 256×256上，MARVAL-Huge实现了2.00的FID分数，比MAR-diffusion快30多倍；MARVAL-RL在CLIP和图像奖励分数上均有提升。

Conclusion: MARVAL为掩码自回归扩散模型提供了首个可行的蒸馏与强化学习路径，实现了快速采样和更优的生成偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [231] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散大语言模型（dLLMs）的轻量级强化学习策略ATPO，通过动态选择高影响力的去噪步骤进行梯度更新，显著提升了推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法在所有去噪步骤中均匀分配策略梯度，忽略了不同步骤重要性的差异。本文旨在识别并利用对最终结果影响更大的关键步骤，以提高训练效率和性能。

Method: 引入三种逐歩指标（基于熵的不确定性、置信度裕度CM和熵变率RoEC）分析生成轨迹，发现存在‘困惑区域’；基于RoEC和CM的混合规则设计自适应梯度分配策略ATPO，在不改变RL目标或奖励的情况下动态调整更新重点。

Result: ATPO在多个基准上显著提升推理准确率和训练稳定性，验证了利用轨迹动态特性对dLLM强化学习的重要性。

Conclusion: 去噪扩散过程中存在可识别的关键步骤，针对性地分配梯度更新能有效提升dLLM的推理能力和训练效率，为后续研究提供了新的优化方向。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [232] [D2D Power Allocation via Quantum Graph Neural Network](https://arxiv.org/abs/2511.15246)
*Tung Giang Le,Xuan Tung Nguyen,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种全量子图神经网络（QGNN），利用参数化量子电路实现消息传递，在设备到设备（D2D）功率控制任务中达到了与经典方法相当的性能，但参数更少且具备固有并行性。


<details>
  <summary>Details</summary>
Motivation: 无线网络复杂性增加，传统GNN在大规模场景下计算成本高，需要更高效的资源管理方法。

Method: 设计了量子图卷积层（QGCL），将特征编码为量子态，使用NISQ兼容的酉操作处理图结构，并通过测量提取嵌入表示。整个模型基于端到端的参数化量子电路（PQC）。

Result: 在D2D功率控制以最大化SINR的任务中，QGNN表现与经典GNN相当，但使用更少参数并展现出内在并行性。

Conclusion: 该工作展示了量子图神经网络在无线网络优化中的潜力，是迈向量子加速无线优化的重要一步。

Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.

</details>


### [233] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出了一种名为EntroPIC的新方法，通过比例-积分控制动态调整正负样本的损失系数，以稳定大语言模型强化学习训练过程中的熵，从而保持有效的探索并防止陷入次优解。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的长期训练中，维持稳定的探索至关重要，而现有强化学习方法难以在包含正负样本的混合训练过程中有效控制熵，导致熵波动和次优收敛。

Method: 提出EntroPIC方法，采用比例-积分（Proportional-Integral）控制机制，动态调节正负样本的损失系数，以自适应地平衡其对熵的影响，从而实现对熵的稳定控制。该方法适用于on-policy和off-policy两种学习设置。

Result: 理论分析表明EntroPIC能有效控制系统熵；实验结果显示该方法能够成功维持目标熵水平，在大规模LLM训练中实现更稳定、更优的强化学习性能。

Conclusion: EntroPIC通过引入控制理论中的PI机制，解决了大模型强化学习中熵不稳定的问题，显著提升了训练的稳定性与效果，为长周期LLM训练提供了一种可靠的方法。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [234] [Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling](https://arxiv.org/abs/2511.15250)
*Jin Ye,Lingmei Wang,Shujian Zhang,Haihang WU*

Main category: cs.LG

TL;DR: 提出一种基于改进的双延迟深度确定性策略梯度（PVTD3）算法的智能调度方法，用于解决可再生能源集成和多重不确定性下的电热联合系统调度优化问题。


<details>
  <summary>Details</summary>
Motivation: 应对可再生能源整合和多重不确定性带来的电热联合系统调度优化挑战。

Method: 引入电网购电变化的惩罚项，采用改进的PVTD3算法进行系统优化。

Result: 在10%、20%和30%可再生能源渗透率下，综合成本分别降低6.93%、12.68%和13.59%，电网购电波动幅度平均减少12.8%；低温储热罐终态值减少7.67-17.67单位，高温储热罐保持在3.59-4.25的安全范围内。

Conclusion: PVTD3算法在经济性、电网稳定性及储能设备可持续调度方面均表现出优越性能。

Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.

</details>


### [235] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT 是一个基于柏拉图表示假设的统一网络断层扫描框架，通过共享潜在状态建模多种网络指标，利用多模态对齐和对比学习提升跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有网络断层扫描方法通常针对特定任务独立建模，依赖有限信号，导致泛化性和可解释性不足。

Method: 提出 PLATONT 框架，将不同网络指标视为共享潜在网络状态的投影，通过多任务学习、多模态对齐和对比学习在统一潜在空间中训练。

Result: 在合成和真实数据集上，PLATONT 在链路性能估计、拓扑推断和流量预测任务中均优于现有方法，表现出更高准确性和鲁棒性。

Conclusion: PLATONT 通过共享潜在表示实现了网络断层扫描任务的统一建模，显著提升了跨任务泛化能力和整体性能。

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [236] [PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles](https://arxiv.org/abs/2511.15522)
*Yinan Yu,Samuel Scheidegger*

Main category: cs.LG

TL;DR: 提出PCARNN-DCBF框架，结合物理编码的控制仿射神经网络与离散控制屏障函数，实现实时高保真且可验证的车辆地理围栏控制。


<details>
  <summary>Details</summary>
Motivation: 现有地理围栏技术难以兼顾高保真学习与可验证控制的结构要求，缺乏对车辆动力学结构的显式建模。

Method: 提出PCARNN-DCBF：PCARNN保留车辆动力学的控制仿射结构，DCBF通过预览机制和QP实时处理多边形约束与执行器饱和。

Result: 在CARLA中测试电动与燃油车辆平台，结构化方法显著优于分析模型与非结构化神经网络基线。

Conclusion: 保留物理结构的建模范式提升了学习模型的可靠性与可验证性，适用于安全关键型实时控制。

Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.

</details>


### [237] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了GRPO-RM，将强化学习方法GRPO应用于表示学习模型的后训练优化，通过预定义输出集和专用奖励函数实现概率驱动优化，并在多个真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索GRPO方法是否可推广到表示学习模型，以提升其在后训练阶段的性能。

Method: 提出GRPO-RM，使用预定义输出集生成输出组，并设计适配表示模型特性的专用奖励函数，实现类似GRPO的概率驱动优化。

Result: 在多个真实世界数据集上的实验表明，所提方法在表示模型后训练中有效提升了性能。

Conclusion: GRPO可以成功扩展到表示学习模型，GRPO-RM为表示模型的优化提供了一种新且有效的路径。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [238] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: 提出SNAP，一种稀疏测试时适应框架，通过减少适应频率和数据使用，在保持精度的同时显著降低边缘设备上的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法依赖频繁更新和高计算成本，难以应用于资源受限的边缘环境。

Method: 引入两类核心组件：类与域代表性记忆（CnDRM）选择具有代表性的少量样本；推理仅批次感知记忆归一化（IoBMN）动态调整归一化统计量，实现高效域对齐。

Result: 在仅使用1%输入数据进行适应的情况下仍保持竞争力的准确性，集成五个先进TTA算法后延迟最多降低93.12%，精度损失小于3.3%。

Conclusion: SNAP在极低适应频率下仍表现鲁棒，具备在延迟敏感的边缘设备上实际部署的强大潜力。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [239] [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)
*Cheng Yin,Yankai Lin,Wang Xu,Sikyuen Tam,Xiangrui Zeng,Zhiyuan Liu,Zhouping Yin*

Main category: cs.LG

TL;DR: 提出DeepThinkVLA，通过混合注意力解码器和两阶段训练策略解决视觉-语言-动作模型中思维与动作的架构冲突，显著提升机器人任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型使用单一自回归解码器同时处理链式思维推理和高维动作生成，导致动作控制差且思维与动作因果关系弱。

Method: 设计一种混合注意力解码器，先用因果注意力生成链式思维，再切换至双向注意力并行解码动作；结合SFT预训练和强化学习微调的两阶段训练方法。

Result: 在LIBERO基准上达到97.0%的成功率，超越标准解码器15.5%，强化学习阶段带来额外2%提升。

Conclusion: DeepThinkVLA通过架构创新和训练策略协同，有效解耦并联结思维与动作，实现当前最优的视觉-语言-动作控制性能。

Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

</details>


### [240] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: 本文提出了Quant-Trim，一种在训练阶段生成对硬件不敏感的低比特量化模型的方法，通过渐进式伪量化和反向剪枝来提升跨不同编译器和精度设置下的模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于不同厂商的编译器在量化缩放、裁剪和算子支持上存在差异，同一浮点模型在不同后端部署时性能不一致，导致需要针对特定硬件调整模型或参数。

Method: 提出Quant-Trim方法，结合渐进式伪量化以对齐训练与部署时的整数量化网格，并采用反向剪枝抑制异常值引起的尺度膨胀，同时保持模型可学习性。该方法不依赖特定量化方案或厂商算子。

Result: 在多种模型和任务上验证了Quant-Trim能有效缩小浮点与低比特模型之间的性能差距，降低对编译器启发式策略和校准的依赖，避免为不同后端重新训练，并在延迟、吞吐量、能耗和成本等边缘指标上表现良好。

Conclusion: Quant-Trim能够生成硬件中立的模型检查点，显著提高低比特量化模型在不同边缘加速器后端上的兼容性和稳定性，减少部署复杂性。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [241] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型（TSFM）中概念的可解释性，发现早期层主要捕获局部时域模式，深层则编码离散和变点信号，而频谱和形变因子最难线性恢复；在复合概念下，模型表现下降，表明概念间存在干扰。


<details>
  <summary>Details</summary>
Motivation: 尽管TSFM在经验上成功，但其内部如何表示基本时间序列概念仍不清楚，本文旨在系统探究其机制。

Method: 通过逐层分析、线性可恢复性测试和表示相似性度量，系统探测概念在不同层的编码方式、线性可恢复性、解耦与抽象演化及概念组合处理能力。

Result: 早期层捕获AR(1)、水平偏移、趋势等局部时域模式，深层编码离散和变点信号，频谱和形变因子线性恢复困难；在组合概念下探针性能下降，显示概念间存在干扰。

Conclusion: 原子级时间概念在TSFM中可被可靠定位，但概念组合仍具挑战，揭示当前TSFM在表示交互时序现象上的关键局限。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [242] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出基于离散Krawtchouk多项式的图神经网络滤波器KrawtchoukNet，通过固定多项式域和可学习形状参数，统一解决了异配图性能下降和高阶多项式过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 标准多项式滤波器（如ChebyNet）因静态低通特性导致在异配图上表现差且高阶时易过平滑，亟需更鲁棒的滤波器设计。

Method: 采用Krawtchouk多项式构建GNN滤波器，固定域大小N为小常数以保证递推系数有界，并引入可学习的形状参数p使滤波器能自适应图数据的频谱特性。

Result: KrawtchoukNet在高阶（K=10）下显著缓解过平滑，在Texas和Cornell等异配图基准上达到SOTA性能，优于GAT、APPNP等标准GNN模型。

Conclusion: KrawtchoukNet通过有界系数和自适应频谱响应，为异配图和过平滑问题提供了统一且有效的解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [243] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出基于连续拉盖尔多项式的图神经网络滤波器LaguerreNet，通过可训练的alpha参数和LayerNorm稳定技术，有效解决异配图性能差和高阶过平滑问题，在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有谱图神经网络（如ChebyNet）因静态低通滤波器导致在异配图上表现差且易发生高阶过平滑；自适应多项式滤波器（如MeixnerNet）虽有潜力，但其连续域扩展与无界系数稳定性仍是未解问题。

Method: 提出LaguerreNet，采用连续拉盖尔多项式构建滤波器，核心alpha参数可训练以自适应学习频谱形状，并引入LayerNorm-based方法解决O(k^2)数值不稳定性。

Result: 实验表明：1) LaguerreNet在具有挑战性的异配图基准上取得SOTA性能；2) 对过平滑高度鲁棒，性能在K=10时仍达峰值，远超ChebyNet的崩溃点。

Conclusion: LaguerreNet通过可学习的连续正交多项式滤波器，统一解决了异配性和过平滑问题，显著提升了谱GNN的表达能力与稳定性。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [244] [STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection](https://arxiv.org/abs/2511.15339)
*Kadir-Kaan Özer,René Ebeling,Markus Enzweiler*

Main category: cs.LG

TL;DR: 本文提出了一种名为STREAM-VAE的变分自编码器，用于汽车遥测时间序列数据中的异常检测，通过分离慢漂移和快尖峰动态提升检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 汽车遥测数据中同时存在慢漂移和快尖峰，传统基于重构的方法（如序列VAE）因使用单一潜在过程而难以有效区分异常，导致尖峰被平滑或方差膨胀，影响异常检测性能。

Method: 提出STREAM-VAE，采用双路径编码器分别处理慢漂移和快尖峰信号动态，并在解码器中将瞬态偏差与正常运行模式分开表示，从而生成稳定的异常评分。

Result: 在汽车遥测数据集和公开SMD基准上的实验表明，该方法在预测、注意力、图模型和VAE等强基线中表现出更优的鲁棒性。

Conclusion: 通过显式分离漂移和尖峰动态，STREAM-VAE提升了异常检测的准确性与稳定性，适用于车载监控和后端车队分析的部署场景。

Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.

</details>


### [245] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列预测中的集成方法，提出了一种多层堆叠框架，通过结合不同堆叠模型的优势，在多种预测场景中显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中集成方法的使用仍不充分，简单的线性组合仍是主流，缺乏对更复杂集成策略（如堆叠）的系统研究和应用。

Method: 评估了33种现有和新提出的集成模型，在50个真实世界数据集上进行实验，并提出一种多层堆叠框架以融合不同堆叠模型的优势。

Result: 堆叠方法能持续提升预测准确性，但没有单一堆叠器在所有任务上表现最佳；所提出的多层堆叠框架在各种预测场景中均表现出更优的准确性。

Conclusion: 基于堆叠的集成方法，特别是多层堆叠框架，具有显著提升时间序列预测性能的潜力，可广泛应用于AutoML系统中。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [246] [Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction](https://arxiv.org/abs/2511.15357)
*Yinan Yu,Falk Dippel,Christina E. Lundberg,Martin Lindgren,Annika Rosengren,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 该论文提出了一种成本感知预测（CAP）框架，结合大语言模型（LLM）代理进行成本效益分析，提升机器学习在临床决策中的可解释性与实用性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型常忽视临床应用中的成本权衡和可解释性，难以直接支持医疗决策。

Method: 开发了一个预测心力衰竭患者1年死亡率的XGBoost模型，并引入临床影响投影（CIP）曲线可视化治疗成本与错误成本；使用四个LLM代理生成个体化成本效益分析描述。

Result: XGB模型表现最佳（AUROC 0.804，AUPRC 0.529）；CIP曲线提供了群体层面的成本分析，LLM生成个体化解释；临床评估反馈良好，但指出需提升LLM在推测性任务中的准确性。

Conclusion: CAP框架通过整合LLM代理与机器学习预测，实现了更透明、可解释的临床决策支持，有助于平衡医疗成本与患者预后。

Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.

</details>


### [247] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种新的局部特征重要性方法——反事实重要性分布（CID），通过生成正负反事实并基于分布差异度量来评估特征重要性，具有良好的数学基础和解释保真度。


<details>
  <summary>Details</summary>
Motivation: 现有特征重要性方法缺乏明确的评估基准，需要一种有理论依据的新度量方式来更可靠地解释模型决策。

Method: 生成正负两类反事实样本，使用核密度估计建模其分布，并基于满足度量性质的分布差异度量对特征进行重要性排序。

Result: 在保真度指标（全面性和充分性）上优于或媲美现有主流局部解释方法，提供了互补的解释视角。

Conclusion: CID是一种有数学基础、高保真度的局部特征重要性方法，可作为模型分析的有效工具。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [248] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: 本文提出PIECE方法，通过参数重要性估计在不访问历史数据或增加模型参数的情况下，有效缓解领域特定后训练中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在领域特定后训练中因灾难性遗忘而失去通用推理能力的问题，提升模型在动态现实环境中的适应性。

Method: 提出PIECE方法，基于Fisher信息（PIECE-F）和结合梯度与曲率的二阶归一化（PIECE-S）两种重要性评估器，仅选择性更新0.1%最相关的核心参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE在保持通用能力的同时，在多种下游任务上实现了最先进的持续学习性能。

Conclusion: PIECE为可扩展、领域自适应的基础模型提供了一条实用路径，能够在不发生灾难性遗忘的情况下高效学习新领域知识。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [249] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: 提出EVA-Net，一种基于EEG的可解释脑龄预测与异常检测框架，通过稀疏注意力Transformer和变分信息瓶颈学习健康衰老流形，实现对轻度认知障碍和阿尔茨海默病的高效识别。


<details>
  <summary>Details</summary>
Motivation: 现有脑龄预测模型在处理仅包含健康样本的弱监督、不完美医疗数据时存在局限，且缺乏可解释性，难以有效识别神经退行性疾病。

Method: 提出EVA-Net，采用稀疏注意力Transformer建模长序列EEG信号，结合变分信息瓶颈学习鲁棒压缩表征，并通过连续原型网络对齐健康衰老流形以增强可解释性。

Result: 在1297名健康个体上训练后，EVA-Net达到SOTA精度；在27名MCI和AD患者中验证，显示出显著增大的脑龄差距和原型对齐误差。

Conclusion: EVA-Net为基于不完美医疗数据的脑龄预测提供了可解释、鲁棒的异常检测框架，具有临床应用潜力。

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [250] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出了一类基于变分拉格朗日公式的状态估计算法，用于非线性、非高斯状态空间模型，通过高斯-马尔可夫近似和广义统计线性回归实现高效递归计算。


<details>
  <summary>Details</summary>
Motivation: 在非线性、非高斯状态空间模型中实现高效且准确的贝叶斯状态估计。

Method: 采用变分拉格朗日方法，将贝叶斯推断转化为受动态约束的熵信任域更新，并结合高斯-马尔可夫近似、广义统计线性回归和傅里叶-厄米特矩匹配来构建前向-后向递归算法。

Result: 推导出具有较低计算复杂度的递归状态估计算法，适用于一般非线性、非高斯模型。

Conclusion: 该框架为非线性滤波问题提供了一个统一且高效的变分推断方法，具有良好的扩展性和数值稳定性。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [251] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 研究了表格上下文学习模型中各层对预测的贡献，发现部分层存在结构冗余，可通过“层如画笔”视角进行模型压缩与可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 尽管表格上下文学习模型与大语言模型架构相似，但其各层如何影响预测尚不清楚，因此需要探究层间表征演化及冗余问题。

Method: 采用“层如画笔”视角分析TabPFN和TabICL模型，研究其在不同层上的潜在表征空间演化过程，并与大语言模型进行对比。

Result: 发现只有部分层共享相同的表征语言，表明存在结构性冗余，且这些模型中的某些层可能是可压缩的。

Conclusion: 表格ICL模型中存在层功能冗余，识别出的共同表征结构有助于模型压缩和提升可解释性。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [252] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出一种基于时间序列基础模型（TSFM）上下文学习的分类方法，无需微调即可对非训练数据进行分类，应用于伺服压力机电机轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 克服传统定制化AI方法在工业维护中泛化能力差的问题，利用预训练TSFM实现无需微调的跨场景分类。

Method: 将频域参考信号转换为伪时序模式，构建协变量与目标信号对，通过提示工程在TSFM中实现上下文学习，沿预测轴输出分类概率。

Result: 该方法在不同工况下均有效识别轴承健康状态，展现出良好的可扩展性和鲁棒性。

Conclusion: 所提方法推动了从专用窄AI向更广泛的AI驱动维护系统的进步，验证了TSFM在工业诊断中的潜力。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [253] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: 提出了一种名为FairEnergy的公平感知节能框架，用于无线边缘系统中的联邦学习，通过联合优化设备选择、带宽分配和压缩级别来平衡能效、公平性和模型精度。


<details>
  <summary>Details</summary>
Motivation: 在资源异构、客户端贡献不均和通信能力受限的无线边缘系统中，现有联邦学习方法难以同时保证能效、公平参与和高模型精度。

Method: 引入包含更新幅度和压缩比的贡献评分机制，构建混合整数非凸优化问题，并通过松弛二进制变量和拉格朗日分解求解，实现设备选择、带宽分配和压缩水平的联合优化。

Result: 实验表明，在非独立同分布数据下，与基线方法相比，FairEnergy在提高模型准确率的同时，能耗降低了最高达79%。

Conclusion: FairEnergy有效平衡了联邦学习中的能效、公平性和准确性，显著降低了能耗，提升了系统整体性能。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [254] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出NTK引导的隐式神经教学（NINT）方法，通过动态选择最大化全局函数更新的坐标来加速隐式神经表示（INR）训练，显著减少训练时间并保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）在高分辨率信号建模中面临巨大计算成本，因需优化数百万坐标，导致训练效率低下。

Method: 利用神经正切核（NTK）分析损失梯度的范数，动态选择对全局函数更新贡献最大的坐标进行训练，结合拟合误差与坐标间的自影响和耦合效应。

Result: NINT在实验中将训练时间几乎减半，同时保持或提升重建质量，优于现有基于采样的加速策略。

Conclusion: NINT通过NTK引导的样本选择机制，实现了INR训练的高效加速，是当前最先进的采样加速方法之一。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [255] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 研究了在多分布学习中样本复杂度与轮次复杂度之间的权衡，提出了优化通过按需采样的新框架，并给出了在可实现和通用对抗情况下的近似最优结果。


<details>
  <summary>Details</summary>
Motivation: 在多分布学习中，如何在有限的轮次内高效地进行自适应采样以平衡样本复杂度和计算效率是一个关键问题。

Method: 引入了一个新的框架OODS（Optimization via On-Demand Sampling），用于抽象样本适应性与轮次之间的权衡，并分析其上下界。

Result: 在可实现情况下，r轮算法的最优样本复杂度约为dk^{Θ(1/r)} / ε；在对抗情况下，提出了一种在Õ(√k)轮内达到近似最优样本复杂度Õ((d + k)/ε²)的算法。

Conclusion: OODS框架能够捕捉大多数现有的MDL算法，且现有技术难以突破多项式轮次复杂度，需要全新的方法来绕过OODS的内在难度。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [256] [CODE: A global approach to ODE dynamics learning](https://arxiv.org/abs/2511.15619)
*Nils Wildt,Daniel M. Tartakovsky,Sergey Oladyshkin,Wolfgang Nowak*

Main category: cs.LG

TL;DR: 提出了一种基于任意多项式混沌展开（aPCE）的常微分方程右端项建模方法CODE，用于从稀疏且含噪声的数据中学习动力学系统，表现出优于NeuralODE和KernelODE的外推能力。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的动力学建模中，通常面临稀疏采样和测量噪声的问题，而现有高灵活性模型（如NeuralODE、KernelODE）在数据稀缺时外推性能较差，因此需要一种更具鲁棒性和泛化能力的方法。

Method: 提出ChaosODE（CODE），采用任意多项式混沌展开（aPCE）对ODE的右端函数进行全局正交多项式表示，从而实现从稀疏观测数据中学习动态系统演化规律。

Result: 在Lotka-Volterra系统上的实验表明，CODE在不同噪声水平、初始条件以及未来长时间预测（包括未见过的初始条件）下均表现出优异的外推能力和稳定性，优于NeuralODE和KernelODE。

Conclusion: 基于aPCE的CODE方法在稀疏和噪声数据下具有更强的外推能力和鲁棒性，为数据驱动的动力学学习提供了更可靠的选择，并给出了优化此类问题的实用指南。

Abstract: Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.

</details>


### [257] [Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges](https://arxiv.org/abs/2511.15652)
*Kim N. Nolle,Ivana Dusparic,Rhodri Cusack,Vinny Cahill*

Main category: cs.LG

TL;DR: 本文探讨了持续强化学习（CRL）在自动驾驶环境中的挑战，通过四个泊车场景的实验揭示了环境抽象、超参数敏感性、灾难性遗忘和神经网络容量利用等关键问题，并提出了未来研究方向，质疑了神经网络在持续学习中的适用性，呼吁计算机科学与神经科学的跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 尽管持续学习在机器学习中取得进展，但在强化学习中的应用仍面临挑战，特别是在动态变化的现实世界场景如自动驾驶中，需要解决模型如何持续适应新任务而不遗忘旧知识的问题。

Method: 在自动驾驶泊车环境中，使用PPO算法让智能体依次学习四种不同角度的泊车任务，模拟持续学习场景，通过实验分析其表现并识别关键挑战。

Result: 实验揭示了持续强化学习中的多个开放性问题：难以找到合适的环境抽象、对超参数高度敏感、存在灾难性遗忘现象，以及神经网络容量利用效率低。

Conclusion: 当前CRL面临重大挑战，需深入研究更优的表示学习、正则化方法和架构设计；同时，神经网络是否适合持续学习值得重新审视，建议推动计算机科学与神经科学的跨学科研究以获得启发。

Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.

</details>


### [258] [Walrus: A Cross-Domain Foundation Model for Continuum Dynamics](https://arxiv.org/abs/2511.15684)
*Michael McCabe,Payel Mukhopadhyay,Tanya Marwah,Bruno Regaldo-Saint Blancard,Francois Rozet,Cristiana Diaconu,Lucas Meyer,Kaze W. K. Wong,Hadi Sotoudeh,Alberto Bietti,Irina Espejo,Rio Fear,Siavash Golkar,Tom Hehir,Keiya Hirashima,Geraud Krawezik,Francois Lanusse,Rudy Morel,Ruben Ohana,Liam Parker,Mariel Pettee,Jeff Shen,Kyunghyun Cho,Miles Cranmer,Shirley Ho*

Main category: cs.LG

TL;DR: Walrus是一个基于Transformer的基础模型，专为流体类连续介质动力学设计，通过新的稳定化方法、分布式训练策略和计算自适应分词技术，在多样化的物理模拟任务中表现出优异的短期和长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型在语言和视觉领域取得了巨大成功，但在物理模拟中的应用受限于数据异构性、不稳定的长期动态以及不同分辨率和维度带来的训练效率问题。

Method: 提出了一种基于谐波分析的稳定化方法、负载均衡的分布式2D/3D训练策略以及计算自适应的分词机制，并基于这些技术构建了Walrus模型。

Result: Walrus在19个来自天体物理、地球科学、流变学等多个领域的多样化场景中进行预训练，实验表明其在下游任务的短期和长期预测上均优于先前的基础模型，且消融研究验证了各项技术对稳定性、训练吞吐量和迁移性能的贡献。

Conclusion: Walrus展示了基础模型在复杂物理模拟中的潜力，推动了机器学习在多领域科学模拟中的统一建模能力。

Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.

</details>


### [259] [The Impact of Quantization on Large Reasoning Model Reinforcement Learning](https://arxiv.org/abs/2511.15694)
*Medha Kumar,Zifei Xu,Xin Wang,Tristan Webb*

Main category: cs.LG

TL;DR: 量化对大规模推理模型的强化学习（RL）影响显著，量化感知RL训练反而损害性能，而后训练量化（PTQ）和QLoRA表现更优。


<details>
  <summary>Details</summary>
Motivation: 探究量化方法对大规模推理模型中强化学习过程的影响，特别是在数学推理任务上的表现差异。

Method: 通过系统性实验比较后训练量化（PTQ）、量化感知训练（QAT）以及QLoRA在RL后的推理性能差异。

Result: 发现量化感知RL训练会负面影响学习过程，而PTQ和QLoRA在数学基准上表现更好，推理性能差距显著。

Conclusion: 在大规模推理模型中，应避免使用量化感知RL训练，优先采用PTQ或QLoRA以保持推理性能。

Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.

</details>
