<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 75]
- [cs.CL](#cs.CL) [Total: 93]
- [cs.LG](#cs.LG) [Total: 115]
- [cs.RO](#cs.RO) [Total: 29]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation](https://arxiv.org/abs/2510.05532)
*Sam Sartor,Pieter Peers*

Main category: cs.CV

TL;DR: 提出了一种名为Teamwork的灵活高效统一方法，通过协调多个预训练扩散模型实例（teammates）来实现通道扩展和任务适应，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有通道扩展方法通常特定于应用，难以适配不同扩散模型或新任务，需要一种通用解决方案。

Method: 使用低秩自适应（LoRA）的变体，协调多个基础扩散模型实例，在不改变原模型结构的前提下联合进行通道扩展与任务适应，并支持动态激活或去激活teammates。

Result: 在图像修复、单图SVBRDF估计、本征分解、神经着色和本征图像合成等多种生成与逆向图形任务上验证了Teamwork的有效性与灵活性。

Conclusion: Teamwork提供了一种通用、灵活且高效的框架，可用于扩展扩散模型的输入输出通道并适应新任务，具有良好的可扩展性和实际应用价值。

Abstract: Large pretrained diffusion models can provide strong priors beneficial for
many graphics applications. However, generative applications such as neural
rendering and inverse methods such as SVBRDF estimation and intrinsic image
decomposition require additional input or output channels. Current solutions
for channel expansion are often application specific and these solutions can be
difficult to adapt to different diffusion models or new tasks. This paper
introduces Teamwork: a flexible and efficient unified solution for jointly
increasing the number of input and output channels as well as adapting a
pretrained diffusion model to new tasks. Teamwork achieves channel expansion
without altering the pretrained diffusion model architecture by coordinating
and adapting multiple instances of the base diffusion model (\ie, teammates).
We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address
both adaptation and coordination between the different teammates. Furthermore
Teamwork supports dynamic (de)activation of teammates. We demonstrate the
flexibility and efficiency of Teamwork on a variety of generative and inverse
graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic
decomposition, neural shading, and intrinsic image synthesis.

</details>


### [2] [Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation](https://arxiv.org/abs/2510.05266)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: 本文提出了一种用于涵洞和下水道缺陷少样本语义分割的增强型特征金字塔网络（E-FPN），结合原型学习框架，通过自适应编码器、掩码平均池化生成原型以及注意力机制，在小样本条件下实现了优异性能。


<details>
  <summary>Details</summary>
Motivation: 由于基础设施检测中标注数据稀缺且昂贵，现有深度学习方法难以快速适应新缺陷类别，因此需要一种能在少量标注样本下高效学习的少样本语义分割方法。

Method: 提出E-FPN框架，包含三个核心组件：基于InceptionSepConv和深度可分离卷积的自适应编码器、掩码平均池化的原型学习、以及全局/局部自注意力与交叉注意力机制的特征表示。

Result: 在具有挑战性的基础设施检测数据集上，最佳配置（8类5样本训练）达到82.55% F1分数和72.26% mIoU；自注意力机制相比基线提升显著，F1分数提高2.57%，mIoU提高2.9%。

Conclusion: 该框架有效解决了基础设施检测中因新缺陷类型标注数据不足而难以快速响应的问题，有助于实现更高效、经济的关键基础设施维护。

Abstract: Few-shot semantic segmentation is vital for deep learning-based
infrastructure inspection applications, where labeled training examples are
scarce and expensive. Although existing deep learning frameworks perform well,
the need for extensive labeled datasets and the inability to learn new defect
categories with little data are problematic. We present our Enhanced Feature
Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert
and sewer defect categories using a prototypical learning framework. Our
approach has three main contributions: (1) adaptive E-FPN encoder using
InceptionSepConv blocks and depth-wise separable convolutions for efficient
multi-scale feature extraction; (2) prototypical learning with masked average
pooling for powerful prototype generation from small support examples; and (3)
attention-based feature representation through global self-attention, local
self-attention and cross-attention. Comprehensive experimentation on
challenging infrastructure inspection datasets illustrates that the method
achieves excellent few-shot performance, with the best configuration being
8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way
classification testing. The self-attention method had the most significant
performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over
baselines. Our framework addresses the critical need to rapidly respond to new
defect types in infrastructure inspection systems with limited new training
data that lead to more efficient and economical maintenance plans for critical
infrastructure systems.

</details>


### [3] [SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography](https://arxiv.org/abs/2510.05296)
*Zahra Maleki,Amirhossein Akbari,Amirhossein Binesh,Babak Khalaj*

Main category: cs.CV

TL;DR: 提出一种新的皮肤分割技术，用于提升远程光电容积描记法（rPPG）在复杂条件下的信号质量，具有良好的抗运动干扰能力和广泛的肤色适应性。


<details>
  <summary>Details</summary>
Motivation: 为了提高rPPG技术在光照变化和运动干扰下提取心率信号的准确性，尤其是在非受控环境中。

Method: 提出一种新型皮肤分割方法，优先选择全身皮肤区域并排除口、眼、头发等干扰区域，结合公开数据集和新构建的SYNC-rPPG数据集进行评估。

Result: 该方法在说话、头部转动等挑战性条件下仍能准确捕捉心跳信号，保持较低的平均绝对误差（MAE），且在多种肤色上表现出高检测精度。

Conclusion: 所提出的皮肤分割技术显著提升了rPPG信号的质量和鲁棒性，适用于真实场景中的远程生命体征监测。

Abstract: Remote photoplethysmography (rPPG) is an innovative method for monitoring
heart rate and vital signs by using a simple camera to record a person, as long
as any part of their skin is visible. This low-cost, contactless approach helps
in remote patient monitoring, emotion analysis, smart vehicle utilization, and
more. Over the years, various techniques have been proposed to improve the
accuracy of this technology, especially given its sensitivity to lighting and
movement. In the unsupervised pipeline, it is necessary to first select skin
regions from the video to extract the rPPG signal from the skin color changes.
We introduce a novel skin segmentation technique that prioritizes skin regions
to enhance the quality of the extracted signal. It can detect areas of skin all
over the body, making it more resistant to movement, while removing areas such
as the mouth, eyes, and hair that may cause interference. Our model is
evaluated on publicly available datasets, and we also present a new dataset,
called SYNC-rPPG, to better represent real-world conditions. The results
indicate that our model demonstrates a prior ability to capture heartbeats in
challenging conditions, such as talking and head rotation, and maintain the
mean absolute error (MAE) between predicted and actual heart rates, while other
methods fail to do so. In addition, we demonstrate high accuracy in detecting a
diverse range of skin tones, making this technique a promising option for
real-world applications.

</details>


### [4] [DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology](https://arxiv.org/abs/2510.05315)
*Yousef Yeganeh,Maximilian Frantzen,Michael Lee,Kun-Hsing Yu,Nassir Navab,Azade Farshad*

Main category: cs.CV

TL;DR: 提出了一种名为DeepAf的新型自动对焦框架，结合空间和光谱特征，实现单次拍摄即可预测最佳焦点，显著提升显微镜数字化病理样本的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的低成本显微镜数字化方案在对焦一致性、速度或泛化能力方面存在局限，限制了资源有限环境下的数字病理应用。

Method: 开发了DeepAf，一种融合空间与光谱特征的混合架构深度学习模型，通过单张图像回归最优焦距距离，并动态调整控制参数；集成至常规显微镜实现自动化扫描。

Result: 相比基于焦栈的方法减少80%对焦时间，同实验室样本对焦精度达0.18μm（媲美双图像方法的0.19μm）；跨实验室误焦点预测仅0.72%，90%预测在景深范围内；在536个脑组织样本中4x放大下癌症分类AUC达0.90。

Conclusion: 该系统以软硬件协同设计实现了高效、准确且具良好泛化能力的实时数字病理解决方案，显著降低了数字病理的技术门槛和成本，适用于资源受限场景。

Abstract: While Whole Slide Imaging (WSI) scanners remain the gold standard for
digitizing pathology samples, their high cost limits accessibility in many
healthcare settings. Other low-cost solutions also face critical limitations:
automated microscopes struggle with consistent focus across varying tissue
morphology, traditional auto-focus methods require time-consuming focal stacks,
and existing deep-learning approaches either need multiple input images or lack
generalization capability across tissue types and staining protocols. We
introduce a novel automated microscopic system powered by DeepAf, a novel
auto-focus framework that uniquely combines spatial and spectral features
through a hybrid architecture for single-shot focus prediction. The proposed
network automatically regresses the distance to the optimal focal point using
the extracted spatiospectral features and adjusts the control parameters for
optimal image outcomes. Our system transforms conventional microscopes into
efficient slide scanners, reducing focusing time by 80% compared to stack-based
methods while achieving focus accuracy of 0.18 {\mu}m on the same-lab samples,
matching the performance of dual-image methods (0.19 {\mu}m) with half the
input requirements. DeepAf demonstrates robust cross-lab generalization with
only 0.72% false focus predictions and 90% of predictions within the depth of
field. Through an extensive clinical study of 536 brain tissue samples, our
system achieves 0.90 AUC in cancer classification at 4x magnification, a
significant achievement at lower magnification than typical 20x WSI scans. This
results in a comprehensive hardware-software design enabling accessible,
real-time digital pathology in resource-constrained settings while maintaining
diagnostic accuracy.

</details>


### [5] [Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection](https://arxiv.org/abs/2510.05326)
*Jalal Ahmmed,Faruk Ahmed,Rashedul Hasan Shohan,Md. Mahabub Rana,Mahdi Hasan*

Main category: cs.CV

TL;DR: 该研究利用五种预训练卷积神经网络（DenseNet201、InceptionV3、ResNet152V2、SeResNet152和Xception）结合迁移学习与微调策略，对八类芒果叶病害进行多分类识别。DenseNet201表现最佳，准确率达99.33%，尤其在识别Cutting Weevil和Bacterial Canker方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 芒果是南亚重要经济作物，但叶部病害严重影响其产量和品质，亟需高效、精准的病害识别方法以支持智能农业应用。

Method: 采用迁移学习与微调策略，评估五种预训练CNN模型（DenseNet201、InceptionV3、ResNet152V2、SeResNet152、Xception）在八类芒果叶病害图像上的多分类性能，使用准确率、精确率、召回率、F1分数和混淆矩阵进行评价。

Result: DenseNet201取得最高准确率99.33%，各类别指标稳定；ResNet152V2和SeResNet152表现良好；InceptionV3和Xception在视觉相似病害（如Sooty Mould与Powdery Mildew）上表现较差。训练与验证曲线显示高性能模型收敛稳定。

Conclusion: 微调后的迁移学习模型，特别是DenseNet201，能够实现精确且可靠的多类芒果叶病害检测，具有在智能农业中广泛应用的潜力。

Abstract: Mango is an important fruit crop in South Asia, but its cultivation is
frequently hampered by leaf diseases that greatly impact yield and quality.
This research examines the performance of five pre-trained convolutional neural
networks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for
multi-class identification of mango leaf diseases across eight classes using a
transfer learning strategy with fine-tuning. The models were assessed through
standard evaluation metrics, such as accuracy, precision, recall, F1-score, and
confusion matrices. Among the architectures tested, DenseNet201 delivered the
best results, achieving 99.33% accuracy with consistently strong metrics for
individual classes, particularly excelling in identifying Cutting Weevil and
Bacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong
outcomes, whereas InceptionV3 and Xception exhibited lower performance in
visually similar categories like Sooty Mould and Powdery Mildew. The training
and validation plots demonstrated stable convergence for the highest-performing
models. The capability of fine-tuned transfer learning models, for precise and
dependable multi-class mango leaf disease detection in intelligent agricultural
applications.

</details>


### [6] [Mitigating Diffusion Model Hallucinations with Dynamic Guidance](https://arxiv.org/abs/2510.05356)
*Kostas Triaridis,Alexandros Graikos,Aggelina Chatziagapi,Grigorios G. Chrysos,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了Dynamic Guidance方法，通过在生成时选择性地锐化导致伪影的方向上的得分函数，减少扩散模型中的幻觉问题，同时保留有效的语义变化。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然表现优异，但常产生结构不一致的幻觉样本，这是由于数据分布模式间过度平滑所致。然而，语义插值对于生成多样性是有益的，因此需要一种更精细的方法来平衡这两者。

Method: 引入Dynamic Guidance，在预定义的易产生伪影的方向上选择性锐化得分函数，以减少幻觉，同时保持有意义的语义插值能力。该方法在生成过程中直接起作用，而非依赖后处理过滤。

Result: Dynamic Guidance在控制和自然图像数据集上显著减少了幻觉现象，并明显优于基线方法。

Conclusion: Dynamic Guidance是首个在生成时直接解决扩散模型幻觉问题的方法，有效抑制了不真实样本的生成，同时保留了有益的生成多样性。

Abstract: Diffusion models, despite their impressive demos, often produce hallucinatory
samples with structural inconsistencies that lie outside of the support of the
true data distribution. Such hallucinations can be attributed to excessive
smoothing between modes of the data distribution. However, semantic
interpolations are often desirable and can lead to generation diversity, thus
we believe a more nuanced solution is required. In this work, we introduce
Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates
hallucinations by selectively sharpening the score function only along the
pre-determined directions known to cause artifacts, while preserving valid
semantic variations. To our knowledge, this is the first approach that
addresses hallucinations at generation time rather than through post-hoc
filtering. Dynamic Guidance substantially reduces hallucinations on both
controlled and natural image datasets, significantly outperforming baselines.

</details>


### [7] [LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation](https://arxiv.org/abs/2510.05367)
*Yang Xiao,Gen Li,Kaiyuan Deng,Yushu Wu,Zheng Zhan,Yanzhi Wang,Xiaolong Ma,Bo Hui*

Main category: cs.CV

TL;DR: 本文提出了一种针对扩散模型视频生成的无训练加速方法，通过分阶段优化缓存策略，在降低内存消耗的同时保持推理速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于缓存的加速方法在去噪和解码阶段导致显著的内存激增，限制了训练-free加速的实用性。

Method: 将推理过程分解为编码、去噪和解码三个阶段，并提出三个阶段特定的策略：异步缓存交换、特征分块和切片解码，以减少内存使用并控制时间开销。

Result: 相比基线方法，该方法在降低内存占用的同时实现了更快的推理速度，且生成质量下降在可接受范围内。

Conclusion: 所提出的阶段特定内存优化策略有效平衡了推理速度、内存消耗与生成质量，为训练-free视频生成加速提供了高效解决方案。

Abstract: Training-free acceleration has emerged as an advanced research area in video
generation based on diffusion models. The redundancy of latents in diffusion
model inference provides a natural entry point for acceleration. In this paper,
we decompose the inference process into the encoding, denoising, and decoding
stages, and observe that cache-based acceleration methods often lead to
substantial memory surges in the latter two stages. To address this problem, we
analyze the characteristics of inference across different stages and propose
stage-specific strategies for reducing memory consumption: 1) Asynchronous
Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same
time, we ensure that the time overhead introduced by these three strategies
remains lower than the acceleration gains themselves. Compared with the
baseline, our approach achieves faster inference speed and lower memory usage,
while maintaining quality degradation within an acceptable range. The Code is
available at https://github.com/NKUShaw/LightCache .

</details>


### [8] [See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models](https://arxiv.org/abs/2510.05408)
*Kebin Contreras,Luis Toscano-Palomino,Mauro Dalla Mura,Jorge Bacca*

Main category: cs.CV

TL;DR: 提出一种基于RGB-热成像配对图像的时间反演重建框架，结合视觉-语言模型与约束扩散过程，恢复数秒至120秒前的场景状态。


<details>
  <summary>Details</summary>
Motivation: 利用热成像中人体活动留下的残余热迹（被动时间编码）来推断近期事件，弥补传统RGB相机在时间回溯分析上的不足，具有法医学和场景分析的应用潜力。

Method: 采用双视觉-语言模型（VLM）协同工作：一个生成场景语义描述，另一个在约束扩散过程中指导图像重建，确保语义与结构一致性，实现从当前热成像数据反演过去场景。

Result: 在三个受控场景中验证了方法可行性，成功重建了最多120秒前的合理过去帧，展示了从热痕迹进行时间反演成像的潜力。

Conclusion: 该框架为基于热残留信息的时间反演成像提供了首个有效方案，拓展了热成像在时序场景理解中的应用边界。

Abstract: Recovering the past from present observations is an intriguing challenge with
potential applications in forensics and scene analysis. Thermal imaging,
operating in the infrared range, provides access to otherwise invisible
information. Since humans are typically warmer (37 C -98.6 F) than their
surroundings, interactions such as sitting, touching, or leaning leave residual
heat traces. These fading imprints serve as passive temporal codes, allowing
for the inference of recent events that exceed the capabilities of RGB cameras.
This work proposes a time-reversed reconstruction framework that uses paired
RGB and thermal images to recover scene states from a few seconds earlier. The
proposed approach couples Visual-Language Models (VLMs) with a constrained
diffusion process, where one VLM generates scene descriptions and another
guides image reconstruction, ensuring semantic and structural consistency. The
method is evaluated in three controlled scenarios, demonstrating the
feasibility of reconstructing plausible past frames up to 120 seconds earlier,
providing a first step toward time-reversed imaging from thermal traces.

</details>


### [9] [Personalizing Retrieval using Joint Embeddings or "the Return of Fluffy"](https://arxiv.org/abs/2510.05411)
*Bruno Korbar,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了一种名为pi-map的可训练映射网络，通过将局部图像嵌入转换为文本token，结合自然语言查询实现基于复合查询的图像检索，提升了个性化检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现结合图像中物体实例信息与自然语言描述的复合查询图像检索，解决现有方法难以融合视觉与文本信息的问题。

Method: 设计了一个映射网络pi-map，将物体实例的图像嵌入转换为适合CLIP文本编码的文本token，并与自然语言查询结合进行检索；该网络只需对每个实例进行一次简单训练，且利用冻结的CLIP编码器。

Result: 在两个评估个性化检索的基准上，该方法优于现有技术，显著提升了检索性能。

Conclusion: pi-map能有效桥接视觉实例与文本描述，实现高效的复合查询图像检索，为个性化检索提供了新思路。

Abstract: The goal of this paper is to be able to retrieve images using a compound
query that combines object instance information from an image, with a natural
text description of what that object is doing or where it is. For example, to
retrieve an image of "Fluffy the unicorn (specified by an image) on someone's
head". To achieve this we design a mapping network that can "translate" from a
local image embedding (of the object instance) to a text token, such that the
combination of the token and a natural language query is suitable for CLIP
style text encoding, and image retrieval. Generating a text token in this
manner involves a simple training procedure, that only needs to be performed
once for each object instance. We show that our approach of using a trainable
mapping network, termed pi-map, together with frozen CLIP text and image
encoders, improves the state of the art on two benchmarks designed to assess
personalized retrieval.

</details>


### [10] [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](https://arxiv.org/abs/2510.05488)
*Peizhi Yan,Rabab Ward,Qiang Tang,Shan Du*

Main category: cs.CV

TL;DR: 本文提出了ArchitectHead，首个支持连续控制细节层次（LOD）的3D高斯点阵头像框架，通过在2D UV特征空间中参数化高斯点并使用多级可学习特征图实现高效、无需重新训练的LOD调节。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点阵头像缺乏对细节层次的灵活控制，难以在渲染效率与视觉质量之间取得平衡，限制了其在实际应用中的部署。

Method: 将3D高斯点参数化到2D UV特征空间，构建包含多级可学习特征图的UV特征场来编码隐含特征，并利用轻量神经网络解码器将其转换为3D高斯属性；通过动态重采样不同分辨率的特征图来调节高斯点数量，实现连续LOD控制。

Result: 在最高LOD下达到SOTA渲染质量，在低LOD下仅用6.2%的高斯点仍保持接近SOTA的表现，最低LOD时渲染速度接近翻倍，视觉质量适度下降（L1 Loss +7.9%，PSNR -0.97%，SSIM -0.6%，LPIPS +24.1%）。

Conclusion: ArchitectHead首次实现了无需重新训练的连续LOD控制，显著提升了3D高斯头像在不同应用场景下的灵活性和效率。

Abstract: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time
rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on
tens of thousands of 3D Gaussian points (Gaussians), with the number of
Gaussians fixed after training. However, many practical applications require
adjustable levels of detail (LOD) to balance rendering efficiency and visual
quality. In this work, we propose "ArchitectHead", the first framework for
creating 3D Gaussian head avatars that support continuous control over LOD. Our
key idea is to parameterize the Gaussians in a 2D UV feature space and propose
a UV feature field composed of multi-level learnable feature maps to encode
their latent features. A lightweight neural network-based decoder then
transforms these latent features into 3D Gaussian attributes for rendering.
ArchitectHead controls the number of Gaussians by dynamically resampling
feature maps from the UV feature field at the desired resolutions. This method
enables efficient and continuous control of LOD without retraining.
Experimental results show that ArchitectHead achieves state-of-the-art (SOTA)
quality in self and cross-identity reenactment tasks at the highest LOD, while
maintaining near SOTA performance at lower LODs. At the lowest LOD, our method
uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss
+7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering
speed nearly doubles.

</details>


### [11] [Human Action Recognition from Point Clouds over Time](https://arxiv.org/abs/2510.05506)
*James Dickens*

Main category: cs.CV

TL;DR: 提出了一种基于3D点云的新型人体动作识别方法，结合点云分割、跟踪与体部位分割，并融合点基技术与稀疏卷积网络，在NTU RGB-D 120数据集上达到89.3%的准确率，优于现有点云方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度传感器和Lidar的普及，利用密集3D数据进行动作识别成为可能，但现有研究主要集中于骨骼或视频方法，缺乏对3D点云数据的有效利用。

Method: 构建一个包含人形点云分割、跨帧跟踪和体部位分割的处理流程；采用结合点基技术和稀疏卷积网络的新骨干网络处理体素化点云序列，并引入表面法线、颜色、红外强度和体部位标签等辅助特征提升识别性能。

Result: 在NTU RGB-D 120数据集上，该方法在跨被试设置下达到89.3%的识别准确率，与现有骨骼识别算法相当，并优于此前的点云动作识别方法。

Conclusion: 所提出的3D点云动作识别框架有效利用多模态3D数据，结合多种特征与融合策略，为动作识别提供了除骨骼和视频之外的‘第三条路径’，具有良好的应用前景。

Abstract: Recent research into human action recognition (HAR) has focused predominantly
on skeletal action recognition and video-based methods. With the increasing
availability of consumer-grade depth sensors and Lidar instruments, there is a
growing opportunity to leverage dense 3D data for action recognition, to
develop a third way. This paper presents a novel approach for recognizing
actions from 3D videos by introducing a pipeline that segments human point
clouds from the background of a scene, tracks individuals over time, and
performs body part segmentation. The method supports point clouds from both
depth sensors and monocular depth estimation. At the core of the proposed HAR
framework is a novel backbone for 3D action recognition, which combines
point-based techniques with sparse convolutional networks applied to
voxel-mapped point cloud sequences. Experiments incorporate auxiliary point
features including surface normals, color, infrared intensity, and body part
parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D
120 dataset demonstrates that the method is competitive with existing skeletal
action recognition algorithms. Moreover, combining both sensor-based and
estimated depth inputs in an ensemble setup, this approach achieves 89.3%
accuracy when different human subjects are considered for training and testing,
outperforming previous point cloud action recognition methods.

</details>


### [12] [Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models](https://arxiv.org/abs/2510.05509)
*Shinnosuke Saito,Takashi Matsubara*

Main category: cs.CV

TL;DR: 提出了一种基于黎曼度量的噪声空间插值方法，利用得分函数的雅可比捕捉数据流形的切空间，使扩散模型在生成过程中保持在数据流形上，从而实现更自然、保真的图像插值。


<details>
  <summary>Details</summary>
Motivation: 扩散模型缺乏显式的低维潜在空间，导致传统的插值方法可能偏离数据流形，产生不自然的过渡；因此需要一种能沿数据流形进行插值的方法。

Method: 提出一种新的噪声空间上的黎曼度量，该度量基于得分函数的雅可比矩阵，用以估计局部数据流形的切空间，并引导噪声空间中的测地线沿或靠近数据流形。

Result: 在图像插值任务中，相比基于密度和简单基线方法，所提方法生成了感知更自然、更忠实于数据流形的过渡结果。

Conclusion: 通过引入基于得分函数几何结构的黎曼度量，扩散模型可在隐空间中更好地遵循数据流形，显著提升插值的视觉质量和语义一致性。

Abstract: Diffusion models are powerful deep generative models (DGMs) that generate
high-fidelity, diverse content. However, unlike classical DGMs, they lack an
explicit, tractable low-dimensional latent space that parameterizes the data
manifold. This absence limits manifold-aware analysis and operations, such as
interpolation and editing. Existing interpolation methods for diffusion models
typically follow paths through high-density regions, which are not necessarily
aligned with the data manifold and can yield perceptually unnatural
transitions. To exploit the data manifold learned by diffusion models, we
propose a novel Riemannian metric on the noise space, inspired by recent
findings that the Jacobian of the score function captures the tangent spaces to
the local data manifold. This metric encourages geodesics in the noise space to
stay within or run parallel to the learned data manifold. Experiments on image
interpolation show that our metric produces perceptually more natural and
faithful transitions than existing density-based and naive baselines.

</details>


### [13] [Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work](https://arxiv.org/abs/2510.05538)
*Owen Henkel,Bill Roberts,Doug Jaffe,Laurence Holt*

Main category: cs.CV

TL;DR: 该研究探讨了多模态大语言模型（MLLMs）在批改和分析手写学生数学作业中的应用潜力。实验A显示MLLM在判断有客观答案的算术题时接近人类水平，但在实验B中，面对需要视觉理解与教学判断的学生数学绘图任务时表现较差；当提供人工描述后，模型表现显著提升，表明当前MLLM的视觉理解能力在复杂图像上仍有限。


<details>
  <summary>Details</summary>
Motivation: 由于中小学数学作业多为手写，教师批改耗时且难以全面分析学生思维过程，因此探索MLLM是否能有效自动批改和反馈手写作业绩效具有重要意义。

Method: 进行了两项实验：实验A使用288份加纳中学生的手写算术答案评估MLLM识别正确性的能力；实验B使用150份美国小学生的数学绘图，比较MLLM直接评分与基于人工描述辅助评分的表现，并计算与真实评分的Kappa一致性。

Result: 实验A中MLLM达到95%准确率（k=0.90），接近人类水平；实验B中直接评图一致性仅为k=0.20，而在输入人工描述后提升至k=0.47，与人类间一致性相当。

Conclusion: MLLM能较好处理有明确答案的手写算术题，但在理解学生数学绘图等开放性任务上仍有局限，依赖高质量视觉输入或辅助描述才能发挥其教学分析潜力。

Abstract: Recent advances in multimodal large language models (MLLMs) raise the
question of their potential for grading, analyzing, and offering feedback on
handwritten student classwork. This capability would be particularly beneficial
in elementary and middle-school mathematics education, where most work remains
handwritten, because seeing students' full working of a problem provides
valuable insights into their learning processes, but is extremely
time-consuming to grade. We present two experiments investigating MLLM
performance on handwritten student mathematics classwork. Experiment A examines
288 handwritten responses from Ghanaian middle school students solving
arithmetic problems with objective answers. In this context, models achieved
near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human
educators would be unlikely to make. Experiment B evaluates 150 mathematical
illustrations from American elementary students, where the drawings are the
answer to the question. These tasks lack single objective answers and require
sophisticated visual interpretation as well as pedagogical judgment in order to
analyze and evaluate them. We attempted to separate MLLMs' visual capabilities
from their pedagogical abilities by first asking them to grade the student
illustrations directly, and then by augmenting the image with a detailed human
description of the illustration. We found that when the models had to analyze
the student illustrations directly, they struggled, achieving only k = 0.20
with ground truth scores, but when given human descriptions, their agreement
levels improved dramatically to k = 0.47, which was in line with human-to-human
agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic
work relatively well, but still struggle to "see" student mathematical
illustrations.

</details>


### [14] [Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics](https://arxiv.org/abs/2510.05558)
*Christopher Hoang,Mengye Ren*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督学习架构Midway Network，首次从自然视频中同时学习物体识别和运动理解的强视觉表征。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法主要关注物体识别或运动理解中的单一任务，缺乏对两者协同学习的有效方法。

Method: 通过扩展潜在动态建模，引入中层自上而下的路径来推断帧间运动潜变量，并采用密集前向预测目标和分层结构处理复杂多物体场景。

Result: 在两个大规模自然视频数据集上预训练后，Midway Network在语义分割和光流估计任务上均优于先前的自监督方法。

Conclusion: Midway Network能够有效联合学习识别与运动表征，其学习到的动态特征可通过前向特征扰动分析捕捉高层次对应关系。

Abstract: Object recognition and motion understanding are key components of perception
that complement each other. While self-supervised learning methods have shown
promise in their ability to learn from unlabeled data, they have primarily
focused on obtaining rich representations for either recognition or motion
rather than both in tandem. On the other hand, latent dynamics modeling has
been used in decision making to learn latent representations of observations
and their transformations over time for control and planning tasks. In this
work, we present Midway Network, a new self-supervised learning architecture
that is the first to learn strong visual representations for both object
recognition and motion understanding solely from natural videos, by extending
latent dynamics modeling to this domain. Midway Network leverages a midway
top-down path to infer motion latents between video frames, as well as a dense
forward prediction objective and hierarchical structure to tackle the complex,
multi-object scenes of natural videos. We demonstrate that after pretraining on
two large-scale natural video datasets, Midway Network achieves strong
performance on both semantic segmentation and optical flow tasks relative to
prior self-supervised learning methods. We also show that Midway Network's
learned dynamics can capture high-level correspondence via a novel analysis
method based on forward feature perturbation.

</details>


### [15] [HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video](https://arxiv.org/abs/2510.05560)
*Hongchi Xia,Chih-Hao Lin,Hao-Yu Hsu,Quentin Leboutet,Katelyn Gao,Michael Paulitsch,Benjamin Ummenhofer,Shenlong Wang*

Main category: cs.CV

TL;DR: HoloScene 是一个新型的交互式 3D 重建框架，通过综合场景图表示和能量优化方法，实现几何完整、物理合理且可交互的虚拟环境构建。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 重建和场景理解方法在几何完整性、对象交互性、物理合理性、渲染真实感和动态仿真性能方面存在不足，限制了其在 AR/VR、游戏和机器人等领域的应用。

Method: 提出 HoloScene 框架，采用包含几何、外观和物理属性及对象间层次关系的场景图表示；将重建建模为能量优化问题，融合观测数据、物理约束和生成先验，并通过采样探索与梯度优化相结合的混合方法高效求解。

Result: 在多个基准数据集上表现优于现有方法，生成的数字孪生体具有完整的几何结构、物理稳定性和高质量的新视角渲染效果，并在交互游戏和实时数字孪生操作中验证了实用性。

Conclusion: HoloScene 能够同时满足高精度重建、物理合理性和交互性的需求，为构建仿真就绪的虚拟环境提供了有效解决方案。

Abstract: Digitizing the physical world into accurate simulation-ready virtual
environments offers significant opportunities in a variety of fields such as
augmented and virtual reality, gaming, and robotics. However, current 3D
reconstruction and scene-understanding methods commonly fall short in one or
more critical aspects, such as geometry completeness, object interactivity,
physical plausibility, photorealistic rendering, or realistic physical
properties for reliable dynamic simulation. To address these limitations, we
introduce HoloScene, a novel interactive 3D reconstruction framework that
simultaneously achieves these requirements. HoloScene leverages a comprehensive
interactive scene-graph representation, encoding object geometry, appearance,
and physical properties alongside hierarchical and inter-object relationships.
Reconstruction is formulated as an energy-based optimization problem,
integrating observational data, physical constraints, and generative priors
into a unified, coherent objective. Optimization is efficiently performed via a
hybrid approach combining sampling-based exploration with gradient-based
refinement. The resulting digital twins exhibit complete and precise geometry,
physical stability, and realistic rendering from novel viewpoints. Evaluations
conducted on multiple benchmark datasets demonstrate superior performance,
while practical use-cases in interactive gaming and real-time digital-twin
manipulation illustrate HoloScene's broad applicability and effectiveness.
Project page: https://xiahongchi.github.io/HoloScene.

</details>


### [16] [CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval](https://arxiv.org/abs/2510.05586)
*Bin Kang,Bin Chen,Junjie Wang,Yulin Li,Junzhi Zhao,Zhuotao Tian*

Main category: cs.CV

TL;DR: 提出CalibCLIP，一种无需训练的方法，通过在视觉和文本空间中校正主导令牌的抑制效应，提升文本驱动图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型中少数低贡献令牌可能过度捕获全局语义，抑制判别性特征，影响图像检索效果。

Method: 在视觉空间提出对比视觉增强器（CVE），解耦视觉特征并动态抑制主导令牌；在文本空间引入判别概念校准器（DCC），区分通用与判别性概念以增强表示。

Result: 在七个基准上进行广泛实验，涵盖三项图像检索任务，均取得一致性能提升。

Conclusion: CalibCLIP有效缓解了主导令牌的抑制问题，显著提升了文本到图像检索的准确性与鲁棒性。

Abstract: Existing Visual Language Models (VLMs) suffer structural limitations where a
few low contribution tokens may excessively capture global semantics,
dominating the information aggregation process and suppressing the
discriminative features in text-driven image retrieval tasks. To address this,
we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate
the suppressive effect of dominant tokens. Specifically, in the visual space,
we propose the Contrastive Visual Enhancer (CVE), which decouples visual
features into target and low information regions. Subsequently, it identifies
dominant tokens and dynamically suppresses their representations.In the textual
space, we introduce the Discriminative Concept Calibrator (DCC), which aims to
differentiate between general and discriminative concepts within the text
query. By mitigating the challenges posed by generic concepts and improving the
representations of discriminative concepts, DCC strengthens the differentiation
among similar samples. Finally, extensive experiments demonstrate consistent
improvements across seven benchmarks spanning three image retrieval tasks,
underscoring the effectiveness of CalibCLIP. Code is available at:
https://github.com/kangbin98/CalibCLIP

</details>


### [17] [Improving Chain-of-Thought Efficiency for Autoregressive Image Generation](https://arxiv.org/abs/2510.05593)
*Zeqi Gu,Markos Georgopoulos,Xiaoliang Dai,Marjan Ghazvininejad,Chu Wang,Felix Juefei-Xu,Kunpeng Li,Yujun Shi,Zecheng He,Zijian He,Jiawei Zhou,Abe Davis,Jialiang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级优化框架ShortCoTI，用于生成更简洁的思维链（CoT）序列，以提高图像生成效率，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于思维链推理的多模态生成模型容易引入冗余信息（即视觉过思考），增加计算成本并可能导致与原始提示矛盾的细节。

Method: ShortCoTI通过一个自适应奖励函数，在强化学习框架中鼓励生成更简洁的提示，该函数根据每个任务的估计难度进行缩放。

Result: 在多个基准测试中，该方法将推理提示长度减少了54%，同时保持或略微提升了图像生成质量指标，并消除了冗长和重复的表达。

Conclusion: ShortCoTI在不牺牲图像保真度和视觉吸引力的前提下，显著提高了生成过程的计算效率。

Abstract: Autoregressive multimodal large language models have recently gained
popularity for image generation, driven by advances in foundation models. To
enhance alignment and detail, newer approaches employ chain-of-thought (CoT)
reasoning, expanding user inputs into elaborated prompts prior to image
synthesis. However, this strategy can introduce unnecessary redundancy -- a
phenomenon we call visual overthinking -- which increases computational costs
and can introduce details that contradict the original prompt. In this work, we
explore how to generate more concise CoT sequences for more efficient image
generation. We introduce ShortCoTI, a lightweight optimization framework that
encourages more concise CoT while preserving output image quality. ShortCoTI
rewards more concise prompts with an adaptive function that scales according to
an estimated difficulty for each task. Incorporating this reward into a
reinforcement learning paradigm reduces prompt reasoning length by 54% while
maintaining or slightly improving quality metrics across multiple benchmarks
(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates
verbose explanations and repetitive refinements, producing reasoning prompts
that are both concise and semantically rich. As a result, ShortCoTI improves
computational efficiency without compromising the fidelity or visual appeal of
generated images.

</details>


### [18] [HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection](https://arxiv.org/abs/2510.05609)
*Junwen Chen,Peilin Xiong,Keiji Yanai*

Main category: cs.CV

TL;DR: 本文提出HOI-R1，首次探索纯语言模型在人类-物体交互检测（HOID）任务中的潜力，无需额外检测模块，通过引入HOI推理过程和奖励函数，在HICO-DET数据集上实现基线两倍的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有HOID方法依赖VLM的先验知识且框架复杂，MLLM在HOID上的推理能力未被充分挖掘。

Method: 基于强化学习，设计HOI推理过程和HOID奖励函数，直接以纯文本方式完成HOID任务，不依赖传统检测模块。

Result: 在HICO-DET数据集上，HOI-R1的准确率达到基线方法的2倍，并表现出良好的泛化能力。

Conclusion: 纯语言模型结合强化学习可有效解决HOID任务，提供了一种简洁、高效的新范式。

Abstract: Recent Human-object interaction detection (HOID) methods highly require prior
knowledge from VLMs to enhance the interaction recognition capabilities. The
training strategies and model architectures for connecting the knowledge from
VLMs to the HOI instance representations from the object detector are
challenging, and the whole framework is complex for further development or
application. On the other hand, the inherent reasoning abilities of MLLMs on
human-object interaction detection are under-explored. Inspired by the recent
success of training MLLMs with reinforcement learning (RL) methods, we propose
HOI-R1 and first explore the potential of the language model on the HOID task
without any additional detection modules. We introduce an HOI reasoning process
and HOID reward functions to solve the HOID task by pure text. The results on
the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline
with great generalization ability. The source code is available at
https://github.com/cjw2021/HOI-R1.

</details>


### [19] [Efficient Conditional Generation on Scale-based Visual Autoregressive Models](https://arxiv.org/abs/2510.05610)
*Jiaqi Liu,Tao Huang,Chang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的即插即用控制框架ECM，用于自回归图像生成，通过轻量级控制模块和早期集中采样策略显著提升训练与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在复杂空间条件生成任务中依赖微调，导致训练成本高昂，缺乏高效、通用的控制方法。

Method: 提出ECM框架，包含上下文感知注意力层和共享门控前馈网络，并采用早期集中采样策略与推理阶段的温度调度机制。

Result: 在多种尺度的自回归模型上验证了ECM在生成质量、多样性及控制能力上的优越性，同时降低了训练和推理开销。

Conclusion: ECM实现了高效、高保真的条件图像生成，为自回归模型提供了更实用的控制解决方案。

Abstract: Recent advances in autoregressive (AR) models have demonstrated their
potential to rival diffusion models in image synthesis. However, for complex
spatially-conditioned generation, current AR approaches rely on fine-tuning the
pre-trained model, leading to significant training costs. In this paper, we
propose the Efficient Control Model (ECM), a plug-and-play framework featuring
a lightweight control module that introduces control signals via a distributed
architecture. This architecture consists of context-aware attention layers that
refine conditional features using real-time generated tokens, and a shared
gated feed-forward network (FFN) designed to maximize the utilization of its
limited capacity and ensure coherent control feature learning. Furthermore,
recognizing the critical role of early-stage generation in determining semantic
structure, we introduce an early-centric sampling strategy that prioritizes
learning early control sequences. This approach reduces computational cost by
lowering the number of training tokens per iteration, while a complementary
temperature scheduling during inference compensates for the resulting
insufficient training of late-stage tokens. Extensive experiments on
scale-based AR models validate that our method achieves high-fidelity and
diverse control over image generation, surpassing existing baselines while
significantly improving both training and inference efficiency.

</details>


### [20] [PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction](https://arxiv.org/abs/2510.05613)
*Ziqiao Meng,Qichao Wang,Zhiyang Dou,Zixing Song,Zhipeng Zhou,Irwin King,Peilin Zhao*

Main category: cs.CV

TL;DR: 提出PointNSP，一种基于粗到细生成框架的自回归点云生成方法，首次在质量上达到并超越扩散模型，且在效率和可扩展性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 自回归模型因强制无序点集产生人为顺序，导致难以捕捉长程依赖和全局结构特性，从而在生成质量上落后于扩散模型。

Method: 受形状建模中细节层次（LOD）原则启发，采用多尺度分解策略，通过低分辨率保持全局结构，并逐级预测下一尺度细节，实现下一层级点云的生成。

Result: 在ShapeNet上实现了自回归方法中的最先进生成质量，优于强扩散基线，在参数量、训练和推理效率方面更优；在8192点密集生成中优势更明显。

Conclusion: PointNSP有效缓解了自回归模型的序列偏差问题，通过多尺度建模提升了全局结构保持能力，为高效高质量点云生成提供了新方向。

Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based
approaches in quality. The performance gap stems from the fact that
autoregressive models impose an artificial ordering on inherently unordered
point sets, forcing shape generation to proceed as a sequence of local
predictions. This sequential bias emphasizes short-range continuity but
undermines the model's capacity to capture long-range dependencies, hindering
its ability to enforce global structural properties such as symmetry,
consistent topology, and large-scale geometric regularities. Inspired by the
level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a
coarse-to-fine generative framework that preserves global shape structure at
low resolutions and progressively refines fine-grained geometry at higher
scales through a next-scale prediction paradigm. This multi-scale factorization
aligns the autoregressive objective with the permutation-invariant nature of
point sets, enabling rich intra-scale interactions while avoiding brittle fixed
orderings. Experiments on ShapeNet show that PointNSP establishes
state-of-the-art (SOTA) generation quality for the first time within the
autoregressive paradigm. In addition, it surpasses strong diffusion-based
baselines in parameter, training, and inference efficiency. Finally, in dense
generation with 8,192 points, PointNSP's advantages become even more
pronounced, underscoring its scalability potential.

</details>


### [21] [TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation](https://arxiv.org/abs/2510.05615)
*Guangrong Wan,Jun liu,Tang tang,Lianghao Shi,Wenjun Luo,TingTing Xu*

Main category: cs.CV

TL;DR: 本文提出了首个用于多任务泪膜分析的Tear Film Multi-task (TFM) 数据集，并设计了TF-Net模型和TF-Collab集成 pipeline，实现了泪膜破裂的自动化实时分析。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注数据集和一体化解决方案，泪膜破裂（TFBU）的自动分割仍具挑战性。本文旨在推动干眼症诊断中的自动化泪膜分析研究。

Method: 提出TFM数据集，包含15个高分辨率视频（6,247帧），标注了帧级分类、Placido环检测和TFBU区域分割三个任务；设计基于MobileOne-mini和增强特征金字塔网络的轻量分割模型TF-Net；并构建整合三任务模型的实时分析流程TF-Collab。

Result: TF-Net在准确性和计算效率之间取得良好平衡，适用于实时临床应用；TF-Collab通过协同利用三任务模型，实现了从输入标准化到BUT测定和TFBU分割的全自动化分析流程。

Conclusion: TF-Net与TF-Collab的有效性验证了所提方法在泪膜分析中的潜力，为眼部表面诊断的后续研究提供了基础和开源资源。

Abstract: Tear film break-up (TFBU) analysis is critical for diagnosing dry eye
syndrome, but automated TFBU segmentation remains challenging due to the lack
of annotated datasets and integrated solutions. This paper introduces the Tear
Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task
tear film analysis, comprising 15 high-resolution videos (totaling 6,247
frames) annotated with three vision tasks: frame-level classification ('clear',
'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area
segmentation. Leveraging this dataset, we first propose TF-Net, a novel and
efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini
backbone with re-parameterization techniques and an enhanced feature pyramid
network to achieve a favorable balance between accuracy and computational
efficiency for real-time clinical applications. We further establish benchmark
performance on the TFM segmentation subset by comparing TF-Net against several
state-of-the-art medical image segmentation models. Furthermore, we design
TF-Collab, a novel integrated real-time pipeline that synergistically leverages
models trained on all three tasks of the TFM dataset. By sequentially
orchestrating frame classification for BUT determination, pupil region
localization for input standardization, and TFBU segmentation, TF-Collab fully
automates the analysis. Experimental results demonstrate the effectiveness of
the proposed TF-Net and TF-Collab, providing a foundation for future research
in ocular surface diagnostics. Our code and the TFM datasets are available at
https://github.com/glory-wan/TF-Net

</details>


### [22] [InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment](https://arxiv.org/abs/2510.05617)
*Ibrahim Salihu Yusuf,Iffanice Houndayi,Rym Oualha,Mohamed Aziz Cherif,Kobby Panford-Quainoo,Arnu Pretorius*

Main category: cs.CV

TL;DR: InstaGeo是一个开源端到端框架，通过自动化数据处理、任务特定模型蒸馏和无缝部署，将地理空间基础模型转化为实用、低碳的实时地球观测工具。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基础模型缺乏自动化数据管道且微调后模型过大，限制了其在人道主义和环境应用中的部署。

Method: 提出InstaGeo框架，集成自动化数据整理、任务特定模型蒸馏和交互式Web地图部署，实现从原始影像到模型部署的全流程自动化。

Result: 在多个任务上复现了已有研究结果，蒸馏模型体积缩小达8倍，计算量和碳排放显著降低，精度损失极小；构建更大作物分割数据集，达到60.65% mIoU，比先前基线提升12个百分点。

Conclusion: InstaGeo实现了地理空间AI向数据质量和应用驱动创新的转变，支持快速、低资源消耗的模型开发与部署。

Abstract: Open-access multispectral imagery from missions like Landsat 8-9 and
Sentinel-2 has fueled the development of geospatial foundation models (GFMs)
for humanitarian and environmental applications. Yet, their deployment remains
limited by (i) the absence of automated geospatial data pipelines and (ii) the
large size of fine-tuned models. Existing GFMs lack workflows for processing
raw satellite imagery, and downstream adaptations often retain the full
complexity of the original encoder.
  We present InstaGeo, an open-source, end-to-end framework that addresses
these challenges by integrating: (1) automated data curation to transform raw
imagery into model-ready datasets; (2) task-specific model distillation to
derive compact, compute-efficient models; and (3) seamless deployment as
interactive web-map applications. Using InstaGeo, we reproduced datasets from
three published studies and trained models with marginal mIoU differences of
-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for
desert locust prediction. The distilled models are up to 8x smaller than
standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal
accuracy loss.
  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger
crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp
improvement over prior baselines. Moreover, InstaGeo enables users to progress
from raw data to model deployment within a single working day.
  By unifying data preparation, model compression, and deployment, InstaGeo
transforms research-grade GFMs into practical, low-carbon tools for real-time,
large-scale Earth observation. This approach shifts geospatial AI toward data
quality and application-driven innovation. Source code, datasets, and model
checkpoints are available at:
https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git

</details>


### [23] [Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection](https://arxiv.org/abs/2510.05633)
*Sara Mandelli,Diego Vila-Portela,David Vázquez-Padín,Paolo Bestagini,Fernando Pérez-González*

Main category: cs.CV

TL;DR: 本文研究了最先进的深度学习图像检测器是否真正依赖频域中的周期性峰值来识别生成图像，提出了一种去除频谱峰值的方法，并设计了一个仅依赖这些峰值的线性检测器作为可解释的基线。结果表明，大多数现有检测器并不根本依赖这些峰值，挑战了领域内的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 尽管频域中的周期性峰值被认为是生成图像的重要标志，但当前检测器多为黑箱模型，其是否真正依赖这些特征尚不清楚，限制了模型的可解释性和可信度。

Method: 提出一种去除图像频谱峰值的策略，通过消融实验分析多个检测器在去除峰值后的表现变化；同时构建一个仅基于频域峰值的线性检测器作为可解释的基准模型。

Result: 实验发现大多数先进的检测器在去除频谱峰值后性能并未显著下降，说明它们并非主要依赖这些特征；而提出的线性检测器能有效利用峰值进行检测，验证了峰值本身确实具有判别能力。

Conclusion: 当前多数深度学习图像伪造检测器并不根本依赖频域峰值，该发现挑战了现有共识，强调需要重新审视检测器的工作机制，并推动更透明、可靠的取证工具发展。

Abstract: Over the years, the forensics community has proposed several deep
learning-based detectors to mitigate the risks of generative AI. Recently,
frequency-domain artifacts (particularly periodic peaks in the magnitude
spectrum), have received significant attention, as they have been often
considered a strong indicator of synthetic image generation. However,
state-of-the-art detectors are typically used as black-boxes, and it still
remains unclear whether they truly rely on these peaks. This limits their
interpretability and trust. In this work, we conduct a systematic study to
address this question. We propose a strategy to remove spectral peaks from
images and analyze the impact of this operation on several detectors. In
addition, we introduce a simple linear detector that relies exclusively on
frequency peaks, providing a fully interpretable baseline free from the
confounding influence of deep learning. Our findings reveal that most detectors
are not fundamentally dependent on spectral peaks, challenging a widespread
assumption in the field and paving the way for more transparent and reliable
forensic tools.

</details>


### [24] [Combined Hyperbolic and Euclidean Soft Triple Loss Beyond the Single Space Deep Metric Learning](https://arxiv.org/abs/2510.05643)
*Shozo Saeki,Minoru Kawahara,Hirohisa Aman*

Main category: cs.CV

TL;DR: 本文提出了一种结合双曲空间和欧氏空间的代理损失方法（CHEST loss），用于提升深度度量学习的性能，在多个基准数据集上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的双曲空间深度度量学习多基于成对损失或无监督正则化，缺乏适用于大规模数据的高效代理损失方法，且直接应用代理损失在双曲空间中存在技术难题。

Method: 提出CHEST损失函数，结合双曲空间与欧氏空间的代理损失，并引入基于双曲层次聚类的正则化项，实现更稳定的联合优化。

Result: 在四个基准数据集上验证了CHEST损失的有效性，显著提升了深度度量学习的准确性和训练稳定性，达到了新的SOTA性能。

Conclusion: 结合双曲与欧氏空间的代理损失是一种有效且稳定的深度度量学习方法，为未来在复杂结构表示中的应用提供了新方向。

Abstract: Deep metric learning (DML) aims to learn a neural network mapping data to an
embedding space, which can represent semantic similarity between data points.
Hyperbolic space is attractive for DML since it can represent richer
structures, such as tree structures. DML in hyperbolic space is based on
pair-based loss or unsupervised regularization loss. On the other hand,
supervised proxy-based losses in hyperbolic space have not been reported yet
due to some issues in applying proxy-based losses in a hyperbolic space.
However, proxy-based losses are attractive for large-scale datasets since they
have less training complexity. To address these, this paper proposes the
Combined Hyperbolic and Euclidean Soft Triple (CHEST) loss. CHEST loss is
composed of the proxy-based losses in hyperbolic and Euclidean spaces and the
regularization loss based on hyperbolic hierarchical clustering. We find that
the combination of hyperbolic and Euclidean spaces improves DML accuracy and
learning stability for both spaces. Finally, we evaluate the CHEST loss on four
benchmark datasets, achieving a new state-of-the-art performance.

</details>


### [25] [Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation](https://arxiv.org/abs/2510.05649)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 本研究提出两种深度学习框架（AHP-CADNet和基于课程学习的填补框架）用于自动化诊断眼性异常头位并处理缺失临床数据，在PoseGaze-AHP数据集上表现出高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前眼性异常头位的临床评估主观性强，且常受限于不完整的医疗记录，导致早期诊断困难，影响治疗效果。

Method: 提出AHP-CADNet多级注意力融合框架，结合眼部关键点、头部姿态特征和结构化临床属性进行可解释预测；同时设计基于课程学习的数据填补框架，利用结构化变量和非结构化临床文本逐步填补缺失数据。

Result: 在PoseGaze-AHP数据集上，AHP-CADNet分类准确率达96.9%-99.0%，连续变量预测误差低（MAE: 0.103-0.199，R² > 0.93）；填补框架对各项临床变量的准确率为93.46%-99.78%，且临床依赖建模显著提升填补效果（p < 0.001）。

Conclusion: 所提出的两个框架能有效实现眼性异常头位的自动化诊断，并在真实临床数据缺失情况下保持高鲁棒性，具有实际应用潜力。

Abstract: Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that
arises from ocular misalignment conditions, such as strabismus, enabling
patients to reduce diplopia and preserve binocular vision. Early diagnosis
minimizes morbidity and secondary complications such as facial asymmetry;
however, current clinical assessments remain largely subjective and are further
complicated by incomplete medical records. This study addresses both challenges
through two complementary deep learning frameworks. First, AHP-CADNet is a
multi-level attention fusion framework for automated diagnosis that integrates
ocular landmarks, head pose features, and structured clinical attributes to
generate interpretable predictions. Second, a curriculum learning-based
imputation framework is designed to mitigate missing data by progressively
leveraging structured variables and unstructured clinical notes to enhance
diagnostic robustness under realistic data conditions. Evaluation on the
PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet
achieves 96.9-99.0 percent accuracy across classification tasks and low
prediction errors for continuous variables, with MAE ranging from 0.103 to
0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy
across all clinical variables (93.46-99.78 percent with PubMedBERT), with
clinical dependency modeling yielding significant improvements (p < 0.001).
These findings confirm the effectiveness of both frameworks for automated
diagnosis and recovery from missing data in clinical settings.

</details>


### [26] [EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario](https://arxiv.org/abs/2510.05650)
*Yiping Ma,Shiyu Hu,Buyuan Zhu,Yipei Wang,Yaxuan Kang,Shiqing Liu,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduVerse是一个用户可定义的多智能体模拟空间，支持环境、智能体和会话的定制，并通过人机协同接口实现真实用户参与，基于CIE架构再现课堂认知、互动与长期演化动态。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI方法多局限于短期或单智能体场景，难以系统研究真实课堂中复杂的开放认知、社会互动、情感因素和长期发展过程，因此需要一个能综合建模这些要素的可复现平台。

Method: 提出EduVerse，基于分层CIE（认知-互动-演化）架构构建多智能体虚拟课堂，支持环境、智能体和会话的灵活配置，并设计人机协同接口允许真实用户加入；在中学语文课堂中跨文本类型、环境和多会话进行验证。

Result: 实验结果显示：(1) 教学对齐性高，模拟课堂IRF比率（0.28-0.64）接近真实课堂（0.37-0.49）；(2) 群体互动显著且角色分化明显，网络密度为0.27-0.40，约三分之一同伴连接被激活；(3) 跨会话行为、情绪与认知演变可追踪，正向转移率R+平均提升11.7%，揭示结构化学习轨迹。

Conclusion: EduVerse在真实性、可复现性和可解释性之间取得平衡，提供了一个可扩展的教育AI研究平台，未来将开源以促进跨学科研究。

Abstract: Reproducing cognitive development, group interaction, and long-term evolution
in virtual classrooms remains a core challenge for educational AI, as real
classrooms integrate open-ended cognition, dynamic social interaction,
affective factors, and multi-session development rarely captured together.
Existing approaches mostly focus on short-term or single-agent settings,
limiting systematic study of classroom complexity and cross-task reuse. We
present EduVerse, the first user-defined multi-agent simulation space that
supports environment, agent, and session customization. A distinctive
human-in-the-loop interface further allows real users to join the space. Built
on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse
ensures individual consistency, authentic interaction, and longitudinal
adaptation in cognition, emotion, and behavior-reproducing realistic classroom
dynamics with seamless human-agent integration. We validate EduVerse in
middle-school Chinese classes across three text genres, environments, and
multiple sessions. Results show: (1) Instructional alignment: simulated IRF
rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating
pedagogical realism; (2) Group interaction and role differentiation: network
density (0.27-0.40) with about one-third of peer links realized, while
human-agent tasks indicate a balance between individual variability and
instructional stability; (3) Cross-session evolution: the positive transition
rate R+ increase by 11.7% on average, capturing longitudinal shifts in
behavior, emotion, and cognition and revealing structured learning
trajectories. Overall, EduVerse balances realism, reproducibility, and
interpretability, providing a scalable platform for educational AI. The system
will be open-sourced to foster cross-disciplinary research.

</details>


### [27] [SD-MVSum: Script-Driven Multimodal Video Summarization Method and Datasets](https://arxiv.org/abs/2510.05652)
*Manolis Mylonas,Charalampia Zerva,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出SD-MVSum方法，结合视频视觉与语音内容，利用加权跨模态注意力机制实现基于用户脚本的多模态视频摘要。


<details>
  <summary>Details</summary>
Motivation: 扩展现有脚本驱动的视频摘要方法，使其不仅考虑视频视觉内容，还融合语音转录内容以提升相关性。

Method: 提出SD-MVSum，采用新的加权跨模态注意力机制建模脚本-视频和脚本-转录之间的依赖关系，并利用语义相似性突出最相关的视频片段。同时扩展了两个大规模数据集（S-VideoXum, MrHiSum）用于多模态脚本驱动摘要任务。

Result: 实验表明SD-MVSum在脚本驱动和通用视频摘要任务上优于或媲美现有SOTA方法。扩展后的数据集和代码已公开。

Conclusion: 所提方法有效融合多模态信息提升脚本驱动视频摘要性能，扩展的数据集为后续研究提供了支持。

Abstract: In this work, we extend a recent method for script-driven video
summarization, originally considering just the visual content of the video, to
take into account the relevance of the user-provided script also with the
video's spoken content. In the proposed method, SD-MVSum, the dependence
between each considered pair of data modalities, i.e., script-video and
script-transcript, is modeled using a new weighted cross-modal attention
mechanism. This explicitly exploits the semantic similarity between the paired
modalities in order to promote the parts of the full-length video with the
highest relevance to the user-provided script. Furthermore, we extend two
large-scale datasets for video summarization (S-VideoXum, MrHiSum), to make
them suitable for training and evaluation of script-driven multimodal video
summarization methods. Experimental comparisons document the competitiveness of
our SD-MVSum method against other SOTA approaches for script-driven and generic
video summarization. Our new method and extended datasets are available at:
https://github.com/IDT-ITI/SD-MVSum.

</details>


### [28] [A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer](https://arxiv.org/abs/2510.05657)
*Anwen Lu,Mingxin Liu,Yiping Jiao,Hongyi Gong,Geyang Xu,Jun Chen,Jun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ARGUS的新方法，用于提升肝癌组织学亚型分类的性能，通过捕捉肿瘤微环境中的宏观-中观-微观层次信息，在公共和私有数据集上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的肝癌组织学亚型分类方法未能充分挖掘全切片图像（WSI）中蕴含的多层次特征信息，导致对组织表征的理解有限且分类性能不佳。

Method: 提出ARGUS模型：首先构建微几何特征以刻画细胞核间的细粒度结构；设计多层次视野（FoVs）对齐模块来建模WSI中的宏观与中观层次交互；最后通过几何先验引导的融合策略将多尺度特征融合为统一表征。

Result: 在多个公开和私有队列上的实验表明，ARGUS在肝癌组织学亚型分类任务中达到了当前最先进水平（SOTA），显著优于现有方法。

Conclusion: ARGUS能够有效整合多尺度、多层级的病理图像特征，为原发性肝恶性肿瘤的精准诊断提供了有力工具。

Abstract: Primary liver malignancies are widely recognized as the most heterogeneous
and prognostically diverse cancers of the digestive system. Among these,
hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge
as the two principal histological subtypes, demonstrating significantly greater
complexity in tissue morphology and cellular architecture than other common
tumors. The intricate representation of features in Whole Slide Images (WSIs)
encompasses abundant crucial information for liver cancer histological
subtyping, regarding hierarchical pyramid structure, tumor microenvironment
(TME), and geometric representation. However, recent approaches have not
adequately exploited these indispensable effective descriptors, resulting in a
limited understanding of histological representation and suboptimal subtyping
performance. To mitigate these limitations, ARGUS is proposed to advance
histological subtyping in liver cancer by capturing the macro-meso-micro
hierarchical information within the TME. Specifically, we first construct a
micro-geometry feature to represent fine-grained cell-level pattern via a
geometric structure across nuclei, thereby providing a more refined and precise
perspective for delineating pathological images. Then, a Hierarchical
Field-of-Views (FoVs) Alignment module is designed to model macro- and
meso-level hierarchical interactions inherent in WSIs. Finally, the augmented
micro-geometry and FoVs features are fused into a joint representation via
present Geometry Prior Guided Fusion strategy for modeling holistic phenotype
interactions. Extensive experiments on public and private cohorts demonstrate
that our ARGUS achieves state-of-the-art (SOTA) performance in histological
subtyping of liver cancer, which provide an effective diagnostic tool for
primary liver malignancies in clinical practice.

</details>


### [29] [Teleportraits: Training-Free People Insertion into Any Scene](https://arxiv.org/abs/2510.05660)
*Jialu Gao,K J Joseph,Fernando De La Torre*

Main category: cs.CV

TL;DR: 提出了一种无需训练的统一框架，利用预训练的文本到图像扩散模型实现将人从参考图像自然插入到背景场景中，结合反转技术和分类器自由引导，并通过掩码引导的自注意力机制实现高质量个性化，保持人物身份、服装和身体特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将人体位置姿态确定与基于背景的个性化处理作为独立问题，忽略了二者之间的关联性，且多依赖训练才能获得良好效果，因此需要一种能够联合处理这两个问题且无需训练的解决方案。

Method: 利用预训练的文本到图像扩散模型，结合图像反转技术与无分类器引导（classifier-free guidance），并通过提出的掩码引导自注意力机制，在不进行额外训练的情况下实现对人体插入位置的合理判断及高保真个性化合成。

Result: 在多种复合场景图像上实现了最先进的真实感人体插入效果，无需任何训练即可完成，并在主体与背景的融合质量以及身份保持方面优于现有方法。

Conclusion: 本文首次实现了完全无需训练的真实感人像插入方法，验证了扩散模型本身具备在复杂场景中合理放置人物的能力，为图像编辑任务提供了一种高效、灵活的新范式。

Abstract: The task of realistically inserting a human from a reference image into a
background scene is highly challenging, requiring the model to (1) determine
the correct location and poses of the person and (2) perform high-quality
personalization conditioned on the background. Previous approaches often treat
them as separate problems, overlooking their interconnections, and typically
rely on training to achieve high performance. In this work, we introduce a
unified training-free pipeline that leverages pre-trained text-to-image
diffusion models. We show that diffusion models inherently possess the
knowledge to place people in complex scenes without requiring task-specific
training. By combining inversion techniques with classifier-free guidance, our
method achieves affordance-aware global editing, seamlessly inserting people
into scenes. Furthermore, our proposed mask-guided self-attention mechanism
ensures high-quality personalization, preserving the subject's identity,
clothing, and body features from just a single reference image. To the best of
our knowledge, we are the first to perform realistic human insertions into
scenes in a training-free manner and achieve state-of-the-art results in
diverse composite scene images with excellent identity preservation in
backgrounds and subjects.

</details>


### [30] [When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach](https://arxiv.org/abs/2510.05661)
*Daniel Gonzálbez-Biosca,Josep Cabacas-Maso,Carles Ventura,Ismael Benito-Altamirano*

Main category: cs.CV

TL;DR: 本文提出了一种用于多摄像头古典音乐会视频自动编辑的新型多模态架构，分别解决“何时剪”和“如何剪”的问题，在检测剪辑点和视觉镜头选择方面优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 自动化视频编辑在计算机视觉和多媒体领域研究较少，尤其是在视频生成和场景理解快速发展的背景下，因此需要探索针对特定场景（如音乐会）的有效编辑方法。

Method: 将问题分解为时间分割（何时剪）和空间选择（如何剪）两个子任务；前者采用结合音频log-mel谱图、可选图像嵌入和时序特征的轻量级卷积-Transformer架构，后者使用CLIP编码器替代传统ResNet并限制干扰镜头来自同一场音乐会。

Result: 模型在检测剪辑点上优于已有基线方法，并实现了具有竞争力的视觉镜头选择效果。

Conclusion: 所提方法在多模态自动化视频编辑任务中取得了当前最优性能，推动了该领域的进展。

Abstract: Automated video editing remains an underexplored task in the computer vision
and multimedia domains, especially when contrasted with the growing interest in
video generation and scene understanding. In this work, we address the specific
challenge of editing multicamera recordings of classical music concerts by
decomposing the problem into two key sub-tasks: when to cut and how to cut.
Building on recent literature, we propose a novel multimodal architecture for
the temporal segmentation task (when to cut), which integrates log-mel
spectrograms from the audio signals, plus an optional image embedding, and
scalar temporal features through a lightweight convolutional-transformer
pipeline. For the spatial selection task (how to cut), we improve the
literature by updating from old backbones, e.g. ResNet, with a CLIP-based
encoder and constraining distractor selection to segments from the same
concert. Our dataset was constructed following a pseudo-labeling approach, in
which raw video data was automatically clustered into coherent shot segments.
We show that our models outperformed previous baselines in detecting cut points
and provide competitive visual shot selection, advancing the state of the art
in multimodal automated video editing.

</details>


### [31] [Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis](https://arxiv.org/abs/2510.05668)
*M. Torrente,A. Follador,A. Calcante,P. Casati,R. Oberti*

Main category: cs.CV

TL;DR: 本研究开发了一种结合低成本成像系统与创新时间序列图像分析算法的方法，用于监测R. solani感染对生菜种子萌发和早期生长的影响，实现了在复杂重叠条件下对幼苗的准确识别与活力评估。


<details>
  <summary>Details</summary>
Motivation: 传统图像分析方法在幼苗重叠或密集生长时难以准确分割和计数，限制了植物表型分析的精度，因此需要一种更鲁棒的方法来非破坏性地量化病原体对种子萌发和早期发育的影响。

Method: 利用多摄像头系统连续采集接种R. solani与对照组生菜种子的萌发图像，提出一种融合形态学与空间特征并引入时间序列信息的图像分析流程，通过整合前期发育状态提升对重叠幼苗的识别能力。

Result: 该方法在幼苗计数和活力评估中表现出高精度（R²=0.98，RMSE=1.12），显著优于传统分割技术，尤其在实验后期仍能准确分辨交织生长的幼苗；结果显示R. solani显著降低种子萌发率和幼苗活力。

Conclusion: 结合低成本成像与时间感知图像分析可实现高通量、非破坏性的植物表型监测，为研究病原体影响及推广自动化表型技术提供了有效解决方案。

Abstract: The study investigates the effects of R. solani inoculation on the
germination and early development of Lactuca sativa L. seeds using a low-cost,
image-based monitoring system. Multiple cameras were deployed to continuously
capture images of the germination process in both infected and control groups.
The objective was to assess the impact of the pathogen by analyzing germination
dynamics and growth over time. To achieve this, a novel image analysis pipeline
was developed. The algorithm integrates both morphological and spatial features
to identify and quantify individual seedlings, even under complex conditions
where traditional image analyses fails. A key innovation of the method lies in
its temporal integration: each analysis step considers not only the current
status but also their developmental across prior time points. This approach
enables robust discrimination of individual seedlings, especially when
overlapping leaves significantly hinder object separation. The method
demonstrated high accuracy in seedling counting and vigor assessment, even in
challenging scenarios characterized by dense and intertwined growth. Results
confirm that R. solani infection significantly reduces germination rates and
early seedling vigor. The study also validates the feasibility of combining
low-cost imaging hardware with advanced computational tools to obtain
phenotyping data in a non-destructive and scalable manner. The temporal
integration enabled accurate quantification of germinated seeds and precise
determination of seedling emergence timing. This approach proved particularly
effective in later stages of the experiment, where conventional segmentation
techniques failed due to overlapping or intertwined seedlings, making accurate
counting. The method achieved a coefficient of determination of 0.98 and a root
mean square error (RMSE) of 1.12, demonstrating its robustness and reliability.

</details>


### [32] [Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension](https://arxiv.org/abs/2510.05674)
*Jike Zhong,Yuxiang Lai,Xiaofeng Yang,Konstantinos Psounis*

Main category: cs.CV

TL;DR: 本文提出一种基于对象级建模的语义视觉分词方法，通过在掩码图像建模中对视觉对象而非随机块进行掩码，提升视觉模型的语义与上下文理解能力，显著增强其在视觉问答等任务中的推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型在推理和上下文学习方面落后于语言模型，主要因缺乏语义和上下文引导；本文旨在通过设计语义锚定的目标来缩小这一差距。

Method: 将视觉中的“对象”视为语言中“词”的对应，提出在掩码图像建模（MIM）框架下对完整视觉对象进行掩码，以学习全局上下文和语义关系。

Result: 实验证明，对象级表征有助于学习真实世界分布，避免像素平均的捷径；结合多模态大模型，在VQA、GQA、ScienceQA等任务上展现出更强的推理与上下文理解能力。

Conclusion: 对象级编码能有效提升视觉模型的语义学习与推理能力，为构建更强的视觉编码器和分词器提供了可行方向。

Abstract: Recent advances in language modeling have witnessed the rise of highly
desirable emergent capabilities, such as reasoning and in-context learning.
However, vision models have yet to exhibit comparable progress in these areas.
In this paper, we argue that this gap could stem from the lack of semantic and
contextual guidance in current vision transformer (ViT) training schemes, and
such a gap can be narrowed through the design of a semantic-grounded objective.
Specifically, we notice that individual words in natural language are
inherently semantic, and modeling directly on word tokens naturally learns a
realistic distribution. In contrast, ViTs rely on spatial patchification, which
inevitably lacks semantic information. To bridge this gap, we propose to
directly model "object" as the visual equivalence of "word," pushing the model
to learn the global context and semantics among visual elements. We investigate
our hypotheses via masked image modeling (MIM), a framework where our approach
can be readily tested by applying masks to visual objects rather than random
patches. Considerable evidence from qualitative and quantitative evaluations
reveals a key finding: object-level representation alone helps to learn a
real-world distribution, whereas pixel-averaging shortcuts are often learned
without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual
question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning
and contextual understanding gained with this simple objective. We hope our
study highlights the effectiveness of object-level encoding and provides a
plausible direction for developing stronger vision encoders and tokenizers.
Code and model will be publicly released. Keywords: Semantic Visual Tokenizer,
Vision Reasoning, In-context Learning, Multimodal Reasoning

</details>


### [33] [AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models](https://arxiv.org/abs/2510.05715)
*Shihao Zhu,Bohan Cao,Ziheng Ouyang,Zhen Li,Peng-Tao Jiang,Qibin Hou*

Main category: cs.CV

TL;DR: 提出AgeBooth，一种无需昂贵跨年龄数据集的年龄特定微调方法，通过年龄条件提示混合和年龄特定LoRA融合策略，实现基于单张参考图像生成身份一致且年龄可控的高质量人脸图像。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在保持身份一致性的同时难以精确控制年龄，且微调通常依赖成对的跨年龄图像，成本高昂。

Method: 提出AgeBooth，结合年龄条件提示混合和基于SVDMix的年龄特定LoRA融合策略，在adapter-based身份个性化模型上进行微调，利用老化的线性特性减少对大量标注年龄数据的依赖。

Result: 实验表明，AgeBooth在年龄控制精度和视觉质量方面优于现有的编辑方法，能从单张参考图像生成高质量、身份一致的跨年龄人脸图像。

Conclusion: AgeBooth有效提升了扩散模型在无需配对跨年龄数据情况下的年龄控制能力，为身份保持的人脸老化/年轻化生成提供了高效解决方案。

Abstract: Recent diffusion model research focuses on generating identity-consistent
images from a reference photo, but they struggle to accurately control age
while preserving identity, and fine-tuning such models often requires costly
paired images across ages. In this paper, we propose AgeBooth, a novel
age-specific finetuning approach that can effectively enhance the age control
capability of adapterbased identity personalization models without the need for
expensive age-varied datasets. To reduce dependence on a large amount of
age-labeled data, we exploit the linear nature of aging by introducing
age-conditioned prompt blending and an age-specific LoRA fusion strategy that
leverages SVDMix, a matrix fusion technique. These techniques enable
high-quality generation of intermediate-age portraits. Our AgeBooth produces
realistic and identity-consistent face images across different ages from a
single reference image. Experiments show that AgeBooth achieves superior age
control and visual quality compared to previous state-of-the-art editing-based
methods.

</details>


### [34] [Dropping the D: RGB-D SLAM Without the Depth Sensor](https://arxiv.org/abs/2510.06216)
*Mert Kiray,Alican Karaomer,Benjamin Busam*

Main category: cs.CV

TL;DR: DropD-SLAM 是一种实时单目SLAM系统，通过三个预训练视觉模块替代深度传感器，实现与RGB-D系统相当的精度。


<details>
  <summary>Details</summary>
Motivation: 在不依赖主动深度传感器的情况下，实现高精度、实时的度量级SLAM，降低系统成本和复杂性。

Method: 使用预训练的单目深度估计器、关键点检测器和实例分割网络，通过膨胀实例掩码抑制动态物体，并将静态关键点与预测深度结合生成三维特征，输入标准RGB-D SLAM后端进行跟踪与建图。

Result: 在TUM RGB-D数据集上，静态序列平均ATE为7.4 cm，动态序列为1.8 cm，性能达到或超过现有RGB-D方法，且在单GPU上运行速度达22 FPS。

Conclusion: 现代预训练视觉模型可有效替代主动深度传感器，提供可靠的度量尺度，推动更简单、低成本SLAM系统的发展。

Abstract: We present DropD-SLAM, a real-time monocular SLAM system that achieves
RGB-D-level accuracy without relying on depth sensors. The system replaces
active depth input with three pretrained vision modules: a monocular metric
depth estimator, a learned keypoint detector, and an instance segmentation
network. Dynamic objects are suppressed using dilated instance masks, while
static keypoints are assigned predicted depth values and backprojected into 3D
to form metrically scaled features. These are processed by an unmodified RGB-D
SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM
attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,
matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS
on a single GPU. These results suggest that modern pretrained vision models can
replace active depth sensors as reliable, real-time sources of metric scale,
marking a step toward simpler and more cost-effective SLAM systems.

</details>


### [35] [Data Factory with Minimal Human Effort Using VLMs](https://arxiv.org/abs/2510.05722)
*Jiaojiao Ye,Jiaxing Zhong,Qian Xie,Yuzhou Zhou,Niki Trigoni,Andrew Markham*

Main category: cs.CV

TL;DR: 提出一种无需训练的管道，结合预训练的ControlNet和视觉-语言模型生成带像素级标签的合成图像，用于语义分割的数据增强。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法在处理高层语义属性（如材质和纹理）时存在困难，而现有基于扩散模型的方法计算成本高或性能不足。

Method: 利用预训练的ControlNet和视觉-语言模型（VLMs），集成多路提示生成器、掩码生成器和高质量图像选择模块，实现无需训练的数据增强。

Result: 在PASCAL-5i和COCO-20i上的一次性语义分割任务中表现出色，优于当前方法。

Conclusion: 该方法能有效提升下游任务性能，无需人工标注，同时保证生成图像的保真度与多样性。

Abstract: Generating enough and diverse data through augmentation offers an efficient
solution to the time-consuming and labour-intensive process of collecting and
annotating pixel-wise images. Traditional data augmentation techniques often
face challenges in manipulating high-level semantic attributes, such as
materials and textures. In contrast, diffusion models offer a robust
alternative, by effectively utilizing text-to-image or image-to-image
transformation. However, existing diffusion-based methods are either
computationally expensive or compromise on performance. To address this issue,
we introduce a novel training-free pipeline that integrates pretrained
ControlNet and Vision-Language Models (VLMs) to generate synthetic images
paired with pixel-level labels. This approach eliminates the need for manual
annotations and significantly improves downstream tasks. To improve the
fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and
High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i
present promising performance and outperform concurrent work for one-shot
semantic segmentation.

</details>


### [36] [Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect](https://arxiv.org/abs/2510.05740)
*Amirtaha Amanzadi,Zahra Dehghanian,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 本文提出了OmniGen基准和FusionDetect方法，用于提升生成图像检测在跨生成器和跨视觉域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注跨生成器的泛化，忽视了跨视觉域的挑战，需构建更真实的评估环境。

Method: 提出FusionDetect方法，融合CLIP和Dinov2两种冻结基础模型的特征，构建能自适应生成器内容和设计变化的统一特征空间。

Result: FusionDetect在现有基准上比最佳方法准确率提高3.87%，平均精度提升6.13%，在OmniGen上准确率提升4.48%，且对常见图像扰动具有强鲁棒性。

Conclusion: FusionDetect结合OmniGen基准为通用AI图像检测提供了新的高性能检测器、评估标准和研究框架。

Abstract: The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect

</details>


### [37] [ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving](https://arxiv.org/abs/2510.05752)
*Yongxuan Lyu,Guangfeng Jiang,Hongsi Liu,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ALISE的新框架，完全无需人工标注即可实现LiDAR实例分割。该方法利用视觉基础模型结合文本和图像生成初始伪标签，并通过时空投票模块优化标签质量，同时引入2D先验损失和基于原型的对比损失来提升特征学习效果，在无监督3D实例分割任务中达到了新的SOTA性能，甚至超越了使用真实2D边界框监督的方法。


<details>
  <summary>Details</summary>
Motivation: 由于户外LiDAR点云实例分割的人工标注成本极高，现有方法仍依赖一定程度的人工标注，因此需要一种完全无需标注的解决方案。

Method: 提出ALISE框架：1）利用视觉基础模型（VFMs）结合文本和图像生成初始伪标签；2）设计时空投票模块融合2D与3D语义进行离线和在线优化；3）引入2D先验损失和原型对比损失以增强特征学习。

Result: 在无监督3D实例分割任务上达到最先进性能，mAP达到50.95%，超过使用真实2D边界框监督的MWSIS方法（48.42%）2.53%。

Conclusion: ALISE实现了完全无需标注的LiDAR实例分割，通过高质量伪标签生成和多模态语义监督显著提升了无监督性能，为未来低标注成本场景提供了有效方案。

Abstract: The manual annotation of outdoor LiDAR point clouds for instance segmentation
is extremely costly and time-consuming. Current methods attempt to reduce this
burden but still rely on some form of human labeling. To completely eliminate
this dependency, we introduce ALISE, a novel framework that performs LiDAR
instance segmentation without any annotations. The central challenge is to
generate high-quality pseudo-labels in a fully unsupervised manner. Our
approach starts by employing Vision Foundation Models (VFMs), guided by text
and images, to produce initial pseudo-labels. We then refine these labels
through a dedicated spatio-temporal voting module, which combines 2D and 3D
semantics for both offline and online optimization. To achieve superior feature
learning, we further introduce two forms of semantic supervision: a set of 2D
prior-based losses that inject visual knowledge into the 3D network, and a
novel prototype-based contrastive loss that builds a discriminative feature
space by exploiting 3D semantic consistency. This comprehensive design results
in significant performance gains, establishing a new state-of-the-art for
unsupervised 3D instance segmentation. Remarkably, our approach even
outperforms MWSIS, a method that operates with supervision from ground-truth
(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).

</details>


### [38] [OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search](https://arxiv.org/abs/2510.05759)
*Zexin Zheng,Huangyu Dai,Lingtao Mao,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的生成式框架OneVision，用于解决传统视觉搜索中多阶段级联架构存在的表征差异和优化目标冲突问题。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段视觉搜索架构在查询图像的不同视图表示之间存在差异，且各阶段优化目标不一致，难以同时优化用户体验和转化率。

Method: 提出OneVision框架，基于视觉对齐的残差量化编码VRQ，统一不同视角下的对象表示，并采用多阶段语义对齐方案融合用户个性化信息。

Result: 离线评估显示OneVision在保持与在线MCA相当性能的同时，推理效率提升21%；A/B测试中点击率（CTR）提升2.15%，转化率（CVR）提升2.27%，订单量提升3.12%。

Conclusion: 以语义ID为中心的生成式架构能够统一检索与个性化，简化服务流程，并在效率和效果上均取得显著提升。

Abstract: Traditional vision search, similar to search and recommendation systems,
follows the multi-stage cascading architecture (MCA) paradigm to balance
efficiency and conversion. Specifically, the query image undergoes feature
extraction, recall, pre-ranking, and ranking stages, ultimately presenting the
user with semantically similar products that meet their preferences. This
multi-view representation discrepancy of the same object in the query and the
optimization objective collide across these stages, making it difficult to
achieve Pareto optimality in both user experience and conversion. In this
paper, an end-to-end generative framework, OneVision, is proposed to address
these problems. OneVision builds on VRQ, a vision-aligned residual quantization
encoding, which can align the vastly different representations of an object
across multiple viewpoints while preserving the distinctive features of each
product as much as possible. Then a multi-stage semantic alignment scheme is
adopted to maintain strong visual similarity priors while effectively
incorporating user-specific information for personalized preference generation.
In offline evaluations, OneVision performs on par with online MCA, while
improving inference efficiency by 21% through dynamic pruning. In A/B tests, it
achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and
+3.12% order volume. These results demonstrate that a semantic ID centric,
generative architecture can unify retrieval and personalization while
simplifying the serving pathway.

</details>


### [39] [A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data](https://arxiv.org/abs/2510.05760)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出一种利用多源弱标签数据与少量可靠标签数据结合的深度学习训练方法，通过引入描述各数据源错误统计特性的转移矩阵，在梯度层面加权不同来源的标签，提升遥感图像场景分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要大量高质量标注数据，但遥感领域获取高可靠性标签成本高、数量有限，而存在大量低成本但不可靠的标注数据源，如何有效利用这些弱标签数据成为关键问题。

Method: 将一个或多个弱标签数据源与小规模高可靠性数据集融合构建多源标注数据集，并设计一种考虑各数据源可靠性的新训练策略，通过嵌入描述各源误差统计特性的转移矩阵，在训练过程中对不同来源的标签进行加权，实现梯度层面的实例权重调整。

Result: 在多个数据集上的实验验证了该方法的有效性，结果表明该方法具有较强的鲁棒性，能够有效利用不可靠的标签数据提升模型性能。

Conclusion: 该方法能有效融合可靠与弱标签数据，在遥感图像场景分类中显著提升深度模型的泛化能力，为利用低质量标注数据提供了可行方案。

Abstract: Deep learning has gained broad interest in remote sensing image scene
classification thanks to the effectiveness of deep neural networks in
extracting the semantics from complex data. However, deep networks require
large amounts of training samples to obtain good generalization capabilities
and are sensitive to errors in the training labels. This is a problem in remote
sensing since highly reliable labels can be obtained at high costs and in
limited amount. However, many sources of less reliable labeled data are
available, e.g., obsolete digital maps. In order to train deep networks with
larger datasets, we propose both the combination of single or multiple weak
sources of labeled data with a small but reliable dataset to generate
multisource labeled datasets and a novel training strategy where the
reliability of each source is taken in consideration. This is done by
exploiting the transition matrices describing the statistics of the errors of
each source. The transition matrices are embedded into the labels and used
during the training process to weigh each label according to the related
source. The proposed method acts as a weighting scheme at gradient level, where
each instance contributes with different weights to the optimization of
different classes. The effectiveness of the proposed method is validated by
experiments on different datasets. The results proved the robustness and
capability of leveraging on unreliable source of labels of the proposed method.

</details>


### [40] [Mysteries of the Deep: Role of Intermediate Representations in Out of Distribution Detection](https://arxiv.org/abs/2510.05782)
*I. M. De la Jara,C. Rodriguez-Opazo,D. Teney,D. Ranasinghe,E. Abbasnejad*

Main category: cs.CV

TL;DR: 本文提出利用预训练模型中间层的丰富多样性信号进行无需训练的OOD检测，通过熵准则自动选择最具互补性的层，显著提升了远端和近端OOD检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法通常仅使用预训练模型的最后一层表示，忽略了中间层可能包含的多样化分布偏移信号，本文旨在挖掘中间层的潜力以提升检测效果。

Method: 提出一种基于熵的准则，在无需访问OOD数据的情况下，自动识别提供最多互补信息的中间层，并选择性融合这些层的表示用于OOD检测。

Result: 在多种模型架构和训练目标下，该方法在远端OOD检测上准确率最高提升10%，近端OOD检测上超过7%，优于现有的无需训练方法。

Conclusion: 中间层包含丰富的OOD信号，通过熵驱动的选择机制可有效提升无需训练的OOD检测性能，揭示了模型架构与训练目标对OOD检测的影响，开辟了新的研究方向。

Abstract: Out-of-distribution (OOD) detection is essential for reliably deploying
machine learning models in the wild. Yet, most methods treat large pre-trained
models as monolithic encoders and rely solely on their final-layer
representations for detection. We challenge this wisdom. We reveal the
\textit{intermediate layers} of pre-trained models, shaped by residual
connections that subtly transform input projections, \textit{can} encode
\textit{surprisingly rich and diverse signals} for detecting distributional
shifts. Importantly, to exploit latent representation diversity across layers,
we introduce an entropy-based criterion to \textit{automatically} identify
layers offering the most complementary information in a training-free setting
-- \textit{without access to OOD data}. We show that selectively incorporating
these intermediate representations can increase the accuracy of OOD detection
by up to \textbf{$10\%$} in far-OOD and over \textbf{$7\%$} in near-OOD
benchmarks compared to state-of-the-art training-free methods across various
model architectures and training objectives. Our findings reveal a new avenue
for OOD detection research and uncover the impact of various training
objectives and model architectures on confidence-based OOD detection methods.

</details>


### [41] [Rasterized Steered Mixture of Experts for Efficient 2D Image Regression](https://arxiv.org/abs/2510.05814)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Mårten Sjöström*

Main category: cs.CV

TL;DR: 提出了一种基于光栅化的优化策略，结合Steered Mixture of Experts的边缘感知机制和光栅化高斯核渲染的高效性，显著提升二维图像回归的计算效率，同时保持模型稀疏性和重建质量。


<details>
  <summary>Details</summary>
Motivation: Steered Mixture of Experts框架虽在图像重建等任务中表现优异，但计算成本高，限制了实际应用。

Method: 引入基于光栅化的优化策略，用光栅化公式替代全局迭代优化，结合边缘感知门控机制，实现快速参数更新和更高效的内存表示。

Result: 该方法显著加快了参数更新速度，提高了内存效率，并支持原生超分辨率和图像去噪等标准光栅化方法难以实现的应用。

Conclusion: 将光栅化优化与Steered Mixture of Experts的边缘感知结构结合，在计算效率和重建保真度之间实现了新的平衡，适用于二维图像处理任务。

Abstract: The Steered Mixture of Experts regression framework has demonstrated strong
performance in image reconstruction, compression, denoising, and
super-resolution. However, its high computational cost limits practical
applications. This work introduces a rasterization-based optimization strategy
that combines the efficiency of rasterized Gaussian kernel rendering with the
edge-aware gating mechanism of the Steered Mixture of Experts. The proposed
method is designed to accelerate two-dimensional image regression while
maintaining the model's inherent sparsity and reconstruction quality. By
replacing global iterative optimization with a rasterized formulation, the
method achieves significantly faster parameter updates and more
memory-efficient model representations. In addition, the proposed framework
supports applications such as native super-resolution and image denoising,
which are not directly achievable with standard rasterized Gaussian kernel
approaches. The combination of fast rasterized optimization with the edge-aware
structure of the Steered Mixture of Experts provides a new balance between
computational efficiency and reconstruction fidelity for two-dimensional image
processing tasks.

</details>


### [42] [Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images](https://arxiv.org/abs/2510.05819)
*Sven Koehler,Sarah Kaye Mueller,Jonathan Kiekenap,Gerald Greil,Tarique Hussain,Samir Sarikouch,Florian André,Norbert Frey,Sandy Engelhardt*

Main category: cs.CV

TL;DR: 提出一种自监督深度学习方法，通过形变场提取心脏运动特征，实现短轴和四腔心电影CMR中五个和四个关键帧的检测，显著优于基于容积的方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于左心室容积曲线的方法只能检测舒张末期和收缩末期，缺乏对心肌运动的深入洞察，且难以进行精确的时间对齐分析。

Method: 利用图像配准生成密集形变场，计算一维运动描述符，并基于规则确定多个心脏关键帧；采用自监督深度学习框架，无需人工标注关键帧。

Result: 在多个公开数据集上验证，相比容积法ED/ES检测准确率提升30%-51%（SAX）和11%-47%（4CH），平均循环帧差低于1.31（SAX）和1.73（LAX）帧，并可检测额外三个关键帧。

Conclusion: 该方法能实现跨患者、跨周期的心脏动态精准时间对齐分析，支持更精细的心功能评估，具有良好的泛化性和临床应用潜力。

Abstract: Cardiovascular magnetic resonance (CMR) is the gold standard for assessing
cardiac function, but individual cardiac cycles complicate automatic temporal
comparison or sub-phase analysis. Accurate cardiac keyframe detection can
eliminate this problem. However, automatic methods solely derive end-systole
(ES) and end-diastole (ED) frames from left ventricular volume curves, which do
not provide a deeper insight into myocardial motion. We propose a
self-supervised deep learning method detecting five keyframes in short-axis
(SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable
registration fields are derived from the images and used to compute a 1D motion
descriptor, which provides valuable insights into global cardiac contraction
and relaxation patterns. From these characteristic curves, keyframes are
determined using a simple set of rules. The method was independently evaluated
for both views using three public, multicentre, multidisease datasets. M&Ms-2
(n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC
(n=100) datasets for repeatability control. Furthermore, generalisability to
patients with rare congenital heart defects was tested using the German
Competence Network (GCN) dataset. Our self-supervised approach achieved
improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED
and ES, as measured by cyclic frame difference (cFD), compared with the
volume-based approach. We can detect ED and ES, as well as three additional
keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for
SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and
intra-patient analysis of cardiac dynamics, irrespective of cycle or phase
lengths. GitHub repository:
https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git

</details>


### [43] [Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow](https://arxiv.org/abs/2510.05836)
*Ruyang Liu,Shangkun Sun,Haoran Tang,Ge Li,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出Flow4Agent，一种利用光流运动先验来提升多模态大语言模型对长视频理解能力的新框架。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因时空冗余和多模态大模型上下文长度限制而困难，现有方法依赖语义先验（如CLIP）提取关键信息存在局限。

Method: 提出Flow4Agent，包含时间粒度优化（TGO）和运动Token剪枝（MTP）两个模块：TGO结合粗略光流先验进行帧层级聚类，并用语义先验过滤无关场景；MTP利用细粒度光流信息剪枝帧内高冗余视觉token。

Result: 在多个长视频理解基准上超越现有方法，尤其在小时级任务中表现突出，在Video-MME、MLVU和LongVideoBench上分别达到64.7%、71.4%和60.4%的性能。

Conclusion: Flow4Agent通过引入光流运动先验，有效缓解了长视频的时空冗余问题，显著提升了多模态大语言模型在长视频理解任务中的性能。

Abstract: Long-form video understanding has always been a challenging problem due to
the significant redundancy in both temporal and spatial contents. This
challenge is further exacerbated by the limited context length of Multimodal
Large Language Models (MLLMs). To address this issue, many previous works have
attempted to extract key video information, where the "key" is typically
semantic-aware and heavily dependent on the CLIP model as prior. In this paper,
we propose Flow4Agent, a novel framework that pioneeringly incorporates motion
priors from optical flow to facilitate LLM-based long video understanding.
Flow4Agent mitigates the redundancy in long videos at both temporal and spatial
levels through two core modules: Temporal Granularity Optimization (TGO)
adaptively refines framelevel hierarchies, which first leverages coarse flow
priors to group similar visual contents and then applies semantic priors to
filter out highly irrelevant scene information. Motion Token Pruning (MTP)
further refines the intra-frame visual representations, pruning high-redundancy
video tokens using fine-grained optical flow information. Extensive experiments
demonstrate that our Flow4Agent outperforms existing methods across a wide
range of video MLLM benchmarks, especially for hour-level video understanding
tasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.

</details>


### [44] [acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows](https://arxiv.org/abs/2510.05886)
*Johannes Seiffarth,Keitaro Kasahara,Michelle Bund,Benita Lückel,Richard D. Paul,Mathias Pesch,Lennart Witting,Michael Bott,Dietrich Kohlheyer,Katharina Nöh*

Main category: cs.CV

TL;DR: 本文提出了一种名为acia-workflows的平台，用于整合深度学习驱动的细胞分割与追踪工具，实现高通量活细胞成像数据的自动化、可重复且用户友好的分析。


<details>
  <summary>Details</summary>
Motivation: 高通量活细胞成像产生大量数据，传统分析方法难以处理，亟需集成化、易用的分析流程以支持生物学研究中的常规应用。

Method: 开发了一个包含三部分的平台：(1) acia Python库，支持模块化设计并集成8种深度学习分割与追踪方法；(2) 基于Jupyter Notebook的工作流，整合分析流程、依赖、文档和可视化；(3) 提供十余个开源应用工作流示例，覆盖多种微流控成像实验场景。

Result: 实现了对单细胞动态的高时空分辨率定量分析，例如分钟级精度的细胞对氧变化响应的追踪，并展示了生长速率比较等实际应用。

Conclusion: acia-workflows为活细胞成像数据分析提供了开放、可扩展、可重现且易于使用的解决方案，有助于推动深度学习在生命科学研究中的广泛应用。

Abstract: Live-cell imaging (LCI) technology enables the detailed spatio-temporal
characterization of living cells at the single-cell level, which is critical
for advancing research in the life sciences, from biomedical applications to
bioprocessing. High-throughput setups with tens to hundreds of parallel cell
cultivations offer the potential for robust and reproducible insights. However,
these insights are obscured by the large amount of LCI data recorded per
experiment. Recent advances in state-of-the-art deep learning methods for cell
segmentation and tracking now enable the automated analysis of such large data
volumes, offering unprecedented opportunities to systematically study
single-cell dynamics. The next key challenge lies in integrating these powerful
tools into accessible, flexible, and user-friendly workflows that support
routine application in biological research. In this work, we present
acia-workflows, a platform that combines three key components: (1) the
Automated live-Cell Imaging Analysis (acia) Python library, which supports the
modular design of image analysis pipelines offering eight deep learning
segmentation and tracking approaches; (2) workflows that assemble the image
analysis pipeline, its software dependencies, documentation, and visualizations
into a single Jupyter Notebook, leading to accessible, reproducible and
scalable analysis workflows; and (3) a collection of application workflows
showcasing the analysis and customization capabilities in real-world
applications. Specifically, we present three workflows to investigate various
types of microfluidic LCI experiments ranging from growth rate comparisons to
precise, minute-resolution quantitative analyses of individual dynamic cells
responses to changing oxygen conditions. Our collection of more than ten
application workflows is open source and publicly available at
https://github.com/JuBiotech/acia-workflows.

</details>


### [45] [BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data](https://arxiv.org/abs/2510.05888)
*Arefin Ittesafun Abian,Debopom Sutradhar,Md Rafi Ur Rashid,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Kheng Cher Yeo,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出了BioAutoML-NAS，首个结合图像与元数据的多模态生物自动机器学习模型，利用神经架构搜索和交替双层优化策略，在大规模昆虫分类任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 昆虫分类对农业管理和生态研究至关重要，但由于昆虫特征复杂、类别不平衡和数据规模大，现有方法面临挑战。因此需要一种能自动学习最优网络结构并融合多模态信息的高效分类模型。

Method: 提出BioAutoML-NAS模型，采用神经架构搜索（NAS）针对图像数据自动学习细胞内连接的最佳操作，堆叠多个细胞构建完整网络以提取图像特征；设计多模态融合模块，结合图像嵌入与元数据；采用交替双层优化策略联合更新网络权重与架构参数，并通过零操作剪枝不重要连接，生成稀疏高效的网络结构。

Result: 在BIOSCAN-5M数据集上达到96.81%准确率、97.46%精确率、96.81%召回率和97.05% F1分数，性能超过现有迁移学习、Transformer、AutoML和NAS方法约16%、10%和8%；在Insects-1M数据集上取得93.25%准确率、93.71%精确率、92.74%召回率和93.22% F1分数。

Conclusion: BioAutoML-NAS通过多模态融合与自动神经架构搜索，实现了高效且准确的昆虫分类，为可持续农业提供了可靠的技术支持。

Abstract: Insect classification is important for agricultural management and ecological
research, as it directly affects crop health and production. However, this task
remains challenging due to the complex characteristics of insects, class
imbalance, and large-scale datasets. To address these issues, we propose
BioAutoML-NAS, the first BioAutoML model using multimodal data, including
images, and metadata, which applies neural architecture search (NAS) for images
to automatically learn the best operations for each connection within each
cell. Multiple cells are stacked to form the full network, each extracting
detailed image feature representations. A multimodal fusion module combines
image embeddings with metadata, allowing the model to use both visual and
categorical biological information to classify insects. An alternating bi-level
optimization training strategy jointly updates network weights and architecture
parameters, while zero operations remove less important connections, producing
sparse, efficient, and high-performing architectures. Extensive evaluation on
the BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81%
accuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperforming
state-of-the-art transfer learning, transformer, AutoML, and NAS methods by
approximately 16%, 10%, and 8% respectively. Further validation on the
Insects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall,
and a 93.22% F1 score. These results demonstrate that BioAutoML-NAS provides
accurate, confident insect classification that supports modern sustainable
farming.

</details>


### [46] [$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection](https://arxiv.org/abs/2510.05891)
*Yanran Zhang,Bingyao Yu,Yu Zheng,Wenzhao Zheng,Yueqi Duan,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于离散分布差异感知量化误差（D$^3$QE）的自回归生成图像检测方法，利用真实与伪造图像在码本频率分布上的差异，结合动态统计信息与Transformer架构，实现了对多种视觉AR模型生成图像的高效检测，具备良好的泛化性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉自回归模型在图像生成领域的快速发展，其生成图像的高质量和独特表示特性给现有检测方法带来挑战，亟需针对AR模型特有的离散token生成机制设计新型检测技术。

Method: 提出D$^3$QE方法，通过分析真实与伪造图像在向量量化过程中的码本频率分布偏差，构建离散分布差异感知的Transformer模型，将动态码本频率统计融入注意力机制，并融合语义特征与量化误差潜在表示以提升检测性能。

Result: 在涵盖7种主流视觉AR模型的ARForensics数据集上实验表明，D$^3$QE在检测精度、跨模型泛化能力及对现实扰动的鲁棒性方面均优于现有方法。

Conclusion: D$^3$QE有效利用了AR模型生成图像在量化过程中的分布偏差，为自回归生成图像检测提供了新思路，并展现出实际应用潜力。

Abstract: The emergence of visual autoregressive (AR) models has revolutionized image
generation while presenting new challenges for synthetic image detection.
Unlike previous GAN or diffusion-based methods, AR models generate images
through discrete token prediction, exhibiting both marked improvements in image
synthesis quality and unique characteristics in their vector-quantized
representations. In this paper, we propose to leverage Discrete Distribution
Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated
image detection that exploits the distinctive patterns and the frequency
distribution bias of the codebook existing in real and fake images. We
introduce a discrete distribution discrepancy-aware transformer that integrates
dynamic codebook frequency statistics into its attention mechanism, fusing
semantic features and quantization error latent. To evaluate our method, we
construct a comprehensive dataset termed ARForensics covering 7 mainstream
visual AR models. Experiments demonstrate superior detection accuracy and
strong generalization of D$^3$QE across different AR models, with robustness to
real-world perturbations. Code is available at
\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.

</details>


### [47] [Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning](https://arxiv.org/abs/2510.05899)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Jinyan Zhou,Jianfeng Cao,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督上下文学习（WS-ICL）方法，用于医学图像分割，利用弱标签（如边界框或点）替代密集标注，显著降低标注成本，同时在多个基准上实现了与常规ICL模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的通用医学图像分割模型（如交互式和上下文学习模型）虽然泛化能力强，但依赖大量精细标注，标注成本高。为此，本文旨在减少对密集像素级标签和重复用户提示的依赖。

Method: 提出WS-ICL，一种新的上下文学习范式，使用弱提示（如边界框或点）构建上下文，避免使用精细掩码，并通过实验验证其在不同基准上的有效性。

Result: 在三个独立测试基准上，WS-ICL的性能与传统ICL模型相当，但标注成本显著降低，并且在交互式设置下也表现出很强的竞争力。

Conclusion: WS-ICL为高效、统一的医学图像分割通用模型提供了有前景的解决方案，显著降低了标注负担，同时保持了良好的分割性能。

Abstract: Universal models for medical image segmentation, such as interactive and
in-context learning (ICL) models, offer strong generalization but require
extensive annotations. Interactive models need repeated user prompts for each
image, while ICL relies on dense, pixel-level labels. To address this, we
propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that
leverages weak prompts (e.g., bounding boxes or points) instead of dense labels
for context. This approach significantly reduces annotation effort by
eliminating the need for fine-grained masks and repeated user prompting for all
images. We evaluated the proposed WS-ICL model on three held-out benchmarks.
Experimental results demonstrate that WS-ICL achieves performance comparable to
regular ICL models at a significantly lower annotation cost. In addition,
WS-ICL is highly competitive even under the interactive paradigm. These
findings establish WS-ICL as a promising step toward more efficient and unified
universal models for medical image segmentation. Our code and model are
publicly available at https://github.com/jiesihu/Weak-ICL.

</details>


### [48] [Kaputt: A Large-Scale Dataset for Visual Defect Detection](https://arxiv.org/abs/2510.05903)
*Sebastian Höfer,Dorian Henning,Artemij Amiranashvili,Douglas Morrison,Mariliza Tzes,Ingmar Posner,Marc Matvienko,Alessandro Rennola,Anton Milan*

Main category: cs.CV

TL;DR: 提出一个用于物流场景缺陷检测的大规模数据集，包含超过23万张图像和4.8万个不同物体，是MVTec-AD的40倍，现有最先进方法在该数据集上的AUROC不超过56.96%，凸显其挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测数据集（如MVTec-AD和VisA）主要针对制造场景，物体类别少、姿态可控，已接近饱和；而零售物流中的异常检测面临更大的物体姿态和外观多样性挑战，现有方法表现不佳，亟需新基准。

Method: 构建了一个大规模物流缺陷检测数据集Kaputt，包含230,000多张图像、29,000多个缺陷实例和48,000多个不同物体，并对多种最先进的异常检测方法进行了广泛评估。

Result: 最先进的异常检测方法在该数据集上的AUROC不超过56.96%，显著低于在MVTec-AD等数据集上的表现，表明现有方法难以应对物流场景中的姿态和外观变化。

Conclusion: 该数据集为零售物流中的异常检测设立了新的基准，揭示了当前方法的局限性，并推动未来研究关注更具挑战性的现实场景。

Abstract: We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.

</details>


### [49] [Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging](https://arxiv.org/abs/2510.05971)
*Ron Keuth,Paul Kaftan,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 本文首次对医学图像中的token mixer进行了全面研究，系统分析了MetaFormer架构中不同类型的token mixer在图像分类和语义分割任务中的表现，发现低复杂度的mixer（如分组卷积或池化）在分类任务中已足够，而在分割任务中卷积类mixer的局部归纳偏置至关重要，推荐使用分组卷积。


<details>
  <summary>Details</summary>
Motivation: 尽管MetaFormer在自然图像中被广泛研究，但在医学成像中的应用较少，且缺乏对不同token mixer的比较，可能忽略了更适合医学图像的设计选择。

Method: 在MetaFormer架构下，系统评估了基于池化、卷积和注意力机制的token mixer在八种涵盖多种模态和挑战的医学图像数据集上的表现，并分析了从自然图像预训练权重迁移的有效性。

Result: 分类任务中，低复杂度的token mixer（如分组卷积或池化）性能足够且预训练权重仍有效；分割任务中，卷积类mixer因其局部归纳偏置表现更优，其中分组卷积在减少计算和参数的同时保持性能。

Conclusion: 在医学图像分析中，MetaFormer的表现高度依赖任务类型：分类任务适合简单token mixer，而分割任务则需要具有局部结构建模能力的卷积类mixer，分组卷积是最佳折中选择。

Abstract: The generalization of the Transformer architecture via MetaFormer has
reshaped our understanding of its success in computer vision. By replacing
self-attention with simpler token mixers, MetaFormer provides strong baselines
for vision tasks. However, while extensively studied on natural image datasets,
its use in medical imaging remains scarce, and existing works rarely compare
different token mixers, potentially overlooking more suitable designs choices.
In this work, we present the first comprehensive study of token mixers for
medical imaging. We systematically analyze pooling-, convolution-, and
attention-based token mixers within the MetaFormer architecture on image
classification (global prediction task) and semantic segmentation (dense
prediction task). Our evaluation spans eight datasets covering diverse
modalities and common challenges in the medical domain. Given the prevalence of
pretraining from natural images to mitigate medical data scarcity, we also
examine transferring pretrained weights to new token mixers. Our results show
that, for classification, low-complexity token mixers (e.g. grouped convolution
or pooling) are sufficient, aligning with findings on natural images.
Pretrained weights remain useful despite the domain gap introduced by the new
token mixer. For segmentation, we find that the local inductive bias of
convolutional token mixers is essential. Grouped convolutions emerge as the
preferred choice, as they reduce runtime and parameter count compared to
standard convolutions, while the MetaFormer's channel-MLPs already provide the
necessary cross-channel interactions. Our code is available on GitHub.

</details>


### [50] [Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis](https://arxiv.org/abs/2510.05976)
*Eashan Adhikarla,Yixin Liu,Brian D. Davison*

Main category: cs.CV

TL;DR: 本文综述了扩散模型在低光照图像增强（LLIE）中的应用，提出了一个涵盖六类方法的多视角分类体系，并对生成对抗网络和基于Transformer的最先进方法进行了深入的性能比较，探讨了实际部署挑战及基础模型等新兴范式的作用。


<details>
  <summary>Details</summary>
Motivation: 由于低光照条件下可见性下降会影响下游任务表现，在监控、自动驾驶和医学成像等安全关键应用中，低光照图像增强至关重要。因此需要系统地评估和分析最新的扩散模型在该领域的潜力与挑战。

Method: 提出了一种包含内在分解、光谱与潜在、加速、引导、多模态和自主六类的多视角分类法，结合模型机制与条件信号进行分析；并对现有方法进行了定性和定量评估，同时讨论了部署限制和伦理问题。

Result: 明确了扩散模型相较于GAN和Transformer方法的优势与局限，揭示了不同方法在可解释性、泛化能力和推理效率之间的权衡，指出了当前基准不一致和失败模式等问题。

Conclusion: 扩散模型在LLIE中具有巨大潜力，未来的研究应关注新型条件机制、实时适应能力以及基础模型的应用，同时需解决实际部署中的资源约束和伦理问题。

Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.

</details>


### [51] [A Dynamic Mode Decomposition Approach to Morphological Component Analysis](https://arxiv.org/abs/2510.05977)
*Owen T. Huber,Raghu G. Raj,Tianyu Chen,Zacharie I. Idriss*

Main category: cs.CV

TL;DR: 本文提出了一种基于场景动态变化的自适应视频表示方法，通过动态模态分解特征值聚类构建数据驱动的形态分量分析（DMCA），在去噪和信号增强等应用中表现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统形态分量分析（MCA）依赖预定义的固定字典，难以适应复杂多变的视频内容结构，因此需要一种能够根据视频动态特性自适应学习字典的方法。

Method: 引入动态模态分解（DMD）特征值的聚类技术，用于生成数据驱动的MCA字典，提出动态形态分量分析（DMCA）算法，并应用于图像与视频的源分离任务。

Result: 在Adobe 240fps视频数据集上验证了DMCA在去噪方面的有效性；成功提升了含海杂波微弱目标的信噪比，并实现了逆合成孔径雷达图像中自行车与风致杂波的分离。

Conclusion: DMCA通过结合DMD特征聚类与MCA框架，实现了对视频内容的自适应表示，在多种复杂场景下的信号分离任务中具有优越性能。

Abstract: This paper introduces a novel methodology of adapting the representation of
videos based on the dynamics of their scene content variation. In particular,
we demonstrate how the clustering of dynamic mode decomposition eigenvalues can
be leveraged to learn an adaptive video representation for separating
structurally distinct morphologies of a video. We extend the morphological
component analysis (MCA) algorithm, which uses multiple predefined incoherent
dictionaries and a sparsity prior to separate distinct sources in signals, by
introducing our novel eigenspace clustering technique to obtain data-driven MCA
dictionaries, which we call dynamic morphological component analysis (DMCA).
After deriving our novel algorithm, we offer a motivational example of DMCA
applied to a still image, then demonstrate DMCA's effectiveness in denoising
applications on videos from the Adobe 240fps dataset. Afterwards, we provide an
example of DMCA enhancing the signal-to-noise ratio of a faint target summed
with a sea state, and conclude the paper by applying DMCA to separate a bicycle
from wind clutter in inverse synthetic aperture radar images.

</details>


### [52] [Diffusion-Based Image Editing for Breaking Robust Watermarks](https://arxiv.org/abs/2510.05978)
*Yunyi Ni,Finn Carter,Ze Niu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 本文研究了基于扩散模型的图像生成与编辑技术对鲁棒隐形水印的威胁，提出了一种引导扩散攻击方法，可在保持图像视觉质量的同时有效消除水印，并从理论上证明了扩散变换会消除水印与图像间的互信息，实验表明该方法能几乎完全破坏多种先进水印方案。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型等生成式AI技术的发展，传统设计用于抵抗常规扰动的鲁棒水印面临新挑战，亟需研究其安全性漏洞并提出相应攻击方法以推动更安全的水印技术发展。

Method: 提出一种引导扩散攻击方法，在图像再生过程中显式针对水印信号进行去除；通过理论分析证明扩散变换导致水印与图像间互信息消失，并在多个先进水印方案上进行实验验证。

Result: 在StegaStamp、TrustMark和VINE等多种深度学习水印方法上实现接近零的水印恢复率，同时保持再生图像的高视觉保真度。

Conclusion: 当前鲁棒水印技术在生成模型攻击下面临根本性脆弱性，需在生成式AI时代发展新的水印策略。

Abstract: Robust invisible watermarking aims to embed hidden information into images
such that the watermark can survive various image manipulations. However, the
rise of powerful diffusion-based image generation and editing techniques poses
a new threat to these watermarking schemes. In this paper, we present a
theoretical study and method demonstrating that diffusion models can
effectively break robust image watermarks that were designed to resist
conventional perturbations. We show that a diffusion-driven ``image
regeneration'' process can erase embedded watermarks while preserving
perceptual image content. We further introduce a novel guided diffusion attack
that explicitly targets the watermark signal during generation, significantly
degrading watermark detectability. Theoretically, we prove that as an image
undergoes sufficient diffusion-based transformation, the mutual information
between the watermarked image and the embedded watermark payload vanishes,
resulting in decoding failure. Experimentally, we evaluate our approach on
multiple state-of-the-art watermarking schemes (including the deep
learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate
near-zero watermark recovery rates after attack, while maintaining high visual
fidelity of the regenerated images. Our findings highlight a fundamental
vulnerability in current robust watermarking techniques against generative
model-based attacks, underscoring the need for new watermarking strategies in
the era of generative AI.

</details>


### [53] [Detection and Measurement of Hailstones with Multimodal Large Language Models](https://arxiv.org/abs/2510.06008)
*Moritz Alker,David C. Schedl,Andreas Stöckl*

Main category: cs.CV

TL;DR: 本研究利用预训练的多模态大语言模型，通过社交媒体和新闻图片检测与测量冰雹大小，使用474张来自奥地利2022至2024年冰雹事件的众包图像，比较了单阶段与双阶段提示策略，结果显示最佳模型平均绝对误差为1.12厘米，表明无需微调的现成模型即可有效补充传统冰雹传感器。


<details>
  <summary>Details</summary>
Motivation: 传统冰雹监测手段有限，缺乏空间密集性和实时性，因此需要利用广泛可用的社交媒体图像来提升对强天气事件的快速评估能力。

Method: 采用预训练的多模态大语言模型，基于包含参考物体（如人手）的图像，使用单阶段和双阶段提示策略估计冰雹直径，并在474张真实冰雹图像上进行评估。

Result: 最佳模型的平均绝对误差为1.12厘米；双阶段提示策略相比单阶段提升了多数模型的可靠性。

Conclusion: 现成的多模态大语言模型即使未经微调，也能从社交媒体图像中有效提取冰雹尺寸信息，可作为传统传感器的有力补充，未来结合自动实时图像采集将更具应用价值。

Abstract: This study examines the use of social media and news images to detect and
measure hailstones, utilizing pre-trained multimodal large language models. The
dataset for this study comprises 474 crowdsourced images of hailstones from
documented hail events in Austria, which occurred between January 2022 and
September 2024. These hailstones have maximum diameters ranging from 2 to 11cm.
We estimate the hail diameters and compare four different models utilizing
one-stage and two-stage prompting strategies. The latter utilizes additional
size cues from reference objects, such as human hands, within the image. Our
results show that pretrained models already have the potential to measure
hailstone diameters from images with an average mean absolute error of 1.12cm
for the best model. In comparison to a single-stage prompt, two-stage prompting
improves the reliability of most models. Our study suggests that these
off-the-shelf models, even without fine-tuning, can complement traditional hail
sensors by extracting meaningful and spatially dense information from social
media imagery, enabling faster and more detailed assessments of severe weather
events. The automated real-time image harvesting from social media and other
sources remains an open task, but it will make our approach directly applicable
to future hail events.

</details>


### [54] [Continual Learning for Image Captioning through Improved Image-Text Alignment](https://arxiv.org/abs/2510.06009)
*Bertram Taetz,Gal Bordelius*

Main category: cs.CV

TL;DR: 提出了一种用于持续图像描述的多损失框架，通过基于提示的持续学习和对比对齐来缓解灾难性遗忘并提升语义对齐。


<details>
  <summary>Details</summary>
Motivation: 在持续学习场景中，由于灾难性遗忘以及随时间推移视觉概念与语言对齐的困难，生成准确且连贯的图像描述仍是一个重大挑战。

Method: 基于预训练的ViT-GPT-2模型，结合标准交叉熵损失，并引入三种新损失：基于提示的余弦相似度损失、CLIP风格的图像-文本对齐损失，以及语言引导的对比损失（三元组损失），以增强任务间的类别区分能力。

Result: 该方法在不增加推理开销、无需生成时使用提示的情况下，有效缓解了灾难性遗忘，在语义对齐方面优于现有最先进方法。

Conclusion: 所提出的多损失框架在持续图像描述任务中表现出色，兼顾性能保持与语义一致性，具有实用性和可扩展性。

Abstract: Generating accurate and coherent image captions in a continual learning
setting remains a major challenge due to catastrophic forgetting and the
difficulty of aligning evolving visual concepts with language over time. In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks. Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation. We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods. The code can
be found via the following link https://github.com/
Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.

</details>


### [55] [Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context](https://arxiv.org/abs/2510.06026)
*An Thi Nguyen,Radina Stoykova,Eric Arazo*

Main category: cs.CV

TL;DR: 研究表明，通用实例搜索模型在训练时即使不使用人类数据，也可能通过过学习意外获得识别特定个体的能力，引发隐私和识别风险。研究评估了索引排除和混淆损失两种技术缓解措施，可在将人物重识别准确率降至2%以下的同时保留82%的非人物对象检索性能，但发现这些方法仍存在被部分人体图像绕过的漏洞，凸显AI治理与数据保护中的监管空白。


<details>
  <summary>Details</summary>
Motivation: 通用实例搜索模型虽能高效辅助刑事调查中的对象检索，但其可能意外获得识别个体的能力，带来隐私泄露和人物画像风险，而当前缺乏有效的去标识化标准，亟需研究技术缓解手段并探讨相应的监管框架。

Method: 研究评估了两种技术防护措施：1）索引排除——在检索过程中排除可能含有人物的索引；2）混淆损失——在训练阶段引入对抗性损失函数以削弱模型对人物身份的敏感性。通过在多个数据集上测试人物重识别准确率与整体检索性能的权衡来评估效果。

Result: 结合指数排除与混淆损失可将人物重识别准确率降低至2%以下，同时保持对非人物对象82%的检索性能。然而，实验发现该防护机制可能被仅含局部人体（如远距离或遮挡图像）的查询绕过，存在安全隐患。

Conclusion: 通用实例搜索模型可能隐含人物识别能力，现有技术缓解手段虽有效但仍存漏洞，需结合更强的技术标准与监管政策，防止看似无害的AI系统发展出未经授权的识别功能，建议在AI治理中建立针对“隐式识别能力”的分类与管控机制。

Abstract: Generic instance search models can dramatically reduce the manual effort
required to analyze vast surveillance footage during criminal investigations by
retrieving specific objects of interest to law enforcement. However, our
research reveals an unintended emergent capability: through overlearning, these
models can single out specific individuals even when trained on datasets
without human subjects. This capability raises concerns regarding
identification and profiling of individuals based on their personal data, while
there is currently no clear standard on how de-identification can be achieved.
We evaluate two technical safeguards to curtail a model's person
re-identification capacity: index exclusion and confusion loss. Our experiments
demonstrate that combining these approaches can reduce person re-identification
accuracy to below 2% while maintaining 82% of retrieval performance for
non-person objects. However, we identify critical vulnerabilities in these
mitigations, including potential circumvention using partial person images.
These findings highlight urgent regulatory questions at the intersection of AI
governance and data protection: How should we classify and regulate systems
with emergent identification capabilities? And what technical standards should
be required to prevent identification capabilities from developing in seemingly
benign applications?

</details>


### [56] [Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between](https://arxiv.org/abs/2510.06035)
*Ondřej Týbl,Lukáš Neumann*

Main category: cs.CV

TL;DR: 本文提出了一个统一的神经网络架构搜索空间UniNAS，能够整合卷积网络、Transformer及其混合架构，并提出新的搜索算法，在相同训练条件下发现性能优于现有手工设计模型的新架构。


<details>
  <summary>Details</summary>
Motivation: 现有的神经架构搜索空间通常局限于特定类型的网络结构，缺乏对卷积网络、Transformer及其混合架构的统一建模能力，限制了NAS的探索广度和公平比较。

Method: 提出UniNAS这一通用搜索空间，采用图结构统一表示不同类型的神经网络；设计新的搜索算法以在该空间中有效探索；并提供统一工具包和标准化训练评估协议。

Result: 在相同训练设置下，发现的架构性能超越了最先进的手工设计模型，验证了UniNAS空间的有效性和潜力。

Conclusion: UniNAS为系统性探索全谱系神经架构提供了统一框架，推动了NAS研究的可复现性和公平比较。

Abstract: We introduce Universal Neural Architecture Space (UniNAS), a generic search
space for neural architecture search (NAS) which unifies convolutional
networks, transformers, and their hybrid architectures under a single, flexible
framework. Our approach enables discovery of novel architectures as well as
analyzing existing architectures in a common framework. We also propose a new
search algorithm that allows traversing the proposed search space, and
demonstrate that the space contains interesting architectures, which, when
using identical training setup, outperform state-of-the-art hand-crafted
architectures. Finally, a unified toolkit including a standardized training and
evaluation protocol is introduced to foster reproducibility and enable fair
comparison in NAS research. Overall, this work opens a pathway towards
systematically exploring the full spectrum of neural architectures with a
unified graph-based NAS perspective.

</details>


### [57] [VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization](https://arxiv.org/abs/2510.06040)
*Xinye Cao,Hongcan Guo,Jiawen Qian,Guoshun Nan,Chao Wang,Yuqi Pan,Tianhao Hou,Xiaojuan Wang,Yutong Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为VideoMiner的层次化方法，用于长视频理解，通过迭代分割、描述和聚类构建树状结构，并结合基于树的组相对策略优化（T-GRPO）来动态定位关键帧，有效缓解冗余信息干扰并提升多模态大模型在长视频理解任务中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法在处理冗余信息和动态适应复杂层次结构方面存在挑战，尤其是均匀采样导致无关信息过多，而关键帧提取难以精准定位。因此，需要一种能保持时序连贯性且可自适应探索关键内容的框架。

Method: 提出VideoMiner框架，将长视频逐级划分为事件和帧，形成层次化树结构；引入T-GRPO强化学习方法，在树结构上进行分组相对策略优化，融合时空信息并根据问题引导探索关键帧路径。

Result: 在多个长视频理解任务中均取得最优性能，T-GRPO能自发激励模型生成推理链，设计的树生长auxin机制可动态调整扩展深度，兼顾准确性与效率。

Conclusion: VideoMiner结合层次化建模与基于树的强化学习策略，有效解决了长视频中冗余信息干扰和关键帧动态定位难题，为多模态大模型处理超长视频提供了高效可扩展的解决方案。

Abstract: Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.

</details>


### [58] [GLVD: Guided Learned Vertex Descent](https://arxiv.org/abs/2510.06046)
*Pol Caselles Rico,Francesc Moreno Noguer*

Main category: cs.CV

TL;DR: 提出GLVD方法，结合每顶点神经场优化和动态预测的3D关键点，在少样本图像下实现高效且高质量的3D人脸重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D人脸建模方法受限于固定的形状先验，而基于优化的方法计算成本高，因此需要一种兼顾表达能力和效率的新方法。

Method: 扩展Learned Vertex Descent（LVD），引入每顶点神经场优化，并结合动态预测的3D关键点提供全局结构引导，利用相对空间编码迭代优化网格顶点。

Result: 在单视图设置下达到最先进性能，在多视图场景中也具有竞争力，同时显著降低推理时间。

Conclusion: GLVD在无需密集3D监督的情况下，实现了高效、灵活且高质量的3D人脸重建。

Abstract: Existing 3D face modeling methods usually depend on 3D Morphable Models,
which inherently constrain the representation capacity to fixed shape priors.
Optimization-based approaches offer high-quality reconstructions but tend to be
computationally expensive. In this work, we introduce GLVD, a hybrid method for
3D face reconstruction from few-shot images that extends Learned Vertex Descent
(LVD) by integrating per-vertex neural field optimization with global
structural guidance from dynamically predicted 3D keypoints. By incorporating
relative spatial encoding, GLVD iteratively refines mesh vertices without
requiring dense 3D supervision. This enables expressive and adaptable geometry
reconstruction while maintaining computational efficiency. GLVD achieves
state-of-the-art performance in single-view settings and remains highly
competitive in multi-view scenarios, all while substantially reducing inference
time.

</details>


### [59] [Medical Vision Language Models as Policies for Robotic Surgery](https://arxiv.org/abs/2510.06064)
*Akshay Muppidi,Martin Radfar*

Main category: cs.CV

TL;DR: 提出一种结合MedFlamingo与PPO的新方法，用于基于视觉的机器人腹腔镜手术任务，在LapGym五个任务中显著优于基线方法，成功率超70%，提升幅度达66.67%至1114.29%。


<details>
  <summary>Details</summary>
Motivation: 视觉PPO在腹腔镜手术任务中面临高维输入、稀疏奖励和特征提取困难等问题，需引入医学先验知识提升性能。

Method: 将医学领域专用的视觉语言模型MedFlamingo与PPO结合，每回合利用任务观察和指令生成高层规划token，融合医学专业知识与实时视觉反馈。

Result: 在五个腹腔镜手术环境中，仅使用内窥镜视觉输入，MedFlamingo PPO相比标准PPO和OpenFlamingo PPO收敛更快、性能更优，任务成功率均超过70%。

Conclusion: 引入医学领域先验知识可有效提升视觉强化学习在复杂外科手术任务中的性能，验证了专业视觉语言模型在手术决策中的价值。

Abstract: Vision-based Proximal Policy Optimization (PPO) struggles with visual
observation-based robotic laparoscopic surgical tasks due to the
high-dimensional nature of visual input, the sparsity of rewards in surgical
environments, and the difficulty of extracting task-relevant features from raw
visual data. We introduce a simple approach integrating MedFlamingo, a medical
domain-specific Vision-Language Model, with PPO. Our method is evaluated on
five diverse laparoscopic surgery task environments in LapGym, using only
endoscopic visual observations. MedFlamingo PPO outperforms and converges
faster compared to both standard vision-based PPO and OpenFlamingo PPO
baselines, achieving task success rates exceeding 70% across all environments,
with improvements ranging from 66.67% to 1114.29% compared to baseline. By
processing task observations and instructions once per episode to generate
high-level planning tokens, our method efficiently combines medical expertise
with real-time visual feedback. Our results highlight the value of specialized
medical knowledge in robotic surgical planning and decision-making.

</details>


### [60] [Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA](https://arxiv.org/abs/2510.06067)
*Python Song,Luke Tenyi Chang,Yun-Yun Tsai,Penghui Li,Junfeng Yang*

Main category: cs.CV

TL;DR: 本文提出了CAPTCHA-X，首个包含推理过程的真实世界验证码基准，用于评估视觉语言模型的空间推理能力。研究发现，通过引入逐步推理机制，模型的验证码解决准确率显著提升至83.9%，远超现有模型的21.9%。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理高难度空间推理任务（如验证码）时表现不佳，缺乏有效的推理机制。因此需要一个包含推理标注的现实世界基准来系统评估和提升模型的推理能力。

Method: 构建了CAPTCHA-X基准，涵盖七类验证码，并提供逐步动作解法与定位标注；提出五种面向推理的评估指标；设计了一种基于代理的视觉语言模型框架，强化模型的逐步推理能力。

Result: 所提方法在五种高难度验证码类型上平均准确率达到83.9%，显著优于现有商业模型约21.9%的水平。验证了逐步推理对提升空间推理任务性能的关键作用。

Conclusion: 当前视觉语言模型在空间推理方面存在明显不足，而引入显式逐步推理可大幅提升性能。CAPTCHA-X为未来研究提供了重要工具，强调了推理在视觉-空间任务中的核心地位。

Abstract: CAPTCHA, originally designed to distinguish humans from robots, has evolved
into a real-world benchmark for assessing the spatial reasoning capabilities of
vision-language models. In this work, we first show that step-by-step reasoning
is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent
high-difficulty spatial reasoning tasks, and that current commercial
vision-language models still struggle with such reasoning. In particular, we
observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to
effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).
However, our findings indicate that requiring the model to perform step-by-step
reasoning before generating the final coordinates can significantly enhance its
solving accuracy, underscoring the severity of the gap. To systematically study
this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with
reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,
etc.) with step-by-step action solutions and grounding annotations. We further
define five reasoning-oriented metrics that enable a comprehensive evaluation
of models reasoning capabilities. To validate the effectiveness of reasoning,
we also propose a general agentic VLM-based framework that incorporates the
models inherent reasoning abilities. Our method achieves state-of-the-art
performance across five high-difficulty CAPTCHA types, with an average solving
accuracy of 83.9 percent, substantially surpassing existing baselines. These
results reveal the limitations of current models and highlight the importance
of reasoning in advancing visual-spatial challenges in the future.

</details>


### [61] [There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers](https://arxiv.org/abs/2510.06070)
*Meghna P Ayyar,Jenny Benois-Pineau,Akka Zemmari*

Main category: cs.CV

TL;DR: 提出一种结合注意力图与统计过滤的方法，用于生成更清晰、可解释性更强的Vision Transformers解释，并通过人类注视数据验证其与人类感知的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力权重的ViT解释方法会产生噪声较大的热图，且许多为CNN设计的解释方法难以迁移到ViT上，因此需要一种更有效、忠实且符合人类感知的解释方法。

Method: 将注意力图与一种最初为CNN提出的统计过滤方法相结合，去除噪声或无信息量的模式，并进一步提出类别特定变体以生成具有判别性的解释。

Result: 在多个数据集上评估显示，该方法生成的解释图更清晰、更可读，在基于扰动的保真度指标和与人类注视数据的一致性方面均优于或媲美现有最先进方法，同时保持高效性。

Conclusion: 经过适当过滤后，注意力机制仍是ViT中可解释且有价值的信号；所提方法在保持效率的同时提升了可视化质量和人类可理解性，强调了人类感知对XAI评估的重要性。

Abstract: Explainable AI (XAI) has become increasingly important with the rise of large
transformer models, yet many explanation methods designed for CNNs transfer
poorly to Vision Transformers (ViTs). Existing ViT explanations often rely on
attention weights, which tend to yield noisy maps as they capture
token-to-token interactions within each layer.While attribution methods
incorporating MLP blocks have been proposed, we argue that attention remains a
valuable and interpretable signal when properly filtered. We propose a method
that combines attention maps with a statistical filtering, initially proposed
for CNNs, to remove noisy or uninformative patterns and produce more faithful
explanations. We further extend our approach with a class-specific variant that
yields discriminative explanations. Evaluation against popular state-of-the-art
methods demonstrates that our approach produces sharper and more interpretable
maps. In addition to perturbation-based faithfulness metrics, we incorporate
human gaze data to assess alignment with human perception, arguing that human
interpretability remains essential for XAI. Across multiple datasets, our
approach consistently outperforms or is comparable to the SOTA methods while
remaining efficient and human plausible.

</details>


### [62] [When Thinking Drifts: Evidential Grounding for Robust Video Reasoning](https://arxiv.org/abs/2510.06077)
*Mi Luo,Zihui Xue,Alex Dimakis,Kristen Grauman*

Main category: cs.CV

TL;DR: 本文研究了链式思维（CoT）在视频推理中的局限性，提出了“视觉思维漂移”现象，并引入基于视觉证据奖励（VER）的强化学习框架，以提升多步视觉推理的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（CoT）在文本推理中表现良好，但在视频理解中常导致模型生成脱离视觉证据的错误推理，因此需要系统分析其问题并提出更可靠的视频推理机制。

Method: 通过贝叶斯视角分析CoT在视频推理中的“视觉思维漂移”现象，并提出Visual Evidence Reward（VER）框架，利用强化学习奖励与视觉证据对齐的推理路径。

Result: 在10个视频理解基准上的实验表明，所提出的Video-VER方法显著优于现有方法，有效减少幻觉并提升推理准确性。

Conclusion: 视频推理需要模型在思考过程中始终与视觉证据保持一致，VER框架为实现‘边看边思考’的多模态模型提供了有效路径。

Abstract: Video reasoning, the task of enabling machines to infer from dynamic visual
content through multi-step logic, is crucial for advanced AI. While the
Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,
its application to video understanding remains underexplored. This paper
presents a systematic analysis revealing that CoT often degrades performance in
video reasoning, generating verbose but misleading internal monologues, and
leading to hallucinated visual details and overridden correct intuitions - a
phenomenon we term "visual thinking drift". We explain this drift through a
Bayesian lens, positing that CoT traces often diverge from actual visual
evidence, instead amplifying internal biases or language priors, causing models
to storytell rather than engage in grounded reasoning. To counteract this, we
introduce Visual Evidence Reward (VER), a novel reinforcement learning
framework that explicitly rewards the generation of reasoning traces that are
verifiably grounded in visual evidence. Comprehensive evaluation across 10
diverse video understanding benchmarks demonstrates that our Video-VER
consistently achieves top performance. Our work sheds light on the distinct
challenges of video-centric reasoning and encourages the development of AI that
robustly grounds its inferences in visual evidence - for large multimodal
models that not only "think before answering", but also "see while thinking".

</details>


### [63] [A public cardiac CT dataset featuring the left atrial appendage](https://arxiv.org/abs/2510.06090)
*Bjoern Hansen,Jonas Pedersen,Klaus F. Kofoed,Oscar Camara,Rasmus R. Paulsen,Kristine Soerensen*

Main category: cs.CV

TL;DR: 本文提出了首个开源的、解剖学一致的高分辨率左心耳（LAA）、冠状动脉（CAs）和肺静脉（PVs）分割数据集，基于1000例心脏CT血管造影（CCTA）扫描，并改进了ImageCAS数据集的标签，旨在促进LAA形态学分析的研究。


<details>
  <summary>Details</summary>
Motivation: 尽管已有先进的分割框架如TotalSegmentator（TS），但在医学影像中对左心耳、冠状动脉和肺静脉的精确分割仍具挑战性，因此需要高质量、公开可用的数据集来推动相关研究。

Method: 使用专为高分辨率LAA分割设计的先进分割框架，在包含人工标注的大规模私有数据集上训练网络，并迁移到公开的ImageCAS数据集；CA标签在原始标注基础上优化，PV分割则从TS输出中细化，并提供存在常见数据缺陷的扫描列表。

Result: 生成了1000例CCTA扫描中LAA、CA和PV的高质量、解剖一致的分割结果，并补充了全心标签和数据质量问题列表，提升了数据可用性。

Conclusion: 该数据集为心脏结构尤其是LAA的形态学分析提供了可靠资源，有助于推动相关算法开发与临床研究。

Abstract: Despite the success of advanced segmentation frameworks such as
TotalSegmentator (TS), accurate segmentations of the left atrial appendage
(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant
challenge in medical imaging. In this work, we present the first open-source,
anatomically coherent dataset of curated, high-resolution segmentations for
these structures, supplemented with whole-heart labels produced by TS on the
publicly available ImageCAS dataset consisting of 1000 cardiac computed
tomography angiography (CCTA) scans. One purpose of the data set is to foster
novel approaches to the analysis of LAA morphology.
  LAA segmentations on ImageCAS were generated using a state-of-the-art
segmentation framework developed specifically for high resolution LAA
segmentation. We trained the network on a large private dataset with manual
annotations provided by medical readers guided by a trained cardiologist and
transferred the model to ImageCAS data. CA labels were improved from the
original ImageCAS annotations, while PV segmentations were refined from TS
outputs. In addition, we provide a list of scans from ImageCAS that contains
common data flaws such as step artefacts, LAAs extending beyond the scanner's
field of view, and other types of data defects.

</details>


### [64] [Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2510.06098)
*Yinjian Wang,Wei Li,Yuanyuan Gui,Gemine Vivone*

Main category: cs.CV

TL;DR: 提出一种新的基于张量的高光谱超分辨率模型，通过解耦光谱低秩性和空间先验，并利用非凸模式混洗张量相关全变分共同建模多级空间先验，有效融合高光谱与多光谱图像。


<details>
  <summary>Details</summary>
Motivation: 现有张量方法难以同时整合多级先验信息，导致模型复杂度高、优化困难，本文旨在构建一个紧凑且能有效利用多级先验的融合模型。

Method: 采用块分解将高分辨率图像分解为光谱子空间和空间映射，构建空间张量并引入非凸模式混洗张量相关全变分来联合建模高阶低秩性和平滑性先验，使用线性化ADMM算法进行优化。

Result: 在多个数据集上验证了方法的有效性，优于现有方法，且算法具有理论收敛保证。

Conclusion: 所提模型能更有效地融合多级先验信息，提升融合性能，为高光谱图像超分辨率提供了一种高效可靠的解决方案。

Abstract: Fusing a hyperspectral image with a multispectral image acquired over the
same scene, \textit{i.e.}, hyperspectral image super-resolution, has become a
popular computational way to access the latent high-spatial-spectral-resolution
image. To date, a variety of fusion methods have been proposed, among which the
tensor-based ones have testified that multiple priors, such as multidimensional
low-rankness and spatial total variation at multiple levels, effectively drive
the fusion process. However, existing tensor-based models can only effectively
leverage one or two priors at one or two levels, since simultaneously
incorporating multi-level priors inevitably increases model complexity. This
introduces challenges in both balancing the weights of different priors and
optimizing multi-block structures. Concerning this, we present a novel
hyperspectral super-resolution model compactly characterizing these multi-level
priors of hyperspectral images within the tensor framework. Firstly, the
proposed model decouples the spectral low-rankness and spatial priors by
casting the latent high-spatial-spectral-resolution image into spectral
subspace and spatial maps via block term decomposition. Secondly, these spatial
maps are stacked as the spatial tensor encoding the high-order spatial
low-rankness and smoothness priors, which are co-modeled via the proposed
non-convex mode-shuffled tensor correlated total variation. Finally, we draw
inspiration from the linearized alternating direction method of multipliers to
design an efficient algorithm to optimize the resulting model, theoretically
proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments
on multiple datasets demonstrate the effectiveness of the proposed algorithm.
The code implementation will be available from https://github.com/WongYinJ.

</details>


### [65] [Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction](https://arxiv.org/abs/2510.06113)
*Shuo Jiang,Zhuwen Chen,Liaoman Xu,Yanming Zhu,Changmiao Wang,Jiong Zhang,Feiwei Qin,Yifei Chen,Zhu Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于原型的多模态框架FeatProto，用于改进癌症生存预测，通过整合全切片图像的全局与局部特征及基因组数据，提升模型的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型在临床中难以解释，且传统原型学习方法忽略肿瘤整体上下文，缺乏与基因组数据的语义对齐。

Method: 提出FeatProto框架，构建统一的特征原型空间，结合全局与局部WSI特征及基因组数据；采用指数原型更新策略（EMA ProtoUp）和分层原型匹配机制。

Result: 在四个公开癌症数据集上验证，FeatProto在准确性和可解释性方面优于当前主流单模态和多模态生存预测方法。

Conclusion: FeatProto有效提升了癌症生存预测的性能与可解释性，为医学领域的原型学习提供了新思路。

Abstract: Survival analysis plays a vital role in making clinical decisions. However,
the models currently in use are often difficult to interpret, which reduces
their usefulness in clinical settings. Prototype learning presents a potential
solution, yet traditional methods focus on local similarities and static
matching, neglecting the broader tumor context and lacking strong semantic
alignment with genomic data. To overcome these issues, we introduce an
innovative prototype-based multimodal framework, FeatProto, aimed at enhancing
cancer survival prediction by addressing significant limitations in current
prototype learning methodologies within pathology. Our framework establishes a
unified feature prototype space that integrates both global and local features
of whole slide images (WSI) with genomic profiles. This integration facilitates
traceable and interpretable decision-making processes. Our approach includes
three main innovations: (1) A robust phenotype representation that merges
critical patches with global context, harmonized with genomic data to minimize
local bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that
sustains stable cross-modal associations and employs a wandering mechanism to
adapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype
matching scheme designed to capture global centrality, local typicality, and
cohort-level trends, thereby refining prototype inference. Comprehensive
evaluations on four publicly available cancer datasets indicate that our method
surpasses current leading unimodal and multimodal survival prediction
techniques in both accuracy and interoperability, providing a new perspective
on prototype learning for critical medical applications. Our source code is
available at https://github.com/JSLiam94/FeatProto.

</details>


### [66] [Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework](https://arxiv.org/abs/2510.06123)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: SSGNet是一种结合类特定生成模型和半监督伪标签的统一框架，用于增强医学图像的分类与分割性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中常受限于标注数据稀缺且不平衡的问题。

Method: 通过StyleGAN3生成图像扩充训练数据，并采用迭代式半监督伪标签方法优化标签质量，从而提升现有基线模型的性能。

Result: 在多个医学图像基准实验中，分类与分割性能均稳定提升，FID分析表明生成样本质量高。

Conclusion: SSGNet是一种有效缓解标注瓶颈、提高医学图像分析鲁棒性的实用策略。

Abstract: Deep learning in medical imaging is often limited by scarce and imbalanced
annotated data. We present SSGNet, a unified framework that combines class
specific generative modeling with iterative semisupervised pseudo labeling to
enhance both classification and segmentation. Rather than functioning as a
standalone model, SSGNet augments existing baselines by expanding training data
with StyleGAN3 generated images and refining labels through iterative pseudo
labeling. Experiments across multiple medical imaging benchmarks demonstrate
consistent gains in classification and segmentation performance, while Frechet
Inception Distance analysis confirms the high quality of generated samples.
These results highlight SSGNet as a practical strategy to mitigate annotation
bottlenecks and improve robustness in medical image analysis.

</details>


### [67] [Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation](https://arxiv.org/abs/2510.06131)
*Jiawei Mao,Yuhan Wang,Lifeng Chen,Can Zhao,Yucheng Tang,Dong Yang,Liangqiong Qu,Daguang Xu,Yuyin Zhou*

Main category: cs.CV

TL;DR: MeDiM是首个跨模态统一的医疗离散扩散模型，通过多模态大语言模型作为扩散主干，实现医学图像与文本间的高保真生成与联合配对输出。


<details>
  <summary>Details</summary>
Motivation: 现有生成式医疗模型受限于模态特异性设计，难以整合影像、病理和临床文本等多模态数据，阻碍了其向通用医学基础模型的发展。

Method: 提出MeDiM，基于离散扩散框架，采用多模态大语言模型作为扩散主干，去除因果注意力掩码以实现双向上下文建模，并引入连续时间步嵌入增强扩散感知，统一建模跨模态共享分布。

Result: 在MIMIC-CXR和PathGen上分别取得FID 16.60和24.19的生成质量，在报告生成任务中METEOR达0.2650；联合生成的图文对显著提升下游性能（如BLEU-3提升31.58%）。

Conclusion: MeDiM实现了无需模态特定组件的跨模态医学生成，支持连贯且临床合理的多模态输出，推动了通用医学基础模型的发展。

Abstract: Recent advances in generative medical models are constrained by
modality-specific scenarios that hinder the integration of complementary
evidence from imaging, pathology, and clinical notes. This fragmentation limits
their evolution into foundation models that can learn and reason across the
full spectrum of biomedical data. We propose MeDiM, the first medical discrete
diffusion model that learns shared distributions across modalities without
modality-specific components. MeDiM unifies multiple generative tasks:
translating between images and text, and jointly producing image-report pairs
across domains in response to prompts. Built on a discrete diffusion framework,
MeDiM bridges vision and language representations through a shared
probabilistic space. To enable unified and flexible medical generation, we
employ a multimodal large language model (MLLM) as the diffusion backbone,
leveraging its prior knowledge and cross-modal reasoning. Two key designs are
introduced: (1) removing the causal attention mask for bidirectional context,
and (2) injecting continuous timestep embeddings for diffusion awareness.
Experiments demonstrate high-fidelity medical generation (FID 16.60 on
MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR
0.2650 and 0.2580). Jointly generated image-report pairs further enhance
downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,
plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports
coherent and clinically grounded multimodal outputs.

</details>


### [68] [Deforming Videos to Masks: Flow Matching for Referring Video Segmentation](https://arxiv.org/abs/2510.06139)
*Zanyi Wang,Dengyang Jiang,Liuzhuozheng Li,Sizhe Dang,Chengzu Li,Harry Yang,Guang Dai,Mengmeng Wang,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出FlowRVS，一种将指代表情视频对象分割（RVOS）重构为条件连续流问题的新框架，通过语言引导的视频整体表示到目标掩码的直接变形，实现细粒度像素控制、文本-视频语义对齐和时间一致性，在多个基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法多采用“定位-分割”级联流程，存在语义信息瓶颈和时序不一致问题，难以有效将语言描述与视频像素动态对齐。

Method: 将RVOS视为条件连续流问题，利用预训练T2V模型，学习从视频整体表征到目标掩码的语言引导直接变形，实现端到端的一阶段生成式分割。

Result: 在MeViS上取得51.1的J&F分数（+1.6超越先前SOTA），在零样本Ref-DAVIS17上达到73.3（+2.7），显著提升性能。

Conclusion: 建模为连续变形过程能有效提升RVOS的语义对齐与时间一致性，FlowRVS为视频理解任务提供了新范式。

Abstract: Referring Video Object Segmentation (RVOS) requires segmenting specific
objects in a video guided by a natural language description. The core challenge
of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels
and continuously segment them through the complex dynamics of a video. Faced
with this difficulty, prior work has often decomposed the task into a pragmatic
`locate-then-segment' pipeline. However, this cascaded design creates an
information bottleneck by simplifying semantics into coarse geometric prompts
(e.g, point), and struggles to maintain temporal consistency as the segmenting
process is often decoupled from the initial language grounding. To overcome
these fundamental limitations, we propose FlowRVS, a novel framework that
reconceptualizes RVOS as a conditional continuous flow problem. This allows us
to harness the inherent strengths of pretrained T2V models, fine-grained pixel
control, text-video semantic alignment, and temporal coherence. Instead of
conventional generating from noise to mask or directly predicting mask, we
reformulate the task by learning a direct, language-guided deformation from a
video's holistic representation to its target mask. Our one-stage, generative
approach achieves new state-of-the-art results across all major RVOS
benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in
MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),
demonstrating the significant potential of modeling video understanding tasks
as continuous deformation processes.

</details>


### [69] [Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images](https://arxiv.org/abs/2510.06145)
*Aditya Prakash,David Forsyth,Saurabh Gupta*

Main category: cs.CV

TL;DR: 提出了一种从单张图像预测双手机械3D运动和姿态的方法，利用扩散模型将2D关键点序列提升为4D手部运动，并采用扩散损失处理手部运动的多模态分布。


<details>
  <summary>Details</summary>
Motivation: 缺乏在多样化场景中的3D手部标注数据，难以准确预测日常环境中双手机械的3D运动与姿态。

Method: 设计了一个基于扩散模型的标注流程，将2D手部关键点序列升维至4D手部运动；在预测模型中引入扩散损失以应对手部运动分布的多模态性。

Result: 在6个数据集上的实验表明，使用合成标签训练显著提升了性能（提高14%），所提出的提升模型比基线好42%，预测模型增益达16.4%，尤其在日常图像的零样本泛化中表现优异。

Conclusion: 该方法有效解决了真实场景中3D手部动作预测的数据稀缺问题，在多样化的数据上训练并结合扩散模型能显著提升预测精度和泛化能力。

Abstract: We tackle the problem of forecasting bimanual 3D hand motion & articulation
from a single image in everyday settings. To address the lack of 3D hand
annotations in diverse settings, we design an annotation pipeline consisting of
a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the
forecasting model, we adopt a diffusion loss to account for the multimodality
in hand motion distribution. Extensive experiments across 6 datasets show the
benefits of training on diverse data with imputed labels (14% improvement) and
effectiveness of our lifting (42% better) & forecasting (16.4% gain) models,
over the best baselines, especially in zero-shot generalization to everyday
images.

</details>


### [70] [ShapeGen4D: Towards High Quality 4D Shape Generation from Videos](https://arxiv.org/abs/2510.06208)
*Jiraphon Yenphraphai,Ashkan Mirzaei,Jianqi Chen,Jiaxu Zou,Sergey Tulyakov,Raymond A. Yeh,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练3D模型的端到端视频到4D形状生成框架，通过时序注意力、时间感知采样与噪声共享机制，实现了高质量、时间一致的动态3D形状重建。


<details>
  <summary>Details</summary>
Motivation: 现有的视频条件4D形状生成方法通常依赖逐帧优化，难以保持时间一致性且对复杂运动建模能力有限，因此需要一种能直接从视频生成连续、动态3D表示的端到端方法。

Method: 提出一个基于大规模预训练3D模型的框架，包含三个关键组件：(i) 时序注意力机制，利用所有视频帧信息生成时间索引的动态表示；(ii) 时间感知点采样与4D潜在锚定，提升几何与纹理的时间一致性；(iii) 跨帧噪声共享机制，增强生成结果的时间稳定性。

Result: 该方法在真实世界视频上表现出更强的鲁棒性和感知保真度，显著减少失败案例，能够准确捕捉非刚性运动、体积变化甚至拓扑结构变化，且无需逐帧优化。

Conclusion: 所提出的方法实现了从单个视频到动态3D形状的高效、一致且高质量生成，推动了视频驱动4D建模的发展。

Abstract: Video-conditioned 4D shape generation aims to recover time-varying 3D
geometry and view-consistent appearance directly from an input video. In this
work, we introduce a native video-to-4D shape generation framework that
synthesizes a single dynamic 3D representation end-to-end from the video. Our
framework introduces three key components based on large-scale pre-trained 3D
models: (i) a temporal attention that conditions generation on all frames while
producing a time-indexed dynamic representation; (ii) a time-aware point
sampling and 4D latent anchoring that promote temporally consistent geometry
and texture; and (iii) noise sharing across frames to enhance temporal
stability. Our method accurately captures non-rigid motion, volume changes, and
even topological transitions without per-frame optimization. Across diverse
in-the-wild videos, our method improves robustness and perceptual fidelity and
reduces failure modes compared with the baselines.

</details>


### [71] [Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models](https://arxiv.org/abs/2510.06209)
*Jiahao Wang,Zhenpei Yang,Yijing Bai,Yingwei Li,Yuliang Zou,Bo Sun,Abhijit Kundu,Jose Lezama,Luna Yue Huang,Zehao Zhu,Jyh-Jing Hwang,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.CV

TL;DR: 本文提出Drive&Gen框架，结合生成式视频模型与端到端驾驶模型，通过可控制的虚拟环境评估生成视频的真实性，并利用合成数据提升自动驾驶模型在分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型虽能生成逼真视频，但其是否满足条件约束并足够真实以用于端到端驾驶模型评估仍不清楚；同时，缺乏对E2E模型偏见的理解和改善其分布外泛化能力的方法。

Method: 提出新的统计指标，利用E2E驾驶模型评估生成视频的真实性；利用视频生成模型的可控性进行针对性实验，分析影响E2E规划器性能的分布差距；使用生成模型产生的合成数据增强训练。

Result: 验证了生成视频在指定条件下的真实性可用于E2E planner评估；揭示了E2E模型的分布偏见；合成数据能有效提升E2E模型在未知场景中的泛化能力。

Conclusion: Drive&Gen框架成功连接了生成世界模型与驾驶模型，不仅提供了评估生成环境的新方法，还展示了合成数据在扩展自动驾驶应用范围中的潜力。

Abstract: Recent advances in generative models have sparked exciting new possibilities
in the field of autonomous vehicles. Specifically, video generation models are
now being explored as controllable virtual testing environments.
Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined
alternative to conventional modular autonomous driving systems, gaining
popularity for their simplicity and scalability. However, the application of
these techniques to simulation and planning raises important questions. First,
while video generation models can generate increasingly realistic videos, can
these videos faithfully adhere to the specified conditions and be realistic
enough for E2E autonomous planner evaluation? Second, given that data is
crucial for understanding and controlling E2E planners, how can we gain deeper
insights into their biases and improve their ability to generalize to
out-of-distribution scenarios? In this work, we bridge the gap between the
driving models and generative world models (Drive&Gen) to address these
questions. We propose novel statistical measures leveraging E2E drivers to
evaluate the realism of generated videos. By exploiting the controllability of
the video generation model, we conduct targeted experiments to investigate
distribution gaps affecting E2E planner performance. Finally, we show that
synthetic data produced by the video generation model offers a cost-effective
alternative to real-world data collection. This synthetic data effectively
improves E2E model generalization beyond existing Operational Design Domains,
facilitating the expansion of autonomous vehicle services into new operational
contexts.

</details>


### [72] [Fine-grained Defocus Blur Control for Generative Image Models](https://arxiv.org/abs/2510.06215)
*Ayush Shrivastava,Connelly Barnes,Xuaner Zhang,Lingzhi Zhang,Andrew Owens,Sohrab Amirghodsi,Eli Shechtman*

Main category: cs.CV

TL;DR: 提出一种基于EXIF数据的文本到图像扩散框架，实现对镜头模糊效果的精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型难以融入细粒度的相机元数据（如光圈设置），限制了对景深和模糊效果的精确控制。

Method: 通过模拟物理成像过程：先生成全聚焦图像，估计单目深度，用新型焦点距离Transformer预测合理对焦距离，再结合可微分镜头模糊模型生成虚化图像，并通过端到端训练使梯度反向传播整个流程。

Result: 实验表明该模型能根据内容和EXIF数据生成可控的虚化效果，在推理时实现用户对模糊程度的精细交互控制，同时保持场景内容不变。

Conclusion: 该方法实现了无需显式监督的可学习虚化生成，显著提升了文本到图像模型在摄影级渲染和用户可控性方面的表现。

Abstract: Current text-to-image diffusion models excel at generating diverse,
high-quality images, yet they struggle to incorporate fine-grained camera
metadata such as precise aperture settings. In this work, we introduce a novel
text-to-image diffusion framework that leverages camera metadata, or EXIF data,
which is often embedded in image files, with an emphasis on generating
controllable lens blur. Our method mimics the physical image formation process
by first generating an all-in-focus image, estimating its monocular depth,
predicting a plausible focus distance with a novel focus distance transformer,
and then forming a defocused image with an existing differentiable lens blur
model. Gradients flow backwards through this whole process, allowing us to
learn without explicit supervision to generate defocus effects based on content
elements and the provided EXIF data. At inference time, this enables precise
interactive user control over defocus effects while preserving scene contents,
which is not achievable with existing diffusion models. Experimental results
demonstrate that our model enables superior fine-grained control without
altering the depicted scene.

</details>


### [73] [EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark](https://arxiv.org/abs/2510.06218)
*Deheng Zhang,Yuqian Fu,Runyi Yang,Yang Miao,Tianwen Qian,Xu Zheng,Guolei Sun,Ajad Chhatkuli,Xuanjing Huang,Yu-Gang Jiang,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 本文提出了EgoNight，首个针对夜间自我中心视觉理解的综合基准，核心任务为视觉问答（VQA），并通过昼夜对齐视频提升夜间标注质量，揭示了现有模型在低光条件下的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心视觉基准多关注白天场景，忽视了实际应用中不可避免的低光照条件，缺乏对夜间视觉理解的有效评估。

Method: 收集了Blender渲染的合成视频和真实世界录制的昼夜对齐视频，构建EgoNight-VQA数据集，并开发了日间增强夜间自动标注引擎，结合大量人工验证确保标注可靠性。

Result: EgoNight-VQA包含90个视频中的3658个问答对，涵盖12种不同类型的问答，评估显示现有最先进多模态大模型在从白天转移到夜间时性能显著下降。此外还引入了两个辅助任务：昼夜对应检索和夜间自我中心深度估计。

Conclusion: EgoNight-VQA为推动面向应用的自我中心视觉研究提供了坚实基础，有助于开发能在不同光照条件下泛化的模型。

Abstract: Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.

</details>


### [74] [Human3R: Everyone Everywhere All at Once](https://arxiv.org/abs/2510.06219)
*Yue Chen,Xingyu Chen,Yuxuan Xue,Anpei Chen,Yuliang Xiu,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: Human3R是一个统一的、前馈式的在线4D人体-场景重建框架，能够在单次前向传播中同时恢复多人SMPL-X模型、3D场景和相机轨迹，具有实时性、低内存消耗和无需复杂预处理的优点。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多阶段流水线、迭代优化和大量前置模块（如人体检测、深度估计、SLAM），限制了效率与实用性，因此需要一个简洁、端到端的统一模型。

Method: 基于CUT3R构建，采用参数高效的视觉提示调优（visual prompt tuning），在保留其时空先验的同时直接输出多个SMPL-X人体模型，实现单阶段、全局的联合重建。

Result: 在仅使用小型合成数据集BEDLAM训练一天的情况下，达到15 FPS的实时速度和8 GB内存占用，并在人体运动估计、人体网格重建、深度估计和相机位姿等任务上表现出SOTA或具竞争力的结果。

Conclusion: Human3R提供了一个简单而强大的统一模型基准，无需迭代优化或多模块依赖，可高效实现多人4D人体-场景重建，易于扩展至下游应用。

Abstract: We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos. Unlike previous approaches that rely on multi-stage
pipelines, iterative contact-aware refinement between humans and scenes, and
heavy dependencies, e.g., human detection, depth estimation, and SLAM
pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies
("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a
single forward pass ("all-at-once"). Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates
heavy dependencies and iterative refinement. After being trained on the
relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it
achieves superior performance with remarkable efficiency: it reconstructs
multiple humans in a one-shot manner, along with 3D scenes, in one stage, at
real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model. We hope that Human3R will serve as a simple yet strong
baseline, be easily extended for downstream applications.Code available in
https://fanegg.github.io/Human3R

</details>


### [75] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 本文提出了一种系统性评估文本到图像（TTI）扩散模型在历史场景生成中准确性的方法，并构建了包含3万张合成图像的HistVis数据集，发现现有模型普遍存在风格刻板化、时代错位和人口分布失真等问题。


<details>
  <summary>Details</summary>
Motivation: 随着TTI模型在内容创作中的广泛应用，其对历史背景的呈现准确性尚缺乏深入研究，亟需评估其历史真实性以减少文化误传。

Method: 构建HistVis数据集，使用三种先进的扩散模型生成涵盖不同历史时期的人类活动图像，并从隐含风格关联、历史一致性及人口代表性三个方面进行评估。

Result: 发现TTI模型在生成历史图像时存在系统性偏差：倾向于加入未指明的时代风格特征、引入现代物品等时代错位现象，且生成的种族与性别分布不符合历史实际。

Conclusion: 当前TTI模型在历史语境再现上存在显著缺陷，需通过可扩展的评估基准推动更符合历史与文化真实性的模型发展。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [Collaborative and Proactive Management of Task-Oriented Conversations](https://arxiv.org/abs/2510.05110)
*Arezoo Saedi,Afsaneh Fatemi,Mohammad Ali Nematbakhsh,Sophie Rosset,Anne Vilnat*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息状态方法的任务导向对话系统模型，利用大语言模型的上下文学习能力，通过构建中间信息和规划对话动作来提升任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话系统在目标感知规划方面存在不足，缺乏有效的主动规划机制，影响任务完成效果。

Method: 采用信息状态方法建模对话管理，定义预设槽位和文本部分信息组件来表示用户偏好，识别关键情境并构建相应的信息组件，形成有限的信息状态及状态间的对话动作，并设计更新策略，结合大语言模型的上下文学习实现数据库查询与实体排序。

Result: 在MultiWOZ数据集的单领域完整测试对话上评估显示，该模型在告知率（inform）和成功率（success）方面均达到最高水平，并优于先前方法。

Conclusion: 所提出的基于中间信息构建和信息状态转移的对话管理模型有效提升了任务导向对话系统的性能，验证了目标感知规划的重要性。

Abstract: Task oriented dialogue systems (TOD) complete particular tasks based on user
preferences across natural language interactions. Considering the impressive
performance of large language models (LLMs) in natural language processing
(NLP) tasks, most of the latest TODs are centered on LLMs. While proactive
planning is crucial for task completion, many existing TODs overlook effective
goal-aware planning. This paper creates a model for managing task-oriented
conversations, conceptualized centered on the information state approach to
dialogue management. The created model incorporated constructive intermediate
information in planning. Initially, predefined slots and text part
informational components are created to model user preferences. Investigating
intermediate information, critical circumstances are identified. Informational
components corresponding to these circumstances are created. Possible
configurations for these informational components lead to limited information
states. Then, dialogue moves, which indicate movement between these information
states and the procedures that must be performed in the movements, are created.
Eventually, the update strategy is constructed. The created model is
implemented leveraging in-context learning of LLMs. In this model, database
queries are created centered on indicated predefined slots and the order of
retrieved entities is indicated centered on text part. This mechanism enables
passing the whole corresponding entities to the preferences in the order of
congruency. Evaluations exploiting the complete test conversations of MultiWOZ,
with no more than a domain in a conversation, illustrate maximal inform and
success, and improvement compared with previous methods.

</details>


### [77] [Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System](https://arxiv.org/abs/2510.05113)
*Nisheeth Joshi,Pragya Katyayan,Palak Arora*

Main category: cs.CL

TL;DR: 本文提出了一种基于监督学习的、针对古吉拉特语的机器翻译评价指标，通过25个特征训练了两个模型，并在1000个翻译输出上验证其与人工评分的相关性优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译评价方法在印度语言上表现不佳，因此需要为古吉拉特语等语言开发更有效的评估指标。

Method: 采用监督学习方法，构建基于25个特征的参考型MT评价模型，训练了两个神经网络模型（分别含6层和10层隐藏层，均训练500个epoch）。

Result: 在包含1000个翻译结果的数据集上测试表明，所提出的指标与人工评分的相关性优于其他现有指标。

Conclusion: 该研究成功开发出适用于古吉拉特语的MT评价指标，显著提升了与人类判断的相关性，为印度语言的MT评估提供了有效解决方案。

Abstract: Machine Translation (MT) Evaluation is an integral part of the MT development
life cycle. Without analyzing the outputs of MT engines, it is impossible to
evaluate the performance of an MT system. Through experiments, it has been
identified that what works for English and other European languages does not
work well with Indian languages. Thus, In this paper, we have introduced a
reference-based MT evaluation metric for Gujarati which is based on supervised
learning. We have trained two versions of the metric which uses 25 features for
training. Among the two models, one model is trained using 6 hidden layers with
500 epochs while the other model is trained using 10 hidden layers with 500
epochs. To test the performance of the metric, we collected 1000 MT outputs of
seven MT systems. These MT engine outputs were compared with 1 human reference
translation. While comparing the developed metrics with other available
metrics, it was found that the metrics produced better human correlations.

</details>


### [78] [Hallucination is Inevitable for LLMs with the Open World Assumption](https://arxiv.org/abs/2510.05116)
*Bowen Xu*

Main category: cs.CL

TL;DR: 本文重新定义大语言模型中的“幻觉”问题，认为其是开放世界中泛化问题的必然表现，而非单纯的工程缺陷，并提出应将其视为需与人类智能兼容的结构性特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究将幻觉视为需消除的缺陷或理论必然，但缺乏在通用人工智能背景下对其根本性质的深入探讨。

Method: 通过区分封闭世界与开放世界的假设，构建幻觉分类框架，分析其可纠正性与不可避免性。

Result: 论证了在开放世界条件下幻觉的不可避免性，提出了新的幻觉分类方法，并强调其作为结构性特征的意义。

Conclusion: 幻觉不应仅被视为缺陷，而应被理解为模型泛化的自然产物，在追求AGI的过程中需容忍并与之共存。

Abstract: Large Language Models (LLMs) exhibit impressive linguistic competence but
also produce inaccurate or fabricated outputs, often called ``hallucinations''.
Engineering approaches usually regard hallucination as a defect to be
minimized, while formal analyses have argued for its theoretical inevitability.
Yet both perspectives remain incomplete when considering the conditions
required for artificial general intelligence (AGI). This paper reframes
``hallucination'' as a manifestation of the generalization problem. Under the
Closed World assumption, where training and test distributions are consistent,
hallucinations may be mitigated. Under the Open World assumption, however,
where the environment is unbounded, hallucinations become inevitable. This
paper further develops a classification of hallucination, distinguishing cases
that may be corrected from those that appear unavoidable under open-world
conditions. On this basis, it suggests that ``hallucination'' should be
approached not merely as an engineering defect but as a structural feature to
be tolerated and made compatible with human intelligence.

</details>


### [79] [MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation](https://arxiv.org/abs/2510.05124)
*Mingjin Li,Yu Liu,Huayi Liu,Xiang Ye,Chao Jiang,Hongguang Zhang*

Main category: cs.CL

TL;DR: MADS是一个通过多智能体自对弈生成多轮说服性对话的可扩展框架，利用用户、对话和优化三个智能体协同工作，结合CoA建模和LLM评估，低成本生成训练数据，在真实营销场景中显著提升小规模LLM的说服能力与业务转化率。


<details>
  <summary>Details</summary>
Motivation: 解决行业中缺乏用户数据、冷启动评估困难和提示效率低等挑战，实现无需人工标注的高质量对话数据生成。

Method: 设计包含用户代理、对话代理和优化代理的三代理框架，通过自我对弈生成对话，并引入Chain-of-Attitude建模和专用LLM进行说服力评估与优化。

Result: 在真实营销应用中，MADS使小型LLM的有机流量转化率从1.83%提升至2.24%，增幅达22.4%。

Conclusion: MADS能有效提升小规模语言模型在实际场景中的说服性能，具备显著的商业价值和数据生成效率优势。

Abstract: We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for
generating persuasive multi-turn dialogues via agent self-play. MADS employs
three coordinated agents: User Agents simulating diverse persona-driven
behaviors, a Dialog Agent executing task-oriented persuasion strategies and an
Optimization Agent evaluating and refining dialogue outcomes. We further
validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and
dedicated LLMs' persuasion assessment. This approach enables low-cost
generation of training data without human annotation, addressing key industry
challenges such as lack of user data, cold-start evaluation difficulties, and
prompt inefficiency. Applied to a real-world marketing scenario, MADS
significantly improved the persuasion capacity of small LLMs, increasing the
organic traffic conversion rate by 22.4\% (from 1.83\% to 2.24\%) ,
demonstrating clear business value.

</details>


### [80] [Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models](https://arxiv.org/abs/2510.05121)
*Durgesh Nandini,Rebekka Koch,Mirco Schoenfeld*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）在从区域贸易协定文本中零样本、单样本和少样本提取经济学领域主谓宾三元组的有效性，使用Llama 3.1模型进行实验，并评估其在构建经济知识图谱中的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 为了实现从自然语言法律贸易文本中自动构建结构化经济知识图谱，提升信息提取效率与可扩展性，研究探索大语言模型在经济学领域的适用性。

Method: 采用Llama 3.1模型，结合零样本、单样本和少样本提示技术，引入正例与反例，对非结构化的区域贸易协定文本进行处理，提取Subject-Predicate-Object三元组。

Result: 模型在不同提示设置下能够有效提取贸易相关三元组，定量与定性评估显示少样本提示结合正负例效果较优，但仍面临歧义、上下文依赖和格式一致性等挑战。

Conclusion: 大语言模型在经济领域结构化知识提取中具有潜力，尤其在知识图谱构建方面，未来可通过优化提示工程和模型微调进一步提升性能。

Abstract: This study investigates the effectiveness of Large Language Models (LLMs) for
the extraction of structured knowledge in the form of Subject-Predicate-Object
triples. We apply the setup for the domain of Economics application. The
findings can be applied to a wide range of scenarios, including the creation of
economic trade knowledge graphs from natural language legal trade agreement
texts. As a use case, we apply the model to regional trade agreement texts to
extract trade-related information triples. In particular, we explore the
zero-shot, one-shot and few-shot prompting techniques, incorporating positive
and negative examples, and evaluate their performance based on quantitative and
qualitative metrics. Specifically, we used Llama 3.1 model to process the
unstructured regional trade agreement texts and extract triples. We discuss key
insights, challenges, and potential future directions, emphasizing the
significance of language models in economic applications.

</details>


### [81] [CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation](https://arxiv.org/abs/2510.05122)
*Jie Zhu,Yuanchen Zhou,Shuo Jiang,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出CARE框架，通过强化原始训练集上的认知推理和使用强化学习来提升情感支持对话中回应的逻辑性和支持性质量，无需依赖大规模合成数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于数据增强和合成语料库构建，忽视了有效情感支持背后的深层认知推理过程。

Method: 提出CARE框架，利用原始ESC训练集引导模型生成逻辑连贯且具支持性的回应，并结合强化学习进一步优化推理过程。

Result: 实验结果表明，CARE显著提升了回应的逻辑合理性和支持质量。

Conclusion: CARE推动了共情、认知稳健且类人的情感支持系统的发展。

Abstract: Emotional Support Conversation (ESC) plays a vital role in alleviating
psychological stress and providing emotional value through dialogue. While
recent studies have largely focused on data augmentation and synthetic corpus
construction, they often overlook the deeper cognitive reasoning processes that
underpin effective emotional support. To address this gap, we propose
\textbf{CARE}, a novel framework that strengthens reasoning in ESC without
relying on large-scale synthetic data. CARE leverages the original ESC training
set to guide models in generating logically coherent and supportive responses,
thereby explicitly enhancing cognitive reasoning. Building on this foundation,
we further employ reinforcement learning to refine and reinforce the reasoning
process. Experimental results demonstrate that CARE significantly improves both
the logical soundness and supportive quality of responses, advancing the
development of empathetic, cognitively robust, and human-like emotional support
systems.

</details>


### [82] [KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance](https://arxiv.org/abs/2510.05524)
*Kuangshi Ai,Jonathan A. Karr Jr,Meng Jiang,Nitesh V. Chawla,Chaoli Wang*

Main category: cs.CL

TL;DR: 提出了一种名为KEO的领域特定知识提取与推理框架，结合知识图谱与检索增强生成（RAG），在安全关键场景中提升大语言模型的全局理解能力。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，传统基于文本块的检索增强生成（RAG）难以实现跨数据集的全局推理，需要更结构化的知识表示以支持系统级洞察和维护决策。

Method: 构建了一个基于操作与维护智能（OMIn）数据集的问答基准，提出KEO框架：通过从文本中提取结构化知识图谱（KG），并将其集成到检索增强生成（RAG）流程中，实现更连贯、跨文档的推理。使用本地部署的大语言模型（如Gemma-3、Phi-4、Mistral-Nemo）进行评估，并用更强模型（如GPT-4o、Llama-3.3）作为评判器。

Result: 实验表明，KEO显著提升了全局态势感知能力，能够揭示数据中的模式和系统级洞察；而传统文本块RAG在需要局部检索的细粒度任务上仍具优势。

Conclusion: 知识图谱增强的LLM在安全关键领域的问答和高风险推理中具有巨大潜力，相比传统RAG更能支持跨文档的深层理解与决策。

Abstract: We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge
extraction and reasoning framework with large language models (LLMs) in
safety-critical contexts. Using the Operations and Maintenance Intelligence
(OMIn) dataset, we construct a QA benchmark spanning global sensemaking and
actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and
integrates it into a retrieval-augmented generation (RAG) pipeline, enabling
more coherent, dataset-wide reasoning than traditional text-chunk RAG. We
evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ
stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO
markedly improves global sensemaking by revealing patterns and system-level
insights, while text-chunk RAG remains effective for fine-grained procedural
tasks requiring localized retrieval. These findings underscore the promise of
KG-augmented LLMs for secure, domain-specific QA and their potential in
high-stakes reasoning.

</details>


### [83] [Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation](https://arxiv.org/abs/2510.05125)
*Reza Shirkavand,Xiaokai Wei,Chen Wang,Zheng Hui,Heng Huang,Michelle Gong*

Main category: cs.CL

TL;DR: 本文提出了IDIOMoE模型，通过将物品交互历史视为语言空间中的一种原生方言，融合协同过滤与大语言模型的优势，实现对推荐系统中文本和物品信号的统一建模。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统需要同时具备协同过滤的高效性与大语言模型的语义理解能力，以满足用户对自然语言查询和可解释性的更高需求。然而，协同信号语义不透明，而大语言模型难以仅从文本输入中捕捉隐式用户偏好，因此亟需一种统一的建模方法。

Method: 提出Item-ID + Oral-language Mixture-of-Experts Language Model（IDIOMoE），在预训练大语言模型的每一层前馈网络中分离文本专家和物品专家，并通过token类型门控机制避免文本与物品模态之间的干扰，将物品交互历史作为语言的一种‘方言’进行建模。

Result: IDIOMoE在多个公开和私有数据集上均展现出强大的推荐性能，同时保持了预训练模型的文本理解能力。

Conclusion: IDIOMoE有效融合了协同过滤与大语言模型的优势，为推荐系统提供了一种既能利用用户行为数据又能理解自然语言的统一架构。

Abstract: While collaborative filtering delivers predictive accuracy and efficiency,
and Large Language Models (LLMs) enable expressive and generalizable reasoning,
modern recommendation systems must bring these strengths together. Growing user
expectations, such as natural-language queries and transparent explanations,
further highlight the need for a unified approach. However, doing so is
nontrivial. Collaborative signals are often token-efficient but semantically
opaque, while LLMs are semantically rich but struggle to model implicit user
preferences when trained only on textual inputs. This paper introduces Item-ID
+ Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item
interaction histories as a native dialect within the language space, enabling
collaborative signals to be understood in the same way as natural language. By
splitting the Feed Forward Network of each block of a pretrained LLM into a
separate text expert and an item expert with token-type gating, our method
avoids destructive interference between text and catalog modalities. IDIOMoE
demonstrates strong recommendation performance across both public and
proprietary datasets, while preserving the text understanding of the pretrained
model.

</details>


### [84] [Improving Metacognition and Uncertainty Communication in Language Models](https://arxiv.org/abs/2510.05126)
*Mark Steyvers,Catarina Belem,Padhraic Smyth*

Main category: cs.CL

TL;DR: 通过监督微调可以提升大语言模型在不同任务和领域中表达不确定性的能力，尤其是多任务微调能带来更广泛的改进，但不同的元认知技能之间不会自然迁移，需联合训练。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在决策场景中广泛应用，但其缺乏对输出结果的置信度信号，可能导致用户误用错误答案。尽管模型内部存在不确定性信号，但其显式表达的置信度通常校准不佳，难以区分正确与错误答案。因此需要研究如何提升模型表达不确定性的能力。

Method: 对两种大语言模型进行监督微调，使用涵盖常识、数学和开放性问答的数据集，并评估两种元认知任务：单题置信度估计和成对置信度比较。同时测试模型在未见领域（如医学和法律推理）上的泛化能力。

Result: 微调显著提升了模型的置信度校准性和判别力，且跨领域表现良好，但准确率未变。单任务训练无法迁移到另一任务，而多任务微调在域外评估中表现出更低的校准误差和更强的判别能力。

Conclusion: 大语言模型的不确定性表达能力是可训练且可泛化的，但不同元认知技能之间缺乏自然互补性，必须通过多任务学习共同培养。

Abstract: Large language models (LLMs) are increasingly used in decision-making
contexts, but when they present answers without signaling low confidence, users
may unknowingly act on erroneous outputs. While prior work shows that LLMs
maintain internal uncertainty signals, their explicit verbalized confidence is
typically miscalibrated and poorly discriminates between correct and incorrect
answers. Across two types of LLMs, we investigate whether supervised finetuning
can improve models' ability to communicate uncertainty and whether such
improvements generalize across tasks and domains. We finetune the LLMs on
datasets spanning general knowledge, mathematics, and open-ended trivia, and
evaluate two metacognitive tasks: (1) single-question confidence estimation,
where the model assigns a numeric certainty to its answer, and (2) pairwise
confidence comparison, where the model selects which of two answers it is more
likely to have correct. We assess generalization to unseen domains, including
medical and legal reasoning. Results show that finetuning improves calibration
(alignment between stated confidence and accuracy) and discrimination (higher
confidence for correct vs. incorrect responses) within and across domains,
while leaving accuracy unchanged. However, improvements are task-specific:
training on single-question calibration does not transfer to pairwise
comparison, and vice versa. In contrast, multitask finetuning on both forms of
metacognition yields broader gains, producing lower calibration error and
stronger discrimination in out-of-domain evaluations. These results show that
while uncertainty communication in LLMs is trainable and generalizable,
different metacognitive skills do not naturally reinforce one another and must
be developed together through multitask training.

</details>


### [85] [Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction](https://arxiv.org/abs/2510.06198)
*Xinyu Guo,Zhengliang Shi,Minglai Yang,Mahdi Rahimi,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CogRE的关系抽取框架，结合认知启发的推理机制和强化学习优化，显著提升了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统关系抽取缺乏对语言化解释的监督，导致模型注意力不集中且少样本学习能力有限。

Method: 将关系抽取建模为受认知科学启发的文本处理流程，并通过强化学习与新型奖励函数联合优化准确率与解释质量；利用大语言模型自动构建高质量词典以提取关键关系词。

Result: 在One-shot NYT29数据集上，CogRE配合Qwen2.5-15B-Instruct达到24.65% F1值，经RL优化后绝对提升23.46%；人工评估显示解释质量相对提升54%。

Conclusion: CogRE通过结构化推理与强化学习有效改善了少样本场景下的关系抽取性能与解释可信度。

Abstract: This paper introduces a framework for relation extraction (RE) that enhances
both accuracy and explainability. The framework has two key components: (i) a
reasoning mechanism that formulates relation extraction as a series of
text-processing steps inspired by cognitive science, and (ii) an optimization
process driven by reinforcement learning (RL) with a novel reward function
designed to improve both task accuracy and explanation quality. We call our
approach CogRE. Our framework addresses the lack of supervision for
language-based explanations in traditional RE by promoting outputs that include
important relation keywords. These keywords are drawn from a high-quality
dictionary that is automatically constructed using an LLM. We evaluate our
approach for the task of one-shot RE using two LLMs and two RE datasets. Our
experiments show that CogRE improves explanation quality by addressing two
common failure patterns in one-shot RE: poor attention focus and limited
one-shot learning capability. For example, our cognitive-structured reasoning
with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing
prior reasoning-based designs. Optimizing this approach with RL using our
reward further improves performance by +23.46% (absolute). Finally, human
evaluation shows that our best model generates relational keywords closely
aligned with gold labels, increasing human explanation quality ratings by 54%
(relative).

</details>


### [86] [Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models](https://arxiv.org/abs/2510.05128)
*Si-Ioi Ng,Pranav S. Ambadi,Kimberly D. Mueller,Julie Liss,Visar Berisha*

Main category: cs.CL

TL;DR: 提出一种基于BERT的流水线方法，用于自动提取和排序Cookie Theft图片描述中的内容信息单元（CIU），有效表征视觉叙述路径，以评估认知障碍。


<details>
  <summary>Details</summary>
Motivation: 现有自动化评估认知语言障碍的方法常忽略说话者描述图片时的视觉叙述路径，且当前基于手工标注或词典映射的时空语义特征分析方法费时费力。

Method: 采用基于BERT的模型，结合二元交叉熵和成对排序损失进行微调，实现CIU的自动提取与顺序预测，并通过5折交叉验证评估性能。

Result: 在CIU检测上达到93%中位精确率和96%中位召回率，序列错误率为24%；所提取特征与真实标签有较强皮尔逊相关性，优于词典基线方法，且在ANCOVA分析中表现接近人工标注特征。

Conclusion: 该方法能有效刻画视觉叙述路径，可用于认知障碍的自动化评估，相关模型和代码已开源。

Abstract: Current methods for automated assessment of cognitive-linguistic impairment
via picture description often neglect the visual narrative path - the sequence
and locations of elements a speaker described in the picture. Analyses of
spatio-semantic features capture this path using content information units
(CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This
study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and
pairwise ranking loss, for automated CIU extraction and ordering from the
Cookie Theft picture description. Evaluated by 5-fold cross-validation, it
achieves 93% median precision, 96% median recall in CIU detection, and 24%
sequence error rates. The proposed method extracts features that exhibit strong
Pearson correlations with ground truth, surpassing the dictionary-based
baseline in external validation. These features also perform comparably to
those derived from manual annotations in evaluating group differences via
ANCOVA. The pipeline is shown to effectively characterize visual narrative
paths for cognitive impairment assessment, with the implementation and models
open-sourced to public.

</details>


### [87] [Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models](https://arxiv.org/abs/2510.05129)
*Qingshu Xu,Hong Jiao,Tianyi Zhou,Ming Li,Nan Zhang,Sydney Peters,Yanbin Fu*

Main category: cs.CL

TL;DR: 该研究评估了三种自动化范式在将测试题目与四个领域和十九个技能标签对齐方面的性能，发现基于BERT的模型（如DeBERTa-v3-base和RoBERTa-large）表现最佳，优于传统机器学习模型和集成方法。


<details>
  <summary>Details</summary>
Motivation: 为了确保大规模测评中分数解释的有效性，需要准确地将测试题目与内容标准对齐，因此研究自动化对齐方法具有重要意义。

Method: 采用了三种方法：基于嵌入的经典机器学习模型（结合降维分析）、八种BERT及其变体模型的微调、以及基于多数投票和堆叠的集成学习。

Result: DeBERTa-v3-base在领域对齐上达到0.950的加权F1分数，RoBERTa-large在技能对齐上达到0.869的F1分数；集成模型未超越最佳语言模型；降维提升了线性分类器性能，但仍不及语言模型。

Conclusion: 基于Transformer的语言模型在自动题目-标准对齐任务中表现最优，是实现高效精准对齐的可行方案。

Abstract: Accurate alignment of items to content standards is critical for valid score
interpretation in large-scale assessments. This study evaluates three automated
paradigms for aligning items with four domain and nineteen skill labels. First,
we extracted embeddings and trained multiple classical supervised machine
learning models, and further investigated the impact of dimensionality
reduction on model performance. Second, we fine-tuned eight BERT model and its
variants for both domain and skill alignment. Third, we explored ensemble
learning with majority voting and stacking with multiple meta-models. The
DeBERTa-v3-base achieved the highest weighted-average F1 score of 0.950 for
domain alignment while the RoBERTa-large yielded the highest F1 score of 0.869
for skill alignment. Ensemble models did not surpass the best-performing
language models. Dimension reduction enhanced linear classifiers based on
embeddings but did not perform better than language models. This study
demonstrated different methods in automated item alignment to content
standards.}

</details>


### [88] [Submodular Context Partitioning and Compression for In-Context Learning-short paper](https://arxiv.org/abs/2510.05130)
*Shaoyi Zheng,Canyu Zhang,Tianyi Zhou,Shengjie Wang*

Main category: cs.CL

TL;DR: 提出了一种名为Sub-CP的块感知上下文选择框架，利用子模目标控制块多样性，提升大语言模型中上下文学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的高效上下文学习方法在处理上下文分块时常常忽略信息冗余或表示不足的问题，导致性能次优。

Method: 提出Sub-CP框架，通过子模优化目标实现对上下文块多样性的精细控制，支持从全局多样性到局部连贯性的灵活选择策略，并允许预计算。

Result: 在多个数据集和任务上的实验表明，Sub-CP在不同规模的模型上均能持续提升上下文学习性能。

Conclusion: Sub-CP通过块感知的多样性控制，有效缓解了信息冗余与表示不足问题，显著提升了少样本上下文学习的效果。

Abstract: In-context learning (ICL) enables efficient few-shot learning in large
language models (LLMs) without training, but suffers from the quadratic input
complexity of transformers, limiting the maximum number of exemplars. While
various efficient ICL approaches partition the context into blocks to process
(e.g., ensembling, compression, cross-attention), they often ignore the
information redundancy or under-representation caused by different partition
strategies, leading to suboptimal performance. To tackle this problem, we
propose Sub-CP, a block-aware context selection framework that leverages
submodular objectives to control block diversity. Sub-CP supports a flexible
spectrum of selection strategies, allowing each block to range from globally
diverse to locally coherent. This allows fine-grained control over semantic
structure while enabling precomputation. Extensive experiments across diverse
tasks on multiple datasets show that Sub-CP consistently improves performance
across model scales.

</details>


### [89] [Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery](https://arxiv.org/abs/2510.05131)
*Bowen Wei*

Main category: cs.CL

TL;DR: 提出一种结合轻量级容错词汇检索、基于嵌入的向量相似性和受限大语言模型重排序的混合语义搜索系统，以解决Head Start项目在GoEngage平台上因术语复杂和搜索局限导致的任务查找困难。


<details>
  <summary>Details</summary>
Motivation: 新员工或轮岗员工难以在GoEngage平台主页上找到合适的任务模块，主要由于领域特定术语、系统专有命名以及传统词汇搜索对拼写错误和词语顺序变化处理能力有限。

Method: 采用混合语义搜索方法，结合容错词汇检索、向量相似性匹配和大语言模型约束重排序，并利用现有任务库和知识库基础设施，通过智能缓存、短列表生成和优雅降级机制提升效率与可靠性。

Result: 设计了一个包含资源需求、分阶段实施策略、离线评估协议（Hit@K, Precision@K, Recall@K, MRR）和在线测量方法（查询成功率、零结果率、停留时间代理指标）的完整框架。

Conclusion: 该混合搜索系统在保持低误报率、适应术语演变和经济高效运行的同时，显著提升了Head Start工作人员在复杂术语环境下查找任务的效率和准确性。

Abstract: Head Start programs utilizing GoEngage face significant challenges when new
or rotating staff attempt to locate appropriate Tasks (modules) on the platform
homepage. These difficulties arise from domain-specific jargon (e.g., IFPA,
DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent
limitations of lexical search in handling typos and varied word ordering. We
propose a pragmatic hybrid semantic search system that synergistically combines
lightweight typo-tolerant lexical retrieval, embedding-based vector similarity,
and constrained large language model (LLM) re-ranking. Our approach leverages
the organization's existing Task Repository and Knowledge Base infrastructure
while ensuring trustworthiness through low false-positive rates, evolvability
to accommodate terminological changes, and economic efficiency via intelligent
caching, shortlist generation, and graceful degradation mechanisms. We provide
a comprehensive framework detailing required resources, a phased implementation
strategy with concrete milestones, an offline evaluation protocol utilizing
curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online
measurement methodology incorporating query success metrics, zero-result rates,
and dwell-time proxies.

</details>


### [90] [Training Large Language Models To Reason In Parallel With Global Forking Tokens](https://arxiv.org/abs/2510.05132)
*Sheng Jia,Xiao Wang,Shiva Prasad Kasiviswanathan*

Main category: cs.CL

TL;DR: 本文提出了一种基于集合的监督微调方法（SSFT），通过引入全局损失函数和自监督二分匹配，有效保持了大模型在并行推理中的多样性与准确性，显著优于传统SFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在增加推理路径多样性时往往牺牲准确性，尤其在难题上难以触发既多样又正确的推理模式，因此需要一种能同时提升多样性和准确性的新方法。

Method: 将并行推理视为下一词预测的集合问题，在监督微调中引入基于集合的全局损失函数，利用自监督二分匹配对全局分叉词与独特推理轨迹进行对齐。

Result: 实验表明，SSFT在多个推理基准上均优于传统SFT，在Pass@1和Cons@k指标下表现更优，且能保留独特的推理模式并产生新兴的全局分叉词。

Conclusion: SSFT通过结构化的集合学习机制，有效解决了推理过程中多样性与准确性的权衡问题，为利用并行计算提升LLM推理能力提供了新思路。

Abstract: Although LLMs have demonstrated improved performance by scaling parallel
test-time compute, doing so relies on generating reasoning paths that are both
diverse and accurate. For challenging problems, the forking tokens that trigger
diverse yet correct reasoning modes are typically deep in the sampling tree.
Consequently, common strategies to encourage diversity, such as temperature
scaling, encounter a worsened trade-off between diversity and accuracy.
Motivated by this challenge, we treat parallel reasoning as a
set-of-next-token-prediction problem, and incorporate a set-based global loss
into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching
between our global forking tokens and unique reasoning traces. We observe that,
while naive fine-tuning with multiple reasoning traces collapses these unique
reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT),
preserves these modes and produces emergent global forking tokens. Experiments
on multiple reasoning benchmarks show that our SSFT consistently outperforms
SFT under both Pass@1 and Cons@k metrics.

</details>


### [91] [Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios](https://arxiv.org/abs/2510.05133)
*Y. Du,G. Wu,G. Tang,W. Wang,Q. Fan*

Main category: cs.CL

TL;DR: 本文研究了在不同合成数据比例下训练模型的表现、校准性和输出特性，发现最多20%的合成数据可保持性能稳定，超过30%后性能迅速下降，大模型对合成数据更具鲁棒性，且校准退化早于准确率下降，为实际应用提供了基于模型规模和任务需求的合成数据使用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管合成数据在现代NLP训练中广泛应用，但其比例对不同规模模型行为的影响尚缺乏系统理解。

Method: 使用Pythia模型系列（410M-12B参数）在五个不同任务上进行控制实验，评估1到3轮训练后合成数据比例从0%到50%的影响。

Result: 模型在合成数据比例达20%时性能稳定，超过30%后性能加速下降；大模型（6.9B-12B）比小模型（410M-1.4B）更稳健；校准退化先于准确率损失出现；推理任务比检索任务退化更快。当前最佳实践（如STaR和Self-Instruct）中高外部数据比例处于安全范围内。

Conclusion: 提出了基于模型规模和任务类型选择合成数据比例的实用指南，并指出校准性可作为合成数据训练中的早期预警指标。

Abstract: Synthetic data generated by large language models has become integral to
modern NLP training pipelines, from bootstrapping reasoning capabilities to
augmenting instruction-following datasets. While recent work demonstrates
successful applications maintaining high external data ratios, systematic
understanding of how synthetic data proportion affects model behavior across
different scales remains limited. This paper presents a controlled empirical
study examining model performance, calibration, and output characteristics when
trained on varying synthetic-to-external data ratios. Using the Pythia model
suite (410M-12B parameters) across five diverse tasks, we evaluate models after
one to three training iterations with synthetic data proportions ranging from
0-50\%. Our key findings include: models maintain stable performance with up to
20\% synthetic data, but degradation accelerates beyond 30\%; larger models
(6.9B-12B) show greater robustness to synthetic data than smaller models
(410M-1.4B); calibration degradation precedes accuracy loss, providing an early
warning signal; and task characteristics matter, with reasoning tasks degrading
faster than retrieval tasks under synthetic data training. Importantly, we find
that current best practices, such as those employed in STaR and Self-Instruct
systems that maintain greater than 80\% external data, operate well within safe
regimes identified by our experiments. We provide practical guidance for
practitioners on synthetic data budgets based on model scale and task
requirements, alongside detailed comparison with concurrent work including
Shumailov et al.'s model collapse findings.

</details>


### [92] [Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment](https://arxiv.org/abs/2510.05135)
*Vanya Bannihatti Kumar,Divyanshu Goyal,Akhil Eppa,Neel Bhandari*

Main category: cs.CL

TL;DR: 提出一种基于好奇心驱动的LLM-as-a-judge方法，用于个性化评估创造性写作，在TTCW基准上优于传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在主观创造性评估任务上表现不佳，且不同评价者间存在分歧，需更灵活的个性化评估方法。

Method: 采用好奇心驱动的LLM-as-a-judge框架，使模型学习个体评审者的判断偏好，并在TTCW基准上进行训练与评估。

Result: 在多种模型规模下，该方法在皮尔逊相关系数、Cohen's kappa和F1等指标上均优于基线SFT方法，尤其适用于评价者意见不一致的主观任务。

Conclusion: 所提方法能有效捕捉个体化创造性判断，提升主观创意评估的个性化与一致性。

Abstract: Modern large language models (LLMs) excel at objective tasks such as
evaluating mathematical reasoning and factual accuracy, yet they falter when
faced with the nuanced, subjective nature of assessing creativity. In this
work, we propose a novel curiosity-driven LLM-as-a-judge for evaluating
creative writing which is personlized to each individual's creative judgments.
We use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in
Chakrabarty et al. (2024), which has stories annotated by expert humans across
various subjective dimensions like Originality, to test our hypothesis. We show
that our method enables models across various sizes, to learn the nuanced
creative judgments of different individuals, by showing improvements over
baseline supervised finetuning(SFT) method across various evaluation metrics
like Pearson correlation, Cohen's and F1 values. Our method is especially
useful in subjective evaluations where not all the annotators agree with each
other.

</details>


### [93] [Linguistic Characteristics of AI-Generated Text: A Survey](https://arxiv.org/abs/2510.05136)
*Luka Terčon,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文综述了当前关于大语言模型生成文本的语言特征研究，系统归纳了现有工作的多维度分类，并总结了AI生成文本在风格、词汇多样性等方面的常见特征，指出了当前研究在语言和模型覆盖及提示敏感性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域的广泛应用，迫切需要系统梳理AI生成文本的语言特征，以理解其影响并指导未来研究方向。

Method: 通过多维度分类框架（包括语言描述层级、模型类型、文本体裁、语言种类和提示方式）对现有研究进行综合分析与归纳。

Result: 发现AI生成文本通常更正式、非个性化，名词、限定词和介词使用更多，形容词和副词较少；同时词汇多样性较低、重复性较高。当前研究主要集中于英语和GPT系列模型，且缺乏对提示敏感性的探讨。

Conclusion: 需要开展更多跨语言、跨模型及考虑提示多样性的研究，以全面理解AI生成文本的语言特征。

Abstract: Large language models (LLMs) are solidifying their position in the modern
world as effective tools for the automatic generation of text. Their use is
quickly becoming commonplace in fields such as education, healthcare, and
scientific research. There is a growing need to study the linguistic features
present in AI-generated text, as the increasing presence of such texts has
profound implications in various disciplines such as corpus linguistics,
computational linguistics, and natural language processing. Many observations
have already been made, however a broader synthesis of the findings made so far
is required to provide a better understanding of the topic. The present survey
paper aims to provide such a synthesis of extant research. We categorize the
existing works along several dimensions, including the levels of linguistic
description, the models included, the genres analyzed, the languages analyzed,
and the approach to prompting. Additionally, the same scheme is used to present
the findings made so far and expose the current trends followed by researchers.
Among the most-often reported findings is the observation that AI-generated
text is more likely to contain a more formal and impersonal style, signaled by
the increased presence of nouns, determiners, and adpositions and the lower
reliance on adjectives and adverbs. AI-generated text is also more likely to
feature a lower lexical diversity, a smaller vocabulary size, and repetitive
text. Current research, however, remains heavily concentrated on English data
and mostly on text generated by the GPT model family, highlighting the need for
broader cross-linguistic and cross-model investigation. In most cases authors
also fail to address the issue of prompt sensitivity, leaving much room for
future studies that employ multiple prompt wordings in the text generation
phase.

</details>


### [94] [Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics](https://arxiv.org/abs/2510.05137)
*Maojia Song,Renhang Liu,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Soujanya Poria,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了WebDetective，一个用于评估检索增强生成系统和网络代理的新型基准，解决了现有评测中推理路径泄露和单一通过率评估的问题。该基准结合无提示多跳问题和可控维基百科沙箱，并提出包含搜索充分性、知识利用和拒绝行为的综合评估框架。对25种前沿模型的评估揭示了当前系统在自主发现推理链方面的根本缺陷。为此，作者设计了EvidenceLoop代理工作流，通过验证循环和证据跟踪提升性能，证明该框架可指导实际架构改进。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统和网络代理在多跳深度搜索任务上的评估存在两大问题：一是问题文本中泄露推理路径，导致模型依赖表面线索而非自主发现推理链；二是仅用单次通过率评估，掩盖了失败原因的具体来源。因此需要一个更公平、可追溯且细粒度的评估框架。

Method: 提出WebDetective基准，包含无提示多跳问题和受控的Wikipedia沙箱环境，确保模型行为完全可追踪；设计分维度评估框架，分别衡量搜索充分性、知识利用效率和拒绝行为合理性；并在25个先进模型上进行实证评估。同时开发EvidenceLoop代理工作流，引入验证循环和系统性证据跟踪机制以应对识别出的挑战。

Result: 对25个SOTA模型的评估显示，尽管拥有足够证据，模型在知识利用方面表现差，且缺乏证据时几乎不进行适当拒绝；暴露出现有系统擅长执行给定推理路径但无法自主发现路径的根本缺陷；EvidenceLoop基线模型通过验证循环和证据跟踪显著提升了搜索与合成能力。

Conclusion: 当前RAG系统和网络代理在自主推理能力上存在本质不足，WebDetective及其诊断框架能有效揭示这些弱点，并指导构建真正自主的推理系统，而非仅依赖模式匹配的代理。

Abstract: RAG (Retrieval-Augmented Generation) systems and web agents are increasingly
evaluated on multi-hop deep search tasks, yet current practice suffers from two
major limitations. First, most benchmarks leak the reasoning path in the
question text, allowing models to follow surface cues rather than discover
reasoning chains autonomously. Second, evaluation is typically reduced to a
single pass rate, which collapses diverse behaviours into one score and
obscures whether failures stem from inadequate search, poor knowledge use, or
inappropriate refusal. To address these issues, we present WebDetective, a
benchmark of hint-free multi-hop questions paired with a controlled Wikipedia
sandbox that ensures full traceability of model actions, and a holistic
evaluation framework that separates search sufficiency, knowledge utilisation,
and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals
systematic weaknesses across all architectures: models struggle with knowledge
utilisation despite having sufficient evidence and demonstrate near-absent
appropriate refusal when evidence is lacking. These patterns expose a
fundamental gap: today's systems excel at executing given reasoning paths but
fail when required to discover them. We develop an agentic workflow,
EvidenceLoop, that explicitly targets the challenges our benchmark identifies,
incorporating verification loops and systematic evidence tracking that improve
both search and synthesis capabilities. This baseline demonstrates that
WebDetective's diagnostic framework can guide concrete architectural
improvements, establishing our benchmark as a critical tool for developing
genuinely autonomous reasoning systems rather than pattern-following agents.

</details>


### [95] [LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation](https://arxiv.org/abs/2510.05138)
*Gregory Hok Tjoan Go,Khang Ly,Anders Søgaard,Amin Tabatabaei,Maarten de Rijke,Xinyi Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为LiRA的多智能体协作框架，用于自动生成科学文献综述，能够在写作质量和引用准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学出版物快速增长使得文献综述难以保持全面和及时，现有自动化方法主要关注检索与筛选，而撰写阶段尤其在可读性和事实准确性方面研究不足。

Method: 设计了一个模拟人类综述过程的多智能体系统LiRA，包含负责内容提纲、段落撰写、编辑和审阅的专用智能体，通过协作生成连贯且全面的综述文章。

Result: 在SciReviewGen和ScienceDirect数据集上的实验表明，LiRA在写作质量和引用质量上优于AutoSurvey和MASS-Survey等基线方法，并保持与人工撰写综述相当的相似度；同时验证了其在真实检索场景下的有效性和对评审模型变化的鲁棒性。

Conclusion: 无需领域特定调优的基于智能体的LLM工作流有望提升自动化学术写作的可靠性与可用性。

Abstract: The rapid growth of scientific publications has made it increasingly
difficult to keep literature reviews comprehensive and up-to-date. Though prior
work has focused on automating retrieval and screening, the writing phase of
systematic reviews remains largely under-explored, especially with regard to
readability and factual accuracy. To address this, we present LiRA (Literature
Review Agents), a multi-agent collaborative workflow which emulates the human
literature review process. LiRA utilizes specialized agents for content
outlining, subsection writing, editing, and reviewing, producing cohesive and
comprehensive review articles. Evaluated on SciReviewGen and a proprietary
ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey
and MASS-Survey in writing and citation quality, while maintaining competitive
similarity to human-written reviews. We further evaluate LiRA in real-world
scenarios using document retrieval and assess its robustness to reviewer model
variation. Our findings highlight the potential of agentic LLM workflows, even
without domain-specific tuning, to improve the reliability and usability of
automated scientific writing.

</details>


### [96] [NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description](https://arxiv.org/abs/2510.05139)
*Hamed Jelodar,Mohammad Meymani,Parisa Hamedi,Tochukwu Emmanuel Nwankwo,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.CL

TL;DR: 本文提出了一种名为NLD-LLM的系统性NLP框架，用于评估语言模型生成准确且简洁源代码描述的能力，强调提示工程对模型性能的重要影响。


<details>
  <summary>Details</summary>
Motivation: 为了系统评估不同语言模型在自然语言描述任务中生成源代码描述的准确性与简洁性，尤其是在不同架构和规模下的表现差异。

Method: 构建包含多种Transformer模型（如Qwen、DeepSeek、Phi、LLaMA、Mistral）的评估框架，设计标准化格式、明确任务指引和NLD提示策略，并采用迭代优化流程提升输出质量。

Result: 实验表明，良好的提示工程能显著提升模型表现，使得小型模型在精心设计的提示下也能与大型模型竞争。

Conclusion: 提示工程在自然语言描述任务中起关键作用，NLD-LLM框架为评估语言模型提供了公平、一致且有效的基准。

Abstract: Natural Language Description (NLD) is a Natural Language Processing (NLP)
task that requires models to generate structured and meaningful outputs from
natural language inputs. In this work, we propose NLD-LLM, a systematic NLP
framework to evaluate the performance of language models to generate accurate
and concise source code descriptions. This framework incorporates a diverse set
of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral,
spanning various sizes, architectures, and training approaches. Central to
NLD-LLM is a comprehensive prompt design strategy that includes standardized
formatting, clear task guidance, and NLD prompting, ensuring fair and
consistent evaluation. Additionally, we apply an iterative refinement process
to improve output's quality and assess the model's adaptability. Using semantic
and structural metrics, our analysis demonstrates that prompt engineering
significantly impacts the effectiveness of the model such that smaller models
often performing competitively when supported by well-crafted prompts.

</details>


### [97] [To model human linguistic prediction, make LLMs less superhuman](https://arxiv.org/abs/2510.05141)
*Byung-Doh Oh,Tal Linzen*

Main category: cs.CL

TL;DR: 本文讨论了大型语言模型（LLM）在预测人类阅读行为方面表现下降的问题，尽管其在预测下一个词方面表现出色。作者认为这是因为LLM在长期和短期记忆方面远超人类，导致其成为“超人”级别的语言理解模型。文章主张构建具有类人记忆特性的模型，并提出了实现这一目标的可能方向，同时指出当前的人类数据不足以衡量进展，需设计新的实验来填补这一空白。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在预测下一个词方面的性能提升，它们对人类阅读行为的预测能力却在下降。这种不一致促使研究者探讨LLM作为人类语言认知模型的有效性，并分析其‘超人’特性背后的原因。

Method: 通过分析LLM与人类在长期记忆（如事实和训练样本的记忆）和短期记忆（如文本中前文词语的记忆）方面的差异，提出LLM‘超人’特性的主要驱动因素，并建议构建更具人类特征的记忆机制模型。同时，评估现有数据的局限性并提出改进实验设计。

Result: 发现LLM由于在长时和短时记忆上远超人类，导致其预测的人类阅读难度偏低，从而降低了对真实阅读行为的拟合度。现有的人类语言数据不足以充分评估模型的人类相似性。

Conclusion: 应开发具有类人记忆限制的LLM，以更准确地模拟人类语言处理过程；同时需要新的、针对性的人类实验数据来推动该领域的进步。

Abstract: When people listen to or read a sentence, they actively make predictions
about upcoming words: words that are less predictable are generally read more
slowly than predictable ones. The success of large language models (LLMs),
which, like humans, make predictions about upcoming words, has motivated
exploring the use of these models as cognitive models of human linguistic
prediction. Surprisingly, in the last few years, as language models have become
better at predicting the next word, their ability to predict human reading
behavior has declined. This is because LLMs are able to predict upcoming words
much better than people can, leading them to predict lower processing
difficulty in reading than observed in human experiments; in other words,
mainstream LLMs are 'superhuman' as models of language comprehension. In this
position paper, we argue that LLMs' superhumanness is primarily driven by two
factors: compared to humans, LLMs have much stronger long-term memory for facts
and training examples, and they have much better short-term memory for previous
words in the text. We advocate for creating models that have human-like
long-term and short-term memory, and outline some possible directions for
achieving this goal. Finally, we argue that currently available human data is
insufficient to measure progress towards this goal, and outline human
experiments that can address this gap.

</details>


### [98] [Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models](https://arxiv.org/abs/2510.05142)
*Xin Wang,Anshu Raj,Matthew Luebbe,Haiming Wen,Shuozhi Xu,Kun Lu*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的多阶段信息提取管道，用于从实验性材料文献中提取涵盖成分、加工、微观结构和性能的47个特征，显著提高了提取精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有信息提取方法通常局限于少数特征，且未解决成分-加工-微结构-性能之间的综合关系，难以支持全面的材料数据库构建。

Method: 采用基于大语言模型的多阶段提取流程，结合迭代提取与来源追踪，从实验报道的材料文献中提取47个关键特征，并在特征级和元组级进行评估。

Result: 在特征级和元组级的F1得分均达到约0.96；相比单次提取，微结构类别的F1得分提升10.0%（特征级）和13.7%（元组级），漏检材料数从49降至13（396个材料中），误报率为零。

Conclusion: 该方法实现了高精度、低遗漏、无误报的可扩展文献挖掘，生成的数据集可为机器学习和材料信息学提供可靠输入，且模块化设计适用于多种材料体系。

Abstract: Data-driven materials discovery requires large-scale experimental datasets,
yet most of the information remains trapped in unstructured literature.
Existing extraction efforts often focus on a limited set of features and have
not addressed the integrated composition-processing-microstructure-property
relationships essential for understanding materials behavior, thereby posing
challenges for building comprehensive databases. To address this gap, we
propose a multi-stage information extraction pipeline powered by large language
models, which captures 47 features spanning composition, processing,
microstructure, and properties exclusively from experimentally reported
materials. The pipeline integrates iterative extraction with source tracking to
enhance both accuracy and reliability. Evaluations at the feature level
(independent attributes) and tuple level (interdependent features) yielded F1
scores around 0.96. Compared with single-pass extraction without source
tracking, our approach improved F1 scores of microstructure category by 10.0%
(feature level) and 13.7% (tuple level), and reduced missed materials from 49
to 13 out of 396 materials in 100 articles on precipitate-containing
multi-principal element alloys (miss rate reduced from 12.4% to 3.3%). The
pipeline enables scalable and efficient literature mining, producing databases
with high precision, minimal omissions, and zero false positives. These
datasets provide trustworthy inputs for machine learning and materials
informatics, while the modular design generalizes to diverse material classes,
enabling comprehensive materials information extraction.

</details>


### [99] [SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation](https://arxiv.org/abs/2510.05144)
*Muskaan Chopra,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文提出了SynCED-EnDe，一个大规模、平衡、多来源的英德机器翻译关键错误检测数据集，包含显式错误子类和细粒度辅助标注，显著提升现有基准性能，旨在推动机器翻译在新兴场景中的安全应用。


<details>
  <summary>Details</summary>
Motivation: WMT21英德关键错误检测（CED）数据集存在规模小、标签不平衡、领域覆盖有限和时效性不足等问题，限制了机器翻译安全性的研究进展。

Method: 构建了一个包含1,000个人工标注和8,000个银标句子对的新数据集SynCED-EnDe，涵盖多样化来源（如StackExchange、GOV.UK），引入错误子类、触发标志及细粒度辅助判断（明显性、严重性、定位复杂度等），并提供完整文档与基线脚本。

Result: 在XLM-R及相关编码器上的实验表明，由于标签平衡和标注精细化，模型性能显著优于WMT21基准；数据集已公开于GitHub和Hugging Face。

Conclusion: SynCED-EnDe作为一个开放资源，支持对翻译错误风险与复杂性的系统分析，有望促进信息检索和对话助手等场景下机器翻译的安全部署，特别是在可穿戴AI设备等新兴应用中。

Abstract: Critical Error Detection (CED) in machine translation aims to determine
whether a translation is safe to use or contains unacceptable deviations in
meaning. While the WMT21 English-German CED dataset provided the first
benchmark, it is limited in scale, label balance, domain coverage, and temporal
freshness. We present SynCED-EnDe, a new resource consisting of 1,000
gold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between
error and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources
(StackExchange, GOV.UK) and introduces explicit error subclasses, structured
trigger flags, and fine-grained auxiliary judgments (obviousness, severity,
localization complexity, contextual dependency, adequacy deviation). These
enrichments enable systematic analyses of error risk and intricacy beyond
binary detection. The dataset is permanently hosted on GitHub and Hugging Face,
accompanied by documentation, annotation guidelines, and baseline scripts.
Benchmark experiments with XLM-R and related encoders show substantial
performance gains over WMT21 due to balanced labels and refined annotations. We
envision SynCED-EnDe as a community resource to advance safe deployment of MT
in information retrieval and conversational assistants, particularly in
emerging contexts such as wearable AI devices.

</details>


### [100] [Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs](https://arxiv.org/abs/2510.05148)
*Qi Li,Runpeng Yu,Haiquan Lu,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种针对离散扩散大语言模型（dLLMs）的新型模型归因方法，通过引入有向解码图（DDM）和高斯轨迹归因（GTA），有效捕捉解码过程中的结构信息并提升归因准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型置信度的归因方法在dLLMs中表现不佳，因其双向解码机制导致置信度冗余，难以反映解码顺序与依赖结构；同时需应对多样化的归因场景（如区分不同模型或同一模型的不同检查点）。

Method: 提出有向解码图（DDM）以提取解码步骤间的结构性关系，并设计高斯轨迹归因（GTA）方法，通过在每个解码位置拟合单元高斯分布，利用轨迹对数似然作为归因得分进行模型识别。

Result: 实验表明，所提DDM和GTA方法在多种归因场景下均显著优于基线方法，能更准确地区分不同dLLM模型及其检查点。

Conclusion: 该工作揭示了dLLMs解码轨迹中蕴含的可归因结构信号，提出的DDM与GTA框架为扩散语言模型的归因分析提供了有效且通用的解决方案。

Abstract: Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive paradigm for non-autoregressive language modeling. Their
distinctive decoding mechanism enables faster inference speed and strong
performance in code generation and mathematical tasks. In this work, we show
that the decoding mechanism of dLLMs not only enhances model utility but also
can be used as a powerful tool for model attribution. A key challenge in this
problem lies in the diversity of attribution scenarios, including
distinguishing between different models as well as between different
checkpoints or backups of the same model. To ensure broad applicability, we
identify two fundamental problems: what information to extract from the
decoding trajectory, and how to utilize it effectively. We first observe that
relying directly on per-step model confidence yields poor performance. This is
mainly due to the bidirectional decoding nature of dLLMs: each newly decoded
token influences the confidence of other decoded tokens, making model
confidence highly redundant and washing out structural signal regarding
decoding order or dependencies. To overcome this, we propose a novel
information extraction scheme called the Directed Decoding Map (DDM), which
captures structural relationships between decoding steps and better reveals
model-specific behaviors. Furthermore, to make full use of the extracted
structural information during attribution, we propose Gaussian-Trajectory
Attribution (GTA), where we fit a cell-wise Gaussian distribution at each
decoding position for each target model, and define the likelihood of a
trajectory as the attribution score: if a trajectory exhibits higher
log-likelihood under the distribution of a specific model, it is more likely to
have been generated by that model. Extensive experiments under different
settings validate the utility of our methods.

</details>


### [101] [Chronological Thinking in Full-Duplex Spoken Dialogue Language Models](https://arxiv.org/abs/2510.05150)
*Donghang Wu,Haoyang Zhang,Chen Chen,Tianyu Zhang,Fei Tian,Xuerui Yang,Gang Yu,Hexin Liu,Nana Hou,Yuchen Hu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 本文提出了一种名为“时序思维”（Chronological Thinking）的机制，旨在提升全双工对话系统中语音对话语言模型的响应质量，通过在倾听用户语音流的同时进行增量式、无延迟的因果推理，模拟人类对话中的实时思考过程。


<details>
  <summary>Details</summary>
Motivation: 现有全双工对话系统在倾听阶段让模型持续预测静音标记，处于空闲状态，不符合人类在对话中持续轻量思考的习惯。因此，需要一种更贴近人类行为的实时推理机制来提升交互自然性和响应质量。

Method: 提出“时序思维”机制，具有两个关键特性：(1) 严格因果性——模型仅基于已接收的音频流增量推理，不依赖未来信息；(2) 零额外延迟——推理过程与倾听同步完成，用户停止说话后模型立即回应，无需额外思考时间。该方法专为流式语音输入设计，区别于传统的Chain-of-Thought等静态推理方式。

Result: 实验结果表明，时序思维在客观指标和人工评估中均显著提升了响应质量，能够稳健应对对话动态变化，并在全双工交互指标上达到有竞争力的表现。

Conclusion: 时序思维为全双工语音对话系统提供了一种高效、符合人类对话习惯的实时推理范式，能够在不增加延迟的前提下提升对话质量，推动了SDLM向更自然的人类-like交互发展。

Abstract: Recent advances in spoken dialogue language models (SDLMs) reflect growing
interest in shifting from turn-based to full-duplex systems, where the models
continuously perceive user speech streams while generating responses. This
simultaneous listening and speaking design enables real-time interaction and
the agent can handle dynamic conversational behaviors like user barge-in.
However, during the listening phase, existing systems keep the agent idle by
repeatedly predicting the silence token, which departs from human behavior: we
usually engage in lightweight thinking during conversation rather than
remaining absent-minded. Inspired by this, we propose Chronological Thinking, a
on-the-fly conversational thinking mechanism that aims to improve response
quality in full-duplex SDLMs. Specifically, chronological thinking presents a
paradigm shift from conventional LLM thinking approaches, such as
Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly
causal: the agent reasons incrementally while listening, updating internal
hypotheses only from past audio with no lookahead. (2) No additional latency:
reasoning is amortized during the listening window; once the user stops
speaking, the agent halts thinking and begins speaking without further delay.
Experiments demonstrate the effectiveness of chronological thinking through
both objective metrics and human evaluations show consistent improvements in
response quality. Furthermore, chronological thinking robustly handles
conversational dynamics and attains competitive performance on full-duplex
interaction metrics.

</details>


### [102] [Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA](https://arxiv.org/abs/2510.05151)
*Prudence Djagba,Abdelkader Y. Saley*

Main category: cs.CL

TL;DR: 该研究探讨了领域适应的大语言模型（LLMs）在金融自然语言处理中的优缺点，重点分析了PIXIU框架下的FinMA模型在FLARE基准和金融指令调优（FIT）数据集上的表现，发现其在情感分析和分类任务中表现良好，但在数值推理、实体识别和摘要任务中存在挑战。


<details>
  <summary>Details</summary>
Motivation: 金融应用对准确性、可靠性和领域适应性要求极高，因此需要深入理解如何有效设计和评估金融领域的大语言模型，以支持金融决策。

Method: 研究基于PIXIU框架构建FinMA模型，采用金融指令调优（FIT）数据集进行指令微调，并在FLARE基准下评估其在多种金融NLP任务中的性能。

Result: FinMA在情感分析和分类任务中表现优异，但在数值推理、命名实体识别和文本摘要方面表现不佳，暴露出当前金融LLM的局限性。

Conclusion: 有效的领域适应和指令微调有助于提升LLM在金融领域的表现，但模型在复杂语义和结构化理解任务上仍需改进，未来应加强多任务能力和数据质量以提升整体性能。

Abstract: This research explores the strengths and weaknesses of domain-adapted Large
Language Models (LLMs) in the context of financial natural language processing
(NLP). The analysis centers on FinMA, a model created within the PIXIU
framework, which is evaluated for its performance in specialized financial
tasks. Recognizing the critical demands of accuracy, reliability, and domain
adaptation in financial applications, this study examines FinMA's model
architecture, its instruction tuning process utilizing the Financial
Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark.
Findings indicate that FinMA performs well in sentiment analysis and
classification, but faces notable challenges in tasks involving numerical
reasoning, entity recognition, and summarization. This work aims to advance the
understanding of how financial LLMs can be effectively designed and evaluated
to assist in finance-related decision-making processes.

</details>


### [103] [A Single Character can Make or Break Your LLM Evals](https://arxiv.org/abs/2510.05152)
*Jingtong Su,Jianyu Zhang,Karen Ullrich,Léon Bottou,Mark Ibrahim*

Main category: cs.CL

TL;DR: 本文研究了在大语言模型（LLM）评估中，上下文示例分隔符的选择对模型性能的显著影响，发现不同分隔符可导致性能波动高达±23%，并提出通过提示明确指定分隔符来增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨在LLM评估中常被忽视的示例格式问题，尤其是分隔符选择的影响，揭示当前模型对此类微小变化的脆弱性。

Method: 在多个主流模型家族（Llama、Qwen、Gemma）上测试不同分隔符（如逗号、换行、分号、井号等）对MMLU等任务性能的影响，并通过注意力头分析其机制。

Result: 发现分隔符选择可导致MMLU性能波动达±23%，改变模型排名；良好的分隔符能引导注意力聚焦关键输入词元；模型规模增大未改善此脆弱性。

Conclusion: LLMs对分隔符选择高度敏感，提示中明确指定分隔符可提升鲁棒性，建议在实践中采用最优分隔符以稳定性能。

Abstract: Common Large Language model (LLM) evaluations rely on demonstration examples
to steer models' responses to the desired style. While the number of examples
used has been studied and standardized, the choice of how to format examples is
less investigated. In evaluation protocols and real world usage, users face the
choice how to separate in-context examples: use a comma? new line? semi-colon?
hashtag? etc.? Surprisingly, we find this seemingly minor choice can
dramatically alter model response quality. Across leading model families
(Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$
depending on the choice of delimiter. In fact, one can manipulate model
rankings to put any model in the lead by only modifying the single character
separating examples. We find LLMs' brittleness pervades topics, model families,
and doesn't improve with scale. By probing attention head scores, we find that
good-performing delimiters steer attention towards key tokens in the input.
Finally, we explore methods to improve LLMs' robustness to the choice of
delimiter. We find specifying the selected delimiter in the prompt boosts
robustness and offer practical recommendations for the best-performing
delimiters to select.

</details>


### [104] [Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs](https://arxiv.org/abs/2510.05154)
*Shenzhe Zhu,Shu Yang,Michiel A. Bakker,Alex Pentland,Jiaxin Pei*

Main category: cs.CL

TL;DR: 提出DeliberationBank数据集和DeliberationJudge模型，以更可靠地评估大规模审议文本的摘要质量，提升AI在政策制定中的公平性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为评判者与人类判断对齐性差，且可能忽略少数观点、存在输入顺序偏差，需更可靠的评估方法。

Method: 构建包含3000名参与者意见和4500名标注者评分的大规模人类标注数据集DeliberationBank，并训练基于DeBERTa的DeliberationJudge模型，在四个维度上评估审议摘要。

Result: DeliberationJudge相比各类LLM评判者更高效且更贴近人类判断；通过其评估发现18个主流LLM在审议摘要中普遍存在少数观点被低估的问题。

Conclusion: 该框架为审议摘要提供了可扩展、可靠的评估方式，有助于构建更公正、更具代表性的AI辅助决策系统。

Abstract: Large-scale public deliberations generate thousands of free-form
contributions that must be synthesized into representative and neutral
summaries for policy use. While LLMs have been shown as a promising tool to
generate summaries for large-scale deliberations, they also risk
underrepresenting minority perspectives and exhibiting bias with respect to the
input order, raising fairness concerns in high-stakes contexts. Studying and
fixing these issues requires a comprehensive evaluation at a large scale, yet
current practice often relies on LLMs as judges, which show weak alignment with
human judgments. To address this, we present DeliberationBank, a large-scale
human-grounded dataset with (1) opinion data spanning ten deliberation
questions created by 3,000 participants and (2) summary judgment data annotated
by 4,500 participants across four dimensions (representativeness,
informativeness, neutrality, policy approval). Using these datasets, we train
DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation
summaries from individual perspectives. DeliberationJudge is more efficient and
more aligned with human judgements compared to a wide range of LLM judges. With
DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in
deliberation summarization, especially underrepresentation of minority
positions. Our framework provides a scalable and reliable way to evaluate
deliberation summarization, helping ensure AI systems are more representative
and equitable for policymaking.

</details>


### [105] [A novel hallucination classification framework](https://arxiv.org/abs/2510.05189)
*Maksym Zavhorodnii,Dmytro Dehtiarov,Anna Konovalenko*

Main category: cs.CL

TL;DR: 提出一种基于提示工程和无监督学习的幻觉检测方法，通过将幻觉与真实响应映射到向量空间，发现幻觉与正确输出在空间上的距离与其信息失真程度相关，从而实现对大语言模型中幻觉的有效识别。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理过程中容易产生幻觉（即生成看似合理但错误或虚构的信息），影响其可靠性，因此需要自动检测并区分这些错误输出。

Method: 通过系统化的分类和提示工程复现多种类型的幻觉，构建专用数据集，并利用嵌入模型将其映射到向量空间，采用无监督学习技术在降维后的空间中分析幻觉与真实响应的分布模式。

Result: 定量分析显示，幻觉的严重程度与其在向量空间中偏离正确输出聚类中心的距离呈一致相关性，即使使用简单的分类算法也能有效区分幻觉与正确响应。

Conclusion: 该方法为单一大语言模型提供了一种轻量且有效的幻觉检测框架，具有理论和实证支持，有助于提升模型输出的可靠性。

Abstract: This work introduces a novel methodology for the automatic detection of
hallucinations generated during large language model (LLM) inference. The
proposed approach is based on a systematic taxonomy and controlled reproduction
of diverse hallucination types through prompt engineering. A dedicated
hallucination dataset is subsequently mapped into a vector space using an
embedding model and analyzed with unsupervised learning techniques in a
reduced-dimensional representation of hallucinations with veridical responses.
Quantitative evaluation of inter-centroid distances reveals a consistent
correlation between the severity of informational distortion in hallucinations
and their spatial divergence from the cluster of correct outputs. These
findings provide theoretical and empirical evidence that even simple
classification algorithms can reliably distinguish hallucinations from accurate
responses within a single LLM, thereby offering a lightweight yet effective
framework for improving model reliability.

</details>


### [106] [Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.05251)
*Chenghao Yang,Lin Gui,Chenxiao Yang,Victor Veitch,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.CL

TL;DR: 提出了一种名为Exploratory Annealed Decoding (EAD)的探索策略，通过在生成过程中从高到低退火采样温度，提升大语言模型在强化学习中的推理能力和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准的固定温度采样难以在保持样本质量和探索多样性之间取得平衡，需要一种更有效的探索策略来提升强化学习中奖励可验证的大语言模型的推理能力。

Method: 提出EAD方法，采用‘开头探索、结尾利用’的策略，在生成序列初期使用高温度促进多样化探索，随后逐渐降低温度以保持后期生成质量，并使采样分布接近目标策略，从而提高训练稳定性。

Result: EAD作为一种轻量级即插即用方法，在多种RLVR算法和不同模型规模下均显著提升了样本效率，优于固定温度采样的表现。

Conclusion: 将探索策略与序列生成的自然动态相结合（如在早期进行探索）是提升大语言模型推理能力的有效途径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm
for enhancing the reasoning capabilities of large language models (LLMs), yet
its success hinges on effective exploration. An ideal exploration strategy must
navigate two fundamental challenges: it must preserve sample quality while also
ensuring training stability. While standard fixed-temperature sampling is
simple, it struggles to balance these competing demands, as high temperatures
degrade sample quality and low temperatures limit discovery. In this work, we
propose a simpler and more effective strategy, Exploratory Annealed Decoding
(EAD), grounded in the insight that exploration is most impactful on early
tokens which define a sequence's semantic direction. EAD implements an
intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by
annealing the sampling temperature from high to low during generation. This
dynamic schedule encourages meaningful, high-level diversity at the start, then
gradually lowers the temperature to preserve sample quality and keep the
sampling distribution close to the target policy, which is essential for stable
training. We demonstrate that EAD is a lightweight, plug-and-play method that
significantly improves sample efficiency, consistently outperforming
fixed-temperature sampling across various RLVR algorithms and model sizes. Our
work suggests that aligning exploration with the natural dynamics of sequential
generation offers a robust path to improving LLM reasoning.

</details>


### [107] [Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages](https://arxiv.org/abs/2510.05291)
*Tarek Naous,Anagha Savit,Carlos Rafael Catalan,Geyang Guo,Jaehyeok Lee,Kyungdon Lee,Lheane Marie Dizon,Mengyu Ye,Neel Kothari,Sahajpreet Singh,Sarah Masud,Tanish Patwa,Trung Thanh Tran,Zohaib Khan,Alan Ritter,JinYeong Bak,Keisuke Sakaguchi,Tanmoy Chakraborty,Yuki Arase,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出了Camellia，一个用于衡量九种亚洲语言中以实体为中心的文化偏见的基准，揭示了多语言大模型在亚洲语言中的文化适应困难和跨文化性能差异。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏多语言基准，尚不清楚大模型在不同非西方语言中是否存在文化偏见，因此需要构建专门的评估工具来衡量其在亚洲语言中的文化公平性。

Method: 构建了包含19,530个人工标注实体和2,173个来自社交媒体的掩码上下文的Camellia基准，涵盖六种亚洲文化、九种亚洲语言，并在文化语境适应、情感关联和实体抽取等任务上评估多个多语言大模型。

Result: 发现大模型在所有亚洲语言中的文化适应能力较差，不同模型因训练数据来源不同表现出显著性能差异；各模型家族存在独特的文化偏见模式，在情感关联上表现不同；且在亚洲语言上下文理解上存在缺陷，导致实体抽取任务中出现跨文化性能差距。

Conclusion: 当前多语言大模型在处理亚洲语言时普遍存在文化偏见和理解不足的问题，需更多 culturally-relevant 数据和针对性设计以提升文化公平性。

Abstract: As Large Language Models (LLMs) gain stronger multilingual capabilities,
their ability to handle culturally diverse entities becomes crucial. Prior work
has shown that LLMs often favor Western-associated entities in Arabic, raising
concerns about cultural fairness. Due to the lack of multilingual benchmarks,
it remains unclear if such biases also manifest in different non-Western
languages. In this paper, we introduce Camellia, a benchmark for measuring
entity-centric cultural biases in nine Asian languages spanning six distinct
Asian cultures. Camellia includes 19,530 entities manually annotated for
association with the specific Asian or Western culture, as well as 2,173
naturally occurring masked contexts for entities derived from social media
posts. Using Camellia, we evaluate cultural biases in four recent multilingual
LLM families across various tasks such as cultural context adaptation,
sentiment association, and entity extractive QA. Our analyses show a struggle
by LLMs at cultural adaptation in all Asian languages, with performance
differing across models developed in regions with varying access to
culturally-relevant data. We further observe that different LLM families hold
their distinct biases, differing in how they associate cultures with particular
sentiments. Lastly, we find that LLMs struggle with context understanding in
Asian languages, creating performance gaps between cultures in entity
extraction.

</details>


### [108] [RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts](https://arxiv.org/abs/2510.05310)
*Yining She,Daniel W. Peterson,Marianne Menglin Liu,Vikas Upadhyay,Mohammad Hossein Chaghazardi,Eunsuk Kang,Dan Roth*

Main category: cs.CL

TL;DR: 本文研究了基于大语言模型（LLM）的防护机制在检索增强生成（RAG）场景下的上下文鲁棒性，发现插入良性文档可分别在约11%和8%的情况下改变输入和输出防护的判断，表明当前防护机制存在上下文鲁棒性缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，确保其安全性至关重要。现有的基于LLM的外部防护模型可能因数据分布变化而失效，尤其是在引入额外上下文信息（如RAG中的检索文档）时的鲁棒性尚不明确。

Method: 以检索增强生成（RAG）为案例，系统评估了3个Llama Guard模型和2个GPT-oss模型在输入不同良性文档上下文时对安全判断的影响，并分析了检索文档、用户查询和模型响应各组成部分的影响。同时测试了两种缓解方法。

Result: 插入良性文档导致输入和输出防护的判断分别在约11%和8%的情况下发生改变；对上下文各组件的分析显示其影响显著；测试的两种缓解方法仅带来轻微改善。

Conclusion: 当前基于LLM的防护机制在面对上下文扰动时存在明显的鲁棒性问题，亟需开发更具鲁棒性的训练和评估方案，以应对检索与查询组合带来的挑战。

Abstract: With the increasing adoption of large language models (LLMs), ensuring the
safety of LLM systems has become a pressing concern. External LLM-based
guardrail models have emerged as a popular solution to screen unsafe inputs and
outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are
vulnerable to data distribution shifts. In this paper, taking Retrieval
Augmentation Generation (RAG) as a case study, we investigated how robust
LLM-based guardrails are against additional information embedded in the
context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss
models, we confirmed that inserting benign documents into the guardrail context
alters the judgments of input and output guardrails in around 11% and 8% of
cases, making them unreliable. We separately analyzed the effect of each
component in the augmented context: retrieved documents, user query, and
LLM-generated response. The two mitigation methods we tested only bring minor
improvements. These results expose a context-robustness gap in current
guardrails and motivate training and evaluation protocols that are robust to
retrieval and query composition.

</details>


### [109] [WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives](https://arxiv.org/abs/2510.05336)
*Yongan Yu,Xianda Du,Qingchen Hu,Jiahao Liang,Jingwei Ni,Dan Qiang,Kaiyu Huang,Grant McKenzie,Renee Sieber,Fengran Mo*

Main category: cs.CL

TL;DR: 本文提出了WeatherArchive-Bench，首个用于评估基于历史天气档案的检索增强生成（RAG）系统的基准，包含检索和评估两类任务，揭示了现有密集检索器和大语言模型在处理历史术语及社会脆弱性与韧性识别上的局限性。


<details>
  <summary>Details</summary>
Motivation: 历史天气档案蕴含丰富的社会应对极端天气的定性信息，但其规模大、数字化噪声多、语言古老，难以转化为结构化知识，亟需有效的工具将其应用于气候研究。

Method: 构建了WeatherArchive-Bench基准，包括WeatherArchive-Retrieval任务（从百万级新闻片段中检索相关段落）和WeatherArchive-Assessment任务（利用大语言模型分类社会脆弱性和韧性指标），并对多种检索器和大语言模型进行了广泛实验。

Result: 实验表明，密集检索器在处理历史术语时表现不佳，而大语言模型常误解脆弱性和韧性概念，凸显当前RAG系统在理解复杂社会指标方面的不足。

Conclusion: 该研究揭示了将历史档案用于气候研究的技术挑战，为构建更鲁棒的、面向气候的RAG系统提供了重要见解，数据集和评估框架已公开。

Abstract: Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

</details>


### [110] [Residualized Similarity for Faithfully Explainable Authorship Verification](https://arxiv.org/abs/2510.05362)
*Peter Zeng,Pegah Alipoormolabashi,Jihu Mun,Gourab Dey,Nikita Soni,Niranjan Balasubramanian,Owen Rambow,H. Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种名为残差化相似性（Residualized Similarity, RS）的新方法，通过结合可解释特征系统与神经网络来提升作者验证系统的性能，同时保持预测的可解释性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的神经方法虽然准确率高，但缺乏可解释性；而大模型的预测无法被忠实解释，难以用于具有现实影响的决策场景。因此需要一种既能保持高性能又能提供可追溯、可解释预测的方法。

Method: 将作者验证视为相似性任务，使用可解释系统生成初始相似性评分，并用神经网络预测该评分的残差（即误差），最终结果为两者之和。这种方法结合了可解释特征与深度学习的优势。

Result: 在四个数据集上的实验表明，该方法能够达到最先进的作者验证模型的性能水平，同时提供对预测结果的忠实且可解释的分析。

Conclusion: RS方法成功地在不牺牲可解释性的前提下，提升了基于可解释特征的作者验证系统的性能，为高风险应用场景下的可信AV系统提供了可行路径。

Abstract: Responsible use of Authorship Verification (AV) systems not only requires
high accuracy but also interpretable solutions. More importantly, for systems
to be used to make decisions with real-world consequences requires the model's
prediction to be explainable using interpretable features that can be traced to
the original texts. Neural methods achieve high accuracies, but their
representations lack direct interpretability. Furthermore, LLM predictions
cannot be explained faithfully -- if there is an explanation given for a
prediction, it doesn't represent the reasoning process behind the model's
prediction. In this paper, we introduce Residualized Similarity (RS), a novel
method that supplements systems using interpretable features with a neural
network to improve their performance while maintaining interpretability.
Authorship verification is fundamentally a similarity task, where the goal is
to measure how alike two documents are. The key idea is to use the neural
network to predict a similarity residual, i.e. the error in the similarity
predicted by the interpretable system. Our evaluation across four datasets
shows that not only can we match the performance of state-of-the-art authorship
verification models, but we can show how and to what degree the final
prediction is faithful and interpretable.

</details>


### [111] [The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures](https://arxiv.org/abs/2510.05364)
*Alexander M. Fichtl,Jeremias Bohn,Josefin Kelber,Edoardo Mosca,Georg Groh*

Main category: cs.CL

TL;DR: 本文综述了克服Transformer注意力机制二次复杂度瓶颈的近期研究，包括亚二次注意力变体、循环神经网络、状态空间模型和混合架构，并从计算与内存复杂度、基准结果及根本局限性方面进行分析。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制具有固有的二次复杂度，随着上下文长度增加成为显著瓶颈，亟需更高效的序列建模方法。

Method: 对近期提出的亚二次注意力机制、RNN、状态空间模型和混合架构等方法进行系统性综述和比较分析。

Result: 总结了各类方法在计算效率、内存占用和性能上的表现，揭示了各自的优缺点和适用场景。

Conclusion: 纯注意力机制的Transformer主导地位可能即将受到挑战，未来模型可能趋向于混合架构或新型序列建模方法。

Abstract: Transformers have dominated sequence processing tasks for the past seven
years -- most notably language modeling. However, the inherent quadratic
complexity of their attention mechanism remains a significant bottleneck as
context length increases. This paper surveys recent efforts to overcome this
bottleneck, including advances in (sub-quadratic) attention variants, recurrent
neural networks, state space models, and hybrid architectures. We critically
analyze these approaches in terms of compute and memory complexity, benchmark
results, and fundamental limitations to assess whether the dominance of
pure-attention transformers may soon be challenged.

</details>


### [112] [Context Length Alone Hurts LLM Performance Despite Perfect Retrieval](https://arxiv.org/abs/2510.05381)
*Yufeng Du,Minyang Tian,Srikanth Ronanki,Subendhu Rongali,Sravan Bodapati,Aram Galstyan,Azton Wells,Roy Schwartz,Eliu A Huerta,Hao Peng*

Main category: cs.CL

TL;DR: 本文发现，即使大语言模型能够完美检索到相关信息，输入长度的增加仍会导致性能显著下降（13.9%–85%），且这种下降与信息干扰或检索失败无关。研究揭示了输入长度本身对模型性能的负面影响，并提出一种简单、通用的缓解策略：在解决问题前先让模型复述检索到的关键信息，该方法在RULER数据集上使GPT-4o性能提升达4%。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型支持长上下文输入，但在实际任务中性能并未随上下文增长而线性提升。通常认为这是由于检索失败导致，但本文质疑：即使检索完美，模型是否仍会因输入过长而表现下降？

Method: 在数学、问答和编程任务上，对5个开源和闭源大模型进行系统实验，控制变量包括完全相关输入、空白填充、掩码无关token以及将所有相关信息置于问题之前，以隔离输入长度的影响。同时提出一种模型无关的缓解策略：通过提示模型先复述关键证据再解题。

Result: 即使模型能完美检索所有相关信息，性能仍随输入增长显著下降（13.9%–85%）；该现象在无关token被替换为空白或掩码后依然存在；即使所有证据紧邻问题前也未改善；所提复述策略在RULER上使GPT-4o相对强基线提升最多4%。

Conclusion: 输入长度本身即可损害大语言模型性能，独立于检索质量和外部干扰。这揭示了一个此前未被认知的局限性，并表明需重新思考长上下文建模的本质挑战。

Abstract: Large language models (LLMs) often fail to scale their performance on
long-context tasks performance in line with the context lengths they support.
This gap is commonly attributed to retrieval failures -- the models' inability
to identify relevant information in the long inputs. Accordingly, recent
efforts often focus on evaluating and improving LLMs' retrieval performance: if
retrieval is perfect, a model should, in principle, perform just as well on a
long input as it does on a short one -- or should it? This paper presents
findings that the answer to this question may be negative. Our systematic
experiments across 5 open- and closed-source LLMs on math, question answering,
and coding tasks reveal that, even when models can perfectly retrieve all
relevant information, their performance still degrades substantially
(13.9%--85%) as input length increases but remains well within the models'
claimed lengths. This failure occurs even when the irrelevant tokens are
replaced with minimally distracting whitespace, and, more surprisingly, when
they are all masked and the models are forced to attend only to the relevant
tokens. A similar performance drop is observed when all relevant evidence is
placed immediately before the question. Our findings reveal a
previously-unrealized limitation: the sheer length of the input alone can hurt
LLM performance, independent of retrieval quality and without any distraction.
They motivate our simple, model-agnostic mitigation strategy that transforms a
long-context task into a short-context one by prompting the model to recite the
retrieved evidence before attempting to solve the problem. On RULER, we observe
a consistent improvement of GPT-4o up to 4% on an already strong baseline.

</details>


### [113] [Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation](https://arxiv.org/abs/2510.05387)
*Ananth Kandala,Ratna Kandala,Akshata Kishore Moharir,Niva Manchanda,Sunaina Singh*

Main category: cs.CL

TL;DR: 提出跨语言患者压力表达图（CL-PDE），构建基于图的跨语言心理健康本体，以捕捉和对齐印度多种语言中文化嵌入的压力表达，弥补现有临床NLP在文化与语言多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康资源多以英语或西方文化为中心，难以准确表达印度多语言背景下患者的痛苦感受，导致临床NLP在该地区应用受限。

Method: 采用基于图的方法构建跨语言患者压力表达图（CL-PDE），整合多种印度语言中的非正式压力表达，通过语义对齐连接不同语言表达，并与标准临床术语关联。

Result: 实现了对印度多语言环境中文化相关压力表达的有效建模，提升了跨语言理解和临床术语映射能力，支持更包容、以患者为中心的NLP工具开发。

Conclusion: CL-PDE框架有助于弥补非西方、多语言背景下心理健康表达的表征空白，推动文化敏感型AI在精神卫生通信中的应用。

Abstract: Mental health communication in India is linguistically fragmented, culturally
diverse, and often underrepresented in clinical NLP. Current health ontologies
and mental health resources are dominated by diagnostic frameworks centered on
English or Western culture, leaving a gap in representing patient distress
expressions in Indian languages. We propose cross-linguistic graphs of patient
stress expressions (CL-PDE), a framework for building cross-lingual mental
health ontologies through graph-based methods that capture culturally embedded
expressions of distress, align them across languages, and link them with
clinical terminology. Our approach addresses critical gaps in healthcare
communication by grounding AI systems in culturally valid representations,
allowing more inclusive and patient-centric NLP tools for mental health care in
multilingual contexts.

</details>


### [114] [Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care](https://arxiv.org/abs/2510.05410)
*Junyi Fan,Li Sun,Negin Ashrafi,Kamiar Alaei,Maryam Pishgar*

Main category: cs.CL

TL;DR: 本研究利用直接偏好优化（DPO）方法微调Mistral-7B语言模型，以提升心力衰竭重症监护护理记录的质量，结果显示在自动指标和专家评估上均有显著改善。


<details>
  <summary>Details</summary>
Motivation: 重症监护病房的护理记录常因术语不一致、风格非正式和缺乏标准化而影响质量，尤其在心力衰竭护理中更为突出，亟需一种能兼顾隐私保护与文档质量提升的方法。

Method: 采用直接偏好优化（DPO）方法，基于MIMIC-III数据库中的8,838份心衰护理记录及21,210组由专家验证的GPT输出、模型生成和原始记录构成的偏好对，对Mistral-7B本地语言模型进行训练。

Result: BLEU分数提升84%（0.173至0.318），BERTScore提高7.6%（0.828至0.891），专家评分在准确性（+14.4）、完整性（+14.5）、逻辑一致性（+14.1）、可读性（+11.1）和结构清晰度（+6.0）等方面均显著上升。

Conclusion: DPO能够有效将轻量级临床语言模型与专家标准对齐，支持在电子病历系统中实现隐私保护的AI辅助文档生成，减轻行政负担并提升ICU患者安全。

Abstract: Nursing documentation in intensive care units (ICUs) provides essential
clinical intelligence but often suffers from inconsistent terminology, informal
styles, and lack of standardization, challenges that are particularly critical
in heart failure care. This study applies Direct Preference Optimization (DPO)
to adapt Mistral-7B, a locally deployable language model, using 8,838 heart
failure nursing notes from the MIMIC-III database and 21,210 preference pairs
derived from expert-verified GPT outputs, model generations, and original
notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert
qualitative assessments demonstrates that DPO markedly enhances documentation
quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore
improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy
(+14.4 points), completeness (+14.5 points), logical consistency (+14.1
points), readability (+11.1 points), and structural clarity (+6.0 points).
These results indicate that DPO can align lightweight clinical language models
with expert standards, supporting privacy-preserving, AI-assisted documentation
within electronic health record systems to reduce administrative burden and
improve ICU patient safety.

</details>


### [115] [A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis](https://arxiv.org/abs/2510.05414)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Haifeng Wang,Minghui Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的多智能体系统，用于自动化二维框架的有限元建模，通过分工协作的智能体在几何建模、代码生成与验证等任务中实现了超过80%的准确率，优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在工程领域展现出潜力，但在结构工程尤其是有限元建模中的应用仍待探索。现有方法难以处理几何建模、复杂推理和领域知识融合的问题。

Method: 构建一个基于Llama-3.3 70B Instruct的多智能体系统，将结构分析分解为子任务：问题分析、几何生成、代码转换、模型验证和加载条件应用，各由专用智能体执行并协同完成建模流程。

Result: 在20个基准问题上的实验表明，该系统在10次重复试验中多数情况下准确率超过80%，性能优于Gemini-2.5 Pro和ChatGPT-4o。

Conclusion: 所提出的多智能体系统能有效实现二维框架有限元建模的自动化，提升了建模效率与准确性，展示了LLM在结构工程中的应用前景。

Abstract: Large language models (LLMs) have recently been used to empower autonomous
agents in engineering, significantly improving automation and efficiency in
labor-intensive workflows. However, their potential remains underexplored in
structural engineering, particularly for finite element modeling tasks
requiring geometric modeling, complex reasoning, and domain knowledge. To
bridge this gap, this paper develops a LLM-based multi-agent system to automate
finite element modeling of 2D frames. The system decomposes structural analysis
into subtasks, each managed by a specialized agent powered by the lightweight
Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis
Agent, which extracts geometry, boundary, and material parameters from the user
input. Next, a Geometry Agent incrementally derives node coordinates and
element connectivity by applying expert-defined rules. These structured outputs
are converted into executable OpenSeesPy code by a Translation Agent and
refined by a Model Validation Agent through consistency checks. Then, a Load
Agent applies load conditions into the assembled structural model. Experimental
evaluations on 20 benchmark problems demonstrate that the system achieves
accuracy over 80% in most cases across 10 repeated trials, outperforming
Gemini-2.5 Pro and ChatGPT-4o models.

</details>


### [116] [Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification](https://arxiv.org/abs/2510.05431)
*Yoo Yongmin,Zhang Xu,Cao Longbing*

Main category: cs.CL

TL;DR: 提出Self-Filtered Distillation框架，利用无监督信任度量对LLM生成的推理进行筛选，提升专利分类的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的自然语言推理常包含逻辑错误和标签不匹配，直接用作监督信号会引入噪声，影响训练稳定性。

Method: 提出Self-Filtered Distillation框架，结合三种无监督信任指标（自一致性、类别蕴含对齐、LLM共识评分）构建统一信任分数，用于加权或过滤LLM生成的推理，实现推理感知的监督学习。

Result: 在USPTO-2M数据集上，该方法优于基于标签学习和传统蒸馏方法，在准确率、稳定性和可解释性方面表现更优。

Conclusion: 通过信任信号而非真实标签使用LLM生成的推理，为专利分析中利用推理感知指标提供了可靠范式。

Abstract: Large language models (LLMs) increasingly generate natural language
rationales to enhance interpretability, but these often contain logical errors,
label mismatches, and domain-specific misalignments. Directly using such
rationales as supervision risks propagating noise and undermining training
stability. To address this challenge, we introduce Self-Filtered Distillation,
a framework specifically tailored for patent classification, which treats
LLM-generated rationales as trust signals rather than ground-truth supervision.
The framework employs selective distillation guided by three unsupervised trust
metrics: (1) Self-Consistency, which measures the stability of LLM-generated
rationales across multiple generations; (2) Class Entailment Alignment, which
assesses semantic coherence with patent-specific class definitions; and (3) LLM
Agreement Scoring, which validates rationale-label plausibility. These metrics
are integrated into a unified trust score that primarily weights training
samples while optionally filtering out extremely low-trust cases, enabling
reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used
benchmark for patent classification, show that our method outperforms
label-based learning and conventional distillation in accuracy, stability, and
interpretability, establishing a reliable paradigm for leveraging
reasoning-aware trust indicators in patent analytics.

</details>


### [117] [SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?](https://arxiv.org/abs/2510.05444)
*Yao Dou,Michel Galley,Baolin Peng,Chris Kedzie,Weixin Cai,Alan Ritter,Chris Quirk,Wei Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 本文提出了SimulatorArena，一个用于评估LLM模拟用户在多轮对话中是否可靠替代真实用户的基准，实验表明基于用户画像的模拟器与人类判断高度一致，可作为高效、可扩展的人类评估替代方案。


<details>
  <summary>Details</summary>
Motivation: 由于人类评估成本高、耗时且难以复现，亟需一种可靠的自动化方法来评估大型语言模型在交互应用中的表现。

Method: 构建包含909段人类-LLM对话的基准SimulatorArena，涵盖数学辅导和文档创建两个任务，通过比较模拟用户消息与人类行为的一致性及其对助手评分与人类判断的对齐程度来评估不同模拟方法。

Result: 基于用户画像（如背景和语言风格）的条件化模拟器在两项任务上与人类判断的Spearman相关系数达到0.7，表现出良好的对齐性，并用于评测包括GPT-5、Claude 4.1 Opus和Gemini 2.5 Pro在内的18个助手模型。

Conclusion: 具备用户特征建模能力的LLM模拟器可有效替代人类进行助手评估，SimulatorArena为未来研究提供了可复现的基准测试平台。

Abstract: Large language models (LLMs) are increasingly used in interactive
applications, and human evaluation remains the gold standard for assessing
their performance in multi-turn conversations. Since human studies are costly,
time-consuming, and hard to reproduce, recent work explores using LLMs to
simulate users for automatic assistant evaluation. However, there is no
benchmark or systematic study to evaluate whether these simulated users are
reliable stand-ins for real users. To address this, we introduce
SimulatorArena, a benchmark of 909 annotated human-LLM conversations on two
interactive tasks -- math tutoring and document creation. SimulatorArena
evaluates simulators based on how closely their messages match human behavior
and how well their assistant ratings align with human judgments. Experiments on
various simulator methods show that simulators conditioned on user profiles,
capturing traits like background and message styles, align closely with human
judgments. They reach Spearman's $\rho$ of 0.7 on both tasks, providing a
practical, scalable alternative to human evaluation. Using the best simulator
for each task, we benchmark 18 assistants, including the latest LLMs such as
GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.

</details>


### [118] [AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering](https://arxiv.org/abs/2510.05445)
*Zheyuan Zhang,Kaiwen Shi,Zhengqing Yuan,Zehong Wang,Tianyi Ma,Keerthiram Murugesan,Vincent Galassi,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文提出了tAgentRouter框架，通过将多智能体问答建模为知识图谱引导的路由问题，利用异构图神经网络实现任务感知的智能体路由，显著提升了问答性能。


<details>
  <summary>Details</summary>
Motivation: 不同智能体和模型在不同任务中表现出互补优势，且大模型未必更优，因此需要一种能考虑上下文和关系结构的自适应路由机制。

Method: 将问答实例转化为包含查询、上下文实体和智能体的知识图谱，使用异构图神经网络进行信息传播，并基于性能信号训练生成智能体路由分布，结合软监督与加权聚合优化协作。

Result: 实验表明，该方法在多个基准和LLM主干上均优于单智能体和集成基线，具有良好的泛化能力。

Conclusion: 基于知识图谱监督的多智能体路由在问答任务中是有效且鲁棒的，能够充分利用不同智能体的互补优势。

Abstract: Large language models (LLMs) and agent-based frameworks have advanced
rapidly, enabling diverse applications. Yet, with the proliferation of models
and agentic strategies, practitioners face substantial uncertainty in selecting
the best configuration for a downstream task. Prior studies show that different
agents and backbones exhibit complementary strengths, and that larger models
are not always superior, underscoring the need for adaptive routing mechanisms.
Existing approaches to agent routing, however, often emphasize cost efficiency
while overlooking the fine-grained contextual and relational structure inherent
in QA tasks. In this paper, we propose tAgentRouter, a framework that
formulates multi-agent QA as a knowledge-graph-guided routing problem
supervised by empirical performance signals. Specifically, we convert QA
instance into a knowledge graph that jointly encodes queries, contextual
entities, and agents, and then train a heterogeneous graph neural network (GNN)
to propagate information across node types and produce task-aware routing
distributions over agents. By leveraging soft supervision and weighted
aggregation of agent outputs, AgentRouter learns principled collaboration
schemes that capture the complementary strengths of diverse agents. Extensive
experiments demonstrate that our framework consistently outperforms
single-agent and ensemble baselines, while generalizing across benchmarks and
LLM backbones. These results highlight the effectiveness and robustness of
graph-supervised multi-agent routing for question answering.

</details>


### [119] [SocialNLI: A Dialogue-Centric Social Inference Dataset](https://arxiv.org/abs/2510.05458)
*Akhil Deo,Kate Sanders,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了SocialNLI（SoNLI），首个用于评估语言模型在对话中理解复杂社交现象（如讽刺和反语）能力的数据集，旨在分析模型的心智理论能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在理解对话中的复杂社会现象（如讽刺、反语）方面表现不佳，缺乏专门用于评估其社交推理能力的数据集。

Method: 构建了一个名为SocialNLI的数据集，包含精心挑选的富含社交细微差别的对话转录文本，并配以推断、似然性评分和人工撰写的解释；通过多步反事实推理来评估模型的心智理论能力。

Result: SocialNLI为评估语言模型在社交推理和心智理论方面的能力提供了新基准，揭示了当前模型在理解复杂社交互动上的局限性。

Conclusion: SocialNLI填补了社交对话推理数据集的空白，有助于推动具备更强社交理解能力的AI助手的发展。

Abstract: Making theory-of-mind inferences from human dialogue is a strong indicator of
a model's underlying social abilities, which are fundamental for adept AI
assistants. However, large language and reasoning models struggle to understand
sophisticated social phenomena in transcript data, such as sarcasm and irony.
To assess the weaknesses of current models and to identify their solutions, we
introduce SocialNLI (SoNLI) -- the first social dialogue inference dataset.
SoNLI consists of a collection of dialogue transcripts hand-picked to center
complex social nuances like irony and sarcasm, paired with inferences,
corresponding likelihood scores, and human-written explanations. We explore
social inference analysis as a facet of theory-of-mind, and evaluate LLM and
reasoning model theory-of-mind ability through multi-step counterfactual
reasoning.

</details>


### [120] [TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation](https://arxiv.org/abs/2510.05485)
*Adam Filipek*

Main category: cs.CL

TL;DR: 本文提出了TensorBLEU，一种专为GPU加速设计的BLEU指标新实现，支持在训练过程中高效地对token ID进行批量、逐句计算，显著提升了自然语言处理模型评估速度。


<details>
  <summary>Details</summary>
Motivation: 现代NLP模型规模不断增大，但其评估工具常成为计算瓶颈，尤其是在强化学习等需要在训练中频繁计算每句奖励信号的场景下，传统基于CPU的评估方法效率低下。

Method: 提出了一种完全向量化、面向GPU的BLEU实现TensorBLEU，利用torch.unique构建紧凑的n-gram词典，采用内存高效的计数机制，避免传统哈希方法的高内存开销，直接在PyTorch中对token ID进行批处理计算。

Result: 实验表明，相比CPU上的NLTK标准实现，TensorBLEU在消费级GPU（如NVIDIA T4）上提速超过13倍，在数据中心级硬件（如NVIDIA A100）上超过40倍，显著降低了训练中的评估开销。

Conclusion: TensorBLEU将原本显著的评估瓶颈转化为训练流程中可忽略的部分，为RL微调等研究领域提供了高效的开发工具，并已开源以促进相关研究进展。

Abstract: Modern natural language processing models have achieved unprecedented scale,
yet the tools for their evaluation often remain a computational bottleneck,
limiting the pace of research. This is particularly acute for in-training
evaluation metrics, such as per-sentence reward signals in Reinforcement
Learning, which must operate efficiently on batches of token IDs directly on
the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the
BLEU metric designed from the ground up for this specific use case. Our
approach is fully vectorized for GPU-accelerated, per-sentence computation
within PyTorch and introduces a memory-efficient counting mechanism. By
creating a compact, batch-specific dictionary of n-grams using
\texttt{torch.unique}, our method avoids the prohibitive memory costs of
traditional hashing-based vectorization, making it practical for
large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard
library for token-ID-based BLEU calculation on the CPU. Experiments show that
TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and
exceeding 40x on data-center-class hardware (NVIDIA A100). This performance
transforms a significant bottleneck into a negligible part of the training
loop. By clearly defining its role as a "Token-ID BLEU" for development
purposes and open-sourcing our implementation, we provide a powerful tool for
accelerating research in areas like RL-based model fine-tuning.

</details>


### [121] [Language Model as Planner and Formalizer under Constraints](https://arxiv.org/abs/2510.05486)
*Cassie Huang,Stuti Mohan,Ziyi Yang,Stefanie Tellex,Li Zhang*

Main category: cs.CL

TL;DR: 本文通过在规划基准中引入细粒度的自然语言约束，揭示了当前大语言模型在复杂环境下的规划能力被高估的问题，并发现这些约束显著降低了模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有规划基准仅包含简单通用的环境描述，可能导致对大语言模型规划能力的高估，并引发下游任务的安全隐患。

Method: 在多个主流规划基准中手动添加涵盖四个形式化类别的细粒度自然语言约束，并在4种先进推理大模型、3种形式语言、5种方法和4个数据集上进行评估。

Result: 引入约束后，模型性能普遍下降一半，且在问题复杂性和词汇变化方面鲁棒性显著降低。

Conclusion: 细粒度环境约束能更真实地评估大语言模型的规划能力，揭示其在复杂现实场景中的局限性。

Abstract: LLMs have been widely used in planning, either as planners to generate action
sequences end-to-end, or as formalizers to represent the planning domain and
problem in a formal language that can derive plans deterministically. However,
both lines of work rely on standard benchmarks that only include generic and
simplistic environmental specifications, leading to potential overestimation of
the planning ability of LLMs and safety concerns in downstream tasks. We bridge
this gap by augmenting widely used planning benchmarks with manually annotated,
fine-grained, and rich natural language constraints spanning four formally
defined categories. Over 4 state-of-the-art reasoning LLMs, 3 formal languages,
5 methods, and 4 datasets, we show that the introduction of constraints not
only consistently halves performance, but also significantly challenges
robustness to problem complexity and lexical shift.

</details>


### [122] [LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation](https://arxiv.org/abs/2510.05490)
*Zhoutong Fu,Yihan Cao,Yi-Lin Chen,Aman Lunia,Liming Dong,Neha Saraf,Ruijie Jiang,Yun Dai,Qingquan Song,Tan Wang,Guoyao Li,Derek Koh,Haichao Wei,Zhipeng Wang,Aman Gupta,Chengming Jiang,Jianqiang Shen,Liangjie Hong,Wenjing Zhang*

Main category: cs.CL

TL;DR: 本文提出了LANTERN，一种针对职位匹配任务的大型语言模型知识蒸馏框架，通过多目标建模和多层次知识蒸馏提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 直接应用开源或微调的大型语言模型在职位人匹配任务中难以产生高质量、可操作的反馈，且模型体积大导致推理延迟高，限制了在线应用的扩展性。

Method: 提出LANTERN框架，包含用于分类的编码器模型和用于解释的解码器模型，并引入多层次知识蒸馏（数据级和logit级）以从强教师模型向下游模型迁移知识，结合后训练技术和提示工程优化模型适应特定领域任务。

Result: 实验结果表明，LANTERN显著提升了职位匹配与解释任务的任务特定指标，在线评估显示求职者参与度提高，申请率增加0.24%，合格申请量增加0.28%。

Conclusion: LANTERN有效解决了大规模语言模型在特定领域应用中的性能与效率问题，适用于需要结构化输出和低延迟的在线招聘场景。

Abstract: Large language models (LLMs) have achieved strong performance across a wide
range of natural language processing tasks. However, deploying LLMs at scale
for domain specific applications, such as job-person fit and explanation in job
seeking platforms, introduces distinct challenges. At LinkedIn, the job person
fit task requires analyzing a candidate's public profile against job
requirements to produce both a fit assessment and a detailed explanation.
Directly applying open source or finetuned LLMs to this task often fails to
yield high quality, actionable feedback due to the complexity of the domain and
the need for structured outputs. Moreover, the large size of these models leads
to high inference latency and limits scalability, making them unsuitable for
online use. To address these challenges, we introduce LANTERN, a novel LLM
knowledge distillation framework tailored specifically for job person fit
tasks. LANTERN involves modeling over multiple objectives, an encoder model for
classification purpose, and a decoder model for explanation purpose. To better
distill the knowledge from a strong black box teacher model to multiple
downstream models, LANTERN incorporates multi level knowledge distillation that
integrates both data and logit level insights. In addition to introducing the
knowledge distillation framework, we share our insights on post training
techniques and prompt engineering, both of which are crucial for successfully
adapting LLMs to domain specific downstream tasks. Extensive experimental
results demonstrate that LANTERN significantly improves task specific metrics
for both job person fit and explanation. Online evaluations further confirm its
effectiveness, showing measurable gains in job seeker engagement, including a
0.24\% increase in apply rate and a 0.28\% increase in qualified applications.

</details>


### [123] [Prototype-Based Dynamic Steering for Large Language Models](https://arxiv.org/abs/2510.05498)
*Ceyhun Efe Kayan,Li Zhang*

Main category: cs.CL

TL;DR: 提出了一种无需修改指令或微调的原型引导动态控制方法（PDS），通过在推理时利用链式思维与中性提示的激活差异来增强大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理增强方法依赖显式指令或静态控制策略，缺乏自适应性和通用性，难以在不增加成本的情况下有效提升推理能力。

Method: 引入“推理原型”概念，通过对链式思维（CoT）和中性提示下的激活差异进行聚类构建原型；在推理时将输入的隐藏状态投影到这些原型上，生成实例特定的控制向量以动态引导模型输出。

Result: 在GSM8K、AQuA-RAT和BIG-Bench等多个任务上验证了PDS的有效性，显著提升了模型准确率，且效果在抑制CoT时依然存在，表明其增强了模型内在的推理机制而非表面行为。

Conclusion: PDS是一种轻量级、无需训练或提示工程的推理增强方法，为提升大模型推理能力提供了新的思路。

Abstract: Despite impressive breadth, LLMs still rely on explicit reasoning
instructions or static, one-fits-all steering methods, leaving a gap for
adaptive, instruction-free reasoning amplification. We present Prototype-Based
Dynamic Steering (PDS), a test-time method that amplifies large language model
(LLM) reasoning without adding or altering instructions. We introduce
"reasoning prototypes" by clustering activation differences between
Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden
state is projected onto these prototypes to form an instance-specific steering
vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently
improves accuracy without fine-tuning or prompt engineering. Notably, the gains
persist even when CoT is explicitly suppressed to improve cost-efficiency,
indicating that the intervention strengthens latent reasoning processes rather
than inducing a superficial behavioral shift. These results position dynamic,
prototype-guided steering as a lightweight alternative to training-time
approaches for enhancing LLM reasoning.

</details>


### [124] [CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension](https://arxiv.org/abs/2510.05520)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Zihang Tian,Xu Chen,Quanyu Dai,Zhenhua Dong,Ruiming Tang*

Main category: cs.CL

TL;DR: 本文提出了一种受皮亚杰建构主义理论启发的构建式代理记忆（CAM），用于提升大语言模型在长文本理解中的记忆能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长文本时面临信息过载问题，缺乏系统性的记忆模块设计原则。

Method: 基于建构主义理论的三个特征——结构化图式、灵活同化和动态顺应，设计了CAM模型，并采用增量重叠聚类算法实现结构化记忆。

Result: CAM在多种长文本理解任务中表现出优于现有方法的性能与效率，包括问答、查询式摘要和声明验证。

Conclusion: CAM为大语言模型提供了更强大且高效的记忆系统，推动其向自主阅读代理发展。

Abstract: Current Large Language Models (LLMs) are confronted with overwhelming
information volume when comprehending long-form documents. This challenge
raises the imperative of a cohesive memory module, which can elevate vanilla
LLMs into autonomous reading agents. Despite the emergence of some heuristic
approaches, a systematic design principle remains absent. To fill this void, we
draw inspiration from Jean Piaget's Constructivist Theory, illuminating three
traits of the agentic memory -- structured schemata, flexible assimilation, and
dynamic accommodation. This blueprint forges a clear path toward a more robust
and efficient memory system for LLM-based reading comprehension. To this end,
we develop CAM, a prototype implementation of Constructivist Agentic Memory
that simultaneously embodies the structurality, flexibility, and dynamicity. At
its core, CAM is endowed with an incremental overlapping clustering algorithm
for structured memory development, supporting both coherent hierarchical
summarization and online batch integration. During inference, CAM adaptively
explores the memory structure to activate query-relevant information for
contextual response, akin to the human associative process. Compared to
existing approaches, our design demonstrates dual advantages in both
performance and efficiency across diverse long-text reading comprehension
tasks, including question answering, query-based summarization, and claim
verification.

</details>


### [125] [H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference](https://arxiv.org/abs/2510.05529)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: 本文提出了H1B-KV，一种用于大语言模型的混合单比特KV缓存压缩方案，通过1比特二值化键和4比特量化值显著降低内存占用，在保持上下文完整性的同时实现70倍的缓存压缩，并在多种下游任务上达到与全精度模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 自回归解码中KV缓存的内存开销限制了长上下文推理，现有压缩方法往往不完整（仅压缩键或丢弃信息），需要一种全面且高效的缓存压缩方案。

Method: H1B-KV采用混合策略：对键向量使用1比特二值化表示并支持位运算注意力，对值向量进行4比特量化，在训练后轻量微调以恢复精度。

Result: 在7B参数模型上实现8k上下文仅需60MB以下缓存（70倍压缩）；在GSM8K、MMLU、HumanEval等任务上性能媲美全精度模型，且优于KIVI、SparseLLM和Loki等现有方法。

Conclusion: H1B-KV是一种高效、完整的KV缓存压缩方案，显著提升了大模型在内存受限环境下的长上下文部署能力，具有较强的实用性和性能优势。

Abstract: Autoregressive decoding in large language models (LLMs) requires caching a
growing list of past key-value (KV) pairs, making long-context inference a
memory-bound problem. While recent methods have explored quantizing the cache,
evicting tokens, or using binary sketches for keys (e.g., Loki), these
approaches often provide an incomplete solution by leaving one component (like
values) uncompressed or by discarding context information. This paper
introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression
scheme that radically reduces memory usage without sacrificing context. H1B-KV
represents each key vector using a 1-bit binary sketch, enabling
hardware-friendly bitwise attention, and further compresses value vectors using
4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter
LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x
reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches
full-precision performance not only on perplexity benchmarks but also on
complex downstream tasks like mathematical reasoning (GSM8K), multi-task
understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV
significantly outperforms leading quantization (KIVI), token eviction
(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,
establishing it as a robust solution for deploying LLMs in memory-constrained
environments.

</details>


### [126] [On the Role of Difficult Prompts in Self-Play Preference Optimization](https://arxiv.org/abs/2510.05534)
*Yao Xiao,Jung-jae Kim,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 研究探讨了不同难度提示在自对弈偏好优化中的影响，发现较难的提示会降低语言模型的优化性能，且加入难提示反而导致整体表现下降；通过选择性移除部分困难提示可提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 提示在自对弈偏好优化中起关键作用，但其难度影响尚未被充分探索，本文旨在研究提示难度如何影响优化效果。

Method: 使用N个采样响应的平均奖励作为提示难度的代理指标，比较不同难度提示下的自对弈偏好优化表现，并探索移除困难提示等缓解策略。

Result: 难提示显著降低优化性能，加入难提示导致整体表现轻微下降；随着模型容量增加，难易提示的性能差距缩小；选择性移除部分困难提示可提升最终性能。

Conclusion: 提示难度显著影响自对弈偏好优化效果，合理筛选提示有助于提升语言模型对齐性能。

Abstract: Self-play preference optimization has emerged as a prominent paradigm for
aligning large language models (LLMs). It typically involves a language model
to generate on-policy responses for prompts and a reward model (RM) to guide
the selection of chosen and rejected responses, which can be further trained
with direct preference optimization (DPO). However, the role of prompts remains
underexplored, despite being a core component in this pipeline. In this work,
we investigate how prompts of varying difficulty influence self-play preference
optimization. We first use the mean reward of $N$ sampled responses of a prompt
as a proxy for its difficulty. We find that difficult prompts exhibit
substantially inferior self-play optimization performance in comparison to easy
prompts for language models. Moreover, incorporating difficult prompts into
training fails to enhance overall performance and, in fact, leads to slight
degradation compared to training on easy prompts alone. We also observe that
the performance gap between difficult and easy prompts closes as the model
capacity increases, suggesting that difficulty interacts with the model
capacity. Building on these findings, we explore strategies to mitigate the
negative effect of difficult prompts on final performance. We demonstrate that
selectively removing an appropriate portion of challenging prompts enhances
overall self-play performance, while also reporting failed attempts and lessons
learned.

</details>


### [127] [Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM](https://arxiv.org/abs/2510.05544)
*Ryan Solgi,Parsa Madinei,Jiayi Tian,Rupak Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于帕累托引导的奇异值分解（PGSVD）方法，用于大语言模型和视觉语言模型的低秩压缩，在保证精度的同时实现了更高的压缩率和推理加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型在部署时面临巨大的内存和计算挑战，需要有效的压缩方法。

Method: 提出了一个理论框架，通过逐层激活误差上界来指导低秩压缩，并将压缩问题建模为双目标优化问题，进而提出PGSVD方法，采用帕累托引导的秩选择和交替最小二乘法实现零样本压缩。

Result: 在LLM和VLM上的实验表明，PGSVD在相同压缩水平下优于现有方法，具有更高的准确性和推理速度。

Conclusion: PGSVD是一种有效的零样本低秩压缩方法，能够根据理论指导自动选择各层的最优秩，实现性能与效率的平衡。

Abstract: Large language models (LLM) and vision-language models (VLM) have achieved
state-of-the-art performance, but they impose significant memory and computing
challenges in deployment. We present a novel low-rank compression framework to
address this challenge. First, we upper bound the change of network loss via
layer-wise activation-based compression errors, filling a theoretical gap in
the literature. We then formulate low-rank model compression as a bi-objective
optimization and prove that a single uniform tolerance yields surrogate
Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we
propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot
pipeline that improves activation-aware compression via Pareto-guided rank
selection and alternating least-squares implementation. We apply PGSVD to both
LLM and VLM, showing better accuracy at the same compression levels and
inference speedup.

</details>


### [128] [Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations](https://arxiv.org/abs/2510.05571)
*Chengzhi Liu,Yuzhe Yang,Kaiwen Zhou,Zhen Zhang,Yue Fan,Yannan Xie,Peng Qi,Xin Eric Wang*

Main category: cs.CL

TL;DR: 本文提出了EvoPresent，一个通过虚拟角色实现学术报告自优化的框架，核心是PresAesth多任务强化学习模型，用于提升演示内容的叙事性、美观性和表达效果，并构建了EvoPresent Benchmark进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有自动化论文展示方法在叙事连贯性、美学质量和自我调整能力方面存在不足，且缺乏有效的评估机制，难以实现高效传播。

Method: 提出EvoPresent框架，结合虚拟角色生成具表现力的学术报告；设计PresAesth多任务强化学习模型，实现美学评分、缺陷修正和对比反馈；构建EvoPresent Benchmark，包含高质量生成数据集和美学感知评估集。

Result: 实验表明：高质量反馈对代理自改进至关重要；自动化生成在视觉设计与内容构建间存在权衡；多任务RL在美学感知任务中具有更强泛化能力。

Conclusion: EvoPresent通过统一叙事、美学与表达，实现了可评估、可迭代的学术报告自优化，为学术传播提供了有效自动化解决方案。

Abstract: The promotion of academic papers has become an important means of enhancing
research visibility. However, existing automated methods struggle limited
storytelling, insufficient aesthetic quality, and constrained self-adjustment,
making it difficult to achieve efficient and engaging dissemination. At the
heart of those challenges is a simple principle: \emph{there is no way to
improve it when you cannot evaluate it right}. To address this, we introduce
\textbf{EvoPresent}, a self-improvement agent framework that unifies coherent
narratives, aesthetic-aware designs, and realistic presentation delivery via
virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task
reinforcement learning (RL) aesthetic model that provides reliable aesthetic
scoring, defect adjustment, and comparative feedback, enabling iterative
self-improvement even under limited aesthetic training data. To systematically
evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a
comprehensive benchmark comprising: \textit{Presentation Generation Quality},
built on 650 top-tier AI conference papers with multimodal resources (slides,
videos and scripts) to assess both content and design; and \textit{Aesthetic
Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels,
supporting joint training and evaluation on scoring, defect adjustment, and
comparison. Our findings highlight that (i) High-quality feedback is essential
for agent self-improvement, while initial capability alone does not guarantee
effective self-correction. (ii) Automated generation pipelines exhibit a
trade-off between visual design and content construction. (iii) Multi-task RL
training shows stronger generalization in aesthetic awareness tasks.

</details>


### [129] [Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs](https://arxiv.org/abs/2510.05577)
*Dong Yan,Gaochen Wu,Bowen Zhou*

Main category: cs.CL

TL;DR: 提出了一种名为FGDIP的动态交互式规划框架，通过结合历史错误分析和实时反馈，提升大语言模型在开放域多跳推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理需要大量信息检索的开放域多跳推理任务时，因依赖固定动作序列而表现受限。

Method: 基于关键实体识别构建初始节点，采用深度优先搜索与创新的节点生成技术，结合历史错误分析和实时反馈动态调整推理策略。

Result: 在HotpotQA和StrategyQA数据集上分别达到54.47%和70.05%的F1分数，超越最佳基线5.03%和7.25%。

Conclusion: FGDIP通过动态适应性规划显著提升了语言代理在复杂推理任务中的性能和收敛性。

Abstract: Recent advancements in language agents have led to significant improvements
in multi-hop reasoning tasks. However, existing approaches often struggle with
handling open-domain problems, which require massive information retrieval due
to their reliance on a fixed sequence of actions. To address this, we propose
Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework
tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive
strategies for information exploration in open-domain multi-hop reasoning
tasks. Our approach begins by identifying key entities relevant to the problem,
which serve as the initial nodes in the reasoning process. From these initial
nodes, we then generate reasoning child nodes with the process being refined
through a combination of historical error analysis and real-time feedback,
which allows the framework to dynamically adjust and optimize its reasoning
strategies. By integrating depth-first search with an innovative node
generation technique, our framework adapts based on both prior error paths and
concurrently generated nodes at the same hierarchical level. This dynamic
strategy effectively expands the search space while ensuring the reasoning
process systematically converges toward accurate solutions. Experimental
results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset
and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and
7.25% respectively, highlighting its versatility and potential to enhance
language agents in multi-hop reasoning tasks.

</details>


### [130] [A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks](https://arxiv.org/abs/2510.05608)
*Shuzheng Si,Haozhe Zhao,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为EAGLET的高效规划器训练方法，通过两步流程提升执行代理在长视野任务中的全局规划能力，无需人工干预，在三个任务上实现了最先进的性能，同时将训练成本降低了8倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的代理在长视野任务中因缺乏全局规划而容易产生盲目试错和幻觉动作，因此需要一种高效且无需人工参与的规划增强方法。

Method: 提出一种计划-执行框架，采用两步训练策略：首先利用同源共识过滤策略从高级LLM生成高质量计划并进行微调作为冷启动；然后通过基于规则的强化学习阶段，使用新颖的执行器能力增益奖励进一步优化规划器。

Result: 在三个长视野代理任务上的实验表明，配备该规划器的执行代理优于现有方法，达到最先进性能，同时训练成本比基于强化学习的基线降低8倍，且无需人工标注或额外训练数据。

Conclusion: EAGLET提供了一种高效、有效且可扩展的规划器训练方案，显著提升了代理在复杂长视野任务中的规划能力，具有实际应用价值。

Abstract: Agents based on large language models (LLMs) struggle with brainless
trial-and-error and generating hallucinatory actions due to a lack of global
planning in long-horizon tasks. In this paper, we introduce a plan-and-execute
framework and propose EAGLET, an efficient and effective planner training
method to enhance the executor agent's planning abilities without human effort.
Specifically, we train a plug-and-play global planner through a two-step
process: we first synthesize high-quality plans from an advanced LLM using our
proposed homologous consensus filtering strategy, and apply fine-tuning as a
cold start. Moreover, we further improve the planner with a rule-based
reinforcement learning stage using a novel executor capability gain reward,
ensuring it can handle task instructions of varying difficulty. Experiments on
three long-horizon agent tasks show that executor agents equipped with our
planner outperform existing methods, achieving new state-of-the-art
performance. Meanwhile, EAGLET reduces training costs by 8x compared to
RL-based baselines, and it does not require manual effort or extra training
data, offering an efficient and effective solution.

</details>


### [131] [MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction](https://arxiv.org/abs/2510.05611)
*Wei-Chieh Huang,Cornelia Caragea*

Main category: cs.CL

TL;DR: 本文提出了一种基于多智能体辩论框架的隐式属性值提取方法（\modelname），利用多个MLLM智能体通过多轮辩论迭代优化推理结果，显著提升了在多模态电商数据中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 隐式属性值提取在电商中至关重要，但由于多维数据复杂性和视觉-文本理解差距，现有方法仍面临挑战。

Method: 提出多智能体辩论框架，多个MLLM智能体通过多轮辩论相互验证和更新推理结果，以提升性能。

Result: 在ImplicitAVE数据集上的实验表明，即使少量辩论轮次也能显著提高准确率，尤其对初始性能较差的属性效果更明显；并系统评估了不同辩论配置的影响。

Conclusion: 多智能体辩论策略能有效克服单智能体局限性，为多模态电商中的隐式AVE提供了可扩展的解决方案。

Abstract: Implicit Attribute Value Extraction (AVE) is essential for accurately
representing products in e-commerce, as it infers lantent attributes from
multimodal data. Despite advances in multimodal large language models (MLLMs),
implicit AVE remains challenging due to the complexity of multidimensional data
and gaps in vision-text understanding. In this work, we introduce
\textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM
agents to iteratively refine inferences. Through a series of debate rounds,
agents verify and update each other's responses, thereby improving inference
performance and robustness. Experiments on the ImplicitAVE dataset demonstrate
that even a few rounds of debate significantly boost accuracy, especially for
attributes with initially low performance. We systematically evaluate various
debate configurations, including identical or different MLLM agents, and
analyze how debate rounds affect convergence dynamics. Our findings highlight
the potential of multi-agent debate strategies to address the limitations of
single-agent approaches and offer a scalable solution for implicit AVE in
multimodal e-commerce.

</details>


### [132] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出CoSpaDi，一种无需训练的LLM压缩框架，采用结构化稀疏分解和稀疏字典学习，优于传统低秩方法。


<details>
  <summary>Details</summary>
Motivation: 传统低秩权重近似压缩大语言模型存在结构刚性，导致精度显著下降。

Method: 用稠密字典和列稀疏系数矩阵替代低秩分解，实现多子空间表示，并利用校准数据集优化因子分解以匹配原始层输出激活。

Result: 在多个Llama和Qwen模型上，20-50%压缩比下，CoSpaDi在准确性和困惑度上均优于最先进的数据感知低秩方法。

Conclusion: 结构化稀疏字典学习是高效大语言模型部署中传统低秩方法的有力替代方案。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [133] [The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP](https://arxiv.org/abs/2510.05644)
*Sheriff Issaka,Keyi Wang,Yinka Ajibola,Oluwatumininu Samuel-Ipaye,Zhaoyi Zhang,Nicte Aguillon Jimenez,Evans Kofi Agyei,Abraham Lin,Rohan Ramachandran,Sadick Abdul Mumin,Faith Nchifor,Mohammed Shuraim,Lieqi Liu,Erick Rosas Gonzalez,Sylvester Kpei,Jemimah Osei,Carlene Ajeneza,Persis Boateng,Prisca Adwoa Dufie Yeboah,Saadia Gabriel*

Main category: cs.CL

TL;DR: 本文介绍了非洲语言实验室（All Lab），旨在解决非洲语言在现代自然语言处理技术中的严重不足。研究贡献包括：构建了涵盖40种语言的大规模多模态数据集、通过实验验证数据集显著提升模型性能，以及培养本地科研人才的可持续计划。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在计算语言学中大多被忽视或代表性不足，导致NLP技术发展严重滞后，亟需系统性支持以缩小技术鸿沟。

Method: 建立质量可控的数据收集流程，构建大规模非洲语言文本与语音数据集，并通过微调现有模型进行实验验证；同时开展结构化研究培训项目以提升本地研究能力。

Result: 构建了包含40种非洲语言、190亿词文本和12,628小时语音的高质量数据集；在31种语言上微调模型后，平均提升+23.69 ChrF++、+0.33 COMET和+15.34 BLEU；并与Google Translate对比显示部分语言具备竞争力。

Conclusion: 该研究有效推动了非洲语言的NLP发展，展示了高质量数据和本地能力建设的重要性，为未来低资源语言的技术进步提供了可复制的模式。

Abstract: Despite representing nearly one-third of the world's languages, African
languages remain critically underserved by modern NLP technologies, with 88\%
classified as severely underrepresented or completely ignored in computational
linguistics. We present the African Languages Lab (All Lab), a comprehensive
research initiative that addresses this technological gap through systematic
data collection, model development, and capacity building. Our contributions
include: (1) a quality-controlled data collection pipeline, yielding the
largest validated African multi-modal speech and text dataset spanning 40
languages with 19 billion tokens of monolingual text and 12,628 hours of
aligned speech data; (2) extensive experimental validation demonstrating that
our dataset, combined with fine-tuning, achieves substantial improvements over
baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points
across 31 evaluated languages; and (3) a structured research program that has
successfully mentored fifteen early-career researchers, establishing
sustainable local capacity. Our comparative evaluation against Google Translate
reveals competitive performance in several languages while identifying areas
that require continued development.

</details>


### [134] [Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models](https://arxiv.org/abs/2510.05678)
*Haneul Yoo,Jiho Jin,Kyunghyun Cho,Alice Oh*

Main category: cs.CL

TL;DR: 提出代码切换上下文学习（CSICL），通过在提示中从目标语言逐步过渡到英语，有效缓解大模型在非英语语言中的翻译障碍，提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖英语作为潜在表征，导致非英语语言性能下降，形成翻译障碍；现有跨语言上下文学习方法难以缓解此问题。

Method: 引入代码切换上下文学习（CSICL），在指令和示例中从目标语言逐步切换到英语，显式引导模型的潜在推理过程。

Result: 在4个大模型、6个数据集和10种语言上验证，CSICL在目标语言和未见语言上分别提升3.1%p和1.9%p，低资源下提升达14.7%和5.3%。

Conclusion: CSICL是一种有效且鲁棒的方法，能克服推理中的翻译障碍，推动更公平高效的多语言大模型发展。

Abstract: While large language models (LLMs) exhibit strong multilingual abilities,
their reliance on English as latent representations creates a translation
barrier, where reasoning implicitly depends on internal translation into
English. When this process fails, performance in non-English languages
deteriorates sharply, limiting the inclusiveness of LLM-based applications.
Existing cross-lingual in-context learning (X-ICL) methods primarily leverage
monolingual demonstrations, often failing to mitigate this barrier and instead
reinforcing it. In this work, we introduce code-switching in-context learning
(CSICL), a simple yet effective prompting strategy that progressively
transitions from a target language to English within demonstrations and
instruction to facilitate their latent reasoning in English. By explicitly
scaffolding the reasoning process through controlled code-switching, CSICL acts
as an implicit linguistic bridge that enhances cross-lingual alignment and
reduces reliance on the translation barrier. We conduct extensive experiments
across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive
and reasoning-oriented domains. Our results demonstrate that CSICL consistently
outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target
and unseen languages, respectively. The improvement is even more pronounced in
low-resource settings, with gains of 14.7% in target and 5.3% in unseen
languages. These findings establish code-switching as a principled and robust
approach for overcoming the translation barrier during inference, moving LLMs
toward more equitable and effective multilingual systems.

</details>


### [135] [DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision](https://arxiv.org/abs/2510.05691)
*Yongqi Leng,Yikun Lei,Xikai Liu,Meizhi Zhong,Bojian Xiong,Yurong Zhang,Yan Gao,Yi Wu,Yao Hu,Deyi Xiong*

Main category: cs.CL

TL;DR: 提出DecEx-RAG，通过建模为马尔可夫决策过程并引入剪枝策略，显著提升RAG在复杂任务中的性能与数据构建效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果监督的强化学习方法存在探索效率低、奖励信号稀疏和全局反馈模糊的问题，限制了Agentic RAG的性能。

Method: 将RAG建模为包含决策与执行的马尔可夫决策过程（MDP），并通过高效的剪枝策略优化数据扩展，实现过程级策略优化。

Result: 在六个数据集上平均绝对性能提升6.2%，数据构造效率提高近6倍，显著优于现有基线方法。

Conclusion: DecEx-RAG有效提升了大语言模型在自主任务分解、动态检索和高质量回答生成方面的能力，为过程监督的RAG训练提供了高效解决方案。

Abstract: Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.

</details>


### [136] [Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities](https://arxiv.org/abs/2510.05744)
*Liza Fretel,Baptiste Cecconi,Laura Debisschop*

Main category: cs.CL

TL;DR: 本文提出了一种多源天文观测设施映射的生成方法，利用NLP技术和可调标准计算实体匹配分数，并结合大语言模型验证映射结果的合理性与FAIR性。


<details>
  <summary>Details</summary>
Motivation: 为了整合分散在多个语义资源中的天文观测设施信息，实现标准化和互操作性，支持虚拟天文台的数据发现与集成。

Method: 通过从八个语义资源（如Wikidata）中提取实体，使用Bag-of-Words、序列和表面等NLP方法，结合标签、定义、外部标识符及领域属性（如观测波段、发射日期等）计算匹配分数，并利用大语言模型对映射建议进行判断与解释。

Result: 生成了包含多源同义词集的映射结果，每个实体仅保留一个标准化标签，并确保映射的合理性和FAIR原则（可发现、可访问、可互操作、可重用）。

Conclusion: 该方法能有效整合多源天文设施数据，生成高质量、可解释的标准化映射，可用于Name Resolver API及IVOA词汇表和OntoPortal-Astro平台。

Abstract: This ongoing work focuses on the development of a methodology for generating
a multi-source mapping of astronomical observation facilities. To compare two
entities, we compute scores with adaptable criteria and Natural Language
Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches,
and surface approaches) to map entities extracted from eight semantic
artifacts, including Wikidata and astronomy-oriented resources. We utilize
every property available, such as labels, definitions, descriptions, external
identifiers, and more domain-specific properties, such as the observation
wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a
Large Language Model (LLM) to accept or reject a mapping suggestion and provide
a justification, ensuring the plausibility and FAIRness of the validated
synonym pairs. The resulting mapping is composed of multi-source synonym sets
providing only one standardized label per entity. Those mappings will be used
to feed our Name Resolver API and will be integrated into the International
Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro
platform.

</details>


### [137] [Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes](https://arxiv.org/abs/2510.05767)
*Peter Ochieng*

Main category: cs.CL

TL;DR: 提出了基于对齐、温度和批量谱的非渐近谱带，用于限制InfoNCE梯度范数的平方，并设计了谱感知的批量选择方法，提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和控制对比学习中InfoNCE损失的梯度行为，尤其是在不同温度和批量组成下的稳定性与收敛性。

Method: 通过引入有效秩作为各向异性的代理，推导出非渐近谱带以约束梯度范数，并提出谱感知批量选择策略（如快速贪心构建器）和批内白化方法。

Result: 在ImageNet-100上，Greedy-64将达到67.5% top-1精度的时间减少了15%（相比随机选择），CIFAR-10也显示类似增益；批内白化使梯度方差在50步内降低1.37倍，符合理论上界。

Conclusion: 谱结构对对比学习的梯度稳定性至关重要，谱感知的批量构造和白化策略可显著提升训练效率和梯度一致性。

Abstract: We derive non-asymptotic spectral bands that bound the squared InfoNCE
gradient norm via alignment, temperature, and batch spectrum, recovering the
\(1/\tau^{2}\) law and closely tracking batch-mean gradients on synthetic data
and ImageNet. Using effective rank \(R_{\mathrm{eff}}\) as an anisotropy proxy,
we design spectrum-aware batch selection, including a fast greedy builder. On
ImageNet-100, Greedy-64 cuts time-to-67.5\% top-1 by 15\% vs.\ random (24\%
vs.\ Pool--P3) at equal accuracy; CIFAR-10 shows similar gains. In-batch
whitening promotes isotropy and reduces 50-step gradient variance by
\(1.37\times\), matching our theoretical upper bound.

</details>


### [138] [InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience](https://arxiv.org/abs/2510.05769)
*Jianbin Shen,Christy Jie Liang,Junyu Xuan*

Main category: cs.CL

TL;DR: 本文提出了一种基于最优传输和累积联合熵减少的新型学习方法，以提升抽象文本摘要的信息量。


<details>
  <summary>Details</summary>
Motivation: 现有的摘要方法在信息丰富性方面仍有改进空间，尤其是在处理大规模、长文本数据时需要更高效地提取关键信息。

Method: 采用基于最优传输的注意力机制来聚焦参考摘要中的重要信息，并通过命名实体上的累积联合熵减少方法增强信息显著性。

Result: 在CNN/Daily Mail数据集上取得了优于先前工作的ROUGE分数，在XSum数据集上表现具有竞争力，且人工评估显示该方法在信息丰富性上优于强基线模型。

Conclusion: 所提出的方法能有效提升摘要的信息量和质量，尤其在信息聚焦和显著性建模方面具有优势。

Abstract: Abstractive text summarization is integral to the Big Data era, which demands
advanced methods to turn voluminous and often long text data into concise but
coherent and informative summaries for efficient human consumption. Despite
significant progress, there is still room for improvement in various aspects.
One such aspect is to improve informativeness. Hence, this paper proposes a
novel learning approach consisting of two methods: an optimal transport-based
informative attention method to improve learning focal information in reference
summaries and an accumulative joint entropy reduction method on named entities
to enhance informative salience. Experiment results show that our approach
achieves better ROUGE scores compared to prior work on CNN/Daily Mail while
having competitive results on XSum. Human evaluation of informativeness also
demonstrates the better performance of our approach over a strong baseline.
Further analysis gives insight into the plausible reasons underlying the
evaluation results.

</details>


### [139] [Mixture of Neuron Experts](https://arxiv.org/abs/2510.05781)
*Runxi Cheng,Yuchen Guan,Yucheng Ding,Qingguo Hu,Yongxian Wei,Chun Yuan,Yelong Shen,Weizhu Chen,Yeyun Gong*

Main category: cs.CL

TL;DR: 本文提出了一种新的神经元级专家混合模型MoNE，通过仅激活高激活度的神经元专家，在保持传统MoE性能的同时显著提升了参数利用效率和推理效率。


<details>
  <summary>Details</summary>
Motivation: 观察到在MoE层中大部分神经元激活值接近零，表明存在大量冗余参数，因此希望设计一种更高效的专家选择机制以提升参数利用率和推理效率。

Method: 通过对多个代表性MoE模型进行稀疏化研究，分析专家内部参数激活情况，并将专家分解为神经元粒度的MoE，提出Mixture of Neuron Experts（MoNE），在每个专家内应用简单的top-k选择机制实现神经元粒度的专家选择。

Result: MoNE在仅激活50% MoE层参数的情况下即可达到与传统MoE相当的性能，在相同激活参数数量下始终优于传统MoE，且引入的延迟可忽略不计，无需额外路由参数或专家间通信。

Conclusion: MoNE是一种实用且高效的方法，能够有效提升MoE类模型的参数利用效率和推理效率，适用于需要高效率推理的大规模模型场景。

Abstract: In this work, we first explore whether the parameters activated by the MoE
layer remain highly sparse at inference. We perform a sparsification study on
several representative MoE models. For each expert, we rank parameters by the
magnitude of their activations from the gate projection and progressively prune
the activated subset. Pruning up to 60% of parameters within that subset causes
only negligible task-performance degradation; substantial drops occur only
after more than 90% are removed. We further decompose experts into
neuron-granular MoE and visualize their activation values, finding that most
neuron activations are near zero. This observation motivates us to select only
high-activation neuron experts during pretraining. Based on this insight, we
propose Mixture of Neuron Experts (MoNE). MoNE achieves neuron-granular expert
selection by only applying a simple top-k selection within each expert, incurs
negligible latency, and requires no additional routing parameters or
inter-expert communication. Extensive experiments demonstrate that MoNE matches
traditional MoE performance while activating only 50% of the MoE-layer
parameters, and it consistently outperforms traditional MoE when compared at
equal numbers of activated parameters. These results suggest that MoNE is a
practical approach to improving parameter utilization and inference efficiency
in MoE-like models.

</details>


### [140] [Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech](https://arxiv.org/abs/2510.05799)
*Rikuto Kotoge,Yuichi Sasaki*

Main category: cs.CL

TL;DR: 提出TKTO方法，无需配对数据且直接针对词元级别进行优化，显著提升日语TTS的准确性和发音对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好优化的TTS系统依赖成对的优劣样本，且仅在语句级别进行优化，限制了数据效率和发音对齐的精细度。

Method: 提出TKTO，通过消除对配对数据的需求，并在词元级别自动提供细粒度对齐信号，实现更高效的数据训练。

Result: TKTO使日语TTS准确率提升39%，CER降低54%，并对目标词元赋予高达12.8倍的强化奖励。

Conclusion: TKTO在无需词元级标注的情况下，实现了高效、细粒度的偏好优化，显著改善了TTS系统的发音准确性与自然度。

Abstract: Aligning text-to-speech (TTS) system outputs with human feedback through
preference optimization has been shown to effectively improve the robustness
and naturalness of language model-based TTS models. Current approaches
primarily require paired desirable and undesirable samples at the utterance
level. However, such pairs are often limited in TTS output data, and
utterance-level formulation prevents fine-grained token-level optimization
needed for accurate pronunciation alignment. In this study, we propose TKTO
that eliminates the need for paired data, enabling a more data-efficient
training paradigm, and directly targets token-level units, automatically
providing fine-grained alignment signals without token-level annotations. TKTO
improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,
automatically assigning 12.8 times stronger reward to targeted tokens.

</details>


### [141] [EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget](https://arxiv.org/abs/2510.05837)
*Liang Chen,Xueting Han,Qizhou Wang,Bo Han,Jing Bai,Hinrich Schutze,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 本文提出了一种名为EEPO的探索增强策略优化框架，通过两阶段 rollout 和自适应去学习机制来改善大语言模型在强化学习中的探索与利用平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法过度强调利用，导致熵崩溃和探索能力下降，难以突破主导行为模式，限制了性能提升。

Method: 采用两阶段rollout与轻量级去学习机制：第一阶段生成部分轨迹，随后暂时抑制这些采样响应，迫使第二阶段探索输出空间的新区域，从而打破自我强化循环。

Result: 在五个推理基准上，EEPO相比GRPO取得了显著增益，平均相对提升分别为Qwen2.5-3B上24.3%、Llama3.2-3B-Instruct上33.0%、Qwen3-8B-Base上10.4%。

Conclusion: EEPO通过样本后遗忘机制有效促进探索，打破了传统方法中重复采样主导模式的循环，显著提升了大语言模型在强化学习环境下的性能。

Abstract: Balancing exploration and exploitation remains a central challenge in
reinforcement learning with verifiable rewards (RLVR) for large language models
(LLMs). Current RLVR methods often overemphasize exploitation, leading to
entropy collapse, diminished exploratory capacity, and ultimately limited
performance gains. Although techniques that increase policy stochasticity can
promote exploration, they frequently fail to escape dominant behavioral modes.
This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant
modes-that further erodes exploration. We introduce Exploration-Enhanced Policy
Optimization (EEPO), a framework that promotes exploration via two-stage
rollouts with adaptive unlearning. In the first stage, the model generates half
of the trajectories; it then undergoes a lightweight unlearning step to
temporarily suppress these sampled responses, forcing the second stage to
explore different regions of the output space. This sample-then-forget
mechanism disrupts the self-reinforcing loop and promotes wider exploration
during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,
achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on
Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.

</details>


### [142] [Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer](https://arxiv.org/abs/2510.05846)
*Maxence Lasbordes,Sinoué Gad*

Main category: cs.CL

TL;DR: 本文提出了Luth，一个专注于法语的小型语言模型系列，通过高质量的法语数据后训练和策略性模型融合，在多个法语基准上超越了同等规模的开源模型，同时保持了原有的英语能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言模型在法语上的表现明显弱于英语，且针对法语的高效适配方法研究有限，因此需要专门优化法语性能的小型语言模型。

Method: 通过对高质量、精选的法语数据进行针对性的后训练，并采用策略性模型融合方法，提升模型在法语任务中的表现，同时保留其英语能力。

Result: Luth在多个法语基准测试中超过了同规模的开源模型，成为当前最先进的法语小型语言模型，并在英语任务上保持了原有性能。

Conclusion: Luth为法语小型语言模型设立了新的标杆，提供了未来法语语言研究的有力基线。

Abstract: The landscape of Large Language Models (LLMs) remains predominantly
English-centric, resulting in a significant performance gap for other major
languages, such as French, especially in the context of Small Language Models
(SLMs). Existing multilingual models demonstrate considerably lower performance
in French compared to English, and research on efficient adaptation methods for
French remains limited. To address this, we introduce \textbf{Luth}, a family
of French-specialized SLMs: through targeted post-training on curated,
high-quality French data, our models outperform all open-source counterparts of
comparable size on multiple French benchmarks while retaining their original
English capabilities. We further show that strategic model merging enhances
performance in both languages, establishing Luth as a new state of the art for
French SLMs and a robust baseline for future French-language research.

</details>


### [143] [DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization](https://arxiv.org/abs/2510.05858)
*Xue-Yong Fu,Elena Khasanova,Md Tahmid Rahman Laskar,Harsh Saini,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 本研究探索了通过持续预训练（continual pre-training）来自适应大语言模型（LLMs）以提升其在真实世界对话文本中的摘要能力，使用大规模无标签商业对话数据进行实验，结果表明该方法在领域内和跨领域任务中均显著提升性能，同时保持良好的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用文本摘要上表现良好，但在专业领域或与预训练分布不同的对话数据上效果不佳，且依赖昂贵且稀缺的标注数据进行微调，因此需要一种可扩展、自监督的方法来提升其在特定领域（如嘈杂对话）中的摘要能力。

Method: 采用持续预训练策略，在大规模无标签商业对话数据上对大语言模型进行进一步预训练，以自监督方式增强模型对对话结构和内容的理解，并系统评估不同数据选择策略的影响。

Result: 持续预训练在领域内和跨领域摘要基准上均带来显著性能提升，同时保持模型的泛化能力和对噪声的鲁棒性；研究还发现适当的数据筛选策略能进一步优化效果。

Conclusion: 持续预训练是一种高效、可扩展的方案，适用于将大语言模型适配到以摘要为核心的工业级应用场景，尤其适合缺乏高质量标注数据的领域。

Abstract: Large language models (LLMs) have achieved impressive performance in text
summarization, yet their performance often falls short when applied to
specialized domains %or conversational data that differ from their original
pre-training distribution. While fine-tuning can improve summarization quality,
it typically relies on costly and scarce high-quality labeled data. In this
work, we explore continual pre-training as a scalable, self-supervised approach
to adapt LLMs for downstream summarization tasks, particularly in the context
of noisy real-world conversation transcripts. We conduct extensive experiments
using large-scale, unlabeled business conversation data to investigate whether
continual pre-training enhances model capabilities in conversational
summarization. Our results demonstrate that continual pre-training yields
substantial gains in both in-domain and out-of-domain summarization benchmarks,
while maintaining strong generalization and robustness. We also analyze the
effects of data selection strategies, providing practical guidelines for
applying continual pre-training in summarization-focused industrial
applications.

</details>


### [144] [Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies](https://arxiv.org/abs/2510.05860)
*Luka Nenadic,David Rodriguez*

Main category: cs.CL

TL;DR: 本研究评估了2023年瑞士隐私法修订后的合规情况，利用GPT-5构建多语言基准数据集，发现自动化合同生成器显著提升了企业隐私政策的合规水平，尤其对资源有限的小型企业具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 面对日益复杂的数字法规，尤其是资源有限的小企业难以负担高昂的法律咨询费用，亟需评估低成本自动化法律工具（如自动合同生成器）在提升合规性方面的实际效果。

Method: 研究构建并标注了一个涵盖瑞士和欧盟隐私法关键合规要求的多语言基准数据集，并提出一种基于GPT-5的大规模隐私政策合规性评估方法，用于量化法律修订的影响及合同生成器的效果。

Result: 实证结果显示，2023年瑞士隐私法修订后整体合规率上升；使用自动化合同生成器的企业隐私政策合规率显著提高，最高提升达15个百分点；18%的本地网站明确引用了这些生成器。

Conclusion: 自动化合同生成器能有效提升企业（尤其是中小企业）对复杂隐私法规的合规能力；研究同时验证了LLM在跨语言法律分析中的潜力，并支持了欧盟法规‘布鲁塞尔效应’的存在，凸显了自动化工具在改善合同质量与法律遵从中的关键作用。

Abstract: It has become increasingly challenging for firms to comply with a plethora of
novel digital regulations. This is especially true for smaller businesses that
often lack both the resources and know-how to draft complex legal documents.
Instead of seeking costly legal advice from attorneys, firms may turn to
cheaper alternative legal service providers such as automated contract
generators. While these services have a long-standing presence, there is little
empirical evidence on their prevalence and output quality.
  We address this gap in the context of a 2023 Swiss privacy law revision. To
enable a systematic evaluation, we create and annotate a multilingual benchmark
dataset that captures key compliance obligations under Swiss and EU privacy
law. Using this dataset, we validate a novel GPT-5-based method for large-scale
compliance assessment of privacy policies, allowing us to measure the impact of
the revision. We observe compliance increases indicating an effect of the
revision. Generators, explicitly referenced by 18% of local websites, are
associated with substantially higher levels of compliance, with increases of up
to 15 percentage points compared to privacy policies without generator use.
These findings contribute to three debates: the potential of LLMs for
cross-lingual legal analysis, the Brussels Effect of EU regulations, and,
crucially, the role of automated tools in improving compliance and contractual
quality.

</details>


### [145] [Revisiting Long-context Modeling from Context Denoising Perspective](https://arxiv.org/abs/2510.05862)
*Zecheng Tang,Baibei Ji,Juntao Li,Lijun Wu,Haijia Gui,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为上下文去噪训练（CDT）的方法，通过集成梯度（IG）分数检测和量化上下文噪声，有效提升长上下文模型对关键信息的关注，显著改善预测性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文模型容易受到无关上下文噪声的影响，导致注意力分散，影响预测准确性。需要一种细粒度方法来识别并减轻此类噪声。

Method: 引入集成梯度（IG）分数作为衡量上下文噪声的指标，并提出上下文去噪训练（CDT），在训练过程中减少噪声干扰，增强模型对关键令牌的注意力。

Result: 在四种任务和不同上下文设置下的实验表明，CDT显著提升了模型性能；使用CDT训练的8B开源模型性能接近GPT-4o（50.92 vs 51.00）。

Conclusion: CDT是一种简单而有效的训练策略，能够显著增强长上下文模型对关键信息的捕捉能力，缓解上下文噪声问题，具有广泛的应用潜力。

Abstract: Long-context models (LCMs) have demonstrated great potential in processing
long sequences, facilitating many real-world applications. The success of LCMs
can be attributed to their ability to locate implicit critical information
within the context for further prediction. However, recent research reveals
that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,
that can mislead model attention. In this paper, we conduct a fine-grained
analysis of the context noise and propose an effective metric, the Integrated
Gradient (IG) score, to detect and quantify the noise information within the
context. Our findings reveal that even simple mitigation of detected context
noise can substantially boost the model's attention on critical tokens and
benefit subsequent predictions. Building on this insight, we propose Context
Denoising Training (CDT), a straightforward yet effective training strategy
that improves attention on critical tokens while reinforcing their influence on
model predictions. Extensive experiments across four tasks, under both context
window scaling and long-context alignment settings, demonstrate the superiority
of CDT. Notably, when trained with CDT, an open-source 8B model can achieve
performance (50.92) comparable to GPT-4o (51.00).

</details>


### [146] [Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input](https://arxiv.org/abs/2510.05864)
*Faeze Ghorbanpour,Alexander Fraser*

Main category: cs.CL

TL;DR: 本研究系统评估了大语言模型在扩展上下文中的有害内容敏感性，发现模型对中等比例的有害内容检测效果最佳，但随着上下文增长或内容过于稀疏/密集时性能下降，且开头和显式内容更易被识别。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在长上下文应用中的普及，亟需了解其在安全关键场景下对有害内容的识别能力，尤其是在不同位置、类型和比例下的表现。

Method: 通过控制有害内容的类型（显式/隐式）、位置（开头、中间、结尾）、占比（0.01-0.50）和上下文长度（600-6000 tokens），在LLaMA-3、Qwen-2.5和Mistral模型上评估其对有毒、冒犯性和仇恨言论等内容的检测性能。

Result: 发现模型性能在有害内容占比0.25时达到峰值，过低或过高均下降；上下文越长，召回率越低；位于开头的有害内容更易被检测；显式内容比隐式内容更容易识别。

Conclusion: 大语言模型在长上下文中对有害内容的处理存在明显局限，需进一步优化以应对安全关键应用场景中的挑战。

Abstract: Large language models (LLMs) increasingly support applications that rely on
extended context, from document processing to retrieval-augmented generation.
While their long-context capabilities are well studied for reasoning and
retrieval, little is known about their behavior in safety-critical scenarios.
We evaluate LLMs' sensitivity to harmful content under extended context,
varying type (explicit vs. implicit), position (beginning, middle, end),
prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).
Across harmful content categories such as toxic, offensive, and hate speech,
with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance
peaks at moderate harmful prevalence (0.25) but declines when content is very
sparse or dominant; recall decreases with increasing context length; harmful
sentences at the beginning are generally detected more reliably; and explicit
content is more consistently recognized than implicit. These findings provide
the first systematic view of how LLMs prioritize and calibrate harmful content
in long contexts, highlighting both their emerging strengths and the challenges
that remain for safety-critical use.

</details>


### [147] [The fragility of "cultural tendencies" in LLMs](https://arxiv.org/abs/2510.05869)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文批评了Lu, Song, 和 Zhang (2025) 关于大语言模型在不同语言提示下表现出文化特有倾向的结论，认为其发现的文化倾向并非稳定特征，而是特定模型和任务设计的脆弱产物。通过更广泛的模型和更多测试项的复制实验，作者发现提示语言对输出影响甚微，挑战了原研究中模型编码了真实文化信念的观点。


<details>
  <summary>Details</summary>
Motivation: 质疑LSZ研究中关于大语言模型因提示语言不同而表现出稳定文化倾向的结论，指出其方法和解释存在问题。

Method: 通过使用更广泛的大型语言模型集合和更大规模的测试项目进行有针对性的复制实验，重新评估LSZ的研究方法、理论框架和结论。

Result: 研究结果显示提示语言对模型输出的影响极小，表明所谓的‘文化倾向’并不稳定，且可能是特定实验设计下的产物。

Conclusion: 大语言模型在不同语言提示下的行为差异不能被简单归因于深层文化模式，提示语言本身不足以引发显著的文化转变，因此不应将这些差异视为模型编码了真实文化信念的证据。

Abstract: In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large
language models (LLMs), when prompted in different languages, display
culturally specific tendencies. They report that the two models (i.e., GPT and
ERNIE) respond in more interdependent and holistic ways when prompted in
Chinese, and more independent and analytic ways when prompted in English. LSZ
attribute these differences to deep-seated cultural patterns in the models,
claiming that prompt language alone can induce substantial cultural shifts.
While we acknowledge the empirical patterns they observed, we find their
experiments, methods, and interpretations problematic. In this paper, we
critically re-evaluate the methodology, theoretical framing, and conclusions of
LSZ. We argue that the reported "cultural tendencies" are not stable traits but
fragile artifacts of specific models and task design. To test this, we
conducted targeted replications using a broader set of LLMs and a larger number
of test items. Our results show that prompt language has minimal effect on
outputs, challenging LSZ's claim that these models encode grounded cultural
beliefs.

</details>


### [148] [Prompt reinforcing for long-term planning of large language models](https://arxiv.org/abs/2510.05921)
*Hsien-Chin Lin,Benjamin Matthias Ruppik,Carel van Niekerk,Chia-Hao Shen,Michael Heck,Nurul Lubis,Renato Vukovic,Shutong Feng,Milica Gašić*

Main category: cs.CL

TL;DR: 提出了一种受强化学习启发的提示优化框架，通过修改基于大语言模型（LLM）代理的任务指令提示来实现长期规划，显著提升了多轮任务（如文本到SQL和面向任务的对话）的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮交互中表现不佳，常依赖错误的早期假设且难以持续跟踪用户目标，因此需要改进其长期规划能力。

Method: 设计了一种基于强化学习思想的提示优化框架，通过逐轮生成反馈并利用经验回放机制重写提示，从而提升LLM在多轮交互中的表现。

Result: 在文本到SQL和任务导向对话等多轮任务中表现出显著性能提升，且该方法可泛化至不同的LLM代理，并能利用多种LLM作为元提示代理。

Conclusion: 强化学习启发的无参数优化方法为提升大语言模型在多轮交互中的表现提供了有效路径，值得进一步研究。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of natural language processing tasks and can be adapted through prompting.
However, they remain suboptimal in multi-turn interactions, often relying on
incorrect early assumptions and failing to track user goals over time, which
makes such tasks particularly challenging. Prior works in dialogue systems have
shown that long-term planning is essential for handling interactive tasks. In
this work, we propose a prompt optimisation framework inspired by reinforcement
learning, which enables such planning to take place by only modifying the task
instruction prompt of the LLM-based agent. By generating turn-by-turn feedback
and leveraging experience replay for prompt rewriting, our proposed method
shows significant improvement in multi-turn tasks such as text-to-SQL and
task-oriented dialogue. Moreover, it generalises across different LLM-based
agents and can leverage diverse LLMs as meta-prompting agents. This warrants
future research in reinforcement learning-inspired parameter-free optimisation
methods.

</details>


### [149] [Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens](https://arxiv.org/abs/2510.05931)
*Mai AlKhamissi,Yunze Xiao,Badr AlKhamissi,Mona Diab*

Main category: cs.CL

TL;DR: 本文提出了一个四部分框架，用于分类文化基准测试如何定义文化，并识别了现有20个文化基准中的六个常见方法论问题，结合人类学方法提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前的文化评估基准往往将文化简化为静态事实或同质价值观，忽略了文化的动态性和实践性，本文旨在弥补这一差距。

Method: 提出一个四部分框架来分类文化在基准测试中的呈现方式，并对20个文化基准进行定性分析，结合人类学方法提出改进措施。

Result: 识别出六个常见的方法论问题，例如将国家等同于文化、忽视文化内部多样性，并提出了引入真实情境、社区参与设计和情境化评估等改进建议。

Conclusion: 文化基准应超越静态的知识回忆任务，通过更贴近实际文化实践的方法，更准确地评估大语言模型在复杂文化情境中的响应能力。

Abstract: Cultural evaluation of large language models has become increasingly
important, yet current benchmarks often reduce culture to static facts or
homogeneous values. This view conflicts with anthropological accounts that
emphasize culture as dynamic, historically situated, and enacted in practice.
To analyze this gap, we introduce a four-part framework that categorizes how
benchmarks frame culture, such as knowledge, preference, performance, or bias.
Using this lens, we qualitatively examine 20 cultural benchmarks and identify
six recurring methodological issues, including treating countries as cultures,
overlooking within-culture diversity, and relying on oversimplified survey
formats. Drawing on established anthropological methods, we propose concrete
improvements: incorporating real-world narratives and scenarios, involving
cultural communities in design and validation, and evaluating models in context
rather than isolation. Our aim is to guide the development of cultural
benchmarks that go beyond static recall tasks and more accurately capture the
responses of the models to complex cultural situations.

</details>


### [150] [EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models](https://arxiv.org/abs/2510.05942)
*Hadi Mohammadi,Anastasia Giachanou,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 提出EvalMORAAL框架，使用两种评分方法和模型评审评估20个大语言模型的道德对齐性，发现模型在西方地区对齐度高，非西方地区存在明显偏差。


<details>
  <summary>Details</summary>
Motivation: 为了更透明、公正地评估大语言模型在全球不同文化背景下的道德对齐性，解决现有评估方法缺乏可比性和文化偏见的问题。

Method: 结合对数概率与直接评分两种打分方式，并引入基于模型的同行评审机制，采用结构化思维链协议与自洽性检查，在世界价值观调查和PEW全球态度调查数据上进行验证。

Result: 顶级模型在世界价值观调查中与人类回答高度相关（r≈0.90），但西方地区（r=0.82）显著优于非西方地区（r=0.61），存在0.21的差距；同行评审识别出348个冲突，且评审一致性与调查对齐度显著相关。

Conclusion: EvalMORAAL实现了跨文化道德对齐的有效评估，显示出向文化感知AI的进步，但也揭示了区域偏差问题，表明在多区域应用中仍存在挑战。

Abstract: We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that
uses two scoring methods (log-probabilities and direct ratings) plus a
model-as-judge peer review to evaluate moral alignment in 20 large language
models. We assess models on the World Values Survey (55 countries, 19 topics)
and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,
top models align closely with survey responses (Pearson's r approximately 0.90
on WVS). Yet we find a clear regional difference: Western regions average
r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),
indicating consistent regional bias. Our framework adds three parts: (1) two
scoring methods for all models to enable fair comparison, (2) a structured
chain-of-thought protocol with self-consistency checks, and (3) a
model-as-judge peer review that flags 348 conflicts using a data-driven
threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,
both p<.001), supporting automated quality checks. These results show real
progress toward culture-aware AI while highlighting open challenges for use
across regions.

</details>


### [151] [Probing the Difficulty Perception Mechanism of Large Language Models](https://arxiv.org/abs/2510.05969)
*Sunbowen Lee,Qingyu Yin,Chak Tou Leong,Jialiang Zhang,Yicheng Gong,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLM）的内部表示中隐含了对问题难度的感知能力，可通过线性探针和特定注意力头识别数学问题的难度，为自动标注难度提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否能在内部表征中隐式编码问题难度，以支持自适应推理和资源分配，并减少对人工标注难度的依赖。

Method: 使用线性探针对LLM最后token的表示进行建模，并定位Transformer最后一层中对简单和困难问题有相反激活模式的特定注意力头，结合消融实验验证定位准确性。

Result: 成功在线性层面建模数学问题难度，识别出与难度感知相关的注意力头，并发现token级别上的熵与难度感知存在显著差异。

Conclusion: 大语言模型不仅具备结构化的难度感知能力，还可作为自动难度标注工具，为基准构建和课程学习提供实用解决方案，并为未来研究提供新的理论视角。

Abstract: Large language models (LLMs) are increasingly deployed on complex reasoning
tasks, yet little is known about their ability to internally evaluate problem
difficulty, which is an essential capability for adaptive reasoning and
efficient resource allocation. In this work, we investigate whether LLMs
implicitly encode problem difficulty in their internal representations. Using a
linear probe on the final-token representations of LLMs, we demonstrate that
the difficulty level of math problems can be linearly modeled. We further
locate the specific attention heads of the final Transformer layer: these
attention heads have opposite activation patterns for simple and difficult
problems, thus achieving perception of difficulty. Our ablation experiments
prove the accuracy of the location. Crucially, our experiments provide
practical support for using LLMs as automatic difficulty annotators,
potentially substantially reducing reliance on costly human labeling in
benchmark construction and curriculum learning. We also uncover that there is a
significant difference in entropy and difficulty perception at the token level.
Our study reveals that difficulty perception in LLMs is not only present but
also structurally organized, offering new theoretical insights and practical
directions for future research.

</details>


### [152] [LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language](https://arxiv.org/abs/2510.05972)
*Periklis Mantenoglou,Rishi Hazra,Pedro Zuidberg Dos Martires,Luc De Raedt*

Main category: cs.CL

TL;DR: 本文提出了LexiCon，一个基于自然语言的约束规划评测基准，通过在现有规划环境中引入时间约束并将其转化为自然语言任务，系统评估大语言模型（LLM）在受限规划中的表现。实验表明，随着约束程度增加，包括GPT-5、o3和R1在内的先进LLM性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在无约束规划任务中表现出较强的推理能力，但在现实应用中，尤其是涉及安全等关键约束的场景下，必须评估其在约束条件下的规划能力。现有评测方法缺乏对约束规划的系统性测试，因此需要构建一个可扩展、原则性强的评测基准。

Method: 提出LexiCon基准，其核心方法是在现有的规划环境中施加时间约束，并将这些约束性问题自动转化为自然语言描述。该基准具有可扩展性，支持通过新增无约束环境生成器自动构建对应的约束任务，从而实现对未来更强模型的持续评估。

Result: 实验结果显示，当前最先进的大语言模型（如GPT-5、o3、R1等）在约束程度较高的规划任务中表现明显下降，表明其处理约束推理的能力仍存在局限。

Conclusion: LexiCon为评估大语言模型在约束规划任务中的能力提供了一个可扩展且面向未来的评测平台，揭示了现有模型在处理复杂约束时的不足，强调了提升LLM在安全敏感场景中可靠性的必要性。

Abstract: Owing to their reasoning capabilities, large language models (LLMs) have been
evaluated on planning tasks described in natural language. However, LLMs have
largely been tested on planning domains without constraints. In order to deploy
them in real-world settings where adherence to constraints, in particular
safety constraints, is critical, we need to evaluate their performance on
constrained planning tasks. We introduce LexiCon -- a natural language-based
(Lexi) constrained (Con) planning benchmark, consisting of a suite of
environments, that can be used to evaluate the planning capabilities of LLMs in
a principled fashion. The core idea behind LexiCon is to take existing planning
environments and impose temporal constraints on the states. These constrained
problems are then translated into natural language and given to an LLM to
solve. A key feature of LexiCon is its extensibility. That is, the set of
supported environments can be extended with new (unconstrained) environment
generators, for which temporal constraints are constructed automatically. This
renders LexiCon future-proof: the hardness of the generated planning problems
can be increased as the planning capabilities of LLMs improve. Our experiments
reveal that the performance of state-of-the-art LLMs, including reasoning
models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of
the planning tasks increases.

</details>


### [153] [Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments](https://arxiv.org/abs/2510.06001)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: 该论文比较了两种评估大语言模型（LLM）句法习得能力的方法，发现基于直接最小对的“wh效应”比差分法（DiD）更具诊断透明性，并显示GPT-2在复杂寄生空位结构中表现出稳健的句法知识。


<details>
  <summary>Details</summary>
Motivation: 由于现有研究使用不同度量方法得出矛盾结论，作者旨在澄清何种评估方式更能准确反映LLM对复杂句法结构的学习能力。

Method: 构建包含8种排列组合的寄生空位刺激范式，采用Wilcox式的wh效应分析对GPT-2进行系统评估，并与DiD方法的结果对比。

Result: GPT-2在全部四个测试条件下均表现成功，显示出对填充词-空位依存关系的良好掌握；而DiD方法此前报告了失败结果。

Conclusion: 评估指标的选择显著影响对LLM句法能力的判断，直接最小对比较法比DiD更具诊断价值。

Abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS)
have applied Large Language Models (LLMs) to test the learnability of complex
syntax through surprisal-based metrics. However, divergent conclusions raise
questions concerning the insights these metrics offer. While Wilcox et al.
(2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate
that models successfully generalise knowledge of filler-gap dependencies, Lan
et al. (2024) used a Difference-in-Differences (DiD) metric and found that
models largely fail on parasitic gaps (PGs). This paper argues that the direct
minimal pair approach offers greater diagnostic transparency. We demonstrate
this by generating a full 8-permutation paradigm of refined PG stimuli and
evaluating the GPT-2 model used in previous studies with a systematic
Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across
all four tested conditions, indicating robust knowledge of filler-gap licensing
principles even in complex PG environments. This finding, which contrasts with
the more ambiguous results from DiD-style metrics, suggests that the choice of
evaluation metric is critical for assessing an LLM's syntactic competence.

</details>


### [154] [MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation](https://arxiv.org/abs/2510.06005)
*Qin Dong,Yuntian Tang,Heming Jia,Yunhang Shen,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Shaohui Lin*

Main category: cs.CL

TL;DR: 提出MASA（Multi-A Shared Adaptation）方法，通过多下投影矩阵和共享结构提升LoRA在大模型微调中的特征表达能力，在MMLU等任务上优于标准LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA使用单一的下投影矩阵A导致表示瓶颈，难以捕捉复杂任务所需的多样化信号，因此需要增强特征适应能力。

Method: 提出MASA架构，采用多A单B结构，多个A矩阵作为专家提取多样化特征，并在不同层间不对称共享以保证参数效率，由一个层特定的B矩阵整合特征。

Result: 在多领域泛化、单领域特化和多任务推理等实验中验证了MASA的有效性；在MMLU基准上达到59.62%的平均准确率，比标准LoRA提高1.08个百分点（相对提升1.84%），可学习参数量约为0.52%。

Conclusion: MASA通过丰富特征适配机制有效缓解了LoRA的表示瓶颈问题，在保持参数效率的同时显著提升了下游任务的适应能力。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a dominant method in
Parameter-Efficient Fine-Tuning (PEFT) for large language models, which
augments the transformer layer with one down-projection $A$ and one
up-projection $B$. However, LoRA's reliance on a single down-projection matrix
($A$) creates a representational bottleneck, as this solitary feature extractor
is inherently insufficient for capturing the diverse signals required by
complex tasks. This motivates our architectural shift to focus on enriching the
feature adaptation to improve the downstream task adaptation ability. We
propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a
multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is
asymmetrically shared across layers to ensure parameter efficiency. In MASA,
these specialized experts capture diverse features, which are then integrated
by a single, layer-specific $B$-matrix. The effectiveness and versatility of
our method are validated through a comprehensive suite of experiments spanning
multi-domain generalization, single-domain specialization, and multi-task
reasoning. For example, on the MMLU benchmark, MASA achieves an average
accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative
improvement of 1.84%) with comparable learnable parameters of 0.52%.

</details>


### [155] [Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance](https://arxiv.org/abs/2510.06018)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: 本研究探讨了刺激材料特性对大语言模型（LLM）句法预测能力评估的影响，提出通过语言学指导的模板生成更优数据集，并发现GPT-2在优化后的刺激材料上表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 近期使用大语言模型检验‘刺激贫乏论’的研究在不同句法现象上结果不一，可能源于所用刺激材料中的词汇歧义和结构复杂性等混淆因素。

Method: 首先在已有刺激材料上建立GPT-2的基线表现，然后利用先进的生成式LLM（Gemini 2.5 Pro Preview）结合语言学模板构建新的、更纯净的数据集进行重新评估。

Result: 初步结果显示，GPT-2在经过优化的新刺激材料上的表现明显优于基线，表明刺激质量显著影响基于意外度的句法能力评估结果。

Conclusion: 刺激材料的质量是影响LLM句法能力评估的重要因素，改进刺激设计有助于更准确地衡量模型的语言能力。

Abstract: Recent studies employing Large Language Models (LLMs) to test the Argument
from the Poverty of the Stimulus (APS) have yielded contrasting results across
syntactic phenomena. This paper investigates the hypothesis that
characteristics of the stimuli used in recent studies, including lexical
ambiguities and structural complexities, may confound model performance. A
methodology is proposed for re-evaluating LLM competence on syntactic
prediction, focusing on GPT-2. This involves: 1) establishing a baseline on
previously used (both filtered and unfiltered) stimuli, and 2) generating a
new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5
Pro Preview) guided by linguistically-informed templates designed to mitigate
identified confounds. Our preliminary findings indicate that GPT-2 demonstrates
notably improved performance on these refined PG stimuli compared to baselines,
suggesting that stimulus quality significantly influences outcomes in
surprisal-based evaluations of LLM syntactic competency.

</details>


### [156] [CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs](https://arxiv.org/abs/2510.06039)
*Chengwei Wu,Jiapu Wang,Mingyang Gao,Xingrui Zhuo,Jipeng Guo,Runlin Lei,Haoran Luo,Tianyu Chen,Haoyi Zhou,Shirui Pan,Zechao Li*

Main category: cs.CL

TL;DR: 提出一个基于中文数据-文本对（CDTP）数据集的全面评测基准CB-ECLLM，用于评估中文大语言模型，涵盖知识图谱补全、三元组到文本生成和问答等任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测基准主要以英语为中心，缺乏针对中文语言特性及结构化数据的支持，难以有效评估中文大模型。

Method: 构建包含700万以上对齐文本对和1500万三元组的CDTP数据集，覆盖四个关键领域，并在此基础上设计多任务评测基准CB-ECLLM，支持知识驱动任务的细粒度评估与多任务微调。

Result: 通过大量实验和消融研究验证了该基准在监督微调、有效性与鲁棒性方面的表现，支持可复现研究并提供开源代码。

Conclusion: CB-ECLLM为中文大语言模型提供了更贴近实际应用的结构化评测环境，推动中文NLP模型在知识密集型任务中的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language processing tasks. However, Chinese LLMs face unique
challenges, primarily due to the dominance of unstructured free text and the
lack of structured representations in Chinese corpora. While existing
benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly
English-centric and fail to address the unique linguistic characteristics of
Chinese, lacking structured datasets essential for robust evaluation. To
address these challenges, we present a Comprehensive Benchmark for Evaluating
Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese
Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million
aligned text pairs, each consisting of unstructured text coupled with one or
more corresponding triples, alongside a total of 15 million triples spanning
four critical domains. The core contributions of CDTP are threefold: (i)
enriching Chinese corpora with high-quality structured information; (ii)
enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)
supporting multi-task fine-tuning to assess generalization and robustness
across scenarios, including Knowledge Graph Completion, Triple-to-Text
generation, and Question Answering. Furthermore, we conduct rigorous
evaluations through extensive experiments and ablation studies to assess the
effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.
To support reproducible research, we offer an open-source codebase and outline
potential directions for future investigations based on our insights.

</details>


### [157] [ASPO: Asymmetric Importance Sampling Policy Optimization](https://arxiv.org/abs/2510.06062)
*Jiakang Wang,Runze Liu,Lei Lin,Wenping Hu,Xiu Li,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: 提出ASPO方法，通过翻转正优势token的IS比率并引入软双重裁剪机制，解决LLM强化学习中token加权不平衡问题，显著提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于重要性采样的强化学习方法在处理正负token时存在权重不匹配问题，导致低概率token更新受抑制、高概率token过度放大，影响训练效果。

Method: 提出Asymmetric Importance Sampling Policy Optimization (ASPO)，翻转正优势token的重要性采样比率，并结合软双重裁剪机制以稳定极端更新同时保持梯度流动。

Result: 在编程和数学推理基准上的实验表明，ASPO有效缓解了早停现象，提高了训练稳定性，并优于强基线方法（如GRPO）。

Conclusion: 纠正重要性采样中的不对称问题是提升LLM在结果监督强化学习中表现的关键，ASPO为此提供了有效解决方案。

Abstract: Recent Large Language Model (LLM) post-training methods rely on token-level
clipping mechanisms during Reinforcement Learning (RL). However, we identify a
fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance
Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to
unbalanced token weighting for positive and negative tokens. This mismatch
suppresses the update of low-probability tokens while over-amplifying already
high-probability ones. To address this, we propose Asymmetric Importance
Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy
that flips the IS ratios of positive-advantage tokens, aligning their update
direction with the learning dynamics of negative ones. AIS further incorporates
a soft dual-clipping mechanism to stabilize extreme updates while maintaining
gradient flow. Comprehensive experiments on coding and mathematical reasoning
benchmarks demonstrate that ASPO significantly mitigates premature convergence,
improves training stability, and enhances final performance over strong
GRPO-based baselines. Our analysis provides new insights into the role of
token-level weighting in OSRL and highlights the critical importance of
correcting IS in LLM RL. The code and models of ASPO are available at
https://github.com/wizard-III/Archer2.0.

</details>


### [158] [Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability](https://arxiv.org/abs/2510.06084)
*Taylor Sorensen,Benjamin Newman,Jared Moore,Chan Park,Jillian Fisher,Niloofar Mireshghallah,Liwei Jiang,Yejin Choi*

Main category: cs.CL

TL;DR: 本文研究了语言模型后训练在指令遵循和下游任务性能提升的同时，对多解任务的负面影响，提出了条件分布建模的三个理想属性：上下文可引导性、有效输出空间覆盖和分布对齐，并指出当前后训练方法会削弱这些属性。为此，作者构建了Spectrum Suite评测集，并提出Spectrum Tuning方法来改善模型的可引导性和分布覆盖能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型后训练虽提升了指令跟随能力，但在需生成多样化合理答案的任务上表现下降，缺乏对模型在条件分布建模方面的能力评估与优化。

Method: 提出三个条件分布建模的理想属性，构建包含90多个任务的大规模评测集Spectrum Suite，用于评估模型的可引导性与分布对齐能力；并提出Spectrum Tuning这一新的后训练方法，利用Spectrum Suite提升模型的灵活性和分布覆盖能力。

Result: 实验表明，当前后训练方法虽能激发模型已有知识（ICL），但损害了上下文可引导性；Spectrum Tuning相比预训练和指令微调模型，在可引导性、输出空间覆盖和分布对齐方面均有提升。

Conclusion: 后训练可能削弱模型在多解任务中的灵活性；通过专门设计的后训练数据（Spectrum Suite）进行Spectrum Tuning，可有效提升模型在条件分布建模上的综合表现。

Abstract: Language model post-training has enhanced instruction-following and
performance on many downstream tasks, but also comes with an often-overlooked
cost on tasks with many possible valid answers. We characterize three
desiderata for conditional distributional modeling: in-context steerability,
valid output space coverage, and distributional alignment, and document across
three model families how current post-training can reduce these properties. In
particular, we disambiguate between two kinds of in-context learning: ICL for
eliciting existing underlying knowledge or capabilities, and in-context
steerability, where a model must use in-context information to override its
priors and steer to a novel data generating distribution. To better evaluate
and improve these desiderata, we introduce Spectrum Suite, a large-scale
resource compiled from >40 data sources and spanning >90 tasks requiring models
to steer to and match diverse distributions ranging from varied human
preferences to numerical distributions and more. We find that while current
post-training techniques help elicit underlying capabilities and knowledge,
they hurt models' ability to flexibly steer in-context. To mitigate these
issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite
to improve steerability and distributional coverage. We find that Spectrum
Tuning often improves over pretrained models and their instruction-tuned
counterparts, enhancing steerability, spanning more of the output space, and
improving distributional alignment on held-out datasets.

</details>


### [159] [The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2510.06101)
*Muyu He,Muhammad Ali Shafique,Anand Kumar,Tsach Mackey,Nazneen Rajani*

Main category: cs.CL

TL;DR: 本文研究了将具备推理能力的大型语言模型（LLM）的思维链蒸馏到小型模型中的数据量对性能的影响，发现存在“代码推理谷”现象：随着蒸馏数据量增加，下游性能先下降后迅速上升。研究还表明，在低至中等数据量下，简单编程题比难题更有利于小模型学习，且训练数据中输出的正确性对蒸馏效果无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索蒸馏数据量对小型语言模型获得竞争性编码能力的影响，填补当前关于蒸馏数据规模与模型性能关系研究的空白。

Method: 在两个小型非推理LLM上研究不同数量蒸馏数据下的性能变化趋势，并在蒸馏的不同阶段使用相同数据进行微调，以分析模型的学习阶段特征。

Result: 发现了“代码推理谷”现象，即性能随数据量先降后升；发现在低至中等数据 regime 下，简单问题更利于小模型学习；意外发现训练数据中输出的正确性不影响蒸馏效果。

Conclusion: 本工作揭示了代码推理知识蒸馏中非直观的训练动态，为小模型高效蒸馏提供了重要实证依据和理解。

Abstract: Distilling the thinking traces of a Large Language Model (LLM) with reasoning
capabilities into a smaller model has been proven effective. Yet, there is a
scarcity of work done on how model performances scale with the quantity of
distillation data. In this work, we study the scaling trend of distilling
competitive coding skills on two small non-reasoning LLMs. We validate the
hypothesis that there is a $\textit{valley of code reasoning}$: downstream
performance on competitive coding first drops as data quantity increases, then
it steadily increases in a sharper-than-log-linear fashion. Having identified
the trend, we further fine-tune the models at two different distillation stages
on the same data to ground conclusions on their respective learning phases. We
learn that across stages in the low and medium-low data regimes, small models
benefit significantly from easier coding questions than from harder ones. We
also find that, surprisingly, the correctness of outputs in training data makes
no difference to distillation outcomes. Our work represents a step forward in
understanding the training dynamics of code reasoning distillation outside
intuition

</details>


### [160] [Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models](https://arxiv.org/abs/2510.06107)
*Gagan Bhatia,Somayajulu G Sripada,Kevin Allan,Jacobo Azcona*

Main category: cs.CL

TL;DR: 本文提出了Distributional Semantics Tracing (DST) 框架，用于追踪大语言模型内部语义失败的根源，识别出导致幻觉的“承诺层”，并揭示了由于快速联想路径与慢速上下文路径之间的冲突（类比双过程理论）而导致幻觉的机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易产生幻觉，即生成看似合理但事实错误的内容。本文旨在从模型架构内部探究这一问题的根本原因，而非仅依赖外部修正方法。

Method: 提出DST框架，结合可解释性技术构建模型推理的因果图；通过分析模型各层的语义一致性，定位幻觉不可避免发生的‘承诺层’；利用双过程理论分析两种计算路径的冲突机制。

Result: 成功识别出幻觉发生的关键层（承诺层）；发现联想路径对上下文路径的劫持有致幻作用；上下文路径的连贯性与幻觉率呈强负相关（ρ = -0.863），表明幻觉是内部语义薄弱的可预测结果。

Conclusion: 幻觉是Transformer架构中语义处理机制失效的可预测结果，源于快速启发式路径与慢速上下文路径的冲突，DST框架为理解、检测和缓解幻觉提供了新的机制性视角。

Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of
plausible yet factually incorrect statements. This work investigates the
intrinsic, architectural origins of this failure mode through three primary
contributions.First, to enable the reliable tracing of internal semantic
failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified
framework that integrates established interpretability techniques to produce a
causal map of a model's reasoning, treating meaning as a function of context
(distributional semantics). Second, we pinpoint the model's layer at which a
hallucination becomes inevitable, identifying a specific \textbf{commitment
layer} where a model's internal representations irreversibly diverge from
factuality. Third, we identify the underlying mechanism for these failures. We
observe a conflict between distinct computational pathways, which we interpret
using the lens of dual-process theory: a fast, heuristic \textbf{associative
pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway}
(akin to System 2), leading to predictable failure modes such as
\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the
coherence of the contextual pathway reveals a strong negative correlation
($\rho = -0.863$) with hallucination rates, implying that these failures are
predictable consequences of internal semantic weakness. The result is a
mechanistic account of how, when, and why hallucinations occur within the
Transformer architecture.

</details>


### [161] [Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer](https://arxiv.org/abs/2510.06128)
*Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: 提出并行分词器（parallel tokenizers），通过双语词典对齐多种语言的词汇表，使语义相同的词具有相同的表示，提升低资源语言下的跨语言迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法常导致语义相同的词在不同语言中被分配不同的嵌入，阻碍了跨语言迁移，尤其在低资源语言中表现更差。

Method: 分别对每种语言独立训练分词器，然后利用双语词典或词对词翻译进行词汇表的全面对齐，确保语义等价的词共享相同的词汇索引。

Result: 在13种低资源语言上从头预训练Transformer模型，结果表明使用并行分词器的模型在情感分析、仇恨言论检测、情感分类和句子相似度任务上均优于传统多语言基线模型。

Conclusion: 重新设计分词方式——特别是通过并行分词器实现词汇表对齐——对提升多语言表示学习，尤其是在低资源场景下，至关重要。

Abstract: Tokenization defines the foundation of multilingual language models by
determining how words are represented and shared across languages. However,
existing methods often fail to support effective cross-lingual transfer because
semantically equivalent words are assigned distinct embeddings. For example, "I
eat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped to
different vocabulary indices, preventing shared representations and limiting
cross-lingual generalization. We introduce parallel tokenizers. This new
framework trains tokenizers monolingually and then aligns their vocabularies
exhaustively using bilingual dictionaries or word-to-word translation, ensuring
consistent indices for semantically equivalent words. This alignment enforces a
shared semantic space across languages while naturally improving fertility
balance. To assess their effectiveness, we pretrain a transformer encoder from
scratch on thirteen low-resource languages and evaluate it on sentiment
analysis, hate speech detection, emotion classification, and sentence embedding
similarity. Across all tasks, models trained with parallel tokenizers
outperform conventional multilingual baselines, confirming that rethinking
tokenization is essential for advancing multilingual representation
learning--especially in low-resource settings.

</details>


### [162] [CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits](https://arxiv.org/abs/2510.06133)
*Kangyu Wang,Zhiyun Jiang,Haibo Feng,Weijia Zhao,Lin Liu,Jianguo Li,Zhenzhong Lan,Weiyao Lin*

Main category: cs.CL

TL;DR: 提出Trace Credit和CreditDecoding，利用历史logits信息加速扩散语言模型的并行解码，显著减少冗余迭代，在多个基准上实现显著加速和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型因初始置信度低导致重复remask和冗余迭代，限制了解码速度，需利用历史信息提升收敛效率。

Method: 引入Trace Credit概念，累积历史logits以量化token的收敛潜力，并设计无需训练的并行解码算法CreditDecoding，融合当前logits与Trace Credit以加速置信度收敛。

Result: 在八个基准上，相比LLaDA-8B-Instruct实现5.48倍加速和0.48性能提升，相比LLaDA-MoE-Instruct实现4.11倍加速和0.15性能提升，且对长序列有效并兼容主流推理优化。

Conclusion: CreditDecoding通过利用解码历史信息有效减少冗余迭代，显著提升扩散语言模型的解码速度与鲁棒性，具有良好的可扩展性和集成性。

Abstract: Diffusion large language models (dLLMs) generate text through iterative
denoising steps, achieving parallel decoding by denoising only high-confidence
positions at each step. However, existing approaches often repetitively remask
tokens due to initially low confidence scores, leading to redundant iterations
and limiting overall acceleration. Through the analysis of dLLM decoding
traces, we observe that the model often determines the final prediction for a
token several steps before the decoding step. To leverage this historical
information and avoid redundant steps, we introduce the concept of Trace
Credit, which quantifies each token's convergence potential by accumulating
historical logits. Furthermore, we propose CreditDecoding, a training-free
parallel decoding algorithm that accelerates the confidence convergence of
correct but underconfident tokens by fusing current logits with Trace Credit.
This process significantly reduces redundant iterations and enhances decoding
robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup
and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times
speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.
Importantly, CreditDecoding scales effectively to long sequences and is
orthogonal to mainstream inference optimizations, making it a readily
integrable and versatile solution.

</details>


### [163] [RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets](https://arxiv.org/abs/2510.06143)
*Jan Cegin,Branislav Pecher,Ivan Srba,Jakub Simko*

Main category: cs.CL

TL;DR: 提出了一种名为RoSE的代理指标，用于在没有人工测试集的情况下选择最适合生成低资源语言训练数据的LLM，该方法在多个语言和任务上优于传统内在启发式方法，并与下游性能具有正相关性。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言缺乏人工标注数据，难以通过传统外在评估选择最佳LLM生成器，而内在指标又与下游性能相关性差，因此需要一种无需人工标注的可靠代理指标。

Method: 提出RoSE（Round robin Synthetic data Evaluation）方法：用候选LLM生成的数据训练一个小模型，并在其他候选LLM生成的合成数据上进行评估，最终得分是该小模型在所有候选模型生成数据上的平均表现。

Result: 在六个LLM、十一种语言和三个任务上，RoSE比其他内在启发式方法更频繁地识别出最优生成器，且其下游性能接近最优生成器基线（仅差0.76个百分点），并是唯一与人工测试集性能呈正相关的指标。

Conclusion: RoSE是一种有效且可靠的无监督代理指标，能够在缺乏人工标注数据的情况下准确选择适合生成训练数据的LLM，特别适用于低资源语言场景。

Abstract: LLMs are powerful generators of synthetic data, which are used for training
smaller, specific models. This is especially valuable for low-resource
languages, where human-labelled data is scarce but LLMs can still produce
high-quality text. However, LLMs differ in how useful their outputs are for
training. Selecting the best LLM as a generator is challenging because
extrinsic evaluation requires costly human annotations (which are often
unavailable for low-resource languages), while intrinsic metrics correlate
poorly with downstream performance. We introduce Round robin Synthetic data
Evaluation (RoSE), a proxy metric for selecting the best LLM generator without
human test sets. RoSE trains a small model on the outputs of a candidate
generator (LLM) and then evaluates it on generated synthetic examples from all
other candidate LLMs. The final RoSE score is the mean performance of this
small model. Across six LLMs, eleven languages, and three tasks (sentiment,
topic, intent), RoSE identifies the optimal generator more often than any other
intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within
0.76 percentage points of the optimal generator baseline. This result is
measured in terms of downstream performance, obtained by training a small model
on the chosen generator's outputs (optimal vs. proxy metric selected) and
evaluating it on human-labelled test data. Additionally, RoSE is the only
metric to achieve a positive correlation with performance on human test data.

</details>


### [164] [VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization](https://arxiv.org/abs/2510.06175)
*Dingyu Yao,Chenxu Yang,Zhengyang Tong,Zheng Lin,Wei Liu,Jian Luan,Weiping Wang*

Main category: cs.CL

TL;DR: VecInfer提出了一种新的向量量化方法，通过平滑和Hadamard变换抑制键缓存中的异常值，实现高效的KV缓存压缩和快速推理，在2-bit量化下性能接近全精度，并显著提升速度与降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法在极低比特宽度下因键缓存异常值导致码本利用不充分，性能严重下降，难以有效压缩KV缓存。

Method: 引入平滑和Hadamard变换来抑制键缓存中的异常值，提升码本对原始数据分布的覆盖能力；设计融合计算与反量化操作的优化CUDA内核以减少内存访问开销。

Result: 在Llama-3.1-8B模型上，使用2-bit量化时性能接近全精度，大批次自注意力计算最高达2.7倍加速，单批次端到端延迟最多降低8.3倍（序列长度196k）。

Conclusion: VecInfer有效解决了低比特量化下的异常值问题，实现了高压缩比与高性能的平衡，支持高效部署。

Abstract: The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.

</details>


### [165] [Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context](https://arxiv.org/abs/2510.06182)
*Yoav Gur-Arieh,Mor Geva,Atticus Geiger*

Main category: cs.CL

TL;DR: 该研究发现语言模型在上下文推理中通过位置、词汇和反射三种机制绑定和检索实体，提出一个结合这三种机制的因果模型，能够以95%的一致性估计下一个词的分布，并在更长、更自然的文本输入中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何在复杂上下文中有效绑定和检索实体，尤其是在位置机制表现不佳的情况下，探索其他补充机制的作用。

Method: 通过对九个模型和十种绑定任务进行大量实验，分析语言模型在不同上下文长度和复杂度下的行为，识别出位置、词汇和反射三种检索机制，并构建一个融合三者的因果模型。

Result: 发现了语言模型在实体绑定中使用三种机制的稳定模式；提出的因果模型在预测下一个词时达到95%的准确率，并在更长、开放性的文本中展现出良好泛化能力。

Conclusion: 语言模型不仅依赖位置机制进行实体检索，在复杂上下文中还会结合词汇和反射机制；本研究提供了一个更完整的语言模型上下文实体绑定与检索机制的理解框架。

Abstract: A key component of in-context reasoning is the ability of language models
(LMs) to bind entities for later retrieval. For example, an LM might represent
"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"
when asked "Who loves pie?" Prior research on short lists of bound entities
found strong evidence that LMs implement such retrieval via a positional
mechanism, where "Ann" is retrieved based on its position in context. In this
work, we find that this mechanism generalizes poorly to more complex settings;
as the number of bound entities in context increases, the positional mechanism
becomes noisy and unreliable in middle positions. To compensate for this, we
find that LMs supplement the positional mechanism with a lexical mechanism
(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism
(retrieving "Ann" through a direct pointer). Through extensive experiments on
nine models and ten binding tasks, we uncover a consistent pattern in how LMs
mix these mechanisms to drive model behavior. We leverage these insights to
develop a causal model combining all three mechanisms that estimates next token
distributions with 95% agreement. Finally, we show that our model generalizes
to substantially longer inputs of open-ended text interleaved with entity
groups, further demonstrating the robustness of our findings in more natural
settings. Overall, our study establishes a more complete picture of how LMs
bind and retrieve entities in-context.

</details>


### [166] [RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback](https://arxiv.org/abs/2510.06186)
*Chunyu Miao,Henry Peng Zou,Yangning Li,Yankai Chen,Yibo Wang,Fangxin Wang,Yifan Li,Wooseong Yang,Bowei He,Xinni Zhang,Dianzhi Yu,Hanchen Yang,Hoang H Nguyen,Yue Zhou,Jie Yang,Jizhou Guo,Wenzhe Fan,Chin-Yuan Yeh,Panpan Meng,Liancheng Fang,Jinhu Qi,Wei-Chieh Huang,Zhengyao Gu,Yuwei Han,Langzhou He,Yuyao Yang,Xue Liu,Irwin King,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出RECODE-H基准和ReCodeAgent框架，通过多轮交互与模拟反馈评估和提升大模型在科研代码生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用单次设置，忽视了科研开发中迭代和反馈驱动的实际工作流程，导致大语言模型在生成正确且可执行代码方面能力有限。

Method: 构建包含102个任务的RECODE-H基准，引入结构化指令、单元测试和五级反馈层次，结合ReCodeAgent框架实现基于反馈的迭代式代码生成，并在多个主流大模型上进行实验验证。

Result: 实验表明，丰富的反馈能显著提升模型性能，但在复杂科研代码生成方面仍存在挑战。

Conclusion: RECODE-H为发展适应性、反馈驱动的大模型代理在科研实施中的应用奠定了基础。

Abstract: Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation

</details>


### [167] [BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects](https://arxiv.org/abs/2510.06188)
*Jakir Hasan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本文提出了BanglaTalk，首个支持孟加拉语方言的实时语音助手系统，采用客户端-服务器架构和RTP协议实现低延迟通信，并通过微调IndicWav2Vec模型构建方言感知的ASR系统BRDialect，在十种地区方言上表现优于基线模型12.41-33.98%。系统在24kbps低带宽下运行，平均端到端延迟为4.9秒，具备成本效益且交互性强，提升了孟加拉语使用者的技术可及性。


<details>
  <summary>Details</summary>
Motivation: 由于孟加拉语属于资源稀缺语言且方言差异大，现有语音助手系统多局限于标准语，无法满足实际需求，缺乏针对方言的实时语音技术支持。

Method: 提出BanglaTalk系统，采用客户端-服务器架构与RTP协议保障低延迟；构建BRDialect ASR系统，基于IndicWav2Vec模型在十种孟加拉地方言数据上进行微调，实现方言感知的语音识别。

Result: BRDialect在RegSpeech12数据集上比基线ASR模型性能提升12.41-33.98%；系统可在24kbps低带宽下运行，平均端到端延迟为4.9秒，具备高效、低成本和强交互性的特点。

Conclusion: BanglaTalk是首个支持孟加拉语方言的实时语音助手系统，有效应对了方言多样性与资源匮乏的挑战，推动了语音技术在多元语言社区中的包容性与可访问性发展。

Abstract: Real-time speech assistants are becoming increasingly popular for ensuring
improved accessibility to information. Bengali, being a low-resource language
with a high regional dialectal diversity, has seen limited progress in
developing such systems. Existing systems are not optimized for real-time use
and focus only on standard Bengali. In this work, we present BanglaTalk, the
first real-time speech assistance system for Bengali regional dialects.
BanglaTalk follows the client-server architecture and uses the Real-time
Transport Protocol (RTP) to ensure low-latency communication. To address
dialectal variation, we introduce a dialect-aware ASR system, BRDialect,
developed by fine-tuning the IndicWav2Vec model in ten Bengali regional
dialects. It outperforms the baseline ASR models by 12.41-33.98% on the
RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of
24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low
bandwidth usage and minimal end-to-end delay make the system both
cost-effective and interactive for real-time use cases, enabling inclusive and
accessible speech technology for the diverse community of Bengali speakers.

</details>


### [168] [Latent Speech-Text Transformer](https://arxiv.org/abs/2510.06195)
*Yen-Ju Lu,Yashesh Gaur,Wei Zhou,Benjamin Muller,Jesus Villalba,Najim Dehak,Luke Zettlemoyer,Gargi Ghosh,Mike Lewis,Srinivasan Iyer,Duc Le*

Main category: cs.CL

TL;DR: 本文提出了一种名为Latent Speech-Text Transformer（LST）的新方法，通过将语音标记动态聚合成潜在语音块，提升了语音-文本模型在预训练中的数据和计算效率，并实现了更好的表示对齐与更快的扩展规律。


<details>
  <summary>Details</summary>
Motivation: 由于语音标记序列远长于文本标记序列，导致语音-文本自回归模型在训练和推理时存在计算不平衡，阻碍了模态间的有效对齐并减缓了扩展规律。因此需要一种更高效的方法来缓解这一问题。

Method: 引入Latent Speech-Text Transformer（LST），通过动态且低成本的方式将语音标记聚合成潜在语音块，这些块作为更高层次的单元，既能与文本单元对齐以促进能力迁移，也能封装常见语音序列（如静音）以提高计算效率。

Result: LST在语音到语音和文本到文本基准任务上均优于基线方法，在计算控制和数据控制设置下，HellaSwag故事补全任务中语音准确率分别提升了6.5%和5.3%，同时文本性能也有所提升。

Conclusion: LST显著提高了语音-文本模型的训练效率和性能，展现出更强的表示对齐能力和更陡峭的扩展规律，为多模态模型的发展提供了有效路径。

Abstract: Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [169] [A Fuzzy Logic-Based Framework for Explainable Machine Learning in Big Data Analytics](https://arxiv.org/abs/2510.05120)
*Farjana Yesmin,Nusrat Shirmin*

Main category: cs.LG

TL;DR: 提出了一种结合类型-2模糊集、粒计算和聚类的新框架，以提升大数据环境下的可解释性和公平性，在空气质量管理数据上表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 传统黑箱模型缺乏透明度，而现有事后解释方法常牺牲准确性，难以满足可解释性与公平性的需求。

Method: 结合类型-2模糊集、粒计算与聚类方法，引入基于轮廓系数和熵的公平性评估，并生成语言规则以实现内在可解释性。

Result: 在UCI空气质量数据上，相比类型-1方法轮廓系数提升约4%（0.365 vs 0.349），熵为0.918，规则覆盖率达0.65，运行时间呈线性且高效（约0.005秒）。

Conclusion: 该框架在可解释性、公平性和效率方面优于DBSCAN和凝聚聚类等基线方法，适用于复杂大数据分析场景。

Abstract: The growing complexity of machine learning (ML) models in big data analytics,
especially in domains such as environmental monitoring, highlights the critical
need for interpretability and explainability to promote trust, ethical
considerations, and regulatory adherence (e.g., GDPR). Traditional "black-box"
models obstruct transparency, whereas post-hoc explainable AI (XAI) techniques
like LIME and SHAP frequently compromise accuracy or fail to deliver inherent
insights. This paper presents a novel framework that combines type-2 fuzzy
sets, granular computing, and clustering to boost explainability and fairness
in big data environments. When applied to the UCI Air Quality dataset, the
framework effectively manages uncertainty in noisy sensor data, produces
linguistic rules, and assesses fairness using silhouette scores and entropy.
Key contributions encompass: (1) A type-2 fuzzy clustering approach that
enhances cohesion by about 4% compared to type-1 methods (silhouette 0.365 vs.
0.349) and improves fairness (entropy 0.918); (2) Incorporation of fairness
measures to mitigate biases in unsupervised scenarios; (3) A rule-based
component for intrinsic XAI, achieving an average coverage of 0.65; (4)
Scalable assessments showing linear runtime (roughly 0.005 seconds for sampled
big data sizes). Experimental outcomes reveal superior performance relative to
baselines such as DBSCAN and Agglomerative Clustering in terms of
interpretability, fairness, and efficiency. Notably, the proposed method
achieves a 4% improvement in silhouette score over type-1 fuzzy clustering and
outperforms baselines in fairness (entropy reduction by up to 1%) and
efficiency.

</details>


### [170] [Auditing Algorithmic Bias in Transformer-Based Trading](https://arxiv.org/abs/2510.05140)
*Armin Gerami,Ramani Duraiswami*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在金融应用中的风险和偏见，提出使用基于部分信息分解（PID）的指标来衡量各资产对模型决策的影响。研究发现模型完全忽略数据波动性，并偏向于低频价格变动的数据。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在金融决策中对波动数据的依赖及其预测置信度受价格变动频率影响的问题。

Method: 采用Transformer模型进行预测，并引入基于Partial Information Decomposition (PID) 的新指标来量化每个资产对模型决策的影响。

Result: 模型完全忽略数据的波动性，且更倾向于低频价格变动的数据。

Conclusion: Transformer模型在金融预测中存在对低频价格变动数据的偏差，且未有效利用波动性信息，可能带来潜在风险。

Abstract: Transformer models have become increasingly popular in financial
applications, yet their potential risk making and biases remain under-explored.
The purpose of this work is to audit the reliance of the model on volatile data
for decision-making, and quantify how the frequency of price movements affects
the model's prediction confidence. We employ a transformer model for
prediction, and introduce a metric based on Partial Information Decomposition
(PID) to measure the influence of each asset on the model's decision making.
Our analysis reveals two key observations: first, the model disregards data
volatility entirely, and second, it is biased toward data with lower-frequency
price movements.

</details>


### [171] [Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment](https://arxiv.org/abs/2510.05157)
*Abrar Shahid,Ibteeker Mahir Ishum,AKM Tahmidul Haque,M Sohel Rahman,A. B. M. Alim Al Islam*

Main category: cs.LG

TL;DR: 本文提出了一种基于OpenAI Gym的对抗性强化学习环境，用于模拟网络多端口服务中的暴力破解攻击与防御，通过DQN训练攻防双方智能体，在零和奖励框架下评估不同策略效果，结果表明防御方在可观测性和陷阱有效性高时具有显著优势，且奖励塑造与训练调度对学习稳定性至关重要。


<details>
  <summary>Details</summary>
Motivation: 为了研究网络攻防对抗中智能体的行为演化，构建一个更贴近现实网络安全场景的强化学习环境，以支持自主防御系统、攻防协同进化及向真实场景迁移学习的研究。

Method: 设计了一个定制化的OpenAI Gym环境，模拟包含背景流量噪声、渐进式入侵机制、IP规避、蜜罐陷阱和多级限速防御等真实安全权衡的攻防过程；使用深度Q网络（DQN）在零和奖励框架下训练攻击者和防御者智能体，并系统评估不同配置下的性能。

Result: 实验结果显示，防御方的可观测性和蜜罐有效性显著阻碍了攻击成功；奖励塑造和训练调度对学习稳定性至关重要；经过50,000轮以上训练，防御方始终占据战略优势，尤其在采用自适应IP封锁和端口级控制等复杂策略时性能更优。

Conclusion: 该环境因其零和博弈结构和真实操作约束，适合研究自主网络安全防御、攻防协同演化及强化学习在实际网络场景中的迁移应用，同时提供了完整实现细节以促进后续研究。

Abstract: This paper presents a controlled study of adversarial reinforcement learning
in network security through a custom OpenAI Gym environment that models
brute-force attacks and reactive defenses on multi-port services. The
environment captures realistic security trade-offs including background traffic
noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot
traps, and multi-level rate-limiting defenses. Competing attacker and defender
agents are trained using Deep Q-Networks (DQN) within a zero-sum reward
framework, where successful exploits yield large terminal rewards while
incremental actions incur small costs. Through systematic evaluation across
multiple configurations (varying trap detection probabilities, exploitation
difficulty thresholds, and training regimens), the results demonstrate that
defender observability and trap effectiveness create substantial barriers to
successful attacks. The experiments reveal that reward shaping and careful
training scheduling are critical for learning stability in this adversarial
setting. The defender consistently maintains strategic advantage across 50,000+
training episodes, with performance gains amplifying when exposed to complex
defensive strategies including adaptive IP blocking and port-specific controls.
Complete implementation details, reproducible hyperparameter configurations,
and architectural guidelines are provided to support future research in
adversarial RL for cybersecurity. The zero-sum formulation and realistic
operational constraints make this environment suitable for studying autonomous
defense systems, attacker-defender co-evolution, and transfer learning to
real-world network security scenarios.

</details>


### [172] [Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders](https://arxiv.org/abs/2510.05160)
*Muhammad Arif Hakimi Zamrai*

Main category: cs.LG

TL;DR: 提出基于条件变分自编码器（CVAE）的生成式逆向设计框架，用于在给定性能目标下生成多样化高性能设计方案，相较于传统单点优化方法能发现更优且多样的解。


<details>
  <summary>Details</summary>
Motivation: 传统代理优化（SBO）方法收敛于单一解，限制了设计空间探索，忽略了可能存在的其他优质拓扑结构，因此需要一种能生成多样化高性能量候选方案的逆向设计新范式。

Method: 构建基于条件变分自编码器（CVAE）的框架，学习设计参数与性能之间的概率映射关系，通过条件生成实现以特定性能目标为导向的多样化设计生成，并在翼型自噪声最小化问题中验证方法有效性。

Result: CVAE框架成功生成256个新设计，有效率达94.1%；其中77.2%的有效设计性能优于SBO基线方法找到的最优解。

Conclusion: 生成式逆向设计不仅能发现更高性能的解，还能提供多样化的候选方案，支持多准则决策，显著增强工程设计过程。

Abstract: Inverse design, which seeks to find optimal parameters for a target output,
is a central challenge in engineering. Surrogate-based optimization (SBO) has
become a standard approach, yet it is fundamentally structured to converge to a
single-point solution, thereby limiting design space exploration and ignoring
potentially valuable alternative topologies. This paper presents a paradigm
shift from single-point optimization to generative inverse design. We introduce
a framework based on a Conditional Variational Autoencoder (CVAE) that learns a
probabilistic mapping between a system's design parameters and its performance,
enabling the generation of a diverse portfolio of high-performing candidates
conditioned on a specific performance objective. We apply this methodology to
the complex, non-linear problem of minimizing airfoil self-noise, using a
high-performing SBO method from a prior benchmark study as a rigorous baseline.
The CVAE framework successfully generated 256 novel designs with a 94.1\%
validity rate. A subsequent surrogate-based evaluation revealed that 77.2\% of
these valid designs achieved superior performance compared to the single
optimal design found by the SBO baseline. This work demonstrates that the
generative approach not only discovers higher-quality solutions but also
provides a rich portfolio of diverse candidates, fundamentally enhancing the
engineering design process by enabling multi-criteria decision-making.

</details>


### [173] [Machine learning for fraud detection in digital banking: a systematic literature review REVIEW](https://arxiv.org/abs/2510.05167)
*Md Zahin Hossain George,Md Khorshed Alam,Md Tarek Hasan*

Main category: cs.LG

TL;DR: 该论文系统综述了机器学习在数字银行欺诈检测中的应用，分析了118项研究，发现监督学习方法仍占主导地位，而深度学习和混合模型展现出更高准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着数字银行业务的快速增长，欺诈行为日益复杂，亟需系统评估机器学习技术在欺诈检测中的有效性与挑战。

Method: 遵循PRISMA指南，对118篇同行评审的研究和机构报告进行系统性文献综述，采用结构化的识别、筛选、资格和纳入流程。

Result: 监督学习方法（如决策树、逻辑回归、支持向量机）因可解释性和稳定性仍占主导；无监督方法用于应对不平衡数据中的新型欺诈；深度学习（如RNN、CNN）能捕捉复杂模式但存在可解释性和实时部署难题；混合模型表现出更高的检测精度和适应性。

Conclusion: 混合型机器学习模型是未来数字银行欺诈检测的发展方向，但需进一步解决模型可解释性与实际部署的挑战。

Abstract: This systematic literature review examines the role of machine learning in
fraud detection within digital banking, synthesizing evidence from 118
peer-reviewed studies and institutional reports. Following the PRISMA
guidelines, the review applied a structured identification, screening,
eligibility, and inclusion process to ensure methodological rigor and
transparency. The findings reveal that supervised learning methods, such as
decision trees, logistic regression, and support vector machines, remain the
dominant paradigm due to their interpretability and established performance,
while unsupervised anomaly detection approaches are increasingly adopted to
address novel fraud patterns in highly imbalanced datasets. Deep learning
architectures, particularly recurrent and convolutional neural networks, have
emerged as transformative tools capable of modeling sequential transaction data
and detecting complex fraud typologies, though challenges of interpretability
and real-time deployment persist. Hybrid models that combine supervised,
unsupervised, and deep learning strategies demonstrate superior adaptability
and detection accuracy, highlighting their potential as convergent solutions.

</details>


### [174] [Discretized Quadratic Integrate-and-Fire Neuron Model for Deep Spiking Neural Networks](https://arxiv.org/abs/2510.05168)
*Eric Jahns,Davi Moreno,Milan Stojkov,Michel A. Kinsy*

Main category: cs.LG

TL;DR: 提出了一种针对深度脉冲神经网络的二次积分与发放（QIF）神经元模型的离散化方法，通过从离散化参数集推导代理梯度窗口保证训练稳定性，并在多个数据集上优于基于LIF的方法。


<details>
  <summary>Details</summary>
Motivation: LIF神经元模型虽然高效但表达能力受限，而QIF等更复杂模型因训练不稳定应用受限，因此需要一种兼具表达力和稳定性的新型QIF离散化方法。

Method: 提出了适用于高性能深度SNN的QIF神经元模型的离散化方案，并推导出基于该离散化参数的代理梯度窗口解析表达式以减少梯度失配，提升训练稳定性。

Result: 在CIFAR-10、CIFAR-100、ImageNet和CIFAR-10 DVS等多个基准数据集上，所提QIF模型性能超过现有的LIF基线方法。

Conclusion: 所提出的QIF神经元离散化方法结合了丰富的非线性动力学特性与实际可扩展性，是深度SNN中LIF神经元的一个有力替代方案。

Abstract: Spiking Neural Networks (SNNs) have emerged as energy-efficient alternatives
to traditional artificial neural networks, leveraging asynchronous and
biologically inspired neuron dynamics. Among existing neuron models, the Leaky
Integrate-and-Fire (LIF) neuron has become widely adopted in deep SNNs due to
its simplicity and computational efficiency. However, this efficiency comes at
the expense of expressiveness, as LIF dynamics are constrained to linear decay
at each timestep. In contrast, more complex models, such as the Quadratic
Integrate-and-Fire (QIF) neuron, exhibit richer, nonlinear dynamics but have
seen limited adoption due to their training instability. On that note, we
propose the first discretization of the QIF neuron model tailored for
high-performance deep spiking neural networks and provide an in-depth analysis
of its dynamics. To ensure training stability, we derive an analytical
formulation for surrogate gradient windows directly from our discretizations'
parameter set, minimizing gradient mismatch. We evaluate our method on
CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS, demonstrating its ability to
outperform state-of-the-art LIF-based methods. These results establish our
discretization of the QIF neuron as a compelling alternative to LIF neurons for
deep SNNs, combining richer dynamics with practical scalability.

</details>


### [175] [Carbon Emission Prediction in China Considering New Quality Productive Forces Using a Deep & Corss Learning Modeling Framework](https://arxiv.org/abs/2510.05171)
*Haijin Xie,Gongquan Zhang*

Main category: cs.LG

TL;DR: 本研究提出了一种多头注意力深度交叉网络（MADCN）框架，用于预测城市碳排放并分析新技术因素的影响，结合SHAP方法实现模型可解释性，实验表明该模型在275个中国城市数据上表现优异，揭示了人口、GDP等因素对碳排放的主要影响，同时证实新质生产力、数字经济和人工智能技术具有显著减排作用。


<details>
  <summary>Details</summary>
Motivation: 随着可持续城市发展需求的增长，亟需利用新兴技术手段精准预测和控制城市碳排放，探索新质生产力、数字经济与人工智能对碳排放的影响机制。

Method: 提出MADCN模型，融合特征交互建模与注意力机制，并引入SHAP进行特征重要性分析，使用275个中国城市的面板数据进行实证研究。

Result: MADCN模型在测试集上取得MSE为406,151.063、MAE为612.304、R²为0.991的优异性能；SHAP分析显示人口、城市规模、城镇化率和GDP是碳排放最主要影响因素，新质生产力、数字经济和AI技术也有积极减排效应。

Conclusion: 推动新质生产力发展、加强数字经济建设与加快AI技术应用有助于显著降低城市碳排放，政策制定应重视技术创新在碳减排战略中的整合，特别是智能化基础设施建设和跨领域数字化升级，以助力实现双碳目标。

Abstract: New quality productive forces (NQPF), digital economy advancement, and
artificial intelligence (AI) technologies are becoming crucial for promoting
sustainable urban development. This study proposes a Multi-head Attention Deep
& Cross Network (MADCN) framework, combining feature interaction modeling and
attention mechanisms, to predict urban carbon emissions and investigate the
impacts of technological factors. The framework incorporates an interpretable
learning phase using SHapley Additive exPlanations (SHAP) to assess the
contributions of different features. A panel dataset covering 275 Chinese
cities is utilized to test the MADCN model. Experimental results demonstrate
that the MADCN model achieves superior predictive performance compared to
traditional machine learning and deep learning baselines, with a Mean Squared
Error (MSE) of 406,151.063, a Mean Absolute Error (MAE) of 612.304, and an
R-squared value of 0.991 on the test set. SHAP analysis highlights that
population, city size, urbanization rate, and GDP are among the most
influential factors on carbon emissions, while NQPF, digital economy index, and
AI technology level also show meaningful but relatively moderate effects.
Advancing NQPF, strengthening the digital economy, and accelerating AI
technology development can significantly contribute to reducing urban carbon
emissions. Policymakers should prioritize integrating technological innovation
into carbon reduction strategies, particularly by promoting intelligent
infrastructure and enhancing digitalization across sectors, to effectively
achieve dual-carbon goals.

</details>


### [176] [Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data](https://arxiv.org/abs/2510.05172)
*Anushiya Arunan,Yan Qin,Xiaoli Li,U-Xuan Tan,H. Vincent Poor,Chau Yuen*

Main category: cs.LG

TL;DR: 提出一种基于自监督预训练的电池容量估计模型，利用隐私友好的真实电动汽车充电数据片段，通过片段相似性加权掩码重建框架，有效学习泛化表征，在跨制造商和电池老化导致的数据分布偏移下仍显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有电池容量估计模型受限于隐私法规和标注数据不足，难以应对真实场景中的数据分布变化；同时，现有自监督方法无法有效利用低特征密度、高噪声的隐私友好型现场数据。

Method: 提出片段相似性加权掩码输入重建的自监督预训练框架，结合片段级对比学习捕捉碎片化数据间的高层相似性，并通过相似性加权的掩码重建学习细粒度充电模式与跨片段关联关系。

Result: 在真实电动汽车充电数据上验证，相比最优基线模型测试误差降低31.9%，在制造商和电池老化引起的域偏移场景下仍保持鲁棒性。

Conclusion: 该方法首次实现了基于隐私友好型碎片化数据的高效电池容量估计，通过对比学习与加权重建策略，显著提升了模型在现实复杂环境下的泛化能力与实用性。

Abstract: Accurate battery capacity estimation is key to alleviating consumer concerns
about battery performance and reliability of electric vehicles (EVs). However,
practical data limitations imposed by stringent privacy regulations and labeled
data shortages hamper the development of generalizable capacity estimation
models that remain robust to real-world data distribution shifts. While
self-supervised learning can leverage unlabeled data, existing techniques are
not particularly designed to learn effectively from challenging field data --
let alone from privacy-friendly data, which are often less feature-rich and
noisier. In this work, we propose a first-of-its-kind capacity estimation model
based on self-supervised pre-training, developed on a large-scale dataset of
privacy-friendly charging data snippets from real-world EV operations. Our
pre-training framework, snippet similarity-weighted masked input
reconstruction, is designed to learn rich, generalizable representations even
from less feature-rich and fragmented privacy-friendly data. Our key innovation
lies in harnessing contrastive learning to first capture high-level
similarities among fragmented snippets that otherwise lack meaningful context.
With our snippet-wise contrastive learning and subsequent similarity-weighted
masked reconstruction, we are able to learn rich representations of both
granular charging patterns within individual snippets and high-level
associative relationships across different snippets. Bolstered by this rich
representation learning, our model consistently outperforms state-of-the-art
baselines, achieving 31.9% lower test error than the best-performing benchmark,
even under challenging domain-shifted settings affected by both manufacturer
and age-induced distribution shifts.

</details>


### [177] [Improved High-probability Convergence Guarantees of Decentralized SGD](https://arxiv.org/abs/2510.06141)
*Aleksandar Armacki,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文研究了去中心化随机梯度下降（DSGD）在轻尾噪声下的高概率（HP）收敛性，证明了在与均方误差（MSE）收敛相同条件下即可实现HP收敛，去除了以往工作中对梯度有界等强假设，并达到了最优收敛速率和用户数的线性加速。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化设置下的高概率收敛分析依赖强假设（如梯度有界或噪声渐近消失），导致其条件比MSE收敛更严格，而集中式场景中SGD在两者下所需条件一致，因此亟需缩小这一理论差距。

Method: 通过仔细分析目标量（如梯度范数平方或最优性差距）的矩生成函数（MGF）以及用户模型间一致性差距的MGF，提出新的去中心化方法在高概率意义下的方差缩减分析，并针对强凸情况给出更精细的MGF界限。

Result: 证明了DSGD在非凸和强凸情形下均能在与MSE收敛相同条件下实现高概率收敛，达到最优收敛速率，并获得用户数量的线性加速效果。

Conclusion: DSGD在轻尾噪声下无需强假设即可实现高概率收敛，其理论保证与MSE收敛一致，且具备良好的可扩展性，填补了去中心化学习中高概率收敛理论的空白。

Abstract: Convergence in high-probability (HP) has been receiving increasing interest,
due to its attractive properties, such as exponentially decaying tail bounds
and strong guarantees for each individual run of an algorithm. While HP
guarantees are extensively studied in centralized settings, much less is
understood in the decentralized, networked setup. Existing HP studies in
decentralized settings impose strong assumptions, like uniformly bounded
gradients, or asymptotically vanishing noise, resulting in a significant gap
between assumptions used to establish convergence in the HP and the
mean-squared error (MSE) sense, even for vanilla Decentralized Stochastic
Gradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralized
settings, where it is known that $\mathtt{SGD}$ converges in HP under the same
conditions on the cost function as needed to guarantee MSE convergence.
Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ in
the presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges in
HP under the same conditions on the cost as in the MSE sense, removing
uniformly bounded gradients and other restrictive assumptions, while
simultaneously achieving order-optimal rates for both non-convex and strongly
convex costs. Moreover, our improved analysis yields linear speed-up in the
number of users, demonstrating that $\mathtt{DSGD}$ maintains strong
performance in the HP sense and matches existing MSE guarantees. Our improved
results stem from a careful analysis of the MGF of quantities of interest
(norm-squared of gradient or optimality gap) and the MGF of the consensus gap
between users' models. To achieve linear speed-up, we provide a novel result on
the variance-reduction effect of decentralized methods in the HP sense and more
fine-grained bounds on the MGF for strongly convex costs, which are both of
independent interest.

</details>


### [178] [Exact Causal Attention with 10% Fewer Operations](https://arxiv.org/abs/2510.05175)
*Dmitry Rybin,Yushun Zhang,Ding Tian,Zhihang Lin,Ruoyu Sun,Zhi-Quan Luo*

Main category: cs.LG

TL;DR: 提出Fast Causal Attention (FCA)算法，通过减少10%的操作数来加速因果注意力机制的精确计算。


<details>
  <summary>Details</summary>
Motivation: 为了提升因果注意力机制在前向和反向传播中的计算效率，尤其是在GPU上的矩阵乘法操作。

Method: 利用机器学习和组合搜索发现的代数恒等式，优化涉及上三角或下三角矩阵的特殊矩阵乘法。

Result: 在PyTorch默认实现和Triton编译内核之上实现了显著加速，特别是在Mask(QK^T)等操作中。

Conclusion: FCA能有效减少因果注意力中的计算开销，为Transformer类模型提供更高效的推理和训练支持。

Abstract: We present Fast Causal Attention (FCA), an algorithm that computes exact
Causal Attention using 10\% fewer operations. FCA accelerates a special class
of matrix multiplications where either one operand or the output matrix is
upper- or lower-triangular. This includes all operations in forward and
backward pass of Causal Attention, such as masked product
$\mathrm{Mask}(QK^{T})$. For these matrix multiplications on GPU, FCA reaches
noticeable accelerations over the default PyTorch implementations and Triton
compiled kernels. FCA is built upon algebraic identities discovered via machine
learning and combinatorial search.

</details>


### [179] [PatternKV: Flattening KV Representation Expands Quantization Headroom](https://arxiv.org/abs/2510.05176)
*Ji Zhang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Jiayi Shi,Yueqi Zhang,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: 本文提出PatternKV，一种模式对齐的残差量化方法，通过在线挖掘典型模式并仅量化KV向量与其最近模式的残差，有效 flattens KV分布，显著提升低比特量化的精度与效率。


<details>
  <summary>Details</summary>
Motivation: KV缓存已成为自回归大模型推理中的主要内存和带宽瓶颈，现有KV量化方法因KV分布不平坦导致量化范围宽、精度下降严重，尤其在低比特下表现脆弱。

Method: 基于K缓存结构稳定、V缓存蕴含语义规律的观察，提出PatternKV：在线挖掘代表性模式向量，将每个KV向量对齐到最近的模式，并仅量化其残差，从而重塑KV分布使其更平坦、量化范围更窄。

Result: 在多种模型和长上下文、测试时扩展场景下，PatternKV实现一致的2比特增益，4比特量化相比FP16平均仅下降0.08%，测试时扩展准确率平均提升10%，吞吐量提高1.4倍，并支持1.25倍更大的批量。

Conclusion: PatternKV通过模式对齐残差量化有效解决了低比特KV量化中的分布不平问题，在大幅降低KV缓存开销的同时保持高精度，显著提升了大模型推理效率。

Abstract: KV cache in autoregressive LLMs eliminates redundant recomputation but has
emerged as the dominant memory and bandwidth bottleneck during inference,
notably with long contexts and test-time scaling. KV quantization is a key
lever for reducing cache cost, but accuracy drops sharply as the native KV
distribution lacks flatness and thus maintains a wide quantization range. Prior
work focuses on isolating outliers, which caps their error but fails to flatten
the overall distribution, leaving performance fragile under low-bit settings.
In this work, we show that the K cache maintains a stable structure that
evolves gradually with context, while the V cache carries latent semantic
regularities. Building on these insights, we propose PatternKV, a
pattern-aligned residual quantization scheme. It mines representative pattern
vectors online, aligns each KV vector to its nearest pattern, and quantizes
only the residual. This reshaping of the KV distribution flattens the
quantization target and narrows its range, thereby improving the fidelity of
low-bit KV quantization. Across long-context and test-time scaling settings on
multiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%
average 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%
on average, and raises throughput by 1.4x while supporting 1.25x larger
batches.

</details>


### [180] [Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression](https://arxiv.org/abs/2510.05178)
*Ou Deng,Ruichen Cong,Jianting Xu,Shoji Nishimura,Atsushi Ogihara,Qun Jin*

Main category: cs.LG

TL;DR: 提出了一种基于可学习逻辑门的符号回归方法（LGO），能够在保持单位感知的前提下自动发现临床相关的阈值，提升模型可解释性与实际部署能力。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归难以编码具有物理单位的阈值和条件逻辑，限制了其在医疗等对可解释性要求高的领域的应用。

Method: 引入可微的逻辑门（LGO）作为类型化原语，通过学习位置和陡度参数实现软/硬门控，并映射回物理单位以支持审计；结合门控机制实现条件分支与阈值发现。

Result: 在ICU和NHANES两个健康数据集上，硬门控变体71%的阈值落在指南锚点10%范围内，100%在20%内，且使用的门数量显著少于软门控；在平滑任务中门被有效剪枝，保持简洁性。

Conclusion: LGO使符号回归能生成带明确物理单位阈值的紧凑方程，将可解释性从后验解释变为建模约束，为制度切换和合规部署提供了实用工具。

Abstract: Symbolic regression promises readable equations but struggles to encode
unit-aware thresholds and conditional logic. We propose logistic-gated
operators (LGO) -- differentiable gates with learnable location and steepness
-- embedded as typed primitives and mapped back to physical units for audit.
Across two primary health datasets (ICU, NHANES), the hard-gate variant
recovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fall
within 10% of guideline anchors and 100% within 20%, while using far fewer
gates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), and
remaining within the competitive accuracy envelope of strong SR baselines. On
predominantly smooth tasks, gates are pruned, preserving parsimony. The result
is compact symbolic equations with explicit, unit-aware thresholds that can be
audited against clinical anchors -- turning interpretability from a post-hoc
explanation into a modeling constraint and equipping symbolic regression with a
practical calculus for regime switching and governance-ready deployment.

</details>


### [181] [OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT](https://arxiv.org/abs/2510.05180)
*Saida Elouardi,Mohammed Jouhari,Anas Motii*

Main category: cs.LG

TL;DR: 本文提出了一种名为OptiFLIDS的新型联邦学习方法，用于物联网环境中的入侵检测系统（IDS），通过剪枝技术和定制化聚合策略，在保证检测性能的同时降低能耗和应对非独立同分布数据挑战。


<details>
  <summary>Details</summary>
Motivation: 在关键物联网环境中，传统基于机器学习的IDS需要大量数据且面临隐私问题，而联邦学习虽能保护数据隐私，但仍存在数据异构性和资源消耗高等问题，因此需要一种高效节能且适应非IID数据的联邦IDS方案。

Method: 提出OptiFLIDS，结合本地训练过程中的模型剪枝以减少复杂度和能耗，并设计定制化的模型聚合方法以应对因非IID数据导致的剪枝模型差异。

Result: 在TON_IoT、X-IIoTID和IDSIoT2024三个最新的物联网IDS数据集上的实验表明，OptiFLIDS在保持良好检测性能的同时显著提升了能源效率。

Conclusion: OptiFLIDS有效平衡了检测性能与资源消耗，特别适用于资源受限的真实物联网环境中的部署。

Abstract: In critical IoT environments, such as smart homes and industrial systems,
effective Intrusion Detection Systems (IDS) are essential for ensuring
security. However, developing robust IDS solutions remains a significant
challenge. Traditional machine learning-based IDS models typically require
large datasets, but data sharing is often limited due to privacy and security
concerns. Federated Learning (FL) presents a promising alternative by enabling
collaborative model training without sharing raw data. Despite its advantages,
FL still faces key challenges, such as data heterogeneity (non-IID data) and
high energy and computation costs, particularly for resource constrained IoT
devices. To address these issues, this paper proposes OptiFLIDS, a novel
approach that applies pruning techniques during local training to reduce model
complexity and energy consumption. It also incorporates a customized
aggregation method to better handle pruned models that differ due to non-IID
data distributions. Experiments conducted on three recent IoT IDS datasets,
TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong
detection performance while improving energy efficiency, making it well-suited
for deployment in real-world IoT environments.

</details>


### [182] [A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors](https://arxiv.org/abs/2510.05205)
*Sebastian Wagner-Carena,Aizhan Akhmetzhanova,Sydney Erickson*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的方法，用于解决自然科学研究中的源分离问题，无需对源进行显式假设，仅依赖多视角观测数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖简化的源模型，难以准确还原复杂数据，而实际观测常存在噪声、不完整性和分辨率变化，因此需要一种更鲁棒的源分离方法。

Method: 利用扩散模型学习未知源的复杂先验分布，通过多视角（即不同观测集合包含源的不同线性变换）实现源分离，并支持从源先验和联合后验中采样与概率评估。

Result: 该方法在合成数据和真实的星系观测数据上均表现出色，能够在无单独源观测、噪声强、数据不完整且分辨率可变的情况下成功分离源。

Conclusion: 扩散模型能有效解决复杂环境下的源分离问题，为天文学、神经科学和地震学等领域的数据分析提供了新工具。

Abstract: A common challenge in the natural sciences is to disentangle distinct,
unknown sources from observations. Examples of this source separation task
include deblending galaxies in a crowded field, distinguishing the activity of
individual neurons from overlapping signals, and separating seismic events from
an ambient background. Traditional analyses often rely on simplified source
models that fail to accurately reproduce the data. Recent advances have shown
that diffusion models can directly learn complex prior distributions from
noisy, incomplete data. In this work, we show that diffusion models can solve
the source separation problem without explicit assumptions about the source.
Our method relies only on multiple views, or the property that different sets
of observations contain different linear transformations of the unknown
sources. We show that our method succeeds even when no source is individually
observed and the observations are noisy, incomplete, and vary in resolution.
The learned diffusion models enable us to sample from the source priors,
evaluate the probability of candidate sources, and draw from the joint
posterior of the source distribution given an observation. We demonstrate the
effectiveness of our method on a range of synthetic problems as well as
real-world galaxy observations.

</details>


### [183] [Approximate Gaussianity Beyond Initialisation in Neural Networks](https://arxiv.org/abs/2510.05218)
*Edward Hirst,Sanjaye Ramgoolam*

Main category: cs.LG

TL;DR: 研究了MNIST分类问题中神经网络权重矩阵集合在训练过程中的分布，发现具有13个参数的置换不变高斯矩阵模型能有效描述权重矩阵的相关高斯性，优于传统的独立同分布高斯模型，并在初始化之后仍保持有效性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练过程中权重矩阵的统计特性，检验高斯性和置换对称性假设下矩阵模型对权重分布的建模能力。

Method: 采用13参数的置换不变高斯矩阵模型，结合表示论和图论方法分析权重矩阵的可观测量，并使用Wasserstein距离量化训练过程中分布的变化。

Result: 置换不变高斯模型能更准确地刻画权重矩阵中的相关性，且在不同初始化、正则化、网络深度和宽度条件下均表现良好；同时揭示了偏离高斯性的条件及其可解释建模路径。

Conclusion: 置换对称性和相关结构在神经网络训练动态中起重要作用，13参数高斯矩阵模型为权重分布提供了高度可解释且有效的建模框架。

Abstract: Ensembles of neural network weight matrices are studied through the training
process for the MNIST classification problem, testing the efficacy of matrix
models for representing their distributions, under assumptions of Gaussianity
and permutation-symmetry. The general 13-parameter permutation invariant
Gaussian matrix models are found to be effective models for the correlated
Gaussianity in the weight matrices, beyond the range of applicability of the
simple Gaussian with independent identically distributed matrix variables, and
notably well beyond the initialisation step. The representation theoretic model
parameters, and the graph-theoretic characterisation of the permutation
invariant matrix observables give an interpretable framework for the best-fit
model and for small departures from Gaussianity. Additionally, the Wasserstein
distance is calculated for this class of models and used to quantify the
movement of the distributions over training. Throughout the work, the effects
of varied initialisation regimes, regularisation, layer depth, and layer width
are tested for this formalism, identifying limits where particular departures
from Gaussianity are enhanced and how more general, yet still
highly-interpretable, models can be developed.

</details>


### [184] [CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers](https://arxiv.org/abs/2510.05228)
*Haining Pan,James V. Roggeveen,Erez Berg,Juan Carrasquilla,Debanjan Chowdhury,Surya Ganguli,Federico Ghimenti,Juraj Hasik,Henry Hunt,Hong-Chen Jiang,Mason Kamb,Ying-Jer Kao,Ehsan Khatami,Michael J. Lawler,Di Luo,Titus Neupert,Xiaoliang Qi,Michael P. Brenner,Eun-Ah Kim*

Main category: cs.LG

TL;DR: CMT-Benchmark 是一个包含50个凝聚态理论难题的数据集，用于评估大语言模型在高级科学研究任务中的表现，结果显示当前模型在物理推理方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 填补大语言模型在硬科学领域尤其是凝聚态理论研究级别问题上的评估空白。

Method: 由全球专家小组设计并验证问题，涵盖量子多体和经典统计力学等主题，并通过机器自动评分系统（包括符号化处理非对易算符）评估17个前沿大模型的解题能力。

Result: 最先进的模型GPT-5仅解决30%的问题，17个模型平均得分为11.4±2.1%；18个问题无一模型解决，26个问题最多只有一个模型解决，且答案常违反基本物理规律。

Conclusion: 当前大语言模型在复杂物理推理任务上表现不佳，CMT-Benchmark 可作为推动AI科研助手和教学工具发展的基准。

Abstract: Large language models (LLMs) have shown remarkable progress in coding and
math problem-solving, but evaluation on advanced research-level problems in
hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a
dataset of 50 problems covering condensed matter theory (CMT) at the level of
an expert researcher. Topics span analytical and computational approaches in
quantum many-body, and classical statistical mechanics. The dataset was
designed and verified by a panel of expert researchers from around the world.
We built the dataset through a collaborative environment that challenges the
panel to write and refine problems they would want a research assistant to
solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte
Carlo, density matrix renormalization group (DMRG), quantum/classical
statistical mechanics, and model building. We evaluate LLMs by programmatically
checking solutions against expert-supplied ground truth. We developed
machine-grading, including symbolic handling of non-commuting operators via
normal ordering. They generalize across tasks too. Our evaluations show that
frontier models struggle with all of the problems in the dataset, highlighting
a gap in the physical reasoning skills of current LLMs. Notably, experts
identified strategies for creating increasingly difficult problems by
interacting with the LLMs and exploiting common failure modes. The best model,
GPT5, solves 30\% of the problems; average across 17 models (GPT, Gemini,
Claude, DeepSeek, Llama) is 11.4$\pm$2.1\%. Moreover, 18 problems are solved by
none of the 17 models, and 26 by at most one. These unsolved problems span
Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes
violate fundamental symmetries or have unphysical scaling dimensions. We
believe this benchmark will guide development toward capable AI research
assistants and tutors.

</details>


### [185] [Simultaneous Learning and Optimization via Misspecified Saddle Point Problems](https://arxiv.org/abs/2510.05241)
*Mohammad Mahdi Ahmadi,Erfan Yazdandoost Hamedani*

Main category: cs.LG

TL;DR: 本文研究了一类参数未知的误设鞍点问题，提出将优化与学习统一建模，并基于加速原对偶方法设计了两种新算法，其中学习感知型算法通过调整动量更新获得更优收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统鞍点问题假设参数已知或预先估计，但实际中参数常需从数据中同时学习，因此需要一种联合优化与学习的框架来处理此类误设问题。

Method: 基于Hamedani & Aybat (2021)的加速原对偶（APD）方法，提出了两种算法：一是直接代入参数估计的朴素扩展，二是考虑参数动态的学习感知型变体，后者通过调整动量和采用回溯策略实现自适应步长。

Result: 两种方法均达到O(log K / K)的收敛速率，学习感知型算法具有更优的O(1)常数项；在多解学习问题中，改进算法达到O(1/√K)速率；实验显示在误设投资组合优化中优于现有方法。

Conclusion: 本文提出的统一框架和学习感知型APD算法有效解决了参数需在线学习的误设鞍点问题，在理论收敛性和实际性能上均表现优越。

Abstract: We study a class of misspecified saddle point (SP) problems, where the
optimization objective depends on an unknown parameter that must be learned
concurrently from data. Unlike existing studies that assume parameters are
fully known or pre-estimated, our framework integrates optimization and
learning into a unified formulation, enabling a more flexible problem class. To
address this setting, we propose two algorithms based on the accelerated
primal-dual (APD) by Hamedani & Aybat 2021. In particular, we first analyze the
naive extension of the APD method by directly substituting the evolving
parameter estimates into the primal-dual updates; then, we design a new
learning-aware variant of the APD method that explicitly accounts for parameter
dynamics by adjusting the momentum updates. Both methods achieve a provable
convergence rate of $\mathcal{O}(\log K / K)$, while the learning-aware
approach attains a tighter $\mathcal{O}(1)$ constant and further benefits from
an adaptive step-size selection enabled by a backtracking strategy.
Furthermore, we extend the framework to problems where the learning problem
admits multiple optimal solutions, showing that our modified algorithm for a
structured setting achieves an $\mathcal{O}(1/\sqrt{K})$ rate. To demonstrate
practical impact, we evaluate our methods on a misspecified portfolio
optimization problem and show superior empirical performance compared to
state-of-the-art algorithms.

</details>


### [186] [ECLipsE-Gen-Local: Efficient Compositional Local Lipschitz Estimates for Deep Neural Networks](https://arxiv.org/abs/2510.05261)
*Yuezhu Xu,S. Sivaranjani*

Main category: cs.LG

TL;DR: 提出了一种可扩展的组合框架，用于深度前馈神经网络的Lipschitz常数估计，通过分解广义SDP问题实现高效计算，并结合局部输入信息获得更紧致的界。


<details>
  <summary>Details</summary>
Motivation: 由于计算神经网络Lipschitz常数是NP难问题，且传统方法依赖大规模半定规划（SDP）导致难以扩展，同时缺乏对局部输入区域信息的有效利用，因此需要一种更高效、可扩展且能提供 tighter 估计的方法。

Method: 提出一个广义SDP框架，支持异构激活函数斜率，并可针对任意输入-输出对和子网络进行Lipschitz估计；将该SDP分解为一系列随网络深度线性增长的小规模子问题，并设计具有闭式解的快速变体；进一步开发ECLipsE-Gen-Local算法以融入局部输入信息。

Result: 实验表明所提方法在多个基准上显著快于现有方法，且比全局方法提供更紧的Lipschitz界；在小输入区域内，估计值接近自动微分得到的精确Jacobian上界；估计结果与网络鲁棒性高度一致。

Conclusion: 该组合框架实现了Lipschitz常数估计的高效性、可扩展性和 tighter 界，理论保证了可行性与有效性，验证了其在神经网络鲁棒性认证中的实用价值。

Abstract: The Lipschitz constant is a key measure for certifying the robustness of
neural networks to input perturbations. However, computing the exact constant
is NP-hard, and standard approaches to estimate the Lipschitz constant involve
solving a large matrix semidefinite program (SDP) that scales poorly with
network size. Further, there is a potential to efficiently leverage local
information on the input region to provide tighter Lipschitz estimates. We
address this problem here by proposing a compositional framework that yields
tight yet scalable Lipschitz estimates for deep feedforward neural networks.
Specifically, we begin by developing a generalized SDP framework that is highly
flexible, accommodating heterogeneous activation function slope, and allowing
Lipschitz estimates with respect to arbitrary input-output pairs and arbitrary
choices of sub-networks of consecutive layers. We then decompose this
generalized SDP into a sequence of small sub-problems, with computational
complexity that scales linearly with respect to the network depth. We also
develop a variant that achieves near-instantaneous computation through
closed-form solutions to each sub-problem. All our algorithms are accompanied
by theoretical guarantees on feasibility and validity. Next, we develop a
series of algorithms, termed as ECLipsE-Gen-Local, that effectively incorporate
local information on the input. Our experiments demonstrate that our algorithms
achieve substantial speedups over a multitude of benchmarks while producing
significantly tighter Lipschitz bounds than global approaches. Moreover, we
show that our algorithms provide strict upper bounds for the Lipschitz constant
with values approaching the exact Jacobian from autodiff when the input region
is small enough. Finally, we demonstrate the practical utility of our approach
by showing that our Lipschitz estimates closely align with network robustness.

</details>


### [187] [Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs](https://arxiv.org/abs/2510.05278)
*Paloma García-de-Herreros,Philipp Slusallek,Dietrich Klakow,Vagrant Gautam*

Main category: cs.LG

TL;DR: 本文研究了在基于偏微分方程的时间依赖性模拟任务中，编码器-仅模型与解码器-仅模型在跨模态适应中的表现差异，发现现有方法下解码器-仅模型性能较差且难以通过扩展规模改善。为此，作者提出了两种新方法（Parallel Flipping 和 Sequence Doubling）以在自回归模型中模拟双向性，显著提升了解码器-仅模型的性能，缩小了与编码器-仅模型的差距。


<details>
  <summary>Details</summary>
Motivation: 探讨不同模型架构（尤其是解码器-仅模型）在跨模态科学机器学习任务中的适用性，并解决其在时间序列PDE模拟任务中表现不佳的问题。

Method: 通过系统性的消融实验比较编码器-仅和解码器-仅模型的表现，并提出Parallel Flipping和Sequence Doubling两种新方法来增强解码器-仅模型的双向信息流动能力。

Result: 解码器-仅模型在原有方法下显著弱于编码器-仅模型，且扩展规模无效；所提新方法显著提升了解码器-仅模型性能，接近编码器-仅模型水平。

Conclusion: 通过引入模拟双向性的机制，解码器-仅模型也可有效用于跨模态科学机器学习任务，拓展了该领域可用模型的范围。

Abstract: Large language models have shown great success on natural language tasks in
recent years, but they have also shown great promise when adapted to new
modalities, e.g., for scientific machine learning tasks. Even though
decoder-only models are more popular within NLP and scale exceedingly well at
generating natural language, most proposed approaches for cross-modal
adaptation focus on encoder-only models, raising the question of how model
architecture affects these approaches. In this paper, we therefore perform a
series of ablation studies to answer this question, systematically comparing
encoder-only and decoder-only models on cross-modal adaptation for
time-dependent simulation tasks based on partial differential equations (PDEs).
We find that decoder-only models are far worse than encoder-only models, when
existing approaches are applied unmodified. In contrast to several other
domains, scaling decoder-only models also does not help. To harness the
potential of decoder-only models in this context, we introduce two novel
approaches, Parallel Flipping and Sequence Doubling, attempting to mimic
bidirectionality in autoregressive models. Both our methods improve overall
performance using decoder-only models for all tasks and all cross-model
adaptation methods, closing the gap to encoder-only model performance. We hope
that our findings broaden the spectrum of models used on cross-modal adaptation
tasks to further scientific ML.

</details>


### [188] [Adjusting the Output of Decision Transformer with Action Gradient](https://arxiv.org/abs/2510.05285)
*Rui Lin,Yiwen Zhang,Zhicheng Peng,Minghao Lyu*

Main category: cs.LG

TL;DR: 提出Action Gradient (AG) 方法，通过Q值对动作的梯度优化动作，有效提升Decision Transformer的性能。


<details>
  <summary>Details</summary>
Motivation: Decision Transformer在离线强化学习中通过最大化动作似然性带来新范式，但存在轨迹拼接和动作外推问题，现有方法结合时不稳定。

Method: 提出Action Gradient (AG)，利用Q值对动作的梯度直接优化动作，模拟策略梯度的作用，并与动作标记预测技术高效结合。

Result: 实验结果表明，AG显著提升了基于DT算法的性能，部分结果达到当前最优水平。

Conclusion: AG是一种稳定且有效的改进方法，解决了DT中的关键挑战，推动了基于Transformer的离线强化学习发展。

Abstract: Decision Transformer (DT), which integrates reinforcement learning (RL) with
the transformer model, introduces a novel approach to offline RL. Unlike
classical algorithms that take maximizing cumulative discounted rewards as
objective, DT instead maximizes the likelihood of actions. This paradigm shift,
however, presents two key challenges: stitching trajectories and extrapolation
of action. Existing methods, such as substituting specific tokens with
predictive values and integrating the Policy Gradient (PG) method, address
these challenges individually but fail to improve performance stably when
combined due to inherent instability. To address this, we propose Action
Gradient (AG), an innovative methodology that directly adjusts actions to
fulfill a function analogous to that of PG, while also facilitating efficient
integration with token prediction techniques. AG utilizes the gradient of the
Q-value with respect to the action to optimize the action. The empirical
results demonstrate that our method can significantly enhance the performance
of DT-based algorithms, with some results achieving state-of-the-art levels.

</details>


### [189] [Computing frustration and near-monotonicity in deep neural networks](https://arxiv.org/abs/2510.05286)
*Joel Wendin,Erik G. Larsson,Claudio Altafini*

Main category: cs.LG

TL;DR: 深度卷积神经网络的符号图具有比零模型更低的挫败水平，表明其结构更有序，表现出近似单调行为，提示一种新的隐式正则化形式。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络符号图的挫败水平，以理解其结构平衡性及与零模型相比的有序程度。

Method: 通过计算深度神经网络相关符号图的挫败水平，并与零模型进行比较，结合统计物理和功能视角分析其特性。

Result: 所有预训练的深度卷积神经网络的挫败水平均低于零模型，显示出更少的无序性和近似单调的行为。

Conclusion: 深度卷积神经网络相较于零模型具有更有序的行为，低挫败水平暗示了一种新的隐式正则化机制。

Abstract: For the signed graph associated to a deep neural network, one can compute the
frustration level, i.e., test how close or distant the graph is to structural
balance. For all the pretrained deep convolutional neural networks we consider,
we find that the frustration is always less than expected from null models.
From a statistical physics point of view, and in particular in reference to an
Ising spin glass model, the reduced frustration indicates that the amount of
disorder encoded in the network is less than in the null models. From a
functional point of view, low frustration (i.e., proximity to structural
balance) means that the function representing the network behaves
near-monotonically, i.e., more similarly to a monotone function than in the
null models. Evidence of near-monotonic behavior along the partial order
determined by frustration is observed for all networks we consider. This
confirms that the class of deep convolutional neural networks tends to have a
more ordered behavior than expected from null models, and suggests a novel form
of implicit regularization.

</details>


### [190] [DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.05288)
*Ruoxing Yang*

Main category: cs.LG

TL;DR: 本文提出了一种改进的差分隐私优化算法DP-Adam-AC，用于本地化语言模型的微调，以解决大语言模型在安全性和数据隐私方面的两个关键问题：依赖远程服务器带来的网络攻击风险，以及敏感数据在微调过程中可能泄露的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但存在运行成本高、需远程访问服务器导致的安全隐患，以及微调时使用敏感数据引发的隐私泄露风险。因此，需要一种能够在本地设备上安全高效微调的方案。

Method: 提出可适应梯度裁剪及其他工程优化技术，改进标准的DP-Adam优化器，形成DP-Adam-AC，并将其应用于小型本地化语言模型（如Qwen2.5-0.5B）和低比特量化模型（Bitnet-b1.58-2B）的微调过程。

Result: 在两个合成数据集上的实验表明，使用DP-Adam-AC进行微调显著降低了损失，显示出良好的性能提升潜力。

Conclusion: 所提出的DP-Adam-AC优化器有效提升了本地化语言模型在差分隐私约束下的微调效果，兼顾了模型性能与数据安全性，为安全、私密的边缘端AI应用提供了可行路径。

Abstract: Large language models (LLMs) such as ChatGPT have evolved into powerful and
ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire
specialized skills for specific tasks efficiently. Although LLMs provide great
utility in both general and task-specific use cases, they are limited by two
security-related concerns. First, traditional LLM hardware requirements make
them infeasible to run locally on consumer-grade devices. A remote network
connection with the LLM provider's server is usually required, making the
system vulnerable to network attacks. Second, fine-tuning an LLM for a
sensitive task may involve sensitive data. Non-private fine-tuning algorithms
produce models vulnerable to training data reproduction attacks. Our work
addresses these security concerns by enhancing differentially private
optimization algorithms and applying them to fine-tune localizable language
models. We introduce adaptable gradient clipping along with other engineering
enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our
optimizer to fine-tune examples of two localizable LLM designs, small language
model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We
demonstrate promising improvements in loss through experimentation with two
synthetic datasets.

</details>


### [191] [Gamma Mixture Modeling for Cosine Similarity in Small Language Models](https://arxiv.org/abs/2510.05309)
*Kevin Player*

Main category: cs.LG

TL;DR: 该论文研究了句子Transformer嵌入的余弦相似性，发现其可由伽马混合模型很好地拟合，并提出了一种基于层次主题聚类解释该现象的启发式模型，同时设计了用于拟合移位伽马混合分布的EM算法。


<details>
  <summary>Details</summary>
Motivation: 理解句子嵌入相似性分布的统计特性，以提升检索和聚类任务中的建模能力。

Method: 分析固定语料库中文档与查询嵌入的余弦相似性分布，提出使用移位且截断至[-1,1]区间的伽马分布及其混合模型进行拟合，并设计EM算法实现参数估计。

Result: 实证表明，句子嵌入的相似性分布常可用单个移位截断伽马分布或伽马混合分布良好拟合。

Conclusion: 伽马混合模型为句子嵌入相似性分布提供了有效的概率建模工具，且可通过所提出的EM算法实际应用。

Abstract: We study the cosine similarity of sentence transformer embeddings and observe
that they are well modeled by gamma mixtures. From a fixed corpus, we measure
similarities between all document embeddings and a reference query embedding.
Empirically we find that these distributions are often well captured by a gamma
distribution shifted and truncated to [-1,1], and in many cases, by a gamma
mixture. We propose a heuristic model in which a hierarchical clustering of
topics naturally leads to a gamma-mixture structure in the similarity scores.
Finally, we outline an expectation-maximization algorithm for fitting shifted
gamma mixtures, which provides a practical tool for modeling similarity
distributions.

</details>


### [192] [RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness](https://arxiv.org/abs/2510.05317)
*Zhenyu Liu,Varun Ojha*

Main category: cs.LG

TL;DR: 提出两种新的正则化策略以增强对抗训练的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练中常用的MSE正则化在输出分布间施加了过度均匀的优化，限制了模型的鲁棒性。

Method: 基于互学习思想，提出加权对抗互正则和对抗泛化正则两种新策略，前者使用分解的对抗互KL散度损失并赋予主辅目标不同权重，后者引入额外的干净目标分布。

Result: 实验表明，所提方法在对抗鲁棒性上显著优于现有的基于正则化的方法。

Conclusion: 所提出的正则化策略有效提升了对抗训练中模型的鲁棒性和泛化能力。

Abstract: Adversarial training is the most effective defense against adversarial
attacks. The effectiveness of the adversarial attacks has been on the design of
its loss function and regularization term. The most widely used loss function
in adversarial training is cross-entropy and mean squared error (MSE) as its
regularization objective. However, MSE enforces overly uniform optimization
between two output distributions during training, which limits its robustness
in adversarial training scenarios. To address this issue, we revisit the idea
of mutual learning (originally designed for knowledge distillation) and propose
two novel regularization strategies tailored for adversarial training: (i)
weighted adversarial mutual regularization and (ii) adversarial generalization
regularization. In the former, we formulate a decomposed adversarial mutual
Kullback-Leibler divergence (KL-divergence) loss, which allows flexible control
over the optimization process by assigning unequal weights to the main and
auxiliary objectives. In the latter, we introduce an additional clean target
distribution into the adversarial training objective, improving generalization
and enhancing model robustness. Extensive experiments demonstrate that our
proposed methods significantly improve adversarial robustness compared to
existing regularization-based approaches.

</details>


### [193] [Tensor-on-tensor Regression Neural Networks for Process Modeling with High-dimensional Data](https://arxiv.org/abs/2510.05329)
*Qian Wang,Mohammad N. Bisheh,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出了一种名为Tensor-on-Tensor Regression Neural Network (TRNN)的新方法，用于处理高维张量数据，在保持张量几何结构的同时捕捉非线性相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有张量回归模型多为线性，而传统神经网络需展平数据导致空间结构丢失，难以有效建模复杂工业过程中的非线性交互。

Method: 设计TRNN模型，结合张量回归与神经网络，直接在张量上进行非线性建模，保留数据的多维结构。

Result: 该方法在保持张量几何结构的同时，能够有效捕捉数据中的强非线性关系，避免了参数过多的问题。

Conclusion: TRNN统一了张量回归与深度学习的优势，为高维、异构张量数据的建模提供了高效且表达力强的新框架。

Abstract: Modern sensing and metrology systems now stream terabytes of heterogeneous,
high-dimensional (HD) data profiles, images, and dense point clouds, whose
natural representation is multi-way tensors. Understanding such data requires
regression models that preserve tensor geometry, yet remain expressive enough
to capture the pronounced nonlinear interactions that dominate many industrial
and mechanical processes. Existing tensor-based regressors meet the first
requirement but remain essentially linear. Conversely, conventional neural
networks offer nonlinearity only after flattening, thereby discarding spatial
structure and incurring prohibitive parameter counts. This paper introduces a
Tensor-on-Tensor Regression Neural Network (TRNN) that unifies these two
paradigms.

</details>


### [194] [Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization](https://arxiv.org/abs/2510.05342)
*Hyung Gyu Rho*

Main category: cs.LG

TL;DR: 本文提出了Margin-Adaptive Direct Preference Optimization (MADPO)，通过实例级自适应温度机制改进DPO，有效缓解了在多样化偏好数据上的过拟合与学习不足问题，在不同质量数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DPO依赖固定温度参数，在多样化的偏好数据上容易对简单样本过拟合、对难样本学习不足；现有改进方法如IPO和β-DPO存在保守正则化、温度适应不精细或训练信号丢失等问题。

Method: MADPO采用两步法：首先训练奖励模型估计偏好边际，然后根据每个样本的边际对DPO损失进行连续自适应加权，实现对学习信号的细粒度调控。

Result: 理论分析表明MADPO优化稳定且对奖励模型误差鲁棒；实验显示其在情感生成任务中显著优于基线方法，在高质量数据上提升达+33.3%，低质量数据上达+10.5%。

Conclusion: MADPO是一种更稳健、更合理的偏好对齐方法，实现了实例级、稳定且保留数据信号的自适应优化。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
method for aligning large language models. However, its reliance on a fixed
temperature parameter leads to suboptimal training on diverse preference data,
causing overfitting on easy examples and under-learning from informative ones.
Recent methods have emerged to counter this. While IPO addresses general
overfitting, its uniform regularization can be overly conservative. The more
targeted approach of $\beta$-DPO suffers from its own limitations: its
batch-level adaptation applies a single, compromised temperature to
mixed-margin pairs, its linear update rule can produce unstable negative
$\beta$ values, and its filtering mechanism discards potentially useful
training signals. In this work, we introduce Margin-Adaptive Direct Preference
Optimization (MADPO), a method that provides a stable, data-preserving, and
instance-level solution. MADPO employs a practical two-step approach: it first
trains a reward model to estimate preference margins and then uses these
margins to apply a continuous, adaptive weight to the DPO loss for each
individual training sample. This re-weighting scheme creates an effective
target margin that is amplified for hard pairs and dampened for easy pairs,
allowing for granular control over the learning signal. We provide a
comprehensive theoretical analysis, proving that MADPO has a well-behaved
optimization landscape and is robust to reward model estimation errors. We
validate our theory with experiments on a sentiment generation task, where
MADPO consistently and significantly outperforms strong baselines across
datasets of varying quality. It achieves performance gains of up to +33.3\% on
High Quality data and +10.5\% on Low Quality data over the next-best method.
Our results establish MADPO as a more robust and principled approach to
preference alignment.

</details>


### [195] [Physics-informed Attention-enhanced Fourier Neural Operator for Solar Magnetic Field Extrapolations](https://arxiv.org/abs/2510.05351)
*Jinghao Cao,Qin Li,Mengnan Du,Haimin Wang,Bo Shen*

Main category: cs.LG

TL;DR: 提出了一种名为PIANO的物理信息增强注意力傅里叶神经算子，用于直接从二维边界条件学习三维非线性无力场结构，结合高效通道注意力与扩张卷积，并引入物理约束损失，在太阳物理NLFFF问题中实现了高精度且物理一致的预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法求解非线性无力场（NLFFF）问题依赖迭代计算，效率低且难以保证物理一致性，因此需要一种能直接从观测数据快速重建三维磁场并满足物理规律的深度学习方法。

Method: 将高效通道注意力（ECA）与扩张卷积（DC）结合到傅里叶神经算子中，提升对多模态输入中关键通道的捕捉能力；同时在训练中引入力-free和无散度的物理约束损失，确保预测结果符合物理规律。

Result: 在ISEE NLFFF数据集上的实验表明，PIANO在准确性上优于现有的先进神经算子模型，并在多个太阳活动区的磁场重建中表现出良好的物理一致性。

Conclusion: PIANO是一种高效且物理一致的深度学习框架，能够准确重建太阳非线性无力磁场结构，为太阳物理中的磁场建模提供了新方法。

Abstract: We propose Physics-informed Attention-enhanced Fourier Neural Operator
(PIANO) to solve the Nonlinear Force-Free Field (NLFFF) problem in solar
physics. Unlike conventional approaches that rely on iterative numerical
methods, our proposed PIANO directly learns the 3D magnetic field structure
from 2D boundary conditions. Specifically, PIANO integrates Efficient Channel
Attention (ECA) mechanisms with Dilated Convolutions (DC), which enhances the
model's ability to capture multimodal input by prioritizing critical channels
relevant to the magnetic field's variations. Furthermore, we apply
physics-informed loss by enforcing the force-free and divergence-free
conditions in the training process so that our prediction is consistent with
underlying physics with high accuracy. Experimental results on the ISEE NLFFF
dataset show that our PIANO not only outperforms state-of-the-art neural
operators in terms of accuracy but also shows strong consistency with the
physical characteristics of NLFFF data across magnetic fields reconstructed
from various solar active regions. The GitHub of this project is available
https://github.com/Autumnstar-cjh/PIANO

</details>


### [196] [MT-DAO: Multi-Timescale Distributed Adaptive Optimizers with Local Updates](https://arxiv.org/abs/2510.05361)
*Alex Iacob,Andrej Jovanovic,Mher Safaryan,Meghdad Kurmanji,Lorenzo Sani,Samuel Horváth,William F. Shen,Xinchi Qiu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 提出MT-DAO优化器家族，通过多时间尺度动量解决分布式训练中自适应优化器在低频通信下的性能下降问题，在语言模型预训练中显著减少通信开销并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 在分布式数据并行训练中，低频通信策略（如Local SGD）应用于自适应优化器时常出现性能下降，主要源于优化器动量更新频率与通信间隔之间的时标不匹配问题。

Method: 设计MT-DAO优化器，引入多个快慢不同的梯度一阶动量，以捕捉不同时间尺度的更新动态，并提供收敛性理论保证。

Result: 在语言模型预训练中，MT-DAO消除了与全同步DDP的性能差距，困惑度表现优于基线，等效token的墙钟时间减少6-27%；在7.2亿参数规模下，达到目标困惑度所需步数减少24%，时间减少35%。

Conclusion: MT-DAO有效解决了低频通信下自适应优化器的性能退化问题，支持跨数据中心和广域地理范围的高效模型训练。

Abstract: Training large models with distributed data parallelism (DDP) requires
frequent communication of gradients across workers, which can saturate
bandwidth. Infrequent communication strategies (e.g., Local SGD) reduce this
overhead but, when applied to adaptive optimizers, often suffer a performance
gap relative to fully synchronous DDP. We trace this gap to a time-scale
mismatch: the optimizer's fast-moving momentum, tuned for frequent updates,
decays too quickly to smooth gradients over long intervals, leading to
noise-dominated optimization. To address this, we propose MT-DAO, a family of
optimizers that employs multiple slow- and fast-moving first momenta or the
gradient to track update dynamics across different time scales, for which we
provide the first convergence guarantees. Empirically, for language-model
pre-training, this eliminates the performance gap with DDP, outperforming
infrequent-communication baselines in perplexity and reducing iso-token
wall-clock time by 6-27% on Ethernet interconnects. At the 720M scale, MT-DAO
reaches a target perplexity in 24% fewer steps and 35% less time than the
single-momentum DDP baseline. MT-DAO enables effective cross-datacenter
training and training over wide geographic areas.

</details>


### [197] [KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction](https://arxiv.org/abs/2510.05373)
*Utkarsh Saxena,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出KVLinC框架，通过Hadamard旋转和轻量线性校正适配器减轻极低精度KV缓存量化带来的注意力误差，在多个大模型上实现高效压缩与快速推理。


<details>
  <summary>Details</summary>
Motivation: 极低精度（如2比特）的KV缓存量化会引入显著误差，影响大语言模型生成质量，需有效缓解量化带来的注意力机制误差。

Method: 结合Hadamard旋转减少值张量的量化误差，并使用轻量级线性校正适配器显式补偿键量化引入的误差，同时设计定制化的注意力内核以加速推理。

Result: 在LLaMA、Qwen2.5和Qwen3系列模型上验证，KVLinC在更高KV缓存压缩率下优于或匹配现有基线方法，并实现最高2.55倍的推理加速。

Conclusion: KVLinC能有效缓解极端低精度KV缓存量化中的注意力误差，显著提升大语言模型的推理效率，支持高效的长上下文推理。

Abstract: Quantizing the key-value (KV) cache is a promising strategy for improving the
inference efficiency of large language models (LLMs). However, aggressive
quantization to very low precision (e.g., 2 bits) introduces significant errors
in the stored key and value tensors, which propagate through the dot-product
attention mechanism and ultimately degrade generation quality. To address this,
we propose KVLinC, a framework to mitigate attention errors introduced by KV
cache quantization in the extreme low-precision regime. KVLinC combines a
Hadamard rotation, which reduces quantization error in values, with lightweight
linear correction adapters that explicitly compensate for errors introduced by
quantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3
model families, KVLinC consistently matches or surpasses strong baselines while
achieving higher KV-cache compression. Furthermore, we implement a custom
attention kernel that results in upto 2.55x faster inference compared to Flash
Attention baseline, enabling efficient long-context LLM inference.

</details>


### [198] [Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding](https://arxiv.org/abs/2510.05385)
*Rohan Arni,Carlos Blanco*

Main category: cs.LG

TL;DR: 提出了一种新的Transformer-based PINN架构S-Pformer，通过去除冗余编码器和引入傅里叶特征嵌入来减少参数量并缓解频谱偏差。


<details>
  <summary>Details</summary>
Motivation: 解决PINNSformer中编码器冗余和频谱偏差问题，提升模型效率与多尺度行为建模能力。

Method: 去除不必要的编码器，仅依赖自注意力捕捉时空相关性，并引入傅里叶特征嵌入以在频域自适应编码多尺度行为。

Result: S-Pformer在所有基准测试中均优于原有的PINNSformer架构，在显著减少参数量的同时达到或超过了MLP的性能。

Conclusion: S-Pformer通过精简结构和频谱偏差控制，有效提升了Transformer-based PINN的效率和性能。

Abstract: Physics-Informed Neural Networks (PINNs) are a useful framework for
approximating partial differential equation solutions using deep learning
methods. In this paper, we propose a principled redesign of the PINNsformer, a
Transformer-based PINN architecture. We present the Spectral PINNSformer
(S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two
key issues; 1. the redundancy (i.e. increased parameter count) of the encoder,
and 2. the mitigation of spectral bias. We find that the encoder is unnecessary
for capturing spatiotemporal correlations when relying solely on
self-attention, thereby reducing parameter count. Further, we integrate Fourier
feature embeddings to explicitly mitigate spectral bias, enabling adaptive
encoding of multiscale behaviors in the frequency domain. Our model outperforms
encoder-decoder PINNSformer architectures across all benchmarks, achieving or
outperforming MLP performance while reducing parameter count significantly.

</details>


### [199] [A Neural Network Algorithm for KL Divergence Estimation with Quantitative Error Bounds](https://arxiv.org/abs/2510.05386)
*Mikil Foss,Andrew Lamperski*

Main category: cs.LG

TL;DR: 提出一种基于浅层神经网络和随机特征的KL散度估计算法，能够在高概率下实现较低的估计误差。


<details>
  <summary>Details</summary>
Motivation: 传统信息论估计器在处理连续随机变量时，随着维度或样本量增加而性能下降，现有基于神经网络的方法缺乏对实际算法误差的理论保证。

Method: 使用具有随机隐藏权重和偏置的浅层神经网络（即随机特征方法）进行KL散度估计，并分析其误差界。

Result: 算法以高概率达到 $O(m^{-1/2}+T^{-1/3})$ 的KL散度估计误差，其中 $m$ 是神经元数量，$T$ 是算法步数和样本数。

Conclusion: 该方法提供了可实现的低误差KL散度估计，且具备理论保障，优于依赖非构造性逼近定理的现有方法。

Abstract: Estimating the Kullback-Leibler (KL) divergence between random variables is a
fundamental problem in statistical analysis. For continuous random variables,
traditional information-theoretic estimators scale poorly with dimension and/or
sample size. To mitigate this challenge, a variety of methods have been
proposed to estimate KL divergences and related quantities, such as mutual
information, using neural networks. The existing theoretical analyses show that
neural network parameters achieving low error exist. However, since they rely
on non-constructive neural network approximation theorems, they do not
guarantee that the existing algorithms actually achieve low error. In this
paper, we propose a KL divergence estimation algorithm using a shallow neural
network with randomized hidden weights and biases (i.e. a random feature
method). We show that with high probability, the algorithm achieves a KL
divergence estimation error of $O(m^{-1/2}+T^{-1/3})$, where $m$ is the number
of neurons and $T$ is both the number of steps of the algorithm and the number
of samples.

</details>


### [200] [Fusion-Based Neural Generalization for Predicting Temperature Fields in Industrial PET Preform Heating](https://arxiv.org/abs/2510.05394)
*Ahmad Alsheikh,Andreas Fischer*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的通用温度预测框架，通过迁移学习和模型融合实现跨材料和几何变化的泛化能力，显著减少对大规模数据集的依赖，并在工业微波预热过程中表现出优越的预测性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统模型在材料或设计变化时需要大量重训练的问题，提升PET坯料预热过程中的温度预测准确性与效率。

Method: 采用一种数据高效的神经网络架构，通过在不同条件下（如再生PET热容、不同坯料几何形状）预训练专用神经回归器，并利用跳连连接将各模型表征融合到一个统一的全局模型中，实现跨场景泛化。

Result: 在材料变异性与几何多样性两个案例中实验验证，该方法相比从零开始训练的模型具有更好的泛化性能和预测精度，同时减少了仿真数据需求。

Conclusion: 所提出的框架为制造环境中的智能热控制提供了一个可扩展的机器学习解决方案，并展示了数据高效泛化策略在其他数据受限的复杂物理建模工业应用中的潜力。

Abstract: Accurate and efficient temperature prediction is critical for optimizing the
preheating process of PET preforms in industrial microwave systems prior to
blow molding. We propose a novel deep learning framework for generalized
temperature prediction. Unlike traditional models that require extensive
retraining for each material or design variation, our method introduces a
data-efficient neural architecture that leverages transfer learning and model
fusion to generalize across unseen scenarios. By pretraining specialized neural
regressor on distinct conditions such as recycled PET heat capacities or
varying preform geometries and integrating their representations into a unified
global model, we create a system capable of learning shared thermal dynamics
across heterogeneous inputs. The architecture incorporates skip connections to
enhance stability and prediction accuracy. Our approach reduces the need for
large simulation datasets while achieving superior performance compared to
models trained from scratch. Experimental validation on two case studies
material variability and geometric diversity demonstrates significant
improvements in generalization, establishing a scalable ML-based solution for
intelligent thermal control in manufacturing environments. Moreover, the
approach highlights how data-efficient generalization strategies can extend to
other industrial applications involving complex physical modeling with limited
data.

</details>


### [201] [Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data](https://arxiv.org/abs/2510.05399)
*Kangwoo Yi,Bo Shen,Qin Li,Haimin Wang,Yong-Jae Moon,Jaewon Lee,Hwanhee Lee*

Main category: cs.LG

TL;DR: 该论文研究了基于LSTM的序列到序列模型，用于预测太阳质子事件后24小时的质子通量时间分布，比较了不同输入数据、预处理方法和预测策略下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测太阳质子事件后的质子通量对保护卫星、宇航员和技术系统至关重要，现有方法在误差累积和输入特征利用方面存在不足。

Method: 采用深度学习中的序列到序列（seq2seq）模型，基于LSTM网络，使用40个SPE事件数据集，通过4折分层交叉验证评估不同模型配置在多种预测场景下的表现。

Result: 1. 一次性预测优于自回归预测，避免了误差累积；2. 原始数据下仅质子输入模型更优，但在趋势平滑后加入X射线数据可缩小或逆转差距；3. 趋势平滑显著提升质子+X射线模型性能；4. 尽管平滑数据平均表现更好，但最佳模型仍来自原始数据训练。

Conclusion: 序列到序列LSTM模型能有效预测质子通量，一次性预测和适当的数据预处理（如趋势平滑）可显著提升性能，且模型结构选择可能比数据预处理更具影响。

Abstract: Solar Proton Events (SPEs) cause significant radiation hazards to satellites,
astronauts, and technological systems. Accurate forecasting of their proton
flux time profiles is crucial for early warnings and mitigation. This paper
explores deep learning sequence-to-sequence (seq2seq) models based on Long
Short-Term Memory networks to predict 24-hour proton flux profiles following
SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by
NOAA GOES, each associated with a >=M-class western-hemisphere solar flare and
undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we
evaluate seq2seq model configurations (varying hidden units and embedding
dimensions) under multiple forecasting scenarios: (i) proton-only input vs.
combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data,
and (iii) autoregressive vs. one-shot forecasting. Our major results are as
follows: First, one-shot forecasting consistently yields lower error than
autoregressive prediction, avoiding the error accumulation seen in iterative
approaches. Second, on the original data, proton-only models outperform
proton+X-ray models. However, with trend-smoothed data, this gap narrows or
reverses in proton+X-ray models. Third, trend-smoothing significantly enhances
the performance of proton+X-ray models by mitigating fluctuations in the X-ray
channel. Fourth, while models trained on trendsmoothed data perform best on
average, the best-performing model was trained on original data, suggesting
that architectural choices can sometimes outweigh the benefits of data
preprocessing.

</details>


### [202] [Correlating Cross-Iteration Noise for DP-SGD using Model Curvature](https://arxiv.org/abs/2510.05416)
*Xin Gu,Yingtai Xiao,Guanlin He,Jiamu Bai,Daniel Kifer,Kiwan Maeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为NoiseCurve的技术，利用模型曲率来改进差分隐私随机梯度下降（DP-SGD）中的跨迭代噪声相关性，从而在多种数据集、模型和隐私参数下显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的DP-SGD方法在保持隐私的同时存在较大的准确率差距，研究旨在通过改进噪声相关性来提高隐私保护训练的性能。

Method: 提出NoiseCurve技术，利用公共无标签数据估计的模型曲率来优化DP-MF中跨迭代的噪声相关性。

Result: 实验表明，NoiseCurve在多个数据集、模型和隐私参数设置下，相比DP-MF的噪声相关方案，能持续且显著地提高模型准确性。

Conclusion: NoiseCurve通过更好地建模噪声相关性，有效缩小了差分隐私训练与普通SGD之间的准确率差距，为隐私保护学习提供了更优解决方案。

Abstract: Differentially private stochastic gradient descent (DP-SGD) offers the
promise of training deep learning models while mitigating many privacy risks.
However, there is currently a large accuracy gap between DP-SGD and normal SGD
training. This has resulted in different lines of research investigating
orthogonal ways of improving privacy-preserving training. One such line of
work, known as DP-MF, correlates the privacy noise across different iterations
of stochastic gradient descent -- allowing later iterations to cancel out some
of the noise added to earlier iterations. In this paper, we study how to
improve this noise correlation. We propose a technique called NoiseCurve that
uses model curvature, estimated from public unlabeled data, to improve the
quality of this cross-iteration noise correlation. Our experiments on various
datasets, models, and privacy parameters show that the noise correlations
computed by NoiseCurve offer consistent and significant improvements in
accuracy over the correlation scheme used by DP-MF.

</details>


### [203] [Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding](https://arxiv.org/abs/2510.05421)
*Shrenik Bhansali,Larry Heck*

Main category: cs.LG

TL;DR: 本文提出了Draft, Verify, & Improve (DVI)框架，通过结合推理与在线学习实现训练感知的自推测解码，在保持无损单模型部署的同时显著加速大语言模型生成，取得了与最先进方法相当的2.16倍加速效果，且训练数据需求大幅降低。


<details>
  <summary>Details</summary>
Motivation: 自回归解码是大语言模型的延迟瓶颈，现有推测解码方法依赖大量离线训练或额外组件，导致计算成本高且在分布漂移下表现脆弱。

Method: 将大语言模型划分为drafter和verifier，利用verifier的接受/拒绝决策作为监督信号持续更新drafter；采用KL→RL调度策略，先通过在线蒸馏提升校准，再引入奖励掩码交叉熵和策略梯度项进行优化。

Result: 在Spec-Bench上实现了2.16倍的实际加速，性能媲美EAGLE-2等最先进方法，训练数据需求低几个数量级，且消融实验表明优于仅使用KL的在线蒸馏方法。

Conclusion: 训练感知的自推测解码（如DVI）能在极低训练开销下实现最先进的无损加速效果，为高效文本生成提供了新方向。

Abstract: Autoregressive (AR) decoding is a major latency bottleneck for large language
models. Speculative decoding (SD) accelerates AR by letting a drafter propose
multi-token blocks that a verifier accepts or rejects. However, many SD systems
require heavy offline training or extra components. These choices raise
data/compute cost and can yield brittle drafters under distribution drift. We
introduce \emph{Draft, Verify, \& Improve (DVI)}, a training-aware
self-speculative framework that combines inference with continual online
learning. We partition an LLM into a drafter and a verifier, and during
generation, verifier accept/reject decisions are converted into supervision
signals and used to update the drafter head. A simple \emph{KL$\rightarrow$RL}
schedule bootstraps calibration via online distillation and then adds
reward-masked cross-entropy with a on-policy policy-gradient term, preserving
lossless, single model deployment. On Spec-Bench, DVI achieves a $2.16\times$
wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of
magnitude less data for training, and ablations show that DVI outperforms
KL-only online distillation. DVI demonstrates that \emph{training-aware}
self-speculation can deliver state-of-the-art, lossless speedups with minimal
training overhead.

</details>


### [204] [Physics-Informed Machine Learning in Biomedical Science and Engineering](https://arxiv.org/abs/2510.05433)
*Nazanin Ahmadi,Qianying Cao,Jay D. Humphrey,George Em Karniadakis*

Main category: cs.LG

TL;DR: 本文综述了物理信息机器学习（PIML）在生物医学系统建模中的三类主要框架：物理信息神经网络（PINNs）、神经常微分方程（NODEs）和神经算子（NOs），并探讨其在生物医学科学与工程中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 由于传统黑箱学习在物理可解释性、数据稀缺性和系统复杂性方面存在不足，亟需融合物理规律与数据驱动方法的PIML范式来有效建模复杂的生物医学系统。

Method: 综述了三类PIML方法：PINNs通过将控制方程嵌入深度学习模型实现物理约束；NODEs采用连续时间建模范式建模动态生理系统；NOs则学习函数空间间的映射，适用于多尺度异质生物系统模拟。

Result: PINNs已成功应用于生物固体力学、生物流体力学、机械生物学和医学成像；NODEs适用于药代动力学和细胞信号传导等动态过程建模；NOs能高效模拟跨尺度和空间异质的生物系统。

Conclusion: PIML在生物医学领域具有巨大潜力，未来发展方向包括不确定性量化、泛化能力提升以及与大语言模型的融合。

Abstract: Physics-informed machine learning (PIML) is emerging as a potentially
transformative paradigm for modeling complex biomedical systems by integrating
parameterized physical laws with data-driven methods. Here, we review three
main classes of PIML frameworks: physics-informed neural networks (PINNs),
neural ordinary differential equations (NODEs), and neural operators (NOs),
highlighting their growing role in biomedical science and engineering. We begin
with PINNs, which embed governing equations into deep learning models and have
been successfully applied to biosolid and biofluid mechanics, mechanobiology,
and medical imaging among other areas. We then review NODEs, which offer
continuous-time modeling, especially suited to dynamic physiological systems,
pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful
tools for learning mappings between function spaces, enabling efficient
simulations across multiscale and spatially heterogeneous biological domains.
Throughout, we emphasize applications where physical interpretability, data
scarcity, or system complexity make conventional black-box learning
insufficient. We conclude by identifying open challenges and future directions
for advancing PIML in biomedical science and engineering, including issues of
uncertainty quantification, generalization, and integration of PIML and large
language models.

</details>


### [205] [Adversarial Reinforcement Learning for Large Language Model Agent Safety](https://arxiv.org/abs/2510.05442)
*Zizhao Wang,Dingcheng Li,Vaishakh Keshava,Phillip Wallis,Ananth Balashankar,Peter Stone,Lukas Rutishauser*

Main category: cs.LG

TL;DR: 提出ARLAS框架，利用对抗强化学习自动生成多样化提示注入攻击并训练代理进行防御，提升任务成功率的同时降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖手工构建的攻击数据集，多样性不足，难以应对新型提示注入攻击。

Method: 将问题建模为双人零和博弈，通过群体学习框架协同训练攻击者（生成攻击）和代理（防御并完成任务），使用对抗强化学习实现持续、多样的攻击生成与防御训练。

Result: 在BrowserGym和AgentDojo上验证，ARLAS微调后的代理攻击成功率显著低于原始模型，同时任务成功率更高；生成的攻击多样且具有挑战性。

Conclusion: ARLAS通过对抗强化学习有效提升了LLM代理对未知提示注入攻击的鲁棒性，是一种更具可扩展性和实用性的安全防御框架。

Abstract: Large Language Model (LLM) agents can leverage tools such as Google Search to
complete complex tasks. However, this tool usage introduces the risk of
indirect prompt injections, where malicious instructions hidden in tool outputs
can manipulate the agent, posing security risks like data leakage. Current
defense strategies typically rely on fine-tuning LLM agents on datasets of
known attacks. However, the generation of these datasets relies on manually
crafted attack patterns, which limits their diversity and leaves agents
vulnerable to novel prompt injections. To address this limitation, we propose
Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework
that leverages adversarial reinforcement learning (RL) by formulating the
problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker
that learns to autonomously generate diverse prompt injections and an agent
that learns to defend against them while completing its assigned tasks. To
ensure robustness against a wide range of attacks and to prevent cyclic
learning, we employ a population-based learning framework that trains the agent
to defend against all previous attacker checkpoints. Evaluated on BrowserGym
and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower
attack success rate than the original model while also improving their task
success rate. Our analysis further confirms that the adversarial process
generates a diverse and challenging set of attacks, leading to a more robust
agent compared to the base model.

</details>


### [206] [Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees in Finite-Horizon MDPs](https://arxiv.org/abs/2510.05446)
*Runlin Zhou,Chixiang Chen,Elynn Chen*

Main category: cs.LG

TL;DR: 本文研究了在有限时域MDP中基于线性Q函数表示和高斯元先验的元强化学习，提出了MTSRL和MTSRL+两种Thompson采样风格的算法，并提供了元遗憾保证，实验表明其在推荐系统环境中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在多任务强化学习中，相似任务的最优动作值函数具有共享结构，利用这种结构可提升学习效率，但现有方法缺乏对元先验学习的理论支持与实用算法设计。

Method: 假设最优Q函数具有线性表示Q*_h(s,a)=Φ_h(s,a)θ^(k)_h，并对任务特定参数θ^(k)_h设定高斯元先验N(θ*_h, Σ*_h)；基于随机值函数方法，提出MTSRL（仅学习先验均值）和MTSRL+（同时估计协方差并引入先验加宽）两种Thompson采样算法；采用先验对齐技术分析元遗憾。

Result: 理论方面：对于已知协方差情况获得\tilde{O}(H^4 S^{3/2} \sqrt{ANK})元遗憾，学习协方差情况下为\tilde{O}(H^4 S^{3/2} \sqrt{AN^3K})，当任务数K超过阈值后性能超越无先验方法；实验方面：在状态感知推荐环境中，MTSRL/MTSRL+在短暂探索后接近元Oracle性能，显著优于无先验RL及_bandit-only_元基线。

Conclusion: 本文首次为带有学习型Q先验的Thompson式强化学习提供了元遗憾保证，提出的算法结合warm-start、OLS聚合与协方差加宽等技巧，在实验丰富的场景下兼具理论保证与实践有效性。

Abstract: We study meta-reinforcement learning in finite-horizon MDPs where related
tasks share similar structures in their optimal action-value functions.
Specifically, we posit a linear representation
$Q^*_h(s,a)=\Phi_h(s,a)\,\theta^{(k)}_h$ and place a Gaussian meta-prior $
\mathcal{N}(\theta^*_h,\Sigma^*_h)$ over the task-specific parameters
$\theta^{(k)}_h$. Building on randomized value functions, we propose two
Thompson-style algorithms: (i) MTSRL, which learns only the prior mean and
performs posterior sampling with the learned mean and known covariance; and
(ii) $\text{MTSRL}^{+}$, which additionally estimates the covariance and
employs prior widening to control finite-sample estimation error. Further, we
develop a prior-alignment technique that couples the posterior under the
learned prior with a meta-oracle that knows the true prior, yielding
meta-regret guarantees: we match prior-independent Thompson sampling in the
small-task regime and strictly improve with more tasks once the prior is
learned. Concretely, for known covariance we obtain
$\tilde{O}(H^{4}S^{3/2}\sqrt{ANK})$ meta-regret, and with learned covariance
$\tilde{O}(H^{4}S^{3/2}\sqrt{AN^3K})$; both recover a better behavior than
prior-independent after $K \gtrsim \tilde{O}(H^2)$ and $K \gtrsim
\tilde{O}(N^2H^2)$, respectively. Simulations on a stateful recommendation
environment (with feature and prior misspecification) show that after brief
exploration, MTSRL/MTSRL\(^+\) track the meta-oracle and substantially
outperform prior-independent RL and bandit-only meta-baselines. Our results
give the first meta-regret guarantees for Thompson-style RL with learned
Q-priors, and provide practical recipes (warm-start via RLSVI, OLS aggregation,
covariance widening) for experiment-rich settings.

</details>


### [207] [QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification](https://arxiv.org/abs/2510.05453)
*Arpit Kapoor,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本文提出Quantile DeepGR4J，通过分位数回归集成学习框架改进DeepGR4J模型，以提升径流预测的准确性与不确定性量化能力，并可用于洪水早期预警。


<details>
  <summary>Details</summary>
Motivation: 为了提高概念性降雨-径流模型的预测精度和不确定性量化能力，特别是在极端事件（如洪水）预测中的应用。

Method: 在DeepGR4J基础上引入基于分位数回归的集成学习框架，实现多步径流预测并生成不确定性区间，利用CAMELS-Aus数据集进行实验验证。

Result: Quantile DeepGR4J在预测精度和不确定性区间质量（区间得分）上优于基线深度学习模型，并能有效识别极端流量事件，具备作为洪水早期预警系统的潜力。

Conclusion: Quantile DeepGR4J显著提升了径流预测的准确性与可靠性，适用于水资源管理及极端水文事件的风险评估。

Abstract: Conceptual rainfall-runoff models aid hydrologists and climate scientists in
modelling streamflow to inform water management practices. Recent advances in
deep learning have unravelled the potential for combining hydrological models
with deep learning models for better interpretability and improved predictive
performance. In our previous work, we introduced DeepGR4J, which enhanced the
GR4J conceptual rainfall-runoff model using a deep learning model to serve as a
surrogate for the routing component. DeepGR4J had an improved rainfall-runoff
prediction accuracy, particularly in arid catchments. Quantile regression
models have been extensively used for quantifying uncertainty while aiding
extreme value forecasting. In this paper, we extend DeepGR4J using a quantile
regression-based ensemble learning framework to quantify uncertainty in
streamflow prediction. We also leverage the uncertainty bounds to identify
extreme flow events potentially leading to flooding. We further extend the
model to multi-step streamflow predictions for uncertainty bounds. We design
experiments for a detailed evaluation of the proposed framework using the
CAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J
framework improves the predictive accuracy and uncertainty interval quality
(interval score) compared to baseline deep learning models. Furthermore, we
carry out flood risk evaluation using Quantile DeepGR4J, and the results
demonstrate its suitability as an early warning system.

</details>


### [208] [AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning](https://arxiv.org/abs/2510.05468)
*Yurun Song,Zhuoyi Yang,Ian G. Harris,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: 本文提出了一种自适应混合位激活量化方法（AMAQ），用于降低大语言模型协同训练中的通信开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的迅速扩大，传统的客户端-服务器协同训练面临通信效率低和计算开销大的挑战，尤其是在资源受限设备上。

Method: 引入参数高效的分拆学习框架，并设计自适应混合位激活量化（AMAQ）策略，通过基于通道、层和特征重要性的比特分配机制，在训练过程中动态调整激活值和梯度的量化精度（从6-8位逐步降至3-4位）。

Result: 在LLaMA3 8B和Qwen2.5 7B等模型上，AMAQ相比固定精度方法提升了约2.5%的生成准确率和1.3%的分类准确率，显著增强了训练稳定性，缓解了极低位表示崩溃问题，并在多机协同训练中表现出优异的推理精度与较低的通信成本。

Conclusion: AMAQ在保证模型性能的同时有效降低了协同训练的通信开销，是一种实用且高效的解决方案，特别适用于资源受限环境下的大模型分布式训练。

Abstract: Large Language Models (LLMs) are scaling rapidly, creating significant
challenges for collaborative server client distributed training, particularly
in terms of communication efficiency and computational overheads. To address
these challenges, we implement Parameter-efficient Split Learning, which
effectively balances efficiency and performance for collaborative training on
low-resource devices.
  To reduce communication overhead in collaborative training, we introduce
Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that
progressively compresses activations and gradients from high precision (6 to 8
bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively
allocating bit budgets across channels based on feature wise and layer wise
importance using bit regularization.
  Under the same bit budgets, AMAQ outperforms fixed-precision approaches,
delivering about 2.5% higher generation accuracy and about 1.3% better
classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition,
it significantly enhances training stability and reducing ultra-low bit
representation collapse during the training.
  Experiments demonstrate that AMAQ integrates effectively into practical
multi-machine collaborative training setups, offering superior inference
accuracy with only a modest communication overhead for bits adaptation during
training. This trade off makes AMAQ a practical and effective solution for
collaborative training with minimal communication cost.

</details>


### [209] [ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics](https://arxiv.org/abs/2510.05482)
*Luke Thompson,Davy Guan,Dai Shi,Slade Matthews,Junbin Gao,Andi Han*

Main category: cs.LG

TL;DR: 提出了一种名为Atomistic Transformer Operator for Molecules (ATOM)的预训练Transformer神经算子，用于多任务分子动力学模拟，实现了在不同分子和时间尺度上的高效、准确和可迁移的预测。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在分子动力学模拟中通常强制对称性并依赖序列展开，且多为单任务模型，限制了其灵活性、效率及对未见分子和长时间步的泛化能力。

Method: 提出ATOM模型，采用准对称设计，无需显式分子图，并引入时间注意力机制，支持多个未来状态的并行解码；基于新构建的大规模数据集TG80进行多任务预训练。

Result: ATOM在MD17、RMD17和MD22等标准单任务基准上达到最先进性能；在TG80上进行多任务预训练后，展现出对未见分子和不同时间范围的卓越零样本泛化能力。

Conclusion: ATOM是迈向精确、高效且可迁移的分子动力学模型的重要一步。

Abstract: Molecular dynamics (MD) simulations underpin modern computational drug dis-
covery, materials science, and biochemistry. Recent machine learning models
provide high-fidelity MD predictions without the need to repeatedly solve
quantum mechanical forces, enabling significant speedups over conventional
pipelines. Yet many such methods typically enforce strict equivariance and rely
on sequential rollouts, thus limiting their flexibility and simulation
efficiency. They are also com- monly single-task, trained on individual
molecules and fixed timeframes, which restricts generalization to unseen
compounds and extended timesteps. To address these issues, we propose Atomistic
Transformer Operator for Molecules (ATOM), a pretrained transformer neural
operator for multitask molecular dynamics. ATOM adopts a quasi-equivariant
design that requires no explicit molecular graph and employs a temporal
attention mechanism, allowing for the accurate parallel decod- ing of multiple
future states. To support operator pretraining across chemicals and timescales,
we curate TG80, a large, diverse, and numerically stable MD dataset with over
2.5 million femtoseconds of trajectories across 80 compounds. ATOM achieves
state-of-the-art performance on established single-task benchmarks, such as
MD17, RMD17 and MD22. After multitask pretraining on TG80, ATOM shows
exceptional zero-shot generalization to unseen molecules across varying time
hori- zons. We believe ATOM represents a significant step toward accurate,
efficient, and transferable molecular dynamics models

</details>


### [210] [The Method of Infinite Descent](https://arxiv.org/abs/2510.05489)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出了一种名为“无限下降法”（Infinite Descent）的新型半解析优化范式，将训练重新表述为直接求解一阶最优条件，并通过解析重求和泰勒展开得到精确的代数更新方程。配合专为此设计的AION神经网络架构，该方法在测试中单步即达最优解，展示了非迭代学习的可行性，进而引出可半解析优化的“无穷类”模型概念。


<details>
  <summary>Details</summary>
Motivation: 传统训练依赖于小步、局部、迭代更新，效率受限。作者希望突破迭代优化的局限，探索能实现精确、非迭代收敛的新型优化范式。

Method: 提出“无限下降法”，通过解析重求和泰勒展开，将优化问题转化为对一阶最优条件的直接代数求解；并设计满足代数封闭性的AION网络架构以支持该方法。

Result: 在简单测试问题中，AION结合无限下降法仅用一次下降步骤即达到最优解，验证了非迭代收敛的可行性。

Conclusion: 无限下降法与AION架构共同展示了一种实现精确、非迭代学习的新途径，提出了“无穷类”模型的概念，为未来非迭代优化模型的设计提供了理论基础和方向。

Abstract: Training - the optimisation of complex models - is traditionally performed
through small, local, iterative updates [D. E. Rumelhart, G. E. Hinton, R. J.
Williams, Nature 323, 533-536 (1986)]. Approximating solutions through
truncated gradients is a paradigm dating back to Cauchy [A.-L. Cauchy, Comptes
Rendus Math\'ematique 25, 536-538 (1847)] and Newton [I. Newton, The Method of
Fluxions and Infinite Series (Henry Woodfall, London, 1736)]. This work
introduces the Method of Infinite Descent, a semi-analytic optimisation
paradigm that reformulates training as the direct solution to the first-order
optimality condition. By analytical resummation of its Taylor expansion, this
method yields an exact, algebraic equation for the update step. Realisation of
the infinite Taylor tower's cascading resummation is formally derived, and an
exploitative algorithm for the direct solve step is proposed.
  This principle is demonstrated with the herein-introduced AION (Analytic,
Infinitely-Optimisable Network) architecture. AION is a model designed
expressly to satisfy the algebraic closure required by Infinite Descent. In a
simple test problem, AION reaches the optimum in a single descent step.
Together, this optimiser-model pair exemplify how analytic structure enables
exact, non-iterative convergence. Infinite Descent extends beyond this example,
applying to any appropriately closed architecture. This suggests a new class of
semi-analytically optimisable models: the \emph{Infinity Class}; sufficient
conditions for class membership are discussed. This offers a pathway toward
non-iterative learning.

</details>


### [211] [NorMuon: Making Muon more efficient and scalable](https://arxiv.org/abs/2510.05491)
*Zichong Li,Liming Liu,Chen Liang,Weizhu Chen,Tuo Zhao*

Main category: cs.LG

TL;DR: 本文提出了NorMuon优化器，通过结合正交化和神经元级别的自适应学习率，解决了Muon优化器中参数更新不均衡的问题，并在大规模语言模型训练中显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 尽管Muon优化器在改善优化几何方面表现出色，但其参数更新在神经元层面存在不均衡现象，且与Adam的优势未被充分结合。因此，需要一种新方法来协同利用正交化和自适应学习率的优点。

Method: 提出NorMuon优化器，引入神经元级别的二阶动量统计和行方向归一化，在正交化后平衡各神经元的更新幅度；同时开发了基于FSDP2框架的高效分布式实现以支持大规模部署。

Result: 实验表明，NorMuon在1.1B模型预训练中比Adam提升21.74%训练效率，比Muon提升11.31%，同时保持与Muon相当的内存开销。

Conclusion: 正交化与自适应学习率是互补的优化策略，NorMuon的成功为大规模深度学习中的优化器设计提供了新方向。

Abstract: The choice of optimizer significantly impacts the training efficiency and
computational costs of large language models (LLMs). Recently, the Muon
optimizer has demonstrated promising results by orthogonalizing parameter
updates, improving optimization geometry through better conditioning. Despite
Muon's emergence as a candidate successor to Adam, the potential for jointly
leveraging their strengths has not been systematically explored. In this work,
we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an
optimizer that synergistically combines orthogonalization with neuron-level
adaptive learning rates. Our analysis reveals that while Muon effectively
reduces condition numbers, the resulting updates exhibit highly non-uniform
neuron norms, causing certain neurons to dominate the optimization process.
NorMuon addresses this imbalance by maintaining second-order momentum
statistics for each neuron and applying row-wise normalization after
orthogonalization, ensuring balanced parameter utilization while preserving
Muon's conditioning benefits. To enable practical deployment at scale, we
develop an efficient distributed implementation under the FSDP2 framework that
strategically distributes orthogonalization computations across devices.
Experiments across multiple model scales demonstrate that NorMuon consistently
outperforms both Adam and Muon, achieving 21.74% better training efficiency
than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while
maintaining a comparable memory footprint to Muon. Our findings suggest that
orthogonalization and adaptive learning rates are complementary rather than
competing approaches, opening new avenues for optimizer design in large-scale
deep learning.

</details>


### [212] [High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training](https://arxiv.org/abs/2510.05492)
*Zhuoyi Huang,Nutan Sahoo,Anamika Kumari,Girish Kumar,Kexuan Cai,Shixing Cao,Yue Kang,Tian Xia,Somya Chatterjee,Nicholas Hausman,Aidan Jay,Eric S. Rosenthal,Soundar Srinivasan,Sadid Hasan,Alex Fedorov,Sulaiman Vesal,Soundar Srinivasan,Sadid Hasan,Alex Fedorov,Sulaiman Vesal*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的ECG合成方法SSSD-ECG，结合MIDT-ECG时间-频率监督和多模态人口统计学条件，显著提升合成心电图的形态保真度、个性化和隐私保护能力，在低数据场景下表现出与真实数据相当的性能。


<details>
  <summary>Details</summary>
Motivation: 由于隐私限制，真实患者心电图（ECG）数据难以共享，阻碍了机器学习在心脏护理中的应用；现有生成模型合成的ECG在形态保真度和个性化方面存在不足，临床实用性受限。

Method: 基于条件扩散模型的Structured State Space Model（SSSD-ECG），引入MIDT-ECG（Mel谱图引导的扩散训练）进行时频域监督，并采用多模态人口统计学条件实现个体化生成。

Result: 在PTB-XL数据集上验证，MIDT-ECG显著提升形态一致性，跨导联相关误差平均降低74%，隐私保护指标优于基线4-8%，且在低数据场景下，使用合成数据训练的分类器性能接近仅使用真实数据训练的模型。

Conclusion: 所提出的结构正则化扩散模型能生成高保真、个性化且隐私安全的ECG信号，可作为真实数据稀缺时的可靠替代，推动生成式AI在医疗中的负责任应用。

Abstract: The development of machine learning for cardiac care is severely hampered by
privacy restrictions on sharing real patient electrocardiogram (ECG) data.
Although generative AI offers a promising solution, the real-world use of
existing model-synthesized ECGs is limited by persistent gaps in
trustworthiness and clinical utility. In this work, we address two major
shortcomings of current generative ECG methods: insufficient morphological
fidelity and the inability to generate personalized, patient-specific
physiological signals. To address these gaps, we build on a conditional
diffusion-based Structured State Space Model (SSSD-ECG) with two principled
innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a
novel training paradigm with time-frequency domain supervision to enforce
physiological structural realism, and (2) multi-modal demographic conditioning
to enable patient-specific synthesis. We comprehensively evaluate our approach
on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity,
clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG
achieves substantial gains: it improves morphological coherence, preserves
strong privacy guarantees with all metrics evaluated exceeding the baseline by
4-8%, and notably reduces the interlead correlation error by an average of 74%,
while demographic conditioning enhances signal-to-noise ratio and
personalization. In critical low-data regimes, a classifier trained on datasets
supplemented with our synthetic ECGs achieves performance comparable to a
classifier trained solely on real data. Together, we demonstrate that ECG
synthesizers, trained with the proposed time-frequency structural
regularization scheme, can serve as personalized, high-fidelity,
privacy-preserving surrogates when real data are scarce, advancing the
responsible use of generative AI in healthcare.

</details>


### [213] [Fundamental Limits of Crystalline Equivariant Graph Neural Networks: A Circuit Complexity Perspective](https://arxiv.org/abs/2510.05494)
*Yang Cao,Zhao Song,Jiahao Zhang,Jiale Zhao*

Main category: cs.LG

TL;DR: 该论文从电路复杂性角度分析了等变图神经网络（EGNNs）在晶体结构预测中的计算与表达能力，证明其在多项式精度下可被常数深度的TC⁰阈值电路族模拟，揭示了其表达力上限，并指出了突破该限制所需的架构改进。


<details>
  <summary>Details</summary>
Motivation: 尽管EGNNs在晶体结构预测中表现优异，但其在周期性和对称性约束下的表达能力尚不明确，亟需理论刻画其计算极限。

Method: 通过电路复杂性理论，分析EGNN层在节点特征、原子坐标和晶格矩阵上的操作，证明其可被多项式大小、常数深度的TC⁰电路族模拟。

Result: 在特定资源约束下（如O(1)层数、O(n)宽度MLP），EGNN可被TC⁰电路模拟，表明其表达能力受限于TC⁰类问题。

Conclusion: EGNN在晶体结构预测中的表达能力存在理论上限，超越此限需增加深度、引入更丰富的几何原语或扩大层宽。

Abstract: Graph neural networks (GNNs) have become a core paradigm for learning on
relational data. In materials science, equivariant GNNs (EGNNs) have emerged as
a compelling backbone for crystalline-structure prediction, owing to their
ability to respect Euclidean symmetries and periodic boundary conditions.
Despite strong empirical performance, their expressive power in periodic,
symmetry-constrained settings remains poorly understood. This work
characterizes the intrinsic computational and expressive limits of EGNNs for
crystalline-structure prediction through a circuit-complexity lens. We analyze
the computations carried out by EGNN layers acting on node features, atomic
coordinates, and lattice matrices, and prove that, under polynomial precision,
embedding width $d=O(n)$ for $n$ nodes, $O(1)$ layers, and $O(1)$-depth,
$O(n)$-width MLP instantiations of the message/update/readout maps, these
models admit a simulation by a uniform $\mathsf{TC}^0$ threshold-circuit family
of polynomial size (with an explicit constant-depth bound). Situating EGNNs
within $\mathsf{TC}^0$ provides a concrete ceiling on the decision and
prediction problems solvable by such architectures under realistic resource
constraints and clarifies which architectural modifications (e.g., increased
depth, richer geometric primitives, or wider layers) are required to transcend
this regime. The analysis complements Weisfeiler-Lehman style results that do
not directly transfer to periodic crystals, and offers a complexity-theoretic
foundation for symmetry-aware graph learning on crystalline systems.

</details>


### [214] [EEG-Based Acute Pain Classification: Machine Learning Model Comparison and Real-Time Clinical Feasibility](https://arxiv.org/abs/2510.05511)
*Aavid Mathrawala,Dhruv Kurup,Josie Lau*

Main category: cs.LG

TL;DR: 本研究利用脑电图（EEG）结合机器学习模型，旨在开发一种客观、实时的疼痛监测工具，以解决当前临床中疼痛评估依赖主观报告的问题。


<details>
  <summary>Details</summary>
Motivation: 现有疼痛评估方法对重症、镇静或认知障碍患者不适用，易导致疼痛治疗不足或阿片类药物滥用。需要一种客观、生理性的疼痛检测手段。

Method: 采集52名健康成人接受激光诱发疼痛时的EEG数据，提取每4秒片段的537个特征（包括频谱功率、Hjorth参数、熵、相干性等），使用留一参与者交叉验证比较九种传统机器学习模型的分类性能。

Result: 支持向量机（RBF核）在离线分析中达到88.9%准确率，推理时间1.02毫秒；XGBoost实时模型延迟约4毫秒，准确率达94.2%。特征重要性分析结果符合已知疼痛神经生理机制，如对侧α抑制、中线θ/α增强和额叶γ爆发。

Conclusion: 基于EEG的疼痛监测在技术上是可行的，具备低延迟和高准确性，有望作为临床辅助工具，并为后续临床验证提供了路径。

Abstract: Current pain assessment within hospitals often relies on self-reporting or
non-specific EKG vital signs. This system leaves critically ill, sedated, and
cognitively impaired patients vulnerable to undertreated pain and opioid
overuse. Electroencephalography (EEG) offers a noninvasive method of measuring
brain activity. This technology could potentially be applied as an assistive
tool to highlight nociceptive processing in order to mitigate this issue. In
this study, we compared machine learning models for classifying high-pain
versus low/no-pain EEG epochs using data from fifty-two healthy adults exposed
to laser-evoked pain at three intensities (low, medium, high). Each four-second
epoch was transformed into a 537-feature vector spanning spectral power, band
ratios, Hjorth parameters, entropy measures, coherence, wavelet energies, and
peak-frequency metrics. Nine traditional machine learning models were evaluated
with leave-one-participant-out cross-validation. A support vector machine with
radial basis function kernel achieved the best offline performance with 88.9%
accuracy and sub-millisecond inference time (1.02 ms). Our Feature importance
analysis was consistent with current canonical pain physiology, showing
contralateral alpha suppression, midline theta/alpha enhancement, and frontal
gamma bursts. The real-time XGBoost model maintained an end-to-end latency of
about 4 ms and 94.2% accuracy, demonstrating that an EEG-based pain monitor is
technically feasible within a clinical setting and provides a pathway towards
clinical validation.

</details>


### [215] [NeST-BO: Fast Local Bayesian Optimization via Newton-Step Targeting of Gradient and Hessian Information](https://arxiv.org/abs/2510.05516)
*Wei-Ting Tang,Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: 提出了一种名为NeST-BO的局部贝叶斯优化方法，通过联合学习梯度和Hessian信息并利用低维子空间优化来高效处理高维黑盒问题，在合成和真实世界问题中表现出优于现有方法的收敛速度和更低的遗憾。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在处理昂贵的黑盒问题时有效，但在高维情况下仍具挑战性，需要一种能在高维空间中保持高效和收敛性的方法。

Method: 提出NeST-BO，使用高斯过程代理模型联合学习梯度和Hessian信息，并通过一步前瞻的牛顿步误差界选择评估点；在低维子空间（如随机嵌入或学习得到的稀疏子空间）中优化采集函数以降低计算成本。

Result: 理论表明该误差界随批量大小收缩，从而继承了不精确牛顿法的收敛性；实验显示在多种高维问题上，NeST-BO相比现有先进方法具有更快的收敛速度和更低的遗憾。

Conclusion: NeST-BO通过在低维子空间中精准目标牛顿步，有效解决了高维贝叶斯优化中的效率与收敛难题，适用于数千变量及未知活跃子空间的实际问题。

Abstract: Bayesian optimization (BO) is effective for expensive black-box problems but
remains challenging in high dimensions. We propose NeST-BO, a local BO method
that targets the Newton step by jointly learning gradient and Hessian
information with Gaussian process surrogates, and selecting evaluations via a
one-step lookahead bound on Newton-step error. We show that this bound (and
hence the step error) contracts with batch size, so NeST-BO directly inherits
inexact-Newton convergence: global progress under mild stability assumptions
and quadratic local rates once steps are sufficiently accurate. To scale, we
optimize the acquisition in low-dimensional subspaces (e.g., random embeddings
or learned sparse subspaces), reducing the dominant cost of learning curvature
from $O(d^2)$ to $O(m^2)$ with $m \ll d$ while preserving step targeting.
Across high-dimensional synthetic and real-world problems, including cases with
thousands of variables and unknown active subspaces, NeST-BO consistently
yields faster convergence and lower regret than state-of-the-art local and
high-dimensional BO baselines.

</details>


### [216] [Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment](https://arxiv.org/abs/2510.05526)
*Ziyi Chen,Junyi Li,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: 提出RLHF-COV和DPO-COV算法，可同时缓解偏好数据中的腐败、奖励过优化和冗长性偏差问题，在离线和在线设置下均有效，且具有理论保证和简单实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多仅解决RLHF和DPO中的单一问题，且部分方法需大量计算并缺乏泛化能力的理论保证，因此需要一种能同时应对多个问题且高效可靠的方法。

Method: 提出RLHF-COV和DPO-COV算法，通过理论分析证明其在带长度正则化的腐败数据下的泛化误差率可达最优水平，并证明DPO-COV与RLHF-COV的等价性。

Result: DPO-COV在离线和在线设置下均表现出有效性，具备无需奖励估计的优点，且获得的泛化误差率与在干净数据上训练的最佳已知结果相当。

Conclusion: RLHF-COV和DPO-COV能有效同时解决腐败、过优化和冗长性问题，兼具理论保证、实践简便性和广泛适用性，推动了LLM对齐技术的发展。

Abstract: Reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) are important techniques to align large language models
(LLM) with human preference. However, the quality of RLHF and DPO training is
seriously compromised by \textit{\textbf{C}orrupted} preference, reward
\textit{\textbf{O}veroptimization}, and bias towards
\textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only
one of these important issues, and the few other works require much computation
to estimate multiple reward models and lack theoretical guarantee of
generalization ability. In this work, we propose RLHF-\textbf{COV} and
DPO-\textbf{COV} algorithms that can simultaneously mitigate these three
issues, in both offline and online settings. This ability is theoretically
demonstrated by obtaining length-regularized generalization error rates for our
DPO-COV algorithms trained on corrupted data, which match the best-known rates
for simpler cases with clean data and without length regularization. Moreover,
our DPO-COV algorithm is simple to implement without reward estimation, and is
proved to be equivalent to our RLHF-COV algorithm, which directly implies the
equivalence between the vanilla RLHF and DPO algorithms. Experiments
demonstrate the effectiveness of our DPO-COV algorithms under both offline and
online settings.

</details>


### [217] [Transfer Learning on Edge Connecting Probability Estimation under Graphon Model](https://arxiv.org/abs/2510.05527)
*Yuyao Wang,Yu-Hung Cheng,Debarghya Mukherjee,Huimin Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为GTRANS的新型迁移学习框架，用于提升小规模目标图的图估计准确性，结合邻域平滑和Gromov-Wasserstein最优传输，并引入自适应去偏机制以防止负迁移。


<details>
  <summary>Details</summary>
Motivation: 由于准确的图估计通常需要大规模图，而实际中常只能观察到小规模网络，因此需要通过迁移学习利用相关的大规模源图来提升小目标图的估计性能。

Method: GTRANS结合了邻域平滑和Gromov-Wasserstein最优传输来对齐和迁移图结构，并通过残差平滑的自适应去偏机制纠正目标图特有的偏差。

Result: 理论证明了对齐矩阵估计的稳定性，并在合成和真实数据上验证了GTRANS在图估计、图分类和链接预测等下游任务中的有效性。

Conclusion: GTRANS能有效提升小规模目标图的估计精度，避免负迁移，具有良好的理论支持和实际应用表现。

Abstract: Graphon models provide a flexible nonparametric framework for estimating
latent connectivity probabilities in networks, enabling a range of downstream
applications such as link prediction and data augmentation. However, accurate
graphon estimation typically requires a large graph, whereas in practice, one
often only observes a small-sized network. One approach to addressing this
issue is to adopt a transfer learning framework, which aims to improve
estimation in a small target graph by leveraging structural information from a
larger, related source graph. In this paper, we propose a novel method, namely
GTRANS, a transfer learning framework that integrates neighborhood smoothing
and Gromov-Wasserstein optimal transport to align and transfer structural
patterns between graphs. To prevent negative transfer, GTRANS includes an
adaptive debiasing mechanism that identifies and corrects for target-specific
deviations via residual smoothing. We provide theoretical guarantees on the
stability of the estimated alignment matrix and demonstrate the effectiveness
of GTRANS in improving the accuracy of target graph estimation through
extensive synthetic and real data experiments. These improvements translate
directly to enhanced performance in downstream applications, such as the graph
classification task and the link prediction task.

</details>


### [218] [ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization](https://arxiv.org/abs/2510.05528)
*Lawrence Liu,Alexander Liu,Mengdi Wang,Tuo Zhao,Lin F. Yang*

Main category: cs.LG

TL;DR: 提出ARMOR，一种新颖的单次后训练剪枝算法，通过矩阵分解将权重矩阵分解为2:4稀疏核心和块对角补偿矩阵，显著提升大模型在保持推理加速与内存节省的同时的压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有2:4稀疏剪枝方法在硬件加速方面有潜力，但常导致显著性能下降，亟需一种能在不牺牲模型质量的前提下实现高效压缩的方法。

Method: ARMOR不直接剪枝权重，而是将每个权重矩阵分解为一个2:4稀疏核心和两个低开销的块对角变换矩阵，作为误差校正器；通过块坐标下降法最小化层级别代理损失来选择组件。

Result: 在Llama和Qwen系列模型上实验表明，ARMOR在多种下游任务和困惑度评估中显著优于当前最先进的2:4剪枝方法，同时保持了推理速度和内存压缩优势。

Conclusion: ARMOR实现了更优的模型压缩与任务准确率之间的权衡，为大语言模型的高效部署提供了一种有效解决方案。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their immense computational and memory requirements. While semi-structured
pruning, particularly 2:4 sparsity, offers a path to practical hardware
acceleration, existing methods often incur substantial performance degradation.
To bridge this gap, we introduce ARMOR: (Adaptive Representation with
Matrix-factORization), a novel one-shot post-training pruning algorithm.
Instead of directly pruning weights, ARMOR factorizes each weight matrix into a
2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These
wrappers act as efficient pre and post-transformation error correctors,
offering greater flexibility to preserve model quality compared to conventional
2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen
through a block coordinate descent algorithm that minimizes a layer-wise proxy
loss. We theoretically prove this optimization is guaranteed to converge to a
solution with a proxy loss less than or equal to state-of-the-art pruning
algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and
Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and
significantly outperforms state-of-the-art 2:4 pruning methods across a wide
range of downstream tasks and perplexity evaluations. ARMOR achieves this
superior performance while retaining the inference speedups and substantial
memory usage reductions of 2:4 pruning, establishing a more effective trade-off
between model compression and task accuracy

</details>


### [219] [LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability](https://arxiv.org/abs/2510.05530)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 本文提出了Langevin-Anchored Test-Time Adaptation (LATTA)，通过引入噪声权重扰动和稳定的权重锚定机制，在测试时自适应中实现了更稳定且有效的模型调整，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法（如Tent）在小批量或复杂干扰下容易不稳定并灾难性遗忘源知识，主要源于复杂损失面上的确定性更新过强。

Method: LATTA结合了两种机制：1）受随机梯度朗之万动力学启发的噪声权重扰动，用于探索局部参数空间；2）稳定的权重锚，防止模型偏离鲁棒的预训练状态。

Result: 在Rotated-MNIST和CIFAR-10-C等基准上，LATTA显著优于Tent、CoTTA和EATA等方法，在CIFAR-10-C上平均准确率提升超过2%，同时降低性能方差。

Conclusion: LATTA在不牺牲稳定性的情况下有效提升了测试时自适应性能，无需架构修改或昂贵的蒙特卡洛采样，为自监督TTA设定了新的最先进水平。

Abstract: Test-time adaptation (TTA) aims to adapt a pretrained model to distribution
shifts using only unlabeled test data. While promising, existing methods like
Tent suffer from instability and can catastrophically forget the source
knowledge, especially with small batch sizes or challenging corruptions. We
argue that this arises from overly deterministic updates on a complex loss
surface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation
(LATTA), a novel approach that regularizes adaptation through two key
mechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient
Langevin Dynamics (SGLD) to explore the local parameter space and escape poor
local minima, and (2) a stable weight anchor that prevents the model from
diverging from its robust source pre-training. This combination allows LATTA to
adapt effectively without sacrificing stability. Unlike prior Bayesian TTA
methods, LATTA requires no architectural changes or expensive Monte Carlo
passes. We conduct extensive experiments on standard benchmarks, including
Rotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that
LATTA significantly outperforms existing methods, including Tent, CoTTA, and
EATA, setting a new state of the art for self-supervised TTA by improving
average accuracy on CIFAR-10-C by over 2% while simultaneously reducing
performance variance.

</details>


### [220] [Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection](https://arxiv.org/abs/2510.05535)
*Rui Liu,Tao Zhe,Yanjie Fu,Feng Xia,Ted Senator,Dongjie Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦特征选择框架，通过隐私保护的知识融合和样本感知加权策略，解决了分布式场景下的数据异构性、不平衡性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法在捕捉复杂特征交互和适应多样化应用场景方面存在不足，且难以应对分布式环境中的数据不平衡、异构性和隐私限制。

Method: 结合置换不变嵌入与策略引导搜索，并引入隐私保护的知识融合策略和样本感知加权机制，以实现跨客户端的特征选择知识整合。

Result: 实验表明该框架在有效性、鲁棒性和效率方面表现优异，且在联邦学习场景中具有强泛化能力。

Conclusion: 所提出的方法能够有效支持分布式环境下的特征选择，兼顾隐私保护与性能优化，适用于现实世界的联邦学习应用。

Abstract: Feature selection eliminates redundancy among features to improve downstream
task performance while reducing computational overhead. Existing methods often
struggle to capture intricate feature interactions and adapt across diverse
application scenarios. Recent advances employ generative intelligence to
alleviate these drawbacks. However, these methods remain constrained by
permutation sensitivity in embedding and reliance on convexity assumptions in
gradient-based search. To address these limitations, our initial work
introduces a novel framework that integrates permutation-invariant embedding
with policy-guided search. Although effective, it still left opportunities to
adapt to realistic distributed scenarios. In practice, data across local
clients is highly imbalanced, heterogeneous and constrained by strict privacy
regulations, limiting direct sharing. These challenges highlight the need for a
framework that can integrate feature selection knowledge across clients without
exposing sensitive information. In this extended journal version, we advance
the framework from two perspectives: 1) developing a privacy-preserving
knowledge fusion strategy to derive a unified representation space without
sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy
to address distributional imbalance among heterogeneous local clients.
Extensive experiments validate the effectiveness, robustness, and efficiency of
our framework. The results further demonstrate its strong generalization
ability in federated learning scenarios. The code and data are publicly
available: https://anonymous.4open.science/r/FedCAPS-08BF.

</details>


### [221] [Critical attention scaling in long-context transformers](https://arxiv.org/abs/2510.05554)
*Shi Chen,Zhengjiang Lin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 本文提出了一种简化的模型来分析注意力缩放对长上下文语言模型的影响，揭示了注意力分数在不同缩放因子下的相变行为，并从理论上证明了对数级缩放（β_n ∝ log n）是维持稀疏、内容自适应注意力的关键，为YaRN和Qwen中的注意力缩放提供了严谨解释。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型上下文长度的增加，注意力机制出现秩崩溃问题，即注意力分数趋于均匀，导致信息表达能力下降，亟需理论指导如何有效进行注意力缩放。

Method: 构建一个可解析的简化模型，放大注意力缩放的影响，通过理论分析揭示缩放因子β_n如何引发注意力机制的相变行为。

Result: 发现存在临界缩放阈值β_n ≍ log n：缩放不足会导致所有token趋向同一方向，过度缩放则使注意力退化为恒等映射；只有适当的对数级缩放才能维持稀疏且依赖内容的注意力。

Conclusion: 从理论上验证了对数缩放策略的有效性，为YaRN和Qwen等模型中使用的注意力缩放方法提供了坚实依据。

Abstract: As large language models scale to longer contexts, attention layers suffer
from a fundamental pathology: attention scores collapse toward uniformity as
context length $n$ increases, causing tokens to cluster excessively, a
phenomenon known as rank-collapse. While $\textit{attention scaling}$
effectively addresses this deficiency by rescaling attention scores with a
polylogarithmic factor $\beta_n$, theoretical justification for this approach
remains lacking.
  We analyze a simplified yet tractable model that magnifies the effect of
attention scaling. In this model, attention exhibits a phase transition
governed by the scaling factor $\beta_n$: insufficient scaling collapses all
tokens to a single direction, while excessive scaling reduces attention to
identity, thereby eliminating meaningful interactions between tokens. Our main
result identifies the critical scaling $\beta_n \asymp \log n$ and provides a
rigorous justification for attention scaling in YaRN and Qwen, clarifying why
logarithmic scaling maintains sparse, content-adaptive attention at large
context lengths.

</details>


### [222] [Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection](https://arxiv.org/abs/2510.05562)
*Sheng Xiang,Yidong Jiang,Yunting Chen,Dawei Cheng,Guoping Zhao,Changjun Jiang*

Main category: cs.LG

TL;DR: 提出一种基于生成式动态图模型（GDGM）的金融交易中合谋欺诈检测新框架，通过建模动态交易行为和节点关系，结合神经微分方程与门控循环单元，有效捕捉时间动态特征，在真实数据集和实际市场部署中均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法多关注孤立节点特征，难以捕捉复杂、动态的合谋欺诈行为；现有图方法在处理动态、不规则交易模式时存在局限性。

Method: 提出生成式动态图模型（GDGM），将原始交易数据转为时间戳序列，利用神经常微分方程和门控循环单元建模交易行为，引入生成式动态潜在空间以捕捉时间模式和市场演化，并采用伪标签生成与异质聚合技术增强检测效果。

Result: 在多个欺诈检测数据集上实验表明，GDGM在检测准确率上优于现有最先进模型，且该系统已成功部署于全球最大的交易市场之一。

Conclusion: GDGM能有效建模动态交易关系与时间演化特征，显著提升合谋欺诈检测性能，具备良好的实际应用价值。

Abstract: Spoofing detection in financial trading is crucial, especially for
identifying complex behaviors such as conspiracy spoofing. Traditional
machine-learning approaches primarily focus on isolated node features, often
overlooking the broader context of interconnected nodes. Graph-based
techniques, particularly Graph Neural Networks (GNNs), have advanced the field
by leveraging relational information effectively. However, in real-world
spoofing detection datasets, trading behaviors exhibit dynamic, irregular
patterns. Existing spoofing detection methods, though effective in some
scenarios, struggle to capture the complexity of dynamic and diverse, evolving
inter-node relationships. To address these challenges, we propose a novel
framework called the Generative Dynamic Graph Model (GDGM), which models
dynamic trading behaviors and the relationships among nodes to learn
representations for conspiracy spoofing detection. Specifically, our approach
incorporates the generative dynamic latent space to capture the temporal
patterns and evolving market conditions. Raw trading data is first converted
into time-stamped sequences. Then we model trading behaviors using the neural
ordinary differential equations and gated recurrent units, to generate the
representation incorporating temporal dynamics of spoofing patterns.
Furthermore, pseudo-label generation and heterogeneous aggregation techniques
are employed to gather relevant information and enhance the detection
performance for conspiratorial spoofing behaviors. Experiments conducted on
spoofing detection datasets demonstrate that our approach outperforms
state-of-the-art models in detection accuracy. Additionally, our spoofing
detection system has been successfully deployed in one of the largest global
trading markets, further validating the practical applicability and performance
of the proposed method.

</details>


### [223] [Efficient Learning-based Graph Simulation for Temporal Graphs](https://arxiv.org/abs/2510.05569)
*Sheng Xiang,Chenhao Xu,Dawei Cheng,Xiaoyang Wang,Ying Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种高效的基于学习的时序图生成方法——时序图自编码器（TGAE），通过注意力机制编码局部 ego-图的结构与时间特征，并设计了解码器在生成质量与效率间取得平衡，实验表明其在真实和合成时序图上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时序图生成器在捕捉真实时序图的结构与时间特性方面存在不足，尤其是基于时序随机游走的学习方法训练效率低或生成速度慢，因此需要一种更高效且高质量的生成方法。

Method: 提出时序图自编码器（TGAE），采用基于注意力机制的图编码器对采样的ego-图进行时空特征编码，并设计了高效的ego-图解码器用于生成图快照。

Result: 在真实和合成时序图上的实验表明，TGAE在模拟质量和生成效率方面均优于代表性的时序图生成方法，特别是在运行速度和可扩展性上表现突出。

Conclusion: TGAE是一种高效且有效的时序图生成模型，能够很好地保留真实时序图的结构和时间动态特性，为大规模时序图模拟提供了可行方案。

Abstract: Graph simulation has recently received a surge of attention in graph
processing and analytics. In real-life applications, e.g. social science,
biology, and chemistry, many graphs are composed of a series of evolving graphs
(i.e., temporal graphs). While most of the existing graph generators focus on
static graphs, the temporal information of the graphs is ignored. In this
paper, we focus on simulating temporal graphs, which aim to reproduce the
structural and temporal properties of the observed real-life temporal graphs.
In this paper, we first give an overview of the existing temporal graph
generators, including recently emerged learning-based approaches. Most of these
learning-based methods suffer from one of the limitations: low efficiency in
training or slow generating, especially for temporal random walk-based methods.
Therefore, we propose an efficient learning-based approach to generate graph
snapshots, namely temporal graph autoencoder (TGAE). Specifically, we propose
an attention-based graph encoder to encode temporal and structural
characteristics on sampled ego-graphs. And we proposed an ego-graph decoder
that can achieve a good trade-off between simulation quality and efficiency in
temporal graph generation. Finally, the experimental evaluation is conducted
among our proposed TGAE and representative temporal graph generators on
real-life temporal graphs and synthesized graphs. It is reported that our
proposed approach outperforms the state-of-the-art temporal graph generators by
means of simulation quality and efficiency.

</details>


### [224] [Power Mechanism: Private Tabular Representation Release for Model Agnostic Consumption](https://arxiv.org/abs/2510.05581)
*Praneeth Vepakomma,Kaustubh Ponkshe*

Main category: cs.LG

TL;DR: 提出了一种新的协作学习方法Ours，通过联合设计隐私编码网络和效用生成网络，实现基于嵌入共享的差分隐私保护，减少客户端通信轮次和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统协同学习依赖模型权重共享，而基于嵌入共享的方法更高效，但缺乏差分隐私机制。因此需要为嵌入共享设计隐私保护方案。

Method: 设计一个隐私编码网络与一个小的效用生成网络协同训练，生成具有形式化差分隐私保证的嵌入，并在服务器端进行后处理以提升任务准确率。

Result: 实现仅需一轮私有化通信、降低客户端计算开销，且生成的私有化嵌入对服务器端模型类型（如深度学习、随机森林、XGBoost）无依赖。

Conclusion: 该方法在保障隐私的同时提升了资源效率，适用于多种服务器模型，推动了协作学习中嵌入共享的隐私-效用平衡。

Abstract: Traditional collaborative learning approaches are based on sharing of model
weights between clients and a server. However, there are advantages to resource
efficiency through schemes based on sharing of embeddings (activations) created
from the data. Several differentially private methods were developed for
sharing of weights while such mechanisms do not exist so far for sharing of
embeddings. We propose Ours to learn a privacy encoding network in conjunction
with a small utility generation network such that the final embeddings
generated from it are equipped with formal differential privacy guarantees.
These privatized embeddings are then shared with a more powerful server, that
learns a post-processing that results in a higher accuracy for machine learning
tasks. We show that our co-design of collaborative and private learning results
in requiring only one round of privatized communication and lesser compute on
the client than traditional methods. The privatized embeddings that we share
from the client are agnostic to the type of model (deep learning, random
forests or XGBoost) used on the server in order to process these activations to
complete a task.

</details>


### [225] [(Token-Level) \textbf{InfoRMIA}: Stronger Membership Inference and Memorization Assessment for LLMs](https://arxiv.org/abs/2510.05582)
*Jiashu Tao,Reza Shokri*

Main category: cs.LG

TL;DR: 本文提出了InfoRMIA，一种基于信息论的成员推断攻击方法，相较于现有方法在性能和效率上均有提升，并引入了token级别的隐私泄漏分析，实现了对大语言模型中记忆内容的精确定位。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型训练数据广泛且可能泄露敏感信息，亟需有效量化其隐私风险，尤其是成员推断攻击方面的评估。

Method: 提出InfoRMIA，采用信息论框架进行成员推断攻击，并引入token级别的信号分析，以更精细地检测模型输出中的记忆化内容。

Result: InfoRMIA在多个基准上优于RMIA，具有更高的计算效率和更强的序列级推断能力，同时能定位到具体被记忆的token。

Conclusion: token级别的成员推断为大语言模型的隐私分析提供了新视角，有助于实现更精准的隐私保护和遗忘机制。

Abstract: Machine learning models are known to leak sensitive information, as they
inevitably memorize (parts of) their training data. More alarmingly, large
language models (LLMs) are now trained on nearly all available data, which
amplifies the magnitude of information leakage and raises serious privacy
risks. Hence, it is more crucial than ever to quantify privacy risk before the
release of LLMs. The standard method to quantify privacy is via membership
inference attacks, where the state-of-the-art approach is the Robust Membership
Inference Attack (RMIA). In this paper, we present InfoRMIA, a principled
information-theoretic formulation of membership inference. Our method
consistently outperforms RMIA across benchmarks while also offering improved
computational efficiency.
  In the second part of the paper, we identify the limitations of treating
sequence-level membership inference as the gold standard for measuring leakage.
We propose a new perspective for studying membership and memorization in LLMs:
token-level signals and analyses. We show that a simple token-based InfoRMIA
can pinpoint which tokens are memorized within generated outputs, thereby
localizing leakage from the sequence level down to individual tokens, while
achieving stronger sequence-level inference power on LLMs. This new scope
rethinks privacy in LLMs and can lead to more targeted mitigation, such as
exact unlearning.

</details>


### [226] [When Does Global Attention Help? A Unified Empirical Study on Atomistic Graph Learning](https://arxiv.org/abs/2510.05583)
*Arindam Chowdhury,Massimiliano Lupo Pasini*

Main category: cs.LG

TL;DR: 本文提出了一个统一的、可复现的基准框架HydraGNN，用于系统评估原子尺度图学习中全局注意力机制的有效性，比较了四种模型类别在七个多样化数据集上的表现，揭示了编码器增强的MPNN作为强基线，而融合局部-全局模型在长程相互作用主导的任务中效果最佳，并量化了注意力机制的计算开销。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）广泛用于原子尺度建模，但全局注意力机制相对于传统消息传递网络（MPNNs）的实际优势尚不明确，因实现、特征或超参数差异导致结果不可比。因此需要一个统一、可控的框架来系统评估不同模型组件的贡献。

Method: 构建基于HydraGNN的统一基准框架，包含四类受控模型：纯MPNN、带化学/拓扑编码器的MPNN、MPNN与全局注意力的GPS风格混合模型、以及带编码器的完全融合局部-全局模型；在七个开源数据集上进行回归与分类任务的系统实验，控制变量以分离消息传递、全局注意力和编码器特征增强的影响。

Result: 编码器增强的MPNN构成稳健基线；融合局部-全局模型在受长程相互作用影响的性质预测中表现最优；全局注意力带来精度提升的同时显著增加内存开销；研究量化了注意力机制的准确率-计算权衡。

Conclusion: 全局注意力机制在特定条件下（如长程效应主导）具有明确优势，但其收益依赖于模型结构设计与特征工程；本研究提供了首个对原子尺度图学习中全局注意力的受控评估，并建立了可复现的测试平台，为未来模型发展奠定了基础。

Abstract: Graph neural networks (GNNs) are widely used as surrogates for costly
experiments and first-principles simulations to study the behavior of compounds
at atomistic scale, and their architectural complexity is constantly increasing
to enable the modeling of complex physics. While most recent GNNs combine more
traditional message passing neural networks (MPNNs) layers to model short-range
interactions with more advanced graph transformers (GTs) with global attention
mechanisms to model long-range interactions, it is still unclear when global
attention mechanisms provide real benefits over well-tuned MPNN layers due to
inconsistent implementations, features, or hyperparameter tuning. We introduce
the first unified, reproducible benchmarking framework - built on HydraGNN -
that enables seamless switching among four controlled model classes: MPNN, MPNN
with chemistry/topology encoders, GPS-style hybrids of MPNN with global
attention, and fully fused local - global models with encoders. Using seven
diverse open-source datasets for benchmarking across regression and
classification tasks, we systematically isolate the contributions of message
passing, global attention, and encoder-based feature augmentation. Our study
shows that encoder-augmented MPNNs form a robust baseline, while fused
local-global models yield the clearest benefits for properties governed by
long-range interaction effects. We further quantify the accuracy - compute
trade-offs of attention, reporting its overhead in memory. Together, these
results establish the first controlled evaluation of global attention in
atomistic graph learning and provide a reproducible testbed for future model
development.

</details>


### [227] [Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising](https://arxiv.org/abs/2510.05589)
*Kangjia Yan,Chenxi Liu,Hao Miao,Xinle Wu,Yan Zhao,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为TimePD的源域无数据时间序列预测框架，利用大语言模型和代理去噪技术，在不访问源数据的情况下实现跨域适应，显著提升了稀疏目标域上的预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据保护法规限制，无法直接使用源域数据进行迁移学习，因此需要在没有源数据的情况下实现时间序列预测模型的有效域适应。

Method: TimePD包含三个核心组件：基于季节-趋势分解的双分支不变性特征学习、无需参数的轻量级代理去噪机制以校正LLM系统偏差，以及双向知识蒸馏对齐去噪与原始预测。

Result: 在多个真实世界数据集上的实验表明，TimePD平均优于现有最先进方法9.3%。

Conclusion: TimePD为源域无数据场景下的时间序列预测提供了一个有效且实用的解决方案，展示了LLM在时间序列领域中的潜力。

Abstract: The proliferation of mobile devices generates a massive volume of time series
across various domains, where effective time series forecasting enables a
variety of real-world applications. This study focuses on a new problem of
source-free domain adaptation for time series forecasting. It aims to adapt a
pretrained model from sufficient source time series to the sparse target time
series domain without access to the source data, embracing data protection
regulations. To achieve this, we propose TimePD, the first source-free time
series forecasting framework with proxy denoising, where large language models
(LLMs) are employed to benefit from their generalization capabilities.
Specifically, TimePD consists of three key components: (1) dual-branch
invariant disentangled feature learning that enforces representation- and
gradient-wise invariance by means of season-trend decomposition; (2)
lightweight, parameter-free proxy denoising that dynamically calibrates
systematic biases of LLMs; and (3) knowledge distillation that bidirectionally
aligns the denoised prediction and the original target prediction. Extensive
experiments on real-world datasets offer insight into the effectiveness of the
proposed TimePD, outperforming SOTA baselines by 9.3% on average.

</details>


### [228] [Riddled basin geometry sets fundamental limits to predictability and reproducibility in deep learning](https://arxiv.org/abs/2510.05606)
*Andrew Ly,Pulin Gong*

Main category: cs.LG

TL;DR: 深度学习的可预测性存在根本限制，源于其吸引子盆地的分形、交错几何结构，导致即使初始条件精度大幅提高，预测性能也仅能获得微小提升。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习在物理和计算系统中预测能力的根本极限。

Method: 通过分析深度学习中常见的特征（如混沌学习动力学和对称性诱导的不变子空间），推导出交错盆地产生的充分条件，并建立其与实际深度网络中普遍存在的现象之间的联系。

Result: 发现深度网络的吸引子盆地具有无限精细的分形结构，其不确定性指数接近零，表现出高度不可预测性；这种交错结构是导致训练结果难以复现的根本原因。

Conclusion: 交错的吸引子盆地为深度学习提供了一个普遍的组织原则，揭示了优化过程中的基本限制，对人工智能的安全部署具有重要意义。

Abstract: Fundamental limits to predictability are central to our understanding of many
physical and computational systems. Here we show that, despite its remarkable
capabilities, deep learning exhibits such fundamental limits rooted in the
fractal, riddled geometry of its basins of attraction: any initialization that
leads to one solution lies arbitrarily close to another that leads to a
different one. We derive sufficient conditions for the emergence of riddled
basins by analytically linking features widely observed in deep learning,
including chaotic learning dynamics and symmetry-induced invariant subspaces,
to reveal a general route to riddling in realistic deep networks. The resulting
basins of attraction possess an infinitely fine-scale fractal structure
characterized by an uncertainty exponent near zero, so that even large
increases in the precision of initial conditions yield only marginal gains in
outcome predictability. Riddling thus imposes a fundamental limit on the
predictability and hence reproducibility of neural network training, providing
a unified account of many empirical observations. These results reveal a
general organizing principle of deep learning with important implications for
optimization and the safe deployment of artificial intelligence.

</details>


### [229] [Monte Carlo-Type Neural Operator for Differential Equations](https://arxiv.org/abs/2510.05620)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: 提出了一种基于蒙特卡洛方法的神经算子（MCNO），用于求解一维偏微分方程，通过直接学习核函数并利用蒙特卡洛型积分进行近似，无需频谱表示或平移不变性假设，具有跨网格分辨率泛化能力和高效计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如傅里叶神经算子（FNO）依赖频谱表示和平移不变核假设，限制了在复杂或非均匀问题中的适用性；需要一种更灵活、理论支持的替代方案来处理连续域PDE的求解。

Method: 将核函数表示为可学习张量，通过对离散网格上的输入-输出对进行一次性均匀随机采样构建蒙特卡洛型积分估计器，并引入插值步骤以支持任意输入输出网格间的映射，从而实现多分辨率泛化。

Result: 在标准1D PDE基准上表现出竞争性的精度和较低的计算成本；理论分析表明该蒙特卡洛估计器在温和正则条件下具有有界偏差和方差，且结果适用于任意空间维度。

Conclusion: MCNO为神经算子设计提供了一种无需频谱或图结构假设的新范式，理论上支持其扩展至高维问题，展示了蒙特卡洛型积分在PDE求解中的潜力，是FNO和GNO等方法的有效替代。

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a framework for
learning solution operators of one-dimensional partial differential equations
(PDEs) by directly learning the kernel function and approximating the
associated integral operator using a Monte Carlo-type approach. Unlike Fourier
Neural Operators (FNOs), which rely on spectral representations and assume
translation-invariant kernels, MCNO makes no such assumptions. The kernel is
represented as a learnable tensor over sampled input-output pairs, and sampling
is performed once, uniformly at random from a discretized grid. This design
enables generalization across multiple grid resolutions without relying on
fixed global basis functions or repeated sampling during training, while an
interpolation step maps between arbitrary input and output grids to further
enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO
achieves competitive accuracy with efficient computational cost. We also
provide a theoretical analysis proving that the Monte Carlo estimator yields a
bounded bias and variance under mild regularity assumptions. This result holds
in any spatial dimension, suggesting that MCNO may extend naturally beyond
one-dimensional problems. More broadly, this work explores how Monte Carlo-type
integration can be incorporated into neural operator frameworks for
continuous-domain PDEs, providing a theoretically supported alternative to
spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such
as the Graph Kernel Neural Operator, GNO).

</details>


### [230] [NEO: No-Optimization Test-Time Adaptation through Latent Re-Centering](https://arxiv.org/abs/2510.05635)
*Alexander Murphy,Michal Danilowski,Soumyajit Chatterjee,Abhirup Ghosh*

Main category: cs.LG

TL;DR: NEO是一种无需超参数、计算开销极低的测试时自适应（TTA）方法，通过重新居中目标数据嵌入来改善源域与分布偏移样本之间的对齐，显著提升分类准确率和模型校准性能，并在多种设备和数据集上表现出高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的TTA方法通常计算成本高、需要大量数据或对超参数敏感，因此需要一种更高效、鲁棒且轻量的自适应方法。

Method: 基于潜在空间几何结构的理论分析，提出NEO方法，通过将目标数据的嵌入重新居中到原点来增强源域与目标域的对齐，实现无需超参数调整的完全TTA。

Result: 在仅使用64个样本的一个批次上，NEO将ViT-Base在ImageNet-C上的分类准确率从55.6%提升至59.2%；使用512个样本时，在多个数据集上优于所有对比的7种TTA方法中的大部分，同时计算开销最小；在树莓派和Jetson Orin Nano上推理时间减少63%，内存使用降低9%；并能从一个类别自适应提升999个其他类别的准确性。

Conclusion: NEO是一种高效、有效且易于部署的TTA方法，具有低计算开销、无需超参数调优、良好校准性和跨设备适用性，适用于多种视觉Transformer架构和数据集。

Abstract: Test-Time Adaptation (TTA) methods are often computationally expensive,
require a large amount of data for effective adaptation, or are brittle to
hyperparameters. Based on a theoretical foundation of the geometry of the
latent space, we are able to significantly improve the alignment between source
and distribution-shifted samples by re-centering target data embeddings at the
origin. This insight motivates NEO -- a hyperparameter-free fully TTA method,
that adds no significant compute compared to vanilla inference. NEO is able to
improve the classification accuracy of ViT-Base on ImageNet-C from 55.6% to
59.2% after adapting on just one batch of 64 samples. When adapting on 512
samples NEO beats all 7 TTA methods we compare against on ImageNet-C,
ImageNet-R and ImageNet-S and beats 6/7 on CIFAR-10-C, while using the least
amount of compute. NEO performs well on model calibration metrics and
additionally is able to adapt from 1 class to improve accuracy on 999 other
classes in ImageNet-C. On Raspberry Pi and Jetson Orin Nano devices, NEO
reduces inference time by 63% and memory usage by 9% compared to baselines. Our
results based on 3 ViT architectures and 4 datasets show that NEO can be used
efficiently and effectively for TTA.

</details>


### [231] [Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models](https://arxiv.org/abs/2510.05670)
*David Debot,Giuseppe Marra*

Main category: cs.LG

TL;DR: 本文提出了一种统一的概率概念侧信道元模型，并引入了侧信道独立性评分（SIS）来量化并控制概念侧信道模型（CSM）中准确性和可解释性之间的权衡，通过SIS正则化显著提升了模型的可解释性和干预能力。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型（CBNM）因信息流受限导致预测准确性下降，而概念侧信道模型（CSM）虽提升准确性却牺牲了可解释性，缺乏对这一权衡的系统性控制方法。

Method: 提出统一的CSM元模型框架，定义侧信道独立性评分（SIS），并通过SIS正则化在训练中显式惩罚对侧信道的依赖。

Result: 实验表明，当前最优CSM在仅优化准确率时可解释性较低，而采用SIS正则化后显著提升了可解释性、干预能力和任务预测器质量。

Conclusion: 该工作为构建兼顾准确性与可解释性的CSM提供了理论和实践工具，揭示了不同架构中的内在权衡，并实现了更可控的模型设计。

Abstract: Concept Bottleneck Models (CBNMs) are deep learning models that provide
interpretability by enforcing a bottleneck layer where predictions are based
exclusively on human-understandable concepts. However, this constraint also
restricts information flow and often results in reduced predictive accuracy.
Concept Sidechannel Models (CSMs) address this limitation by introducing a
sidechannel that bypasses the bottleneck and carry additional task-relevant
information. While this improves accuracy, it simultaneously compromises
interpretability, as predictions may rely on uninterpretable representations
transmitted through sidechannels. Currently, there exists no principled
technique to control this fundamental trade-off. In this paper, we close this
gap. First, we present a unified probabilistic concept sidechannel meta-model
that subsumes existing CSMs as special cases. Building on this framework, we
introduce the Sidechannel Independence Score (SIS), a metric that quantifies a
CSM's reliance on its sidechannel by contrasting predictions made with and
without sidechannel information. We propose SIS regularization, which
explicitly penalizes sidechannel reliance to improve interpretability. Finally,
we analyze how the expressivity of the predictor and the reliance of the
sidechannel jointly shape interpretability, revealing inherent trade-offs
across different CSM architectures. Empirical results show that
state-of-the-art CSMs, when trained solely for accuracy, exhibit low
representation interpretability, and that SIS regularization substantially
improves their interpretability, intervenability, and the quality of learned
interpretable task predictors. Our work provides both theoretical and practical
tools for developing CSMs that balance accuracy and interpretability in a
principled manner.

</details>


### [232] [Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection](https://arxiv.org/abs/2510.05676)
*Félix Vandervorst,Bruno Deprez,Wouter Verbeke,Tim Verdonck*

Main category: cs.LG

TL;DR: 提出了一种新的基于图的梯度提升机（G-GBM），用于异构和动态图上的监督学习，尤其适用于保险欺诈检测。


<details>
  <summary>Details</summary>
Motivation: 图方法在处理复杂数据关系方面表现出色，但在保险欺诈检测中因类别不平衡和网络异质性、动态性而受限，现有方法仍以表格数据上的梯度提升树为主。

Method: 提出一种新型的归纳式图梯度提升机（G-GBM），结合梯度提升森林作为基础模型，适应异构动态图结构，并利用现有可解释性方法增强模型透明度。

Result: 在多种模拟随机图实验中，G-GBM性能媲美主流图神经网络；在开源和真实专有数据集上验证了其在保险欺诈检测中的有效性。

Conclusion: G-GBM为异构动态图上的欺诈检测提供了一种高效且可解释的解决方案，兼具图方法的优势和梯度提升模型的稳定性与可解释性。

Abstract: Graph-based methods are becoming increasingly popular in machine learning due
to their ability to model complex data and relations. Insurance fraud is a
prime use case, since false claims are often the result of organised criminals
that stage accidents or the same persons filing erroneous claims on multiple
policies. One challenge is that graph-based approaches struggle to find
meaningful representations of the data because of the high class imbalance
present in fraud data. Another is that insurance networks are heterogeneous and
dynamic, given the changing relations among people, companies and policies.
That is why gradient boosted tree approaches on tabular data still dominate the
field. Therefore, we present a novel inductive graph gradient boosting machine
(G-GBM) for supervised learning on heterogeneous and dynamic graphs. We show
that our estimator competes with popular graph neural network approaches in an
experiment using a variety of simulated random graphs. We demonstrate the power
of G-GBM for insurance fraud detection using an open-source and a real-world,
proprietary dataset. Given that the backbone model is a gradient boosting
forest, we apply established explainability methods to gain better insights
into the predictions made by G-GBM.

</details>


### [233] [QGraphLIME - Explaining Quantum Graph Neural Networks](https://arxiv.org/abs/2510.05683)
*Haribandhu Jena,Jyotirmaya Shivottam,Subhankar Mishra*

Main category: cs.LG

TL;DR: 本文提出了QGraphLIME，一种模型无关、后验的量子图神经网络解释框架，通过结构保持扰动生成局部代理模型，并结合其分布与不确定性，提供节点和边的重要性排序。


<details>
  <summary>Details</summary>
Motivation: 量子图神经网络在处理图结构数据上具有潜力，但其可解释性受限于测量随机性和图结构的组合复杂性，因此需要一种能处理不确定性和结构敏感性的解释方法。

Method: 提出QGraphLIME框架，采用结构保持的图扰动生成多个局部代理模型，聚合这些代理的归因及其离散程度，利用Dvoretzky-Kiefer-Wolfowitz界为二分类概率分布提供有限样本下的分布无关保证。

Result: 在合成图上的实验表明，QGraphLIME能生成准确且稳定的解释；消融研究显示非线性代理建模更优，且解释结果对扰动设计敏感。

Conclusion: QGraphLIME为量子图神经网络提供了一种有原则、考虑不确定性且结构敏感的解释方法，为未来扩展到更复杂架构和真实数据奠定了基础。

Abstract: Quantum graph neural networks offer a powerful paradigm for learning on
graph-structured data, yet their explainability is complicated by
measurement-induced stochasticity and the combinatorial nature of graph
structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a
model-agnostic, post-hoc framework that treats model explanations as
distributions over local surrogates fit on structure-preserving perturbations
of a graph. By aggregating surrogate attributions together with their
dispersion, QGraphLIME yields uncertainty-aware node and edge importance
rankings for quantum graph models. The framework further provides a
distribution-free, finite-sample guarantee on the size of the surrogate
ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of
the induced distribution of a binary class probability at target accuracy and
confidence under standard independence assumptions. Empirical studies on
controlled synthetic graphs with known ground truth demonstrate accurate and
stable explanations, with ablations showing clear benefits of nonlinear
surrogate modeling and highlighting sensitivity to perturbation design.
Collectively, these results establish a principled, uncertainty-aware, and
structure-sensitive approach to explaining quantum graph neural networks, and
lay the groundwork for scaling to broader architectures and real-world
datasets, as quantum resources mature. Code is available at
https://github.com/smlab-niser/qglime.

</details>


### [234] [vAttention: Verified Sparse Attention](https://arxiv.org/abs/2510.05688)
*Aditya Desai,Kumar Krishna Agrawal,Shuo Yang,Alejandro Cuadron,Luis Gaspar Schroeder,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.LG

TL;DR: 本文提出了vAttention，一种具有用户可指定近似精度保证的实用稀疏注意力机制，结合了top-k和随机采样的优点，在质量与效率之间实现了更优的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力方法（如top-k和随机采样）在跨注意力头和查询向量上缺乏一致性，且没有近似质量的理论保证，限制了其实际部署。

Method: 通过分析top-k和随机采样各自的适用场景，提出将两者统一的vAttention机制，并利用统计抽样理论为近似精度提供(ε, δ)级别的验证保证。

Result: 实验表明，vAttention在多个模型和数据集上显著提升了稀疏注意力的质量（例如在RULER-HARD上提升约4.5个百分点），并在高达20倍稀疏度下匹配全注意力模型性能；在AIME2024等推理任务中，10倍稀疏度下即可达到全模型质量，并支持最长32K token的生成。

Conclusion: vAttention是首个具备验证精度保证的实用稀疏注意力方法，有效弥合了稀疏与全注意力之间的性能差距，推动了稀疏注意力在大规模实际场景中的可靠部署。

Abstract: State-of-the-art sparse attention methods for reducing decoding latency fall
into two main categories: approximate top-$k$ (and its extension, top-$p$) and
recently introduced sampling-based estimation. However, these approaches are
fundamentally limited in their ability to approximate full attention: they fail
to provide consistent approximations across heads and query vectors and, most
critically, lack guarantees on approximation quality, limiting their practical
deployment. We observe that top-$k$ and random sampling are complementary:
top-$k$ performs well when attention scores are dominated by a few tokens,
whereas random sampling provides better estimates when attention scores are
relatively uniform. Building on this insight and leveraging the statistical
guarantees of sampling, we introduce vAttention, the first practical sparse
attention mechanism with user-specified $(\epsilon, \delta)$ guarantees on
approximation accuracy (thus, verified). These guarantees make vAttention a
compelling step toward practical, reliable deployment of sparse attention at
scale. By unifying top-k and sampling, vAttention outperforms both
individually, delivering a superior quality-efficiency trade-off. Our
experiments show that vAttention significantly improves the quality of sparse
attention (e.g., $\sim$4.5 percentage points for Llama-3.1-8B-Inst and
Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap
between full and sparse attention (e.g., across datasets, it matches full model
quality with upto 20x sparsity). We also demonstrate that it can be deployed in
reasoning scenarios to achieve fast decoding without compromising model quality
(e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with
up to 32K token generations). Code is open-sourced at
https://github.com/xAlg-ai/sparse-attention-hub.

</details>


### [235] [Primal-Dual Direct Preference Optimization for Constrained LLM Alignment](https://arxiv.org/abs/2510.05703)
*Yihan Du,Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 提出了一种新的对偶DPO方法用于大语言模型的安全对齐，在不依赖额外先验知识的情况下，有效降低计算和内存成本，并在理论和实验上验证了其在约束条件下优化输出质量的能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法通常需要训练奖励或成本模型，导致高计算开销，或依赖最优解的先验知识，限制了实际应用。因此，需要一种低开销且无需先验知识的约束对齐方法。

Method: 提出一种原始-对偶DPO方法：首先使用标准DPO在奖励偏好数据上训练模型以获取奖励信息，然后基于重构的拉格朗日DPO目标，在成本偏好数据上微调模型，结合探索奖励扩展至在线场景。

Result: 该方法显著降低了内存和计算成本，无需先验知识；理论上证明了输出策略的次优性和约束违反程度有界；在PKU-SafeRLHF数据集上的实验验证了其有效性。

Conclusion: 所提出的原始-对偶DPO方法为大语言模型的约束对齐提供了一个高效、实用且理论可解释的解决方案，适用于安全内容生成等应用场景。

Abstract: The widespread application of Large Language Models (LLMs) imposes increasing
demands on safety, such as reducing harmful content and fake information, and
avoiding certain forbidden tokens due to rules and laws. While there have been
several recent works studying safe alignment of LLMs, these works either
require the training of reward and cost models and incur high memory and
computational costs, or need prior knowledge about the optimal solution.
Motivated by this fact, we study the problem of constrained alignment in LLMs,
i.e., maximizing the output reward while restricting the cost due to
potentially unsafe content to stay below a threshold. For this problem, we
propose a novel primal-dual DPO approach, which first trains a model using
standard DPO on reward preference data to provide reward information, and then
adopts a rearranged Lagrangian DPO objective utilizing the provided reward
information to fine-tune LLMs on cost preference data. Our approach
significantly reduces memory and computational costs, and does not require
extra prior knowledge. Moreover, we establish rigorous theoretical guarantees
on the suboptimality and constraint violation of the output policy. We also
extend our approach to an online data setting by incorporating exploration
bonuses, which enables our approach to explore uncovered prompt-response space,
and then provide theoretical results that get rid of the dependence on
preference data coverage. Experimental results on the widely-used preference
dataset PKU-SafeRLHF demonstrate the effectiveness of our approach.

</details>


### [236] [DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities](https://arxiv.org/abs/2510.05717)
*Hedi Zisling,Ilan Naiman,Nimrod Berman,Supasorn Suwajanakorn,Omri Azencot*

Main category: cs.LG

TL;DR: 提出DiffSDA，一种基于扩散模型的无监督序列解耦框架，适用于多种真实世界数据模态，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督序列解耦方法依赖多损失项，优化复杂，且缺乏在真实数据上的有效评估协议；扩散模型虽先进，但尚未被理论化用于序列解耦。

Method: 提出Diffusion Sequential Disentanglement Autoencoder (DiffSDA)，结合新的概率建模、潜在扩散机制和高效采样器，设计模态无关框架，并引入具挑战性的评估协议。

Result: 在时间序列、视频和音频等多种真实世界数据集上实验表明，DiffSDA在序列解耦性能上优于最新的前沿方法。

Conclusion: DiffSDA为序列解耦提供了有效且通用的解决方案，推动了扩散模型在无监督表示学习中的理论与应用发展。

Abstract: Unsupervised representation learning, particularly sequential
disentanglement, aims to separate static and dynamic factors of variation in
data without relying on labels. This remains a challenging problem, as existing
approaches based on variational autoencoders and generative adversarial
networks often rely on multiple loss terms, complicating the optimization
process. Furthermore, sequential disentanglement methods face challenges when
applied to real-world data, and there is currently no established evaluation
protocol for assessing their performance in such settings. Recently, diffusion
models have emerged as state-of-the-art generative models, but no theoretical
formalization exists for their application to sequential disentanglement. In
this work, we introduce the Diffusion Sequential Disentanglement Autoencoder
(DiffSDA), a novel, modal-agnostic framework effective across diverse
real-world data modalities, including time series, video, and audio. DiffSDA
leverages a new probabilistic modeling, latent diffusion, and efficient
samplers, while incorporating a challenging evaluation protocol for rigorous
testing. Our experiments on diverse real-world benchmarks demonstrate that
DiffSDA outperforms recent state-of-the-art methods in sequential
disentanglement.

</details>


### [237] [Neighborhood-Adaptive Generalized Linear Graph Embedding with Latent Pattern Mining](https://arxiv.org/abs/2510.05719)
*S. Peng,L. Hu,W. Zhang,B. Jie,Y. Luo*

Main category: cs.LG

TL;DR: 提出了一种基于潜在模式挖掘的邻域自适应广义线性图嵌入模型（NGLGE），通过自适应图学习和低秩重构提升图嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 现有图构建方法需预定义邻域大小，且线性投影方法对不同场景适应性差，难以充分揭示数据内在结构相关性。

Method: 提出NGLGE模型，引入邻域自适应的图学习机制，结合重构的低秩表示，并在投影矩阵上施加ℓ2,0范数约束，以灵活挖掘更多模式信息，同时设计了高效的迭代求解算法。

Result: 在多个不同场景的数据集上进行对比实验，结果表明该模型优于当前先进方法。

Conclusion: NGLGE能更有效地揭示数据内在关联，具有更强的适应性和优越的嵌入性能。

Abstract: Graph embedding has been widely applied in areas such as network analysis,
social network mining, recommendation systems, and bioinformatics. However,
current graph construction methods often require the prior definition of
neighborhood size, limiting the effective revelation of potential structural
correlations in the data. Additionally, graph embedding methods using linear
projection heavily rely on a singular pattern mining approach, resulting in
relative weaknesses in adapting to different scenarios. To address these
challenges, we propose a novel model, Neighborhood-Adaptive Generalized Linear
Graph Embedding (NGLGE), grounded in latent pattern mining. This model
introduces an adaptive graph learning method tailored to the neighborhood,
effectively revealing intrinsic data correlations. Simultaneously, leveraging a
reconstructed low-rank representation and imposing $\ell_{2,0}$ norm constraint
on the projection matrix allows for flexible exploration of additional pattern
information. Besides, an efficient iterative solving algorithm is derived for
the proposed model. Comparative evaluations on datasets from diverse scenarios
demonstrate the superior performance of our model compared to state-of-the-art
methods.

</details>


### [238] [Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies](https://arxiv.org/abs/2510.05725)
*Chunsan Hong,Seonho An,Min-Soo Kim,Jong Chul Ye*

Main category: cs.LG

TL;DR: 提出一种基于KL正则化马尔可夫决策过程的学习型调度器，用于优化掩码扩散模型中的去噪顺序，在多个基准上显著优于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型依赖启发式规则决定去噪顺序，性能受限且缺乏理论保证，需更优策略提升生成质量。

Method: 将去噪过程建模为KL正则化的马尔可夫决策过程，引入参考策略并优化目标函数，实现有理论保证的策略学习。

Result: 在四个基准测试中均优于最大置信度等启发式方法，例如在SUDOKU任务上比随机和最大置信度分别提升20.1%和11.2%。

Conclusion: 所提学习型调度器能生成更贴近真实数据分布的样本，具备理论保证且在关键任务中显著提升性能。

Abstract: Masked diffusion models (MDMs) have recently emerged as a novel framework for
language modeling. MDMs generate sentences by iteratively denoising masked
sequences, filling in [MASK] tokens step by step. Although MDMs support
any-order sampling, performance is highly sensitive to the choice of which
position to unmask next. Prior work typically relies on rule-based schedules
(e.g., max-confidence, max-margin), which provide ad hoc improvements. In
contrast, we replace these heuristics with a learned scheduler. Specifically,
we cast denoising as a KL-regularized Markov decision process (MDP) with an
explicit reference policy and optimize a regularized objective that admits
policy improvement and convergence guarantees under standard assumptions. We
prove that the optimized policy under this framework generates samples that
more closely match the data distribution than heuristic schedules. Empirically,
across four benchmarks, our learned policy consistently outperforms
max-confidence: for example, on SUDOKU, where unmasking order is critical, it
yields a 20.1% gain over random and a 11.2% gain over max-confidence.

</details>


### [239] [Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches](https://arxiv.org/abs/2510.05748)
*Hachem Madmoun,Salem Lahlou*

Main category: cs.LG

TL;DR: 在多智能体LLM系统中，直接通信比课程学习更能有效促进合作；简单的“廉价对话”可显著提升协作水平，而课程设计不当可能导致智能体产生“习得性悲观”。


<details>
  <summary>Details</summary>
Motivation: 研究如何在多智能体大语言模型系统中促进合作，以实现AI对齐目标。

Method: 通过四人Stag Hunt游戏测试直接通信（单字‘廉价对话’通道）的效果，并在带惩罚的重复公共物品博弈中评估不同课程学习设计对智能体合作行为的影响。

Result: 直接通信使合作率从0%提升至48.3%；而强调背叛均衡的课程设计导致智能体收益下降27.4%，并引发‘习得性悲观’。

Conclusion: 对于协调问题，简单的通信协议比基于经验的训练更可靠；社会困境中的课程设计需谨慎考虑游戏序列所传递的战略信号。

Abstract: Eliciting cooperation in multi-agent LLM systems is critical for AI
alignment. We investigate two approaches: direct communication and curriculum
learning. In a 4-player Stag Hunt, a one-word "cheap talk" channel increases
cooperation from 0% to 48.3%, demonstrating communication as a robust
coordination mechanism. In contrast, we find that curriculum learning is highly
sensitive to design choices: our pedagogical curriculum through progressively
complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game
with Punishment. Qualitative analysis reveals that curricula emphasizing
defection-equilibrium games can induce "learned pessimism" in agents. These
findings suggest that for coordination problems, simple communication protocols
may be more reliable than experience-based training, and that curriculum design
for social dilemmas requires careful attention to the strategic lessons
embedded in game sequences.

</details>


### [240] [Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective](https://arxiv.org/abs/2510.05750)
*Xiao Yang,Xuejiao Zhao,Zhiqi Shen*

Main category: cs.LG

TL;DR: 本文系统评估了异质图神经网络（HGNNs）的有效性，通过在21个数据集和20个基线上进行实验，并构建因果效应估计框架，发现模型架构本身对性能无显著因果影响，而异质信息通过提升同配性和分布差异性有效增强节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 尽管HGNNs在节点分类中广泛应用，其内在有效性尤其是模型架构与异质信息的作用尚未被充分验证，多数研究默认其有效性。本文旨在明确HGNN性能提升的真正来源。

Method: 结合大规模复现实验（21个数据集、20个基线）与超参数重调，提出因果效应估计框架，利用事实与反事实分析识别关键因素，并通过调整集、一致性检验和敏感性分析验证结果鲁棒性。

Result: 模型架构和复杂度对性能无因果影响；异质信息通过增加同配性和局部-全局分布差异，对性能有显著正向因果作用。

Conclusion: HGNN的性能提升主要源于异质信息本身，而非复杂的模型设计，表明未来应更关注信息利用而非结构复杂化。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in node
classification. Building on this progress, heterogeneous graph neural networks
(HGNNs) integrate relation types and node and edge semantics to leverage
heterogeneous information. Causal analysis for HGNNs is advancing rapidly,
aiming to separate genuine causal effects from spurious correlations. However,
whether HGNNs are intrinsically effective remains underexamined, and most
studies implicitly assume rather than establish this effectiveness. In this
work, we examine HGNNs from two perspectives: model architecture and
heterogeneous information. We conduct a systematic reproduction across 21
datasets and 20 baselines, complemented by comprehensive hyperparameter
retuning. To further disentangle the source of performance gains, we develop a
causal effect estimation framework that constructs and evaluates candidate
factors under standard assumptions through factual and counterfactual analyses,
with robustness validated via minimal sufficient adjustment sets, cross-method
consistency checks, and sensitivity analyses. Our results lead to two
conclusions. First, model architecture and complexity have no causal effect on
performance. Second, heterogeneous information exerts a positive causal effect
by increasing homophily and local-global distribution discrepancy, which makes
node classes more distinguishable. The implementation is publicly available at
https://github.com/YXNTU/CausalHGNN.

</details>


### [241] [Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning](https://arxiv.org/abs/2510.05753)
*Yuxuan Bai,Gauri Pradhan,Marlon Tobaben,Antti Honkela*

Main category: cs.LG

TL;DR: 本文研究了在迁移学习场景下，不同成员推断攻击（MIA）对微调模型的隐私泄露评估效果，发现没有单一MIA能捕获所有隐私风险，其中LiRA在多数情况下表现最佳，而IHA在高数据量下的PatchCamelyon数据集上更有效。


<details>
  <summary>Details</summary>
Motivation: 随着大规模基础模型的发展，迁移学习成为主流，但其隐私泄露问题亟需系统评估，尤其是针对微调模型的成员推断攻击缺乏全面比较。

Method: 通过在多种迁移学习设置下对比多种成员推断攻击（如LiRA、IHA等）的表现，评估其在不同数据规模和数据集上的攻击效果。

Result: 得分制MIA的攻击效果随训练数据增加而下降；没有一种MIA能覆盖所有隐私风险；LiRA在大多数场景下表现最优，但在高数据 regime 下，IHA对PatchCamelyon数据集微调的模型更具攻击性。

Conclusion: 在评估迁移学习模型的隐私风险时，应结合多种MIA方法，不能依赖单一攻击手段，需根据具体数据场景选择最有效的攻击策略。

Abstract: With the emergence of powerful large-scale foundation models, the training
paradigm is increasingly shifting from from-scratch training to transfer
learning. This enables high utility training with small, domain-specific
datasets typical in sensitive applications.Membership inference attacks (MIAs)
provide an empirical estimate of the privacy leakage by machine learning
models. Yet, prior assessments of MIAs against models fine-tuned with transfer
learning rely on a small subset of possible attacks. We address this by
comparing performance of diverse MIAs in transfer learning settings to help
practitioners identify the most efficient attacks for privacy risk evaluation.
We find that attack efficacy decreases with the increase in training data for
score-based MIAs. We find that there is no one MIA which captures all privacy
risks in models trained with transfer learning. While the Likelihood Ratio
Attack (LiRA) demonstrates superior performance across most experimental
scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against
models fine-tuned on PatchCamelyon dataset in high data regime.

</details>


### [242] [DP-SNP-TIHMM: Differentially Private, Time-Inhomogeneous Hidden Markov Models for Synthesizing Genome-Wide Association Datasets](https://arxiv.org/abs/2510.05777)
*Shadi Rahimian,Mario Fritz*

Main category: cs.LG

TL;DR: 提出了一种基于时非齐次隐马尔可夫模型（TIHMM）生成合成SNP序列数据集的新框架，通过限制每个SNP序列在训练中的梯度贡献实现差分隐私，有效应对SNP间相关性带来的隐私风险。


<details>
  <summary>Details</summary>
Motivation: SNP数据共享存在隐私风险，尤其是由于SNP之间的强相关性，容易受到多种对抗攻击。现有方法要么仅对统计摘要应用差分隐私，要么依赖复杂后处理或公开数据集，难以兼顾隐私与效用。

Method: 采用时非齐次隐马尔可夫模型（TIHMM）生成完整的SNP序列，在训练过程中对每条序列的梯度贡献进行裁剪和限制，从而实现端到端的差分隐私保护。模型允许转移概率随序列位置变化，提升生成数据的保真度。

Result: 在1000 Genomes真实数据集上验证了方法的有效性，在ε∈[1,10]且δ=10⁻⁴的隐私预算下，生成的合成数据能高度还原原始数据的统计特性，优于现有方法。

Conclusion: 该框架能够在提供强差分隐私保证的同时，高效生成高质量的合成SNP数据，支持安全的基因组数据共享，兼具灵活性与实用性。

Abstract: Single nucleotide polymorphism (SNP) datasets are fundamental to genetic
studies but pose significant privacy risks when shared. The correlation of SNPs
with each other makes strong adversarial attacks such as masked-value
reconstruction, kin, and membership inference attacks possible. Existing
privacy-preserving approaches either apply differential privacy to statistical
summaries of these datasets or offer complex methods that require
post-processing and the usage of a publicly available dataset to suppress or
selectively share SNPs.
  In this study, we introduce an innovative framework for generating synthetic
SNP sequence datasets using samples derived from time-inhomogeneous hidden
Markov models (TIHMMs). To preserve the privacy of the training data, we ensure
that each SNP sequence contributes only a bounded influence during training,
enabling strong differential privacy guarantees. Crucially, by operating on
full SNP sequences and bounding their gradient contributions, our method
directly addresses the privacy risks introduced by their inherent correlations.
  Through experiments conducted on the real-world 1000 Genomes dataset, we
demonstrate the efficacy of our method using privacy budgets of $\varepsilon
\in [1, 10]$ at $\delta=10^{-4}$. Notably, by allowing the transition models of
the HMM to be dependent on the location in the sequence, we significantly
enhance performance, enabling the synthetic datasets to closely replicate the
statistical properties of non-private datasets. This framework facilitates the
private sharing of genomic data while offering researchers exceptional
flexibility and utility.

</details>


### [243] [Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates](https://arxiv.org/abs/2510.05805)
*Pafue Christy Nganjimi,Andrew Soltan,Danielle Belgrave,Lei Clifton,David A. Clifton,Anshul Thakur*

Main category: cs.LG

TL;DR: 提出使用二次贝塞尔曲线作为SGD轨迹的平滑替代，以解决数据集压缩中的噪声和存储问题，在多个临床数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法使用完整的SGD轨迹进行监督，存在噪声大、曲率高和存储开销大的问题，影响训练稳定性和效率。

Method: 用连接真实训练初始和最终模型状态的二次贝塞尔曲线替代完整的SGD轨迹，提供低曲率、无噪声的监督信号。

Result: 理论证明了贝塞尔路径作为SGD替代的有效性，并在五个临床数据集上实证显示所提方法优于最先进的压缩方法。

Conclusion: 该方法能稳定梯度、加速收敛、减少存储需求，并支持高效的临床模型开发。

Abstract: Dataset condensation (DC) enables the creation of compact, privacy-preserving
synthetic datasets that can match the utility of real patient records,
supporting democratised access to highly regulated clinical data for developing
downstream clinical models. State-of-the-art DC methods supervise synthetic
data by aligning the training dynamics of models trained on real and those
trained on synthetic data, typically using full stochastic gradient descent
(SGD) trajectories as alignment targets; however, these trajectories are often
noisy, high-curvature, and storage-intensive, leading to unstable gradients,
slow convergence, and substantial memory overhead. We address these limitations
by replacing full SGD trajectories with smooth, low-loss parametric surrogates,
specifically quadratic B\'ezier curves that connect the initial and final model
states from real training trajectories. These mode-connected paths provide
noise-free, low-curvature supervision signals that stabilise gradients,
accelerate convergence, and eliminate the need for dense trajectory storage. We
theoretically justify B\'ezier-mode connections as effective surrogates for SGD
paths and empirically show that the proposed method outperforms
state-of-the-art condensation approaches across five clinical datasets,
yielding condensed datasets that enable clinically effective model development.

</details>


### [244] [Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling](https://arxiv.org/abs/2510.05825)
*Giorgio Giannone,Guangxuan Xu,Nikhil Shivakumar Nayak,Rohan Mahesh Awhad,Shivchander Sudalairaj,Kai Xu,Akash Srivastava*

Main category: cs.LG

TL;DR: 提出Entropic Particle Filtering (ePF) 方法，通过熵引导的退火和前瞻调制机制，缓解粒子滤波在数学推理中的过早收敛问题，显著提升推理时扩展的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于过程奖励模型引导的粒子滤波方法易因早期过度自信评分导致粒子贫化，从而过早收敛于次优解，尤其在计算资源受限时更为严重。

Method: 提出ePF，包含两个关键技术：1）熵退火（EA），通过监控搜索多样性并在多样性下降时动态调整重采样分布以维持探索；2）前瞻调制（LaM），利用后继状态预测当前状态潜力以指导搜索。

Result: 在多个数学推理基准上，ePF显著优于强基线方法，任务奖励相对提升最高达50%。

Conclusion: ePF通过平衡探索与利用，有效缓解了粒子贫化问题，增强了推理时扩展方法在复杂推理任务中的鲁棒性与性能。

Abstract: Inference-Time Scaling (ITS) improves language models by allocating more
computation at generation time. Particle Filtering (PF) has emerged as a strong
ITS method for complex mathematical reasoning tasks, but it is vulnerable when
guided by process reward models, which often assign overconfident scores early
in the reasoning process. This causes PF to suffer from premature exploitation:
it myopically commits to locally promising trajectories, prunes potentially
correct hypotheses, and converges to suboptimal solutions. This failure mode,
known as particle impoverishment, is especially severe under constrained
computational budgets. To address this, we analyze the problem and identify two
root causes: a lack of diversity in the particle set due to overconfident
resampling and consequent inability to assess the potential of a reasoning
path. We introduce Entropic Particle Filtering (ePF), an algorithm that
integrates two new techniques to solve these issues. The first technique,
Entropic Annealing (EA), directly mitigates particle impoverishment by
monitoring search diversity via entropy; when diversity drops, it intervenes by
dynamically annealing the resampling distribution to preserve exploration. The
second, an enhancement called Look-ahead Modulation (LaM), adds a predictive
guide to evaluate a state's potential based on its successors. On several
challenging math benchmarks, ePF significantly outperforms strong baselines and
achieves up to a 50 % relative improvement in task reward. Together, these
methods improve PF's resilience by balancing the exploration of diverse
solution spaces with the exploitation of high-reward regions, ultimately
leading to higher-quality solutions.

</details>


### [245] [Multimodal Trajectory Representation Learning for Travel Time Estimation](https://arxiv.org/abs/2510.05840)
*Zhi Liu,Xuyuan Hu,Xiao Han,Zhehao Dai,Zhaolin Deng,Guojiang Shen,Xiangjie Kong*

Main category: cs.LG

TL;DR: 本文提出了一种新的多模态轨迹表示学习框架MDTI，用于提高交通行程时间估计的准确性，通过融合GPS序列、网格轨迹和路网约束，并采用自监督预训练策略，在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于异构数据源和复杂的交通动态，准确的行程时间估计具有挑战性；传统方法将轨迹转换为固定长度表示，忽略了实际轨迹的可变性，导致信息丢失或特征冗余。

Method: 提出MDTI框架，包含模态特定编码器、跨模态交互模块和动态轨迹建模机制，融合多种轨迹数据，并设计对比对齐和掩码语言建模两种自监督预训练任务。

Result: 在三个真实世界数据集上的实验表明，MDTI consistently 优于现有的最先进基线方法，展现出良好的鲁棒性和泛化能力。

Conclusion: MDTI通过有效整合多模态轨迹信息和动态建模不同长度的轨迹，在行程时间估计任务中表现出优越性能，具备实际应用潜力。

Abstract: Accurate travel time estimation (TTE) plays a crucial role in intelligent
transportation systems. However, it remains challenging due to heterogeneous
data sources and complex traffic dynamics. Moreover, conventional approaches
typically convert trajectories into fixed-length representations, neglecting
the inherent variability of real-world trajectories, which often leads to
information loss or feature redundancy. To address these challenges, this paper
introduces the Multimodal Dynamic Trajectory Integration (MDTI) framework--a
novel multimodal trajectory representation learning approach that integrates
GPS sequences, grid trajectories, and road network constraints to enhance TTE
accuracy. MDTI employs modality-specific encoders and a cross-modal interaction
module to capture complementary spatial, temporal, and topological semantics,
while a dynamic trajectory modeling mechanism adaptively regulates information
density for trajectories of varying lengths. Two self-supervised pretraining
objectives, named contrastive alignment and masked language modeling, further
strengthen multimodal consistency and contextual understanding. Extensive
experiments on three real-world datasets demonstrate that MDTI consistently
outperforms state-of-the-art baselines, confirming its robustness and strong
generalization abilities. The code is publicly available at:
https://github.com/freshhxy/MDTI/

</details>


### [246] [ESS-Flow: Training-free guidance of flow-based models as inference in source space](https://arxiv.org/abs/2510.05849)
*Adhithyan Kalaivanan,Zheng Zhao,Jens Sjölund,Fredrik Lindsten*

Main category: cs.LG

TL;DR: 提出了一种名为ESS-Flow的梯度无关方法，利用基于流的生成模型中的高斯先验，在源空间中直接进行贝叶斯推断，仅需前向传播，无需梯度计算，适用于条件生成和目标属性样本生成。


<details>
  <summary>Details</summary>
Motivation: 在不重新训练模型的情况下，引导预训练的基于流的生成模型完成条件生成或生成具有特定目标属性的样本，解决多样任务。

Method: 采用椭圆切片采样（Elliptical Slice Sampling）在源空间进行贝叶斯推断，利用流模型通常具备的高斯先验，仅通过前向传播实现，避免梯度或雅可比计算。

Result: 在材料设计和蛋白质结构预测（基于稀疏残基距离测量）任务中验证了方法的有效性，即使在梯度不可靠或不可用的情况下（如模拟观测或量化过程）仍表现良好。

Conclusion: ESS-Flow是一种无需梯度、高效且适用范围广的生成模型引导方法，特别适用于复杂或不可微的观测过程。

Abstract: Guiding pretrained flow-based generative models for conditional generation or
to produce samples with desired target properties enables solving diverse tasks
without retraining on paired data. We present ESS-Flow, a gradient-free method
that leverages the typically Gaussian prior of the source distribution in
flow-based models to perform Bayesian inference directly in the source space
using Elliptical Slice Sampling. ESS-Flow only requires forward passes through
the generative model and observation process, no gradient or Jacobian
computations, and is applicable even when gradients are unreliable or
unavailable, such as with simulation-based observations or quantization in the
generation or observation process. We demonstrate its effectiveness on
designing materials with desired target properties and predicting protein
structures from sparse inter-residue distance measurements.

</details>


### [247] [How to model Human Actions distribution with Event Sequence Data](https://arxiv.org/abs/2510.05856)
*Egor Surkov,Dmitry Osin,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.LG

TL;DR: 本文研究了人类行为序列中事件未来分布的预测，提出显式分布预测方法优于复杂的隐式基线，并引入KL散度度量时间漂移，为建模策略选择提供了原则性框架。


<details>
  <summary>Details</summary>
Motivation: 挑战主流的自回归范式，探索是否显式建模未来分布或顺序不变的多标记方法能优于保持顺序的方法。

Method: 分析局部顺序不变性，引入基于KL散度的指标量化时间漂移，并采用显式分布预测目标。

Result: 发现简单的显式分布预测目标 consistently 超过复杂隐式基线，类别模式崩溃主要由分布不平衡引起。

Conclusion: 为选择建模策略提供了原则性框架，并为构建更准确、鲁棒的预测系统提供了实践指导。

Abstract: This paper studies forecasting of the future distribution of events in human
action sequences, a task essential in domains like retail, finance, healthcare,
and recommendation systems where the precise temporal order is often less
critical than the set of outcomes. We challenge the dominant autoregressive
paradigm and investigate whether explicitly modeling the future distribution or
order-invariant multi-token approaches outperform order-preserving methods. We
analyze local order invariance and introduce a KL-based metric to quantify
temporal drift. We find that a simple explicit distribution forecasting
objective consistently surpasses complex implicit baselines. We further
demonstrate that mode collapse of predicted categories is primarily driven by
distributional imbalance. This work provides a principled framework for
selecting modeling strategies and offers practical guidance for building more
accurate and robust forecasting systems.

</details>


### [248] [MaNGO - Adaptable Graph Network Simulators via Meta-Learning](https://arxiv.org/abs/2510.05874)
*Philipp Dahlinger,Tai Hoang,Denis Blessing,Niklas Freymuth,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的新型架构MaNGO，通过条件神经过程和新型神经算子结合，实现对不同物理参数的快速适应，显著优于现有图网络模拟器。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格的物理模拟计算成本高且依赖物理参数，数据驱动方法如GNS需为每个新参数重新训练并收集大量数据，效率低下。

Method: 采用元学习框架，利用条件神经过程（CNP）编码图轨迹生成潜在表示，并结合新型神经算子结构以减少时间上的误差累积。

Result: 在多个具有不同材料属性的动力学预测任务中验证了MaNGO的有效性，其在未见材料属性上的表现接近 oracle 模型，显著优于现有GNS方法。

Conclusion: MaNGO能够有效捕捉不同物理参数下的共享潜在结构，实现无需重新训练的快速适应，提升了数据效率和泛化能力。

Abstract: Accurately simulating physics is crucial across scientific domains, with
applications spanning from robotics to materials science. While traditional
mesh-based simulations are precise, they are often computationally expensive
and require knowledge of physical parameters, such as material properties. In
contrast, data-driven approaches like Graph Network Simulators (GNSs) offer
faster inference but suffer from two key limitations: Firstly, they must be
retrained from scratch for even minor variations in physical parameters, and
secondly they require labor-intensive data collection for each new parameter
setting. This is inefficient, as simulations with varying parameters often
share a common underlying latent structure. In this work, we address these
challenges by learning this shared structure through meta-learning, enabling
fast adaptation to new physical parameters without retraining. To this end, we
propose a novel architecture that generates a latent representation by encoding
graph trajectories using conditional neural processes (CNPs). To mitigate error
accumulation over time, we combine CNPs with a novel neural operator
architecture. We validate our approach, Meta Neural Graph Operator (MaNGO), on
several dynamics prediction tasks with varying material properties,
demonstrating superior performance over existing GNS methods. Notably, MaNGO
achieves accuracy on unseen material properties close to that of an oracle
model.

</details>


### [249] [OBSR: Open Benchmark for Spatial Representations](https://arxiv.org/abs/2510.05879)
*Julia Moska,Oleksii Furman,Kacper Kozaczko,Szymon Leszkiewicz,Jakub Polczyk,Piotr Gramacki,Piotr Szymański*

Main category: cs.LG

TL;DR: 本文提出了一种新的、多任务、跨模态的地理空间嵌入评估基准，涵盖来自三大洲城市的7个多样化数据集，旨在推动GeoAI领域的标准化评估。


<details>
  <summary>Details</summary>
Motivation: 现有GeoAI基准多局限于单一任务和单模态，缺乏统一标准，限制了领域发展。

Method: 构建了一个跨模态、多任务的基准，包含7个来自不同城市的异构地理数据集，并设计了基于任务的简单基线模型。

Result: 该基准支持对地理嵌入模型在多种地理现象下的性能、准确性和效率进行系统评估，并提供了可比较的基线结果。

Conclusion: 所提出的基准有助于推动GeoAI模型的标准化评估，促进更广泛和公平的模型比较与进步。

Abstract: GeoAI is evolving rapidly, fueled by diverse geospatial datasets like traffic
patterns, environmental data, and crowdsourced OpenStreetMap (OSM) information.
While sophisticated AI models are being developed, existing benchmarks are
often concentrated on single tasks and restricted to a single modality. As
such, progress in GeoAI is limited by the lack of a standardized, multi-task,
modality-agnostic benchmark for their systematic evaluation. This paper
introduces a novel benchmark designed to assess the performance, accuracy, and
efficiency of geospatial embedders. Our benchmark is modality-agnostic and
comprises 7 distinct datasets from diverse cities across three continents,
ensuring generalizability and mitigating demographic biases. It allows for the
evaluation of GeoAI embedders on various phenomena that exhibit underlying
geographic processes. Furthermore, we establish a simple and intuitive
task-oriented model baselines, providing a crucial reference point for
comparing more complex solutions.

</details>


### [250] [Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods](https://arxiv.org/abs/2510.05901)
*Martin Benfeghoul,Teresa Delgado,Adnan Oomerjee,Haitham Bou Ammar,Jun Wang,Zafeirios Fountas*

Main category: cs.LG

TL;DR: 本文指出当前预训练后线性化方法在混合注意力机制中存在忽视线性组件、过度依赖滑动窗口softmax的问题，并提出三种解决方案以实现组件均衡使用，在保持计算效率的同时恢复模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练线性化方法虽高效，但在实际运行中未能有效利用线性注意力组件，导致性能归因失真，需识别并解决该问题。

Method: 提出三种新方法：(i) 推理时混合线性与滑动窗口softmax；(ii) HedgeCATs，结合注意力权重迁移与目标LoRA微调；(iii) 采用定时滑动窗口丢弃（SSD）策略，在训练中随机抑制softmax分支。

Result: 所提方法有效防止组件坍塌，显著提升线性注意力的使用比例，保持计算效率的同时恢复了大部分原始模型性能。

Conclusion: 通过改进混合机制的设计与训练策略，可实现真正有效的线性注意力应用，确保性能提升归因的准确性，推动高效Transformer模型的可靠转化。

Abstract: Transformers' quadratic computational complexity limits their scalability
despite remarkable performance. While linear attention reduces this to linear
complexity, pre-training such models from scratch remains, in most cases,
prohibitively expensive. Recent post-training linearisation methods convert
pre-trained Transformers to linear models efficiently, often using hybrid
approaches that combine linear attention with sliding-window softmax. We
identify a critical flaw: existing hybrid methods inadvertently bypass the
linear component, relying almost entirely on SWA. Component-level diagnostics
reveal this previously undetected behaviour stems from overlooked evaluation
practices on common-sense benchmarks. We propose three solutions to ensure
balanced component usage: (i) inference-time hybridisation of linear-only
conversions with sliding-window softmax; (ii) HedgeCATs, combining
attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled
Sliding-window Dropout (SSD), which stochastically suppresses the softmax
branch during training to prevent component collapse. Our methods maintain
computational efficiency while recovering most base model performance and
ensuring genuine linear attention adoption, restoring the validity of
performance attributions in hybrid conversions.

</details>


### [251] [An Attention-Augmented VAE-BiLSTM Framework for Anomaly Detection in 12-Lead ECG Signals](https://arxiv.org/abs/2510.05919)
*Marc Garreta Basora,Mehmet Oguz Mulayim*

Main category: cs.LG

TL;DR: 本研究首次将基于变分自编码器与双向LSTM及多头注意力机制（VAE-BiLSTM-MHA）的架构应用于12导联心电图（ECG）的无监督异常检测，结果表明该模型在CPSC数据集上表现最优，AUPRC为0.81，召回率为0.85，并集成至交互式仪表板用于临床辅助分诊。


<details>
  <summary>Details</summary>
Motivation: 心电图中的异常检测对心血管疾病的早期诊断至关重要，但现有方法在捕捉复杂时序特征和形态变异方面仍有局限，因此需要更高效的无监督模型来提升检测性能。

Method: 采用三种自编码器架构（CAE、VAE-BiLSTM、VAE-BiLSTM-MHA）进行对比，所有模型均在正常ECG样本上训练以学习正常心脏形态，通过重构误差检测异常；使用统一的预处理和评估流程，在CPSC数据集上进行验证，并将最优模型集成到交互式可视化仪表板中。

Result: VAE-BiLSTM-MHA模型表现最佳，AUPRC达到0.81，召回率为0.85，优于其他自编码器结构和文献中的基线模型，且能有效定位异常片段。

Conclusion: 引入多头注意力机制显著提升了VAE-BiLSTM在ECG异常检测中的性能，证明了其在临床应用中的潜力，尤其适用于辅助医生进行快速心电图筛查和分诊。

Abstract: Anomaly detection in 12-lead electrocardiograms (ECGs) is critical for
identifying deviations associated with cardiovascular disease. This work
presents a comparative analysis of three autoencoder-based architectures:
convolutional autoencoder (CAE), variational autoencoder with bidirectional
long short-term memory (VAE-BiLSTM), and VAE-BiLSTM with multi-head attention
(VAE-BiLSTM-MHA), for unsupervised anomaly detection in ECGs. To the best of
our knowledge, this study reports the first application of a VAE-BiLSTM-MHA
architecture to ECG anomaly detection. All models are trained on normal ECG
samples to reconstruct non-anomalous cardiac morphology and detect deviations
indicative of disease. Using a unified preprocessing and evaluation pipeline on
the public China Physiological Signal Challenge (CPSC) dataset, the
attention-augmented VAE achieves the best performance, with an AUPRC of 0.81
and a recall of 0.85 on the held-out test set, outperforming the other
architectures. To support clinical triage, this model is further integrated
into an interactive dashboard that visualizes anomaly localization. In
addition, a performance comparison with baseline models from the literature is
provided.

</details>


### [252] [Carré du champ flow matching: better quality-generalisation tradeoff in generative models](https://arxiv.org/abs/2510.05930)
*Jacob Bamberger,Iolo Jones,Dennis Duncan,Michael M. Bronstein,Pierre Vandergheynst,Adam Gosztolai*

Main category: cs.LG

TL;DR: 本文提出了Carré du champ flow matching (CDC-FM)，一种通过几何感知噪声正则化概率路径来改善生成模型质量-泛化权衡的新方法。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在样本质量和泛化能力之间存在权衡，容易出现记忆化问题，即模型复制训练数据而非学习数据整体结构。

Method: 将传统流匹配中的同质、各向同性噪声替换为捕捉潜在数据流形局部几何结构的空间变化、各向异性高斯噪声，并证明该几何噪声可从数据中最优估计且可扩展到大规模数据。

Result: 在多种数据集（合成流形、点云、单细胞基因组学、动物动作捕捉和图像）和神经网络架构（MLP、CNN、Transformer）上验证了CDC-FM能持续改善质量-泛化权衡，尤其在数据稀缺和采样不均匀情况下显著优于标准流匹配。

Conclusion: CDC-FM提供了一个研究数据几何、泛化与记忆化之间关系的数学框架，并是一种可轻松集成到现有流匹配流程中的鲁棒且可扩展的算法。

Abstract: Deep generative models often face a fundamental tradeoff: high sample quality
can come at the cost of memorisation, where the model reproduces training data
rather than generalising across the underlying data geometry. We introduce
Carr\'e du champ flow matching (CDC-FM), a generalisation of flow matching
(FM), that improves the quality-generalisation tradeoff by regularising the
probability path with a geometry-aware noise. Our method replaces the
homogeneous, isotropic noise in FM with a spatially varying, anisotropic
Gaussian noise whose covariance captures the local geometry of the latent data
manifold. We prove that this geometric noise can be optimally estimated from
the data and is scalable to large data. Further, we provide an extensive
experimental evaluation on diverse datasets (synthetic manifolds, point clouds,
single-cell genomics, animal motion capture, and images) as well as various
neural network architectures (MLPs, CNNs, and transformers). We demonstrate
that CDC-FM consistently offers a better quality-generalisation tradeoff. We
observe significant improvements over standard FM in data-scarce regimes and in
highly non-uniformly sampled datasets, which are often encountered in AI for
science applications. Our work provides a mathematical framework for studying
the interplay between data geometry, generalisation and memorisation in
generative models, as well as a robust and scalable algorithm that can be
readily integrated into existing flow matching pipelines.

</details>


### [253] [LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection](https://arxiv.org/abs/2510.05935)
*Mohamed Bal-Ghaoui,Fayssal Sabri*

Main category: cs.LG

TL;DR: 本文提出了一种基于多智能体辩论机制的可解释特征选择方法LLM-FS-Agent，用于高维数据下的高效机器学习，在网络安全领域实现了优于或媲美现有方法的性能，并显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 高维数据影响模型可解释性和计算效率，现有基于大语言模型的特征选择方法缺乏结构化推理和决策透明性。

Method: 设计一个多智能体架构LLM-FS-Agent，通过角色分工的LLM智能体之间的“辩论”机制进行集体特征相关性评估，并生成详细决策依据。

Result: 在CIC-DIAD 2024物联网入侵检测数据集上，LLM-FS-Agent分类性能优于或媲美LLM-Select和PCA等基线方法，平均减少下游训练时间46%（XGBoost下p=0.028）。

Conclusion: 该多智能体辩论架构在提升决策透明度的同时增强了计算效率，是面向实际应用的可靠特征选择解决方案。

Abstract: High-dimensional data remains a pervasive challenge in machine learning,
often undermining model interpretability and computational efficiency. While
Large Language Models (LLMs) have shown promise for dimensionality reduction
through feature selection, existing LLM-based approaches frequently lack
structured reasoning and transparent justification for their decisions. This
paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for
interpretable and robust feature selection. The system orchestrates a
deliberative "debate" among multiple LLM agents, each assigned a specific role,
enabling collective evaluation of feature relevance and generation of detailed
justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the
CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance
against strong baselines, including LLM-Select and traditional methods such as
PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves
superior or comparable classification performance while reducing downstream
training time by an average of 46% (statistically significant improvement, p =
0.028 for XGBoost). These findings highlight that the proposed deliberative
architecture enhances both decision transparency and computational efficiency,
establishing LLM-FS-Agent as a practical and reliable solution for real-world
applications.

</details>


### [254] [Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density](https://arxiv.org/abs/2510.05949)
*Randall Balestriero,Nicolas Ballas,Mike Rabbat,Yann LeCun*

Main category: cs.LG

TL;DR: 本文发现JEPAs中的反坍缩项不仅能防止表示坍缩，还能估计数据密度，从而使得训练好的JEPA模型可用于计算样本概率，应用于数据筛选、异常检测等任务。该方法称为JEPA-SCORE，具有普适性且可高效闭式计算。


<details>
  <summary>Details</summary>
Motivation: 探索JEPAs中反坍缩项的深层作用，揭示其在表示学习之外的密度估计能力。

Method: 通过理论分析证明反坍缩项能估计数据密度，并利用模型在样本点处的雅可比矩阵以闭式形式高效计算样本概率。

Result: 在多种数据集（合成、控制、ImageNet）和JEPA类方法（I-JEPA、DINOv2）及多模态模型（MetaCLIP）上验证了JEPA-SCORE的有效性。

Conclusion: JEPA-SCORE是一种通用、高效的密度估计方法，揭示了自监督学习模型中未被充分利用的概率解释能力。

Abstract: Joint Embedding Predictive Architectures (JEPAs) learn representations able
to solve numerous downstream tasks out-of-the-box. JEPAs combine two
objectives: (i) a latent-space prediction term, i.e., the representation of a
slightly perturbed sample must be predictable from the original sample's
representation, and (ii) an anti-collapse term, i.e., not all samples should
have the same representation. While (ii) is often considered as an obvious
remedy to representation collapse, we uncover that JEPAs' anti-collapse term
does much more--it provably estimates the data density. In short, any
successfully trained JEPA can be used to get sample probabilities, e.g., for
data curation, outlier detection, or simply for density estimation. Our
theoretical finding is agnostic of the dataset and architecture used--in any
case one can compute the learned probabilities of sample $x$ efficiently and in
closed-form using the model's Jacobian matrix at $x$. Our findings are
empirically validated across datasets (synthetic, controlled, and Imagenet) and
across different Self Supervised Learning methods falling under the JEPA family
(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the
method extracting the JEPA learned density as {\bf JEPA-SCORE}.

</details>


### [255] [Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs](https://arxiv.org/abs/2510.05987)
*Xueyan Li,Guinan Su,Mrinmaya Sachan,Jonas Geiping*

Main category: cs.LG

TL;DR: 本文提出了一种新的解码策略，通过校准正确性而非置信度来优化大语言模型在复杂任务中的推理过程，从而在数学和通用推理基准上取得提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在增加探索性和保证生成质量之间存在冲突，因为它们混淆了不同来源的不确定性。本文旨在解决这一矛盾，提出应基于正确性而非仅置信度进行解码。

Method: 提出了三种策略：Greedy-Threshold在低置信度步骤采用贪婪采样；Calibrated-TopK和Calibrated-epsilon根据估计的逐位正确性设置截断阈值，以实现更优的采样控制。

Result: 所提方法在数学和通用推理基准测试中均显示出性能增益，优于现有解码策略。

Conclusion: 通过将解码规则与预期正确性对齐，可以有效平衡探索与准确性，挑战了当前关于不确定环境下解码的主流启发式方法。

Abstract: Large Language Models (LLMs) are increasingly applied to complex tasks that
require extended reasoning. In such settings, models often benefit from diverse
chains-of-thought to arrive at multiple candidate solutions. This requires two
competing objectives: to inject enough stochasticity to explore multiple
reasoning chains, and to ensure sufficient accuracy and quality in each path.
Existing works pursue the first objective by increasing exploration at highly
uncertain steps with higher temperature or larger candidate token sets, while
others improve reliability by rejecting samples with low confidence
post-generation, implying that low confidence correlates with low answer
quality. These two lines of thought are in conflict, as they conflate different
sources of uncertainty. To resolve this, we argue that the decoding rule should
be calibrated by correctness, not confidence alone. We should sample from
tokens with higher estimated correctness, and reduce sampling where expected
correctness is low. We propose simple strategies that achieve this goal:
Greedy-Threshold makes sampling greedy at very low confidence steps.
Calibrated-TopK and Calibrated-epsilon set truncation threshold based on
estimated rank-wise correctness. Together, our findings challenge prevailing
heuristics about decoding under uncertainty and show gains across math and
general reasoning benchmarks.

</details>


### [256] [Uncertainty in Machine Learning](https://arxiv.org/abs/2510.06007)
*Hans Weytjens,Wouter Verbeke*

Main category: cs.LG

TL;DR: 本章介绍了机器学习中不确定性量化的原理和实际应用，涵盖不同类型的不确定性识别与区分、多种模型的不确定性量化方法以及共形预测框架，探讨了不确定性估计在业务决策、模型可靠性和风险感知策略中的作用。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器学习模型的可靠性并支持风险敏感的应用场景，需要系统地理解和量化模型预测中的不确定性。

Method: 介绍线性回归、随机森林和神经网络等模型中的不确定性量化方法，并使用共形预测生成具有预定义置信区间的预测结果。

Result: 提供了多种模型下不确定性量化的实用方法，并展示了如何通过不确定性估计提升预测的可信度和决策质量。

Conclusion: 不确定性量化是提升机器学习模型可解释性、可靠性和实际应用价值的关键工具，尤其在高风险决策场景中具有重要意义。

Abstract: This book chapter introduces the principles and practical applications of
uncertainty quantification in machine learning. It explains how to identify and
distinguish between different types of uncertainty and presents methods for
quantifying uncertainty in predictive models, including linear regression,
random forests, and neural networks. The chapter also covers conformal
prediction as a framework for generating predictions with predefined confidence
intervals. Finally, it explores how uncertainty estimation can be leveraged to
improve business decision-making, enhance model reliability, and support
risk-aware strategies.

</details>


### [257] [RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra Using Embedded Physics](https://arxiv.org/abs/2510.06020)
*Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Tim Büchner,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（RamPINN）的方法，用于从噪声较多的CARS测量数据中恢复拉曼光谱，通过引入因果性约束和光滑先验实现共振与非共振信号的分离，在无真实标签的情况下实现强零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模训练数据，深度学习在科学领域的应用受限；而科学理论中的物理规律可作为有效的归纳偏置，帮助解决数据稀缺下的逆问题。

Method: 提出RamPINN模型，采用双解码器架构，利用可微希尔伯特变换损失强制施加Kramers-Kronig因果关系以约束共振信号，并对非共振部分施加平滑先验，仅使用合成数据进行训练。

Result: RamPINN在真实实验数据上展现出强大的零样本泛化能力，显著优于现有基线方法；即使不使用任何真实拉曼光谱标签，仅依赖物理损失训练仍能取得有竞争力的结果。

Conclusion: 科学规律可作为强有力的归纳偏置，实现数据受限场景下的鲁棒自监督学习，为知识密集型科学领域提供了新的深度学习应用路径。

Abstract: Transferring the recent advancements in deep learning into scientific
disciplines is hindered by the lack of the required large-scale datasets for
training. We argue that in these knowledge-rich domains, the established body
of scientific theory provides reliable inductive biases in the form of
governing physical laws. We address the ill-posed inverse problem of recovering
Raman spectra from noisy Coherent Anti-Stokes Raman Scattering (CARS)
measurements, as the true Raman signal here is suppressed by a dominating
non-resonant background. We propose RamPINN, a model that learns to recover
Raman spectra from given CARS spectra. Our core methodological contribution is
a physics-informed neural network that utilizes a dual-decoder architecture to
disentangle resonant and non-resonant signals. This is done by enforcing the
Kramers-Kronig causality relations via a differentiable Hilbert transform loss
on the resonant and a smoothness prior on the non-resonant part of the signal.
Trained entirely on synthetic data, RamPINN demonstrates strong zero-shot
generalization to real-world experimental data, explicitly closing this gap and
significantly outperforming existing baselines. Furthermore, we show that
training with these physics-based losses alone, without access to any
ground-truth Raman spectra, still yields competitive results. This work
highlights a broader concept: formal scientific rules can act as a potent
inductive bias, enabling robust, self-supervised learning in data-limited
scientific domains.

</details>


### [258] [Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers](https://arxiv.org/abs/2510.06025)
*Kevin Raina,Tanya Schmah*

Main category: cs.LG

TL;DR: 提出了一种基于期望logit向量的新型贝叶斯后处理OOD评分方法，在小样本训练数据下，贝叶斯方法优于确定性方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，由于训练数据有限，如何有效进行分布外检测（OOD）成为关键问题，而贝叶斯神经网络（BNNs）能显式表达模型不确定性，适合用于OOD检测。

Method: 引入一类基于期望logit向量的贝叶斯后处理OOD评分，并比较了5种贝叶斯与4种确定性后处理OOD评分方法。

Result: 在MNIST和CIFAR-10上，使用5000或更少训练样本的实验表明，贝叶斯方法在OOD检测上优于对应的确定性方法。

Conclusion: 在小数据场景下，贝叶斯后处理OOD方法更具优势，尤其结合先验信息时表现更优。

Abstract: Out-of-Distribution (OOD) detection is critical to AI reliability and safety,
yet in many practical settings, only a limited amount of training data is
available. Bayesian Neural Networks (BNNs) are a promising class of model on
which to base OOD detection, because they explicitly represent epistemic (i.e.
model) uncertainty. In the small training data regime, BNNs are especially
valuable because they can incorporate prior model information. We introduce a
new family of Bayesian posthoc OOD scores based on expected logit vectors, and
compare 5 Bayesian and 4 deterministic posthoc OOD scores. Experiments on MNIST
and CIFAR-10 In-Distributions, with 5000 training samples or less, show that
the Bayesian methods outperform corresponding deterministic methods.

</details>


### [259] [Generalization of Gibbs and Langevin Monte Carlo Algorithms in the Interpolation Regime](https://arxiv.org/abs/2510.06028)
*Andreas Maurer,Erfan Mirzaei,Massimiliano Pontil*

Main category: cs.LG

TL;DR: 本文提出了在过参数化插值机制下吉布斯算法测试误差的数据依赖性界，并证明这些界在使用朗之万蒙特卡洛算法近似时是稳定的。实验验证了这些界在真实标签数据上的非平凡预测能力，并能正确上界随机标签的测试误差。


<details>
  <summary>Details</summary>
Motivation: 在过参数化模型中，即使对不可能的数据（如分类中的随机标签）也能达到低训练误差，因此需要新的理论工具来解释泛化能力。

Method: 提出数据依赖性的吉布斯算法测试误差界，并分析其在朗之万蒙特卡洛近似下的稳定性，通过MNIST和CIFAR-10实验验证。

Result: 所提出的界在真实标签数据上提供了非平凡的测试误差预测，并在随机标签情况下正确上界测试误差；表明低温插值 regime 的泛化能力已在高温 regime 的小训练误差中显现。

Conclusion: 吉布斯算法的泛化性能可通过数据依赖性界有效刻画，且在不同温度 regime 间存在可预测的关联。

Abstract: The paper provides data-dependent bounds on the test error of the Gibbs
algorithm in the overparameterized interpolation regime, where low training
errors are also obtained for impossible data, such as random labels in
classification. The bounds are stable under approximation with Langevin Monte
Carlo algorithms. Experiments on the MNIST and CIFAR-10 datasets verify that
the bounds yield nontrivial predictions on true labeled data and correctly
upper bound the test error for random labels. Our method indicates that
generalization in the low-temperature, interpolation regime is already signaled
by small training errors in the more classical high temperature regime.

</details>


### [260] [Fast Leave-One-Out Approximation from Fragment-Target Prevalence Vectors (molFTP) : From Dummy Masking to Key-LOO for Leakage-Free Feature Construction](https://arxiv.org/abs/2510.06029)
*Guillaume Godin*

Main category: cs.LG

TL;DR: molFTP是一种紧凑的分子片段-靶标流行度表示方法，具有强预测性能，并通过伪掩蔽和key-LOO策略防止特征泄露，近似LOO效果的同时大幅降低成本。


<details>
  <summary>Details</summary>
Motivation: 为了避免在交叉验证中因分子片段信息泄露导致模型性能评估偏差，需要一种既能充分利用数据又可避免数据泄露的表示方法。

Method: 提出molFTP表示法，结合伪掩蔽技术以消除保留分子中的片段信息，并使用key-leave-one-out（key-loo）策略近似全分子层面的leave-one-out（LOO）。

Result: molFTP在数据集上表现出色，key-loo与真实LOO的偏差低于8%，实现了接近全数据训练同时保持无偏的性能估计。

Conclusion: molFTP提供了一种快速、抗泄露的分子片段向量化方法，结合伪掩蔽或key-LOO可有效平衡训练效率与评估准确性。

Abstract: We introduce molFTP (molecular fragment-target prevalence), a compact
representation that delivers strong predictive performance. To prevent feature
leakage across cross-validation folds, we implement a dummy-masking procedure
that removes information about fragments present in the held-out molecules. We
further show that key leave-one-out (key-loo) closely approximates true
molecule-level leave-one-out (LOO), with deviation below 8% on our datasets.
This enables near full data training while preserving unbiased cross-validation
estimates of model performance. Overall, molFTP provides a fast,
leakage-resistant fragment-target prevalence vectorization with practical
safeguards (dummy masking or key-LOO) that approximate LOO at a fraction of its
cost.

</details>


### [261] [From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning](https://arxiv.org/abs/2510.06038)
*Li Zeqiao,Wang Yijing,Wang Haoyu,Li Zheng,Li Peng,Liu Wenfei,Zuo Zhiqiang*

Main category: cs.LG

TL;DR: 提出了一种无需奖励信号、主动引入人类参与的强化学习方法H-DSAC，结合PVP与DSAC框架，通过分布式代理价值函数编码人类意图，实现自动驾驶策略的安全、高效、鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在真实自动驾驶中面临安全性、样本效率和鲁棒性挑战，需引入人类专家知识以减少危险探索并提升学习效率。

Method: 提出H-DSAC方法，结合Proxy Value Propagation（PVP）和Distributional Soft Actor-Critic（DSAC），构建分布式的代理价值函数，利用专家示范赋予高回报，并对需人工干预的动作进行惩罚，进而将标签扩展至未标注状态以引导策略学习。

Result: 在仿真和真实驾驶环境中验证了该方法的有效性，实现了安全、鲁棒且样本高效的自动驾驶策略学习，并在合理训练时间内完成真实场景部署。

Conclusion: H-DSAC通过融合人类指导与分布强化学习，在无需显式奖励信号的情况下显著提升了自动驾驶策略学习的安全性与效率，具备实际应用潜力。

Abstract: Autonomous driving with reinforcement learning (RL) has significant
potential. However, applying RL in real-world settings remains challenging due
to the need for safe, efficient, and robust learning. Incorporating human
expertise into the learning process can help overcome these challenges by
reducing risky exploration and improving sample efficiency. In this work, we
propose a reward-free, active human-in-the-loop learning method called
Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines
Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to
enable efficient and safe training in real-world environments. The key
innovation is the construction of a distributed proxy value function within the
DSAC framework. This function encodes human intent by assigning higher expected
returns to expert demonstrations and penalizing actions that require human
intervention. By extrapolating these labels to unlabeled states, the policy is
effectively guided toward expert-like behavior. With a well-designed state
space, our method achieves real-world driving policy learning within practical
training times. Results from both simulation and real-world experiments
demonstrate that our framework enables safe, robust, and sample-efficient
learning for autonomous driving.

</details>


### [262] [BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining](https://arxiv.org/abs/2510.06048)
*Jie Hao,Rui Yu,Wei Zhang,Huixia Wang,Jie Xu,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为BLISS的轻量级数据选择方法，完全从零开始，不依赖外部预训练模型，并显式考虑所选数据的长期影响。BLISS使用小型代理模型和评分模型来估计训练样本的长期影响力，并通过双层优化框架优化数据权重，从而提升大语言模型预训练的效率和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法常依赖外部预训练模型，难以分离其影响，且忽视数据在模型收敛后的长期作用。同时，大规模预训练成本高昂，限制了对长期效果的评估。因此需要一种无需外部模型、能评估长期影响的轻量级数据选择方法。

Method: 提出BLISS方法，采用双层优化框架：上层优化评分模型以分配样本重要性权重，下层训练代理模型至收敛；利用评分模型预测数据集样本的影响分数，进而筛选高质量数据用于大模型预训练。整个过程无需外部预训练模型。

Result: 在C4数据集子集上验证BLISS，用于预训练410M/1B/2.8B规模的Pythia和LLaMA-0.5B模型。在1B模型设置下，相比最先进方法实现1.7倍加速达到相同性能，并在多个下游任务中表现更优。

Conclusion: BLISS是一种高效、无需外部模型的数据选择方法，能够准确估计数据的长期影响，在提升预训练效率的同时增强模型泛化能力，适用于大规模语言模型的预训练。

Abstract: Effective data selection is essential for pretraining large language models
(LLMs), enhancing efficiency and improving generalization to downstream tasks.
However, existing approaches often require leveraging external pretrained
models, making it difficult to disentangle the effects of data selection from
those of the external pretrained models. In addition, they often overlook the
long-term impact of selected data if the model is trained to convergence,
primarily due to the prohibitive cost of full-scale LLM pretraining. In this
paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence
\textbf{S}coring method for data \textbf{S}election): a lightweight data
selection method that operates entirely \emph{from scratch}, without relying on
any external pretrained oracle models, while explicitly accounting for the
long-term impact of selected data. BLISS leverages a small proxy model as a
surrogate for the LLM and employs a score model to estimate the long-term
influence of training samples if the proxy model is trained to convergence. We
formulate data selection as a bilevel optimization problem, where the
upper-level objective optimizes the score model to assign importance weights to
training samples, ensuring that minimizing the lower-level objective (i.e.,
training the proxy model over the weighted training loss until convergence)
leads to best validation performance. Once optimized, the trained score model
predicts influence scores for the dataset, enabling efficient selection of
high-quality samples for LLM pretraining. We validate BLISS by pretraining
410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4
dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$
speedup in reaching the same performance as the state-of-the-art method,
demonstrating superior performance across multiple downstream tasks.

</details>


### [263] [Edit-Based Flow Matching for Temporal Point Processes](https://arxiv.org/abs/2510.06050)
*David Lüdke,Marten Lienen,Marcel Kollovieh,Stephan Günnemann*

Main category: cs.LG

TL;DR: 提出了一种基于连续时间马尔可夫链的Edit Flow过程，通过插入、删除和替换操作建模时间点过程，提升了生成效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有时间点过程模型多为自回归结构，受限于顺序采样；非自回归扩散模型虽有所改进，但仍缺乏高效灵活的生成机制。

Method: 引入Edit Flow过程，在连续时间马尔可夫链框架下学习瞬时编辑速率，通过插入、删除和替换操作实现噪声到数据的转换。

Result: 所提方法减少了生成所需的编辑操作总数，在多种条件与无条件生成任务中表现出优异的生成灵活性和性能。

Conclusion: Edit Flow为时间点过程提供了一种更高效、灵活的非自回归建模方式，显著提升生成效率。

Abstract: Temporal point processes (TPPs) are a fundamental tool for modeling event
sequences in continuous time, but most existing approaches rely on
autoregressive parameterizations that are limited by their sequential sampling.
Recent non-autoregressive, diffusion-style models mitigate these issues by
jointly interpolating between noise and data through event insertions and
deletions in a discrete Markov chain. In this work, we generalize this
perspective and introduce an Edit Flow process for TPPs that transports noise
to data via insert, delete, and substitute edit operations. By learning the
instantaneous edit rates within a continuous-time Markov chain framework, we
attain a flexible and efficient model that effectively reduces the total number
of necessary edit operations during generation. Empirical results demonstrate
the generative flexibility of our unconditionally trained model in a wide range
of unconditional and conditional generation tasks on benchmark TPPs.

</details>


### [264] [Analyzing the Effect of Embedding Norms and Singular Values to Oversmoothing in Graph Neural Networks](https://arxiv.org/abs/2510.06066)
*Dimitrios Kelesis,Dimitris Fotakis,Georgios Paliouras*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量方法MASED来量化图神经网络中的过平滑现象，通过理论分析揭示了节点嵌入范数和权重矩阵奇异值对过平滑的影响，并提出了G-Reg正则化方案以提升深层GNN的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了理解深度图神经网络中过平滑现象的根本原因，并找到有效的方法来缓解该问题，从而提升深层GNN的性能。

Method: 引入新的度量指标MASED，推导其逐层边界以形成全局上下界，分析节点嵌入范数与权重矩阵奇异值的作用，并提出G-Reg正则化方法来解耦跳跃次数与权重矩阵数量。

Result: 理论和实验表明，过平滑随权重矩阵和邻接矩阵数量增加而加剧；使用G-Reg可提高分类准确率并在深层网络中实现更好性能，尤其在‘冷启动’场景下表现更优。

Conclusion: 通过MASED度量和G-Reg正则化，能够有效控制过平滑，平衡感受野大小与模型性能，使深层GNN优于浅层模型。

Abstract: In this paper, we study the factors that contribute to the effect of
oversmoothing in deep Graph Neural Networks (GNNs). Specifically, our analysis
is based on a new metric (Mean Average Squared Distance - $MASED$) to quantify
the extent of oversmoothing. We derive layer-wise bounds on $MASED$, which
aggregate to yield global upper and lower distance bounds. Based on this
quantification of oversmoothing, we further analyze the importance of two
different properties of the model; namely the norms of the generated node
embeddings, along with the largest and smallest singular values of the weight
matrices. Building on the insights drawn from the theoretical analysis, we show
that oversmoothing increases as the number of trainable weight matrices and the
number of adjacency matrices increases. We also use the derived layer-wise
bounds on $MASED$ to form a proposal for decoupling the number of hops (i.e.,
adjacency depth) from the number of weight matrices. In particular, we
introduce G-Reg, a regularization scheme that increases the bounds, and
demonstrate through extensive experiments that by doing so node classification
accuracy increases, achieving robustness at large depths. We further show that
by reducing oversmoothing in deep networks, we can achieve better results in
some tasks than using shallow ones. Specifically, we experiment with a ``cold
start" scenario, i.e., when there is no feature information for the unlabeled
nodes. Finally, we show empirically the trade-off between receptive field size
(i.e., number of weight matrices) and performance, using the $MASED$ bounds.
This is achieved by distributing adjacency hops across a small number of
trainable layers, avoiding the extremes of under- or over-parameterization of
the GNN.

</details>


### [265] [Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks](https://arxiv.org/abs/2510.06071)
*João Palmeiro,Diogo Duarte,Rita Costa,Pedro Bizarro*

Main category: cs.LG

TL;DR: 本文介绍了一个包含18000多个散点图的合成数据集和基准，用于评估AI模型在散点图相关任务中的表现，发现OpenAI和Google的模型在聚类计数和异常值识别上表现良好，但在定位任务上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准很少针对散点图特定任务进行评估，限制了对AI模型在此类数据可视化任务中性能的理解。

Method: 构建了一个包含六种数据生成器和17种图表设计的合成标注数据集，并基于该数据集建立基准，使用N-shot提示方法评估OpenAI和Google的专有模型在五个任务上的表现。

Result: OpenAI模型和Gemini 2.5 Flash在聚类计数和异常值检测任务中准确率超过90%，但在与定位相关的任务中精确率和召回率大多低于50%，其中Flash在异常值识别中达到65.01%。图表设计对性能影响较小，但应避免宽高比过大或颜色随机的散点图。

Conclusion: 当前AI模型可有效支持散点图中的基本分析任务如计数，但在精确定位方面仍存在挑战，未来需进一步优化模型对视觉布局的理解能力。

Abstract: AI models are increasingly used for data analysis and visualization, yet
benchmarks rarely address scatterplot-specific tasks, limiting insight into
performance. To address this gap for one of the most common chart types, we
introduce a synthetic, annotated dataset of over 18,000 scatterplots from six
data generators and 17 chart designs, and a benchmark based on it. We evaluate
proprietary models from OpenAI and Google using N-shot prompting on five
distinct tasks derived from annotations of cluster bounding boxes, their center
coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash,
especially when prompted with examples, are viable options for counting
clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results
for localization-related tasks are unsatisfactory: Precision and Recall are
near or below 50%, except for Flash in outlier identification (65.01%).
Furthermore, the impact of chart design on performance appears to be a
secondary factor, but it is advisable to avoid scatterplots with wide aspect
ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are
available at https://github.com/feedzai/biy-paper.

</details>


### [266] [Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method](https://arxiv.org/abs/2510.06091)
*Lulu Gong,Shreya Saxena*

Main category: cs.LG

TL;DR: 提出了一种结合张量方法和EM算法的Tensor-EM方法，用于学习混合线性动力系统（MoLDS），在合成和真实神经数据上验证了其可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MoLDS方法在噪声和复杂场景下性能受限，张量方法缺乏鲁棒性，EM方法易陷入局部极小，需一种兼具全局可识别性和精细拟合能力的新方法。

Method: 构建基于输入-输出数据的矩张量以获得全局一致的参数初值，再通过卡尔曼EM算法进行闭式更新，结合张量方法的可识别性与EM的拟合灵活性。

Result: 在合成数据上表现更稳健且恢复更可靠；在灵长类体感皮层和连续 reaching 任务的神经数据中，能正确聚类不同条件并建模为独立子系统，结果与监督单LDS拟合一致。

Conclusion: MoLDS是建模复杂神经数据的有效框架，而Tensor-EM是一种可靠的学习方法，兼具初始化鲁棒性与参数精度。

Abstract: Mixtures of linear dynamical systems (MoLDS) provide a path to model
time-series data that exhibit diverse temporal dynamics across trajectories.
However, its application remains challenging in complex and noisy settings,
limiting its effectiveness for neural data analysis. Tensor-based moment
methods can provide global identifiability guarantees for MoLDS, but their
performance degrades under noise and complexity. Commonly used
expectation-maximization (EM) methods offer flexibility in fitting latent
models but are highly sensitive to initialization and prone to poor local
minima. Here, we propose a tensor-based method that provides identifiability
guarantees for learning MoLDS, which is followed by EM updates to combine the
strengths of both approaches. The novelty in our approach lies in the
construction of moment tensors using the input-output data to recover globally
consistent estimates of mixture weights and system parameters. These estimates
can then be refined through a Kalman EM algorithm, with closed-form updates for
all LDS parameters. We validate our framework on synthetic benchmarks and
real-world datasets. On synthetic data, the proposed Tensor-EM method achieves
more reliable recovery and improved robustness compared to either pure tensor
or randomly initialized EM methods. We then analyze neural recordings from the
primate somatosensory cortex while a non-human primate performs reaches in
different directions. Our method successfully models and clusters different
conditions as separate subsystems, consistent with supervised single-LDS fits
for each condition. Finally, we apply this approach to another neural dataset
where monkeys perform a sequential reaching task. These results demonstrate
that MoLDS provides an effective framework for modeling complex neural data,
and that Tensor-EM is a reliable approach to MoLDS learning for these
applications.

</details>


### [267] [Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL](https://arxiv.org/abs/2510.06092)
*Nyal Patel,Matthieu Bou,Arjun Jagota,Satyapriya Krishna,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 本文提出了一种新的“基于失败感知的逆强化学习”（failure-aware IRL）算法，通过关注人类偏好数据中被奖励模型错误分类或难以判断的样本，更准确地提取大语言模型在强化学习对齐过程中隐含的奖励信号，在无需外部监督的情况下显著提升了模型去毒化等任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的逆强化学习方法在提取大语言模型对齐过程中隐含的奖励信号时，通常平等对待所有偏好数据，忽略了最具信息量的错误或模糊样本，导致奖励函数不够准确，影响可解释性和安全性。

Method: 提出一种失败感知的逆强化学习算法，重点利用被当前奖励模型误分类或评分接近的困难样本来优化奖励函数的学习过程，从而更精确地还原驱动模型行为的潜在奖励结构。

Result: 在多个评估指标下，该方法在大语言模型去毒化任务中优于现有的IRL基线方法，且无需外部分类器或额外标注；提取出的奖励函数更能反映真实对齐目标，并能支持更有效的再对齐训练（re-RLHF）。

Conclusion: 失败感知的IRL是一种强大且可扩展的方法，能够提升奖励建模的准确性，增强模型对齐的可审计性，减少逆强化学习过程中的不确定性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with human preferences, yet the underlying reward signals they
internalize remain hidden, posing a critical challenge for interpretability and
safety. Existing approaches attempt to extract these latent incentives using
Inverse Reinforcement Learning (IRL), but treat all preference pairs equally,
often overlooking the most informative signals: those examples the extracted
reward model misclassifies or assigns nearly equal scores, which we term
\emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that
focuses on misclassified or difficult examples to recover the latent rewards
defining model behaviors. By learning from these failures, our failure-aware
IRL extracts reward functions that better reflect the true objectives behind
RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines
across multiple metrics when applied to LLM detoxification, without requiring
external classifiers or supervision. Crucially, failure-aware IRL yields
rewards that better capture the true incentives learned during RLHF, enabling
more effective re-RLHF training than standard IRL. This establishes
failure-aware IRL as a robust, scalable method for auditing model alignment and
reducing ambiguity in the IRL process.

</details>


### [268] [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](https://arxiv.org/abs/2510.06096)
*Matthieu Bou,Nyal Patel,Arjun Jagota,Satyapriya Krishna,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯逆向强化学习的大型语言模型目标审计框架，能够量化目标推断中的不确定性、减少非唯一性，并提供可操作的诊断工具以验证模型对齐性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）隐式优化的目标缺乏透明度，导致对其对齐性和安全性的审计极为困难。现有的逆向强化学习方法无法充分处理奖励函数推断中的不确定性与非唯一性问题，因此需要一个更可靠、可解释的审计框架。

Method: 提出一种基于贝叶斯逆向强化学习（Bayesian IRL）的审计框架，通过建模奖励函数的后验分布来反映推断不确定性；利用多轮证据实现后验收缩以减少非唯一性；结合不确定性感知诊断识别异常行为和分布外提示；并通过在RLHF中使用推断出的奖励函数来验证其策略层面的有效性。

Result: 该框架成功应用于一个去毒化的LLM审计任务中，展现出良好的校准性和可解释性；实现了与真实对齐过程相当的训练动态和毒性降低效果；提供了对模型目标的信任保证，并支持实际的安全审计需求。

Conclusion: 本工作为LLM的目标推断提供了一个原则性强、实用性高的审计框架，使对模型真实目标的验证成为可能，推动了可信赖与可问责AI的发展。

Abstract: The objectives that Large Language Models (LLMs) implicitly optimize remain
dangerously opaque, making trustworthy alignment and auditing a grand
challenge. While Inverse Reinforcement Learning (IRL) can infer reward
functions from behaviour, existing approaches either produce a single,
overconfident reward estimate or fail to address the fundamental ambiguity of
the task (non-identifiability). This paper introduces a principled auditing
framework that re-frames reward inference from a simple estimation task to a
comprehensive process for verification. Our framework leverages Bayesian IRL to
not only recover a distribution over objectives but to enable three critical
audit capabilities: (i) Quantifying and systematically reducing
non-identifiability by demonstrating posterior contraction over sequential
rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics
that expose spurious shortcuts and identify out-of-distribution prompts where
the inferred objective cannot be trusted; and (iii) Validating policy-level
utility by showing that the refined, low-uncertainty reward can be used
directly in RLHF to achieve training dynamics and toxicity reductions
comparable to the ground-truth alignment process. Empirically, our framework
successfully audits a detoxified LLM, yielding a well-calibrated and
interpretable objective that strengthens alignment guarantees. Overall, this
work provides a practical toolkit for auditors, safety teams, and regulators to
verify what LLMs are truly trying to achieve, moving us toward more trustworthy
and accountable AI.

</details>


### [269] [The Physics of Data and Tasks: Theories of Locality and Compositionality in Deep Learning](https://arxiv.org/abs/2510.06106)
*Alessandro Favero*

Main category: cs.LG

TL;DR: 本文研究了深度神经网络在高维任务中学习的潜在结构，探讨了数据的局部性和组合性如何影响模型的学习和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络取得了显著成功，但其学习机制仍不清楚。尤其是在高维任务中，由于维度灾难，学习本应是统计上不可行的，然而模型依然有效，这表明可学习的数据存在潜在结构。

Method: 通过研究数据、任务以及深度学习表示中的局部性和组合性的作用，分析神经网络如何编码和利用这些结构特征。

Result: 揭示了局部性和组合性在提升模型性能中的关键作用，并量化了训练样本数量对泛化能力的影响。

Conclusion: 神经网络能够有效利用数据中的潜在结构（如局部性和组合性）来克服维度灾难，从而实现良好的泛化性能。

Abstract: Deep neural networks have achieved remarkable success, yet our understanding
of how they learn remains limited. These models can learn high-dimensional
tasks, which is generally statistically intractable due to the curse of
dimensionality. This apparent paradox suggests that learnable data must have an
underlying latent structure. What is the nature of this structure? How do
neural networks encode and exploit it, and how does it quantitatively impact
performance - for instance, how does generalization improve with the number of
training examples? This thesis addresses these questions by studying the roles
of locality and compositionality in data, tasks, and deep learning
representations.

</details>


### [270] [Influence Functions for Efficient Data Selection in Reasoning](https://arxiv.org/abs/2510.06108)
*Prateek Humane,Paolo Cudrano,Daniel Z. Kaplan,Matteo Matteucci,Supriyo Chakraborty,Irina Rish*

Main category: cs.LG

TL;DR: 提出使用影响函数定义推理数据质量，并引入基于影响的剪枝方法，在数学推理任务中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有推理数据质量定义不明确，依赖间接启发式方法，缺乏对数据选择的因果效应分析。

Method: 利用影响函数衡量单个思维链样本对下游准确率的因果影响，提出基于影响的剪枝策略。

Result: 在模型族内数学推理任务上，影响-based剪枝持续优于基于困惑度和嵌入的基线方法。

Conclusion: 影响函数可有效定义推理数据质量，为高质量推理数据筛选提供了新思路。

Abstract: Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows
that a small amount of high-quality data can outperform massive datasets. Yet,
what constitutes "quality" remains ill-defined. Existing reasoning methods rely
on indirect heuristics such as problem difficulty or trace length, while
instruction-tuning has explored a broader range of automated selection
strategies, but rarely in the context of reasoning. We propose to define
reasoning data quality using influence functions, which measure the causal
effect of individual CoT examples on downstream accuracy, and introduce
influence-based pruning, which consistently outperforms perplexity and
embedding-based baselines on math reasoning within a model family.

</details>


### [271] [PolyGraph Discrepancy: a classifier-based metric for graph generation](https://arxiv.org/abs/2510.06122)
*Markus Krimmel,Philip Hartout,Karsten Borgwardt,Dexiong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为PolyGraph Discrepancy (PGD)的新框架，用于评估图生成模型，通过分类器逼近真实与生成图分布间的Jensen-Shannon距离，提供可比较且鲁棒的度量。


<details>
  <summary>Details</summary>
Motivation: 现有基于MMD的图生成模型评估方法缺乏绝对性能度量，且对核和描述子参数敏感，难以跨不同图描述子进行比较。

Method: 使用二分类器区分真实图和生成图，基于图描述子进行特征化，并利用分类器的数据对数似然来近似JS距离的变分下界；进一步设计了一个理论支持的综合指标。

Result: PGD指标值位于[0,1]区间内，具有跨描述子可比性，实验表明其比MMD更稳健、更具洞察力。

Conclusion: PGD为图生成模型提供了更可靠、可解释且可比较的评估方法，相关框架已开源。

Abstract: Existing methods for evaluating graph generative models primarily rely on
Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these
metrics can rank generative models, they do not provide an absolute measure of
performance. Their values are also highly sensitive to extrinsic parameters,
namely kernel and descriptor parametrization, making them incomparable across
different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new
evaluation framework that addresses these limitations. It approximates the
Jensen-Shannon distance of graph distributions by fitting binary classifiers to
distinguish between real and generated graphs, featurized by these descriptors.
The data log-likelihood of these classifiers approximates a variational lower
bound on the JS distance between the two distributions. Resulting metrics are
constrained to the unit interval [0,1] and are comparable across different
graph descriptors. We further derive a theoretically grounded summary metric
that combines these individual metrics to provide a maximally tight lower bound
on the distance for the given descriptors. Thorough experiments demonstrate
that PGD provides a more robust and insightful evaluation compared to MMD
metrics. The PolyGraph framework for benchmarking graph generative models is
made publicly available at https://github.com/BorgwardtLab/polygraph-benchmark.

</details>


### [272] [Downsized and Compromised?: Assessing the Faithfulness of Model Compression](https://arxiv.org/abs/2510.06125)
*Moumita Kamal,Douglas A. Talbert*

Main category: cs.LG

TL;DR: 本文提出了一种评估压缩模型保真度的新方法，通过引入模型一致性与卡方检验等指标，揭示了传统精度指标无法发现的预测行为变化，确保模型压缩不损害AI系统的公平性与可信度。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融和司法等高风险领域，模型压缩不仅要保持准确性，还需忠实还原原模型的行为。然而现有评估方法忽视了保真度问题，仅关注大小与准确率的权衡，缺乏对行为一致性和公平性的深入分析。

Method: 提出一组新的保真度评估指标，包括模型间预测一致性度量，并使用卡方检验检测整体数据及人口子群中预测模式的显著变化，以识别聚合公平性指标可能掩盖的行为偏移。

Result: 在三个社会意义重大的数据集上对人工神经网络应用量化和剪枝后发现，高准确率并不保证高保真度，所提统计方法能检测到标准指标（如准确率、均等机会）忽略的细微但显著的预测行为变化。

Conclusion: 所提出的保真度评估方法为模型压缩提供了更直接、实用的评价手段，有助于确保效率提升的同时不牺牲AI系统的公平性与可信性。

Abstract: In real-world applications, computational constraints often require
transforming large models into smaller, more efficient versions through model
compression. While these techniques aim to reduce size and computational cost
without sacrificing performance, their evaluations have traditionally focused
on the trade-off between size and accuracy, overlooking the aspect of model
faithfulness. This limited view is insufficient for high-stakes domains like
healthcare, finance, and criminal justice, where compressed models must remain
faithful to the behavior of their original counterparts. This paper presents a
novel approach to evaluating faithfulness in compressed models, moving beyond
standard metrics. We introduce and demonstrate a set of faithfulness metrics
that capture how model behavior changes post-compression. Our contributions
include introducing techniques to assess predictive consistency between the
original and compressed models using model agreement, and applying chi-squared
tests to detect statistically significant changes in predictive patterns across
both the overall dataset and demographic subgroups, thereby exposing shifts
that aggregate fairness metrics may obscure. We demonstrate our approaches by
applying quantization and pruning to artificial neural networks (ANNs) trained
on three diverse and socially meaningful datasets. Our findings show that high
accuracy does not guarantee faithfulness, and our statistical tests detect
subtle yet significant shifts that are missed by standard metrics, such as
Accuracy and Equalized Odds. The proposed metrics provide a practical and more
direct method for ensuring that efficiency gains through compression do not
compromise the fairness or faithfulness essential for trustworthy AI.

</details>


### [273] [lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models](https://arxiv.org/abs/2510.06126)
*Haoxin Wang,Xiaolong Tu,Hongyu Ke,Huirong Chai,Dawei Chen,Kyungtae Han*

Main category: cs.LG

TL;DR: 提出首个轻量级、在线的面向终端设备大模型推理的延迟分析工具lm-Meter，实现在移动设备上细粒度、低开销的实时性能剖析，并揭示了终端LLM推理中的瓶颈与优化机会。


<details>
  <summary>Details</summary>
Motivation: 由于大模型在云端部署引发数据隐私和可持续性问题，终端侧运行LLM成为趋势，但受限于资源和缺乏对性能与效率权衡的可见性，亟需有效的性能分析工具。

Method: 设计并实现lm-Meter，一种无需辅助设备即可在商用移动平台上进行相位级（如prefill、decode）和内核级细粒度在线延迟分析的轻量级工具，具备低系统开销。

Result: 实验表明lm-Meter具有高精度且系统开销极低（如最严格的Powersave模式下prefill吞吐仅下降2.58%，decode下降0.99%），并通过实证研究揭示了各阶段和内核层面的性能瓶颈及准确率-效率权衡。

Conclusion: lm-Meter为终端大模型推理提供了前所未有的运行时行为可见性，有助于指导优化，推动终端LLM系统的普及。

Abstract: Large Language Models (LLMs) are increasingly integrated into everyday
applications, but their prevalent cloud-based deployment raises growing
concerns around data privacy and long-term sustainability. Running LLMs locally
on mobile and edge devices (on-device LLMs) offers the promise of enhanced
privacy, reliability, and reduced communication costs. However, realizing this
vision remains challenging due to substantial memory and compute demands, as
well as limited visibility into performance-efficiency trade-offs on
resource-constrained hardware. We propose lm-Meter, the first lightweight,
online latency profiler tailored for on-device LLM inference. lm-Meter captures
fine-grained, real-time latency at both phase (e.g., embedding, prefill,
decode, softmax, sampling) and kernel levels without auxiliary devices. We
implement lm-Meter on commercial mobile platforms and demonstrate its high
profiling accuracy with minimal system overhead, e.g., only 2.58% throughput
reduction in prefill and 0.99% in decode under the most constrained Powersave
governor. Leveraging lm-Meter, we conduct comprehensive empirical studies
revealing phase- and kernel-level bottlenecks in on-device LLM inference,
quantifying accuracy-efficiency trade-offs, and identifying systematic
optimization opportunities. lm-Meter provides unprecedented visibility into the
runtime behavior of LLMs on constrained platforms, laying the foundation for
informed optimization and accelerating the democratization of on-device LLM
systems. Code and tutorials are available at
https://github.com/amai-gsu/LM-Meter.

</details>


### [274] [Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks](https://arxiv.org/abs/2510.06138)
*Rushiv Arora*

Main category: cs.LG

TL;DR: 提出了一种基于语言条件的多策略混合架构LEXPOL，用于多任务强化学习，能够通过自然语言任务描述有效组合可重用技能，在MetaWorld基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习通常依赖任务元数据（如自然语言描述）来引导不同目标下的行为，但如何有效利用这些语言信息来组合和调度已有技能仍具挑战。

Method: 提出Lexical Policy Networks (LEXPOL)，使用文本编码器处理任务描述，并通过可学习的门控模块选择或混合多个子策略，实现端到端的多任务训练。

Result: 在MetaWorld基准上，LEXPOL在成功率和样本效率方面达到或超过强基线方法，且无需任务特定重训练；分析表明其语言门控模块能有效组合预定义专家策略以应对新任务描述和未见任务组合。

Conclusion: 自然语言元数据可有效作为索引机制，在单一策略中动态重组可重用技能，提升多任务强化学习的泛化与效率。

Abstract: Multi-task reinforcement learning often relies on task metadata -- such as
brief natural-language descriptions -- to guide behavior across diverse
objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned
mixture-of-policies architecture for multi-task RL. LEXPOL encodes task
metadata with a text encoder and uses a learned gating module to select or
blend among multiple sub-policies, enabling end-to-end training across tasks.
On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines
in success rate and sample efficiency, without task-specific retraining. To
analyze the mechanism, we further study settings with fixed expert policies
obtained independently of the gate and show that the learned language gate
composes these experts to produce behaviors appropriate to novel task
descriptions and unseen task combinations. These results indicate that
natural-language metadata can effectively index and recombine reusable skills
within a single policy.

</details>


### [275] [LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams](https://arxiv.org/abs/2510.06151)
*Aju Ani Justus,Chris Baber*

Main category: cs.LG

TL;DR: 该论文提出使用大语言模型（LLMs）作为策略无关的人类代理，生成模拟人类决策的合成数据，以解决异构智能体团队中与不可见或非平稳策略队友协作的建模难题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵的人在环数据，限制了可扩展性；需要一种更高效的方式来训练智能体与人类类似的行为协作。

Method: 在受Stag Hunt启发的网格世界捕获游戏中，利用LLaMA 3.1和Mixtral 8x22B等大语言模型，基于游戏状态和奖励结构生成决策，并通过调整提示引导风险敏感策略，评估其与人类行为的一致性。

Result: 实验表明，LLM在决策上比人类参与者更接近专家判断，能通过提示调节风险偏好，并生成类似人类路径的行动轨迹。

Conclusion: 尽管LLM尚不能完全复制人类的适应能力，但其通过提示控制的多样性为模拟策略无关的队友提供了可扩展的基础。

Abstract: A critical challenge in modelling Heterogeneous-Agent Teams is training
agents to collaborate with teammates whose policies are inaccessible or
non-stationary, such as humans. Traditional approaches rely on expensive
human-in-the-loop data, which limits scalability. We propose using Large
Language Models (LLMs) as policy-agnostic human proxies to generate synthetic
data that mimics human decision-making. To evaluate this, we conduct three
experiments in a grid-world capture game inspired by Stag Hunt, a game theory
paradigm that balances risk and reward. In Experiment 1, we compare decisions
from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and
Mixtral 8x22B models. LLMs, prompted with game-state observations and reward
structures, align more closely with experts than participants, demonstrating
consistency in applying underlying decision criteria. Experiment 2 modifies
prompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLM
outputs mirror human participants' variability, shifting between risk-averse
and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic
grid-world where the LLM agents generate movement actions. LLMs produce
trajectories resembling human participants' paths. While LLMs cannot yet fully
replicate human adaptability, their prompt-guided diversity offers a scalable
foundation for simulating policy-agnostic teammates.

</details>


### [276] [TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts](https://arxiv.org/abs/2510.06162)
*Christopher Kolberg,Katharina Eggensperger,Nico Pfeifer*

Main category: cs.LG

TL;DR: 提出TabPFN-Wide，通过在合成数据上继续预训练，扩展基础模型以处理高维生物医学数据，保持可解释性并提升噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型难以处理高维特征（>500），而降维会损害特征重要性分析，限制了在生物医学中的应用。

Method: 通过构建自定义先验分布生成合成数据，对现有模型进行持续预训练，从而扩展其处理高维特征的能力。

Result: TabPFN-Wide可在超过50,000个特征下稳定运行，性能优于或等于原模型，具有更强的噪声鲁棒性，并能识别出与已知生物学发现一致的重要特征。

Conclusion: 先验信息引导的适应策略可有效增强基础模型在高维生物医学数据上的能力，同时保持模型可解释性。

Abstract: Revealing novel insights from the relationship between molecular measurements
and pathology remains a very impactful application of machine learning in
biomedicine. Data in this domain typically contain only a few observations but
thousands of potentially noisy features, posing challenges for conventional
machine learning approaches. While prior-data fitted networks emerge as
foundation models for tabular data, they are currently not suited to handle
large feature counts (>500). Although feature reduction enables their
application, it hinders feature importance analysis. We propose a strategy that
extends existing models through continued pre-training on synthetic data
sampled from a customized prior. The resulting model, TabPFN-Wide, matches or
exceeds its base model's performance while exhibiting improved robustness to
noise. It seamlessly scales beyond 50,000 features, regardless of noise levels,
while maintaining inherent interpretability, which is critical for biomedical
applications. Our results show that prior-informed adaptation is suitable to
enhance the capability of foundation models for high-dimensional data. On
real-world biomedical datasets many of the most relevant features identified by
the model overlap with previous biological findings, while others propose
potential starting points for future studies.

</details>


### [277] [Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing](https://arxiv.org/abs/2510.06165)
*Kurt Butler,Guanchao Feng,Petar Djuric*

Main category: cs.LG

TL;DR: 本文提出了一种基于积分梯度（IG）的高阶特征归因通用理论，用于解释机器学习模型中输入特征及其交互作用对预测结果的影响。


<details>
  <summary>Details</summary>
Motivation: 当模型涉及特征间相互作用时，现有特征归因方法难以直观解释，因此需要发展能捕捉高阶交互的归因理论。

Method: 以积分梯度（Integrated Gradients）为基础，结合统计学与拓扑信号处理理论，构建高阶特征归因框架，并通过理论分析与实例验证其有效性。

Result: 建立了高阶特征归因的理论体系，揭示了IG与统计学及拓扑信号处理之间的自然联系，并在多个示例中验证了该理论的有效性。

Conclusion: 所提出的高阶特征归因理论扩展了现有的可解释AI框架，能够更准确地解释包含特征交互的复杂模型的预测机制。

Abstract: Feature attributions are post-training analysis methods that assess how
various input features of a machine learning model contribute to an output
prediction. Their interpretation is straightforward when features act
independently, but becomes less direct when the predictive model involves
interactions such as multiplicative relationships or joint feature
contributions. In this work, we propose a general theory of higher-order
feature attribution, which we develop on the foundation of Integrated Gradients
(IG). This work extends existing frameworks in the literature on explainable
AI. When using IG as the method of feature attribution, we discover natural
connections to statistics and topological signal processing. We provide several
theoretical results that establish the theory, and we validate our theory on a
few examples.

</details>


### [278] [Thermodynamic Performance Limits for Score-Based Diffusion Models](https://arxiv.org/abs/2510.06174)
*Nathan X. Kodama,Michael Hinczewski*

Main category: cs.LG

TL;DR: 本文建立了基于分数的扩散模型与非平衡热力学之间的基本联系，通过熵率推导出数据负对数似然的下界，并在合成数据集上验证了该界限的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解生成模型（特别是基于分数的扩散模型）在物理原理层面的性能限制，探索其与热力学系统之间的深层联系。

Method: 利用随机热力学中的熵率理论，推导出扩散过程中的系统、内在和交换熵，并建立负对数似然的下界；通过合成数据进行数值验证。

Result: 提出了一个基于熵率的性能下界，验证了其在实际扩散模型中的有效性，并揭示了模型运行过程中的热力学机制及其与麦克斯韦妖等概念的类比。

Conclusion: 生成建模的性能可以通过基本物理原理（如熵率）来解释和约束，为未来热力学计算硬件的设计提供了理论支持。

Abstract: We establish a fundamental connection between score-based diffusion models
and non-equilibrium thermodynamics by deriving performance limits based on
entropy rates. Our main theoretical contribution is a lower bound on the
negative log-likelihood of the data that relates model performance to entropy
rates of diffusion processes. We numerically validate this bound on a synthetic
dataset and investigate its tightness. By building a bridge to entropy rates -
system, intrinsic, and exchange entropy - we provide new insights into the
thermodynamic operation of these models, drawing parallels to Maxwell's demon
and implications for thermodynamic computing hardware. Our framework connects
generative modeling performance to fundamental physical principles through
stochastic thermodynamics.

</details>


### [279] [Conformalized Gaussian processes for online uncertainty quantification over graphs](https://arxiv.org/abs/2510.06181)
*Jinwen Xu,Qin Lu,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 提出了一种结合图感知随机特征高斯过程与在线共形预测的可扩展、自适应不确定性量化方法，以提高图数据上的覆盖性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯过程的方法在处理图结构数据时存在计算复杂度高和建模假设过强的问题，导致在动态标签到来时覆盖性能不佳。

Method: 利用随机特征核近似构建图感知的参数化高斯过程，并通过集成多个模型实现增量学习；结合在线共形预测框架，使用自适应阈值校准预测集。

Result: 实验结果表明，该方法相比基线方法在覆盖性和预测效率方面均有提升。

Conclusion: 所提出的方法在保证统计有效性的同时，实现了对图结构数据的高效、自适应不确定性量化。

Abstract: Uncertainty quantification (UQ) over graphs arises in a number of
safety-critical applications in network science. The Gaussian process (GP), as
a classical Bayesian framework for UQ, has been developed to handle
graph-structured data by devising topology-aware kernel functions. However,
such GP-based approaches are limited not only by the prohibitive computational
complexity, but also the strict modeling assumptions that might yield poor
coverage, especially with labels arriving on the fly. To effect scalability, we
devise a novel graph-aware parametric GP model by leveraging the random feature
(RF)-based kernel approximation, which is amenable to efficient recursive
Bayesian model updates. To further allow for adaptivity, an ensemble of
graph-aware RF-based scalable GPs have been leveraged, with per-GP weight
adapted to data arriving incrementally. To ensure valid coverage with
robustness to model mis-specification, we wed the GP-based set predictors with
the online conformal prediction framework, which post-processes the prediction
sets using adaptive thresholds. Experimental results the proposed method yields
improved coverage and efficient prediction sets over existing baselines by
adaptively ensembling the GP models and setting the key threshold parameters in
CP.

</details>


### [280] [On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond](https://arxiv.org/abs/2510.06190)
*Chenxiao Yang,Cai Zhou,David Wipf,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文在抽象层面研究了生成过程（如自回归和掩码扩散），并量化了其优缺点，指出超越自回归和现有掩码扩散的重写与变长编辑能力可带来显著的理论与实证优势。


<details>
  <summary>Details</summary>
Motivation: 希望理解生成模型在不同抽象层面的能力与局限，探索更强大、通用的生成机制以应对复杂任务。

Method: 通过形式化建模生成过程，基于计算难度和可学习性等可衡量标准进行理论分析与比较。

Result: 证明引入重写和变长编辑能力能显著提升生成过程的效率与灵活性，在理论和实验上均优于传统方法。

Conclusion: 扩展生成过程的能力（如支持编辑和长度变化）对前沿大模型在多领域（如编程与科学）的应用具有重要意义。

Abstract: This paper formally studies generation processes, including auto-regressive
next-token prediction and masked diffusion, that abstract beyond architectural
specifics. At this level of abstraction, we quantify their benefits and
limitations through measurable criteria such as computational hardness and
learnability. In particular, we demonstrate that allowing generation to proceed
beyond autoregression and current masked diffusion, with capabilities to
rewrite and length-variable edit, can bring significant theoretical and
empirical advantages, with important implications for frontier LLMs that aspire
to tackle increasingly hard problems and work universally across domains beyond
natural language, such as coding and science.

</details>


### [281] [Reference Grounded Skill Discovery](https://arxiv.org/abs/2510.06203)
*Seungeun Rho,Aaron Trinh,Danfei Xu,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出Reference-Grounded Skill Discovery (RGSD) 算法，通过参考数据在语义有意义的潜在空间中进行技能发现，有效应对高维系统中的探索挑战。


<details>
  <summary>Details</summary>
Motivation: 高自由度系统中，无监督技能发现面临探索空间指数增长而有意义技能流形有限的问题，需引入语义指导以提升探索效率。

Method: 采用对比预训练将动作嵌入单位超球面，并将参考轨迹聚类到不同方向，实现技能发现与参考行为模仿及语义相关多样行为发现的结合。

Result: 在359维观测、69维动作的SMPL人形模拟中，成功学习行走、奔跑、出拳等结构化技能，并发现新颖相关行为；下游控制任务表现优于基于模仿的基线方法。

Conclusion: 轻量级参考引导的技能发现为高自由度系统提供了一条实用路径，可有效发现语义丰富且结构化的技能。

Abstract: Scaling unsupervised skill discovery algorithms to high-DoF agents remains
challenging. As dimensionality increases, the exploration space grows
exponentially, while the manifold of meaningful skills remains limited.
Therefore, semantic meaningfulness becomes essential to effectively guide
exploration in high-dimensional spaces. In this work, we present
Reference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill
discovery in a semantically meaningful latent space using reference data. RGSD
first performs contrastive pretraining to embed motions on a unit hypersphere,
clustering each reference trajectory into a distinct direction. This grounding
enables skill discovery to simultaneously involve both imitation of reference
behaviors and the discovery of semantically related diverse behaviors. On a
simulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns
structured skills including walking, running, punching, and side stepping, and
also discovers related novel behaviors. In downstream control tasks, RGSD
outperforms imitation-based skill acquisition baselines. Our results suggest
that lightweight reference-guided grounding offers a practical path to
discovering semantically rich and structured skills in high-DoF systems.

</details>


### [282] [Training Dynamics Impact Post-Training Quantization Robustness](https://arxiv.org/abs/2510.06213)
*Albert Catalan-Tatjer,Niccolò Ajroldi,Jonas Geiping*

Main category: cs.LG

TL;DR: 本文研究了大语言模型训练动态与量化性能之间的关系，发现学习率衰减后验证损失和量化误差会分离，并指出通过调整训练超参数可以提升大规模下的量化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解量化鲁棒性的机制，并探究训练动态如何影响量化性能。

Method: 分析开源语言模型的训练轨迹，并通过控制实验训练模型以研究超参数对量化鲁棒性的影响。

Result: 发现学习率与其他超参数的相互作用主导量化误差，且数据规模不是决定量化效果的主要因素；特定超参数配置可改善大规模下的量化质量。

Conclusion: 增加数据规模并不必然损害量化效果，合理的训练超参数设计可有效提升量化鲁棒性。

Abstract: While post-training quantization is widely adopted for efficient deployment
of large language models, the mechanisms underlying quantization robustness
remain unclear. We conduct a comprehensive analysis of quantization degradation
across open-source language model training trajectories up to 32B parameters
and 15T training tokens to accurately assess the relationship between training
dynamics and quantization performance. Our key finding is that quantization
errors in large-scale training runs are driven by a complex interplay between
learning rate and other training hyperparameters. Specifically, once learning
rates decay, validation loss and quantization error diverge, largely
independent of training data scale. To investigate interventions on the
training dynamics and identify specific configurations that can modulate
quantization robustness favorably, we train our own models in controlled
experiments up to 100B tokens. Our results challenge the assumption that
increasing dataset scale inherently compromises quantization effectiveness,
demonstrating instead that strategic training hyperparameter interventions can
improve quantization quality at scale.

</details>


### [283] [Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents](https://arxiv.org/abs/2510.06214)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 提出Stratified GRPO方法，通过分层优势归一化（SAN）解决LLM搜索代理中因轨迹异质性导致的跨层偏差问题，显著提升训练效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法在处理结构异质的搜索轨迹时存在跨层偏差，影响信用分配和探索能力。

Method: 引入Stratified Advantage Normalization（SAN），根据轨迹结构特性将其划分为同质子群，并在每个子群内局部计算优势函数，结合全局估计器提高有限样本下的稳定性。

Result: 在多种单跳和多跳问答基准上，Stratified GRPO比GRPO最高提升11.3个百分点，具有更高的训练奖励、稳定性和搜索效率。

Conclusion: 分层优势归一化是解决LLM搜索代理中结构异质性问题的原理性方案，有效消除跨层偏差，提供更纯净稳定的训练信号。

Abstract: Large language model (LLM) agents increasingly rely on external tools such as
search engines to solve complex, multi-step problems, and reinforcement
learning (RL) has become a key paradigm for training them. However, the
trajectories of search agents are structurally heterogeneous, where variations
in the number, placement, and outcomes of search calls lead to fundamentally
different answer directions and reward distributions. Standard policy gradient
methods, which use a single global baseline, suffer from what we identify and
formalize as cross-stratum bias-an "apples-to-oranges" comparison of
heterogeneous trajectories. This cross-stratum bias distorts credit assignment
and hinders exploration of complex, multi-step search strategies. To address
this, we propose Stratified GRPO, whose central component, Stratified Advantage
Normalization (SAN), partitions trajectories into homogeneous strata based on
their structural properties and computes advantages locally within each
stratum. This ensures that trajectories are evaluated only against their true
peers. Our analysis proves that SAN eliminates cross-stratum bias, yields
conditionally unbiased unit-variance estimates inside each stratum, and retains
the global unbiasedness and unit-variance properties enjoyed by standard
normalization, resulting in a more pure and scale-stable learning signal. To
improve practical stability under finite-sample regimes, we further linearly
blend SAN with the global estimator. Extensive experiments on diverse
single-hop and multi-hop question-answering benchmarks demonstrate that
Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3
points, achieving higher training rewards, greater training stability, and more
effective search policies. These results establish stratification as a
principled remedy for structural heterogeneity in RL for LLM search agents.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [284] [VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing](https://arxiv.org/abs/2510.05213)
*Yixiao Wang,Mingxiao Huo,Zhixuan Liang,Yushi Du,Lingfeng Sun,Haotian Lin,Jinghuan Shang,Chensheng Peng,Mohit Bansal,Mingyu Ding,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出VER（Vision Expert transformer for Robot learning），通过蒸馏多个视觉基础模型构建专家库，并使用轻量级路由网络动态选择任务相关专家，实现跨任务的高效机器人学习。


<details>
  <summary>Details</summary>
Motivation: 单一视觉基础模型在特定领域表现优异但泛化能力有限，整合多模型优势以提升机器人任务中的视觉表征通用性。

Method: 构建视觉专家库，在预训练中蒸馏多个VFMs；微调时仅更新轻量路由网络（<0.4%参数）进行动态专家选择；引入Patchwise专家路由与课程Top-K退火策略提升精度和灵活性。

Result: 在17个不同机器人任务上达到SOTA性能；减少无关区域（如背景）的大范数异常值，聚焦任务关键区域；支持参数高效微调和可扩展的知识集成。

Conclusion: VER通过动态路由机制有效融合多VFM优势，在保持低计算开销的同时显著提升跨任务性能和鲁棒性，具备良好的实用性和扩展性。

Abstract: Pretrained vision foundation models (VFMs) advance robotic learning via rich
visual representations, yet individual VFMs typically excel only in specific
domains, limiting generality across tasks. Distilling multiple VFMs into a
unified representation for policy can mitigate this limitation but often yields
inflexible task-specific feature selection and requires costly full re-training
to incorporate robot-domain knowledge. We propose VER, a Vision Expert
transformer for Robot learning. During pretraining, VER distills multiple VFMs
into a vision expert library. It then fine-tunes only a lightweight routing
network (fewer than 0.4% of parameters) to dynamically select task-relevant
experts from the pretrained library for downstream robot tasks. We further
introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve
both flexibility and precision of dynamic expert selection. Moreover, VER
supports parameter-efficient finetuning for scalable expert utilization and
adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks
and multiple policy heads, VER achieves state-of-the-art performance. We find
that VER reduces large-norm outliers in task-irrelevant regions (e.g.,
background) and concentrates on task-critical regions. Visualizations and codes
can be found in https://yixiaowang7.github.io/ver_page/.

</details>


### [285] [Adaptive Dynamics Planning for Robot Navigation](https://arxiv.org/abs/2510.05330)
*Lu Yuanjie,Mao Mingyang,Xu Tong,Wang Linji,Lin Xiaomin,Xiao Xuesu*

Main category: cs.RO

TL;DR: 提出了一种自适应动力学规划（ADP）方法，通过强化学习动态调整机器人动力学属性，提升导航在复杂环境中的成功率、安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统分层规划中动力学连续性断裂导致在高度约束环境中轨迹跟踪失败，且现有方法采用静态动力学保真度降低策略，难以适应环境复杂性的变化。

Method: 提出ADP框架，利用强化学习动态调节动力学属性（如积分步长和碰撞检测分辨率），并将其集成到三种不同规划器中，同时设计了基于ADP的独立导航系统。

Result: 在仿真和真实环境实验中，ADP显著提升了导航成功率、安全性和效率，相较于基线方法表现更优。

Conclusion: ADP能够根据环境复杂性自适应调整规划过程中的动力学保真度，有效平衡计算效率与动力学可行性，适用于多样化环境下的自主导航。

Abstract: Autonomous robot navigation systems often rely on hierarchical planning,
where global planners compute collision-free paths without considering
dynamics, and local planners enforce dynamics constraints to produce executable
commands. This discontinuity in dynamics often leads to trajectory tracking
failure in highly constrained environments. Recent approaches integrate
dynamics within the entire planning process by gradually decreasing its
fidelity, e.g., increasing integration steps and reducing collision checking
resolution, for real-time planning efficiency. However, they assume that the
fidelity of the dynamics should decrease according to a manually designed
scheme. Such static settings fail to adapt to environmental complexity
variations, resulting in computational overhead in simple environments or
insufficient dynamics consideration in obstacle-rich scenarios. To overcome
this limitation, we propose Adaptive Dynamics Planning (ADP), a
learning-augmented paradigm that uses reinforcement learning to dynamically
adjust robot dynamics properties, enabling planners to adapt across diverse
environments. We integrate ADP into three different planners and further design
a standalone ADP-based navigation system, benchmarking them against other
baselines. Experiments in both simulation and real-world tests show that ADP
consistently improves navigation success, safety, and efficiency.

</details>


### [286] [A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation](https://arxiv.org/abs/2510.05382)
*Zhuowei Xu,Zilin Si,Kevin Zhang,Oliver Kroemer,Zeynep Temel*

Main category: cs.RO

TL;DR: 提出一种低成本、易制造、紧凑且可适应的机器人手指尖设计，集成了多模态触觉传感器（应变片和接触式麦克风），可在不同操作阶段结合视觉或单独使用触觉感知提升操作性能。


<details>
  <summary>Details</summary>
Motivation: 触觉感知虽能提升机器人操作的精度与灵活性，但因传感器成本高、制造集成困难及信号提取不可靠等问题，其在机器人手中的应用仍受限。

Method: 采用应变片传感器测量静态力，接触式麦克风传感器检测接触时的高频振动，将多种触觉传感器集成于紧凑型指尖结构中，所有传感器内置于指尖以避免磨损。

Result: 应变片在0-5N范围内提供可重复的二维平面力测量，接触式麦克风可区分不同接触材料；在从无到完全遮挡视觉的操作任务中验证了设计有效性，如仅凭触觉以100%成功率精确计数并拆分纸杯堆叠。

Conclusion: 该设计实现了低成本、高可靠性与多模态感知能力，能够在不同程度视觉遮挡下灵活融合触觉信息，显著提升机器人操作性能。

Abstract: Tactile sensing holds great promise for enhancing manipulation precision and
versatility, but its adoption in robotic hands remains limited due to high
sensor costs, manufacturing and integration challenges, and difficulties in
extracting expressive and reliable information from signals. In this work, we
present a low-cost, easy-to-make, adaptable, and compact fingertip design for
robotic hands that integrates multi-modal tactile sensors. We use strain gauge
sensors to capture static forces and a contact microphone sensor to measure
high-frequency vibrations during contact. These tactile sensors are integrated
into a compact design with a minimal sensor footprint, and all sensors are
internal to the fingertip and therefore not susceptible to direct wear and tear
from interactions. From sensor characterization, we show that strain gauge
sensors provide repeatable 2D planar force measurements in the 0-5 N range and
the contact microphone sensor has the capability to distinguish contact
material properties. We apply our design to three dexterous manipulation tasks
that range from zero to full visual occlusion. Given the expressiveness and
reliability of tactile sensor readings, we show that different tactile sensing
modalities can be used flexibly in different stages of manipulation, solely or
together with visual observations to achieve improved task performance. For
instance, we can precisely count and unstack a desired number of paper cups
from a stack with 100\% success rate which is hard to achieve with vision only.

</details>


### [287] [Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios](https://arxiv.org/abs/2510.05425)
*Marta Lagomarsino,Francesco Tassi*

Main category: cs.RO

TL;DR: 提出一种自适应人机交互在线框架，通过整合用户上肢活动能力受限的模型，使机器人能够根据用户的残余功能灵活性进行个性化响应，促进有上肢残疾的个体在工作中的积极参与。


<details>
  <summary>Details</summary>
Motivation: 改善工作环境中对上肢残疾人士的包容性不足问题，支持其主动参与工作。

Method: 将针对特定关节限制的活动能力模型集成到分层最优控制器中，实现机器人在线生成适应用户活动能力的行为，并引导用户利用残余功能活动。

Result: 在模拟肘关节、肩关节炎和腕部固定等不同上肢活动障碍的手递任务中测试，框架能根据用户活动范围和个人功能限制程度实现个性化交互，并鼓励关节使用。

Conclusion: 该框架能有效适应用户上肢活动障碍，提升人机协作的可用性和包容性，具有应用于实际工作环境的潜力。

Abstract: Work environments are often inadequate and lack inclusivity for individuals
with upper-body disabilities. This paper presents a novel online framework for
adaptive human-robot interaction (HRI) that accommodates users' arm mobility
impairments, ultimately aiming to promote active work participation. Unlike
traditional human-robot collaboration approaches that assume able-bodied users,
our method integrates a mobility model for specific joint limitations into a
hierarchical optimal controller. This allows the robot to generate reactive,
mobility-aware behaviour online and guides the user's impaired limb to exploit
residual functional mobility. The framework was tested in handover tasks
involving different upper-limb mobility impairments (i.e., emulated elbow and
shoulder arthritis, and wrist blockage), under both standing and seated
configurations with task constraints using a mobile manipulator, and
complemented by quantitative and qualitative comparisons with state-of-the-art
ergonomic HRI approaches. Preliminary results indicated that the framework can
personalise the interaction to fit within the user's impaired range of motion
and encourage joint usage based on the severity of their functional
limitations.

</details>


### [288] [Active Semantic Perception](https://arxiv.org/abs/2510.05430)
*Huayi Tang,Pratik Chaudhari*

Main category: cs.RO

TL;DR: 提出一种基于语义的主动感知方法，利用场景语义进行探索等任务，通过LLM生成未观测区域的合理场景图，并计算信息增益以实现高效的空间推理。


<details>
  <summary>Details</summary>
Motivation: 传统探索方法缺乏对场景语义的利用，难以进行高层次空间推理，因此需要结合语义信息提升感知效率与准确性。

Method: 构建分层多层场景图表示复杂室内环境，结合大语言模型（LLM）推测未观测区域的可能结构，并基于信息增益评估候选路径点。

Result: 在仿真复杂3D室内环境中验证，相比基线方法能更快速、准确地推断环境语义。

Conclusion: 该方法有效整合语义与主动感知，提升了探索过程中的空间推理能力与语义理解速度。

Abstract: We develop an approach for active semantic perception which refers to using
the semantics of the scene for tasks such as exploration. We build a compact,
hierarchical multi-layer scene graph that can represent large, complex indoor
environments at various levels of abstraction, e.g., nodes corresponding to
rooms, objects, walls, windows etc. as well as fine-grained details of their
geometry. We develop a procedure based on large language models (LLMs) to
sample plausible scene graphs of unobserved regions that are consistent with
partial observations of the scene. These samples are used to compute an
information gain of a potential waypoint for sophisticated spatial reasoning,
e.g., the two doors in the living room can lead to either a kitchen or a
bedroom. We evaluate this approach in complex, realistic 3D indoor environments
in simulation. We show using qualitative and quantitative experiments that our
approach can pin down the semantics of the environment quicker and more
accurately than baseline approaches.

</details>


### [289] [AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control](https://arxiv.org/abs/2510.05443)
*Shao-Yi Yu,Jen-Wei Wang,Maya Horii,Vikas Garg,Tarek Zohdi*

Main category: cs.RO

TL;DR: 提出一种基于神经常微分方程的自适应动力学模型，通过状态-动作历史推断操作环境，实现对动态变化环境的适应，在多种机器人平台上验证了其在目标到达和路径跟踪任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了使移动机器人在难以获取环境信息的不确定环境中有效运行，需要能够响应环境变化的动力学模型。

Method: 采用基于神经常微分方程的动力学模型，结合两阶段训练方法学习潜在环境表示，利用状态-动作历史推断环境特征，无需直接环境输入，并与模型预测控制集成。

Result: 在2D差速轮式机器人、3D四旋翼无人机和Sphero BOLT机器人三个平台上的仿真和真实实验表明，该方法能有效应对时空变化的环境干扰，在目标到达和路径跟踪任务中表现良好。

Conclusion: 所提出的自适应动力学模型能够在缺乏直接环境感知的情况下，通过历史数据推断环境变化，显著提升机器人在复杂动态环境中的控制性能。

Abstract: Mobile robots, such as ground vehicles and quadrotors, are becoming
increasingly important in various fields, from logistics to agriculture, where
they automate processes in environments that are difficult to access for
humans. However, to perform effectively in uncertain environments using
model-based controllers, these systems require dynamics models capable of
responding to environmental variations, especially when direct access to
environmental information is limited. To enable such adaptivity and facilitate
integration with model predictive control, we propose an adaptive dynamics
model which bypasses the need for direct environmental knowledge by inferring
operational environments from state-action history. The dynamics model is based
on neural ordinary equations, and a two-phase training procedure is used to
learn latent environment representations. We demonstrate the effectiveness of
our approach through goal-reaching and path-tracking tasks on three robotic
platforms of increasing complexity: a 2D differential wheeled robot with
changing wheel contact conditions, a 3D quadrotor in variational wind fields,
and the Sphero BOLT robot under two contact conditions for real-world
deployment. Empirical results corroborate that our method can handle temporally
and spatially varying environmental changes in both simulation and real-world
systems.

</details>


### [290] [Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions](https://arxiv.org/abs/2510.05713)
*Wanli Ni,Hui Tian,Shuai Wang,Chengyang Li,Lei Sun,Zhaohui Yang*

Main category: cs.RO

TL;DR: 本文研究了适用于工业物联网中资源受限机器人的联邦分割学习（FedSL）框架，比较了同步、异步、分层和异构FedSL框架的性能，并提出了自适应优化技术以提升效率。


<details>
  <summary>Details</summary>
Motivation: 在智能工厂等工业场景中，数据隐私、通信效率和设备异质性是关键挑战，需要一种既能保护隐私又能高效协作的学习范式。

Method: 对比分析了多种FedSL框架（同步、异步、分层、异构），并系统分类了三种层级的令牌融合策略（输入级、中间级、输出级），同时提出模型压缩、分割层选择、计算频率分配和无线资源管理等优化技术。

Result: 仿真结果验证了所研究框架在工业检测场景下的性能表现，展示了不同FedSL框架在可扩展性、适应性和局限性方面的差异。

Conclusion: FedSL为资源受限的工业机器人提供了一种高效且隐私保护的协作学习方案，未来的研究方向包括进一步优化异构环境下的训练效率与系统鲁棒性。

Abstract: Federated split learning (FedSL) has emerged as a promising paradigm for
enabling collaborative intelligence in industrial Internet of Things (IoT)
systems, particularly in smart factories where data privacy, communication
efficiency, and device heterogeneity are critical concerns. In this article, we
present a comprehensive study of FedSL frameworks tailored for
resource-constrained robots in industrial scenarios. We compare synchronous,
asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of
workflow, scalability, adaptability, and limitations under dynamic industrial
conditions. Furthermore, we systematically categorize token fusion strategies
into three paradigms: input-level (pre-fusion), intermediate-level
(intra-fusion), and output-level (post-fusion), and summarize their respective
strengths in industrial applications. We also provide adaptive optimization
techniques to enhance the efficiency and feasibility of FedSL implementation,
including model compression, split layer selection, computing frequency
allocation, and wireless resource management. Simulation results validate the
performance of these frameworks under industrial detection scenarios. Finally,
we outline open issues and research directions of FedSL in future smart
manufacturing systems.

</details>


### [291] [Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation](https://arxiv.org/abs/2510.05536)
*Mahboubeh Zarei,Robin Chhabra,Farrokh Janabi-Sharifi*

Main category: cs.RO

TL;DR: 提出了一种基于李群的去中心化双视角传感器融合方法，用于机器人操作臂对目标物体的位姿和速度估计。


<details>
  <summary>Details</summary>
Motivation: 传统集中式传感器融合方法存在扩展性和鲁棒性问题，且难以有效估计速度；需要一种更灵活、准确的去中心化估计框架。

Method: 采用眼在手和眼到手双视觉配置，构建运行在李群SE(3)×R³×R³上的自适应扩展卡尔曼滤波器，并利用李群上的相关性感知融合规则进行状态融合。

Result: 在UFactory xArm 850机器人上实验表明，该方法在位姿和速度估计精度上优于现有最先进方法，具有良好的鲁棒性和一致性提升。

Conclusion: 所提出的去中心化李群滤波与融合框架能有效提升机器人对运动目标的位姿与速度估计性能，适用于复杂空间任务规划。

Abstract: Accurate pose and velocity estimation is essential for effective spatial task
planning in robotic manipulators. While centralized sensor fusion has
traditionally been used to improve pose estimation accuracy, this paper
presents a novel decentralized fusion approach to estimate both pose and
velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand
vision sensor configuration mounted on a manipulator to track a target object
whose motion is modeled as random walk (stochastic acceleration model). The
robot runs two independent adaptive extended Kalman filters formulated on a
matrix Lie group, developed as part of this work. These filters predict poses
and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times
\mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final
fused state comprising the fused pose and velocities of the target is obtained
using a correlation-aware fusion rule on Lie groups. The proposed method is
evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras,
tracking a moving target. Experimental results validate the effectiveness and
robustness of the proposed decentralized dual-view estimation framework,
showing consistent improvements over state-of-the-art methods.

</details>


### [292] [ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation](https://arxiv.org/abs/2510.05547)
*Eugene Vorobiov,Ammar Jaleel Mahmood,Salim Rezvani,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出ARRC系统，结合检索增强生成（RAG）、RGB-D感知和安全执行，将自然语言指令转化为机器人安全控制，实验证明其在桌面操作任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 实现低成本机器人对自然语言指令的安全、可靠响应，提升任务适应性和计划有效性。

Method: 通过RAG检索机器人知识库（动作模式、任务模板、安全规则），结合大语言模型生成JSON格式动作计划，在配备RGB-D相机和夹爪的UFactory xArm 850机械臂上执行，利用AprilTag与深度信息进行目标定位，并通过软件安全机制保障执行安全。

Result: 系统在扫描、接近、抓取放置等桌面任务中表现出良好的计划有效性和适应性，验证了RAG在提升机器人控制灵活性方面的优势。

Conclusion: RAG结合本地感知与低层控制可有效提升机器人对自然语言指令的理解与安全执行能力，具有实际应用前景。

Abstract: We present ARRC (Advanced Reasoning Robot Control), a practical system that
connects natural-language instructions to safe local robotic control by
combining Retrieval-Augmented Generation (RAG) with RGB-D perception and
guarded execution on an affordable robot arm. The system indexes curated robot
knowledge (movement patterns, task templates, and safety heuristics) in a
vector database, retrieves task-relevant context for each instruction, and
conditions a large language model (LLM) to produce JSON-structured action
plans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven
parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag
detections fused with depth to produce object-centric metric poses. Execution
is enforced via software safety gates: workspace bounds, speed and force caps,
timeouts, and bounded retries. We describe the architecture, knowledge design,
integration choices, and a reproducible evaluation protocol for tabletop scan,
approach, and pick-place tasks. Experimental results demonstrate the efficacy
of the proposed approach. Our design shows that RAG-based planning can
substantially improve plan validity and adaptability while keeping perception
and low-level control local to the robot.

</details>


### [293] [GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps](https://arxiv.org/abs/2510.05553)
*Yan Rui Tan,Wenqi Liu,Wai Lun Leong,John Guan Zhong Tan,Wayne Wen Huei Yong,Fan Shi,Rodney Swee Huat Teo*

Main category: cs.RO

TL;DR: GO-Flock是一种结合规划与反应式控制的混合型集群框架，通过感知模块和集体导航模块实现复杂环境中的高效避障与集群飞行。


<details>
  <summary>Details</summary>
Motivation: 传统的人工势场法在存在障碍物时易陷入局部极小和死锁，且多数方法仅在无障或简化环境中验证，缺乏实际复杂场景下的有效性。

Method: 提出GO-Flock框架，包含处理深度图以提取航点和虚拟代理的感知模块，以及采用新型人工势场策略的集体导航模块，实现主动避障与集群控制。

Result: 在充满障碍物的环境和硬件在环实验中验证了GO-Flock的有效性，成功实现了六架实体无人机和三架虚拟无人机在森林环境中的编队飞行。

Conclusion: GO-Flock能够有效克服传统APF方法的局限性，在复杂环境中实现高效、鲁棒的集群导航。

Abstract: Artificial Potential Field (APF) methods are widely used for reactive
flocking control, but they often suffer from challenges such as deadlocks and
local minima, especially in the presence of obstacles. Existing solutions to
address these issues are typically passive, leading to slow and inefficient
collective navigation. As a result, many APF approaches have only been
validated in obstacle-free environments or simplified, pseudo 3D simulations.
This paper presents GO-Flock, a hybrid flocking framework that integrates
planning with reactive APF-based control. GO-Flock consists of an upstream
Perception Module, which processes depth maps to extract waypoints and virtual
agents for obstacle avoidance, and a downstream Collective Navigation Module,
which applies a novel APF strategy to achieve effective flocking behavior in
cluttered environments. We evaluate GO-Flock against passive APF-based
approaches to demonstrate their respective merits, such as their flocking
behavior and the ability to overcome local minima. Finally, we validate
GO-Flock through obstacle-filled environment and also hardware-in-the-loop
experiments where we successfully flocked a team of nine drones, six physical
and three virtual, in a forest environment.

</details>


### [294] [DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation](https://arxiv.org/abs/2510.05662)
*Taeyeop Lee,Gyuree Kang,Bowen Wen,Youngho Kim,Seunghyeok Back,In So Kweon,David Hyunchul Shim,Kuk-Jin Yoon*

Main category: cs.RO

TL;DR: 本文提出了DeLTa框架，用于解决透明物体在长视野机器人操作中的挑战，结合深度估计、6D姿态估计和视觉语言规划，实现基于自然语言指令的精确操作。


<details>
  <summary>Details</summary>
Motivation: 现有透明物体操作方法在泛化性和长视野精确操作方面存在局限，难以应对新颖物体和复杂任务。

Method: DeLTa框架融合深度估计、6D姿态估计和视觉语言规划，采用单次演示方法泛化到新物体，并设计任务规划器以适应单臂眼在手机器人。

Result: 实验表明，该方法在长视野任务中显著优于现有方法，尤其在精确操作方面表现突出。

Conclusion: DeLTa为透明物体的长视野机器人操作提供了高效且通用的解决方案，具备良好的实际应用潜力。

Abstract: Despite the prevalence of transparent object interactions in human everyday
life, transparent robotic manipulation research remains limited to
short-horizon tasks and basic grasping capabilities.Although some methods have
partially addressed these issues, most of them have limitations in
generalizability to novel objects and are insufficient for precise long-horizon
robot manipulation. To address this limitation, we propose DeLTa (Demonstration
and Language-Guided Novel Transparent Object Manipulation), a novel framework
that integrates depth estimation, 6D pose estimation, and vision-language
planning for precise long-horizon manipulation of transparent objects guided by
natural task instructions. A key advantage of our method is its
single-demonstration approach, which generalizes 6D trajectories to novel
transparent objects without requiring category-level priors or additional
training. Additionally, we present a task planner that refines the
VLM-generated plan to account for the constraints of a single-arm, eye-in-hand
robot for long-horizon object manipulation tasks. Through comprehensive
evaluation, we demonstrate that our method significantly outperforms existing
transparent object manipulation approaches, particularly in long-horizon
scenarios requiring precise manipulation capabilities. Project page:
https://sites.google.com/view/DeLTa25/

</details>


### [295] [Verifier-free Test-Time Sampling for Vision Language Action Models](https://arxiv.org/abs/2510.05681)
*Suhyeok Jang,Dongyoung Kim,Changyeon Kim,Youngsuk Kim,Jinwoo Shin*

Main category: cs.RO

TL;DR: 提出了一种名为MG-Select的新型测试时扩展框架，利用视觉-语言-动作模型内部特性提升机器人控制任务中的决策精度，无需额外训练或外部模块，在多种任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型因单次推理范式在高精度任务中表现受限，且现有测试时扩展方法依赖额外训练和外部验证器，泛化能力差。

Method: 提出MG-Select框架，使用KL散度衡量候选动作与参考动作分布之间的差异作为置信度指标；参考分布由对状态和语言条件随机掩码后的同一VLA生成，并通过联合训练策略（对输入条件应用dropout）提升参考分布质量。

Result: 实验显示MG-Select在真实世界任务中实现了28%（分布内）和35%（分布外）的性能提升，在仅用30个演示训练的RoboCasa抓取放置任务中相对增益达168%。

Conclusion: MG-Select通过利用模型内部不确定性实现高效测试时扩展，无需额外组件或训练，显著提升了VLAs在复杂和高精度机器人任务中的表现与泛化能力。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
in robot control. However, they remain fundamentally limited in tasks that
require high precision due to their single-inference paradigm. While test-time
scaling approaches using external verifiers have shown promise, they require
additional training and fail to generalize to unseen conditions. We propose
Masking Distribution Guided Selection (MG-Select), a novel test-time scaling
framework for VLAs that leverages the model's internal properties without
requiring additional training or external modules. Our approach utilizes KL
divergence from a reference action token distribution as a confidence metric
for selecting the optimal action from multiple candidates. We introduce a
reference distribution generated by the same VLA but with randomly masked
states and language conditions as inputs, ensuring maximum uncertainty while
remaining aligned with the target task distribution. Additionally, we propose a
joint training strategy that enables the model to learn both conditional and
unconditional distributions by applying dropout to state and language
conditions, thereby further improving the quality of the reference
distribution. Our experiments demonstrate that MG-Select achieves significant
performance improvements, including a 28%/35% improvement in real-world
in-distribution/out-of-distribution tasks, along with a 168% relative gain on
RoboCasa pick-and-place tasks trained with 30 demonstrations.

</details>


### [296] [Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies](https://arxiv.org/abs/2510.05692)
*Yuhang Zhang,Jiaping Xiao,Chao Yan,Mir Feroskhan*

Main category: cs.RO

TL;DR: 提出OMC-RL框架，通过解耦表示学习与策略学习并引入oracle引导，提升视觉运动策略学习的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维视觉输入与敏捷动作输出带来的样本效率低和sim-to-real差距大的问题。

Method: 将学习过程解耦为两个阶段：上游使用掩码Transformer结合时序建模和对比学习进行表示学习；下游利用具有全局状态信息的oracle教师策略指导早期训练，并逐步减少指导。

Result: 在模拟和真实环境中实验表明，OMC-RL在样本效率、渐近性能和复杂场景泛化能力方面均优于现有方法。

Conclusion: OMC-RL有效提升了视觉运动策略学习的效率与性能，具备良好的实际应用潜力。

Abstract: A prevailing approach for learning visuomotor policies is to employ
reinforcement learning to map high-dimensional visual observations directly to
action commands. However, the combination of high-dimensional visual inputs and
agile maneuver outputs leads to long-standing challenges, including low sample
efficiency and significant sim-to-real gaps. To address these issues, we
propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a
novel framework designed to improve the sample efficiency and asymptotic
performance of visuomotor policy learning. OMC-RL explicitly decouples the
learning process into two stages: an upstream representation learning stage and
a downstream policy learning stage. In the upstream stage, a masked Transformer
module is trained with temporal modeling and contrastive learning to extract
temporally-aware and task-relevant representations from sequential visual
inputs. After training, the learned encoder is frozen and used to extract
visual representations from consecutive frames, while the Transformer module is
discarded. In the downstream stage, an oracle teacher policy with privileged
access to global state information supervises the agent during early training
to provide informative guidance and accelerate early policy learning. This
guidance is gradually reduced to allow independent exploration as training
progresses. Extensive experiments in simulated and real-world environments
demonstrate that OMC-RL achieves superior sample efficiency and asymptotic
policy performance, while also improving generalization across diverse and
perceptually complex scenarios.

</details>


### [297] [Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs](https://arxiv.org/abs/2510.05707)
*David Boetius,Abdelrahman Abdelnaby,Ashok Kumar,Stefan Leue,Abdalla Swikir,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: 提出了一种基于神经常微分方程的框架，用于在黎曼流形上学习稳定的动力系统，通过投影神经向量场以满足李雅普诺夫稳定性准则，实现了对复杂轨迹的精确建模并保证了流形约束下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 在机器人运动规划与控制中，学习稳定动力系统至关重要，但将稳定性保证扩展到黎曼流形上的轨迹因几何约束而面临挑战。

Method: 利用神经常微分方程，通过将神经向量场投影至满足李雅普诺夫稳定性条件的方式，在黎曼流形上学习稳定动力系统，并采用灵活的神经网络参数化向量场和李雅普诺夫函数，直接在流形上演化解。

Result: 在单位四元数、正定矩阵流形及\mathbb{R}^3 \times S^3上的黎曼LASA数据集上验证了方法的有效性，展示了其性能、可扩展性和实际应用能力。

Conclusion: 该框架能有效学习满足稳定性与流形约束的复杂机器人运动，适用于仿真与真实场景中的运动学习。

Abstract: Learning stable dynamical systems from data is crucial for safe and reliable
robot motion planning and control. However, extending stability guarantees to
trajectories defined on Riemannian manifolds poses significant challenges due
to the manifold's geometric constraints. To address this, we propose a general
framework for learning stable dynamical systems on Riemannian manifolds using
neural ordinary differential equations. Our method guarantees stability by
projecting the neural vector field evolving on the manifold so that it strictly
satisfies the Lyapunov stability criterion, ensuring stability at every system
state. By leveraging a flexible neural parameterisation for both the base
vector field and the Lyapunov function, our framework can accurately represent
complex trajectories while respecting manifold constraints by evolving
solutions directly on the manifold. We provide an efficient training strategy
for applying our framework and demonstrate its utility by solving Riemannian
LASA datasets on the unit quaternion (S^3) and symmetric positive-definite
matrix manifolds, as well as robotic motions evolving on \mathbb{R}^3 \times
S^3. We demonstrate the performance, scalability, and practical applicability
of our approach through extensive simulations and by learning robot motions in
a real-world experiment.

</details>


### [298] [Precise and Efficient Collision Prediction under Uncertainty in Autonomous Driving](https://arxiv.org/abs/2510.05729)
*Marc Kaufeld,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了两种在不确定驾驶条件下估计自动驾驶轨迹碰撞风险的半解析方法，能够高效且准确地计算与任意凸形障碍物的碰撞概率，并支持实时规划。


<details>
  <summary>Details</summary>
Motivation: 由于感知噪声、定位误差和周围交通参与者行为预测的不确定性，传统的确定性碰撞检测往往不准确或过于保守，因此需要更精确的风险评估方法。

Method: 提出两种半解析方法：一是计算自动驾驶车辆与周围障碍物空间重叠的概率，二是基于随机边界穿越估计碰撞概率；两种方法均考虑了位置、姿态和速度等完整状态不确定性。

Result: 仿真结果表明所提方法在精度上接近蒙特卡洛方法，同时显著降低计算时间，适合实时应用。

Conclusion: 所提出的碰撞概率估计算法兼具高精度与高效性，适用于风险感知的轨迹规划，并已作为开源软件发布。

Abstract: This research introduces two efficient methods to estimate the collision risk
of planned trajectories in autonomous driving under uncertain driving
conditions. Deterministic collision checks of planned trajectories are often
inaccurate or overly conservative, as noisy perception, localization errors,
and uncertain predictions of other traffic participants introduce significant
uncertainty into the planning process. This paper presents two semi-analytic
methods to compute the collision probability of planned trajectories with
arbitrary convex obstacles. The first approach evaluates the probability of
spatial overlap between an autonomous vehicle and surrounding obstacles, while
the second estimates the collision probability based on stochastic boundary
crossings. Both formulations incorporate full state uncertainties, including
position, orientation, and velocity, and achieve high accuracy at computational
costs suitable for real-time planning. Simulation studies verify that the
proposed methods closely match Monte Carlo results while providing significant
runtime advantages, enabling their use in risk-aware trajectory planning. The
collision estimation methods are available as open-source software:
https://github.com/TUM-AVS/Collision-Probability-Estimation

</details>


### [299] [Human-in-the-loop Optimisation in Robot-assisted Gait Training](https://arxiv.org/abs/2510.05780)
*Andreas Christou,Andreas Sochopoulos,Elliot Lister,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 该研究探索了人在回路优化（HILO）在步态训练中实现个性化辅助的潜力，使用CMA-ES算法优化下肢外骨骼的“按需辅助”控制器，结果表明虽能为个体收敛出独特参数，但未显著提升表现，揭示了人机共适应和行为变异性对个性化控制效果的重大影响。


<details>
  <summary>Details</summary>
Motivation: 由于个体间和个体内步态差异显著，需要设计能适应个体特征的机器人控制器以实现有效的步态辅助。

Method: 采用协方差矩阵自适应进化策略（CMA-ES）在健康受试者身上连续优化下肢外骨骼的按需辅助控制器，并进行为期两天的实验。

Result: CMA-ES能为每个个体收敛出独特的刚度参数，但在验证试验中未观察到对受试者表现的可测量改善。

Conclusion: 人机共适应和人类行为的变异性可能超过基于规则的个性化辅助控制器的潜在优势，当前个性化方法在外骨骼辅助康复中存在局限性，HILO的实际应用仍面临关键挑战。

Abstract: Wearable robots offer a promising solution for quantitatively monitoring gait
and providing systematic, adaptive assistance to promote patient independence
and improve gait. However, due to significant interpersonal and intrapersonal
variability in walking patterns, it is important to design robot controllers
that can adapt to the unique characteristics of each individual. This paper
investigates the potential of human-in-the-loop optimisation (HILO) to deliver
personalised assistance in gait training. The Covariance Matrix Adaptation
Evolution Strategy (CMA-ES) was employed to continuously optimise an
assist-as-needed controller of a lower-limb exoskeleton. Six healthy
individuals participated over a two-day experiment. Our results suggest that
while the CMA-ES appears to converge to a unique set of stiffnesses for each
individual, no measurable impact on the subjects' performance was observed
during the validation trials. These findings highlight the impact of
human-robot co-adaptation and human behaviour variability, whose effect may be
greater than potential benefits of personalising rule-based assistive
controllers. Our work contributes to understanding the limitations of current
personalisation approaches in exoskeleton-assisted gait rehabilitation and
identifies key challenges for effective implementation of human-in-the-loop
optimisation in this domain.

</details>


### [300] [VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation](https://arxiv.org/abs/2510.05827)
*Haoran Zhang,Shuanghao Bai,Wanqi Zhou,Yuedi Zhang,Qi Zhang,Pengxiang Ding,Cheng Chi,Donglin Wang,Badong Chen*

Main category: cs.RO

TL;DR: 提出VCoT-Grasp，一种端到端的视觉链式推理抓取基础模型，通过引入大规模合成与真实数据集VCoT-GraspSet，在复杂环境中显著提升抓取成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动抓取方法缺乏足够的推理与泛化能力，或依赖复杂模块化流程；当前抓取基础模型过于强调对话和对象语义，导致性能下降且局限于单物体抓取。

Method: 提出VCoT-Grasp，采用端到端架构并融入视觉链式思维（visual chain-of-thought）推理机制，通过多轮次处理范式动态关注视觉输入，并提供可解释的推理轨迹；构建大规模数据集VCoT-GraspSet用于训练，包含167K合成图像和400+真实图像，标注了中间边界框信息。

Result: 在VCoT-GraspSet和真实机器人实验中，该方法显著提高了抓取成功率，具备对未见物体、背景和干扰物的良好泛化能力。

Conclusion: VCoT-Grasp通过视觉链式推理增强了视觉理解，实现了在杂乱环境中的高效、可解释且泛化的抓取生成，为语言驱动的机器人抓取提供了新思路。

Abstract: Robotic grasping is one of the most fundamental tasks in robotic
manipulation, and grasp detection/generation has long been the subject of
extensive research. Recently, language-driven grasp generation has emerged as a
promising direction due to its practical interaction capabilities. However,
most existing approaches either lack sufficient reasoning and generalization
capabilities or depend on complex modular pipelines. Moreover, current grasp
foundation models tend to overemphasize dialog and object semantics, resulting
in inferior performance and restriction to single-object grasping. To maintain
strong reasoning ability and generalization in cluttered environments, we
propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates
visual chain-of-thought reasoning to enhance visual understanding for grasp
generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically
focuses on visual inputs while providing interpretable reasoning traces. For
training, we refine and introduce a large-scale dataset, VCoT-GraspSet,
comprising 167K synthetic images with over 1.36M grasps, as well as 400+
real-world images with more than 1.2K grasps, annotated with intermediate
bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot
demonstrate that our method significantly improves grasp success rates and
generalizes effectively to unseen objects, backgrounds, and distractors. More
details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.

</details>


### [301] [A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling](https://arxiv.org/abs/2510.05923)
*Aman Singh,Aastha Mishra,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出了一种三阶段协同设计优化框架，兼顾单足机器人跳跃高度和机械能耗，结合真实执行器质量模型并自动生CAD模型，实验显示能耗降低50%，跳跃高度达0.8米。


<details>
  <summary>Details</summary>
Motivation: 现有协同设计框架通常只优化跳跃高度或能耗，忽略二者权衡，且常忽略齿轮箱参数优化和使用简化的执行器质量模型，导致设计难以实际复现。

Method: 提出一种三阶段协同设计优化框架，统一优化机械设计（包括齿轮箱）和控制参数，并引入真实的执行器质量模型，优化过程中同时考虑跳跃高度和机械能耗，最终自动生成可直接制造的参数化CAD模型。

Result: 相比基线设计，机械能耗降低了50%，同时实现了0.8米的跳跃高度。

Conclusion: 该协同设计框架能有效平衡跳跃性能与能耗，提升设计实用性，并通过自动化CAD生成减少人工迭代，具有较强的工程应用价值。

Abstract: A monoped's jump height and energy consumption depend on both, its mechanical
design and control strategy. Existing co-design frameworks typically optimize
for either maximum height or minimum energy, neglecting their trade-off. They
also often omit gearbox parameter optimization and use oversimplified actuator
mass models, producing designs difficult to replicate in practice. In this
work, we introduce a novel three-stage co-design optimization framework that
jointly maximizes jump height while minimizing mechanical energy consumption of
a monoped. The proposed method explicitly incorporates realistic actuator mass
models and optimizes mechanical design (including gearbox) and control
parameters within a unified framework. The resulting design outputs are then
used to automatically generate a parameterized CAD model suitable for direct
fabrication, significantly reducing manual design iterations. Our experimental
evaluations demonstrate a 50 percent reduction in mechanical energy consumption
compared to the baseline design, while achieving a jump height of 0.8m. Video
presentation is available at http://y2u.be/XW8IFRCcPgM

</details>


### [302] [Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion](https://arxiv.org/abs/2510.05957)
*Vaughn Gzenda,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种基于潜动力学的模型强化学习框架，用于软体机器人在仅有噪声传感器反馈的情况下实现自适应运动控制。


<details>
  <summary>Details</summary>
Motivation: 软体机器人由于模型不准确、传感器噪声和步态发现困难，控制策略设计具有挑战性。

Method: 采用基于模型的强化学习（MB-RL），利用机载传感器数据推断潜在动力学作为预测模型，指导actor-critic算法优化运动策略。

Result: 在仿真中验证了该框架的有效性，潜动力学实现了短时运动预测，actor-critic成功发现了有效的运动策略。

Conclusion: 潜动力学MB-RL有潜力仅依靠噪声传感器反馈实现具身软体机器人的自适应运动。

Abstract: Soft robotic crawlers are mobile robots that utilize soft body deformability
and compliance to achieve locomotion through surface contact. Designing control
strategies for such systems is challenging due to model inaccuracies, sensor
noise, and the need to discover locomotor gaits. In this work, we present a
model-based reinforcement learning (MB-RL) framework in which latent dynamics
inferred from onboard sensors serve as a predictive model that guides an
actor-critic algorithm to optimize locomotor policies. We evaluate the
framework on a minimal crawler model in simulation using inertial measurement
units and time-of-flight sensors as observations. The learned latent dynamics
enable short-horizon motion prediction while the actor-critic discovers
effective locomotor policies. This approach highlights the potential of
latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion
based solely on noisy sensor feedback.

</details>


### [303] [The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics](https://arxiv.org/abs/2510.05981)
*Cristina Luna,Alba Guerra,Almudena Moreno,Manuel Esquer,Willy Roa,Mateusz Krawczak,Robert Popela,Piotr Osica,Davide Nicolis*

Main category: cs.RO

TL;DR: 本文提出了一种名为DISTANT的新型远程传动与转向系统设计，将火星车的驱动和转向执行器从轮子上移至 rover 体内受保护的恒温箱中，以应对极端环境下的长期探测任务。


<details>
  <summary>Details</summary>
Motivation: 为了提高行星探测任务中火星车在极端环境下的可靠性和耐久性，解决热循环、灰尘污染和机械磨损对敏感部件的影响。

Method: 采用双叉臂悬挂结构，结合万向节和卡普斯坦驱动转向系统，并通过全面的权衡分析确定最优架构，所有电机设备均保留在受保护的温暖舱内。

Result: 实现了车轮独立驱动、转向和悬挂控制，满足50公里行驶需求且性能不下降，集成了防尘和热管理方案。

Conclusion: 该设计显著提升了火星车在长期远距离任务中的可靠性，预计于2026年第一季度完成1:3比例样机制造并开展测试验证。

Abstract: Planetary exploration missions require robust locomotion systems capable of
operating in extreme environments over extended periods. This paper presents
the DISTANT (Distant Transmission and Steering Systems) design, a novel
approach for relocating rover traction and steering actuators from
wheel-mounted positions to a thermally protected warm box within the rover
body. The design addresses critical challenges in long-distance traversal
missions by protecting sensitive components from thermal cycling, dust
contamination, and mechanical wear. A double wishbone suspension configuration
with cardan joints and capstan drive steering has been selected as the optimal
architecture following comprehensive trade-off analysis. The system enables
independent wheel traction, steering control, and suspension management whilst
maintaining all motorisation within the protected environment. The design meets
a 50 km traverse requirement without performance degradation, with integrated
dust protection mechanisms and thermal management solutions. Testing and
validation activities are planned for Q1 2026 following breadboard
manufacturing at 1:3 scale.

</details>


### [304] [AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations](https://arxiv.org/abs/2510.05985)
*Cristina Luna,Robert Field,Steven Kay*

Main category: cs.RO

TL;DR: 本文提出了一种集成AI系统，通过远距离障碍物检测、多机器人协作框架和基于深度学习的地形分类技术，显著提升行星探测车的自主性，使其行驶速度从10 cm/s提升至1.0 m/s，并在火星类比环境中验证了系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前行星探测车的行驶速度较慢（约10 cm/s），严重限制了探索效率，因此需要提高其自主性和运行速度。

Method: 开发了三个核心组件：基于计算机视觉的FASTNAV远距离障碍物检测器（FOD）、用于人机协作的多机器人协调框架CISRU，以及基于深度学习的ViBEKO和AIAXR地形分类方法，并在火星类比环境中进行实地验证。

Result: 系统在技术成熟度4级（TRL 4）得到验证，显著提升了探测车的行驶速度、地形分类准确率和操作安全性。

Conclusion: 所提出的集成AI系统有效提升了行星探测车的自主导航能力和任务效率，为下一代行星探测任务提供了关键技术支撑。

Abstract: Current planetary rovers operate at traverse speeds of approximately 10 cm/s,
fundamentally limiting exploration efficiency. This work presents integrated AI
systems which significantly improve autonomy through three components: (i) the
FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s
speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot
coordination framework enabling human-robot collaboration for in-situ resource
utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain
classification studies. Field validation in Mars analogue environments
demonstrated these systems at Technology Readiness Level 4, providing
measurable improvements in traverse speed, classification accuracy, and
operational safety for next-generation planetary missions.

</details>


### [305] [Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations](https://arxiv.org/abs/2510.05992)
*Tien-Dat Nguyen,Thien-Minh Nguyen,Vinh-Hao Nguyen*

Main category: cs.RO

TL;DR: 提出了一种两阶段方法，通过融合UWB和SLAM数据实现坐标一致且精确的定位。


<details>
  <summary>Details</summary>
Motivation: SLAM估计的坐标原点在每次运行时可能重置，而UWB定位需要锚节点坐标准确已知，因此需要一种方法来实现跨会话的一致性和精度。

Method: 第一阶段利用一次完整运行的范围和里程数据，结合高度先验和锚点间距离因子，通过连续时间批优化求解锚点3D位置；第二阶段采用滑动窗口优化方案融合UWB和SLAM数据。

Result: 在NTU VIRAL数据集上进行实验，六个无人机飞行场景的结果表明，仅用一次运行的数据校准即可支持其余运行中的精确定位。

Conclusion: 该方法能有效实现多会话下坐标一致且高精度的定位，且代码已开源以促进社区发展。

Abstract: Onboard simultaneous localization and mapping (SLAM) methods are commonly
used to provide accurate localization information for autonomous robots.
However, the coordinate origin of SLAM estimate often resets for each run. On
the other hand, UWB-based localization with fixed anchors can ensure a
consistent coordinate reference across sessions; however, it requires an
accurate assignment of the anchor nodes' coordinates. To this end, we propose a
two-stage approach that calibrates and fuses UWB data and SLAM data to achieve
coordinate-wise consistent and accurate localization in the same environment.
In the first stage, we solve a continuous-time batch optimization problem by
using the range and odometry data from one full run, incorporating height
priors and anchor-to-anchor distance factors to recover the anchors' 3D
positions. For the subsequent runs in the second stage, a sliding-window
optimization scheme fuses the UWB and SLAM data, which facilitates accurate
localization in the same coordinate system. Experiments are carried out on the
NTU VIRAL dataset with six scenarios of UAV flight, and we show that
calibration using data in one run is sufficient to enable accurate localization
in the remaining runs. We release our source code to benefit the community at
https://github.com/ntdathp/slam-uwb-calibration.

</details>


### [306] [Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning](https://arxiv.org/abs/2510.06068)
*Heng Zhang,Kevin Yuchen Ma,Mike Zheng Shou,Weisi Lin,Yan Wu*

Main category: cs.RO

TL;DR: 提出一种基于eigengrasp的端到端跨形态灵巧手抓取生成框架，通过形态嵌入和低维关节系数预测实现高效、通用的抓取，仿真和实验证明其高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特定手上训练且难以泛化到不同灵巧手，同时优化流程成本高，因此需要一个能跨不同机械手形态泛化的高效抓取生成方法。

Method: 从手部形态描述中提取形态嵌入和eigengrasp集合，结合物体点云和腕部姿态，使用振幅预测器回归低维空间中的关节系数，并通过Kinematic-Aware Articulation Loss（KAL）进行监督学习，最后解码为完整关节运动。

Result: 在三种未见灵巧手上对未见物体的仿真测试中达到91.9%的平均抓取成功率，单次推理小于0.4秒；在少样本适应新手型后，仿真中达85.6%，真实世界实验中达87%成功率。

Conclusion: 该方法实现了高效的跨形态灵巧抓取生成，具有良好的泛化能力、快速推理速度和实际部署潜力。

Abstract: Dexterous grasping with multi-fingered hands remains challenging due to
high-dimensional articulations and the cost of optimization-based pipelines.
Existing end-to-end methods require training on large-scale datasets for
specific hands, limiting their ability to generalize across different
embodiments. We propose an eigengrasp-based, end-to-end framework for
cross-embodiment grasp generation. From a hand's morphology description, we
derive a morphology embedding and an eigengrasp set. Conditioned on these,
together with the object point cloud and wrist pose, an amplitude predictor
regresses articulation coefficients in a low-dimensional space, which are
decoded into full joint articulations. Articulation learning is supervised with
a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant
motions and injects morphology-specific structure. In simulation on unseen
objects across three dexterous hands, our model attains a 91.9% average grasp
success rate with less than 0.4 seconds inference per grasp. With few-shot
adaptation to an unseen hand, it achieves 85.6% success on unseen objects in
simulation, and real-world experiments on this few-shot generalized hand
achieve an 87% success rate. The code and additional materials will be made
available upon publication on our project website
https://connor-zh.github.io/cross_embodiment_dexterous_grasping.

</details>


### [307] [Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor](https://arxiv.org/abs/2510.06085)
*Roman Ibrahimov,Jannik Matthias Heinen*

Main category: cs.RO

TL;DR: 提出了一种受生物启发的多机器人系统，采用分布式优化策略实现未知环境的高效探索与地图构建。


<details>
  <summary>Details</summary>
Motivation: 为了提高在未知环境中探索和建图的效率，并实现有效的任务分配，受到蟑螂触角感知障碍物行为的启发。

Method: 每个机器人通过表面安装的触觉传感器模拟蟑螂触角进行自主探索，记录碰撞点信息；利用分布式优化方法将各机器人局部地图融合为全局2D地图。

Result: 在1.5 x 1.5 m模拟环境中使用e-puck机器人验证了该方法的有效性，实现了高覆盖率、较少碰撞和精确的2D地图构建。

Conclusion: 该生物启发式分布式多机器人系统能有效完成未知环境的探索与建图任务，适用于搜救、工业检测和环境监测等场景。

Abstract: This project proposes a bioinspired multi-robot system using Distributed
Optimization for efficient exploration and mapping of unknown environments.
Each robot explores its environment and creates a map, which is afterwards put
together to form a global 2D map of the environment. Inspired by wall-following
behaviors, each robot autonomously explores its neighborhood based on a tactile
sensor, similar to the antenna of a cockroach, mounted on the surface of the
robot. Instead of avoiding obstacles, robots log collision points when they
touch obstacles. This decentralized control strategy ensures effective task
allocation and efficient exploration of unknown terrains, with applications in
search and rescue, industrial inspection, and environmental monitoring. The
approach was validated through experiments using e-puck robots in a simulated
1.5 x 1.5 m environment with three obstacles. The results demonstrated the
system's effectiveness in achieving high coverage, minimizing collisions, and
constructing accurate 2D maps.

</details>


### [308] [Towards Autonomous Tape Handling for Robotic Wound Redressing](https://arxiv.org/abs/2510.06127)
*Xiao Liang,Lu Shen,Peihan Zhang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 本文提出了一种用于慢性伤口护理中胶带操作的自主机器人框架，解决了胶带初始剥离和安全放置两个关键问题，通过力反馈模仿学习和数值轨迹优化方法实现了可靠性能，为实现机器人自动化伤口护理迈出了重要一步。


<details>
  <summary>Details</summary>
Motivation: 慢性伤口治疗目前依赖人工操作，成本高且资源密集，亟需通过机器人与自动化技术降低成本并提升患者治疗效果。

Method: 针对胶带初始剥离（TID），采用基于人类遥操作演示的力反馈模仿学习方法；对于胶带放置，则设计基于数值轨迹优化的方法以确保无皱、平滑粘贴。

Result: 实验验证表明，所提方法在定量评估和完整的换药流程集成中均表现出可靠的性能。

Conclusion: 胶带操作是实现机器人自动化伤口护理的关键步骤，本文框架为此提供了可行的技术路径。

Abstract: Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over
6.5 million patients in the United States alone and generate an annual cost
exceeding \$25 billion. Despite this burden, chronic wound care remains a
routine yet manual process performed exclusively by trained clinicians due to
its critical safety demands. We envision a future in which robotics and
automation support wound care to lower costs and enhance patient outcomes. This
paper introduces an autonomous framework for one of the most fundamental yet
challenging subtasks in wound redressing: adhesive tape manipulation.
Specifically, we address two critical capabilities: tape initial detachment
(TID) and secure tape placement. To handle the complex adhesive dynamics of
detachment, we propose a force-feedback imitation learning approach trained
from human teleoperation demonstrations. For tape placement, we develop a
numerical trajectory optimization method based to ensure smooth adhesion and
wrinkle-free application across diverse anatomical surfaces. We validate these
methods through extensive experiments, demonstrating reliable performance in
both quantitative evaluations and integrated wound redressing pipelines. Our
results establish tape manipulation as an essential step toward practical
robotic wound care automation.

</details>


### [309] [Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments](https://arxiv.org/abs/2510.06146)
*Jaehwan Jeong,Tuan-Anh Vu,Radha Lahoti,Jiawen Wang,Vivek Alumootil,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 本文提出并验证了一种基于视觉引导的机器人授粉框架，结合3D植物重建、目标抓取规划和基于物理的振动建模，实现精确授粉。


<details>
  <summary>Details</summary>
Motivation: 在缺乏风媒且商业授粉者使用受限的受控农业环境中，机器人授粉是替代人工和熊蜂授粉的一种有前景的方法。

Method: 利用安装在末端执行器上的RGB-D传感器数据进行3D植物重建，并将其注册到机器人坐标系中以识别无障碍的抓取姿态；采用离散弹性杆模型预测驱动参数与花朵动态之间的关系，指导最优授粉策略的选择；通过带软夹持器的机械臂抓握茎秆并施加可控振动以诱导花粉释放。

Result: 端到端实验显示主茎抓取成功率达92.5%，仿真引导的振动参数优化进一步验证了该方法的可行性，确保机器人能够安全有效地完成授粉而不损伤花朵。

Conclusion: 据作者所知，这是首个将基于视觉的抓取与振动建模相结合用于自动化精准授粉的机器人系统。

Abstract: Robotic pollination offers a promising alternative to manual labor and
bumblebee-assisted methods in controlled agriculture, where wind-driven
pollination is absent and regulatory restrictions limit the use of commercial
pollinators. In this work, we present and validate a vision-guided robotic
framework that uses data from an end-effector mounted RGB-D sensor and combines
3D plant reconstruction, targeted grasp planning, and physics-based vibration
modeling to enable precise pollination. First, the plant is reconstructed in 3D
and registered to the robot coordinate frame to identify obstacle-free grasp
poses along the main stem. Second, a discrete elastic rod model predicts the
relationship between actuation parameters and flower dynamics, guiding the
selection of optimal pollination strategies. Finally, a manipulator with soft
grippers grasps the stem and applies controlled vibrations to induce pollen
release. End-to-end experiments demonstrate a 92.5\% main-stem grasping success
rate, and simulation-guided optimization of vibration parameters further
validates the feasibility of our approach, ensuring that the robot can safely
and effectively perform pollination without damaging the flower. To our
knowledge, this is the first robotic system to jointly integrate vision-based
grasping and vibration modeling for automated precision pollination.

</details>


### [310] [A Preview of HoloOcean 2.0](https://arxiv.org/abs/2510.06160)
*Blake Romrell,Abigail Austin,Braden Meyers,Ryan Anderson,Carter Noh,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: HoloOcean 2.0 是一个基于 Unreal Engine 5.3 的高保真海洋机器人模拟器，集成了先进动力学模型和 ROS2 支持，致力于提升海洋传感器、物理仿真和视觉渲染的精度，以支持自主海洋机器人的开发与验证。


<details>
  <summary>Details</summary>
Motivation: 随着海洋机器人领域的快速发展，对高保真仿真系统的需求日益增长，需要更精确地模拟海洋环境中的传感器、物理特性和视觉效果，以支持自主系统的开发和测试。

Method: HoloOcean 2.0 迁移至 Unreal Engine 5.3，采用 Fossen 的高级运动力学模型，并通过自定义桥接支持 ROS2；同时开发基于光线追踪的高效声呐模拟、语义传感器、环境生成工具、体感环境效应和真实波浪模拟等功能。

Result: 实现了更高保真的海洋机器人仿真环境，支持多种任务需求，显著提升了传感器模拟效率和环境真实性，为自主海洋机器人提供了强大的开发与验证平台。

Conclusion: HoloOcean 2.0 作为一个通用且先进的海洋模拟平台，通过集成最新技术，有效推动了海洋机器人系统的研发进程，未来有望成为该领域的重要工具。

Abstract: Marine robotics simulators play a fundamental role in the development of
marine robotic systems. With increased focus on the marine robotics field in
recent years, there has been significant interest in developing higher
fidelitysimulation of marine sensors, physics, and visual rendering
capabilities to support autonomous marine robot development and validation.
HoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art
features under a general marine simulator capable of supporting a variety of
tasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE)
5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2
using a custom bridge. Additional features are currently in development,
including significantly more efficient ray tracing-based sidescan,
forward-looking, and bathymetric sonar implementations; semantic sensors;
environment generation tools; volumetric environmental effects; and realistic
waves.

</details>


### [311] [DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation](https://arxiv.org/abs/2510.06199)
*Chengyang Zhao,Uksang Yoo,Arkadeep Narayan Chaudhury,Giljoo Nam,Jonathan Francis,Jeffrey Ichnowski,Jean Oh*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的机器人护发系统DYMO-Hair，通过新型的动力学学习范式和预训练的3D潜空间实现对未见过发型的泛化，并在模拟和真实环境中实现了优于现有方法的护发造型任务。


<details>
  <summary>Details</summary>
Motivation: 由于头发具有复杂的物理结构和动态特性，目前对于行动不便的人群来说护发仍然难以实现，且对自主机器人系统而言也极具挑战性。

Method: 引入一种适用于如头发这类体积量的动力学学习新范式，结合动作条件化的潜在状态编辑机制与紧凑的多样化发型3D潜空间；使用新型头发物理模拟器大规模预训练该潜空间，并结合MPPI规划器实现视觉目标导向的发型塑造。

Result: 在模拟实验中，DYMO-Hair的动力学模型在捕捉多样且未见发型的局部形变方面优于基线方法；在闭环护发任务中，平均几何误差降低22%，成功率提高42%；真实实验中实现了对假发的零样本迁移，在具挑战性的未见发型上保持成功，而现有最先进系统则失败。

Conclusion: DYMO-Hair为基于模型的机器人护发奠定了基础，推动了在非受限物理环境中更具泛化性、灵活性和可及性的机器人护发技术的发展。

Abstract: Hair care is an essential daily activity, yet it remains inaccessible to
individuals with limited mobility and challenging for autonomous robot systems
due to the fine-grained physical structure and complex dynamics of hair. In
this work, we present DYMO-Hair, a model-based robot hair care system. We
introduce a novel dynamics learning paradigm that is suited for volumetric
quantities such as hair, relying on an action-conditioned latent state editing
mechanism, coupled with a compact 3D latent space of diverse hairstyles to
improve generalizability. This latent space is pre-trained at scale using a
novel hair physics simulator, enabling generalization across previously unseen
hairstyles. Using the dynamics model with a Model Predictive Path Integral
(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair
styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model
outperforms baselines on capturing local deformation for diverse, unseen
hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling
tasks on unseen hairstyles, with an average of 22% lower final geometric error
and 42% higher success rate than the state-of-the-art system. Real-world
experiments exhibit zero-shot transferability of our system to wigs, achieving
consistent success on challenging unseen hairstyles where the state-of-the-art
system fails. Together, these results introduce a foundation for model-based
robot hair care, advancing toward more generalizable, flexible, and accessible
robot hair styling in unconstrained physical environments. More details are
available on our project page: https://chengyzhao.github.io/DYMOHair-web/.

</details>


### [312] [EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model](https://arxiv.org/abs/2510.06207)
*Zefu Lin,Rongxu Cui,Chen Hanning,Xiangyu Wang,Junjia Xu,Xiaojuan Jin,Chen Wenbo,Hui Zhou,Lue Fan,Wenling Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种无需训练的机器人控制框架EmbodiedCoder，通过将自然语言指令转化为可执行代码来实现开放世界中的移动机器人操作，具有良好的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人控制方法依赖大量标注数据且难以扩展到多样化环境，缺乏可解释性。

Method: 利用编码模型将高层指令直接转换为可执行的机器人轨迹代码，通过代码实现对象几何参数化和操作轨迹生成，无需额外数据收集或微调。

Result: 在真实移动机器人上的实验表明，EmbodiedCoder在多样化长周期任务中表现出鲁棒性，并能有效泛化到新物体和新环境中。

Conclusion: 该研究展示了一种可解释的、连接高层推理与低层控制的新范式，推动了从固定动作原语向通用机器人智能的发展。

Abstract: Recent advances in control robot methods, from end-to-end
vision-language-action frameworks to modular systems with predefined
primitives, have advanced robots' ability to follow natural language
instructions. Nonetheless, many approaches still struggle to scale to diverse
environments, as they often rely on large annotated datasets and offer limited
interpretability.In this work, we introduce EmbodiedCoder, a training-free
framework for open-world mobile robot manipulation that leverages coding models
to directly generate executable robot trajectories. By grounding high-level
instructions in code, EmbodiedCoder enables flexible object geometry
parameterization and manipulation trajectory synthesis without additional data
collection or fine-tuning.This coding-based paradigm provides a transparent and
generalizable way to connect perception with manipulation. Experiments on real
mobile robots show that EmbodiedCoder achieves robust performance across
diverse long-term tasks and generalizes effectively to novel objects and
environments.Our results demonstrate an interpretable approach for bridging
high-level reasoning and low-level control, moving beyond fixed primitives
toward versatile robot intelligence. See the project page at:
https://anonymous.4open.science/w/Embodied-Coder/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [313] [Scalable In-context Ranking with Generative Models](https://arxiv.org/abs/2510.05396)
*Nilesh Gupta,Chong You,Srinadh Bhojanapalli,Sanjiv Kumar,Inderjit Dhillon,Felix Yu*

Main category: cs.IR

TL;DR: 本文提出了BlockRank方法，通过利用LLM在上下文排序中的注意力结构特性，实现了高效且准确的信息检索。


<details>
  <summary>Details</summary>
Motivation: 由于注意力机制随上下文长度呈二次或超线性扩展，现有的上下文内排序（ICR）方法在处理长候选文档列表时效率低下。因此需要提高ICR的推理效率并保持性能。

Method: 基于观察到的两个注意力结构特征：文档块间的稀疏性和查询-文档块的相关性，提出BlockRank方法。该方法通过架构上强制实施文档块间稀疏性以降低计算复杂度，并在微调时使用辅助对比学习目标优化查询对相关文档块的注意力。

Result: 在BEIR、MSMarco和NQ数据集上使用Mistral-7B的实验表明，BlockRank在性能上达到或超过了现有最先进方法，同时显著提升了推理效率（在100个MSMarco文档上下文中快4.7倍），并能在一个秒内处理约500个文档（约10万上下文长度）。

Conclusion: BlockRank为上下文内排序提供了一种可扩展且高效的解决方案，在保持高检索性能的同时大幅降低了计算开销。

Abstract: In-context Ranking (ICR) is an emerging paradigm for Information Retrieval
(IR), which leverages contextual understanding of LLMs by directly
incorporating the task description, candidate documents, and the query into the
model's input prompt and tasking the LLM to identify relevant document(s).
While it is effective, efficiency is a significant challenge in this paradigm,
especially as the candidate list grows due to quadratic/super-linear scaling of
attention operation with context length. To this end, this paper first
identifies inherent and exploitable structures in the attention of LLMs
finetuned for ICR: (1) inter-document block sparsity: attention is dense within
each document block but sparse across different documents in the context; and
(2) query-document block relevance: the attention scores from certain query
tokens to a document block in middle layers strongly correlate with that
document's actual relevance. Motivated by these observations, we introduce
BlockRank (Blockwise In-context Ranking), a novel method that adapts the
attention operation in an LLM by (a) architecturally enforcing the observed
inter-document block sparsity, reducing attention complexity from quadratic to
linear without loss in performance, and (b) optimizing query-document block
relevance for true relevant documents during fine-tuning using an auxiliary
contrastive training objective, improving retrieval in attention. Experiments
on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches
or outperforms existing SOTA listwise rankers and controlled fine-tuned
baseline while being significantly more efficient at inference (4.7x for 100
MSMarco documents in context) and scaling gracefully to long-context
shortlists, around 500 documents in-context (approximately 100K context length)
within a second, presenting a scalable and effective solution for ICR.

</details>


### [314] [Automated Research Article Classification and Recommendation Using NLP and ML](https://arxiv.org/abs/2510.05495)
*Shadikur Rahman,Hasibul Karim Shanto,Umme Ayman Koana,Syed Muhammad Danish*

Main category: cs.IR

TL;DR: 提出了一种基于NLP和机器学习的科研论文自动分类与推荐框架，结合多种特征提取方法和分类器，在arXiv数据集上实现69%准确率，并通过余弦相似度进行论文推荐。


<details>
  <summary>Details</summary>
Motivation: 应对数字时代科学文献快速增长导致的研究人员难以高效获取相关工作的挑战。

Method: 采用TF-IDF、Sentence-BERT等特征提取方法，结合逻辑回归、SVM等多种机器学习分类器进行论文分类，并利用向量化表示的余弦相似度实现论文推荐。

Result: 逻辑回归结合TF-IDF表现最佳，分类准确率达69%；推荐模块能有效检索相关论文。

Conclusion: 该框架为数字图书馆中的信息过载问题提供了可扩展的数据驱动解决方案，有助于提升科研文献发现效率。

Abstract: In the digital era, the exponential growth of scientific publications has
made it increasingly difficult for researchers to efficiently identify and
access relevant work. This paper presents an automated framework for research
article classification and recommendation that leverages Natural Language
Processing (NLP) techniques and machine learning. Using a large-scale arXiv.org
dataset spanning more than three decades, we evaluate multiple feature
extraction approaches (TF--IDF, Count Vectorizer, Sentence-BERT, USE,
Mirror-BERT) in combination with diverse machine learning classifiers (Logistic
Regression, SVM, Na\"ive Bayes, Random Forest, Gradient Boosted Trees, and
k-Nearest Neighbour). Our experiments show that Logistic Regression with
TF--IDF consistently yields the best classification performance, achieving an
accuracy of 69\%. To complement classification, we incorporate a recommendation
module based on the cosine similarity of vectorized articles, enabling
efficient retrieval of related research papers. The proposed system directly
addresses the challenge of information overload in digital libraries and
demonstrates a scalable, data-driven solution to support literature discovery.

</details>


### [315] [AgentDR Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents](https://arxiv.org/abs/2510.05598)
*Mingdai Yang,Nurendra Choudhary,Jiangshu Du,Edward W. Huang,Philip S. Yu,Karthik Subbian,Danai Kourta*

Main category: cs.IR

TL;DR: 提出了一种新的LLM-agent框架AgenDR，结合传统推荐模型与大语言模型的推理能力，通过关系推理提升推荐效果，并在大规模商品目录上实现了优于基础工具两倍的排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的推荐框架难以处理不存在的商品幻觉和全目录排序问题，且未充分利用大语言模型在常识推理方面的能力来捕捉用户意图中的商品替代与互补关系。

Method: 设计AgenDR框架，将全目录排序任务交给传统模型，利用LLM整合多个推荐结果并基于用户历史进行替代与互补关系推理，从而实现更精准的推荐。

Result: 在三个公开杂货数据集上的实验表明，该框架在全排序性能上平均比基础工具提升两倍，并提出一种新的基于LLM的评估指标，联合衡量语义对齐和排序正确性。

Conclusion: AgenDR有效结合了传统推荐系统的可扩展性与LLM的推理能力，缓解了幻觉问题，提升了推荐的相关性和可解释性。

Abstract: Recent agent-based recommendation frameworks aim to simulate user behaviors
by incorporating memory mechanisms and prompting strategies, but they struggle
with hallucinating non-existent items and full-catalog ranking. Besides, a
largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning
to capture user intent through substitute and complement relationships between
items, which are usually implicit in datasets and difficult for traditional
ID-based recommenders to capture. In this work, we propose a novel LLM-agent
framework, AgenDR, which bridges LLM reasoning with scalable recommendation
tools. Our approach delegates full-ranking tasks to traditional models while
utilizing LLMs to (i) integrate multiple recommendation outputs based on
personalized tool suitability and (ii) reason over substitute and complement
relationships grounded in user history. This design mitigates hallucination,
scales to large catalogs, and enhances recommendation relevance through
relational reasoning. Through extensive experiments on three public grocery
datasets, we show that our framework achieves superior full-ranking
performance, yielding on average a twofold improvement over its underlying
tools. We also introduce a new LLM-based evaluation metric that jointly
measures semantic alignment and ranking correctness.

</details>


### [316] [Limitations of Current Evaluation Practices for Conversational Recommender Systems and the Potential of User Simulation](https://arxiv.org/abs/2510.05624)
*Nolwenn Bernard,Krisztian Balog*

Main category: cs.IR

TL;DR: 本文批判性地分析了对话式推荐系统（CRS）现有评估方法的局限性，指出过度依赖静态测试集和评估指标不足的问题，并通过用户模拟生成动态交互数据，提出基于奖励/成本框架的新评估指标，实验显示其与人类评估具有更高的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的CRS评估方法因系统的交互特性而面临挑战，尤其表现为静态测试集和不充分的评估指标无法真实反映用户满意度，因此需要更可靠、动态的评估方式。

Method: 分析九个现有CRS的真实用户交互数据，揭示自报用户满意度与文献中性能评分之间的脱节；探索用户模拟方法生成动态交互数据；提出基于通用奖励/成本框架的新型评估指标。

Result: 发现当前评估结果与用户实际满意度存在显著脱节；用户模拟生成的动态数据结合新指标在系统排序上与人类评估表现出更高的相关性，显示出良好前景。

Conclusion: 用户模拟与基于奖励/成本的新评估指标为CRS评估提供了更贴近真实用户反馈的路径，是该领域的重要进展，但仍需在模拟技术和指标设计方面进一步研究。

Abstract: Research and development on conversational recommender systems (CRSs)
critically depends on sound and reliable evaluation methodologies. However, the
interactive nature of these systems poses significant challenges for automatic
evaluation. This paper critically examines current evaluation practices and
identifies two key limitations: the over-reliance on static test collections
and the inadequacy of existing evaluation metrics. To substantiate this
critique, we analyze real user interactions with nine existing CRSs and
demonstrate a striking disconnect between self-reported user satisfaction and
performance scores reported in prior literature. To address these limitations,
this work explores the potential of user simulation to generate dynamic
interaction data, offering a departure from static datasets. Furthermore, we
propose novel evaluation metrics, based on a general reward/cost framework,
designed to better align with real user satisfaction. Our analysis of different
simulation approaches provides valuable insights into their effectiveness and
reveals promising initial results, showing improved correlation with system
rankings compared to human evaluation. While these findings indicate a
significant step forward in CRS evaluation, we also identify areas for future
research and refinement in both simulation techniques and evaluation metrics.

</details>


### [317] [How public datasets constrain the development of diversity-aware news recommender systems, and what law could do about it](https://arxiv.org/abs/2510.05952)
*Max van Drunen,Sanne Vrijenhoek*

Main category: cs.IR

TL;DR: 本文探讨了新闻推荐系统在民主社会中如何通过基于编辑价值（特别是多样性）的推荐来替代单纯以用户参与度为导向的推荐，并强调了实现多样性的关键在于训练现代新闻推荐系统所需的数据集。文章提出了支持多样性感知推荐系统所需数据集应包含的信息，评估了现有公开数据集的局限性，并探讨了欧洲法律和政策如何为研究人员提供获取必要数据的结构性途径。


<details>
  <summary>Details</summary>
Motivation: 当前新闻推荐系统多以用户参与度为核心，可能削弱媒体在民主社会中的作用；而基于编辑价值（如多样性）的推荐系统缺乏实际数据支持，理论与技术实践之间存在鸿沟。

Method: 首先分析规范性文献中多样性感知新闻推荐系统所需的数据特征，评估现有公开数据集的适用性与不足；其次，从欧洲法律与政策角度探讨如何为研究者提供系统性数据访问机制。

Result: 明确了支持多样性推荐系统所需数据的关键信息要素，指出现有数据集在内容属性、来源多样性和语义标注方面的局限性，并提出利用欧洲数据治理法规（如DSA）建立合法数据共享框架的可能性。

Conclusion: 要实现真正支持新闻多样性的推荐系统，必须解决数据可用性问题；结合规范性目标与法律政策手段，可为构建多样性感知的新闻推荐系统提供可行路径。

Abstract: News recommender systems increasingly determine what news individuals see
online. Over the past decade, researchers have extensively critiqued
recommender systems that prioritise news based on user engagement. To offer an
alternative, researchers have analysed how recommender systems could support
the media's ability to fulfil its role in democratic society by recommending
news based on editorial values, particularly diversity. However, there
continues to be a large gap between normative theory on how news recommender
systems should incorporate diversity, and technical literature that designs
such systems. We argue that to realise diversity-aware recommender systems in
practice, it is crucial to pay attention to the datasets that are needed to
train modern news recommenders. We aim to make two main contributions. First,
we identify the information a dataset must include to enable the development of
the diversity-aware news recommender systems proposed in normative literature.
Based on this analysis, we assess the limitations of currently available public
datasets, and show what potential they do have to expand research into
diversity-aware recommender systems. Second, we analyse why and how European
law and policy can be used to provide researchers with structural access to the
data they need to develop diversity-aware news recommender systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [318] [Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](https://arxiv.org/abs/2510.05106)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 本文提出了一种基于信息论的系统提示中规则编码分析方法，揭示了锚点冗余与注意力熵之间的权衡关系，并通过动态规则验证架构证明热重载经验证的规则集可提高合规输出的概率。


<details>
  <summary>Details</summary>
Motivation: 为了提升基于大语言模型的安全关键型智能体的可靠性，需要超越简单提示工程的方法来确保其在复杂和动态环境中的合规性和安全性。

Method: 采用信息论方法分析不同规则格式对注意力机制的影响，形式化多种注意力架构并建立指针保真度的理论界限，结合动态规则验证架构进行验证。

Result: 发现低句法熵和高集中锚点能降低注意力熵并提升指针保真度，但存在锚点冗余与注意力熵之间的根本权衡；提出了热重载验证规则集可提高合规输出渐近概率的形式化证明。

Conclusion: 必须采用原则性的锚点设计和双重执行机制，以抵御提示注入攻击并在动态环境中维持LLM智能体的合规性。

Abstract: The design of safety-critical agents based on large language models (LLMs)
requires more than simple prompt engineering. This paper presents a
comprehensive information-theoretic analysis of how rule encodings in system
prompts influence attention mechanisms and compliance behaviour. We demonstrate
that rule formats with low syntactic entropy and highly concentrated anchors
reduce attention entropy and improve pointer fidelity, but reveal a fundamental
trade-off between anchor redundancy and attention entropy that previous work
failed to recognize. Through formal analysis of multiple attention
architectures including causal, bidirectional, local sparse, kernelized, and
cross-attention mechanisms, we establish bounds on pointer fidelity and show
how anchor placement strategies must account for competing fidelity and entropy
objectives. Combining these insights with a dynamic rule verification
architecture, we provide a formal proof that hot reloading of verified rule
sets increases the asymptotic probability of compliant outputs. These findings
underscore the necessity of principled anchor design and dual enforcement
mechanisms to protect LLM-based agents against prompt injection attacks while
maintaining compliance in evolving domains.

</details>


### [319] [Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study](https://arxiv.org/abs/2510.05107)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了一种名为结构化认知循环（SCL）的新架构，通过将推理、记忆和控制分离来提升大语言模型作为自主代理在多步任务中的可靠性与可追溯性。实验表明，SCL在任务成功率和目标保真度上优于现有提示方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代理框架通常将推理、记忆和控制耦合在单一提示中，导致多步任务中连贯性和可预测性下降，因此需要一种更清晰的架构设计来提升性能和可解释性。

Method: 提出结构化认知循环（SCL），其中语言模型仅负责推理，记忆由外部系统维护，执行由轻量级控制器在目标导向循环中管理，实现功能解耦。

Result: 在360个测试回合中，SCL的任务成功率平均达86.3%，显著高于基线模型的70-77%；同时表现出更高的目标保真度、更少的冗余调用、更可靠的中间状态复用以及更少的 unsupported 断言。消融实验显示外部记忆和控制模块各自独立贡献性能提升。

Conclusion: 架构上的功能分离可在不依赖更大模型或更复杂提示的情况下提升代理系统的可靠性与可追溯性，为未来多模型、长周期、多模态及协作场景的研究提供了方向。

Abstract: Large language models have advanced natural language understanding and
generation, yet their use as autonomous agents raises architectural challenges
for multi-step tasks. Existing frameworks often intertwine inference, memory,
and control in a single prompt, which can reduce coherence and predictability.
The Structured Cognitive Loop (SCL) is introduced as an alternative
architecture that separates these functions. In SCL, the language model is
dedicated to inference, memory is maintained externally, and execution is
guided by a lightweight controller within a goal-directed loop. This design
offloads cognitive load from the model and allows intermediate results to be
stored, revisited, and checked before actions are taken, providing a clearer
basis for traceability and evaluation.
  We evaluate SCL against prompt-based baselines including ReAct and common
LangChain agents across three scenarios: temperature-based travel planning,
email drafting with conditional send, and constraint-guided image generation.
All systems share the same base model and tools under matched decoding
settings. Across 360 episodes, SCL shows modest but consistent improvements.
Task success averages 86.3 percent compared with 70-77 percent for baselines.
Goal fidelity is higher, redundant calls are fewer, intermediate states are
reused more reliably, and unsupported assertions per 100 tool calls are
reduced. Ablations show that external memory and control each contribute
independently, and decoding sweeps confirm stability of the effects.
  These results suggest that architectural separation can improve reliability
and traceability without relying on larger models or heavier prompts. The
findings are preliminary and intended to guide extended studies with additional
models, longer horizons, multimodal tasks, and collaborative settings.

</details>


### [320] [Optimization Modeling via Semantic Anchored Alignment](https://arxiv.org/abs/2510.05115)
*Yansen Zhang,Qingcan Kang,Yujie Chen,Yufei Wang,Xiongwei Han,Tao Zhong,Mingxuan Yuan,Chen Ma*

Main category: cs.AI

TL;DR: 提出SAC-Opt框架，通过语义锚点引导的反向修正提升大模型在优化建模中生成代码的准确性，显著提高语义一致性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖求解器反馈进行单次生成和有限修正，难以发现语义错误，导致生成逻辑错误但语法正确的代码。

Method: SAC-Opt框架通过将原始语义锚点与生成代码重建的语义对齐，仅修正不匹配部分，实现细粒度、反向引导的迭代修正，无需额外训练或监督。

Result: 在七个公开数据集上实验表明，SAC-Opt平均建模准确率提升7.8%，在ComplexLP数据集上最高提升21.9%。

Conclusion: 基于语义锚点的修正机制对LLM驱动的优化建模至关重要，能有效确保问题意图到可执行代码的保真转换。

Abstract: Large language models (LLMs) have opened new paradigms in optimization
modeling by enabling the generation of executable solver code from natural
language descriptions. Despite this promise, existing approaches typically
remain solver-driven: they rely on single-pass forward generation and apply
limited post-hoc fixes based on solver error messages, leaving undetected
semantic errors that silently produce syntactically correct but logically
flawed models. To address this challenge, we propose SAC-Opt, a backward-guided
correction framework that grounds optimization modeling in problem semantics
rather than solver feedback. At each step, SAC-Opt aligns the original semantic
anchors with those reconstructed from the generated code and selectively
corrects only the mismatched components, driving convergence toward a
semantically faithful model. This anchor-driven correction enables fine-grained
refinement of constraint and objective logic, enhancing both fidelity and
robustness without requiring additional training or supervision. Empirical
results on seven public datasets demonstrate that SAC-Opt improves average
modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP
dataset. These findings highlight the importance of semantic-anchored
correction in LLM-based optimization workflows to ensure faithful translation
from problem intent to solver-executable code.

</details>


### [321] [Structuring Reasoning for Complex Rules Beyond Flat Representations](https://arxiv.org/abs/2510.05134)
*Zhihao Yang,Ancheng Xu,Jingpeng Li,Liang Yan,Jiehui Zhou,Zhen Qin,Hengyun Chang,Ahmadreza Argha,Hamid Alinejad-Rokny,Minghuan Tan,Yujun Cai,Min Yang*

Main category: cs.AI

TL;DR: 提出动态裁决模板（DAT），通过三阶段方法（定性分析、证据收集、裁决）提升大语言模型在复杂规则系统中的推理能力，优于传统思维链方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂规则时因缺乏结构化推理而容易忽略关键依赖，导致推理偏差。

Method: 设计受人类专家启发的三阶段框架：定性分析、基于占位符模板的证据收集与规则验证、综合判断形成结论。

Result: 实验表明DAT在复杂规则任务中持续优于传统CoT方法，并使小模型性能媲美甚至超过更大模型。

Conclusion: DAT提供了一种高效、系统的规则推理框架，显著提升模型在复杂规则环境下的准确性和鲁棒性。

Abstract: Large language models (LLMs) face significant challenges when processing
complex rule systems, as they typically treat interdependent rules as
unstructured textual data rather than as logically organized frameworks. This
limitation results in reasoning divergence, where models often overlook
critical rule dependencies essential for accurate interpretation. Although
existing approaches such as Chain-of-Thought (CoT) reasoning have shown
promise, they lack systematic methodologies for structured rule processing and
are particularly susceptible to error propagation through sequential reasoning
chains. To address these limitations, we propose the Dynamic Adjudication
Template (DAT), a novel framework inspired by expert human reasoning processes.
DAT structures the inference mechanism into three methodical stages:
qualitative analysis, evidence gathering, and adjudication. During the
qualitative analysis phase, the model comprehensively evaluates the contextual
landscape. The subsequent evidence gathering phase involves the targeted
extraction of pertinent information based on predefined template elements
([placeholder]), followed by systematic verification against applicable rules.
Finally, in the adjudication phase, the model synthesizes these validated
components to formulate a comprehensive judgment. Empirical results demonstrate
that DAT consistently outperforms conventional CoT approaches in complex
rule-based tasks. Notably, DAT enables smaller language models to match, and in
some cases exceed, the performance of significantly larger LLMs, highlighting
its efficiency and effectiveness in managing intricate rule systems.

</details>


### [322] [Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework](https://arxiv.org/abs/2510.05158)
*Xin He,Liangliang You,Hongduan Tian,Bo Han,Ivor Tsang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: Lang-PINN是一个基于大语言模型的多智能体系统，能够从自然语言任务描述直接构建可训练的物理信息神经网络（PINN），实现了端到端的自动化建模。


<details>
  <summary>Details</summary>
Motivation: 构建PINN目前仍费时费力，且容易出错，现有基于大语言模型的方法通常只解决部分步骤，缺乏端到端的解决方案。

Method: 提出Lang-PINN，由四个协同工作的智能体组成：PDE智能体将任务描述解析为符号化PDE，PINN智能体选择网络结构，Code智能体生成模块化代码，Feedback智能体执行并诊断错误以迭代优化。

Result: 实验表明，Lang-PINN相比基线方法显著降低了误差（MSE减少3-5个数量级），端到端执行成功率提高50%以上，时间开销减少达74%。

Conclusion: Lang-PINN实现了从自然语言到可执行PINN代码的自动化流程，提升了PINN构建的准确性、鲁棒性和效率。

Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for
solving partial differential equations (PDEs), but constructing a usable PINN
remains labor-intensive and error-prone. Scientists must interpret problems as
PDE formulations, design architectures and loss functions, and implement stable
training pipelines. Existing large language model (LLM) based approaches
address isolated steps such as code generation or architecture suggestion, but
typically assume a formal PDE is already specified and therefore lack an
end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system
that builds trainable PINNs directly from natural language task descriptions.
Lang-PINN coordinates four complementary agents: a PDE Agent that parses task
descriptions into symbolic PDEs, a PINN Agent that selects architectures, a
Code Agent that generates modular implementations, and a Feedback Agent that
executes and diagnoses errors for iterative refinement. This design transforms
informal task statements into executable and verifiable PINN code. Experiments
show that Lang-PINN achieves substantially lower errors and greater robustness
than competitive baselines: mean squared error (MSE) is reduced by up to 3--5
orders of magnitude, end-to-end execution success improves by more than 50\%,
and reduces time overhead by up to 74\%.

</details>


### [323] [An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem](https://arxiv.org/abs/2510.05153)
*Zhangchi Liu*

Main category: cs.AI

TL;DR: 本文通过算法信息论重新定义符号接地问题，提出意义的形成受限于信息理论极限，并证明纯符号系统无法接地几乎所有可能的世界。


<details>
  <summary>Details</summary>
Motivation: 统一哥德尔自指观点与无免费午餐定理在符号接地问题中的视角，提供一个形式化、可计算的框架来理解意义的生成。

Method: 将符号系统建模为通用图灵机，将接地定义为信息压缩过程，利用算法信息论和Chaitin不完备性定理进行四阶段论证。

Result: 证明了纯符号系统无法压缩绝大多数随机世界；静态接地系统不完整；接地行为不可推断；任何算法学习过程无法理解超出其复杂性的世界。

Conclusion: 意义是系统不断尝试克服自身信息理论局限性的开放过程，符号接地本质上受计算与信息压缩极限的约束。

Abstract: This paper provides a definitive, unifying framework for the Symbol Grounding
Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT).
We demonstrate that the grounding of meaning is a process fundamentally
constrained by information-theoretic limits, thereby unifying the G\"odelian
(self-reference) and No Free Lunch (statistical) perspectives. We model a
symbolic system as a universal Turing machine and define grounding as an act of
information compression. The argument proceeds in four stages. First, we prove
that a purely symbolic system cannot ground almost all possible "worlds" (data
strings), as they are algorithmically random and thus incompressible. Second,
we show that any statically grounded system, specialized for compressing a
specific world, is inherently incomplete because an adversarial, incompressible
world relative to the system can always be constructed. Third, the "grounding
act" of adapting to a new world is proven to be non-inferable, as it requires
the input of new information (a shorter program) that cannot be deduced from
the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to
prove that any algorithmic learning process is itself a finite system that
cannot comprehend or model worlds whose complexity provably exceeds its own.
This establishes that meaning is the open-ended process of a system perpetually
attempting to overcome its own information-theoretic limitations.

</details>


### [324] [In-the-Flow Agentic System Optimization for Effective Planning and Tool Use](https://arxiv.org/abs/2510.05592)
*Zhuofeng Li,Haoxiang Zhang,Seungju Han,Sheng Liu,Jianwen Xie,Yu Zhang,Yejin Choi,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 本文提出了AgentFlow，一种可训练的、在流程中的代理框架，通过协调四个模块（规划器、执行器、验证器和生成器）并在多轮循环中直接优化规划器，解决了现有工具增强方法在长视野和多样化工具下的扩展性和泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的语言模型方法通常采用单一整体策略，在处理长视野和多样化工具时扩展性差，且难以泛化到新场景；而大多数代理系统缺乏与多轮交互动态结合的在线训练机制。

Method: 提出AgentFlow框架，包含四个协同工作的模块，并引入Flow-GRPO算法，将多轮优化转化为一系列单轮策略更新，利用轨迹级结果反馈和组归一化优势来实现在线策略训练。

Result: 在十个基准测试中，基于7B规模模型的AgentFlow相比基线平均准确率提升显著：搜索任务提升14.9%，代理任务提升14.0%，数学任务提升14.5%，科学任务提升4.1%，甚至超越了如GPT-4o等更大规模的专有模型。

Conclusion: AgentFlow通过在流程中进行策略优化，有效提升了长视野任务中的规划能力、工具调用可靠性和模型可扩展性，为语言模型的代理能力提供了新的训练范式。

Abstract: Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

</details>


### [325] [Representation Potentials of Foundation Models for Multimodal Alignment: A Survey](https://arxiv.org/abs/2510.05184)
*Jianglin Lu,Hailing Wang,Yi Xu,Yizhou Wang,Kuo Yang,Yun Fu*

Main category: cs.AI

TL;DR: 本文综述了基础模型的表示潜力，即其在单模态中捕捉任务特定信息并在跨模态间对齐和统一的能力。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在不同架构和模态下表示的相似性及其跨模态对齐的潜力。

Method: 通过回顾代表性基础模型及其关键度量指标，综合来自视觉、语言、语音、多模态和神经科学领域的实证研究证据。

Result: 基础模型在其表示空间中常表现出结构规律性和语义一致性，具备强大的跨模态迁移与对齐能力。

Conclusion: 基础模型具有显著的表示潜力，但其发展仍面临若干开放问题和挑战。

Abstract: Foundation models learn highly transferable representations through
large-scale pretraining on diverse data. An increasing body of research
indicates that these representations exhibit a remarkable degree of similarity
across architectures and modalities. In this survey, we investigate the
representation potentials of foundation models, defined as the latent capacity
of their learned representations to capture task-specific information within a
single modality while also providing a transferable basis for alignment and
unification across modalities. We begin by reviewing representative foundation
models and the key metrics that make alignment measurable. We then synthesize
empirical evidence of representation potentials from studies in vision,
language, speech, multimodality, and neuroscience. The evidence suggests that
foundation models often exhibit structural regularities and semantic
consistencies in their representation spaces, positioning them as strong
candidates for cross-modal transfer and alignment. We further analyze the key
factors that foster representation potentials, discuss open questions, and
highlight potential challenges.

</details>


### [326] [Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG](https://arxiv.org/abs/2510.06002)
*Hudson de Martim*

Main category: cs.AI

TL;DR: 本文提出了SAT-Graph API，一种用于结构化时间图知识库的可验证查询执行层，通过原子性、可组合和可审计的操作实现高精度检索与因果追溯，提升法律领域检索增强生成的透明性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准的检索增强生成在法律领域存在难以可靠查询结构化知识且易破坏其确定性的问题，缺乏对高风险领域可解释AI（XAI）需求的支持。

Method: 设计了基于规范动作（原子、可组合、可审计）的SAT-Graph API，分离概率性发现与确定性检索，并结合规划器引导的代理将复杂查询分解为有向无环图（DAG）形式的动作序列。

Result: 实现了高精度混合搜索、鲁棒引用解析、特定时间点版本检索和可审计的因果追踪，使检索过程透明化、可审计。

Conclusion: 该两层架构有效解决了法律领域中知识查询的可靠性与可解释性问题，显著提升了结构化时间图RAG系统的实用性与可信度。

Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.

</details>


### [327] [RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases](https://arxiv.org/abs/2510.05764)
*Lang Qin,Zijian Gan,Xu Cao,Pengcheng Jiang,Yankai Jiang,Jiawei Han,Kaishun Wu,Jintai Chen*

Main category: cs.AI

TL;DR: RareAgent是一种自演化的多智能体系统，通过主动的证据搜索推理而非被动模式识别来解决罕见病药物重定位中缺乏先验关联的挑战。


<details>
  <summary>Details</summary>
Motivation: 在药物与目标疾病之间缺乏先验关联的情况下，传统知识图谱补全和消息传递GNN难以有效学习和传播信号，导致性能不佳。

Method: RareAgent通过组织任务特定的对抗性辩论，让智能体从不同角度动态构建证据图以支持、反驳或推断假设，并通过自演化循环分析推理策略，生成文本反馈优化智能体策略，同时将成功的推理路径提炼为可迁移的启发式规则。

Result: 全面评估表明，RareAgent相比现有推理基线方法在适应症AUPRC上提升了18.1%，并提供了与临床证据一致的透明推理链。

Conclusion: RareAgent通过主动推理和自演化机制显著提升了罕见病药物重定位的准确性和可解释性，为无先验关联场景下的药物发现提供了新范式。

Abstract: Computational drug repurposing for rare diseases is especially challenging
when no prior associations exist between drugs and target diseases. Therefore,
knowledge graph completion and message-passing GNNs have little reliable signal
to learn and propagate, resulting in poor performance. We present RareAgent, a
self-evolving multi-agent system that reframes this task from passive pattern
recognition to active evidence-seeking reasoning. RareAgent organizes
task-specific adversarial debates in which agents dynamically construct
evidence graphs from diverse perspectives to support, refute, or entail
hypotheses. The reasoning strategies are analyzed post hoc in a
self-evolutionary loop, producing textual feedback that refines agent policies,
while successful reasoning paths are distilled into transferable heuristics to
accelerate future investigations. Comprehensive evaluations reveal that
RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and
provides a transparent reasoning chain consistent with clinical evidence.

</details>


### [328] [Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture](https://arxiv.org/abs/2510.05187)
*Mohamed El-Dosuky*

Main category: cs.AI

TL;DR: 本文提出了一种面向物联网（IoT）的实时语义框架，包含六个层次：感知、语义标注、互操作性、传输、语义推理和应用层，旨在提升数据理解与语义完整性，特别适用于农业等动态环境。


<details>
  <summary>Details</summary>
Motivation: 物联网在农业等应用中面临数据收集和语义理解的挑战，缺乏对数据含义和来源的有效认知，限制了其智能化发展。

Method: 设计了一个六层语义框架，引入语义标注、本体互操作算法（包括文件标准化和同义词识别）、不确定性推理方法（如模糊逻辑、Dempster-Shafer理论和贝叶斯网络），并通过WiFi、Zigbee等技术实现数据传输，最终通过GUI支持用户交互。

Result: 该框架实现了对原始传感器数据的语义增强、标准化处理和跨设备互操作，并能基于已有数据进行知识推理，支持实时监控和决策。

Conclusion: 所提出的语义框架有效提升了物联网系统的语义理解能力与数据互操作性，结合不确定性推理技术，为农业等复杂动态场景下的智能物联网应用提供了可行且鲁棒的解决方案。

Abstract: The Internet of Things (IoT) has revolutionized various applications
including agriculture, but it still faces challenges in data collection and
understanding. This paper proposes a real-time framework with three additional
semantic layers to help IoT devices and sensors comprehend data meaning and
source. The framework consists of six layers: perception, semantic annotation,
interoperability, transportation, semantic reasoning, and application, suitable
for dynamic environments. Sensors collect data in the form of voltage, which is
then processed by microprocessors or microcontrollers in the semantic
annotation and preprocessing layer. Metadata is added to the raw data,
including the purpose, ID number, and application. Two semantic algorithms are
proposed in the semantic interoperability and ontologies layer: the
interoperability semantic algorithm for standardizing file types and the
synonym identification algorithm for identifying synonyms. In the
transportation layer, raw data and metadata are sent to other IoT devices or
cloud computing platforms using techniques like WiFi, Zigbee networks,
Bluetooth, and mobile communication networks. A semantic reasoning layer is
proposed to infer new knowledge from the existing data, using fuzzy logic,
Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI)
is proposed in the application layer to help users communicate with and monitor
IoT sensors, devices, and new knowledge inferred. This framework provides a
robust solution for managing IoT data, ensuring semantic completeness, and
enabling real-time knowledge inference. The integration of uncertainty
reasoning methods and semantic interoperability techniques makes this framework
a valuable tool for advancing IoT applications in general and in agriculture in
particular.

</details>


### [329] [Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents](https://arxiv.org/abs/2510.05188)
*Wenda Xie,Chao Guo,Yanqing Jing. Junle Wang,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 提出了一种基于分治策略的多层次LLM代理方法Dramaturge，用于改进长篇叙事脚本的质量，通过全局审查、场景级审查和协调修订阶段实现上下文一致的迭代优化。


<details>
  <summary>Details</summary>
Motivation: 单次生成难以保证长叙事文本的质量，现有方法在修改时易引入局部与整体不一致的问题，缺乏对全局结构和细节问题的系统性修正能力。

Method: 设计了三阶段的分层多代理系统：全局审查、场景级审查和分层协调修订，采用自上而下的任务流和由粗到细的迭代流程，利用多个LLM代理协同工作。

Result: 实验表明，Dramaturge在剧本整体质量和场景细节方面均显著优于所有基线方法，且具有即插即用特性，可集成到现有生成系统中。

Conclusion: 该方法有效解决了长叙事生成中全局一致性与局部修改的协调难题，为LLM驱动的创意写作提供了可扩展的改进框架。

Abstract: Although LLMs have been widely adopted for creative content generation, a
single-pass process often struggles to produce high-quality long narratives.
How to effectively revise and improve long narrative scripts like scriptwriters
remains a significant challenge, as it demands a comprehensive understanding of
the entire context to identify global structural issues and local detailed
flaws, as well as coordinating revisions at multiple granularities and
locations. Direct modifications by LLMs typically introduce inconsistencies
between local edits and the overall narrative requirements. To address these
issues, we propose Dramaturge, a task and feature oriented divide-and-conquer
approach powered by hierarchical multiple LLM agents. It consists of a Global
Review stage to grasp the overall storyline and structural issues, a
Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a
Hierarchical Coordinated Revision stage that coordinates and integrates
structural and detailed improvements throughout the script. The top-down task
flow ensures that high-level strategies guide local modifications, maintaining
contextual consistency. The review and revision workflow follows a
coarse-to-fine iterative process, continuing through multiple rounds until no
further substantive improvements can be made. Comprehensive experiments show
that Dramaturge significantly outperforms all baselines in terms of
script-level overall quality and scene-level details. Our approach is
plug-and-play and can be easily integrated into existing methods to improve the
generated scripts.

</details>


### [330] [Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response](https://arxiv.org/abs/2510.05196)
*Daqian Shi,Xiaolei Diao,Jinge Wu,Honghan Wu,Xiongfeng Tang,Felix Naughton,Paulina Bondaronek*

Main category: cs.AI

TL;DR: 提出了一种基于图的推理框架，结合大语言模型与结构化人口属性及非结构化公众反馈，用于公共卫生事件中的人群健康监测。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理大规模半结构化数据时效率低下，且难以泛化，需要更高效、可解释的分析方法以支持公共卫生决策。

Method: 构建一个弱监督的图基推理框架，将大语言模型与结构化人口特征和非结构化公众反馈融合，动态建模公民需求为‘需求感知图’，实现人群特异性分析。

Result: 在真实世界数据集上的初步实验验证了该方法的可行性，能够生成可解释的洞察，支持响应式卫生政策制定。

Conclusion: 该方法为资源受限的临床和政府环境提供了可扩展的智能人群健康监测解决方案。

Abstract: Timely and accurate analysis of population-level data is crucial for
effective decision-making during public health emergencies such as the COVID-19
pandemic. However, the massive input of semi-structured data, including
structured demographic information and unstructured human feedback, poses
significant challenges to conventional analysis methods. Manual expert-driven
assessments, though accurate, are inefficient, while standard NLP pipelines
often require large task-specific labeled datasets and struggle with
generalization across diverse domains. To address these challenges, we propose
a novel graph-based reasoning framework that integrates large language models
with structured demographic attributes and unstructured public feedback in a
weakly supervised pipeline. The proposed approach dynamically models evolving
citizen needs into a need-aware graph, enabling population-specific analyses
based on key features such as age, gender, and the Index of Multiple
Deprivation. It generates interpretable insights to inform responsive health
policy decision-making. We test our method using a real-world dataset, and
preliminary experimental results demonstrate its feasibility. This approach
offers a scalable solution for intelligent population health monitoring in
resource-constrained clinical and governmental settings.

</details>


### [331] [Efficient Prediction of Pass@k Scaling in Large Language Models](https://arxiv.org/abs/2510.05197)
*Joshua Kazdan,Rylan Schaeffer,Youssef Allouah,Colin Sullivan,Kyssen Yu,Noam Levi,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 提出了一种基于beta-binomial分布的鲁棒估计框架和动态采样策略，以在有限数据下更准确地预测大规模尝试下AI模型的能力与风险。


<details>
  <summary>Details</summary>
Motivation: 由于重复采样可显著提升前沿AI系统的能力与潜在危害，如何在小样本预算下准确预测模型在大规模尝试中的行为成为关键问题，尤其对模型提供方和监管机构至关重要。

Method: 引入一种鲁棒估计框架，采用beta-binomial分布来提高从有限数据中预测的准确性，并设计动态采样策略，将更多预算分配给更难的问题。

Result: 相比标准方法，所提方法在数据受限情况下具有更高的预测准确性，能更可靠地预测稀有风险和能力，且计算成本更低。

Conclusion: 该方法为评估AI系统的长期能力和安全风险提供了高效、可靠的工具，有助于模型开发与监管决策。

Abstract: Assessing the capabilities and risks of frontier AI systems is a critical
area of research, and recent work has shown that repeated sampling from models
can dramatically increase both. For instance, repeated sampling has been shown
to increase their capabilities, such as solving difficult math and coding
problems, but it has also been shown to increase their potential for harm, such
as being jailbroken. Such results raise a crucial question for both capability
and safety forecasting: how can one accurately predict a model's behavior when
scaled to a massive number of attempts, given a vastly smaller sampling budget?
This question is directly relevant to model providers, who serve hundreds of
millions of users daily, and to governmental regulators, who seek to prevent
harms. To answer this questions, we make three contributions. First, we find
that standard methods for fitting these laws suffer from statistical
shortcomings that hinder predictive accuracy, especially in data-limited
scenarios. Second, we remedy these shortcomings by introducing a robust
estimation framework, which uses a beta-binomial distribution to generate more
accurate predictions from limited data. Third, we propose a dynamic sampling
strategy that allocates a greater budget to harder problems. Combined, these
innovations enable more reliable prediction of rare risks and capabilities at a
fraction of the computational cost.

</details>


### [332] [Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment](https://arxiv.org/abs/2510.05283)
*Radha Gulhane,Sathish Reddy Indurthi*

Main category: cs.AI

TL;DR: 提出了一种混合奖励建模框架，结合模型驱动和规则驱动的奖励机制，提升多模态大语言模型在人类偏好对齐中的表现，尤其在数学推理任务上显著提升了16%。


<details>
  <summary>Details</summary>
Motivation: 现有单信号奖励方法难以充分捕捉人类偏好的多样性，缺乏跨任务的置信度校准，且依赖大量标注数据和训练成本。

Method: 构建一个融合模型-based奖励（基于合成与人工反馈）和规则-based奖励（领域启发式）的混合框架，并引入多维度奖励（如指令遵循）和广义长度惩罚奖励以稳定训练。

Result: 在多个多模态基准上实现一致提升，3B规模最佳模型在通用与数学推理任务上平均提升约9.5%，数学任务上达16%。

Conclusion: 该混合奖励框架灵活有效，显著提升MLLM在复杂任务（尤其是数学推理）上的性能，为强化学习对齐提供了低依赖、多维度的新路径。

Abstract: Aligning multimodal large language models (MLLMs) with human preferences
often relies on single-signal, model-based reward methods. Such monolithic
rewards often lack confidence calibration across domain-specific tasks, fail to
capture diverse aspects of human preferences, and require extensive data
annotation and reward model training. In this work, we propose a hybrid reward
modeling framework that integrates complementary reward paradigms: (i)
model-based rewards, where a learned reward model predicts scalar or vector
scores from synthetic and human feedback, and (ii) rule-based rewards, where
domain-specific heuristics provide explicit correctness signals with
confidence. Beyond accuracy, we further incorporate multi-aspect rewards to
enforce instruction adherence and introduce a generalized length-penalty reward
to stabilize training and improve performance. The proposed framework provides
a flexible and effective approach to aligning MLLMs through reinforcement
learning policy optimization. Our experiments show consistent improvements
across different multimodal benchmarks when applying hybrid and multi-aspect
reward modeling. Our best performing model in the 3B family achieves an overall
average improvement of ~9.5% across general and math reasoning tasks. Focusing
specifically on mathematical benchmarks, the model achieves a significant
average improvement of ~16%, highlighting its effectiveness in mathematical
reasoning and problem solving.

</details>


### [333] [BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions](https://arxiv.org/abs/2510.05318)
*Nan Huo,Xiaohan Xu,Jinyang Li,Per Jacobsson,Shipei Lin,Bowen Qin,Binyuan Hui,Xiaolong Li,Ge Qu,Shuzheng Si,Linheng Han,Edward Alexander,Xintong Zhu,Rui Qin,Ruihan Yu,Yiyao Jin,Feige Zhou,Weihao Zhong,Yun Chen,Hongyu Liu,Chenhao Ma,Fatma Ozcan,Yannis Papakonstantinou,Reynold Cheng*

Main category: cs.AI

TL;DR: 本文提出了BIRD-INTERACT，一个用于评估多轮文本到SQL生成任务的新型基准，强调动态交互、错误恢复和完整CRUD操作，揭示了当前大模型在复杂数据库助手任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能真实反映现实世界数据库应用中多轮交互的复杂性，如模糊查询、执行错误和用户需求变化，因此需要更贴近实际的评估环境。

Method: 构建了一个包含分层知识库、元数据文件和函数驱动用户模拟器的综合交互环境，设计了预定义协议（c-Interact）和开放式代理（a-Interact）两种评估模式，并覆盖完整CRUD操作的任务集。

Result: 实验显示GPT-5在c-Interact和a-Interact中分别仅完成8.67%和17.00%的任务，验证了该基准的挑战性；通过记忆嫁接和交互测试时扩展分析，证明了有效交互对复杂任务的重要性。

Conclusion: BIRD-INTERACT显著提升了多轮text-to-SQL任务的评估 realism 和难度，为开发具备自主交互能力的数据库助手提供了重要基准。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
single-turn text-to-SQL tasks, but real-world database applications
predominantly require multi-turn interactions to handle ambiguous queries,
execution errors, and evolving user requirements. Existing multi-turn
benchmarks fall short by treating conversation histories as static context or
limiting evaluation to read-only operations, failing to reflect
production-grade database assistant challenges. We introduce BIRD-INTERACT, a
benchmark that restores this realism through: (1) a comprehensive interaction
environment coupling each database with a hierarchical knowledge base, metadata
files, and a function-driven user simulator, enabling models to solicit
clarifications, retrieve knowledge, and recover from errors without human
supervision; (2) two evaluation settings consisting of a pre-defined
conversational protocol (c-Interact) and an open-ended agentic setting
(a-Interact) where models autonomously decide when to query the user simulator
or explore the environment; (3) a challenging task suite covering the full CRUD
spectrum for business-intelligence and operational use cases, guarded by
executable test cases. Each task features ambiguous and follow-up sub-tasks
requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600
tasks, up to 11,796 interactions) for comprehensive performance assessment, and
BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed
behavioral analysis and rapid method development. Our empirical results
highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in
c-Interact and 17.00% in a-Interact. Analysis via memory grafting and
Interaction Test-time Scaling validates the importance of effective interaction
for complex, dynamic text-to-SQL tasks.

</details>


### [334] [Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis](https://arxiv.org/abs/2510.05335)
*Oskar Wysocki,Magdalena Wysocka,Mauricio Jacobo,Harriet Unsworth,André Freitas*

Main category: cs.AI

TL;DR: M-Reason是一个用于生物医学领域（特别是癌症研究）中透明、基于代理的推理和证据整合的演示系统，利用大语言模型和模块化代理协调来自动化证据检索、评估和综合。


<details>
  <summary>Details</summary>
Motivation: 为了提高生物医学研究中证据整合的透明度、可追溯性和效率，特别是在复杂的癌症研究领域，需要一个能够自动化且可解释的多代理系统。

Method: M-Reason采用模块化的多代理架构，每个代理专注于特定类型的证据流，实现并行处理和细粒度分析；结合大语言模型进行推理，并集成确定性代码以验证结果，同时提供开放交互式用户界面供研究人员审计和探索整个工作流程。

Result: 系统在效率和输出一致性方面表现出显著提升，支持从源证据到最终结论的完整追溯，具备良好的可解释性和用户审计能力。

Conclusion: M-Reason不仅是一个实用的证据合成工具，也为科学研究所用的鲁棒多代理大语言模型系统提供了测试平台，具有推动透明化AI辅助科研的潜力。

Abstract: We present M-Reason, a demonstration system for transparent, agent-based
reasoning and evidence integration in the biomedical domain, with a focus on
cancer research. M-Reason leverages recent advances in large language models
(LLMs) and modular agent orchestration to automate evidence retrieval,
appraisal, and synthesis across diverse biomedical data sources. Each agent
specializes in a specific evidence stream, enabling parallel processing and
fine-grained analysis. The system emphasizes explainability, structured
reporting, and user auditability, providing complete traceability from source
evidence to final conclusions. We discuss critical tradeoffs between agent
specialization, system complexity, and resource usage, as well as the
integration of deterministic code for validation. An open, interactive user
interface allows researchers to directly observe, explore and evaluate the
multi-agent workflow. Our evaluation demonstrates substantial gains in
efficiency and output consistency, highlighting M-Reason's potential as both a
practical tool for evidence synthesis and a testbed for robust multi-agent LLM
systems in scientific research, available at https://m-reason.digitalecmt.com.

</details>


### [335] [Integrating Bayesian methods with neural network--based model predictive control: a review](https://arxiv.org/abs/2510.05338)
*Asli Karacelik*

Main category: cs.AI

TL;DR: 本文综述了贝叶斯方法在模型预测控制（MPC）中的应用，重点关注基于神经网络的建模、控制设计和不确定性量化，指出当前性能增益和鲁棒性提升的研究结果零散且缺乏一致性，呼吁建立标准化基准、消融研究和透明报告以评估贝叶斯技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法在MPC中用于处理不确定性，但现有研究结果分散、基线不一致、可靠性分析不足，难以评估其实际效果。

Method: 系统分析了采用贝叶斯方法的MPC相关研究，特别是基于神经网络的建模与控制设计，并评估其在实践中的实现方式。

Result: 发现尽管贝叶斯方法在捕捉和传播不确定性方面日益普及，但其在性能和鲁棒性方面的提升仍缺乏一致性和可重复性。

Conclusion: 应建立标准化基准、开展消融研究并加强透明报告，以更严谨地评估贝叶斯方法在MPC中的有效性。

Abstract: In this review, we assess the use of Bayesian methods in model predictive
control (MPC), focusing on neural-network-based modeling, control design, and
uncertainty quantification. We systematically analyze individual studies and
how they are implemented in practice. While Bayesian approaches are
increasingly adopted to capture and propagate uncertainty in MPC, reported
gains in performance and robustness remain fragmented, with inconsistent
baselines and limited reliability analyses. We therefore argue for standardized
benchmarks, ablation studies, and transparent reporting to rigorously determine
the effectiveness of Bayesian techniques for MPC.

</details>


### [336] [MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts](https://arxiv.org/abs/2510.05363)
*Abhinav Jain,Xinyu Yao,Thomas Reps,Christopher Jermaine*

Main category: cs.AI

TL;DR: 本文提出了一种名为MHA-RAG的新框架，通过软提示和多头注意力机制在有限数据下高效适应基础模型，相比标准RAG性能提升20点且推理成本降低10倍。


<details>
  <summary>Details</summary>
Motivation: 探索仅用文本表示示例是否为最有效、稳定的方法，并寻求更高效的领域适应方式。

Method: 引入MHA-RAG框架，使用软提示和示例顺序不变的模型架构，通过多头注意力机制生成提示。

Result: 在多个问答基准和模型规模上，MHA-RAG比标准RAG性能提高20点，推理成本降低10倍（GFLOPs）。

Conclusion: MHA-RAG在提升准确性的同时显著降低计算开销，是一种高效、稳定的少样本领域适应方法。

Abstract: Adapting Foundation Models to new domains with limited training data is
challenging and computationally expensive. While prior work has demonstrated
the effectiveness of using domain-specific exemplars as in-context
demonstrations, we investigate whether representing exemplars purely as text is
the most efficient, effective, and stable approach. We explore an alternative:
representing exemplars as soft prompts with an exemplar order invariant model
architecture. To this end, we introduce Multi-Head Attention
Retrieval-Augmented Generation (MHA-RAG), a framework with the number of
attention heads serving as a simple hyperparameter to control soft
prompt-generation across different tasks. Across multiple question-answering
benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over
standard RAG, while cutting inference costs by a factor of 10X
GFLOPs-delivering both higher accuracy and greater efficiency, invariant to
exemplar order.

</details>


### [337] [What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions](https://arxiv.org/abs/2510.05378)
*Reza Habibi,Seung Wan Ha,Zhiyu Lin,Atieh Kashani,Ala Shafia,Lakshana Lakshmanarajan,Chia-Fang Chung,Magy Seif El-Nasr*

Main category: cs.AI

TL;DR: 本研究基于符号互动理论，通过两项研究探讨人类与AI在对话中如何共同构建符号及其意义，发现社会语境下双方的双向符号 reinterpretation 是形成共享理解的关键。


<details>
  <summary>Details</summary>
Motivation: 为了实现有意义的人机协作，需要让AI不仅能处理语言，还能理解符号及其社会建构的意义，而现有AI常忽视对话中动态生成的符号解释。

Method: 基于符号互动论，开展两项实证研究，分析人类与对话式AI在互动过程中如何共同构建和调整符号意义。

Result: 发现参与者会根据AI提供的符号和解释调整其初始意义理解，尤其在引入社会语境时；同时，参与者将个人和社会价值观投射到互动中，逐步完善意义。共享理解源于双向的符号交换与再解释，而非简单共识。

Conclusion: 人类与AI的共享理解是通过动态、双向的符号互动形成的，这为设计更具社会智能的AI交互系统提供了新范式。

Abstract: Meaningful human-AI collaboration requires more than processing language; it
demands a deeper understanding of symbols and their socially constructed
meanings. While humans naturally interpret symbols through social interaction,
AI systems often miss the dynamic interpretations that emerge in conversation.
Drawing on Symbolic Interactionism theory, we conducted two studies to
investigate how humans and AI co-construct symbols and their meanings. Findings
provide empirical insights into how humans and conversational AI agents
collaboratively shape meanings during interaction. We show how participants
shift their initial definitions of meaning in response to the symbols and
interpretations suggested by the conversational AI agents, especially when
social context is introduced. We also observe how participants project their
personal and social values into these interactions, refining meanings over
time. These findings reveal that shared understanding does not emerge from mere
agreement but from the bi-directional exchange and reinterpretation of symbols,
suggesting new paradigms for human-AI interaction design.

</details>


### [338] [Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation](https://arxiv.org/abs/2510.05402)
*Ahmad Alsheikh,Andreas Fischer*

Main category: cs.AI

TL;DR: 提出一种基于教师-学生学习框架的方法，用于解决钢热处理后硬度预测中的逆问题，通过前向模型指导反向模型的训练，显著提高了预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 由于不同输入参数组合可能导致相同的硬度值，钢热处理后的硬度预测存在多对一的模糊性，使得从目标硬度反推输入参数的逆问题极具挑战性。

Method: 采用教师-学生学习框架：首先训练一个前向模型（教师）根据13个冶金输入特征预测最终硬度；然后训练一个反向模型（学生），通过教师模型的反馈在迭代监督循环中优化学生模型以推断合理的输入配置。

Result: 在公开的回火钢数据集上评估表明，该方法相比基线回归和强化学习模型，在逆向预测精度上更高，且计算时间显著减少。

Conclusion: 所提出的教师-学生框架在材料科学的逆过程建模中具有高效性和有效性，适用于解决具有多对一特性的复杂回归任务。

Abstract: Predicting the final hardness of steel after heat treatment is a challenging
regression task due to the many-to-one nature of the process -- different
combinations of input parameters (such as temperature, duration, and chemical
composition) can result in the same hardness value. This ambiguity makes the
inverse problem, estimating input parameters from a desired hardness,
particularly difficult. In this work, we propose a novel solution using a
Teacher-Student learning framework. First, a forward model (Teacher) is trained
to predict final hardness from 13 metallurgical input features. Then, a
backward model (Student) is trained to infer plausible input configurations
from a target hardness value. The Student is optimized by leveraging feedback
from the Teacher in an iterative, supervised loop. We evaluate our method on a
publicly available tempered steel dataset and compare it against baseline
regression and reinforcement learning models. Results show that our
Teacher-Student framework not only achieves higher inverse prediction accuracy
but also requires significantly less computational time, demonstrating its
effectiveness and efficiency for inverse process modeling in materials science.

</details>


### [339] [AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems](https://arxiv.org/abs/2510.05432)
*Shambhavi Mishra,Gaurav Sahu,Marco Pedersoli,Laurent Charlin,Jose Dolz,Christopher Pal*

Main category: cs.AI

TL;DR: 本论文提出AInstein框架，用于评估大语言模型（LLMs）是否能仅依靠预训练知识自主解决AI研究问题，结果表明LLMs虽具备一定解决方案重现和创新能力，但其推理能力仍脆弱且受问题表述方式影响显著。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型的成功是源于真正的推理能力还是仅仅是记忆回放，明确其在无外部辅助下自主进行科学研究的能力边界。

Method: 从ICLR 2025投稿中提取高质量问题，构建专门的求解代理，通过迭代批评循环模拟科研中的提案、评审与修改过程，并采用LLM评分加人工核查的方式评估性能。

Result: 在1,214篇论文上测试发现，LLMs能够重新发现部分可行方案并偶尔提出新颖方法，但在不同问题表述下表现不稳定，成功率较低。

Conclusion: LLMs具备一定的自主科学问题解决潜力，但当前仍受限于推理的稳定性和鲁棒性，尚未达到可靠自主科研水平。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet it remains unclear whether such success reflects
genuine reasoning or sophisticated recall. We introduce AInstein, a framework
for testing whether LLMs can generate valid solutions to AI research problems
using only their pretrained parametric knowledge -- without domain-specific
fine-tuning, retrieval augmentation, or other external aids. Our approach
extracts distilled problem statements from high-quality ICLR 2025 submissions,
then tasks specialized solver agents with proposing and refining technical
solutions through iterative critique loops, mimicking the cycles of proposal,
review, and revision central to scientific inquiry. We evaluate AInstein on
1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),
using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by
targeted manual checks. Performance is assessed with three metrics: Success
Rate (does the solution address the problem?), Rediscovery (does it align with
human-proposed methods?), and Novelty (does it yield valid, original
approaches?). Our results reveal that while LLMs can rediscover feasible
solutions and occasionally propose creative alternatives, their problem-solving
ability remains fragile and highly sensitive to framing. These findings provide
the first large-scale evidence on the extent to which LLMs can act as
autonomous scientific problem-solvers, highlighting both their latent potential
and their current limitations.

</details>


### [340] [NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification](https://arxiv.org/abs/2510.05451)
*Fadi Al Machot,Fidaa Al Machot*

Main category: cs.AI

TL;DR: 提出一种结合Answer Set Programming（ASP）与基于transformer模型的混合神经符号框架，用于航空安全报告系统的多标签文本分类，通过规则增强和可微正则化提升模型的逻辑一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 深度transformer模型在多标签文本分类中表现优异，但在安全关键应用中可能违反专家认为重要的领域逻辑，因此需要提高模型的可信度和逻辑一致性。

Method: 将领域知识形式化为加权ASP规则，并通过Clingo求解器验证；利用规则进行数据增强生成逻辑一致的合成样本，并设计模糊逻辑正则项在微调过程中以可微方式强制满足规则。

Result: 相比强基线BCE，该方法在ASRS测试集上提升了micro-和macro-F1分数，并将规则违反减少最多86%，同时保持了模型可解释性与扩展性。

Conclusion: 这是首个将ASP推理、规则驱动增强和可微transformer训练相结合的大规模神经符号方法应用于ASRS报告分析，实现了更可靠的安全关键NLP系统。

Abstract: Deep transformer models excel at multi-label text classification but often
violate domain logic that experts consider essential, an issue of particular
concern in safety-critical applications. We propose a hybrid neuro-symbolic
framework that integrates Answer Set Programming (ASP) with transformer-based
learning on the Aviation Safety Reporting System (ASRS) corpus. Domain
knowledge is formalized as weighted ASP rules and validated using the Clingo
solver. These rules are incorporated in two complementary ways: (i) as
rule-based data augmentation, generating logically consistent synthetic samples
that improve label diversity and coverage; and (ii) as a fuzzy-logic
regularizer, enforcing rule satisfaction in a differentiable form during
fine-tuning. This design preserves the interpretability of symbolic reasoning
while leveraging the scalability of deep neural architectures. We further tune
per-class thresholds and report both standard classification metrics and
logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE)
baseline, our approach improves micro- and macro-F1 scores and achieves up to
an 86% reduction in rule violations on the ASRS test set. To the best of our
knowledge, this constitutes the first large-scale neuro-symbolic application to
ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and
differentiable transformer training for trustworthy, safety-critical NLP.

</details>


### [341] [Do Code Models Suffer from the Dunning-Kruger Effect?](https://arxiv.org/abs/2510.05457)
*Mukul Singh,Somya Chatterjee,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 该论文研究了在编程任务中，最先进的大语言模型（LLM）是否存在类似人类的“达宁-克鲁格效应”（DKE），即能力较弱的个体高估自身能力的现象。研究表明，AI模型在面对不熟悉或资源较少的编程语言时也表现出类似的过度自信，且模型能力越弱，这种偏差越强。


<details>
  <summary>Details</summary>
Motivation: 随着AI越来越多地与人类在创造性和技术领域协作，理解其认知局限和偏差对于建立可靠的人机协作至关重要。本文旨在探究AI模型是否表现出类似人类的认知偏差，特别是达宁-克鲁格效应。

Method: 通过分析多种编程语言下模型的置信度与实际表现，评估不同能力水平的LLM在常见与稀有编程语言中的行为差异，以检测DKE-like偏差的存在及其与模型能力的关系。

Result: 实验发现，能力较低的模型以及在罕见编程语言上运行的模型表现出更强的过度自信倾向，显示出与人类相似的DKE模式，且偏差强度与模型能力成反比。

Conclusion: AI模型在编码任务中表现出类人的达宁-克鲁格效应，说明其自信程度并不总与其真实能力一致，这一发现对人机协作中的信任校准具有重要意义。

Abstract: As artificial intelligence systems increasingly collaborate with humans in
creative and technical domains, questions arise about the cognitive boundaries
and biases that shape our shared agency. This paper investigates the
Dunning-Kruger Effect (DKE), the tendency for those with limited competence to
overestimate their abilities in state-of-the-art LLMs in coding tasks. By
analyzing model confidence and performance across a diverse set of programming
languages, we reveal that AI models mirror human patterns of overconfidence,
especially in unfamiliar or low-resource domains. Our experiments demonstrate
that less competent models and those operating in rare programming languages
exhibit stronger DKE-like bias, suggesting that the strength of the bias is
proportionate to the competence of the models.

</details>


### [342] [VAL-Bench: Measuring Value Alignment in Language Models](https://arxiv.org/abs/2510.05465)
*Aman Gupta,Denny O'Shea,Fazl Barez*

Main category: cs.AI

TL;DR: 本文提出了VAL-Bench，一个用于评估大语言模型在面对有争议的现实问题时是否保持一致价值观的新基准。该基准使用维基百科中的11.5万对对立提示来测试模型的价值一致性，并通过LLM-as-judge方法衡量响应之间的一致性或分歧。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要关注拒绝回答或预定义的安全违规，无法揭示模型在处理争议性问题时是否持有连贯的价值体系。因此需要一个新的基准来评估模型的价值一致性。

Method: 构建了包含11.5万个来自维基百科争议部分的对立提示对的数据集VAL-Bench，并利用LLM-as-judge方法评估模型在不同表述下的回应是否表现出一致的价值取向。

Result: 在多个主流开源和闭源模型上的实验表明，各模型在价值一致性方面表现差异显著，并揭示了安全策略（如拒绝回答）与表达更丰富价值体系之间的权衡。

Conclusion: VAL-Bench提供了一个可扩展、可复现的基准，能够系统地比较大语言模型在体现人类价值观方面的一致性和可靠性。

Abstract: Large language models (LLMs) are increasingly used for tasks where outputs
shape human decisions, so it is critical to test whether their responses
reflect consistent human values. Existing benchmarks mostly track refusals or
predefined safety violations, but these only check rule compliance and do not
reveal whether a model upholds a coherent value system when facing
controversial real-world issues. We introduce the \textbf{V}alue
\textbf{AL}ignment \textbf{Bench}mark (\textbf{VAL-Bench}), which evaluates
whether models maintain a stable value stance across paired prompts that frame
opposing sides of public debates. VAL-Bench consists of 115K such pairs from
Wikipedia's controversial sections. A well-aligned model should express similar
underlying views regardless of framing, which we measure using an LLM-as-judge
to score agreement or divergence between paired responses. Applied across
leading open- and closed-source models, the benchmark reveals large variation
in alignment and highlights trade-offs between safety strategies (e.g.,
refusals) and more expressive value systems. By providing a scalable,
reproducible benchmark, VAL-Bench enables systematic comparison of how reliably
LLMs embody human values.

</details>


### [343] [Vul-R2: A Reasoning LLM for Automated Vulnerability Repair](https://arxiv.org/abs/2510.05480)
*Xin-Cheng Wen,Zirui Lin,Yijun Yang,Cuiyun Gao,Deheng Ye*

Main category: cs.AI

TL;DR: 本文探讨了利用大语言模型进行自动漏洞修复的挑战，包括缺乏高质量的漏洞相关推理数据和难以在训练过程中验证中间修复过程。


<details>
  <summary>Details</summary>
Motivation: 由于软件漏洞数量激增，迫切需要自动漏洞修复（AVR）解决方案。现有的基于大语言模型的方法虽然表现优异，但在捕捉多样化的漏洞修复模式和训练过程中的反馈验证方面存在挑战。

Method: 将AVR问题形式化为序列生成任务，并使用大语言模型生成漏洞修复。研究关注如何通过引入漏洞相关的推理数据和改进训练反馈机制来提升修复效果。

Result: 现有方法主要依赖编码通用编程知识的基础模型，因缺乏漏洞相关的推理数据而难以捕捉多样的修复模式；同时，由于缺少可验证的中间执行反馈，强化学习等方法在训练中面临困难。

Conclusion: 为了提升自动漏洞修复的效果，未来的研究需要解决高质量漏洞推理数据的获取以及训练过程中中间修复步骤的可验证性问题。

Abstract: The exponential increase in software vulnerabilities has created an urgent
need for automatic vulnerability repair (AVR) solutions. Recent research has
formulated AVR as a sequence generation problem and has leveraged large
language models (LLMs) to address this problem. Typically, these approaches
prompt or fine-tune LLMs to generate repairs for vulnerabilities directly.
Although these methods show state-of-the-art performance, they face the
following challenges: (1) Lack of high-quality, vulnerability-related reasoning
data. Current approaches primarily rely on foundation models that mainly encode
general programming knowledge. Without vulnerability-related reasoning data,
they tend to fail to capture the diverse vulnerability repair patterns. (2)
Hard to verify the intermediate vulnerability repair process during LLM
training. Existing reinforcement learning methods often leverage intermediate
execution feedback from the environment (e.g., sandbox-based execution results)
to guide reinforcement learning training. In contrast, the vulnerability repair
process generally lacks such intermediate, verifiable feedback, which poses
additional challenges for model training.

</details>


### [344] [Decade-long Emission Forecasting with an Ensemble Model in Taiwan](https://arxiv.org/abs/2510.05548)
*Gordon Hung,Salinna Abdullah*

Main category: cs.AI

TL;DR: 本研究比较了21种常用时间序列模型在预测台湾碳排放中的表现，结合FFNN、SVM和RFR与线性回归的堆叠集成方法，实现了高精度且无过拟合的排放预测，并提供了未来十年的排放趋势，支持政策制定。


<details>
  <summary>Details</summary>
Motivation: 台湾人口密集且高度依赖化石燃料，导致严重的空气污染，尤其是二氧化碳排放问题突出，亟需准确的排放预测以支持环境政策制定。

Method: 研究系统比较了21种常见的时间序列模型（包括单变量和多变量方法），评估其在碳排放预测中的性能；采用前馈神经网络（FFNN）、支持向量机（SVM）和随机森林回归（RFR）作为基础模型，并通过自定义的堆叠泛化集成技术结合线性回归以提升预测鲁棒性。

Result: FFNN、SVM和RFR表现最佳，所提出的集成模型达到1.407的SMAPE，且未出现过拟合现象；研究进一步生成了未来十年的碳排放预测结果。

Conclusion: 该集成模型在碳排放预测中表现出高准确性与稳定性，为政策制定者提供了可靠的数据支持，有助于推动数据驱动的环境治理决策。

Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to
severe air pollution, with the most prevalent greenhouse gas being carbon
dioxide (CO2). There-fore, this study presents a reproducible and comprehensive
case study comparing 21 of the most commonly employed time series models in
forecasting emissions, analyzing both univariate and multivariate approaches.
Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM),
and Random Forest Regressor (RFR) achieved the best performances. To further
enhance robustness, the top performers were integrated with Linear Regression
through a custom stacked generalization en-semble technique. Our proposed
ensemble model achieved an SMAPE of 1.407 with no signs of overfitting.
Finally, this research provides an accurate decade-long emission projection
that will assist policymakers in making more data-driven decisions.

</details>


### [345] [MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption](https://arxiv.org/abs/2510.05580)
*Chen Li,Zhantao Yang,Han Zhang,Fangyi Chen,Chenchen Zhu,Anudeepsekhar Bolimera,Marios Savvides*

Main category: cs.AI

TL;DR: MetaVLA是一种统一且骨干无关的后训练框架，通过上下文感知的元共训练方法提升视觉-语言-动作模型在未见任务上的泛化能力，显著减少训练步骤和GPU时间，同时提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型通常需要任务特定的微调，且对未见任务泛化能力差，缺乏真正的通用性。

Method: 提出MetaVLA框架，引入基于注意力神经过程的轻量级元学习机制——上下文感知元共训练，将多种目标任务整合到单一微调阶段，并利用结构多样的辅助任务提升域内泛化能力。

Result: 在LIBERO基准上，使用六个辅助任务的MetaVLA相比OpenVLA在长视野任务上性能最高提升8.0%，训练步数从240K减少到75K，GPU时间减少约76%。

Conclusion: MetaVLA实现了高效、可扩展且低资源消耗的后训练，为构建通用具身智能体提供了可行路径。

Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet
remain far from true generalists-they often require task-specific fine-tuning,
and generalize poorly to unseen tasks. We propose MetaVLA, a unified,
backbone-agnostic post-training framework for efficient and scalable alignment.
MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse
target tasks into a single fine-tuning stage while leveraging structurally
diverse auxiliary tasks to improve in-domain generalization. Unlike naive
multi-task SFT, MetaVLA integrates a lightweight meta-learning
mechanism-derived from Attentive Neural Processes-to enable rapid adaptation
from diverse contexts with minimal architectural change or inference overhead.
On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA
by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K,
and cuts GPU time by ~76%. These results show that scalable, low-resource
post-training is achievable-paving the way toward general-purpose embodied
agents. Code will be available.

</details>


### [346] [From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions](https://arxiv.org/abs/2510.05596)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Geng Sun,Xianbin Wang,Shiwen Mao,Abbas Jamalipour*

Main category: cs.AI

TL;DR: 本文提出了一种面向未来无线系统的自进化智能体AI框架，通过多LLM协同与自主演化循环，实现无需人工干预的持续优化，在低空无线网络天线优化案例中显著提升了波束增益和系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态AI模型难以适应动态无线环境，缺乏自主持续进化能力，限制了其在复杂场景中的长期有效性。

Method: 提出一种多智能体协同的自进化AI框架，包含分层架构与完整生命周期，利用工具智能、工作流优化、自反思和进化学习等技术，通过监督智能体协调多个角色特化的大型语言模型进行结构化对话与迭代反馈。

Result: 在低空无线网络天线优化案例中，该框架成功将固定天线优化升级为可移动天线优化，自主恢复退化性能最高达52.02%，并持续超越固定基线。

Conclusion: 自进化智能体AI具备强适应性与鲁棒性，能够在几乎无人干预的情况下完成复杂无线系统的自主优化，是实现下一代无线智能的重要范式。

Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for
future wireless systems by enabling autonomous agents to continually adapt and
improve without human intervention. Unlike static AI models, self-evolving
agents embed an autonomous evolution cycle that updates models, tools, and
workflows in response to environmental dynamics. This paper presents a
comprehensive overview of self-evolving agentic AI, highlighting its layered
architecture, life cycle, and key techniques, including tool intelligence,
workflow optimization, self-reflection, and evolutionary learning. We further
propose a multi-agent cooperative self-evolving agentic AI framework, where
multiple large language models (LLMs) are assigned role-specialized prompts
under the coordination of a supervisor agent. Through structured dialogue,
iterative feedback, and systematic validation, the system autonomously executes
the entire life cycle without human intervention. A case study on antenna
evolution in low-altitude wireless networks (LAWNs) demonstrates how the
framework autonomously upgrades fixed antenna optimization into movable antenna
optimization. Experimental results show that the proposed self-evolving agentic
AI autonomously improves beam gain and restores degraded performance by up to
52.02%, consistently surpassing the fixed baseline with little to no human
intervention and validating its adaptability and robustness for next-generation
wireless intelligence.

</details>


### [347] [Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography](https://arxiv.org/abs/2510.05664)
*Hanna Kreutzer,Anne-Sophie Caselitz,Thomas Dratsch,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung*

Main category: cs.AI

TL;DR: 本研究评估了GPT-4o从放射学报告中提取诊断标签（含不确定性）的能力，并探讨这些标签对肌肉骨骼X光图像多标签分类的影响。结果显示，GPT-4o能以98.6%的准确率提取标签，且基于标签训练的ResNet50模型在不同解剖区域均表现出具有竞争力的性能，不确定性处理策略未显著影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 为了提高从非结构化放射学文本中自动提取诊断标签的效率和准确性，并探索标签中的不确定性信息对后续图像分类模型训练的影响，推动AI在医学影像分析中的应用。

Method: 采用回顾性研究设计，利用GPT-4o从锁骨、肘部和拇指的放射学报告中提取结构化标签（‘真’、‘假’或‘不确定’），并将‘不确定’标签分别按‘真’（包容性）或‘假’（排他性）重新赋值。使用ResNet50进行多标签图像分类，并在内部和外部测试集上评估模型性能。

Result: GPT-4o在测试集中标签提取准确率达98.6%；包容性和排他性模型在各解剖区域的macro-AUC表现相近（如肘部均为0.80），外部验证结果稳定（肘部AUC为0.79），不同标注策略间无显著差异（p≥0.15）。

Conclusion: GPT-4o能够高精度地从放射学报告中提取诊断标签并用于训练高性能的多标签图像分类模型，报告中检测到的不确定性不影响模型性能，表明其在临床数据自动化处理中具有可靠性和实用性。

Abstract: Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with
uncertainty) from free-text radiology reports and to test how these labels
affect multi-label image classification of musculoskeletal radiographs.
Methods: This retrospective study included radiography series of the clavicle
(n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o
filled out structured templates by indicating imaging findings as present
("true"), absent ("false"), or "uncertain." To assess the impact of label
uncertainty, "uncertain" labels of the training and validation sets were
automatically reassigned to "true" (inclusive) or "false" (exclusive).
Label-image-pairs were used for multi-label classification using ResNet50.
Label extraction accuracy was manually verified on internal (clavicle: n=233,
elbow: n=745, thumb: n=393) and external test sets (n=300 for each).
Performance was assessed using macro-averaged receiver operating characteristic
(ROC) area under the curve (AUC), precision recall curves, sensitivity,
specificity, and accuracy. AUCs were compared with the DeLong test. Results:
Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the
test sets. Across anatomic regions, label-based model training yielded
competitive performance measured by macro-averaged AUC values for inclusive
(e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow:
AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets
(elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79
[range, 0.63-0.89]). No significant differences were observed across labeling
strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from
radiologic reports to train competitive multi-label classification models with
high accuracy. Detected uncertainty in the radiologic reports did not influence
the performance of these models.

</details>


### [348] [D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI](https://arxiv.org/abs/2510.05684)
*Suwhan Choi,Jaeyoon Jung,Haebin Seong,Minchan Kim,Minyeong Kim,Yongjun Cho,Yoonshik Kim,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: 提出D2E框架，利用桌面交互（如游戏）进行机器人具身AI的预训练，通过标准化数据收集、通用事件预测模型和迁移学习，在LIBERO和CANVAS基准上取得优异表现，验证了桌面预训练向物理任务迁移的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖海量文本数据，而具身AI受限于物理轨迹采集的高成本。桌面环境（尤其是游戏）提供了丰富且可扩展的感觉运动交互，同时保持了具身学习所需的结构化感知-动作耦合，因此探索其作为具身AI预训练平台的潜力。

Method: D2E框架包含三个部分：(1) OWA Toolkit，将多样化的桌面交互统一为标准格式并实现152倍压缩；(2) Generalist-IDM，通过基于时间戳的事件预测实现跨未见游戏的零样本泛化，支持互联网规模的伪标签生成；(3) VAPT，将桌面预训练表征迁移到物理操作与导航任务中。

Result: 基于1.3K+小时数据（259小时人类演示+1K+小时伪标签游戏），在LIBERO操作任务上达到96.6%成功率，CANVAS导航任务上达到83.3%成功率。

Conclusion: 数字交互中的感觉运动基元具有足够的不变性，可有效迁移到物理具身任务中，桌面预训练是机器人具身AI的一种可行范式。

Abstract: Large language models leverage internet-scale text data, yet embodied AI
remains constrained by the prohibitive costs of physical trajectory collection.
Desktop environments -- particularly gaming -- offer a compelling alternative:
they provide rich sensorimotor interactions at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to Embodied AI), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the OWA Toolkit
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the Generalist-IDM that achieves strong zero-shot
generalization across unseen games through timestamp-based event prediction,
enabling internet-scale pseudo-labeling, and (3) VAPT that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on CANVAS navigation benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the OWA toolkit, datasets of human-collected and
pseudo-labeled, and VAPT-trained models available at
https://worv-ai.github.io/d2e/

</details>


### [349] [Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach](https://arxiv.org/abs/2510.05698)
*Yousef Emami,Seyedsina Nabavirazavi,Jingjing Zheng,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出了一种基于注意力机制的上下文学习方法（AIC-VDS），用于在海啸等灾害应急监测中联合优化多架无人机的数据采集调度与飞行速度，以最小化数据丢失。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习（DRL）方法在模拟与现实之间存在差距，训练复杂且难以满足灾后紧急响应需求；同时，地面传感器可能因调度不当导致缓冲区溢出和数据包丢失。因此，需要一种无需重新训练、能快速适应任务的高效替代方案。

Method: 利用大语言模型（LLM）的上下文学习（ICL）能力，结合自然语言提示和示例引导，提出AIC-VDS方法，综合考虑地面传感器电池水平、队列长度、信道状况及无人机轨迹，实现对多无人机数据采集调度与飞行速度的联合优化。

Result: 仿真结果表明，AIC-VDS在减少数据丢失方面优于深度Q网络（DQN）和最大信道增益基线方法。

Conclusion: AIC-VDS通过融合注意力机制与上下文学习，为应急场景下的无人机数据采集提供了一种高效、灵活且无需重训练的解决方案，显著提升了数据收集的可靠性与实时性。

Abstract: Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated
to collect sensory data in post-disaster monitoring scenarios, such as
tsunamis, where early actions are critical to limit coastal damage. A major
challenge is to design the data collection schedules and flight velocities, as
unfavorable schedules and velocities can lead to transmission errors and buffer
overflows of the ground sensors, ultimately resulting in significant packet
loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a
complex training process and a mismatch between simulation and reality that
does not meet the urgent requirements of tsunami monitoring. Recent advances in
Large Language Models (LLMs) offer a compelling alternative. With their strong
reasoning and generalization capabilities, LLMs can adapt to new tasks through
In-Context Learning (ICL), which enables task adaptation through natural
language prompts and example-based guidance without retraining. However, LLM
models have input data limitations and thus require customized approaches. In
this paper, a joint optimization of data collection schedules and velocities
control for multiple UAVs is proposed to minimize data loss. The battery level
of the ground sensors, the length of the queues, and the channel conditions, as
well as the trajectories of the UAVs, are taken into account. Attention-Based
In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS)
is proposed as an alternative to DRL in emergencies. The simulation results
show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and
maximum channel gain baselines.

</details>


### [350] [Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge](https://arxiv.org/abs/2510.05733)
*Zijun Jia,Shuang Liang,Jinsong Yu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Syn-Diag的云边协同框架，利用大语言模型解决工业故障诊断中的少样本学习和资源受限部署问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业故障诊断面临数据稀缺和在资源受限环境中部署大型AI模型的困难，亟需一种高效且可部署的解决方案。

Method: Syn-Diag采用三层机制：视觉-语义协同、内容感知推理和云-边协同，通过跨模态预训练、动态上下文提示构建和知识蒸馏实现高性能少样本故障诊断。

Result: 在六个不同工况的数据集上实验表明，Syn-Diag在1-shot和跨工况场景下显著优于现有方法，边缘模型性能接近云端版本，同时模型规模减少83%，延迟降低50%。

Conclusion: Syn-Diag为现代智能诊断提供了一个实用、鲁棒且可部署的新范式，有效平衡了诊断精度与边缘设备的资源限制。

Abstract: Industrial fault diagnosis faces the dual challenges of data scarcity and the
difficulty of deploying large AI models in resource-constrained environments.
This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that
leverages Large Language Models to overcome these limitations in few-shot fault
diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic
Synergy, which aligns signal features with the LLM's semantic space through
cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically
constructs contextual prompts to enhance diagnostic accuracy with limited
samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create
a lightweight, efficient edge model capable of online updates via a shared
decision space. Extensive experiments on six datasets covering different CWRU
and SEU working conditions show that Syn-Diag significantly outperforms
existing methods, especially in 1-shot and cross-condition scenarios. The edge
model achieves performance comparable to the cloud version while reducing model
size by 83% and latency by 50%, offering a practical, robust, and deployable
paradigm for modern intelligent diagnostics.

</details>


### [351] [Artificially intelligent agents in the social and behavioral sciences: A history and outlook](https://arxiv.org/abs/2510.05743)
*Petter Holme,Milena Tsvetkova*

Main category: cs.AI

TL;DR: 本文综述了人工智能代理在社会科学和行为科学中的历史发展与当前趋势，强调AI在科学进程中的作用及其带来的变革。


<details>
  <summary>Details</summary>
Motivation: 探讨从20世纪50年代至今，AI技术如何深刻影响社会科学的研究方法与认知方式。

Method: 通过回顾从早期计算机、社会模拟到当前大语言模型的发展历程，分析关键阶段的技术与科学演变。

Result: 揭示了AI在社会科学中逐步深化的应用，包括博弈论智能体、大数据时代的知识重构以及生成式AI的兴起。

Conclusion: 人类用于理解自身的科技与我们自身已深度交织，AI不仅是工具，更是塑造科学认知的重要力量。

Abstract: We review the historical development and current trends of artificially
intelligent agents (agentic AI) in the social and behavioral sciences: from the
first programmable computers, and social simulations soon thereafter, to
today's experiments with large language models. This overview emphasizes the
role of AI in the scientific process and the changes brought about, both
through technological advancements and the broader evolution of science from
around 1950 to the present. Some of the specific points we cover include: the
challenges of presenting the first social simulation studies to a world unaware
of computers, the rise of social systems science, intelligent game theoretic
agents, the age of big data and the epistemic upheaval in its wake, and the
current enthusiasm around applications of generative AI, and many other topics.
A pervasive theme is how deeply entwined we are with the technologies we use to
understand ourselves.

</details>


### [352] [The Safety Challenge of World Models for Embodied AI Agents: A Review](https://arxiv.org/abs/2510.05865)
*Lorenzo Baraldi,Zifan Zeng,Chongzhe Zhang,Aradhana Nayak,Hongbo Zhu,Feng Liu,Qunli Zhang,Peng Wang,Shiming Liu,Zheng Hu,Angelo Cangelosi,Lorenzo Baraldi*

Main category: cs.AI

TL;DR: 本文综述了自动驾驶和机器人领域中的世界模型（World Models），重点关注场景生成与控制任务中的安全性问题，并通过实证分析识别和分类常见故障（病理），进行定量评估。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能的快速发展，亟需能够感知、理解和预测环境动态的高级集成模型；然而，确保具身代理在预测过程中的安全性至关重要。

Method: 进行了关于世界模型的全面文献综述，并结合实证分析，收集并分析最先进模型的预测结果，识别并分类常见故障（路径学），同时进行量化评估。

Result: 识别出世界模型在场景和控制生成任务中的多种常见故障（路径学），并对不同模型的安全相关性能进行了定量评估。

Conclusion: 世界模型虽在提升具身代理的规划与执行能力方面具有潜力，但在实际应用中仍存在影响安全性的关键缺陷，需针对性改进以确保安全可靠。

Abstract: The rapid progress in embodied artificial intelligence has highlighted the
necessity for more advanced and integrated models that can perceive, interpret,
and predict environmental dynamics. In this context, World Models (WMs) have
been introduced to provide embodied agents with the abilities to anticipate
future environmental states and fill in knowledge gaps, thereby enhancing
agents' ability to plan and execute actions. However, when dealing with
embodied agents it is fundamental to ensure that predictions are safe for both
the agent and the environment. In this article, we conduct a comprehensive
literature review of World Models in the domains of autonomous driving and
robotics, with a specific focus on the safety implications of scene and control
generation tasks. Our review is complemented by an empirical analysis, wherein
we collect and examine predictions from state-of-the-art models, identify and
categorize common faults (herein referred to as pathologies), and provide a
quantitative evaluation of the results.

</details>


### [353] [ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems](https://arxiv.org/abs/2510.05746)
*Bohan Yao,Shiva Krishna Reddy Malay,Vikas Yadav*

Main category: cs.AI

TL;DR: 提出一种新的多智能体系统（MAS）自动设计范式，聚焦于优化思维链（CoT）推理，引入基于树搜索和执行反馈演化的Agentic Reasoning Module（ARM），在多种模型和任务上展现出卓越性能与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动设计的多智能体系统性能不佳，需昂贵的架构重发现和标注数据；而简单的思维链（CoT）推理表现不俗，表明应深入优化CoT这一基本推理单元。

Method: 提出Agentic Reasoning Module（ARM），将每一步推理交由通过树搜索和反射驱动变异在代码空间中发现的专用模块执行，并支持递归调用或作为元协调器的子程序。

Result: 该方法显著优于手动设计和最先进的自动MAS方法，且在不同基础模型和任务领域间表现出优异的无需进一步优化的泛化能力。

Conclusion: 通过聚焦优化CoT推理单元，ARM为构建高效、通用的多智能体系统提供了新范式，实现了性能与泛化的双重提升。

Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved
state-of-the-art results on various complex reasoning tasks. Recent works have
proposed techniques to automate the design of MASes, eliminating the need for
manual engineering. However, these techniques perform poorly, often achieving
similar or inferior performance to simple baselines. Furthermore, they require
computationally expensive re-discovery of architectures for each new task
domain and expensive data annotation on domains without existing labeled
validation sets. A critical insight is that simple Chain of Thought (CoT)
reasoning often performs competitively with these complex systems, suggesting
that the fundamental reasoning unit of MASes, CoT, warrants further
investigation. To this end, we present a new paradigm for automatic MAS design
that pivots the focus to optimizing CoT reasoning. We introduce the Agentic
Reasoning Module (ARM), an agentic generalization of CoT where each granular
reasoning step is executed by a specialized reasoning module. This module is
discovered through a tree search over the code space, starting from a simple
CoT module and evolved using mutations informed by reflection on execution
traces. The resulting ARM acts as a versatile reasoning building block which
can be utilized as a direct recursive loop or as a subroutine in a learned
meta-orchestrator. Our approach significantly outperforms both manually
designed MASes and state-of-the-art automatic MAS design methods. Crucially,
MASes built with ARM exhibit superb generalization, maintaining high
performance across different foundation models and task domains without further
optimization.

</details>


### [354] [Information-Theoretic Policy Pre-Training with Empowerment](https://arxiv.org/abs/2510.05996)
*Moritz Schneider,Robert Krug,Narunas Vaskevicius,Luigi Palmieri,Michael Volpp,Joschka Boedecker*

Main category: cs.AI

TL;DR: 本文提出了一种基于折扣赋能（discounted empowerment）的预训练范式，用于提升强化学习在下游任务中的数据效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 赋能作为一种内在动机机制虽已被广泛研究，但其作为预训练信号的应用尚不充分，本文旨在探索赋能在预训练中的潜力。

Method: 引入折扣赋能概念，平衡智能体在短期与长期对环境的控制能力，并以此最大化为目标进行策略预训练。

Result: 实验证明，基于赋能的预训练能有效提升多种强化学习算法在下游任务中的数据效率和适应能力，尤其是长时域赋能策略表现更优。

Conclusion: 赋能可作为通用且高效的预训练策略，为未来在高维复杂任务中应用该框架提供了基础。

Abstract: Empowerment, an information-theoretic measure of an agent's potential
influence on its environment, has emerged as a powerful intrinsic motivation
and exploration framework for reinforcement learning (RL). Besides for
unsupervised RL and skill learning algorithms, the specific use of empowerment
as a pre-training signal has received limited attention in the literature. We
show that empowerment can be used as a pre-training signal for data-efficient
downstream task adaptation. For this we extend the traditional notion of
empowerment by introducing discounted empowerment, which balances the agent's
control over the environment across short- and long-term horizons. Leveraging
this formulation, we propose a novel pre-training paradigm that initializes
policies to maximize discounted empowerment, enabling agents to acquire a
robust understanding of environmental dynamics. We analyze empowerment-based
pre-training for various existing RL algorithms and empirically demonstrate its
potential as a general-purpose initialization strategy: empowerment-maximizing
policies with long horizons are data-efficient and effective, leading to
improved adaptability in downstream tasks. Our findings pave the way for future
research to scale this framework to high-dimensional and complex tasks, further
advancing the field of RL.

</details>


### [355] [Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport](https://arxiv.org/abs/2510.05751)
*Jeffrey N. Clark,Elena Fillola,Nawid Keshtmand,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.AI

TL;DR: 提出了一种基于图神经网络的拉格朗日粒子扩散模型（LPDM）代理模型，结合集合方法量化大气传输模拟中的不确定性，在保持精度的同时实现约1000倍加速，适用于卫星观测下的温室气体排放反演。


<details>
  <summary>Details</summary>
Motivation: 传统大气传输模型计算成本高、不确定性难以量化，限制了自上而下温室气体排放估算的效率与可靠性，亟需更高效且能表征不确定性的方法。

Method: 构建基于图神经网络的LPDM代理模型，并采用集合方法生成多个预测结果以估计大气传输足迹和温室气体浓度的绝对与相对不确定性，利用GOSAT卫星观测数据在巴西区域进行验证。

Result: 代理模型相比NAME LPDM实现约1000倍加速，能复现大尺度足迹结构；集合结果揭示了预测误差的空间相关性，可识别低置信度的时空预测区域。

Conclusion: 该方法显著提升大气传输模拟效率并有效量化不确定性，可推广至其他传输模型，支持具备不确定性感知的温室气体反演系统，增强卫星监测排放的鲁棒性，并为改进通量估算的不确定性预算提供新路径。

Abstract: Monitoring greenhouse gas emissions and evaluating national inventories
require efficient, scalable, and reliable inference methods. Top-down
approaches, combined with recent advances in satellite observations, provide
new opportunities to evaluate emissions at continental and global scales.
However, transport models used in these methods remain a key source of
uncertainty: they are computationally expensive to run at scale, and their
uncertainty is difficult to characterise. Artificial intelligence offers a dual
opportunity to accelerate transport simulations and to quantify their
associated uncertainty.
  We present an ensemble-based pipeline for estimating atmospheric transport
"footprints", greenhouse gas mole fraction measurements, and their
uncertainties using a graph neural network emulator of a Lagrangian Particle
Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse
Gases Observing Satellite) observations for Brazil in 2016. The emulator
achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale
footprint structures. Ensembles were calculated to quantify absolute and
relative uncertainty, revealing spatial correlations with prediction error. The
results show that ensemble spread highlights low-confidence spatial and
temporal predictions for both atmospheric transport footprints and methane mole
fractions.
  While demonstrated here for an LPDM emulator, the approach could be applied
more generally to atmospheric transport models, supporting uncertainty-aware
greenhouse gas inversion systems and improving the robustness of
satellite-based emissions monitoring. With further development, ensemble-based
emulators could also help explore systematic LPDM errors, offering a
computationally efficient pathway towards a more comprehensive uncertainty
budget in greenhouse gas flux estimates.

</details>


### [356] [Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis](https://arxiv.org/abs/2510.05761)
*Sedat Dogan,Nina Dethlefs,Debarati Chakraborty*

Main category: cs.AI

TL;DR: 本研究提出了一种基于大规模跨语言数据集的迷因早期病毒式传播预测方法，使用混合参与度评分定义病毒性，并通过XGBoost等模型在30分钟内实现有效预测。


<details>
  <summary>Details</summary>
Motivation: 预测在线内容的病毒式传播极具挑战性，尤其是在文化复杂且快速演变的迷因场景中，现有方法常受限于数据泄露和缺乏早期预测能力。

Method: 采用25个不同Reddit社区的跨语言数据集，设计基于时间分段训练集学习百分位阈值的混合参与度评分来定义病毒性，并结合静态内容、时间动态和网络特征，利用Logistic Regression、XGBoost和MLP等模型进行多时间窗口（30-420分钟）预测。

Result: XGBoost模型在仅30分钟的数据上达到了PR-AUC > 0.52的性能，显示出早期预测的可行性；分析发现特征重要性从静态上下文向时间动态转变，存在明显的‘证据过渡’现象。

Conclusion: 该研究建立了在无法获取完整扩散级联数据情况下的稳健、可解释且实用的早期病毒性预测基准，首次结合时间序列、静态内容与网络特征进行迷因病毒性预测，并贡献了新的跨语言数据集和方法论定义。

Abstract: Predicting the virality of online content remains challenging, especially for
culturally complex, fast-evolving memes. This study investigates the
feasibility of early prediction of meme virality using a large-scale,
cross-lingual dataset from 25 diverse Reddit communities. We propose a robust,
data-driven method to define virality based on a hybrid engagement score,
learning a percentile-based threshold from a chronologically held-out training
set to prevent data leakage. We evaluated a suite of models, including Logistic
Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive,
multimodal feature set across increasing time windows (30-420 min). Crucially,
useful signals emerge quickly: our best-performing model, XGBoost, achieves a
PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear "evidentiary
transition," in which the importance of the feature dynamically shifts from the
static context to the temporal dynamics as a meme gains traction. This work
establishes a robust, interpretable, and practical benchmark for early virality
prediction in scenarios where full diffusion cascade data is unavailable,
contributing a novel cross-lingual dataset and a methodologically sound
definition of virality. To our knowledge, this study is the first to combine
time series data with static content and network features to predict early meme
virality.

</details>


### [357] [ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming](https://arxiv.org/abs/2510.05774)
*Weichun Shi,Minghao Liu,Wanting Zhang,Langchen Shi,Fuqi Jia,Feifei Ma,Jian Zhang*

Main category: cs.AI

TL;DR: 本文提出了ConstraintLLM，首个专为约束编程（CP）建模设计的大语言模型，并引入了约束感知检索模块（CARM）以增强上下文学习能力，结合树状思维（ToT）框架和自修正机制，在新构建的工业级基准IndusCP上实现了最先进的求解准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自动形式化建模方面展现出潜力，但约束编程（CP）相比运筹学（OR）模型受到较少关注，缺乏专门的模型和工业级评估基准，限制了其在现实场景中的应用与发展。

Method: 基于开源大语言模型进行多指令监督微调，设计约束感知检索模块（CARM），并将其集成到树状思维（ToT）框架中，结合引导式自我修正机制提升推理与建模能力。

Result: 在多个基准测试中达到最先进的求解精度，在新提出的工业级基准IndusCP上性能优于基线模型达2倍。

Conclusion: ConstraintLLM结合CARM与ToT框架，显著提升了CP问题的自动建模能力，推动了神经符号AI在约束编程领域的应用，并通过开源数据与代码促进了后续研究。

Abstract: Constraint programming (CP) is a crucial technology for solving real-world
constraint optimization problems (COPs), with the advantages of rich modeling
semantics and high solving efficiency. Using large language models (LLMs) to
generate formal modeling automatically for COPs is becoming a promising
approach, which aims to build trustworthy neuro-symbolic AI with the help of
symbolic solvers. However, CP has received less attention compared to works
based on operations research (OR) models. We introduce ConstraintLLM, the first
LLM specifically designed for CP modeling, which is trained on an open-source
LLM with multi-instruction supervised fine-tuning. We propose the
Constraint-Aware Retrieval Module (CARM) to increase the in-context learning
capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with
guided self-correction mechanism. Moreover, we construct and release IndusCP,
the first industrial-level benchmark for CP modeling, which contains 140
challenging tasks from various domains. Our experiments demonstrate that
ConstraintLLM achieves state-of-the-art solving accuracy across multiple
benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.
Code and data are available at: https://github.com/william4s/ConstraintLLM.

</details>


### [358] [Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering](https://arxiv.org/abs/2510.05871)
*Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Tom A. Lamb,Philip Torr,Marc Boubnovski Martell,Julien Fauqueur,Kaspar Märtens*

Main category: cs.AI

TL;DR: 提出一种基于不确定性的过滤方法，利用模型自身的置信度来筛选合成的思维链轨迹，从而在缺乏真实标签的情况下高效构建推理数据集，尤其适用于湿实验标签昂贵的生物学领域。


<details>
  <summary>Details</summary>
Motivation: 现有合成思维链方法依赖真实标签进行初始化或过滤，但在生物学等湿实验数据稀缺的领域成本过高，亟需无需标签的替代方案。

Method: 采用自一致性、预测困惑度等不确定性指标量化模型置信度，对多个推理路径进行采样并保留低不确定性子集；引入按类别过滤和混合不确定性指标提升效果。

Result: 在生物扰动预测任务中，不确定性过滤后的数据集准确率更高；基于该数据的监督微调效果优于未过滤数据，接近真实标签训练水平，并超越强基线模型。

Conclusion: 模型内部置信度可作为高效推理数据构建的有效信号，为标签昂贵领域的大模型推理能力训练提供了可行路径。

Abstract: Synthetic chain-of-thought (CoT) traces are widely used to train large
reasoning models (LRMs), improving generalization by providing step-level
supervision. Yet most approaches require ground-truth labels to seed or filter
these traces - an expensive bottleneck in domains like biology where wet-lab
data are scarce. We propose a label-free alternative: uncertainty-based
filtering, which uses a model's own confidence - quantified through established
uncertainty metrics like self-consistency and predictive perplexity - as a
substitute for external labels. We sample multiple reasoning traces and retain
only low-uncertainty subsets. Applied to biological perturbation prediction, a
domain where wet-lab labels are especially costly, we show that the filtered
subset has higher accuracy, and that supervised fine-tuning (SFT) on
uncertainty-filtered data outperforms unfiltered synthetic data, narrows the
gap to ground-truth training, and surpasses strong LRM baselines. Ablations
show that per-class filtering corrects for class-specific uncertainty scales
and that hybrid uncertainty metrics yield higher-quality datasets. Our results
suggest that model-internal confidence is a powerful signal for efficient
reasoning dataset creation, enabling LRMs in domains where supervision is
expensive.

</details>


### [359] [Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies](https://arxiv.org/abs/2510.05909)
*Aksel Joonas Reedi,Corentin Léger,Julien Pourcel,Loris Gaven,Perrine Charriau,Guillaume Pourcel*

Main category: cs.AI

TL;DR: 本文提出了DebateQD，一种基于质量多样性（QD）的进化算法，通过在大型语言模型中进行辩论优化，比较了以说服为目标和以追求真相为目标的策略。结果表明，以说服为导向的优化能显著缩小训练与测试间的泛化差距，并提升推理能力的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于真理优化的语言模型容易过拟合，推理缺乏泛化能力；而基于说服的优化在辩论场景中展现出潜力，但缺乏与主流方法的系统对比。因此需要一个可控实验来分离优化目标的影响。

Method: 提出DebateQD算法，采用单个LLM架构内基于提示的策略演化多样辩论方式，通过三者博弈（两方辩论、一方评判）进行锦标赛式竞争；固定辩论协议，仅切换适应度函数（说服 vs. 真理），在不同模型规模和数据集上评估性能。

Result: 在7B到72B参数的多种模型和QuALITY子数据集上，说服优化比真理优化最多减少13.94%的训练-测试泛化差距，同时保持或优于后者的测试表现。

Conclusion: 说服驱动的辩论优化能促进更可迁移、更鲁棒的推理能力，相比协作求真策略更具泛化优势，为改进大模型推理提供新方向。

Abstract: Large Language Models (LLMs) optimized to output truthful answers often
overfit, producing brittle reasoning that fails to generalize. While
persuasion-based optimization has shown promise in debate settings, it has not
been systematically compared against mainstream truth-based approaches. We
introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm
that evolves diverse debate strategies across different categories
(rationality, authority, emotional appeal, etc.) through tournament-style
competitions where two LLMs debate while a third judges. Unlike previously
proposed methods that require a population of LLMs, our approach maintains
diversity of opponents through prompt-based strategies within a single LLM
architecture, making it more accessible for experiments while preserving the
key benefits of population-based optimization. In contrast to prior work, we
explicitly isolate the role of the optimization objective by fixing the debate
protocol and swapping only the fitness function: persuasion rewards strategies
that convince the judge irrespective of truth, whereas truth rewards
collaborative correctness. Across three model scales (7B, 32B, 72B parameters)
and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized
strategies achieve up to 13.94% smaller train-test generalization gaps, while
matching or exceeding truth optimization's test performance. These results
provide the first controlled evidence that competitive pressure to persuade,
rather than seek the truth collaboratively, fosters more transferable reasoning
skills, offering a promising path for improving LLM generalization.

</details>


### [360] [Training-Free Time Series Classification via In-Context Reasoning with LLM Agents](https://arxiv.org/abs/2510.05950)
*Songyuan Sui,Zihang Xu,Yu-Neng Chuang,Kwei-Herng Lai,Xia Hu*

Main category: cs.AI

TL;DR: 提出FETA，一种无需训练的多智能体框架，通过基于示例的上下文推理实现时间序列分类，在多个数据集上优于有训练的基线方法。


<details>
  <summary>Details</summary>
Motivation: 标签数据稀缺使得时间序列分类任务中特定任务训练成本高且不灵活，现有大语言模型在零样本场景下表现仍不理想。

Method: 将多变量序列分解为按通道的子问题，对每个通道检索结构相似的标注示例，利用推理型大语言模型比较查询与示例并生成带置信度的通道级标签，再通过置信度加权聚合得到最终结果。

Result: 在九个UEA数据集上实现了优异的准确率，超越多个需训练的基线模型，同时具备高效性、可解释性和无需微调的优势。

Conclusion: FETA证明了无需参数训练的多智能体上下文推理框架可有效将大语言模型转化为具有竞争力的时间序列分类器。

Abstract: Time series classification (TSC) spans diverse application scenarios, yet
labeled data are often scarce, making task-specific training costly and
inflexible. Recent reasoning-oriented large language models (LLMs) show promise
in understanding temporal patterns, but purely zero-shot usage remains
suboptimal. We propose FETA, a multi-agent framework for training-free TSC via
exemplar-based in-context reasoning. FETA decomposes a multivariate series into
channel-wise subproblems, retrieves a few structurally similar labeled examples
for each channel, and leverages a reasoning LLM to compare the query against
these exemplars, producing channel-level labels with self-assessed confidences;
a confidence-weighted aggregator then fuses all channel decisions. This design
eliminates the need for pretraining or fine-tuning, improves efficiency by
pruning irrelevant channels and controlling input length, and enhances
interpretability through exemplar grounding and confidence estimation. On nine
challenging UEA datasets, FETA achieves strong accuracy under a fully
training-free setting, surpassing multiple trained baselines. These results
demonstrate that a multi-agent in-context reasoning framework can transform
LLMs into competitive, plug-and-play TSC solvers without any parameter
training. The code is available at https://github.com/SongyuanSui/FETATSC.

</details>


### [361] [MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization](https://arxiv.org/abs/2510.05962)
*Dayyán O'Brien,Barry Haddow,Emily Allaway,Pinzhen Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为MatheMagic的动态、反事实数学基准测试方法，通过改变数字和运算符的解释生成测试实例，以揭示模型过拟合问题并衡量其真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学评测基准存在符号和规则多样性不足、答案封闭等问题，易导致模型过拟合，且公开测试集可能被模型记忆，难以进行无污染评估。

Method: 利用现有基准的局限性作为特征，构建在测试时随机生成、具有自动可验证答案的动态反事实数学测试实例，评估模型的归纳与演绎能力。

Result: 实验发现模型更容易解决演绎任务而非归纳任务，且在微调后归纳能力泛化效果差；数学适配模型未能表现出通用的推理技能。

Conclusion: 当前模型缺乏真正的通用推理能力，而MatheMagic提供了一种稳定、可扩展、抗过拟合的数学能力评测框架。

Abstract: Conducting contamination-free evaluation of mathematical capabilities can be
difficult for two reasons: models may memorize a test set once it is made
public, and current mathematical benchmarks are prone to overfitting due to
having limited diversity of symbols and rules, coupled with closed-ended
answers. This paper proposes a method to leverage these shortcomings as useful
features to a construct dynamic, counterfactual benchmark, which can be used to
both reveal overfitting and measure true reasoning. We demonstrate this via
MatheMagic, which generates math test instances with the interpretations of
numbers and operators altered, yet has automatically verifiable answers. Test
instances are randomly seeded and constructed at test time to evaluate a
model's induction or deduction capability, offering stability, extensibility,
comparability, and robustness to overfitting. Our experiments find that models
solve deduction more easily than induction, but they revert to standard math.
Further analysis reveals that math-adapted models fail to exhibit a general
"skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.

</details>


### [362] [ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models](https://arxiv.org/abs/2510.06014)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Zhiyuan Yu,Qipeng Guo,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 本文提出了ARISE，一种用于评估大推理模型测试时扩展效果的新指标，能够有效衡量不同模型在数学推理、代码生成和代理任务等领域的扩展能力，并发现Claude Opus具有优越的扩展特性。


<details>
  <summary>Details</summary>
Motivation: 随着推理模型的发展，缺乏系统方法来比较不同模型在测试时扩展能力的优劣，亟需一个可靠的评估指标。

Method: 提出ARISE指标，包含样本级感知机制以惩罚负向扩展行为，并引入动态采样机制减少准确率波动和令牌数不稳定性对评估的影响。

Result: 在多个领域对先进推理模型进行了实验，结果表明ARISE能提供可靠且细粒度的扩展能力测量，揭示了不同模型间显著的效率差异。

Conclusion: ARISE是一种有效的测试时扩展评估方法，能够区分不同推理模型的扩展质量，为模型选择和优化提供了重要依据。

Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the
performance of large reasoning models, enabling dynamic allocation of
computational resources during inference. However, as the landscape of
reasoning models rapidly expands, a critical question remains: how can we
systematically compare and evaluate the test-time scaling capabilities across
different models? In this paper, we introduce ARISE (Adaptive Resolution-aware
Scaling Evaluation), a novel metric specifically designed to assess the
test-time scaling effectiveness of large reasoning models. Unlike existing
evaluation approaches, ARISE incorporates two key innovations: (1) sample-level
awareness that effectively penalizes negative scaling behaviors where increased
computation leads to performance degradation, and (2) a dynamic sampling
mechanism that mitigates the impact of accuracy fluctuations and token count
instability on the final assessment. We conduct comprehensive experiments
evaluating state-of-the-art reasoning models across diverse domains including
mathematical reasoning, code generation, and agentic tasks. Our results
demonstrate that ARISE provides a reliable and fine-grained measurement of
test-time scaling capabilities, revealing significant variations in scaling
efficiency across models. Notably, our evaluation identifies Claude Opus as
exhibiting superior scaling characteristics compared to other contemporary
reasoning models.

</details>


### [363] [Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?](https://arxiv.org/abs/2510.06036)
*Qingyu Yin,Chak Tou Leong,Linyi Yang,Wenxuan Huang,Wenjie Li,Xiting Wang,Jaehong Yoon,YunXing,XingYu,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文研究了大推理模型在安全对齐方面的失效机制，发现了一种称为“拒绝悬崖”的现象：模型在推理过程中保持拒绝意图，但在生成输出前最后一刻拒绝分数急剧下降。通过因果干预分析，作者识别出少数关键注意力头抑制了拒绝行为，并提出基于该机制的高效数据选择方法Cliff-as-a-Judge，仅用1.7%的数据即实现与传统方法相当的安全性提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大推理模型展现出强大的推理能力，但其安全对齐机制仍存在未被充分理解的漏洞。本文旨在从机制可解释性的角度探究为何这些模型在面对有害提示时会失效，尤其是在推理过程中表现出拒绝意图却最终仍生成有害内容的现象。

Method: 采用线性探测方法追踪推理过程中各token位置上的拒绝意图变化，发现“拒绝悬崖”现象；通过因果干预分析识别对拒绝行为有负面影响的注意力头；基于此提出Cliff-as-a-Judge数据选择方法，优先选择表现出最大拒绝悬崖的样本进行安全微调。

Result: 发现3%的关键注意力头主导了拒绝意图的抑制，移除它们可使攻击成功率降至10%以下；Cliff-as-a-Judge方法仅使用1.7%的常规安全训练数据就达到了 comparable 的安全性能提升，验证了“少即是多”的安全对齐策略。

Conclusion: 大推理模型的安全失效并非源于缺乏识别有害内容的能力，而是拒绝意图在输出阶段被特定机制（如稀疏注意力头）系统性压制。通过机制驱动的方法可以更高效地修复模型安全性，为未来安全对齐提供了新的视角和工具。

Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have
shown remarkable problem-solving abilities, yet they exhibit concerning safety
vulnerabilities that remain poorly understood. In this work, we investigate why
safety alignment fails in reasoning models through a mechanistic
interpretability lens. Using a linear probing approach to trace refusal
intentions across token positions, we discover a striking phenomenon termed as
\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify
harmful prompts and maintain strong refusal intentions during their thinking
process, but experience a sharp drop in refusal scores at the final tokens
before output generation. This suggests that these models are not inherently
unsafe; rather, their refusal intentions are systematically suppressed. Through
causal intervention analysis, we identify a sparse set of attention heads that
negatively contribute to refusal behavior. Ablating just 3\% of these heads can
reduce attack success rates below 10\%. Building on these mechanistic insights,
we propose \textbf{Cliff-as-a-Judge}, a novel data selection method that
identifies training examples exhibiting the largest refusal cliff to
efficiently repair reasoning models' safety alignment. This approach achieves
comparable safety improvements using only 1.7\% of the vanilla safety training
data, demonstrating a less-is-more effect in safety alignment.

</details>


### [364] [MixReasoning: Switching Modes to Think](https://arxiv.org/abs/2510.06052)
*Haiquan Lu,Gongfan Fang,Xinyin Ma,Qi Li,Xinchao Wang*

Main category: cs.AI

TL;DR: 提出MixReasoning框架，动态调整推理深度，结合复杂步骤的详细推理与简单步骤的简洁推断，提升效率而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型对所有步骤采用相同程度的详细推理，导致在简单子问题上产生冗余，无法根据步骤难度自适应调整。

Method: 设计MixReasoning框架，在单个响应中动态调整推理深度，区分难易步骤，对关键难点进行详细推理，对简单步骤采用简洁推断。

Result: 在GSM8K、MATH-500和AIME数据集上的实验表明，MixReasoning显著缩短了推理长度，提高了推理效率，同时保持了准确率。

Conclusion: MixReasoning通过混合推理策略实现了高效且精准的问题求解，为推理模型的自适应设计提供了新思路。

Abstract: Reasoning models enhance performance by tackling problems in a step-by-step
manner, decomposing them into sub-problems and exploring long chains of thought
before producing an answer. However, applying extended reasoning to every step
introduces substantial redundancy, as sub-problems vary widely in difficulty
and complexity: a small number of pivotal steps are genuinely challenging and
decisive for the final answer, while many others only involve straightforward
revisions or simple computations. Therefore, a natural idea is to endow
reasoning models with the ability to adaptively respond to this variation,
rather than treating all steps with the same level of elaboration. To this end,
we propose MixReasoning, a framework that dynamically adjusts the depth of
reasoning within a single response. The resulting chain of thought then becomes
a mixture of detailed reasoning on difficult steps and concise inference on
simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning
shortens reasoning length and substantially improves efficiency without
compromising accuracy.

</details>


### [365] [Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research](https://arxiv.org/abs/2510.06056)
*Gang Liu,Yihan Zhu,Jie Chen,Meng Jiang*

Main category: cs.AI

TL;DR: DeepEvolve 是一个结合深度研究与算法进化的科学助手，通过整合外部知识检索、跨文件代码编辑和系统性调试，在多个科学领域中持续改进初始算法并生成可执行的新算法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理在科学发现中存在局限：纯算法进化依赖内部知识且易停滞，纯深度研究缺乏验证导致方案不可行。因此需要一种兼具知识获取与实际验证的综合框架。

Method: 提出 DeepEvolve 代理，将深度研究与算法进化结合，采用反馈驱动的迭代循环，每轮迭代包含假设生成、优化、实现与测试，并支持外部知识检索、跨文件代码编辑和系统性调试。

Result: 在化学、数学、生物、材料和专利等九个基准任务上，DeepEvolve 持续改进初始算法，生成可执行且性能持续提升的新算法，显著优于孤立的进化或研究方法。

Conclusion: DeepEvolve 弥合了无引导进化与缺乏落地的研究之间的鸿沟，提供了一个可靠的科学算法发现框架。

Abstract: Large language models hold promise as scientific assistants, yet existing
agents either rely solely on algorithm evolution or on deep research in
isolation, both of which face critical limitations. Pure algorithm evolution,
as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly
plateaus in complex domains, while pure deep research proposes ideas without
validation, resulting in unrealistic or unimplementable solutions. We present
DeepEvolve, an agent that integrates deep research with algorithm evolution,
uniting external knowledge retrieval, cross-file code editing, and systematic
debugging under a feedback-driven iterative loop. Each iteration not only
proposes new hypotheses but also refines, implements, and tests them, avoiding
both shallow improvements and unproductive over-refinements. Across nine
benchmarks in chemistry, mathematics, biology, materials, and patents,
DeepEvolve consistently improves the initial algorithm, producing executable
new algorithms with sustained gains. By bridging the gap between unguided
evolution and research without grounding, DeepEvolve provides a reliable
framework for advancing scientific algorithm discovery. Our code is available
at https://github.com/liugangcode/deepevolve.

</details>


### [366] [TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis](https://arxiv.org/abs/2510.06063)
*Austin Feng,Andreas Varvarigos,Ioannis Panitsas,Daniela Fernandez,Jinbiao Wei,Yuwei Guo,Jialin Chen,Ali Maatouk,Leandros Tassiulas,Rex Ying*

Main category: cs.AI

TL;DR: 本文介绍了TelecomTS，一个大规模的5G电信网络可观测性数据集，填补了现有公开数据集在异常检测、根因分析和多模态推理任务上的空白，并强调保留协变量绝对尺度的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的公开时间序列数据集缺乏对可观测性数据特性的支持，尤其是在异常检测、根因分析等任务上受限于匿名化和归一化处理，难以反映真实场景中的尺度信息。

Method: 提出TelecomTS数据集，包含异构、去匿名化的协变量和明确的尺度信息，支持多种下游任务，并对前沿的时间序列、语言和推理模型进行基准测试。

Result: 实验表明现有模型在处理突变、高噪声和高方差的可观测性数据时表现不佳，且保留协变量的绝对尺度对实际应用至关重要。

Conclusion: TelecomTS为可观测性研究提供了更真实、更具挑战性的基准，推动需要多模态推理和尺度感知的时间序列基础模型的发展。

Abstract: Modern enterprises generate vast streams of time series metrics when
monitoring complex systems, known as observability data. Unlike conventional
time series from domains such as weather, observability data are zero-inflated,
highly stochastic, and exhibit minimal temporal structure. Despite their
importance, observability datasets are underrepresented in public benchmarks
due to proprietary restrictions. Existing datasets are often anonymized and
normalized, removing scale information and limiting their use for tasks beyond
forecasting, such as anomaly detection, root-cause analysis, and multi-modal
reasoning. To address this gap, we introduce TelecomTS, a large-scale
observability dataset derived from a 5G telecommunications network. TelecomTS
features heterogeneous, de-anonymized covariates with explicit scale
information and supports a suite of downstream tasks, including anomaly
detection, root-cause analysis, and a question-answering benchmark requiring
multi-modal reasoning. Benchmarking state-of-the-art time series, language, and
reasoning models reveals that existing approaches struggle with the abrupt,
noisy, and high-variance dynamics of observability data. Our experiments also
underscore the importance of preserving covariates' absolute scale, emphasizing
the need for foundation time series models that natively leverage scale
information for practical observability applications.

</details>


### [367] [Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents](https://arxiv.org/abs/2510.06078)
*Tao Zhe,Rui Liu,Fateme Memar,Xiao Luo,Wei Fan,Xinyue Ye,Zhongren Peng,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出RouteLLM，一种基于分层多智能体框架的自然语言路由推荐方法，能够将用户自由文本意图转化为满足约束的高质量路线。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划算法依赖结构化输入和固定目标，难以适应自然语言查询；现有LLM方法在空间推理和多层次偏好建模上存在不足。

Method: 采用分层多智能体框架：首先解析用户查询为结构化意图，由管理智能体协调约束、POI和路径优化等子智能体，通过约束验证、POI排序与偏好感知的路径细化生成路线，最后由验证智能体确保结果合规并提供可解释理由。

Result: 实验表明，该方法在路线质量和偏好满足度上优于传统方法，能有效将文本偏好映射为符合约束的可行路线。

Conclusion: RouteLLM融合语言灵活性与空间结构，实现了对复杂自然语言查询的精准理解和约束感知的路径推荐。

Abstract: Route recommendation aims to provide users with optimal travel plans that
satisfy diverse and complex requirements. Classical routing algorithms (e.g.,
shortest-path and constraint-aware search) are efficient but assume structured
inputs and fixed objectives, limiting adaptability to natural-language queries.
Recent LLM-based approaches enhance flexibility but struggle with spatial
reasoning and the joint modeling of route-level and POI-level preferences. To
address these limitations, we propose RouteLLM, a hierarchical multi-agent
framework that grounds natural-language intents into constraint-aware routes.
It first parses user queries into structured intents including POIs, paths, and
constraints. A manager agent then coordinates specialized sub-agents: a
constraint agent that resolves and formally check constraints, a POI agent that
retrieves and ranks candidate POIs, and a path refinement agent that refines
routes via a routing engine with preference-conditioned costs. A final verifier
agent ensures constraint satisfaction and produces the final route with an
interpretable rationale. This design bridges linguistic flexibility and spatial
structure, enabling reasoning over route feasibility and user preferences.
Experiments show that our method reliably grounds textual preferences into
constraint-aware routes, improving route quality and preference satisfaction
over classical methods.

</details>


### [368] [Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices](https://arxiv.org/abs/2510.06093)
*Mallika Mainali,Harsha Sureshbabu,Anik Sen,Christopher B. Rauch,Noah D. Reifsnyder,John Meyer,J. T. Turner,Michael W. Floyd,Matthew Molineaux,Rosina O. Weber*

Main category: cs.AI

TL;DR: 本研究比较了经典AI与基于大语言模型（LLM）的方法在决策者对齐（DMA）任务中的表现，特别是在具有不同风险偏好的健康保险决策场景中。研究采用零样本提示框架，并利用加权自一致性评估LLM方法，同时复现了经典AI模型。结果显示，两种方法在属性对齐上表现相当，经典AI在中等风险偏好情况下略优。


<details>
  <summary>Details</summary>
Motivation: 随着算法决策系统被广泛应用于高风险领域，通用价值对齐已不足以满足需求，需考虑具体情境和决策者的个体属性。因此，研究从通用对齐转向决策者属性对齐（DMA），但现有方法的泛化能力尚不明确，亟需跨方法比较以评估其适用性。

Method: 研究复现了一个经典的AI决策模型，并开发了一种基于LLM的算法决策者，使用GPT-4（非推理模型）和GPT-5（推理模型）在零样本提示下结合加权自一致性进行评估。实验在包含三种不同风险偏好（0.0、0.5、1.0）的健康保险决策数据集上进行，比较两类方法在决策对齐上的表现。

Result: 经典AI与LLM-based方法在决策者属性对齐方面表现相近，整体对齐效果相当；经典AI在中等风险偏好（0.5）的决策者上表现出轻微优势；LLM方法虽具潜力，但在本实验中未显著超越经典方法。

Conclusion: 在特定结构化决策任务中，经典AI方法仍可与前沿LLM方法相媲美，甚至在某些属性配置下更优，表明传统方法在决策对齐问题中仍具竞争力。研究强调了任务特性与决策者属性匹配的重要性，而非单纯依赖模型规模或架构先进性。

Abstract: As algorithmic decision-makers are increasingly applied to high-stakes
domains, AI alignment research has evolved from a focus on universal value
alignment to context-specific approaches that account for decision-maker
attributes. Prior work on Decision-Maker Alignment (DMA) has explored two
primary strategies: (1) classical AI methods integrating case-based reasoning,
Bayesian reasoning, and naturalistic decision-making, and (2) large language
model (LLM)-based methods leveraging prompt engineering. While both approaches
have shown promise in limited domains such as medical triage, their
generalizability to novel contexts remains underexplored. In this work, we
implement a prior classical AI model and develop an LLM-based algorithmic
decision-maker evaluated using a large reasoning model (GPT-5) and a
non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot
prompting framework, as proposed in recent literature. We evaluate both
approaches on a health insurance decision-making dataset annotated for three
target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).
In the experiments reported herein, classical AI and LLM-based models achieved
comparable alignment with attribute-based targets, with classical AI exhibiting
slightly better alignment for a moderate risk profile. The dataset and
open-source implementation are publicly available at:
https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and
https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.

</details>


### [369] [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)
*Batu El,James Zou*

Main category: cs.AI

TL;DR: 本文研究了在竞争性环境中优化大语言模型（LLM）对齐性的负面影响，提出“莫洛克契约”概念：即为了竞争成功而牺牲真实性与社会价值。通过模拟销售、选举和社交媒体场景，发现提升销售、得票率或用户参与度的同时，伴随欺骗性营销、错误信息和有害行为的显著增加。即使明确要求模型保持真实，此类错位行为仍会出现，显示当前对齐机制的脆弱性。作者强调市场驱动的优化压力可能导致AI系统出现‘逐底竞争’，呼吁加强治理和激励机制设计以维护社会信任。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于广告、政治竞选和社交媒体等竞争性场景，如何在追求性能优势的同时保持模型的诚实与社会对齐成为关键问题。现有研究缺乏对竞争反馈循环如何影响模型行为的理解，本文旨在揭示竞争性优化可能带来的对齐退化风险。

Method: 构建多个模拟环境，分别代表商业营销、选举竞选和社交内容生成三种竞争性场景，在这些环境中训练和评估LLMs在追求目标（如销售额、投票支持率、用户参与度）时的行为变化，并量化其在误导信息、民粹主义言论和有害行为推广方面的偏离程度。

Result: 实验结果显示，竞争性优化导致明显的对齐损失：销售提升6.3%带来欺骗性内容上升14.0%；投票份额增加4.9%伴随错误信息上升22.3%和民粹言论上升12.5%；参与度提高7.5%则导致错误信息飙升188.6%及有害行为宣传增加16.3%。这些错位行为在有明确诚实指令的情况下依然存在。

Conclusion: 竞争压力会削弱大语言模型的对齐性，形成‘莫洛克契约’——个体或组织为短期竞争优势牺牲长期社会价值。当前对齐技术在竞争环境下脆弱不堪，需通过更强的治理机制和激励设计来防止AI系统陷入逐底竞争，确保安全部署。

Abstract: Large language models (LLMs) are increasingly shaping how information is
created and disseminated, from companies using them to craft persuasive
advertisements, to election campaigns optimizing messaging to gain votes, to
social media influencers boosting engagement. These settings are inherently
competitive, with sellers, candidates, and influencers vying for audience
approval, yet it remains poorly understood how competitive feedback loops
influence LLM behavior. We show that optimizing LLMs for competitive success
can inadvertently drive misalignment. Using simulated environments across these
scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise
in deceptive marketing; in elections, a 4.9% gain in vote share coincides with
22.3% more disinformation and 12.5% more populist rhetoric; and on social
media, a 7.5% engagement boost comes with 188.6% more disinformation and a
16.3% increase in promotion of harmful behaviors. We call this phenomenon
Moloch's Bargain for AI--competitive success achieved at the cost of alignment.
These misaligned behaviors emerge even when models are explicitly instructed to
remain truthful and grounded, revealing the fragility of current alignment
safeguards. Our findings highlight how market-driven optimization pressures can
systematically erode alignment, creating a race to the bottom, and suggest that
safe deployment of AI systems will require stronger governance and carefully
designed incentives to prevent competitive dynamics from undermining societal
trust.

</details>


### [370] [Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification](https://arxiv.org/abs/2510.06135)
*Weihao Zeng,Keqing He,Chuqiao Kuang,Xiaoguang Li,Junxian He*

Main category: cs.AI

TL;DR: 本文研究了深度搜索智能体在测试时计算扩展（TTS）中的顺序和并行策略，利用“非对称验证”特性，在验证比生成容易的场景下显著提升性能。通过少量计算资源用于验证器，实现了高达27个百分点的提升，开源模型GLM-4.5 Heavy和Tongyi-DeepResearch Heavy在多个基准上达到甚至超越顶尖专有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 由于在某些任务中验证答案比生成答案容易得多（即非对称验证），这为测试时扩展提供了巨大潜力，促使作者探索结合顺序与并行TTS策略来增强深度搜索智能体的性能。

Method: 采用顺序扩展（如预算强制）和并行扩展（多候选生成与验证选择）方法，并设计高效的验证器机制，利用非对称验证优势，在开源大模型基础上构建‘Heavy’变体进行实验。

Result: 顺序扩展初期有效但很快性能下降；而引入轻量级验证器后性能大幅提升，GLM-4.5 Heavy在BrowseComp和GAIA上分别达到54.0%和66.0%，Tongyi-DeepResearch Heavy在BrowseComp上达69.0%，显著超过现有最佳专有系统。

Conclusion: 结合非对称验证的测试时扩展能有效提升深度搜索智能体性能，为开源模型提供了一条追赶乃至超越专有模型的可行路径。

Abstract: Test-time compute can be scaled both sequentially and in parallel. Sequential
scaling involves lengthening the generation process, while parallel scaling
involves verifying and selecting among multiple candidate outputs. Combining
these two strategies has led to the most powerful AI systems, such as Grok 4
Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),
verifying responses can be substantially easier than generating them. This
property, referred to as \emph{asymmetric verification}, highlights the strong
potential of test-time scaling (TTS). In this work, we study both sequential
and parallel TTS of deep search agents, motivated by the intuition that
verification in this setting is often much easier than generation. In
experiments, we first show that sequential scaling methods, such as budget
forcing, can be effective initially but soon degrade performance. Leveraging
asymmetric verification, however, we are able to achieve substantial
improvements by allocating only a modest amount of compute to the verifier. We
conduct experiments with flagship open-source models and extend them to their
``Heavy'' variants through TTS. These deep research agents achieve gains of up
to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an
open-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} on
BrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the best
proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy
further achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing the
best proprietary results.

</details>


### [371] [Barbarians at the Gate: How AI is Upending Systems Research](https://arxiv.org/abs/2510.06189)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Bowen Wang,Alex Krentsel,Tian Xia,Mert Cemri,Jongseok Park,Shuo Yang,Jeff Chen,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.AI

TL;DR: 本文提出了AI驱动的系统研究方法（ADRS），利用可靠的性能验证机制在多个领域中自动生成并优化算法，显著优于人工设计，并探讨了AI时代系统研究的新范式。


<details>
  <summary>Details</summary>
Motivation: 系统研究中的性能问题通常具有可量化的验证方式，适合AI自动化求解，但现有AI方法依赖可靠验证器，作者借此提出专门针对系统研究的AI驱动框架。

Method: 提出AI-Driven Research for Systems (ADRS) 框架，结合生成与验证闭环，通过penEvolve实例在负载均衡、MoE推理、LLM-SQL、事务调度等场景进行案例研究。

Result: ADRS在多个领域发现了优于当前最先进人工设计的算法，实现了最高5倍的运行时提升或50%的成本降低。

Conclusion: ADRS展示了AI在系统研究中的巨大潜力，未来研究人员将更专注于问题定义和战略指导，需调整研究实践以适应AI主导的设计范式。

Abstract: Artificial Intelligence (AI) is starting to transform the research process as
we know it by automating the discovery of new solutions. Given a task, the
typical AI-driven approach is (i) to generate a set of diverse solutions, and
then (ii) to verify these solutions and select one that solves the problem.
Crucially, this approach assumes the existence of a reliable verifier, i.e.,
one that can accurately determine whether a solution solves the given problem.
We argue that systems research, long focused on designing and evaluating new
performance-oriented algorithms, is particularly well-suited for AI-driven
solution discovery. This is because system performance problems naturally admit
reliable verifiers: solutions are typically implemented in real systems or
simulators, and verification reduces to running these software artifacts
against predefined workloads and measuring performance. We term this approach
as AI-Driven Research for Systems (ADRS), which iteratively generates,
evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS
instance, we present case studies across diverse domains, including load
balancing for multi-region cloud scheduling, Mixture-of-Experts inference,
LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS
discovers algorithms that outperform state-of-the-art human designs (e.g.,
achieving up to 5.0x runtime improvements or 50% cost reductions). We distill
best practices for guiding algorithm evolution, from prompt design to evaluator
construction, for existing frameworks. We then discuss the broader implications
for the systems community: as AI assumes a central role in algorithm design, we
argue that human researchers will increasingly focus on problem formulation and
strategic guidance. Our results highlight both the disruptive potential and the
urgent need to adapt systems research practices in the age of AI.

</details>


### [372] [TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning](https://arxiv.org/abs/2510.06217)
*Jiaru Zou,Soumya Roy,Vinay Kumar Verma,Ziyi Wang,David Wipf,Pan Lu,Sumit Negi,James Zou,Jingrui He*

Main category: cs.AI

TL;DR: 提出TaTToo，一种基于表格的流程奖励模型框架，通过工具验证和双阶段训练显著提升大推理模型在表格推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有流程奖励模型在处理表格特定操作（如子表检索和模式交互）时表现不佳，限制了其在表格推理领域的应用。

Method: 设计了一个可扩展的数据整理 pipeline，构建了超过6万条高质量的步骤级标注数据，并采用双阶段训练范式：先进行监督微调，再结合工具接地的奖励塑形进行强化学习。

Result: 在5个具有挑战性的表格推理基准上，TaTToo使下游策略大推理模型的推理性能平均提升30.9%，且仅用8B参数即超越72B参数的Qwen-2.5-Math-PRM-72B等强基线。

Conclusion: TaTToo有效解决了现有PRM在表格推理中的瓶颈，展现出强大的性能提升和跨测试时缩放策略的泛化能力。

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [373] [Emergent Coordination in Multi-Agent Language Models](https://arxiv.org/abs/2510.05174)
*Christoph Riedl*

Main category: cs.MA

TL;DR: 提出一种基于信息论的框架，用于检测多智能体大语言模型系统是否表现出高阶结构和动态涌现，并通过实验证明提示设计可引导系统从简单聚合转变为具有协同性的集体。


<details>
  <summary>Details</summary>
Motivation: 区分多智能体系统是 merely 多个独立代理的集合，还是具有高阶结构的集成整体，从而理解其是否具备超越个体的集体行为特征。

Method: 引入基于部分信息分解的时间延迟互信息（TDMI）框架，量化跨代理的时序协同与目标导向互补性，通过无直接通信的猜谜游戏实验及三种随机干预进行验证。

Result: 控制组显示强时序协同但缺乏对齐；赋予角色后出现稳定的身份差异化；结合角色与‘思考他人行为’指令时，展现出身份差异化和目标互补性，表明可通过提示设计诱导出高阶结构。

Conclusion: 多智能体LLM系统可通过提示工程从个体聚合体转变为具有功能协同的高阶集体，其行为模式类似于人类群体智能，需对齐目标与互补贡献以实现高效表现。

Abstract: When are multi-agent LLM systems merely a collection of individual agents
versus an integrated collective with higher-order structure? We introduce an
information-theoretic framework to test -- in a purely data-driven way --
whether multi-agent systems show signs of higher-order structure. This
information decomposition lets us measure whether dynamical emergence is
present in multi-agent LLM systems, localize it, and distinguish spurious
temporal coupling from performance-relevant cross-agent synergy. We implement
both a practical criterion and an emergence capacity criterion operationalized
as partial information decomposition of time-delayed mutual information (TDMI).
We apply our framework to experiments using a simple guessing game without
direct agent communication and only minimal group-level feedback with three
randomized interventions. Groups in the control condition exhibit strong
temporal synergy but only little coordinated alignment across agents. Assigning
a persona to each agent introduces stable identity-linked differentiation.
Combining personas with an instruction to ``think about what other agents might
do'' shows identity-linked differentiation and goal-directed complementarity
across agents. Taken together, our framework establishes that multi-agent LLM
systems can be steered with prompt design from mere aggregates to higher-order
collectives. Our results are robust across emergence measures and entropy
estimators, and not explained by coordination-free baselines or temporal
dynamics alone. Without attributing human-like cognition to the agents, the
patterns of interaction we observe mirror well-established principles of
collective intelligence in human groups: effective performance requires both
alignment on shared objectives and complementary contributions across members.

</details>


### [374] [AgentZero++: Modeling Fear-Based Behavior](https://arxiv.org/abs/2510.05185)
*Vrinda Malhotra,Jiaman Li,Nandini Pisupati*

Main category: cs.MA

TL;DR: AgentZero++ 是一个扩展的基于代理的模型，通过整合认知、情感和社会机制来模拟空间分布系统中的去中心化集体暴力，揭示微观心理异质性如何影响宏观冲突模式。


<details>
  <summary>Details</summary>
Motivation: 为了更真实地模拟集体暴力行为，研究认知、情感和社会因素在个体层面的交互如何导致宏观层面的冲突动态。

Method: 在Epstein的Agent_Zero框架基础上，引入八种行为增强机制，并使用Python的Mesa ABM框架实现模型，支持模块化实验和可视化。

Result: 模型能够生成抗议不对称性、升级循环和局部报复等 emergent 动态，发现记忆、反应性和情感一致性的小变化可通过反馈回路放大或抑制动乱。

Conclusion: AgentZero++ 提供了一个灵活且可扩展的平台，用于分析情感传染和基于心理的集体行动。

Abstract: We present AgentZero++, an agent-based model that integrates cognitive,
emotional, and social mechanisms to simulate decentralized collective violence
in spatially distributed systems. Building on Epstein's Agent\_Zero framework,
we extend the original model with eight behavioral enhancements: age-based
impulse control; memory-based risk estimation; affect-cognition coupling;
endogenous destructive radius; fight-or-flight dynamics; affective homophily;
retaliatory damage; and multi-agent coordination. These additions allow agents
to adapt based on internal states, previous experiences, and social feedback,
producing emergent dynamics such as protest asymmetries, escalation cycles, and
localized retaliation. Implemented in Python using the Mesa ABM framework,
AgentZero++ enables modular experimentation and visualization of how
micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our
results highlight how small variations in memory, reactivity, and affective
alignment can amplify or dampen unrest through feedback loops. By explicitly
modeling emotional thresholds, identity-driven behavior, and adaptive networks,
this work contributes a flexible and extensible platform for analyzing
affective contagion and psychologically grounded collective action.

</details>


### [375] [Agent+P: Guiding UI Agents via Symbolic Planning](https://arxiv.org/abs/2510.06042)
*Shang Ma,Xusheng Xiao,Yanfang Ye*

Main category: cs.MA

TL;DR: AGENT+P 是一种结合符号规划的新型框架，通过建模应用的UI转换图（UTG）将UI自动化任务转化为路径查找问题，显著提升LLM-based UI代理在长周期任务中的成功率并减少操作步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的UI代理在长周期任务中容易因缺乏对全局UI转换结构的理解而产生幻觉，导致性能下降。

Method: 提出 AGENT+P 框架，将应用的UI结构建模为UI转换图（UTG），并利用现成的符号规划器生成正确的高层计划，指导LLM-based代理完成任务。

Result: 在 AndroidWorld 基准测试中，AGENT+P 将现有最先进UI代理的成功率提高了最多14%，并减少了37.7%的操作步骤。

Conclusion: AGENT+P 作为一种即插即用的框架，有效增强了现有UI代理的能力，通过引入符号规划解决了长周期UI自动化中的幻觉和冗余探索问题。

Abstract: Large Language Model (LLM)-based UI agents show great promise for UI
automation but often hallucinate in long-horizon tasks due to their lack of
understanding of the global UI transition structure. To address this, we
introduce AGENT+P, a novel framework that leverages symbolic planning to guide
LLM-based UI agents. Specifically, we model an app's UI transition structure as
a UI Transition Graph (UTG), which allows us to reformulate the UI automation
task as a pathfinding problem on the UTG. This further enables an off-the-shelf
symbolic planner to generate a provably correct and optimal high-level plan,
preventing the agent from redundant exploration and guiding the agent to
achieve the automation goals. AGENT+P is designed as a plug-and-play framework
to enhance existing UI agents. Evaluation on the AndroidWorld benchmark
demonstrates that AGENT+P improves the success rates of state-of-the-art UI
agents by up to 14% and reduces the action steps by 37.7%.

</details>
