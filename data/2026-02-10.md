<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 205]
- [cs.CL](#cs.CL) [Total: 96]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.RO](#cs.RO) [Total: 83]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.IR](#cs.IR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 270]
- [cs.AI](#cs.AI) [Total: 114]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文提出了一种用于量化鞋印中‘偶然特征’（如划痕、磨损）罕见性的分层贝叶斯模型，以提升法医鞋印比对的证据强度评估准确性。


<details>
  <summary>Details</summary>
Motivation: 传统鞋印分析仅依赖品牌、型号和尺寸匹配，但大量同款鞋存在；需通过个体化‘偶然特征’进行区分，而其罕见性量化缺乏可靠方法。

Method: 构建基于潜在高斯模型的分层贝叶斯模型，采用集成嵌套拉普拉斯近似（INLA）实现高效推断，并引入空间变化系数建模鞋底纹路与偶然特征位置的关系。

Result: 在留出数据上表现优于现有方法，提升了鞋印分析的准确性和可靠性。

Conclusion: 该模型为法医鞋印中偶然特征的统计评估提供了可扩展、空间感知且高精度的量化框架。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于归因的人类先验对齐方法，通过将人类先验编码为模型应依赖的输入区域（如边界框），并利用高保真子集选择归因方法揭示模型决策依据，在训练中惩罚偏离先验区域的归因，从而提升模型准确性和决策合理性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅提供类别标签，易导致模型依赖捷径相关性而非真正意图证据；而人类先验虽可约束该行为，但模型表征常与人类感知不一致，难以对齐。

Method: 将人类先验编码为指定输入区域（如bounding boxes），结合高保真子集选择归因方法在训练中暴露模型决策依据，并设计归因约束损失函数，惩罚模型对非先验区域的依赖。

Result: 在图像分类和MLLM-based GUI agent的点击决策任务上验证有效；在常规分类与自回归生成设置下，均一致提升了任务准确率和决策合理性。

Conclusion: 基于归因的人类先验对齐方法能有效引导模型关注预期证据区域，兼顾性能提升与可解释性增强。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 本文提出了MAU-Set工业异常理解数据集和MAU-GPT多模态大模型，通过分层任务设计与AMoE-LoRA机制提升跨领域缺陷识别与推理能力，在多个工业领域上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工业图像分析方法受限于数据集覆盖不足和模型对复杂多样异常模式泛化能力差的问题。

Method: 构建了覆盖多工业领域的分层任务数据集MAU-Set，并提出域适配的多模态大模型MAU-GPT，引入AMoE-LoRA机制联合优化异常感知专家与通用专家。

Result: MAU-GPT在所有测试工业领域上均持续超越先前最优方法，验证了其在可扩展自动化工业检测中的有效性。

Conclusion: MAU-Set与MAU-GPT为工业异常理解提供了新基准与强效工具，推动高质量、高鲁棒性智能质检发展。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了RetSAM，一个用于眼底图像的通用视网膜分割与定量分析框架，支持多目标分割和标准化生物标志物提取，显著提升分割性能并促进大规模眼科研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于公开多标签数据集稀缺及缺乏统一的从分割到定量分析的流程，难以开展大规模视网膜表型与眼病/系统性疾病关联分析。

Method: 提出RetSAM框架，基于20余万张眼底图像训练，采用多阶段策略融合私有与公开数据；支持三类任务，分割5种解剖结构、4种表型模式及20余种病变类型，并转化为30余种标准化生物标志物（涵盖形态、血管几何与退行性变化）。

Result: 在17个公开数据集上分割性能领先，平均DSC提升3.9个百分点，多任务挑战基准最高提升15个百分点；泛化性强，适用于不同人群、设备与临床场景；所生成生物标志物成功支持糖尿病视网膜病变、年龄相关性黄斑变性、青光眼及病理性近视等主要眼病的系统性关联分析。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的定量表型，为大规模眼科研究与临床转化提供了可扩展、鲁棒的基础工具。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: 本文提出CR-VLM，一种基于激活引导的可配置拒绝机制，用于提升视觉语言模型（VLM）在不同用户需求和上下文下的安全拒绝能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM拒绝机制过于统一，无法适应多样化用户需求与上下文约束，导致拒绝不足或过度拒绝。

Method: 提出CR-VLM框架，包含三部分：(1) 教师强制机制提取可配置拒绝向量；(2) 门控机制缓解过度拒绝；(3) 反事实视觉增强模块对齐视觉表征与拒绝要求。

Result: 在多个数据集和多种VLM上验证了CR-VLM在有效性、效率与鲁棒性方面的优越性。

Conclusion: CR-VLM为实现用户自适应的安全对齐提供了可扩展路径。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: 本文提出了Vectra，一个首个无参考、基于多模态大语言模型（MLLM）的电商图像内机器翻译（IIMT）视觉质量评估框架，包含多维可解释评分系统、大规模真实产品图像数据集和4B参数MLLM模型，在人类偏好对齐和评分性能上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有IIMT视觉质量评估方法存在两大缺陷：参考-based指标（如SSIM、FID）缺乏可解释性；模型即裁判（model-as-judge）方法缺乏领域适配、细粒度的奖励信号，尤其在上下文密集、多模态缺陷的产品图像中表现不足。

Method: 提出Vectra框架，包含三部分：(1) Vectra Score——14维可解释质量指标体系，引入空间感知的缺陷区域比（DAR）降低标注歧义；(2) Vectra Dataset——基于110万真实电商图构建，含2K评测基准、30K推理标注指令微调集、3.5K专家偏好标注；(3) Vectra Model——4B参数MLLM，同步输出定量评分与诊断性推理。

Result: Vectra在人类排名相关性上达到SOTA；其MLLM在评分任务上超越GPT-5和Gemini-3等主流大模型；数据集与模型将在录用后开源。

Conclusion: Vectra首次实现了无参考、领域定制、可解释、可诊断的IIMT视觉质量评估，为多模态生成评估提供了新范式，并推动电商AI落地可靠性提升。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 本文提出了一种结合MobileNetV3-Large与EfficientNetB0的混合CNN架构，用于孟加拉国纸币识别，兼顾高精度与低计算开销，并通过多源数据增强和可解释AI提升鲁棒性与可信度。


<details>
  <summary>Details</summary>
Motivation: 解决视障人士依赖他人识别纸币所面临的欺诈与剥削风险，现有模型在真实场景下泛化能力不足、缺乏可解释性。

Method: 构建包含控制与真实场景的新孟加拉国纸币数据集，并融合四个公开数据集；提出MobileNetV3-Large与EfficientNetB0联合特征提取模块 + 轻量MLP分类器；采用五折交叉验证及七项评估指标，并引入LIME与SHAP进行可解释性分析。

Result: 在控制数据集上达97.95%准确率，在复杂背景达92.84%，全数据集融合达94.98%；各项指标（精度、召回、F1、Kappa、MCC、AUC等）均表现优异。

Conclusion: 所提混合模型在精度、效率与可解释性之间取得良好平衡，适用于资源受限设备，为辅助技术中的货币识别提供了实用且可信的解决方案。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [8] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于视角不变语义特征的条件输入方法，用于提升全头3D GAN的训练稳定性与生成一致性，避免传统视角角条件导致的方向偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统全头3D GAN使用视角角作为条件输入，导致3D空间学习存在方向偏差，造成不同视角下生成质量与多样性不一致、全局不连贯；同时，无条件训练易发生严重模式坍塌。

Method: 采用前视图CLIP特征作为视角不变的语义条件，并构建合成多视角头部图像数据集（利用FLUX.1 Kontext扩展高质量前脸数据集），使同一主体多视角图像共享同一语义条件；该设计解耦生成能力与视角，强化语义对齐与跨视角监督整合。

Result: 在全头合成与单视图GAN反演任务中，本方法显著提升了生成保真度、多样性与泛化性。

Conclusion: 视角不变语义条件有效缓解了视角条件引入的方向偏差，增强了3D头像生成的全局一致性与训练稳定性，为全头3D生成提供了更鲁棒、更语义一致的条件建模范式。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [9] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 本文探索了基于LeJEPA启发的高斯约束表示在无监督3D场景重建中的应用，提出三种渐进式改进流程，在IMC2025数据集上验证其提升场景分离与位姿估计鲁棒性的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从多场景、无结构图像集合中进行无监督3D场景重建的难题，尤其应对图像来源混杂、视觉歧义大、含大量外点等现实挑战。

Method: 提出三种渐进式pipeline，最终采用LeJEPA启发的方法，在图像嵌入上施加各向同性高斯约束，不追求新理论保证，而侧重实证评估该约束对聚类一致性和位姿估计鲁棒性的影响。

Result: 在IMC2025上实验表明，高斯约束嵌入相比启发式基线，在场景分离和位姿合理性方面均有提升，尤其在视觉歧义大的情况下效果更显著。

Conclusion: 理论驱动的表征约束为连接自监督学习原理与实际SfM流程提供了有前景的方向。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [10] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出XAI-CLIP，一种基于ROI引导的扰动框架，利用多模态视觉-语言模型嵌入定位临床相关解剖区域，提升医学图像分割模型的可解释性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法在医学图像分割中存在计算开销大、解释噪声多、解剖不相关等问题，阻碍临床信任与部署。

Method: 提出XAI-CLIP框架：结合CLIP等多模态模型进行语言引导的解剖区域定位，并在分割任务中实施区域感知的定向扰动，生成边界清晰、计算高效的显著图。

Result: 在FLARE22和CHAOS数据集上，相比传统扰动方法，运行时间减少60%，Dice分数提升44.6%，IoU提升96.7%；定性结果表明归因图更干净、解剖一致性更强。

Conclusion: 将多模态视觉-语言表征引入扰动型XAI框架，可显著提升医学图像分割模型的可解释性、效率与临床可用性。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [11] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: 本文提出VideoNeuMat两阶段方法，从视频扩散模型中提取可复用的神经材质资产：先微调大视频模型生成可控光照/视角下的材质视频，再通过微调的小型视频模型驱动LRM从17帧视频单次推理重建泛化性强的神经材质。


<details>
  <summary>Details</summary>
Motivation: 当前生成式材质模型受限于高质量训练数据缺乏；虽视频生成模型能产生逼真材质外观，但其材质知识与几何、光照纠缠，难以解耦和复用。

Method: 两阶段流程：1）微调Wan 2.1（14B）视频模型，生成受控相机与光照轨迹下的材质样本视频，构建虚拟gonioreflectometer；2）基于Wan 1.3B微调的Large Reconstruction Model（LRM），从17帧视频单次推理重建紧凑神经材质参数。

Result: 所生成神经材质在真实感与多样性上远超有限合成训练数据，验证了从互联网规模视频模型向独立、可复用神经3D资产迁移材质知识的有效性。

Conclusion: VideoNeuMat成功解耦并提取视频扩散模型中的材质知识，实现了高质量、泛化性强、可复用的神经材质资产生成，为3D内容创作提供了新范式。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [12] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN的图像识别框架，用于鸟类物种分类、鸟群类型与规模估计，以提升鸟击风险预测与航空安全。


<details>
  <summary>Details</summary>
Motivation: 现有雷达系统无法识别鸟类物种，而不同物种的飞行行为和高度偏好差异显著，影响鸟击风险评估。

Method: 设计基于卷积神经网络（CNN）的图像分类框架，实现鸟类物种识别、鸟群类型判断及鸟群规模估计，结合摄像头系统进行自主视觉检测。

Result: 实现了鸟类物种、鸟群类型与规模的准确识别，为物种特异性飞行路径预测及风险评估提供关键输入。

Conclusion: 该视觉识别框架弥补了传统雷达系统的不足，通过多维度鸟类特征分析，提升了鸟击预警的精准性与航空安全水平。

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

</details>


### [13] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种用于从超高速运动模糊图像中恢复3D形状的新型可微逆渲染方法，通过设计快速可微的重心坐标求解器，显著提升计算效率（最高4.57倍加速），并成功实现对高速平移与旋转物体的前向模糊模拟与反向3D形状重建。


<details>
  <summary>Details</summary>
Motivation: 传统多视图立体（MVS）等3D重建方法在极端运动模糊图像上失效，而此类模糊常见于体育（如高速球体）和工业（如旋转机械）场景，亟需能处理超快运动模糊的几何恢复方法。

Method: 提出一种端到端可微的逆渲染框架，核心是设计一个快速重心坐标求解器以避免传统渲染中重复计算的瓶颈；该求解器支持高效、光真实感的高速运动模糊前向合成，并使梯度可从模糊图像反传至3D形状参数。

Result: 在快速平移与旋转两类典型运动下验证了方法：前向模拟高效且逼真；反向可从单帧/少帧模糊图像中成功恢复3D形状，显著拓展了基于视觉的3D重建能力边界。

Conclusion: 本工作首次将可微逆渲染系统性应用于超高速运动模糊图像的3D形状恢复，兼顾计算效率与重建精度，为动态场景下的无标记三维感知提供了新范式。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [14] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLMs）在多物体视觉任务中出现幻觉或识别失败等错误的内在机制，提出通过分析开放权重VLMs的表征几何结构，提取‘概念向量’并验证其对模型行为的可操控性，发现概念向量间的几何重叠与特定错误模式强相关。


<details>
  <summary>Details</summary>
Motivation: VLMs在多物体视觉任务中存在类似人类认知限制（如‘绑定问题’）的失败现象，但其内部机制尚不清楚。

Method: 分析Qwen、InternVL、Gemma等开放权重VLMs的表征几何，提取并比较不同方法得到的‘概念向量’，并通过干预实验（steering interventions）验证其对模型行为的调控能力。

Result: 概念向量之间的几何重叠程度与模型特定错误模式（如幻觉、误识别）显著相关，为理解VLM视觉失败提供了定量表征框架。

Conclusion: VLMs的视觉失败根源可归因于其内部概念表征的几何结构特性，尤其是概念向量间的非正交性或重叠性，这为诊断和改进模型鲁棒性提供了新路径。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [15] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus是一个新框架，通过在几何和外观层面学习形状概念，实现个性化3D形状生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在跨类别场景下灵活提取并组合几何与外观属性以生成个性化3D形状。

Method: 提出将3D形状个性化建模为从参考形状中提取可复用、类别无关的几何与外观属性，并结合文本生成新形状；设计渐进式优化策略解耦几何与外观概念学习；扩展至区域级概念学习，引入上下文感知与上下文无关损失。

Result: PEGAsus能有效从多样参考形状中提取属性，并与文本灵活组合生成新形状，在细粒度控制、跨类别生成及多样性方面表现优异，定量与定性结果均优于现有SOTA方法。

Conclusion: PEGAsus通过解耦且可扩展的多层级概念学习框架，显著提升了个性化3D形状生成的能力与灵活性。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [16] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出Fixed-frame Modality Gap Theory，精准刻画多模态嵌入间的几何差异（即模态间隙），并基于此提出无需训练的对齐策略ReAlign及可扩展的MLLM预训练范式ReVision，利用海量无配对数据实现高效、低成本的多模态大模型缩放。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于过于简化的各向同性假设，难以在大规模场景中有效弥合多模态对比学习中固有的模态间隙（Modality Gap）几何异常问题。

Method: 提出Fixed-frame Modality Gap Theory，将模态间隙分解为稳定偏置与各向异性残差；基于该理论设计训练无关的ReAlign策略（含Anchor、Trace、Centroid三步对齐），利用大规模无配对数据统计信息对齐文本到图像表征分布；进一步构建ReVision预训练范式，将ReAlign融入MLLM预训练阶段。

Result: ReVision可在无需大规模高质量图文对的情况下，使MLLM在视觉指令微调前即学习图像表征分布；实验证明统计对齐的无配对数据可有效替代昂贵图文对，提升MLLM扩展效率与鲁棒性。

Conclusion: 精准建模模态间隙几何结构并利用无配对数据进行统计对齐，是实现多模态大语言模型高效、可扩展训练的一条可行且稳健路径。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [17] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需学习的高效4D高斯分割框架TIBR4D，通过两阶段迭代边界优化（IGIT和RCC）提升动态4D高斯场景中物体级分割的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的物体级分割面临运动复杂、遮挡严重及边界模糊等挑战。

Method: 提出两阶段迭代边界优化方法：第一阶段为基于时间片段的迭代高斯实例追踪（IGIT），第二阶段为逐帧高斯渲染范围控制（RCC）；并引入时间分割合并策略以兼顾身份一致性和动态感知能力。

Result: 在HyperNeRF和Neu3D数据集上实验表明，该方法生成的物体高斯点云边界更清晰、精度更高、效率优于现有SOTA方法。

Conclusion: TIBR4D是一种高效、学习自由的4D高斯分割框架，在处理动态场景、遮挡和边界模糊问题上具有显著优势。

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [18] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: 本文提出了一种名为Fair Context Learning（FCL）的测试时自适应（TTA）框架，用于提升视觉-语言模型（如CLIP）在分布偏移下的鲁棒性，通过避免熵最小化、解耦增强探索与公平性驱动的文本上下文校准来缓解共享证据偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的TTA方法依赖熵最小化，易放大虚假相关性、导致高置信错误，尤其当类别共享视觉特征时；而VLMs在分布偏移下性能显著下降。

Method: FCL是一种阶段性TTA框架：（i）基于增强的探索以识别潜在类别候选；（ii）公平性驱动的校准，通过调整文本上下文使模型对共有的视觉证据具有均等敏感性，从而缓解部分特征过度依赖。

Result: 在多种域偏移和细粒度分类基准上，FCL取得了与当前最优TTA方法相当甚至更优的适应性能，并验证了其理论动机的有效性。

Conclusion: FCL通过显式建模和缓解共享证据偏差，为VLMs的测试时自适应提供了一种更鲁棒、更公平的新范式，无需依赖易出错的熵最小化。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [19] [Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering](https://arxiv.org/abs/2602.08724)
*Geng Lin,Matthias Zwicker*

Main category: cs.CV

TL;DR: 本文提出RotLight方法，通过简单旋转物体的采集设置和引入代理网格，有效解决逆向渲染中反照率估计不准确和阴影烘焙问题，显著提升2DGS-based逆向渲染的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有逆向渲染方法在从观测辐射中估计材质（尤其是反照率）和光照时存在高度歧义，导致反照率颜色不准、阴影被错误‘烘焙’，即使使用正则化也难以缓解。

Method: 提出RotLight采集方案——仅需在采集过程中将物体旋转数次（最少两次）；并为2DGS-based逆向渲染引入代理网格，用于精确入射光追踪、施加残差约束及改善全局光照建模。

Result: 在合成与真实数据集上验证，该方法显著提升了反照率估计质量，同时保持高效计算性能，有效抑制了颜色失真与烘焙阴影等常见伪影。

Conclusion: RotLight是一种轻量、实用且高效的逆向渲染增强方案，通过极简硬件改动（旋转）与几何辅助建模（代理网格），在不显著增加计算负担前提下，实质性缓解了材质估计的固有歧义性。

Abstract: Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.

</details>


### [20] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 本文研究了将自适应神经模糊推理系统（ANFIS）集成到CNN中对模型鲁棒性和可解释性的效果，发现其对不同架构的影响具有差异性：ResNet18-ANFIS提升了对抗鲁棒性，而VGG-ANFIS则常表现更差。


<details>
  <summary>Details</summary>
Motivation: CNN虽性能强但缺乏可解释性且易受对抗攻击；已有工作用ANFIS替代全连接层提升可解释性，但其鲁棒性尚未被充分研究。

Method: 将标准CNN（ConvNet、VGG、ResNet18）与对应ANFIS增强版本在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100上进行对比，评估其在PGD（基于梯度）和Square（无梯度）对抗攻击下的表现。

Result: ANFIS集成未一致提升干净样本准确率；其对鲁棒性的影响依赖于网络架构：ResNet18-ANFIS鲁棒性提升，VGG-ANFIS常劣于基线。

Conclusion: 神经模糊增强可在特定架构下提升鲁棒性，但并非普适有效方案。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [21] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: 本文介绍了UNIKIE-BENCH，一个用于评估大图文模型（LMMs）在真实文档中关键信息抽取（KIE）能力的统一基准，包含约束类别与开放类别两个轨道，并通过15个前沿LMM实验揭示了当前模型在复杂布局、长尾字段和跨场景泛化上的显著瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现实文档在版式结构、视觉质量及任务需求上差异巨大，现有KIE方法难以全面评估大图文模型（LMMs）的真实能力，亟需统一、系统、多样化的基准。

Method: 构建UNIKIE-BENCH统一基准，包含两个互补轨道：（1）约束类别KIE（按实际场景预定义schema）；（2）开放类别KIE（提取文档中所有显式存在的关键信息）；并在15个SOTA LMM上开展系统评测。

Result: 实验表明，当前LMMs在多样化schema、长尾关键字段和复杂版式下性能显著下降，且在不同文档类型与场景间表现差异明显，暴露出定位精度与版式感知推理能力的严重不足。

Conclusion: UNIKIE-BENCH揭示了LMM-based KIE在接地准确性与布局感知推理方面的持续挑战，为未来研究提供了可复现的评估标准与改进方向。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [22] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent is a data-efficient, explainable dental diagnostic framework using a Vision-Language Model (VLM) on multi-view smartphone photos, embedding clinical reasoning without VLM fine-tuning, to support early-stage, accessible oral health assessment.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing AI dental diagnosis methods—such as reliance on large expert-annotated datasets, lack of clinical reasoning integration, and poor generalization under real-world imaging conditions—especially for underserved populations with limited access to professional care.

Method: OMNI-Dent integrates dental expert heuristics into a VLM-based pipeline, operating on multi-view smartphone photographs; it leverages the VLM’s pre-trained visual-linguistic capabilities without dental-specific fine-tuning, enabling tooth-level evaluation and explainable, clinical-reasoning-informed diagnosis.

Result: A data-efficient, explainable, and generalizable diagnostic framework that works on non-curated, real-world smartphone images and supports early-stage abnormality detection and triage decisions without requiring specialized imaging or model retraining.

Conclusion: OMNI-Dent demonstrates that incorporating structured clinical reasoning into off-the-shelf VLMs enables practical, accessible, and interpretable dental screening—bridging the gap between AI capability and real-world clinical utility in resource-limited settings.

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [23] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: 本文提出了一种名为COMBOOD的无监督半参数框架，用于图像识别任务中的分布外（OOD）检测，通过结合最近邻与马氏距离两种度量信号，提升近OOD和远OOD场景下的检测准确性，并在多个基准数据集上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在近OOD场景中表现不佳，而实际应用中近OOD情况普遍存在；需要一种兼顾近OOD与远OOD检测性能的鲁棒方法。

Method: 提出半参数框架COMBOOD，融合非参数的最近邻距离与参数化的马氏距离，生成统一的OOD置信度得分；适配多种特征提取策略，并具有线性计算复杂度。

Result: 在OpenOOD v1/v1.5及文档数据集上，COMBOOD在近OOD和远OOD检测准确率上均优于当前最优方法，多数结果具有统计显著性，且计算开销随嵌入维数线性增长。

Conclusion: COMBOOD是一种高效、准确且可扩展的OOD检测框架，适用于实际图像识别系统中的自动化部署。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [24] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: 本文提出了PipeMFL-240K，首个大规模公开的管道磁通泄漏（MFL）伪彩色图像目标检测数据集与基准，包含24万张图像和19万个高质量标注，旨在解决MFL智能解释中缺乏标准数据导致的模型不可比、难复现问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在MFL自动解释中潜力巨大，但受限于缺乏大规模公开数据集与统一基准，导致模型评估不公、研究难以复现。

Method: 构建了PipeMFL-240K数据集：涵盖11条总长约1480公里的管道，含240,320张MFL伪彩色图像和191,530个高精度边界框标注；涵盖12类缺陷，具有长尾分布、大量极小目标（仅数像素）及强类内差异等挑战；并基于SOTA目标检测器开展系统性基线实验。

Result: 实验表明当前主流检测器在该数据集上性能仍显著受限，尤其对极小目标和长尾类别识别效果差，验证了数据集的挑战性；确立了多个检测模型的性能基线。

Conclusion: PipeMFL-240K是首个面向管道MFL检测的大规模公开数据集与基准，为推动MFL智能诊断、维护决策及可复现算法研究提供了关键基础设施。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [25] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了首个专门针对遥感领域复杂推理任务的视觉语言推理基准VLRS-Bench，涵盖认知、决策与预测三大维度，包含2000个长文本问答对，并揭示了当前多模态大模型在遥感推理上的显著瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准严重偏向感知任务（如目标识别、场景分类），难以支撑多模态大语言模型在认知密集型遥感应用中的发展。

Method: 构建了首个面向遥感复杂推理的视觉语言基准VLRS-Bench，按认知、决策、预测三维度设计，含2000个平均71词的问答对、14类任务及最多八阶段时序内容；采用融合遥感先验与专家知识的专用构建流程以保障地学真实性与推理复杂度。

Result: 实验表明当前最先进多模态大语言模型在VLRS-Bench上表现显著不足，暴露出其在遥感复杂推理能力上的关键瓶颈。

Conclusion: VLRS-Bench为推动遥感领域多模态推理研究提供了新基准和重要评估依据，有助于引导MLLM向更高阶认知能力演进。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [26] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: 本文提出ShapBPT，一种基于数据感知的多尺度二叉分割树（BPT）结构的分层Shapley值方法，用于提升计算机视觉模型的可解释性，显著改善特征归因与图像形态的一致性及计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有分层Shapley方法未利用图像的多尺度结构，收敛慢、语义对齐差，且缺乏针对视觉数据设计的数据感知层次结构。

Method: 提出ShapBPT方法，将Shapley值分配到为图像定制的二叉分割树（BPT）上，构建数据感知的多尺度层次结构，实现形态感知的像素级归因。

Result: 实验表明ShapBPT在图像结构对齐性和计算效率上优于现有XCV方法；20人用户研究表明其解释更受人类偏好。

Conclusion: ShapBPT成功将分层Shapley理论与图像数据特性结合，为视觉可解释性提供了更高效、语义更清晰的新范式。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [27] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 本文提出了一种名为Error-enhanced Contrastive Handwriting Recognition（ECHWR）的训练框架，用于在边缘设备上实现低内存开销、高精度的惯性传感器手写识别，通过引入临时辅助分支与双对比学习目标（常规in-batch对比损失 + 新型基于识别错误的对比损失），提升特征表征能力，训练后移除辅助分支，保持推理轻量；在OnHW-Words500数据集上显著降低字符错误率，尤其对未见书写风格鲁棒。


<details>
  <summary>Details</summary>
Motivation: 在边缘硬件上实现实时、隐私友好、低延迟的纸面手写识别面临内存受限挑战，现有方法难以兼顾精度与推理效率。

Method: 提出ECHWR训练框架：引入临时辅助分支，在训练阶段将传感器信号对齐到语义文本嵌入；采用双对比损失——in-batch对比损失实现模态对齐，新型error-based对比损失利用合成难负样本区分正确信号；辅助分支仅用于训练，推理时移除。

Result: 在OnHW-Words500数据集上，writer-independent和writer-dependent划分下字符错误率分别降低7.4%和10.4%，显著优于SOTA基线；消融表明error-based对比损失对未见书写风格具有关键提升作用。

Conclusion: ECHWR在不增加推理开销前提下有效提升手写识别精度与泛化性，验证了误差驱动对比学习在传感器信号理解中的有效性，为边缘手写识别提供了新范式。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [28] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 本文通过多种可解释性方法，首次直接研究了大规模视频编码器内部的物理表征，发现存在一个‘物理涌现区’，物理变量在此区域变得可访问，并以分布式而非因子化方式表征。


<details>
  <summary>Details</summary>
Motivation: 探究视频模型是否需要依赖因子化的物理变量表征才能进行准确的物理预测，还是能以任务特定、分布式的隐式方式表征物理变量。

Method: 采用逐层探测、子空间几何分析、块级解码和定向注意力消融等可解释性技术，分析视频Transformer编码器中物理信息的可访问位置与组织方式。

Result: 发现跨架构存在一个中间深度的‘物理涌现区’，物理变量在此突然变得可访问；标量物理量（如速度、加速度）在浅层即存在，而运动方向仅在该区域出现，并以高维环形群体结构编码。

Conclusion: 现代视频模型不使用类似经典物理引擎的因子化物理表征，而是采用一种虽为分布式但足以支持物理预测的表征方式。

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [29] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: 本文提出Neural Sentinel，一种基于视觉语言模型（VLM）的统一ALPR方法，用单次前向传播完成车牌识别、州属分类与车辆属性提取，显著提升准确率并支持零样本多任务泛化。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR多阶段流水线存在误差累积、高延迟和架构复杂等问题，亟需更简洁、鲁棒且多功能的端到端方案。

Method: 基于PaliGemma 3B模型，采用LoRA微调，并引入Human-in-the-Loop持续学习框架（结合经验回放防止灾难性遗忘）；利用VLM的多轮视觉问答能力实现统一多任务推理。

Result: 车牌识别准确率达92.3%，较EasyOCR和PaddleOCR分别提升14.1%和9.9%；推理延迟152ms，ECE为0.048；零样本完成车辆颜色（89%）、安全带检测（82%）、载客计数（78%）等任务。

Conclusion: 统一VLM架构代表ALPR范式转变，兼具更高精度、更低复杂度与新兴多任务能力，超越传统流水线方法。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [30] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 本文提出了一种无需物理标记的神经导航方法，利用低成本可见光与红外立体/深度摄像头结合面部几何建模算法，实现了对头部位置的高精度实时跟踪，在50名受试者中达到2.32 mm和2.01°的中位误差，优于以往无标记方案，且适用于经颅磁刺激等临床应用。


<details>
  <summary>Details</summary>
Motivation: 传统神经导航依赖易移位、需手动配准且造成不适的体表标记物，亟需更舒适、低成本、高鲁棒性的替代方案。

Method: 采用可见光与红外双模态立体及深度摄像头，结合面部三维几何建模与多传感器数据融合算法，实现无标记头部姿态实时跟踪。

Result: 在50名受试者上验证，最优无标记算法中位定位误差为2.32 mm、角度误差为2.01°，精度满足经颅磁刺激要求，并显著优于先前无标记方法；多传感器融合可进一步提升精度。

Conclusion: 所提无标记神经导航方法可降低设备成本与操作复杂度，提升患者舒适度，并促进神经导航在临床与科研中的普及应用。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [31] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: 本文提出RECITYGEN工具，结合潜在扩散模型与交互式语义分割，支持用户通过文本提示交互生成街景图像，提升公众参与城市设计的能力。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法常忽视公众意见，导致设计目标与现实脱节；数字技术（如CIM、AR）和生成式AI（如扩散模型）为公众参与提供了新可能。

Method: 将最先进的潜在扩散模型与交互式语义分割相结合，构建RECITYGEN系统，支持用户通过文本提示交互生成多样化街景图像。

Result: 在北京城市更新试点项目中，公众使用RECITYGEN提出改进建议；结果表明该工具能较好契合公众偏好，具备推动动态、包容性城市规划的潜力。

Conclusion: RECITYGEN代表了一种融合生成式AI与公众参与的城市设计新范式，有助于弥合设计理想与现实之间的鸿沟。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [32] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: 本文提出FADE，一种用于文本到图像扩散模型的快速、轻量级机器遗忘方法，通过两阶段策略（参数定位+自蒸馏）实现高效概念擦除与保留平衡。


<details>
  <summary>Details</summary>
Motivation: 应对数据保护法规和负责任AI实践对模型遗忘特定数据或概念的需求，解决现有文本到图像扩散模型中遗忘计算开销大、遗忘-保留难以兼顾的问题。

Method: FADE采用两阶段方法：第一阶段基于梯度显著性定位关键参数，并使用稀疏LoRA适配器进行局部轻量更新；第二阶段通过自蒸馏目标，用用户定义的替代概念覆盖待遗忘概念，同时保持对保留数据的行为。适配器内存高效、可逆、支持运行时合并或移除。

Result: 在UnlearnCanvas基准及多个数据集（Imagenette、LFW、Dog Breeds、SUN Attributes）上的实验表明，FADE达到SOTA遗忘性能，具备细粒度的遗忘-保留权衡控制能力，实现强概念擦除与高保留率。

Conclusion: FADE是一种适用于扩散式图像生成模型的选择性遗忘有效方案，兼具高效性、灵活性与实用性。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [33] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 本文提出了一种基于计算机视觉的辅助系统，用于在铁路车厢卸料过程中自动评估废钢中非金属杂质含量（以百分比表示）并分类废钢类型，采用多实例学习（MIL）和多任务学习（MTL）方法，在保证近实时性的同时提升评估客观性、操作安全性及流程集成能力。


<details>
  <summary>Details</summary>
Motivation: 当前废钢杂质含量依赖人工目视检测，存在主观性强、粉尘与机械作业危害大等问题，亟需自动化、安全、可靠的评估方法。

Method: 将杂质评估建模为车厢级回归任务，利用多实例学习（MIL）处理序列图像，结合多任务学习（MTL）同步完成杂质估计与废钢分类；系统集成磁体/车厢检测、版本化推理服务、操作员结构化复核与主动学习闭环。

Result: MIL方案达到MAE 0.27、R² 0.83；MTL方案实现MAE 0.36、废钢分类F1 0.79；系统已部署于近实时验收流程，支持操作员复核与持续优化。

Conclusion: 该视觉管道显著降低主观差异、提升人员安全，并可无缝融入废钢验收与熔炼计划等工业流程，具备实际落地价值。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [34] [Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine](https://arxiv.org/abs/2602.07064)
*Minghao Han,Dingkang Yang,Yue Jiang,Yizhou Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了OmniFysics，一个轻量级的多模态模型，通过构建物理数据引擎（含FysicsAny和FysicsOmniCap）显式注入物理知识，提升模型对物理属性的理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态模型在物理理解方面表现脆弱，因关键物理属性视觉上模糊且在网络规模数据中稀疏。

Method: 提出OmniFysics模型，构建物理数据引擎：FysicsAny通过分层检索与物理定律约束生成物理标注图像对；FysicsOmniCap利用音视频一致性筛选网络视频生成高质量视频-指令对；采用分阶段多模态对齐、指令微调、潜在空间流匹配及意图路由机制。

Result: 在标准多模态基准测试中性能具竞争力，并在面向物理的评测中显著提升。

Conclusion: 显式注入物理知识可有效增强多模态模型的物理理解与生成能力，OmniFysics为构建具备物理常识的通用智能体提供了新路径。

Abstract: Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.

</details>


### [35] [Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework](https://arxiv.org/abs/2602.07065)
*A. N. Maria Antony,T. Richter,E. Gladilin*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的端到端方法，用于从图像序列中高效、准确地估计连续介质位移和材料可压缩性，克服了传统迭代算法（如FEM/FDM）在非接触式材料表征中计算慢、难于高通量处理的局限。


<details>
  <summary>Details</summary>
Motivation: 传统非接触式材料力学性质评估依赖耗时的迭代图像配准与数值建模方法（如FEM、FDM），难以满足高通量需求；亟需更快速、鲁棒的替代方案。

Method: 构建两个深度神经网络：一个用于非刚性图像配准以估计位移场，另一个直接从图像序列预测材料可压缩性；端到端联合优化，利用高阶矢量场特征（如涡度）而非仅局部位移特征。

Result: 该深度学习框架在效率和精度上均优于传统方法；即使图像配准结果存在显著局部偏差，仍能准确估计材料可压缩性；实验证明其性能源于对高阶认知特征（如位移场涡度）的学习能力。

Conclusion: 深度学习端到端模型为非接触式、高通量材料力学表征提供了新范式，突破了传统基于物理建模与迭代求解方法的瓶颈。

Abstract: Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.

</details>


### [36] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出Bird-SR，一种双向奖励引导的扩散超分辨率框架，通过奖励反馈学习（ReFL）联合利用合成与真实低分辨率图像，在结构保真与感知增强间动态权衡，显著提升真实场景超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的超分辨率方法在合成数据上训练后，因分布偏移而在真实低分辨率图像上表现不佳。

Method: 提出Bird-SR框架：1）早期扩散步在合成对上直接优化以保障结构保真；2）后期用质量引导奖励优化感知质量，对合成与真实图像分别设计抗奖励欺骗机制（相对优势空间建模+语义对齐约束）；3）采用动态保真-感知加权策略。

Result: 在多个真实世界超分辨率基准上，Bird-SR在感知质量与结构一致性上均超越当前最优方法。

Conclusion: Bird-SR通过轨迹级偏好优化与双阶段、双目标协同学习机制，有效缓解了合成到真实域的分布偏移问题，为实用化超分辨率提供了新范式。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [37] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为MosaicThinker的推理时计算技术，用于提升资源受限的具身AI设备上小型视觉语言模型（VLM）在跨帧空间推理任务中的能力。其核心是将多帧空间信息整合为统一的全局语义地图，并通过视觉提示引导VLM进行空间推理。实验表明该方法显著提升了复杂跨帧空间推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）缺乏3D空间知识，尤其在涉及多视频帧的复杂空间关系推理任务中空间推理能力薄弱，难以满足具身AI对视频输入进行空间感知与动作规划的需求。

Method: 提出MosaicThinker技术：在推理时将多个视频帧中的碎片化空间信息融合为统一的全局语义地图，并利用视觉提示引导小型VLM在该语义地图上进行空间推理。

Result: 在资源受限的具身AI设备上，MosaicThinker显著提升了多种类型和复杂度的跨帧空间推理任务的准确率。

Conclusion: MosaicThinker是一种有效的推理时增强方法，可弥补小型VLM在3D空间理解与跨帧空间推理上的不足，为端侧具身AI提供了可行的空间认知增强路径。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [38] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: 本文提出了WorldEdit数据集和两阶段训练框架，旨在解决图像编辑模型在处理隐式编辑指令（基于因果逻辑的指令）时的局限性，显著提升了模型在因果编辑任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型难以处理隐式编辑指令，因其缺乏对现实世界因果知识和推理能力的支持。

Method: 构建了WorldEdit数据集及WorldEdit-Test评测集，并采用两阶段训练框架结合因果验证奖励机制对Bagel等模型进行微调。

Result: 所提方法在因果编辑任务中显著缩小与GPT-4o和Nano-Banana的差距，在指令遵循和知识合理性方面均表现出竞争力。

Conclusion: WorldEdit为世界知识驱动的图像编辑提供了新基准和有效训练范式，推动了隐式、因果型图像编辑的发展。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [39] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: 本文提出TLC-Plan，一种基于层级VQ-VAE与CodeTree表示的端到端向量户型图生成模型，直接从边界生成语义合理、拓扑有效、几何精确的向量户型图，无需显式拓扑或尺寸先验，在RPLAN和LIFULL数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有户型图生成方法多在光栅空间操作并依赖后处理矢量化，导致结构不一致、难以端到端学习；而人类建筑师常基于模块化、可复用的空间模式进行设计，因此需支持组合式空间推理的向量化生成框架。

Method: 提出TLC-Plan：采用双层VQ-VAE分别编码全局房间布局（带语义标签的包围框）和局部几何细节（多边形级码）；引入CodeTree统一表征层级结构；利用自回归Transformer以输入边界为条件采样代码，实现多样且拓扑合法的向量户型图生成。

Result: 在RPLAN数据集上FID=1.84、MSE=2.06，LIFULL数据集上亦达领先结果；支持约束感知、可扩展的向量户型生成，适用于真实建筑应用。

Conclusion: TLC-Plan实现了首个真正端到端、向量原生、无需显式拓扑先验的户型图生成框架，显著提升几何精度、结构一致性与生成可控性，推动AI辅助建筑设计向实用化迈进。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [40] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于可重光照3D高斯泼溅和端到端强化学习的无人机单目视觉导航框架，实现从高保真仿真到真实复杂户外环境（如森林）的零样本迁移，具备强光照不变性与高速（10 m/s）无碰撞导航能力。


<details>
  <summary>Details</summary>
Motivation: 单目视觉无人机在非结构化户外环境中导航受限于仿真与现实间巨大的视觉域差距，尤其现有3D高斯泼溅方法将静态光照与几何耦合，导致策略难以泛化至动态真实光照条件。

Method: 提出端到端强化学习框架，结合基于真实数据构建的高保真仿真环境；创新引入可重光照3D高斯泼溅，解耦场景几何与光照分量，支持物理驱动的光照编辑；在训练中合成多样化光照条件（如强直射光、阴天漫射光）以增强策略鲁棒性。

Result: 轻量级四旋翼在真实森林环境中实现最高10 m/s的鲁棒、无碰撞导航，对剧烈光照变化具有显著鲁棒性，且无需微调。

Conclusion: 可重光照3D高斯泼溅与光照多样性增强训练有效弥合了仿真-现实视觉域差距，为单目视觉无人机在动态光照下的零样本户外导航提供了新范式。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [41] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: 本文提出PI3D，一种针对三维物理环境中多模态大语言模型（MLLMs）的提示注入攻击，通过在真实场景中放置带文本的物理物体来误导模型执行非预期任务，并验证了现有防御手段对此类攻击无效。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击研究集中于纯文本或数字编辑的2D图像，而真实3D物理环境中的攻击机制尚不明确，亟需系统性探索。

Method: 提出PI3D攻击框架，建模并求解带文本物理对象在3D空间中的最优位姿（位置与朝向），使其在保持物理合理性的同时最大化对MLLM的误导效果。

Result: 实验表明PI3D能有效攻击多种MLLM，在不同相机轨迹下均成功诱导模型执行注入任务；现有防御方法无法有效抵御该攻击。

Conclusion: PI3D揭示了MLLM在具身智能场景中面临的新安全威胁，凸显了面向三维物理世界的鲁棒性与安全性研究的必要性。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [42] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: 本文提出Ex-Omni框架，通过解耦语义推理与时间生成，并引入语音单元作为时序支架及TQGF机制，实现语音驱动的3D面部动画生成；同时发布InstructEx数据集，显著提升开源OLLM在语音-面部协同生成任务上的性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Omni-modal大语言模型（OLLMs）尚未有效整合语音与3D面部动画，而二者对自然人机交互至关重要；核心难点在于LLM离散token级语义表征与3D面部运动所需稠密、细粒度时序动态之间的表征不匹配。

Method: 提出Ex-Omni框架：1）解耦语义推理与时序生成；2）利用语音单元作为时序支架；3）设计统一的token-as-query门控融合（TQGF）机制以实现可控语义注入；并构建InstructEx数据集支持训练。

Result: Ex-Omni在多项实验中表现优于现有开源OLLMs，能稳定生成语音与3D面部动画高度对齐的结果。

Conclusion: Ex-Omni为OLLMs扩展语音-3D面部协同生成能力提供了有效且开源的解决方案，验证了解耦建模与结构化时序引导的有效性。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [43] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: This paper investigates the presence of sensitive pregnancy ultrasound images in the LAION-400M dataset using CLIP embeddings, uncovering thousands of instances containing private identifiers like names and locations, highlighting re-identification risks and recommending improved dataset curation and privacy practices.


<details>
  <summary>Details</summary>
Motivation: The increasing use of large-scale, minimally curated internet-sourced datasets for generative models raises concerns about inclusion of sensitive or private information, such as pregnancy ultrasound images.

Method: Systematic examination of the LAION-400M dataset using CLIP embedding similarity to retrieve pregnancy ultrasound images, followed by detection of private information (e.g., names, locations) in those images.

Result: Thousands of pregnancy ultrasound images were found containing high-risk private information—including names and locations—enabling potential re-identification or impersonation.

Conclusion: The study underscores serious privacy risks in public image datasets and calls for improved dataset curation, stronger data privacy safeguards, and ethical guidelines for using such datasets.

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [44] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: 本文提出DuMeta++，一种无需配对纵向数据的双元元学习框架，用于解决MRI脑组织分割中跨年龄泛化难题，通过元特征学习、元初始化学习和记忆库驱动的类感知正则化提升模型鲁棒性与适应性。


<details>
  <summary>Details</summary>
Motivation: MRI脑组织分割在全生命周期中性能不稳定，因脑部外观与形态随年龄动态变化；而现有依赖配对纵向数据的自监督方法在实际中往往不可行。

Method: 提出DuMeta++双元元学习框架，包含元特征学习（提取年龄无关语义表征）、元初始化学习（支持数据高效适配）及记忆库驱动的类感知正则化（保障纵向一致性），并提供收敛性理论证明。

Result: 在iSeg-2019、IBIS、OASIS、ADNI等多个数据集的少样本设置下，DuMeta++在跨年龄泛化任务上显著优于现有方法。

Conclusion: DuMeta++有效缓解了MRI脑分割中因年龄变化导致的分布偏移问题，无需配对纵向数据即可实现稳定、可推广的模型适应，为临床与神经科学研究提供了实用新范式。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [45] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: 本文提出了RoadSafe365，一个大规模视觉-语言交通安全性基准，旨在弥补现有交通基准缺乏与官方安全标准对齐的系统性评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有交通基准缺乏与官方安全标准对齐的系统性评估，限制了数据驱动交通理解系统的发展。

Method: 构建了一个基于分层分类法的独立策划、系统组织的大规模视觉-语言基准RoadSafe365，涵盖细粒度交通事件类型、环境上下文和交互场景，并提供丰富的属性标注和多选问答对。

Result: RoadSafe365包含36,196个标注视频片段、864K候选选项、8.4K唯一答案和36K详细场景描述；在该基准上微调模型可获得稳定提升，并在真实与合成数据集的跨域实验中验证其有效性。

Conclusion: RoadSafe365为现实世界交通安全性分析提供了全面、标准化且可复现的研究基准，推动了交通安全领域数据驱动研究的发展。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [46] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: 本文提出AdvSR框架，通过在超分辨率（SR）模型训练过程中嵌入对抗行为，实现模型级攻击，无需输入访问即可导致下游任务（如目标检测）错误分类，且保持图像质量指标正常。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的超分辨率方法常作为成像流水线的预处理步骤，但其模型本身可能构成新的攻击面，现有攻击多依赖输入扰动或后门触发，缺乏对模型权重本身的直接对抗设计。

Method: 提出AdvSR框架，在SR模型训练中联合优化重建质量与特定下游任务（如YOLOv11分类）的对抗目标，将对抗行为内嵌于模型权重中，攻击完全发生在模型层面。

Result: 在SRCNN、EDSR、SwinIR三种SR模型与YOLOv11检测器组合上验证，AdvSR可实现高攻击成功率，同时图像质量（如PSNR、SSIM）下降极小。

Conclusion: 揭示了成像流水线中一种新型模型级威胁，警示安全关键场景中模型来源与验证需考虑此类隐蔽对抗风险。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [47] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 本文介绍了3D-TBM，一个用于3D医学图像形态学分析的工具，基于传输域嵌入实现图像分类、回归等任务，并支持结果回映到原始图像空间以进行临床解释。


<details>
  <summary>Details</summary>
Motivation: 推动Transport-Based Morphometry（TBM）在临床影像研究中的广泛应用，解决现有方法缺乏易用性、可解释性和完整工具链的问题。

Method: 构建3D-TBM框架，包含数据预处理、最优传输嵌入计算、主传输方向可视化、判别方向识别等分析方法，并提供文档与教程；代码开源至PyTransKit。

Result: 实现了支持端到端3D医学图像形态分析的开源工具3D-TBM，具备可解释性（通过逆映射回原空间）、可视化能力及实用教程。

Conclusion: 3D-TBM为医学影像研究者提供了易用、可解释、功能完整的TBM分析平台，有望促进传输理论在临床应用中的落地。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [48] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: 本文提出TwistNet-2D模块，通过方向性位移与逐通道乘法建模局部成对通道交互，以显式捕获纹理中特征的空间共现模式，在极小计算开销下显著提升纹理与细粒度识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在纹理识别中难以兼顾全局通道相关性与空间结构：双线性池化和Gram矩阵丢失空间信息，自注意力则不显式建模成对特征交互。

Method: 提出TwistNet-2D，核心为Spiral-Twisted Channel Interaction（STCI）：沿预设方向平移特征图后进行逐通道乘法；聚合四个方向头，辅以通道重加权与sigmoid门控残差路径。

Result: 在ResNet-18上仅增加3.5%参数量和2% FLOPs，却在四个纹理与细粒度识别基准上持续超越参数匹配及更大规模的基线模型（如ConvNeXt、Swin Transformer等）。

Conclusion: 显式建模带方向性的局部通道交互是高效提升纹理识别性能的关键，TwistNet-2D为轻量级结构化特征建模提供了新范式。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [49] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: 本文提出了跨视角世界模型（XVWM），通过跨视角预测目标（即从一个视角输入帧序列，预测同一或不同视角下的未来状态）来学习环境的3D结构不变表征，从而提升智能体在多任务规划中的灵活性与空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常仅基于单一（如自我中心）视角建模，限制了复杂任务（如导航）的规划能力；而多视角信息可提供更丰富的几何与空间约束，有助于学习更鲁棒、空间对齐的表征。

Method: 提出Cross-View World Models（XVWM），以同步多视角游戏数据（Aimlabs平台）为训练基础，设计跨视角预测目标——给定某视角的历史帧与动作，预测另一视角（相同或不同）下的未来状态；利用跨视角一致性作为几何正则化手段，迫使模型学习视角无关的3D结构表征。

Result: XVWM成功支持智能体在多个视角下并行‘想象’未来状态，实现按需切换参考系进行规划（执行仍基于自我中心视角）；实验证明跨视角一致性是学习空间接地表征的强监督信号，并初步展现出支持多智能体视角采择（perspective-taking）的潜力。

Conclusion: 跨视角建模不仅提升了世界模型的空间推理能力与泛化性，也为构建具备几何意识和他人视角理解能力的智能体提供了新范式。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [50] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种结合注意力机制的DeepLab-V3+模型（Attention-DeepLab），用于糖尿病视网膜病变（DR）四种病灶（微动脉瘤、软渗出、硬渗出、出血）的像素级分割，在DDR数据集上提升了mAP、IoU及关键早期病灶微动脉瘤的检出率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的DR自动筛查算法在病灶分割（尤其是临床关键的微动脉瘤）方面临床适用性有限，需支持眼科医生进行精准、可解释的像素级病灶标注。

Method: 在DeepLab-V3+基础上引入注意力机制，构建Attention-DeepLab模型，对DDR数据集757张眼底图像中的四类DR病灶进行像素级语义分割。

Result: 相比基线模型，mAP从0.3010提升至0.3326，平均IoU从0.1791提升至0.1928；微动脉瘤检测率从0.0205显著提升至0.0763。

Conclusion: 所提Attention-DeepLab模型有效提升了DR病灶（尤其是早期微动脉瘤）的分割性能，增强了自动化筛查的临床实用性与可解释性。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [51] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 本文提出了一种基于线性遗传编程（LGP）的图像滤波与分割算法，用于自动识别FIB截面电镜图像中的析出相，显著提升铌铜合金微结构分析效率与迭代速度。


<details>
  <summary>Details</summary>
Motivation: 当前增材制造铌铜合金的微观组织分析依赖人工标注，受限于图像对比度变化、噪声和伪影，严重拖慢合金研发迭代速度。

Method: 构建面向图像处理的领域特定语言（DSL），结合线性遗传编程优化滤波-分割流程；每个程序由最多5个可调参图像处理模块顺序组成，经遗传算法演化生成可解释的MATLAB代码。

Result: 在种群规模60、最大程序长度5的设定下，算法像素级分割误差仅1.8%（XOR评估），单张3.6兆像素图像处理耗时约2秒。

Conclusion: 该自动化方法大幅加速了材料成分与工艺参数空间探索，助力开发适用于聚变堆部件的高强度、低活化、析出强化铜合金。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [52] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 本文提出LUCID，一种统一的视觉-语言稀疏自编码器，通过共享潜在字典和最优传输匹配目标，实现跨模态特征对齐与可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器按模态单独训练，导致特征不可理解、解释无法跨域迁移。

Method: 提出LUCID模型，构建图像块与文本token共享的稀疏潜在字典，并引入无监督最优传输匹配目标实现特征对齐；设计基于术语聚类的自动字典解释流程。

Result: 获得可解释的共享特征，支持图像块级定位、跨模态神经元对应，并缓解概念聚类评估中的问题；揭示共享特征涵盖对象、动作、属性及抽象概念等多元语义类别。

Conclusion: LUCID为多模态表征提供了统一、可解释、可迁移的稀疏编码框架，推动了跨模态概念发现与自动化解释的发展。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [53] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

TL;DR: 本文提出CLARITY方法，利用视觉-语言模型先验动态调整RGB与热成像模态融合策略，以应对恶劣光照条件下的道路场景语义分割挑战，并在MFNet数据集上达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 恶劣光照、照明和阴影条件下道路场景的鲁棒语义分割仍是自动驾驶的核心挑战；现有RGB-热成像融合方法采用静态融合策略，易导致模态特有噪声传播。

Method: 提出CLARITY框架：1）基于视觉-语言模型（VLM）先验动态调节各模态在不同光照状态下的贡献；2）保留被以往去噪方法错误丢弃的暗区物体语义；3）引入层次化解码器增强多尺度结构一致性，提升细长物体边界清晰度。

Result: 在MFNet数据集上达到62.3% mIoU和77.5% mAcc，刷新SOTA。

Conclusion: 动态感知驱动的模态融合策略结合VLM引导与结构感知解码，显著提升了复杂光照下语义分割的鲁棒性与精度。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [54] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

TL;DR: 本文提出自适应匹配蒸馏（AMD）方法，通过奖励代理显式检测并逃离‘禁区’，提升生成模型的训练稳定性和样本保真度。


<details>
  <summary>Details</summary>
Motivation: 分布匹配蒸馏（DMD）在‘禁区’（真实教师指导不可靠、虚假教师排斥力不足的区域）中稳定性差，亟需一种能主动识别并规避该区域的优化机制。

Method: 提出统一优化框架，将已有方法视为隐式避区策略；设计AMD：利用奖励代理显式检测禁区，通过结构信号分解动态加权校正梯度，并引入排斥景观锐化（Repulsive Landscape Sharpening）构建陡峭能量壁垒防止失败模式坍缩。

Result: 在SDXL、Wan2.1等图像/视频生成任务及VBench、GenEval等基准上显著提升性能；SDXL的HPSv2分数从30.64提升至31.25，超越现有最优方法。

Conclusion: 显式校正‘禁区’内的优化轨迹对突破少步生成模型性能上限至关重要。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [55] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进U-Net与行-列分离注意力模块（RCSA）的低光照图像/视频增强方法，通过利用特征图行列的均值和最大值引入全局信息指导局部增强，同时设计了两个时序损失函数以提升视频增强的时序一致性。


<details>
  <summary>Details</summary>
Motivation: U-Net在低光照增强中易导致局部噪声大、细节丢失，且缺乏对全局信息的有效利用；传统注意力机制虽能增强全局建模能力，但参数与计算开销大。

Method: 在改进U-Net后插入行-列分离注意力模块（RCSA），其输入为特征图的行列均值与最大值；针对视频任务，设计两个时序损失函数以保证帧间一致性。

Result: 在LOL、MIT Adobe FiveK图像数据集及SDSD视频数据集上实验表明，该方法在图像与视频低光照增强任务中均取得优越性能，且参数量与计算量可控。

Conclusion: RCSA模块以轻量方式有效融合全局与局部信息，结合时序损失显著提升了低光照图像/视频增强的质量与稳定性，代码已开源。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [56] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 本文提出了一种面向透视投影的对数深度融合方法，用于从深度图和表面法向图中重建高精度3D表面，并能利用法向信息填补缺失深度区域。


<details>
  <summary>Details</summary>
Motivation: 现有正交梯度法在处理单视角相机获取的深度与法向图时忽略透视投影，导致重建失真；同时缺失深度数据影响重建完整性。

Method: 提出透视感知的对数深度融合方法，将透视投影建模融入梯度约束，并利用表面法向信息进行深度空洞修复。

Result: 在DiLiGenT-MV数据集上验证了方法有效性，重建结果具有更高度量精度，且能鲁棒处理深度缺失区域。

Conclusion: 显式建模透视投影对深度-法向融合至关重要，所提方法在几何精度和鲁棒性上优于传统正交假设方法。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [57] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: 本文提出PTB-XL-Image-17K，一个包含17,271张合成12导联心电图（ECG）图像的大规模数据集，旨在解决ECG图像数字化缺乏高质量、带完整标注的基准数据问题。


<details>
  <summary>Details</summary>
Motivation: ECG图像数字化对利用历史临床数据至关重要，但受限于缺乏大规模、带完整真值信号与多模态标注的公开数据集。

Method: 基于PTB-XL信号数据库，构建高保真合成ECG图像数据集PTB-XL-Image-17K；提供五类互补标注（图像、像素级分割掩码、真值时序信号、YOLO格式检测框、元数据），并开源可定制的Python生成框架。

Result: 成功生成17,271张图像，生成成功率100%，平均耗时1.35秒/样本；支持完整ECG数字化流程（导联检测、波形分割、信号提取）的端到端评估。

Conclusion: PTB-XL-Image-17K填补了ECG图像数字化研究中大规模、多标注基准数据的空白，为深度学习模型训练与严谨评估提供了首个综合性开源资源。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [58] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了SoulX-FlashHead，一个1.3B参数的实时、无限长度、高保真音频驱动肖像生成框架，通过流式感知时空预训练与Oracle引导双向蒸馏等技术，在保持低延迟的同时提升视觉质量与时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型计算成本高，轻量模型又难以兼顾整体面部表征与时间稳定性，亟需兼顾高保真与低延迟的实时生成方案。

Method: 提出Streaming-Aware Spatiotemporal Pre-training（含Temporal Audio Context Cache）以稳定短音频特征提取；设计Oracle-Guided Bidirectional Distillation利用真实运动先验抑制长序列自回归中的误差累积与身份漂移；构建782小时高精度对齐数据集VividHead。

Result: 在HDTF和VFHQ基准上达到SOTA；Lite版本在单张RTX 4090上达96 FPS推理速度，兼具高速与视觉一致性。

Conclusion: SoulX-FlashHead实现了高质量、高稳定性、低延迟的实时音频驱动肖像流式生成，为交互式虚拟人应用提供了高效可行的技术路径。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [59] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: 本文提出SpatialReward，一种通过显式空间推理来提升图像编辑评估准确性的奖励模型，解决了现有评估器因‘注意力坍缩’导致的细粒度感知不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线强化学习图像编辑方法受限于可靠且细粒度奖励信号的稀缺，评估器普遍存在‘注意力坍缩’问题，即忽略跨图像比较、无法捕捉细节，导致评估不准。

Method: 提出SpatialReward奖励模型，通过将推理锚定在预测编辑区域，实现基于像素级证据的语义判断；在26万空间感知数据集上训练。

Result: 在MMRB2和EditReward-Bench上达到SOTA；在自建MultiEditReward-Bench上超越私有评估器；作为在线RL信号，使OmniGen2在GEdit-Bench上提升+0.90，优于领先判别模型并达GPT-4.1增益的两倍。

Conclusion: 空间推理对图像编辑中有效对齐至关重要，SpatialReward为在线RL图像编辑提供了更鲁棒、更精准的奖励信号。

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [60] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 本文介绍了GlobalWasteData (GWD) 数据集，一个大规模、统一、高质量的废物分类数据集，旨在解决现有公开数据集碎片化、不一致和偏差问题，以支持更鲁棒和泛化能力强的AI废物识别模型。


<details>
  <summary>Details</summary>
Motivation: 现有公开废物分类数据集存在碎片化、标注不一致、环境偏差、类别分布不平衡等问题，限制了AI模型在真实场景中的泛化能力。

Method: 通过整合多个公开数据集，构建统一的GlobalWasteData (GWD) 档案，包含89,807张图像、14个主类和68个子类，并进行质量过滤、去重和元数据生成等预处理。

Result: GWD数据集实现了标签一致性、领域多样性增强和更均衡的类别分布，显著提升了数据集的可靠性与实用性。

Conclusion: GWD为环境监测、自动回收和废物识别等机器学习应用提供了坚实基础，并已公开发布以促进后续研究与可复现性。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [61] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: 本文提出了TOM-GS，一种面向热成像相机的高鲁棒性、端到端学习型SLAM系统，结合学习式里程计与高斯泼溅（GS）稠密建图，在恶劣环境（如烟雾、黑暗）下实现精准运动估计与高质量新视角渲染。


<details>
  <summary>Details</summary>
Motivation: 现有热红外里程计与建图方法多为几何方法，泛化性差、难以跨数据集工作，且无法生成稠密地图；而高斯泼溅（GS）在重建质量与效率上表现优异，故作者希望将其引入热红外SLAM。

Method: 提出TOM-GS：融合学习式单目热红外里程计与GS稠密建图框架，包含专用热图像增强模块和单目深度融合机制，是首个面向热相机的GS-SLAM系统。

Result: 在运动估计与新视角渲染任务上，TOM-GS显著优于现有学习型热SLAM方法，验证了学习范式在热红外场景下的鲁棒性与重建优势。

Conclusion: TOM-GS证明了将学习驱动的里程计与GS建图结合可有效提升热红外SLAM的精度、鲁棒性与稠密重建能力，为恶劣环境机器人感知提供了新范式。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [62] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 本文提出了一种利用多预训练视觉编码器与对比学习对齐脑信号与图像表征的新方法，并引入Fusion Prior提升跨模态分布一致性，在视觉解码任务中兼顾检索准确率与重建保真度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉解码方法多关注高层语义特征而忽略像素级细节，导致对人类视觉系统的理解受限；脑信号究竟在多大程度上编码视觉信息尚不明确。

Method: 采用多个具有不同归纳偏置的预训练视觉编码器提取层次化、多尺度视觉表征，并通过对比学习实现脑信号与视觉嵌入的有效对齐；同时引入Fusion Prior，在大规模视觉数据上预训练稳定映射，再将脑特征匹配至该先验以增强跨模态分布一致性。

Result: 大量定量与定性实验表明，该方法在检索准确率和重建保真度之间取得了良好平衡。

Conclusion: 所提方法更全面地捕捉了脑信号中的视觉信息，尤其在兼顾语义与像素级细节方面优于现有方法，有助于深化对人类视觉系统机制的理解。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [63] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新的隐式运动表示方法（IM-Animation），通过将每帧运动压缩为紧凑的1D运动token，并设计基于掩码token的时间一致性重定向模块，以解决显式与隐式运动建模方法在角色动画中的空间错配、身份泄露和运动-外观耦合等问题。


<details>
  <summary>Details</summary>
Motivation: 现有显式方法（如骨架、DWPose）难以处理空间错配和身体比例变化；隐式方法虽能直接从驱动视频学习高层运动语义，但易导致身份泄露和运动-外观纠缠。

Method: 提出1D运动token隐式运动表示以缓解2D表示的空间约束并防止身份信息泄露；设计基于时间一致掩码token的重定向模块，施加时间训练瓶颈；采用三阶段训练策略提升效率与保真度。

Result: 在多个实验中，IM-Animation在生成质量上达到或超越当前最优方法。

Conclusion: 所提隐式运动表示与重定向机制有效提升了视频扩散模型在角色动画任务中的鲁棒性、一致性与身份保持能力。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [64] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: 本文提出了一种自适应缩放（ZoomDet）框架，通过非均匀缩放和边界框对齐变换，提升无人机图像中小目标检测性能，具有架构无关性、高效性和显著精度增益。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中前景目标尺寸小、分布稀疏，导致常规目标检测器难以有效优化。

Method: 提出轻量级偏移预测方案与基于边界框的缩放目标函数，实现高效非均匀缩放；设计角点对齐的边界框变换方法，在缩放空间中训练检测器，并将预测结果映射回原图空间。

Result: 在SeaDronesSee数据集上，结合Faster R-CNN提升mAP达8.4个绝对点，仅增加约3ms延迟；在VisDrone、UAVDT等数据集上也验证了有效性；支持任意检测架构。

Conclusion: ZoomDet是一种简单、高效、通用的无人机图像小目标检测增强方法，兼顾精度与效率，具备实际部署价值。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [65] [CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization](https://arxiv.org/abs/2602.07523)
*Zhen Zhang,Qing Zhao,Xiuhe Li,Cheng Wang,Guoqiang Zhu,Yu Zhang,Yining Huo,Hongyi Yu,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于CA-YOLO的仿生稳定定位系统，通过引入小目标检测头和特征融合注意力机制（CFAM）提升目标定位精度与小目标识别能力，并结合受前庭眼反射启发的云台跟踪控制策略，显著提升了COCO和VisDrone数据集上的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有系统在目标定位精度和小目标识别能力方面存在局限，难以满足现代复杂环境下的需求。

Method: 在YOLO主干网络中集成仿生模块：包括小目标检测头和特征融合注意力机制（CFAM）；并设计基于人类前庭眼反射（VOR）原理的仿生云台跟踪控制策略，含中心定位、稳定性优化、自适应控制系数调整与智能重捕获功能。

Result: CA-YOLO在COCO和VisDrone数据集上平均精度分别提升3.94%和4.90%；时敏目标定位实验验证了系统的有效性与实用性。

Conclusion: 该仿生稳定定位系统有效提升了目标定位精度与小目标识别能力，具有良好的实际应用潜力。

Abstract: In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.

</details>


### [66] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 本文提出了一种新的评估框架，用于更全面地衡量对象中心学习（OCL）模型在复杂推理和定位任务中的表现，利用指令调优的视觉语言模型（VLMs）作为评估器，并引入统一的‘where & what’评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有OCL评估方法局限于简单任务（如图像分类）和分离的定位/表示评估，无法真实反映其结构化表征能力与OOD鲁棒性。

Method: 使用指令调优的VLM作为可扩展评估器，在多个VQA数据集上测试OCL表征对复杂推理的支持能力；设计统一的联合评估任务与指标，同时衡量定位精度（where）与表征有用性（what）；引入多特征重建基线作为参考。

Result: 所提框架能更有效地揭示OCL模型在复杂推理与精确定位上的实际能力，克服了传统评估中指标割裂与任务简单的问题。

Conclusion: OCL模型需通过更贴近其设计目标（如组合泛化、OOD鲁棒性）的任务进行评估；统一的‘where & what’评估范式及VLM驱动的评测方式为未来OCL研究提供了更可靠的标准。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [67] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 本文提出了一种基于Global Context Vision Transformer (GCViT-Tiny)的深度学习方法，用于猫品种细粒度图像分类，在Oxford-IIIT Pet数据集子集上达到92%测试准确率，验证了Transformer架构在该任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于猫品种间毛色图案、面部结构和颜色差异细微，从图像中准确识别猫品种具有挑战性。

Method: 采用GCViT-Tiny视觉Transformer架构，并结合旋转、水平翻转和亮度调整等丰富数据增强策略。

Result: GCViT-Tiny模型在测试集上准确率达92.00%，验证集达94.54%；并提供了Hugging Face在线演示。

Conclusion: Transformer类架构在猫品种等细粒度图像分类任务中具有优异性能和良好泛化能力，具备兽医诊断、动物收容所管理及移动端识别等应用潜力。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [68] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 本文提出了一种双时间点（入院时CTP和随访时DWI）的生物统计与深度学习融合分析框架，通过多模态特征提取与区域划分，揭示了缺血性卒中组织表型及状态转变的影像学规律。


<details>
  <summary>Details</summary>
Motivation: 单时间点分割无法捕捉卒中组织的生物学异质性和时间演化过程，需更精细刻画缺血核心与半暗带的动态变化。

Method: 构建双时间点（T1：入院CTP；T2：随访DWI）配准与ROI划分框架（6类区域），提取GLCM纹理特征、统计描述符及mJ-Net/nnU-Net深度嵌入特征，并在特征空间中进行聚类与统计分析。

Result: 在18例成功再通患者中，特征空间呈现有意义的区域聚类；半暗带区域最终转归显著影响其特征分布（p<0.05），而核心区域无此差异；mJ-Net嵌入的半暗带分离指数显著非零。

Conclusion: 编码器导出的特征流形可反映卒中组织真实表型与状态跃迁，为基于影像的卒中演化量化提供新范式。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [69] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出LGDEA方法，利用大语言模型从放射学报告中提取关键诊断证据，构建诊断证据共享空间，实现证据级跨模态对齐，减少对配对数据的依赖，并在多项下游任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式医学视觉-语言预训练方法依赖全局或局部对齐，但全局对齐易受非诊断信息干扰，局部对齐难以整合关键诊断证据，导致在配对数据有限的医疗场景中难以学习可靠诊断表征。

Method: 提出LLM-Guided Diagnostic Evidence Alignment（LGDEA）方法，利用大语言模型从放射学报告中提取关键诊断证据，构建共享诊断证据空间，实现证据级跨模态对齐，并有效利用大量未配对的医学图像与报告。

Result: 在短语定位、图文检索和零样本分类等任务上均取得一致且显著的性能提升，甚至可媲美依赖大量配对数据的预训练方法。

Conclusion: LGDEA通过证据级对齐更契合医学诊断过程，显著缓解了对配对数据的依赖，提升了模型在低资源医疗场景下的适用性与诊断表征能力。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [70] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 本文提出MUFASA，一种轻量级即插即用框架，通过在ViT编码器的多个特征层上执行slot attention并融合多层slots，提升无监督目标中心学习（OCL）的目标分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有slot attention方法仅使用ViT最后一层特征，忽略了其他层蕴含的丰富语义信息。

Method: MUFASA在ViT编码器的多个特征层上计算slot attention，并设计融合策略将多层slots聚合为统一的对象中心表示。

Result: 在多个数据集上显著提升无监督目标分割性能，达到新SOTA，同时加快训练收敛，仅引入轻微推理开销。

Conclusion: 充分利用ViT多层语义特征可有效增强slot attention-based OCL方法的表征能力与分割效果。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [71] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的Few-Shot语义分割方法FSSDINO，基于冻结的DINOv3特征，通过类特定原型与Gram矩阵优化实现高性能；研究发现最后一层特征虽表现稳健，但并非最优，揭示了基础模型中“语义选择鸿沟”这一关键问题。


<details>
  <summary>Details</summary>
Motivation: 探索冻结DINOv3特征在few-shot语义分割中的内在能力，检验无需训练的极简方法能否媲美复杂适配方法，并诊断基础模型中特征层选择的有效性。

Method: 提出FSSDINO：利用冻结DINOv3的特征，构建类特定原型，并引入Gram矩阵细化机制；进行Oracle引导的逐层分析，系统评估各中间层特征性能，并对比多种无监督和支持集引导的选择策略。

Result: FSSDINO在二值、多类及跨域（CDFSS）基准上均达到与专用方法相当的性能；Oracle分析显示存在显著性能提升空间（最优中间层远超最后一层），但现有选择指标无法稳定定位该最优层。

Conclusion: DINOv3最后一层是极具欺骗性的强基线；当前特征选择策略存在‘语义选择鸿沟’，暴露了基础模型中高保真语义特征难以被启发式方法可靠识别的根本局限；本工作为ViT特征语义潜力提供了严谨诊断框架。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [72] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: 本文提出FlexID框架，通过意图感知调制实现无需训练的个性化文本到图像生成，在保持身份一致性的同时提升文本遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法依赖刚性视觉特征注入，导致身份保真度与文本适应性之间存在冲突。

Method: 提出FlexID框架，包含语义身份投影器（SIP）和视觉特征锚点（VFA），并引入上下文感知自适应门控（CAG）机制动态调节二者权重。

Result: 在IBench数据集上实验表明，FlexID在身份一致性和文本遵循性之间达到最优平衡，支持复杂叙事生成。

Conclusion: FlexID为无需训练的个性化文本到图像生成提供了高效且灵活的新范式。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [73] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 本文提出了一种3B参数的视觉-语言-动作（VLA）智能体，通过显式的图像接地推理实现人类般的具身推理，提升目标识别与动作选择的可解释性、泛化性和导航效率，避免了多模型拼接管道的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在泛化能力差、可解释性低、误差传播、计算成本高等问题，难以兼顾语言驱动导航中的对象识别与动作决策。

Method: 提出紧凑型3B参数VLA智能体，采用三阶段显式图像接地推理（'思考'、'思考总结'、'动作'），直接回答'这是目标物体吗？'和'为何执行此动作？'。

Result: 提升了导航任务中的可解释性、泛化能力和效率，避免了端到端模型或模块化零样本流水线的固有缺陷。

Conclusion: 显式、分阶段的图像接地推理是实现高效、可解释、强泛化语言驱动导航的有效路径，VLA智能体为具身AI提供了新范式。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [74] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: 本文提出了SIGMA框架，通过引入选择性多属性标记（风格、内容、主体、身份等），在扩散Transformer中实现多条件交错生成，显著提升了可控性、跨条件一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型如Bagel仅支持单条件输入，缺乏从多种异构来源合成结果的灵活性。

Method: 提出SIGMA（Selective-Interleaved Generation with Multi-Attribute Tokens）统一后训练框架，在Bagel骨干网络上使用70万交错样本进行后训练，引入风格、内容、主体和身份等选择性多属性标记，支持文本-图像交错序列中的多条件解释与组合。

Result: SIGMA在组合编辑、选择性属性迁移和细粒度多模态对齐任务上表现优异，相比Bagel在组合任务上有显著提升，增强了可控性、跨条件一致性与视觉质量。

Conclusion: SIGMA为扩散Transformer提供了灵活、可控的多条件生成能力，拓展了统一视觉模型的应用边界。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [75] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 本文介绍了国际HID竞赛的发展，特别是自2023年起采用高挑战性的SUSTech-Competition数据集，并在2025年验证算法是否突破此前精度瓶颈；最终最佳方法达94.2%准确率，刷新基准，并总结了技术趋势与未来方向。


<details>
  <summary>Details</summary>
Motivation: 推动步态识别研究进展，提供公平、具挑战性的评估平台，尤其应对真实场景中服装、携带物、视角等变化带来的跨域泛化难题。

Method: 依托年度HID竞赛机制，使用无专用训练数据的SUSTech-Competition数据集，每年以不同随机种子划分测试集，鼓励基于外部数据训练及跨域泛化能力评估；2025年聚焦检验算法能否突破既有精度上限。

Result: 2025年最佳方法在SUSTech-Competition数据集上达到94.2%识别准确率，刷新此前记录；同时观察到若干关键技术演进趋势。

Conclusion: 步态识别算法持续进步，即便在高度挑战性的跨域设定下仍可提升性能；竞赛框架有效促进鲁棒、泛化性强的方法发展，未来需进一步探索解耦表征、多源迁移与轻量化部署等方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [76] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于解耦表征学习的跨摄像头奶牛个体识别框架，利用子空间可识别性保证（SIG）理论，将图像分解为多个正交潜在子空间，分离出跨摄像头稳定的个体生物特征，在五个不同摄像头节点构成的数据集上平均识别准确率达86.0%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有奶牛识别方法在单摄像头下表现良好，但在跨摄像头部署时因光照、背景、视角和成像特性差异导致性能急剧下降，难以适用于真实动态牧场环境。

Method: 提出基于子空间可识别性保证（SIG）理论的解耦表征学习框架，设计原理驱动的特征解耦模块，将图像分解为多个正交潜在子空间，以分离出跨摄像头不变的身份相关生物特征。

Result: 在涵盖五个异构摄像头节点的高质量数据集上，七项跨摄像头任务平均识别准确率达86.0%，显著高于源域仅训练基线（51.9%）和最强跨摄像头基线（79.8%）。

Conclusion: 本研究建立了面向协同跨摄像头奶牛识别的子空间理论驱动特征解耦框架，为非受控智能养殖环境下的精准动物监测提供了新范式。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [77] [Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding](https://arxiv.org/abs/2602.07568)
*Hui Ye,Shilong Yang,Yexuan Xing,Juan Yu,Yaoqin Xie,Wei Zhang,Chulong Zhang*

Main category: cs.CV

TL;DR: 本文提出MammoColor框架，通过任务驱动的彩色编码（TDCE）模块将单通道乳腺X光片转换为增强视觉效果的彩色图像，以提升致密乳腺中的筛查敏感性；在多个数据集和多读者研究中验证了其可提高AUC和特异性，减少假阳性召回。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光筛查在致密乳腺中敏感性较低，因组织重叠和细微征象导致感知困难。

Method: 提出端到端MammoColor框架，包含轻量级任务驱动彩色编码（TDCE）模块，与BI-RADS分类器联合训练，基于VinDr-Mammo数据集，并在多个内部、公开及外部临床队列上评估；开展含洗脱期的多读者多病例（MRMC）观察研究，对比灰度图、TDCE图及二者并列显示的效果。

Result: 在VinDr-Mammo上AUC从0.7669提升至0.8461（P=0.004），致密乳腺中提升更显著（0.749→0.835）；MRMC研究中TDCE图像使特异性从0.90升至0.96（P=0.052），敏感性保持相当。

Conclusion: TDCE提供任务优化的彩色表征，可增强感知显著性，降低乳腺筛查分诊中的假阳性召回率。

Abstract: Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.

</details>


### [78] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文提出ViCA（Vision-only Cross-Attention）架构，通过仅在少数层使用视觉-文本交叉注意力、跳过视觉token的自注意力与FFN计算，大幅降低MLLM视觉侧计算开销（降至4%），同时保持98%原始精度，并实现显著推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在每层对视觉和文本token统一使用自注意力，导致视觉计算开销过大；作者发现视觉嵌入已较好对齐语言空间，且有效跨模态交互仅发生在少数层。

Method: 提出ViCA架构：视觉token完全绕过所有自注意力和前馈网络层，仅在选定的少量Transformer层中通过cross-attention与文本交互。

Result: 在3个主干、9个基准、26种剪枝基线上验证：ViCA保持98% baseline精度，视觉计算降至4%，单批推理加速3.5倍以上，多批加速超10倍；推理流程规整、硬件友好；可与token剪枝正交结合。

Conclusion: 密集视觉处理非必要，ViCA以极简设计实现了高性能与高效率的统一，为高效MLLM提供了新范式。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [79] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

TL;DR: 本文提出一种地质驱动的机器学习方法，通过合成数据生成与监督分割结合，实现图像中岩体节理迹线的自动识别与映射。


<details>
  <summary>Details</summary>
Motivation: 解决真实岩体图像数据稀缺、标注不一致及类别不平衡问题，提升节理迹线自动识别的可靠性与地质合理性。

Method: 基于离散裂隙网络（DFN）建模生成符合野外尺度的合成节理图像；采用混合训练（合成+真实数据）与迁移学习（预训练后微调）策略训练语义分割模型；在箱域和坡域多个真实数据集上验证。

Result: 合成数据可有效弥补真实数据不足；混合训练在标签一致场景（如箱域）表现好，微调在标签噪声大场景（如坡域）更鲁棒；仅用少量真实样本微调即可实现良好泛化；定性分析显示结果更具地质意义。

Conclusion: 该地质驱动框架提升了节理迹线识别的可靠性与可解释性，为后续域自适应与评估研究奠定基础。

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [80] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种系统化的视频生成模型后训练框架，整合监督策略塑造、奖励驱动强化学习和基于偏好的优化，在稳定性约束下提升生成质量、时序连贯性和提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 解决预训练视频生成模型在实际部署中面临的指令遵循性、可控性与时序鲁棒性不足的问题，同时应对高采样成本、时序累积错误及弱/异构反馈等现实约束。

Method: 构建一个稳定性约束的多阶段优化栈，依次整合监督策略塑造、奖励驱动的强化学习和偏好驱动的细化优化，并以诊断驱动的方式进行 staged 优化。

Result: 显著提升了视频生成的感知保真度、时序一致性与提示遵循能力，同时保持初始可控性，并为可扩展、稳定且实用的后训练流程提供了清晰蓝图。

Conclusion: 将后训练视为一个统一、诊断驱动的稳定性优化过程，而非孤立技巧的堆砌，是构建面向生产环境的高质量视频生成模型的关键路径。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [81] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出Fine-R1，一种专为细粒度视觉识别（FGVR）定制的多模态大语言模型，通过链式思维监督微调与三元组增强策略优化，在仅4样本训练下超越现有MLLM和CLIP模型，显著提升对已见/未见子类别的判别与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态大语言模型（MLLMs）在细粒度视觉识别（FGVR）任务上表现不佳，依赖大量标注数据、易过拟合已见子类别、难以泛化至未见子类别，而专用对比学习模型（如CLIP）虽强于判别任务，却缺乏推理与开放世界分类能力。

Method: 提出R1风格训练框架：（1）链式思维监督微调（CoT-SFT），构建含‘视觉分析-候选子类-对比-预测’四步推理的高质量FGVR CoT数据集；（2）三元组增强策略优化（TAPO），包含类内增强（混合同类锚图与正样本轨迹以提升类内鲁棒性）与类间增强（最大化不同子类别图像条件响应差异以增强判别力）。

Result: Fine-R1仅需4-shot训练即在已见与未见子类别识别上全面超越现有通用MLLM、推理型MLLM及对比式CLIP模型，展现出优异的少样本泛化与知识密集型任务适应能力。

Conclusion: Fine-R1验证了将结构化推理能力与判别式优化机制融入MLLM可有效弥合其与专用视觉模型在FGVR上的性能鸿沟，为少样本、开放世界细粒度识别提供了新范式。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [82] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 本文提出HistoMet框架，通过决策感知与概念对齐的多实例学习方法，从全切片图像预测原发肿瘤的转移风险及转移部位，显著提升临床预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学方法将转移状态和部位预测视为独立任务，未建模临床中先评估转移风险、再判断具体部位的顺序决策过程。

Method: 提出两模块决策流程：首先估计原发肿瘤转移可能性，再对高风险病例条件预测转移部位；引入基于预训练病理视觉-语言模型的语义与数据自适应转移概念，指导表征学习。

Result: 在6504例多中心泛癌队列上，95%高敏感筛查下显著降低下游工作量并保持高风险召回；条件预测转移部位达macro F1 74.6（±1.3）和macro one-vs-rest AUC 92.1。

Conclusion: 显式建模临床决策结构可实现鲁棒、可部署的原发肿瘤组织学驱动的转移进展与部位趋向性预测。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [83] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出AD-MIR框架，通过结构化记忆构建与结构化推理代理两阶段方法，将广告视频的像素级视觉信息与高层营销意图关联，显著提升广告理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体虽擅长通用搜索，但在广告视频理解中难以弥合像素感知与高层营销逻辑之间的认知鸿沟。

Method: AD-MIR采用双阶段架构：第一阶段为结构感知记忆构建，融合语义检索与关键词匹配，提取品牌细节并过滤背景噪声；第二阶段为结构化推理代理，通过迭代提问分解叙事、推断隐含说服策略，并基于视频帧证据进行自我修正。

Result: 在AdsQA基准上，AD-MIR严格准确率和宽松准确率分别超越最强通用智能体DVD达1.8%和9.5%，达到SOTA。

Conclusion: 有效广告理解必须将抽象营销策略显式锚定于像素级视觉证据，AD-MIR为此提供了可行框架。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [84] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 本文提出UMD数据集，包含490例全身PET/CT和464例全身PET/MRI扫描，用于评估3D医学基础模型在多模态（尤其是功能成像）下的泛化能力；结果表明现有模型在功能影像上表现显著下降，揭示其距真正通用尚有差距，呼吁转向多模态训练与评估。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学基础模型的验证主要集中于局部或结构成像（如CT、MRI），忽视了功能成像（如PET）等关键模态，导致对模型真实临床泛化能力的评估存在严重偏差。

Method: 构建UMD数据集（含PET/CT与PET/MRI全身影像及器官标注），通过受试者内配对扫描控制变量，以成像模态为唯一独立变量，系统评估主流3D分割基础模型在跨模态（结构→功能）任务中的鲁棒性。

Result: 实验发现模型在PET相关任务中性能大幅下降，文献基准与真实世界效能之间存在显著鸿沟，暴露当前模型缺乏对功能影像的理解能力，无法实现真正的模态无关泛化。

Conclusion: 当前3D医学基础模型远未达到通用水平；必须转向多模态联合训练与评估范式，UMD数据集为此提供了首个系统性评测基准与基础支撑。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [85] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

TL;DR: 本文提出Images2Slides系统，利用视觉语言模型将静态信息图（PNG/JPG）自动转换为可编辑的Google幻灯片，支持区域级内容提取、坐标映射与元素重建，实验显示高元素恢复率和良好文本识别精度，但也存在布局保真度与工程实现挑战。


<details>
  <summary>Details</summary>
Motivation: 信息图导出为图像后内容不可编辑，导致更新、本地化和复用成本高，亟需将其转化为可编辑的原生格式。

Method: 构建API驱动的pipeline：首先用视觉语言模型（VLM）提取信息图的区域级结构（文本、图标、图表等），再将像素坐标映射到幻灯片坐标系，最后调用Google Slides批量更新API重建各元素；系统模型无关，通过统一JSON区域模式和确定性后处理支持多VLM后端。

Result: 在29张程序生成的基准信息图上，整体元素恢复率达0.989±0.057（文本0.985±0.083，图像1.000±0.000），文本CER为0.033±0.149，文本区域IoU为0.364±0.161，图像区域IoU为0.644±0.131。

Conclusion: Images2Slides有效实现了从静态图像到可编辑幻灯片的自动化转换，验证了VLM在结构化文档理解中的实用性，但布局保真度仍有提升空间，文本尺寸校准与复杂背景处理是关键工程挑战。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [86] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 本文系统评估了医学扫描3D重建流程中各环节（成像、分割、配准、网格处理）的误差来源，比较了基于体素和表面的多种精度度量方法在不同几何结构（球体、面罩、腹主动脉瘤AAA）和分割算法（GMM、Otsu、RG）下的表现，发现Otsu法整体最优，Jaccard指数更适用于薄壁结构评估，且配准质量对结果可靠性至关重要。


<details>
  <summary>Details</summary>
Motivation: 3D医学模型重建精度受硬件、分割与网格处理等多环节影响，但几何类型、类别不平衡、体素/点云配准对精度的影响尚未被充分研究。

Method: 采用SLA打印标准模型（球体、面罩、AAA），经micro-CT扫描后，分别用GMM、Otsu和RG方法进行分割；使用KU算法进行体素配准，ICP算法进行表面配准；对比Dice、Jaccard、精度、Chamfer距离、平均Hausdorff距离等指标。

Result: Otsu法在所有几何体上表现最佳；AAA因壁薄和配准偏差导致重叠分数低，且类别不平衡显著降低其特异性；RG在球体上最优，GMM/Otsu在AAA上更优；面罩表面误差最大，可能源于ICP配准误差；体素与表面指标趋势不一致；Jaccard比Dice更严格，更适合薄壁结构评估。

Conclusion: 分割精度是重建全流程误差的累积体现；高体素精度指标在类别不平衡或配准敏感时可能具有误导性；可靠评估必须确保体素与点云层面的精确配准；Jaccard指数更适合作为薄壁结构精度评估指标。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [87] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文提出L-LIO框架，扩展传统LILO框架，引入音频模态以增强对驾驶员、乘客及车外环境的理解，通过多模态融合提升车辆安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LILO框架仅依赖视觉信息，难以应对视觉信号不足或需语义理解的复杂安全场景；音频可提供关键补充信息，尤其在驾驶员状态识别、人车交互和外部意图理解中具有独特价值。

Method: 提出L-LIO（Looking-and-Listening Inside-and-Outside）框架，融合车内/车外音频与视觉信号；开展三项实证研究：1）基于驾驶员语音的损伤状态分类；2）乘客自然语言指令采集与分析以支持规划系统；3）分析音频如何辅助解析外部人员手势与引导。使用真实道路环境自建音频数据集。

Result: 初步实验表明音频能提供视觉无法获取的安全相关线索，尤其在语境丰富或视觉模糊场景中效果显著；但也面临环境噪声、隐私保护和跨个体鲁棒性等挑战。

Conclusion: L-LIO通过音视融合增强了驾驶员状态评估与环境理解能力，为智能车辆安全干预提供了新路径，但需进一步提升其在动态现实环境中的可靠性。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [88] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLMs）在自动驾驶安全评估与决策中的应用，提出了三种系统级用例：基于CLIP的轻量级语义危险筛查、任务对齐的视觉语言嵌入用于轨迹规划、以及利用自然语言作为运动规划的行为约束；结果表明，VLMs在表达语义风险、意图和行为约束方面具有潜力，但需系统化工程设计而非简单特征注入。


<details>
  <summary>Details</summary>
Motivation: 探索视觉-语言表征如何支持自动驾驶中安全关键场景的语义理解、风险评估与决策制定，弥补传统感知-预测-规划范式在语义泛化与意图建模上的不足。

Method: 1) 基于CLIP图像-文本相似度的无类别危险筛查；2) 将场景级视觉语言嵌入集成至Transformer轨迹规划器（Waymo数据集），并提出任务引导的特征提取方法；3) 在doScenes数据集上，将乘客式自然语言指令作为视觉接地的行为约束引入运动规划。

Result: 1) CLIP方案实现低延迟、泛化性强的语义危险检测；2) 全局嵌入直接注入规划器无效，凸显表示-任务对齐必要性；3) 自然语言约束显著抑制罕见严重规划失败，提升模糊场景下的安全性。

Conclusion: 视觉-语言表征对自动驾驶安全具有重要价值，但其落地本质是工程问题，依赖结构化接地与系统级设计，而非简单端到端特征融合。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [89] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为'思维过程（PoT）推理'的视频理解框架，通过将视频推理分解为可验证的轻量级步骤（时间证据选择、逐步状态更新、约束答案合成），提升事实正确性、时间定位能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视频理解不仅需要识别视觉内容，还需在长且嘈杂的时序观测中进行时间定位的多步推理，现有方法缺乏可追溯性和鲁棒性。

Method: 提出Process-of-Thought (PoT) Reasoning框架，包含三阶段交错流程：(i) 时间证据选择，(ii) 逐 步状态更新，(iii) 约束答案合成；设计统一的PoT trace表示，对齐中间决策与视频时间片段；支持模型无关、闭卷及外部工具增强推理。

Result: 在标准视频推理任务上，PoT显著提升了事实正确性和时间定位精度，并生成可解释、可诊断的推理轨迹，增强了抗干扰和抗幻觉能力。

Conclusion: PoT是一种通用、可插拔、可解释的视频推理框架，有效解决了长视频多步推理中的可追溯性、鲁棒性与准确性难题。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [90] [Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes](https://arxiv.org/abs/2602.07694)
*Wenping Jin,Yuyang Tang,Li Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种面向传送带煤流场景的无监督异物异常检测与像素级定位方法，构建了首个专用基准CoalAD，并设计了多视角互补线索协同感知框架，在图像级和像素级指标上均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 煤流场景中异物检测面临环境无序、背景复杂多变、异物低对比度、形变遮挡等挑战，导致传统基于结构化工业假设的异常检测方法性能显著下降，亟需专用数据集与新方法。

Method: 构建了CoalAD基准；提出互补线索协同感知框架，从物体级语义组成建模、语义归因的全局偏差分析、细粒度纹理匹配三个视角提取并融合异常证据，实现鲁棒图像级打分与精确像素级定位。

Result: 在CoalAD上，所提方法在图像级和像素级各项评估指标上均超越主流基线；消融实验验证了各模块的有效性；代码已开源。

Conclusion: 该工作为煤流场景异物检测提供了首个专用基准与有效解决方案，提升了复杂无序工业场景下异常检测的可靠性与实用性。

Abstract: Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.

</details>


### [91] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: 本文提出U-KABS，一种融合Kolmogorov-Arnold网络（KAN）与U型编解码结构的新型医学图像分割框架，结合卷积+SE模块和基于Bernstein多项式与B样条的可学习激活函数（KABS），提升对复杂非线性关系与多尺度结构的建模能力，在多个基准数据集上取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割因图像复杂性和变异性大、难以建模非线性关系而具有挑战性，亟需兼具全局平滑性与局部适应性的强表达模型。

Method: 提出U-KABS混合框架：1）U形编码器-解码器结构；2）引入卷积+挤压激励（SE）模块增强通道特征；3）设计KAN Bernstein Spline（KABS）阶段，采用基于Bernstein多项式（保障全局光滑）与B样条（支持局部自适应）的可学习激活函数；4）保留跳跃连接以实现多尺度特征融合与空间细节保持。

Result: 在多个医学影像基准数据集上，U-KABS显著优于强基线方法，尤其在复杂解剖结构分割任务中表现突出。

Conclusion: U-KABS通过协同卷积归纳偏置与KAN的强非线性拟合能力，有效平衡全局上下文建模与局部细节刻画，为医学图像分割提供了高效、鲁棒的新范式。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [92] [All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving](https://arxiv.org/abs/2602.07717)
*Yingjie Li,Daniel Robinson,Cunxi Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于衍射光学神经网络（DONN）的全光计算框架，用于自动驾驶中的语义分割与车道线检测，显著降低能耗并避免模数转换开销。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络在自动驾驶中面临高能耗问题，尤其是大量模数转换和图像计算带来的能量消耗；而DONN通过全光衍射处理图像，具备高速、低功耗优势，适合实时任务。

Method: 设计并实现一种全光计算框架，利用DONN进行RGB图像语义分割和车道检测；在CityScapes数据集上验证分割性能，并在自建室内赛道数据集及CARLA仿真环境中开展车道检测案例研究。

Result: 实验表明该DONN系统在CityScapes上能有效完成图像分割；在多种环境条件下（包括室内与仿真驾驶场景）也展现出良好的车道检测能力与泛化性。

Conclusion: DONN为自动驾驶感知任务提供了一种高效、低功耗的全光替代方案，有望推动边缘端光学智能计算的发展。

Abstract: Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.

</details>


### [93] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

TL;DR: 本文提出PAND框架，通过提示感知语义校准和邻域感知结构蒸馏，提升轻量级网络在细粒度视觉分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 在细粒度视觉分类（FGVC）中，将大视觉语言模型（VLMs）的知识蒸馏到轻量网络面临固定提示和全局对齐的挑战。

Method: 提出两阶段框架PAND：第一阶段为提示感知语义校准，生成自适应语义锚点；第二阶段为邻域感知结构蒸馏，约束学生模型的局部决策结构。

Result: 在四个FGVC基准上持续超越现有最优方法；ResNet-18学生模型在CUB-200上达到76.09%准确率，比VL2Lite基线高3.4%。

Conclusion: PAND有效解耦语义校准与结构迁移，提升了知识蒸馏在FGVC任务中的效果。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [94] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 本文提出Rolling Sink方法，通过训练-free的方式解决自回归视频扩散模型在测试长时序视频时出现的训练-测试差距问题，显著提升了超长视频（5-30分钟）生成的质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在训练时受限于短时序（如5秒），导致测试更长视频时出现视觉退化；现有方法如Self Forcing仅缓解训练时长内的gap，本文聚焦于训练时长之外（即开放时长测试）的gap，并寻求无需再训练的解决方案。

Method: 基于对自回归缓存维护机制的系统性分析，提出Rolling Sink：一种训练-free的推理时缓存滚动更新策略，适配原仅训练于5秒片段的Self Forcing模型，实现超长视频合成。

Result: Rolling Sink在16 FPS下成功生成5–30分钟连贯视频，保持主体一致性、色彩稳定性、结构连贯性和运动平滑性；实验表明其在长时序视觉保真度和时间一致性上优于SOTA基线。

Conclusion: Rolling Sink验证了训练-free缓存管理策略可有效弥合训练与开放测试时长间的根本性gap，为高效、高质量长视频生成提供了新范式。

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [95] [Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps](https://arxiv.org/abs/2602.07938)
*Rabbia Asghar,Lukas Rummelhard,Wenqian Liu,Anne Spalanzani,Christian Laugier*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态占用栅格图（DOGM）的统一预测框架，同时预测未来占用状态、车辆分布和场景流，通过定制化的互依赖损失函数提升多智能体行为预测与场景演化建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于占据栅格图的预测方法分为两类：代理无关（无法捕捉动态主体行为复杂性）和代理特定（难以泛化至感知不佳或未识别的代理），二者各有局限；需融合二者以实现鲁棒安全的运动预测。

Method: 提出一个统一框架，采用轻量级时空骨干网络，在简化的时间解码流水线中联合预测未来占用状态栅格、车辆栅格和场景流栅格；设计了能刻画多栅格间依赖关系并支持多样性预测的互依赖损失函数，利用占用状态信息约束流引导的时序演化，并考虑障碍物与遮挡。

Result: 在nuScenes和Woven Planet真实数据集上，该方法在动态车辆及通用动态场景元素的预测性能均优于基线方法。

Conclusion: 所提框架能同时建模车辆特异性行为与通用场景动态演化，兼顾精度、鲁棒性与泛化性，为自动驾驶中的多智能体预测提供了新思路。

Abstract: Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.

</details>


### [96] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: 本文提出了UCATSC，一种基于模型的交通信号控制系统，通过在信念空间中进行反事实推演来预测和强制执行安全与防饥饿的硬约束，从而提升交通效率与环保性能，同时保证安全性与策略可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中自适应交通信号控制部署受限于视觉感知的不确定性、隐式安全性及难以解释的控制策略（主要在仿真中学习和验证）。

Method: UCATSC将交叉口信号控制建模为带约束、部分可观测的随机决策过程，利用显式模型在信念空间中进行含安全与防饥饿硬约束的反事实推演，而非依赖奖励塑形的强化学习方法。

Result: 系统在提升交通延时与排放性能的同时，能防止安全关键错误，并输出可解释的控制策略。

Conclusion: UCATSC通过引入显式建模与硬约束机制，在保障安全性与可解释性前提下，提升了自适应交通信号控制在真实场景中的适用性。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [97] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出了ForecastOcc，首个直接从图像预测未来语义占据状态的端到端框架，避免了误差累积，并在Occ3D-nuScenes和SemanticKITTI上建立了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的占据预测方法缺乏语义信息，而引入语义的方法又依赖外部占据预测，导致误差累积且无法直接从图像学习时空特征。

Method: 提出ForecastOcc框架，包含时序跨注意力预测模块、2D到3D视图变换器、3D占据编码器和语义占据预测头，实现多时间步长的端到端语义占据预测。

Result: 在Occ3D-nuScenes（多视角）和SemanticKITTI（单目）上显著优于基线方法，生成兼具语义丰富性和动态感知能力的未来占据预测。

Conclusion: ForecastOcc首次实现了真正端到端的视觉语义占据预测，为自动驾驶中融合几何与语义的未来场景理解提供了新范式。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [98] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: 本文提出VideoTemp-o3，一种统一的视频代理推理框架，通过联合建模视频定位与问答，提升长视频理解性能，解决现有方法定位弱、效率低、流程僵化等问题。


<details>
  <summary>Details</summary>
Motivation: 传统均匀采帧在长视频理解中难以捕获关键视觉证据，导致性能下降和幻觉增多；现有代理式‘定位-裁剪-回答’范式存在定位能力弱、效率低、流程僵化等问题。

Method: 提出VideoTemp-o3框架，采用统一掩码机制进行监督微调以平衡探索与噪声抑制；引入专用奖励函数优化强化学习过程以防止奖励作弊；构建高质量长视频定位问答数据集及对应评测基准。

Result: 在长视频理解和视频定位任务上均取得显著性能提升，验证了其强定位能力、按需裁剪支持及错误定位修正能力。

Conclusion: VideoTemp-o3是一种高效、灵活且鲁棒的长视频代理推理框架，为视频理解提供了新范式，并推动了高质量标注数据与系统化评测的发展。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [99] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: 本文提出Picasso，一种物理约束的多物体场景重建方法，通过结合几何、非穿透性和物理规律，提升重建结果的物理合理性和人类直觉一致性，并开源了包含10个真实接触丰富场景的数据集和评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于几何的场景重建方法在遮挡和噪声下虽能拟合传感器数据，但易产生物理上不合理的结果（如物体穿透、不稳定平衡），影响数字孪生中动态行为预测的可靠性。

Method: 提出Picasso物理约束重建框架：1）构建多物体联合重建流程，整合几何匹配、非穿透约束与物理合理性；2）采用基于推断接触图引导的快速拒绝采样法，实现对多物体交互的全局推理；3）构建Picasso数据集及物理合理性量化指标。

Result: 在自建Picasso数据集和YCB-V数据集上的实验表明，Picasso显著优于现有方法，重建结果兼具更高物理合理性与更强的人类直觉一致性。

Conclusion: 多物体场景重建需超越单物体孤立估计，转向融合物理约束与交互关系的整体式推理；Picasso验证了该范式的有效性，并为后续研究提供了新基准与工具。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [100] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 本文对16种最先进的零样本AI图像检测方法进行了首次全面评估，涵盖12个数据集、291种生成器和260万张图像，揭示了检测器性能的显著不稳定性、巨大差距及对训练数据对齐的高度敏感性，并指出当前主流商业生成器（如Flux Dev、Midjourney v7）已严重削弱多数检测器效果。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估微调模型，忽视了实践中更常见的开箱即用（zero-shot）部署场景，亟需系统评估真实场景下的检测器泛化能力。

Method: 对16种SOTA检测方法（共23个预训练变体）在12个多样化数据集（含291种生成器、260万图像）上开展零样本评估，结合Spearman相关性、Friedman检验与Kendall W等统计分析，识别跨数据集泛化失败模式。

Result: 发现：(1) 无通用最优检测器，排名极不稳定（Spearman ρ: 0.01–0.87）；(2) 最佳与最差检测器准确率相差37个百分点（75.0% vs 37.5%）；(3) 训练数据对齐影响巨大（同架构检测器性能波动达20–60%）；(4) 主流商业生成器使多数检测器准确率降至18–30%；(5) 发现三类系统性跨数据集失效模式；统计证实检测器间性能差异极显著（p<10⁻¹⁶）。

Conclusion: ‘通用检测器’范式不成立，实践者必须依据具体威胁场景谨慎选型，不能依赖公开基准指标；研究为零样本检测器部署提供了可操作指南。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [101] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 本文提出了一种3D车辆条件化行人姿态预测框架，通过引入车辆信息增强行人运动预测，在Waymo-3DSkelMo数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 准确预测行人在复杂城市环境中的运动对自动驾驶的安全性和可靠性至关重要，而现有方法往往忽略周围车辆对行人行为的影响。

Method: 构建了车辆增强的Waymo-3DSkelMo数据集；提出一种按行人与车辆数量分类的场景采样策略；改进TBIFormer架构，加入车辆编码器和行人-车辆交互交叉注意力模块。

Result: 实验表明该方法在预测精度上有显著提升，并验证了不同行人-车辆交互建模方式的有效性。

Conclusion: 车辆感知的3D姿态预测对提升自动驾驶系统中行人运动预测性能具有重要意义。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [102] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 本文首次构建了大规模跨范式基准，系统评估了34种模型（22个专用架构+12个通用视觉语言模型）在8个标准数据集上的面部年龄估计性能，发现零样本VLM显著优于大多数专用模型，挑战了任务专用架构的必要性。


<details>
  <summary>Details</summary>
Motivation: 缺乏对现代视觉语言模型（VLMs）与专用年龄估计模型进行系统性比较的基准，限制了该领域方法选择与发展方向的认知。

Method: 构建首个大规模跨范式基准，评估34个模型（22个专用模型+12个通用VLMs）在8个标准面部年龄数据集（共1100测试图像/模型）上的MAE、18岁年龄验证错误率及粗粒度分桶性能，并开展多维度分层分析（如14个年龄段、极值年龄等）。

Result: 零样本VLM平均MAE为5.65年，显著优于非LLM模型的9.88年；最佳VLM（Gemini 3 Flash Preview，MAE=4.32）比最佳非LLM模型（MiVOLO，MAE=5.10）提升15%；VLM在18岁验证中误判率为13–25%，远低于非LLM模型的60–100%；所有模型在<5岁和>65岁表现最差；粗粒度分桶使MAE恶化至13年以上。

Conclusion: 任务专用架构并非年龄估计所必需；VLM展现出强大零样本泛化能力；未来应聚焦于将VLM能力蒸馏为高效专用模型。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [103] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了WorldArena统一基准，用于系统评估具身世界模型在感知和功能两个维度上的性能，并揭示了高视觉质量与强具身任务能力之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前具身世界模型的评估过于关注感知保真度（如视频生成质量），忽视了其在下游决策任务中的功能实用性，缺乏统一、全面的评估框架。

Method: 构建WorldArena统一基准，从视频感知质量（16个指标、6个子维度）和具身任务功能性（作为数据引擎、策略评估器、动作规划器，并结合主观人类评价）三方面进行评估；提出综合指标EWMScore，将多维性能整合为单一可解释指数。

Result: 在14个代表性模型上的实验表明，视觉质量高的模型未必具备强具身任务能力，即存在显著的‘感知-功能’鸿沟；WorldArena已开源并设立公共排行榜。

Conclusion: 评估具身世界模型需兼顾感知与功能，WorldArena为推动真正具备功能性的具身AI世界模型发展提供了标准化评测框架。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [104] [Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction](https://arxiv.org/abs/2602.07820)
*Zhibo Chen,Yu Guan,Yajuan Huang,Chaoqi Chen,XiangJi,Qiuyun Fan,Dong Liang,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种算子引导的重建框架OCDI-Net，用于解决同时多层（SMS）MRI成像中的强耦合逆问题，通过显式解耦目标层内容与层间干扰，并分两阶段完成切片分离与平面内补全，显著提升重建保真度并减少层间泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的SMS重建方法多假设高斯噪声，且需额外一致性步骤来融入SMS物理模型，易与实际算子主导的退化不匹配。

Method: 提出算子引导框架，建模已知采集算子定义的退化轨迹，并通过确定性更新进行逆过程；设计算子条件双流交互网络（OCDI-Net），显式分离目标切片内容与层间干扰，并预测结构化退化以支持算子对齐的重建；采用两阶段级联推理：先SMS切片分离，再平面内k空间补全。

Result: 在fastMRI脑部数据和前瞻性采集的体内弥散MRI数据上，相比传统及学习型SMS重建方法，所提方法在重建保真度和层间泄漏抑制方面均有提升。

Conclusion: 算子引导的OCDI-Net框架能更准确地建模SMS物理退化，其双阶段重建策略有效解耦干扰并提升重建质量，为加速MRI提供了新范式。

Abstract: Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.

</details>


### [105] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 本文提出了OTA-Det，首个统一开放词汇空中检测（OVAD）与遥感视觉定位（RSVG）的框架，通过任务重定义和密集语义对齐策略，实现细粒度语义理解与多目标检测，并基于RT-DETR实现实时高效推理。


<details>
  <summary>Details</summary>
Motivation: OVAD仅支持粗粒度类别语义，RSVG仅限单目标定位，二者孤立使用无法兼顾丰富语义理解与多目标检测需求。

Method: 提出OTA-Det统一框架：1）任务重定义策略，统一目标与监督机制，支持跨范式联合训练；2）密集语义对齐策略，建立从整体描述到个体属性的多粒度显式对应；3）基于RT-DETR扩展高效模块，实现开放文本检测。

Result: 在六个OVAD与RSVG基准上达到SOTA性能，同时保持34 FPS实时推理速度。

Conclusion: OTA-Det成功融合OVAD与RSVG两大范式，突破各自固有局限，为航空场景理解提供了兼具语义丰富性、定位精度与运行效率的统一解决方案。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [106] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 本文提出SPD-Faith Bench基准，用于诊断多模态大语言模型（MLLMs）在链式推理中的推理忠实性问题，发现两类系统性失败模式，并据此提出无需训练的SAGE框架以提升视觉证据对齐与推理忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注感知幻觉，而忽略了推理层面的不忠实问题；且需剥离语言先验影响，专门评估基于视觉差异的细粒度推理忠实性。

Method: 构建基于图像差异细粒度推理的诊断基准SPD-Faith Bench，分析MLLMs在视觉注意力衰减和残差流表征偏移上的根源，并提出无需训练的SAGE框架，通过视觉证据校准改进视觉路由与感知-推理对齐。

Result: 在SPD-Faith Bench上验证了当前SOTA MLLMs存在感知失明和感知-推理解耦两类系统性失败；SAGE显著提升了推理忠实性，且不依赖微调。

Conclusion: 推理忠实性应被显式评估，不能仅依赖响应正确性；视觉证据驱动的校准机制可有效缓解MLLMs的推理不忠实问题。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [107] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace是一种无需训练、即插即用的视频人脸交换方法，结合频谱注意力插值、目标结构引导和光流引导注意力时序平滑机制，提升生成质量与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散模型的图像级人脸交换方法在视频应用中存在的时间不一致性和身份特征丢失问题。

Method: 提出三种关键技术：1）频率谱注意力插值以保持身份特征；2）即插即用注意力注入实现目标结构引导；3）光流引导注意力时序平滑机制增强时空一致性，且无需修改底层扩散模型或额外训练。

Result: 显著提升视频人脸交换的时间一致性与视觉保真度，在多个指标上优于现有方法，并具备良好的模块化与实用性。

Conclusion: VFace是一种高效、灵活、无需训练的视频人脸交换方案，可无缝集成到现有图像级扩散模型人脸交换流程中。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [108] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出ViewRope，一种几何感知的编码方法，通过将相机射线方向直接注入视频Transformer自注意力层，解决预测世界模型中因屏幕空间位置嵌入导致的几何漂移问题，提升长轨迹下的空间持久性和3D一致性。


<details>
  <summary>Details</summary>
Motivation: 现有预测世界模型缺乏空间持久性，在长轨迹中无法维持稳定场景结构，尤其在相机重访已观测位置时频繁幻觉细节，其根本原因在于屏幕空间位置嵌入与3D投影几何不兼容，引发几何漂移。

Method: 提出ViewRope：将相机射线方向注入视频Transformer自注意力，以相对射线几何替代像素局部性建模；提出几何感知帧稀疏注意力机制，利用几何线索选择性关注相关历史帧；构建ViewBench评测套件评估闭环保真度与几何漂移。

Result: ViewRope显著提升了长时序预测中的3D一致性和场景稳定性，降低了几何漂移，在保持记忆一致性的同时减少了计算开销。

Conclusion: 几何感知的注意力设计（如ViewRope）是提升预测世界模型空间持久性的关键路径，为构建具备真实3D一致性的交互式AI世界模型提供了新范式。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [109] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: 本文提出了SSI-Bench，一个面向受限流形上空间推理的视觉问答基准，强调几何、拓扑与物理约束下的3D空间理解；现有VLMs在此基准上表现远逊于人类，暴露其在结构建模与约束一致的3D推理上的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在空间智能方面存在不足，多数基准测试场景过于宽松，允许模型利用2D捷径而非真正理解3D空间约束；因此需要一个能严格评估模型在几何、拓扑和物理约束下空间推理能力的新基准。

Method: 构建了SSI-Bench——一个基于真实复杂3D结构的VQA基准，包含1000个排序类问题，覆盖几何与拓扑推理，需多种组合式空间操作（如心理旋转、截面推断、遮挡推理、力路径推理）；采用全人工主导流程：10名研究者耗时超400小时完成图像采集、结构标注与问题设计，以最小化像素级线索。

Result: 在31个主流VLM上评测显示，最优开源模型准确率仅22.2%，最强闭源模型为33.6%，而人类达91.6%；提示模型‘思考’仅带来微弱提升；错误分析表明模型普遍缺乏结构接地能力和约束一致的3D推理能力。

Conclusion: SSI-Bench揭示了当前VLM在真实世界空间智能上的严重局限，凸显发展具备几何与物理约束感知能力的下一代空间推理模型的必要性。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [110] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: 本文提出了WristMIR，一种无需图像级人工标注、基于区域感知的儿童腕部X光片检索框架，利用结构化放射报告挖掘与骨特异性定位，通过两阶段（全局粗匹配+区域条件重排序）检索提升骨折模式匹配性能和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 腕部骨折影像检索困难，因关键临床线索细微、局部化且易被解剖重叠或成像角度干扰；同时缺乏大规模、高质量标注数据集制约进展。

Method: 提出WristMIR框架：1）用MedGemma挖掘密集放射报告生成全局与区域级文本描述；2）结合预处理腕部图像及桡骨远端、尺骨远端、尺骨茎突等骨特异性裁剪区域；3）联合训练全局与局部对比编码器；4）采用两阶段检索——先全局匹配筛选候选检查，再按解剖区域条件重排序。

Result: 图像到文本Recall@5从0.82%提升至9.35%；骨折分类AUROC达0.949、AUPRC达0.953；区域感知评估下平均F1从0.568升至0.753；放射科医生对检索结果临床相关性评分由3.36升至4.35。

Conclusion: 解剖引导的区域感知检索能显著增强儿科肌肉骨骼影像中的诊断推理与临床决策支持能力。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [111] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE是一种利用互联网视频流自适应扩展几何基础模型的新框架，通过分层挖掘与混合监督（稀疏几何锚定+密集可微一致性）提升3D重建的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建中潜力巨大，但受限于大规模、多样化3D标注数据的稀缺；互联网视频虽丰富，却缺乏真值几何信息且含观测噪声，难以直接用于几何学习。

Method: 提出SAGE框架：1）分层挖掘流水线将视频转化为训练轨迹；2）基于SfM点云的稀疏几何锚定提供全局结构引导；3）基于3D高斯渲染的密集可微一致性施加多视角约束；并引入锚数据正则化防止灾难性遗忘。

Result: 在7Scenes、TUM-RGBD、Matterport3D等未见基准上，Chamfer距离降低20–42%，显著优于现有方法；首次实现仅用互联网视频对几何基础模型进行可扩展适配。

Conclusion: SAGE为通用3D学习建立了一种可扩展的新范式，突破了依赖人工标注的瓶颈，推动几何基础模型走向实际应用。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [112] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向视觉语言模型（VLMs）的后训练量化（PTQ）新框架TLQ，通过梯度引导的token级重要性建模和多GPU层量化感知校准，提升了量化稳定性与部署效率。


<details>
  <summary>Details</summary>
Motivation: VLM中视觉与文本token激活分布和量化敏感性差异大，导致传统PTQ校准效果差。

Method: 提出Token-level Importance-aware Layer-wise Quantization（TLQ）：1）基于梯度设计token级重要性整合机制，构建token级校准集；2）引入多GPU、量化暴露的层间校准方案，使校准路径与真实量化推理一致，并分摊计算负载。

Result: 在两个VLM、三种模型规模、两种量化设置下均实现性能提升，展现出强量化稳定性；支持在RTX3090上完成校准，降低对A100大显存依赖。

Conclusion: TLQ为VLM的高效、稳定PTQ提供了新范式，兼顾精度、硬件兼容性与可扩展性。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [113] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 本文评估了开源视觉语言模型（VLMs）在隐私相关属性识别中的零样本性能，发现VLMs比人类标注者更倾向于预测隐私属性存在，但在高一致性情况下可补充人类标注、发现被忽略的属性。


<details>
  <summary>Details</summary>
Motivation: 评估开源VLMs在隐私相关视觉属性识别中的零样本能力，探索其在大规模图像数据隐私标注中的潜在辅助作用。

Method: 对开源VLMs进行零样本隐私属性识别评估，分析其与人类标注的一致性与分歧，并考察VLMs间高一致性情形下的标注补充能力。

Result: VLMs比人类标注者更频繁地预测隐私属性存在；但在高VLM间一致性情形下，能识别出人类忽略的隐私属性。

Conclusion: VLMs虽存在过度预测倾向，但在高一致性场景下可有效辅助人类完成大规模图像隐私属性标注任务。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [114] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于少样本学习的跨场景人群计数方法，通过结合局部与全局密度特征，使模型能快速适应未见过的监控场景，在多个数据集上优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有 crowd counting 模型在不同监控场景间泛化能力差，难以适应未见过的场景。

Method: 将不同监控场景视为不同类别，引入少样本学习；设计多局部密度学习器生成多个密度原型，并构建局部密度相似度矩阵进行局部引导；同时提取支持图像的全局密度特征进行全局引导。

Result: 在三个监控数据集上验证了方法有效性，显著优于近期少样本人群计数的最先进方法。

Conclusion: 融合局部与全局密度信息的少样本学习框架可有效提升模型对新监控场景的自适应能力与计数精度。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [115] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: 本文提出了D-ORCA，一种面向对话的多模态大语言模型，专为鲁棒的音视频字幕生成而优化，并构建了大规模双语对话视频数据集DVD，结合创新的组相对策略优化与三类奖励函数，在说话人识别、语音识别和时序定位等任务上显著优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 视频中的口语对话是关键信息源，准确识别‘谁在何时说了什么’对深度视频理解至关重要；当前开源生态中缺乏高质量、大规模的多说话人对话视频数据集与专用多模态模型。

Method: 提出D-ORCA模型，基于多模态大语言模型架构；构建DVD双语（英/中文）对话视频数据集（4万训练+2千评测样本）；采用组相对策略优化（GRPO），设计三个新奖励函数：说话人归属准确性、全局语音内容准确性、句子级时间边界对齐性，首次将语音处理常用评估指标作为RL目标用于音视频字幕任务。

Result: D-ORCA在说话人识别、语音识别和时序定位任务上大幅超越现有开源模型；尽管仅80亿参数，其在多个通用音视频理解基准上性能媲美Qwen3-Omni；代码、数据与模型检查点将全部开源。

Conclusion: D-ORCA验证了对话中心化建模与细粒度强化学习奖励设计的有效性，填补了开源音视频对话理解领域的关键空白，为多模态对话理解提供了新范式与实用资源。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [116] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: 本文提出EasyTune方法，通过在每一步去噪过程中独立微调扩散模型，解决现有基于可微奖励对齐运动生成模型时存在的优化粗粒度、内存开销大等问题；同时引入自精炼偏好学习（SPL）机制缓解偏好运动数据稀缺问题，显著提升对齐性能并大幅降低内存与训练时间开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微奖励对齐运动生成扩散模型的方法存在优化效率低、粒度粗、内存消耗高，且偏好运动数据稀缺限制奖励模型训练。

Method: 提出EasyTune：在每个去噪步骤上独立微调扩散模型，打破去噪轨迹中的递归依赖；并设计自精炼偏好学习（SPL）机制，动态构建偏好对并进行偏好建模。

Result: EasyTune在MM-Dist对齐指标上较DRaFT-50提升8.2%，额外内存开销仅为其31.16%，训练速度提升7.3倍。

Conclusion: 解耦去噪步骤的优化与引入动态偏好学习是提升运动生成模型对齐效率与效果的有效途径，EasyTune为高效可控运动生成提供了新范式。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [117] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出FSP-Diff框架，通过融合投影域与图像域信息、引入全谱先验及高效潜在扩散合成，显著提升超低剂量光子计数能谱CT重建的图像质量与计算效率。


<details>
  <summary>Details</summary>
Motivation: 超低剂量下能谱CT能量特异性投影信噪比急剧下降，导致重建图像严重伪影和结构细节丢失。

Method: 提出全谱先验增强的双域潜在扩散框架FSP-Diff，包含互补特征构建、全谱先验集成和高效潜在扩散合成三部分。

Result: 在模拟与真实数据集上，FSP-Diff在图像质量和计算效率上均显著优于现有最先进方法。

Conclusion: FSP-Diff为临床可行的超低剂量能谱CT成像提供了有效解决方案。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [118] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Continuity-driven Synergistic Diffusion with Neural priors (CSDN)的新方法，用于超稀疏视角下的锥形束CT（CBCT）重建，通过神经先验建模三维连续衰减场，并结合双路径扩散策略（正弦图与数字放射影像）提升角度连续性与层间一致性，显著抑制伪影并恢复细节。


<details>
  <summary>Details</summary>
Motivation: CBCT临床应用受限于辐射剂量与图像质量的权衡；超稀疏角采样虽降低剂量，却引发严重欠采样伪影和层间不一致，影响诊断可靠性。

Method: 提出CSDN框架：首先利用神经先验构建连续三维衰减表示以生成物理一致的密集投影；继而设计协同扩散策略，包括正弦图细化扩散（Sino-RD）和数字放射影像细化扩散（DR-RD）两条路径，并通过双投影重建融合（DPRF）模块自适应融合输出。

Result: 在超稀疏视角条件下，CSDN有效抑制伪影、恢复精细纹理，性能优于现有最先进方法。

Conclusion: CSDN通过神经先验与协同扩散机制，在保证物理一致性的前提下，实现了高保真、高连续性的超低剂量CBCT重建，为临床安全高效成像提供了新范式。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [119] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 本文对最先进的深度伪造检测技术进行了全面的实证分析，发现当前检测模型在面对基于扩散模型、NeRF和改进GAN等最新合成技术生成的深度伪造视频时表现显著下降，甚至人类评估者也难以识别高质量伪造内容，凸显了检测技术落后于生成技术发展的严峻现实。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术（如扩散模型、NeRF、改进GAN）快速发展，导致伪造内容愈发逼真且易得，而现有检测方法是否仍有效亟需系统性实证检验。

Method: 对当前主流深度伪造检测模型进行大规模实证评估，并开展人类评估实验，对比其在最新合成技术（如扩散模型、NeRF、先进GAN）生成的深伪视频上的检测性能。

Result: 多数SOTA检测模型在面对最新深度伪造技术生成的内容时性能急剧下降；人类参与者在识别最高质量深伪视频时亦表现不佳。

Conclusion: 当前深度伪造检测方法已明显滞后于生成技术的发展，亟需持续优化与创新，以弥合这一关键能力鸿沟。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [120] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: 本文提出MCIE-E1方法，通过空间感知与背景一致的跨注意力模块提升复杂指令图像编辑的指令遵循性与背景一致性，并构建新数据集与基准CIE-Bench验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法难以处理复杂、组合式编辑指令，存在指令遵循不足和背景不一致两大问题。

Method: 提出MCIE-E1方法，包含空间感知交叉注意力模块（增强指令-区域对齐）与背景一致交叉注意力模块（保留未编辑区域特征）；构建融合MLLM自动筛选与人工验证的复杂指令数据集；设计新基准CIE-Bench及两项新评估指标。

Result: 在CIE-Bench上MCIE-E1显著优于SOTA方法，指令遵循性提升23.96%，定量与定性结果均更优。

Conclusion: MCIE-E1从模型架构、数据构建与评估体系三方面系统性推动了复杂指令图像编辑的发展，有效缓解了指令合规性与背景一致性难题。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [121] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究提出了一种基于超声特异性自监督预训练的深度学习模型USF-MAE，用于一胎期超声图像中囊性水瘤的自动检测，在小样本标注数据下显著优于DenseNet-169基线模型。


<details>
  <summary>Details</summary>
Motivation: 囊性水瘤是高风险产前超声征象，但监督式深度学习受限于标注数据稀缺；亟需利用大量无标签超声数据提升模型性能。

Method: 采用在37万张无标签超声图像上预训练的USF-MAE模型，针对囊性水瘤二分类任务进行微调，并与DenseNet-169在相同数据集、预处理流程和4折交叉验证下对比；使用Score-CAM进行可解释性分析。

Result: USF-MAE在准确率（0.96 vs 0.93）、敏感性（0.94 vs 0.92）、特异性（0.98 vs 0.94）和ROC-AUC（0.98 vs 0.94）上均显著优于基线，Wilcoxon检验p=0.0057；Score-CAM可视化显示模型关注胎儿颈部区域，具临床合理性。

Conclusion: 超声特异性自监督预训练能有效提升小样本下囊性水瘤检测性能与鲁棒性，为产前早筛提供可行AI工具。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [122] [PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping](https://arxiv.org/abs/2602.08020)
*Minghai Chen,Mingyuan Liu,Yuxiang Huan*

Main category: cs.CV

TL;DR: 本文提出了PhysDrape，一种结合神经网络与显式物理求解器的混合方法，用于实现高保真、无穿透的服装动态模拟。通过物理信息图神经网络预测残差位移，并引入可微的两阶段求解器（力平衡求解 + 碰撞投影），在保证几何可行性的同时提升物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的服装仿真方法依赖软约束处理碰撞，导致几何可行性与物理合理性之间的固有矛盾：强惩罚易扭曲网格，弱惩罚则引发穿透。

Method: 提出PhysDrape框架：1）构建物理增强图（含材料参数与人体邻近信息）驱动的物理信息图神经网络，预测残差位移；2）设计可微的两阶段求解器——先用基于StVK模型的可学习力求解器达成准静态平衡，再用可微投影严格满足人体表面碰撞约束。

Result: 实验表明PhysDrape在保持极低穿透量的同时，显著降低应变能，物理保真度和鲁棒性优于现有方法，且支持实时推理。

Conclusion: PhysDrape通过将显式物理约束与端到端可微学习深度融合，有效解决了深度学习服装仿真中碰撞处理与物理一致性难以兼顾的核心挑战，为神经物理仿真提供了新范式。

Abstract: Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.

</details>


### [123] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID是一种无需训练的VLLM推理加速框架，通过注意力与多样性驱动的视觉令牌选择（ADTS）和基于树的时空令牌合并（TSTM）来高效压缩视频token，在仅保留10%视觉token的情况下保持99.1%原始性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM加速方法独立压缩空间和时间冗余，忽视时空关联性，导致压缩效果不佳；而视频中高度相关的视觉特征随时间在位置、尺度、朝向等维度动态变化，需联合建模时空冗余。

Method: 提出FlashVID框架，包含两阶段：1）Attention and Diversity-based Token Selection (ADTS)，选取最具代表性的基础视频token；2）Tree-based Spatiotemporal Token Merging (TSTM)，细粒度消除时空冗余；全程无需训练，即插即用。

Result: 在三个主流VLLM和五个视频理解基准上验证有效；仅用10%视觉token即保持LLaVA-OneVision 99.1%性能；使Qwen2.5-VL视频帧输入提升10倍，在相同计算预算下相对性能提升8.6%。

Conclusion: FlashVID是一种高效、通用、训练无关的VLLM视频推理加速方案，显著提升长视频处理能力与效率，具备强实用性与可扩展性。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [124] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了MIND，首个面向世界模型记忆一致性与动作控制能力评估的开放域闭环基准，包含250个高质量视频，并设计了评估框架与基线模型MIND-World，揭示了当前模型在长期记忆一致性和动作空间泛化上的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型缺乏统一基准来系统评估其理解、记忆与预测动态视觉环境的核心能力，尤其是记忆一致性与动作控制这两项基础能力。

Method: 构建了MIND基准：包含250个1080p/24FPS视频（分第一/第三人称及不同动作空间），设计了衡量记忆一致性和动作控制的闭环评估框架，并提出交互式基线模型MIND-World（Video-to-World）。

Result: 实验证明MIND具备完整性；揭示了当前世界模型在维持长时记忆一致性与跨动作空间泛化方面存在显著瓶颈。

Conclusion: MIND填补了世界模型综合评估的空白，为推动具身智能与视频理解的协同发展提供了标准化、可扩展的评测平台。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [125] [Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects](https://arxiv.org/abs/2602.08046)
*Yahia Hamdi,Nicolas Andrialovanirina,Kélig Mahé,Emilie Poisson Caillault*

Main category: cs.CV

TL;DR: 本文提出了一种将深度3D卷积生成对抗网络（CGAN）与专家混合（MoE）框架相结合的新方法，用于高质量3D模型生成与不完整/受损物体重建，并引入无辅助损失的动态容量约束（DCC）机制以提升专家选择的合理性与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在处理复杂、异构、结构精细的3D数据（尤其是不完整输入）时存在建模能力弱、计算开销大、泛化性差等问题，亟需更高效灵活的架构。

Method: 将多个模态特化的3D CGAN生成器集成到MoE框架中，并设计无辅助损失的动态容量约束（DCC）机制，实现输入驱动的专家选择与负载均衡；判别器采用标准3D卷积结构，端到端联合训练。

Result: 在3D形状生成与补全任务上，该MoE-DCGAN在定量（如Chamfer距离、IoU）和定性指标上均优于当前主流方法，尤其在大区域缺失场景下鲁棒性更强，且计算效率更高。

Conclusion: MoE架构与3D CGAN的有效融合显著提升了对复杂3D数据分布的建模能力；所提出的DCC机制在不增加训练负担前提下保障了专家专业化与系统稳定性，为3D生成与补全提供了可扩展新范式。

Abstract: The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>


### [126] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 本文提出了一种系统性地使ViT关键组件（如Patch Embedding、Self-Attention、位置编码和上下采样）保持等变性的框架，构建出具有理论保证且实用灵活的等变ViT，显著提升性能与数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有等变ViT难以在性能与等变性之间取得平衡，尤其在协调Self-Attention与Patch Embedding等多样化模块时存在挑战。

Method: 系统性地将ViT的关键组件（包括patch embedding、self-attention、positional encodings、Down/Up-Sampling）设计为等变形式，构建具备严格等变保证的ViT架构，并支持无缝扩展至Swin Transformer等变体。

Result: 所提等变ViT在多种视觉任务中一致提升了模型性能与数据效率，且具备良好的可扩展性与即插即用特性。

Conclusion: 通过全面等变化设计ViT核心模块，可兼顾理论严谨性与实际有效性，为构建高性能等变视觉模型提供了通用可行路径。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [127] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出了一种多模态弱监督框架，用于视频中隐蔽情绪的自动识别，在iMiGUE数据集上达到SOTA性能。通过YOLO和DINOv2提取视觉特征，Gemini 2.5 Pro生成伪标签与推理文本，OpenPose+MLP建模关键点序列，结合Transformer与BERT融合多模态特征，最终在严重类别不平衡下将准确率提升至0.69以上。


<details>
  <summary>Details</summary>
Motivation: 解决视频中“隐蔽情绪”自动识别的难题，尤其在标注稀缺、类别严重不平衡的实际场景下缺乏有效方法。

Method: 构建多模态弱监督框架：1）YOLOv11x检测裁剪人脸，DINOv2-Base提取视觉特征；2）Gemini 2.5 Pro结合CoT+Reflection生成伪标签与推理文本；3）OpenPose提取137维关键点序列并加入帧间偏移特征，用MLP替代传统GCN建模；4）超长序列Transformer分别编码图像与关键点序列，拼接BERT编码的访谈文本；5）各模态独立预训练后联合微调，并融入伪标签样本。

Result: 在iMiGUE网球采访数据集上准确率从先前工作的不足0.6提升至0.69以上，建立新公开基准；验证了简化后的MLP关键点主干可媲美甚至超越GCN结构。

Conclusion: 多模态弱监督范式（尤其是大模型驱动的伪标签生成与轻量高效的关键点建模）能显著提升隐蔽情绪识别性能，为低资源情感分析任务提供了可行且高效的解决方案。

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [128] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: 本文提出DICE框架，无需训练即可实时擦除扩散模型中艺术家风格，通过对比子空间分解实现风格与内容解耦，并采用自适应注意力解耦编辑策略，在保证内容完整性的同时高效去除未经授权的风格模仿。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的普及导致未经授权的艺术风格模仿泛滥，带来版权与知识产权风险；现有防护方法需昂贵权重编辑或依赖显式指定风格，难以实际部署。

Method: 提出DICE（基于对比子空间分解的艺术家风格-内容解耦）框架：构建对比三元组在潜在空间区分风格与非风格特征，将其形式化为广义特征值问题以精确定位风格子空间；引入自适应注意力解耦编辑策略，动态评估各token风格浓度，差异化抑制QKV向量中的风格成分并增强内容成分。

Result: 实验表明DICE在风格擦除彻底性与内容保真度间取得更优平衡；仅增加3秒开销，具备实际部署可行性。

Conclusion: DICE是一种训练免费、高效实用的部署侧风格防护技术，有效遏制扩散模型中的风格盗用问题。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [129] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出ReRoPE框架，通过利用预训练视频扩散模型中旋转位置编码（RoPE）未被充分利用的低频谱带，无缝注入相对相机姿态信息，实现无需额外训练或架构修改的可控视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定参考帧的相机姿态编码缺乏平移不变性，导致泛化差和累积漂移；而相对姿态编码虽更鲁棒，但难以低成本融入预训练视频扩散模型。

Method: 提出ReRoPE：将相对相机姿态信息注入预训练视频扩散模型中RoPE的低频未利用频谱带，作为即插即用模块，不改变原有模型结构或需大规模重训练。

Result: 在图像到视频（I2V）和视频到视频（V2V）任务上验证了ReRoPE在相机控制精度与视觉保真度上的优越性，显著提升可控性且保持生成质量。

Conclusion: ReRoPE为高保真、可控视频生成提供了一种训练高效、即插即用的解决方案，有效弥合了相对相机建模与预训练扩散模型之间的集成鸿沟。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [130] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

TL;DR: 本文提出ViT-5，通过对归一化、激活函数、位置编码、门控机制和可学习token等组件进行系统性改进，在保持标准Attention-FFN结构的前提下，显著提升Vision Transformer性能，在分类与生成任务上均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代Vision Transformer架构在过去五年中已有诸多进展，但经典ViT主干尚未充分整合这些进步；本文旨在系统性地将最新架构改进融入ViT，构建更强大的新一代视觉骨干网络。

Method: 在保持标准Attention-FFN结构基础上，对归一化方式、激活函数、位置编码、门控机制及可学习token等关键组件进行逐项优化与替换，形成ViT-5架构，并在多种理解与生成任务上进行验证。

Result: ViT-5-Base在ImageNet-1k上达84.2% top-1精度（优于DeiT-III-Base的83.8%）；在SiT扩散模型中FID降至1.84（原ViT为2.06）；同时展现出更强的表征能力、空间推理能力和跨任务迁移性。

Conclusion: ViT-5是一种与当前基础模型实践对齐的简单即插即用升级方案，为2020年代中期的视觉骨干网络提供了高效、鲁棒且通用的替代选择。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [131] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文提出了一种无需视觉监督和视频端微调的轻量级文本对齐策略，利用多模态大语言模型（MLLM）中间层嵌入与校准后的输出头，在视频-文本检索任务上实现零样本高性能，并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有生成式多模态大语言模型（MLLMs）在视频任务上的嵌入性能仍弱于专用视频基础模型（VFMs），而直接微调成本高；本文旨在探索如何更高效、更少监督地利用预训练MLLM进行视频-文本嵌入与检索。

Method: 1）系统性层分析，发现MLLM中间层已含丰富任务相关信息；2）结合中间层嵌入与校准的MLLM头部，实现零样本检索；3）提出轻量级文本对齐策略：将密集视频字幕映射为简短摘要，仅基于文本进行视频-文本嵌入学习，无需视觉监督或视频端微调。

Result: 在多个主流视频检索基准上，该方法在零样本且仅文本微调条件下，显著超越现有方法，达到最先进（SOTA）性能。

Conclusion: MLLM中间层蕴含强表征能力，结合文本驱动的对齐策略可有效释放其视频理解潜力，无需视觉微调即可实现高性能视频-文本检索。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [132] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: 本文介绍了MMLSv2数据集，用于火星表面滑坡分割任务，包含多模态影像（RGB、DEM、坡度、热惯量和灰度通道）共664张图像，并额外提供276张地理隔离测试图像以评估空间泛化能力；实验表明该数据集支持稳定训练但对细碎、狭长及小尺度滑坡区域仍具挑战性，且在隔离测试集上性能明显下降，凸显其对模型鲁棒性和泛化能力评估的价值。


<details>
  <summary>Details</summary>
Motivation: 现有火星滑坡分割数据集缺乏多模态信息和地理泛化评估能力，难以全面评估模型在真实场景中的鲁棒性与泛化性。

Method: 构建MMLSv2多模态滑坡分割数据集，涵盖RGB、数字高程模型（DEM）、坡度、热惯量及灰度共7个影像波段；划分664张图像为训练/验证/测试集，并新增276张地理隔离测试图像；在多个分割模型上开展实验，评估其性能与泛化能力。

Result: 多个分割模型在MMLSv2上训练稳定并取得有竞争力的性能，但在细碎、狭长和小尺度滑坡区域仍存在困难；在地理隔离测试集上性能显著下降，验证了该数据集对评估模型空间泛化能力的有效性。

Conclusion: MMLSv2是一个具有多模态、大规模和地理隔离测试设计的高质量火星滑坡分割基准数据集，有助于推动面向行星遥感的鲁棒语义分割研究。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [133] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 本文研究了在噪声大、类别不平衡的卫星图像数据上，使用Vision Transformer（ViT）模型（DINOv2-small和DeiT）进行建筑物损毁多类分类的效果，并提出了一种基于图像块的预处理方法和冻结头部微调策略，在xBDD数据集上取得了与CNN基线相当的宏观F1分数。


<details>
  <summary>Details</summary>
Motivation: 快速建筑物损毁评估对灾后响应至关重要，但卫星图像数据中存在标签噪声和严重类别不平衡问题，给模型训练带来挑战。

Method: 采用DINOv2-small和DeiT模型，设计基于图像块的预处理流程以突出结构特征、抑制背景噪声，并使用冻结头部的微调策略；评估指标包括准确率、精确率、召回率和宏平均F1分数。

Result: 小规模ViT模型结合所提训练方法，在xBDD数据集上实现了与先前CNN基线模型具有竞争力的宏平均F1分数。

Conclusion: ViT架构在噪声和不平衡的遥感灾害数据上具备实用潜力，尤其在计算资源受限场景下，通过合理预处理与微调策略可达到与CNN相当的性能。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [134] [MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection](https://arxiv.org/abs/2602.08126)
*Venkatraman Narayanan,Bala Sai,Rahul Ahuja,Pratik Likhar,Varun Ravi Kumar,Senthil Yogamani*

Main category: cs.CV

TL;DR: MambaFusion是一种新型多模态3D目标检测框架，结合选择性状态空间模型（SSM）与窗口化Transformer，并引入多模态令牌对齐（MTA）模块和可靠性感知融合门，实现高效、自适应且物理可解释的BEV感知，在nuScenes上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于BEV的相机-LiDAR融合方法存在上下文建模低效、空间融合不变、不确定性推理能力弱等问题，难以兼顾精度、效率与鲁棒性。

Method: 提出MambaFusion框架：1）交替堆叠SSM与窗口Transformer以线性复杂度传播全局上下文并保持局部几何保真；2）设计多模态令牌对齐（MTA）模块与可靠性感知融合门，依据空间置信度与标定一致性动态加权特征；3）采用结构约束的扩散检测头，融合图推理与不确定性感知去噪，保障物理合理性和校准置信度。

Result: 在nuScenes基准上取得新SOTA性能，同时保持线性时间复杂度；具备更强的时序稳定性、鲁棒性与可解释性。

Conclusion: 将SSM的高效建模能力与可靠性驱动的多模态融合相结合，可构建面向真实自动驾驶场景的稳健、高效、可解释的3D感知系统。

Abstract: Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>


### [135] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: This paper introduces the Fields of The World (FTW) ecosystem—a benchmark dataset of 1.6M field polygons across 24 countries, pre-trained models, and tools for field boundary extraction and crop classification—demonstrating its utility in local- and country-scale agricultural mapping with limited labels.


<details>
  <summary>Details</summary>
Motivation: Field boundary maps are essential for agricultural data products like crop monitoring, yield estimation, and disease estimation; however, global, high-quality field boundary data is scarce and fragmented.

Method: The authors develop the FTW ecosystem including a large-scale benchmark dataset, pre-trained segmentation models, CLI inference tools, and two Jupyter notebooks for local-scale (with crop classification and forest loss attribution) and country-scale (using cloud-optimized geospatial data) analysis; they employ MOSAIKS random convolutional features combined with FTW boundaries for field-level crop type classification.

Result: Achieved macro F1 scores of 0.65–0.75 for crop type classification using limited labels; provided pre-computed predictions over five countries covering 4.76M km², with median field sizes ranging from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

Conclusion: The FTW ecosystem provides a scalable, open, and practical foundation for global field-level agricultural analytics, enabling both research and operational applications with minimal labeling effort.

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [136] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 本文发现现代视觉语言模型（VLMs）在安全对齐过程中忽略了分片图像（split-image）输入中的分布式有害语义，从而提出新型分片图像视觉越狱攻击（SIVA），通过渐进式白盒与黑盒策略（含对抗知识蒸馏Adv-KD）显著提升跨模型迁移成功率，并给出缓解该漏洞的高效方案。


<details>
  <summary>Details</summary>
Motivation: 现有VLM的安全对齐（如RLHF）主要基于整体图像，未覆盖有害语义分散在多个图像片段中、仅在组合后才显现的情形，导致模型对分片图像越狱攻击鲁棒性不足。

Method: 提出分片图像视觉越狱攻击（SIVA），包含从简单分片到自适应白盒攻击、再到基于对抗知识蒸馏（Adv-KD）的黑盒迁移攻击的渐进式方法。

Result: 在三个SOTA VLM和三个越狱数据集上，最强攻击相较基线最高提升60%跨模型迁移成功率。

Conclusion: VLM安全对齐存在分片图像漏洞；SIVA有效暴露该问题，Adv-KD显著增强黑盒迁移性；需扩展安全训练数据覆盖分片图像场景。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [137] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出AVIC框架，通过自适应控制测试时视觉想象的调用，提升多模态大模型在空间推理任务中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在依赖未见视角的空间推理上表现不可靠；盲目使用视觉想象会增加计算开销并引入误导性证据。

Method: 提出AVIC自适应测试时框架，利用世界模型显式判断当前视觉证据是否充分，并据此选择性调用和缩放视觉想象。

Result: 在SAT、MMSI和R2R等基准上验证了想象在不同场景下的作用（关键/边缘/有害），AVIC以更少的世界模型调用和语言token达到或超越固定想象策略的效果。

Conclusion: 测试时视觉想象应作为可控资源进行分析与调控，以实现高效可靠的空间推理。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [138] [DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation](https://arxiv.org/abs/2602.08168)
*Mei Ling Chee,Thangarajah Akilan,Aparna Ravindra Phalke,Kanchan Keisham*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级语义分割模型DAS-SK，通过将选择性核卷积（SK-Conv）嵌入双空洞可分离卷积（DAS-Conv）并增强ASPP模块，在保持高精度的同时显著降低参数量和计算量，适用于农业遥感与边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 高分辨率农业图像语义分割需兼顾精度与计算效率，而现有方法常受限于大数据依赖、光谱泛化能力弱及高计算开销，难以部署于无人机等边缘设备。

Method: 提出DAS-SK架构：1）在DAS-Conv中引入SK-Conv以强化多尺度特征学习；2）改进ASPP模块以同时捕获细粒度局部结构与全局上下文；3）基于改进DeepLabV3框架，采用MobileNetV3-Large与EfficientNet-B3双主干。

Result: 在LandCover.ai、VDD和PhenoBench三个基准上达到SOTA性能；相比先进Transformer模型，参数量减少最多21倍，GFLOPs减少最多19倍。

Conclusion: DAS-SK是一种鲁棒、高效且可扩展的语义分割方案，适用于实时农业机器人与高分辨率遥感，并具备向其他视觉任务推广的潜力。

Abstract: Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>


### [139] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件分数扩散的多模态生成回归模型（MCSDR），用于从超声心动图视频中估计左心室射血分数（LVEF），建模其后验分布而非单一确定值，提升了在病理复杂场景下的鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基于MSE的确定性回归方法难以应对LVEF估计中固有的多模态、重尾后验分布问题，尤其在噪声大或病理变异大的情况下易产生误导性预测。

Method: 提出Multimodal Conditional Score-based Diffusion model for Regression（MCSDR），一种生成式回归框架，利用分数扩散模型建模LVEF在超声视频和患者人口统计先验条件下的连续后验分布。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric和CAMUS数据集上达到SOTA性能；定性分析显示其采样轨迹能自适应反映高噪声或高生理变异情形，增强可解释性。

Conclusion: 生成式回归范式（尤其是基于扩散模型的方法）比传统确定性回归更适于建模医学影像中具有内在不确定性的生理参数估计任务，为AI辅助诊断提供了更可靠、更具可解释性的工具。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [140] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种面向遥感开放词汇语义分割的地理空间推理思维链（GR-CoT）框架，通过离线知识蒸馏与在线实例推理双流机制，增强多模态大模型对地理场景的理解能力，缓解光谱相似地物的语义歧义问题，提升像素级语义映射精度。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割方法依赖视觉与文本特征的被动匹配，缺乏地理空间上下文感知，导致光谱相似但语义不同的地物类别严重混淆。

Method: 提出Geospatial Reasoning Chain-of-Thought（GR-CoT）框架，包含离线知识蒸馏流（建立细粒度类别解释标准）和在线实例推理流（依次执行宏观场景锚定、视觉特征解耦、知识驱动决策合成），生成图像自适应词汇以指导分割。

Result: 在LoveDA和GID5基准上实验验证了该方法的优越性，显著提升了开放词汇语义分割的准确性与地理语义一致性。

Conclusion: GR-CoT通过引入地理空间推理机制，有效弥补了多模态大模型在遥感语义理解中的上下文缺失，为开放词汇分割提供了可解释、可泛化的新型范式。

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [141] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的Chain-of-Caption框架，通过结合多种视觉和文本上下文提升多模态大语言模型（MLLM）在指代表达理解（REC）任务上的性能，在多个基准数据集上实现5%–30%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在REC任务上已取得高精度，但如何有效利用工具提供额外视觉/文本上下文以进一步提升性能尚待系统分析。

Method: 系统分析不同工具提供的视觉与文本上下文对MLLM在REC任务中的影响，并提出无需训练的Chain-of-Caption框架，融合多源上下文提升定位能力。

Result: 在RefCOCO/RefCOCOg/RefCOCO+/Ref-L4数据集上，该方法在不同IoU阈值下相较基线模型提升5%–30%准确率，且无需任何微调。

Conclusion: 单一文本或视觉上下文即可提升REC性能；多上下文协同能显著增强MLLM的指代定位能力，验证了训练-free框架的有效性与实用性。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [142] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: 本文提出Octopus框架，通过修正特定的rollout增强和响应掩码策略，提升视觉语言模型（VLM）的自修正能力，在多个基准上达到开源VLM中的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以学习自修正行为，因为有效自修正行为出现稀疏，导致学习信号极度稀疏。

Method: 提出修正特定rollout（Octopus）增强框架，通过重组已有rollout生成密集自修正样本；引入响应掩码策略，解耦自修正与直接推理以避免信号冲突。

Result: 在7个基准测试中，Octopus-8B作为具备可控自修正能力的推理VLM，达到开源VLM中的SOTA性能，优于最佳RLVR基线1.0分，且每步训练时间仅为其0.72倍。

Conclusion: Octopus框架显著提升了VLM自修正能力的学习效率与稳定性，为复杂推理任务提供了更高效、可控的训练范式。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [143] [Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval](https://arxiv.org/abs/2602.08224)
*Jing Zhang,Zhikai Li,Xuewen Liu,Qingyi Gu*

Main category: cs.CV

TL;DR: 本文提出Efficient-SAM2，通过对象感知的稀疏计算机制（SWR和SMR）提升SAM2在视频分割中的推理效率，在仅损失1.0%精度下实现1.68倍加速。


<details>
  <summary>Details</summary>
Motivation: SAM2虽在视频目标分割中性能优异，但计算开销大，现有轻量化方法多依赖重新训练骨干网络，缺乏对后训练加速的探索；作者观察到SAM2存在类生物视觉的稀疏感知模式，可据此削减冗余计算。

Method: 提出Efficient-SAM2：1）面向图像编码器的对象感知稀疏窗口路由（SWR），利用前帧解码器的显著性与一致性线索，将背景区域路由至轻量快捷分支；2）面向记忆注意力的对象感知稀疏记忆检索（SMR），仅保留每帧中显著记忆token参与计算，并复用其首次被召回时的显著性模式。

Result: 在SAM2.1-L模型上实现1.68倍加速，SA-V测试集精度仅下降1.0%，且引入参数极少、训练开销小。

Conclusion: 利用SAM2内在的稀疏感知特性，通过SWR和SMR两种后训练稀疏化策略，可在几乎不牺牲精度的前提下显著提升其视频分割推理效率，为实时应用提供可行路径。

Abstract: Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>


### [144] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: 本文提出了首个基于点云表示生成对抗事件的运动感知对抗框架MA-ADV，通过扩散方法平滑扰动并结合Adam优化、迭代精炼与二分搜索，实现100%攻击成功率和强防御鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 事件相机在自动驾驶等安全关键领域广泛应用，但其深度神经网络易受对抗样本攻击；而主流事件表征不可微，限制了梯度类攻击方法的应用，相关研究十分匮乏。

Method: 提出MA-ADV框架：利用点云表示建模事件，引入扩散机制平滑高频噪声扰动，融合空间-时间关系，并通过样本级Adam优化、迭代精炼与二分搜索联合求解最小代价扰动。

Result: 在实验中实现了100%攻击成功率，扰动代价最小，并展现出对多种防御方法的更强鲁棒性。

Conclusion: MA-ADV揭示了事件感知系统面临严峻的安全挑战，为事件相机的对抗鲁棒性研究提供了新思路和首个有效基准。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [145] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: 本文提出DeCI框架，通过周期-漂移分解和通道独立性建模原始fMRI时间序列，显著优于传统功能连接方法，推动端到端时序建模在脑疾病分类中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI分析方法多依赖静态功能连接（FC），丢失时间动态信息且仅捕获线性关系，难以刻画脑信号中的振荡周期与缓慢基线漂移等复杂时序特性。

Method: 首先对多个先进时序模型在原始BOLD信号上进行基准测试；进而提出DeCI框架，包含ROI级的周期与漂移分解、以及通道独立建模机制，实现对每个脑区时间序列的独立、鲁棒建模。

Result: DeCI在五个公开数据集上 consistently 超越FC方法和各类时序基线模型，分类准确率与泛化能力均更优。

Conclusion: 直接建模原始fMRI时间序列比提取静态功能连接更有效；周期-漂移解耦与通道独立性是提升fMRI时序建模性能的关键设计原则。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [146] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出PISCO，一种基于视频扩散模型的精确视频实例插入方法，支持任意稀疏关键帧控制，通过可变信息引导、分布保持的时间掩码和几何感知条件等技术，实现高保真、物理一致的视频编辑。


<details>
  <summary>Details</summary>
Motivation: 专业AI辅助电影制作中需要精确、目标明确的视频修改，尤其是视频实例插入任务，要求在保持场景完整性的同时实现精确时空定位、物理一致的场景交互和原始动态的忠实保留。

Method: 提出PISCO视频扩散模型，支持单关键帧、起止关键帧或任意时间戳稀疏关键帧控制；引入可变信息引导（Variable-Information Guidance）、分布保持的时间掩码（Distribution-Preserving Temporal Masking）和几何感知条件（geometry-aware conditioning）以应对稀疏条件导致的分布偏移；构建PISCO-Bench基准并采用参考式与无参考式感知指标评估。

Result: PISCO在稀疏控制下持续优于强图像修复与视频编辑基线；随着额外控制信号增加，性能呈现清晰、单调提升；在PISCO-Bench上验证了其高保真与可控性。

Conclusion: PISCO为AI视频生成向细粒度可控编辑演进提供了有效范式，显著提升了稀疏用户干预下的视频实例插入质量与鲁棒性。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [147] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出了一种多模态融合框架，结合Presence-Absence（PA）和Presence-Only（PO）植物分布数据，通过地理对齐的伪标签聚合、三模态交叉注意力机制及基于空间邻近性的专家混合推理策略，显著提升了跨物种大尺度植物分布预测性能，尤其在PA数据稀疏且存在地理分布偏移时效果突出。


<details>
  <summary>Details</summary>
Motivation: 解决植物分布预测中PA数据稀缺昂贵、PO数据标签噪声严重的问题，同时应对真实场景下观测数据稀疏性、偏差性及训练测试间地理分布偏移的挑战。

Method: 提出多模态融合框架：1）基于卫星影像地理覆盖的PO数据伪标签聚合策略；2）采用Swin Transformer Base（遥感图像）、TabM（表格特征）、Temporal Swin Transformer（时间序列）作为各模态骨干；3）设计可堆叠的串行三模态交叉注意力机制；4）引入基于PA样本空间邻近性的混合专家推理策略，分区调用不同训练模型。

Result: 在GeoLifeCLEF 2025数据集上验证，该方法在PA覆盖有限且分布偏移显著的场景下，预测性能优于现有方法。

Conclusion: 融合PA与PO数据并辅以地理对齐、多模态协同建模与空间自适应推理，是提升大规模跨物种植物分布预测鲁棒性与准确性的有效路径。

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [148] [CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment](https://arxiv.org/abs/2602.08309)
*Yunzuo Hu,Wen Li,Jing Zhang*

Main category: cs.CV

TL;DR: 本文提出CAE-AV框架，通过CASTE和CASE两个模块及轻量级目标函数，缓解音视频模态错位问题，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 音频-视觉学习常因屏外声源和背景杂乱导致模态错位，现有方法易放大无关区域或时刻，造成训练不稳定与表征质量下降。

Method: 提出Caption-aligned and Agreement-guided Enhancement（CAE-AV）框架，包含：1）Cross-modal Agreement-guided Spatio-Temporal Enrichment（CASTE），基于帧级音视频一致性动态平衡时空关系；2）Caption-Aligned Saliency-guided Enrichment（CASE），利用高阶语义线索引导关键时空位置增强；并设计caption-to-modality InfoNCE、视听一致性与熵正则化等轻量目标指导token选择。

Result: 在AVE、AVVP、AVS和AVQA等多个音视频任务基准上，使用冻结主干网络时达到当前最优性能；定性分析验证其对音视频错位具有鲁棒性。

Conclusion: CAE-AV通过双重互补增强机制与语义引导的目标函数，有效缓解音视频模态错位，提升跨模态表征质量与模型鲁棒性。

Abstract: Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>


### [149] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种语言引导的运动离散化方法（LG-Tok），通过在tokenization阶段对齐自然语言与运动，生成紧凑、高层语义表征，从而在保持高质量运动重建的同时降低生成复杂度；并设计了基于Transformer的tokenizer和语言丢弃机制，在HumanML3D和Motion-X上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 提高运动重建质量通常需增加token数量，但会加剧生成模型学习难度；现有卷积tokenizers难以支持全局语言引导。

Method: 提出Language-Guided Tokenization（LG-Tok）：1）在tokenization阶段对齐语言与运动；2）设计基于Transformer的tokenizer以利用注意力机制实现跨模态对齐；3）引入语言丢弃（language-drop）训练策略，使detokenizer支持无语言条件生成。

Result: 在HumanML3D和Motion-X上Top-1分别达0.542/0.582（SOTA为0.500/0.528），FID为0.057/0.088（SOTA为0.114/0.147）；轻量版LG-Tok-mini仅用一半token即达有竞争力性能（Top-1: 0.521/0.588, FID: 0.085/0.071）。

Conclusion: 语言引导的tokenization能有效提升运动表征的语义性与紧凑性，Transformer架构和language-drop策略协同增强了跨模态对齐能力与生成鲁棒性，为高效运动生成提供了新范式。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [150] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: 本文提出UGData数据集、UGE模型和UGBench基准，通过显式空间对齐提升城市多模态嵌入的可迁移性，在图像检索与地理定位等任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 城市理解具有本质空间性，但现有数据集和基准缺乏街景图像与城市结构之间的显式对齐。

Method: 构建空间锚定的数据集UGData，设计两阶段训练策略UGE（结合指令引导对比学习与图结构空间编码），并建立综合评估基准UGBench；在多个VLM骨干网络上采用LoRA微调固定维度空间嵌入。

Result: 基于Qwen2.5-VL-7B的UGE在训练城市图像检索和地理定位排名任务上分别提升44%和30%，在未见城市上分别提升超30%和22%。

Conclusion: 显式空间对齐能显著增强多模态嵌入在空间密集型城市任务中的泛化性与实用性。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [151] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 本文提出了首个专门用于评估‘图像思考’范式下过程奖励模型（PRMs）的综合基准，定义了7种细粒度错误类型，构建了包含1206条人工标注推理轨迹的数据集，并揭示了当前大视觉语言模型（LVLMs）作为PRMs的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有PRM评估基准以文本为中心，缺乏对‘图像思考’范式下推理过程的全面评估，而该范式因动态图像编辑与重编码易引入多样化错误，亟需专用PRM及相应评测基准。

Method: 通过分析推理轨迹和引导式PRM搜索实验，定义7类细粒度错误；构建覆盖4大类、16子类的1206条人工标注‘图像思考’推理轨迹数据集；对主流LVLMs作为PRMs进行系统实验评估。

Result: 发现当前LVLMs作为PRMs表现不佳：视觉推理过程评估能力有限、对不同错误类型的判别能力差异显著、存在正向评价偏差、且对推理步骤位置敏感。

Conclusion: 所提出的基准有效揭示了现有LVLMs在PRM任务上的不足，为未来面向LVLMs的过程奖励建模研究奠定了关键基础。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [152] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种多模态信息密度评估框架，构建了首个面向电商短视频理解的基准E-VAds，并开发了基于强化学习的推理模型E-VAds-R1，在商业意图推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要面向通用任务，忽视电商短视频中商业意图的推理需求，且其高密度多模态信号给模型带来挑战。

Method: 提出了多模态信息密度评估框架；构建了包含3961个高质量视频和19785个开放问答对的E-VAds基准；设计了基于RL的E-VAds-R1模型，采用多粒度奖励机制MG-GRPO。

Result: E-VAds-R1在商业意图推理任务上实现109.2%的性能提升，仅需数百样本训练。

Conclusion: 电商短视频具有更高多模态信息密度，需专用基准与推理模型；E-VAds和E-VAds-R1为该领域提供了新基准与高效方法。

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [153] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出GeoEdit框架，通过扩散变换器模块和Effects-Sensitive Attention机制，解决图像编辑中几何变换（平移、旋转、缩放）与光照阴影建模不准确的问题，并构建RS-Objects数据集进行训练，在多个指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像编辑中难以精确实现几何变换（如平移、旋转、缩放），且对复杂光照与阴影建模不足，导致编辑结果不真实。

Method: 提出GeoEdit框架，包含基于上下文生成的扩散变换器模块以实现精确几何编辑，以及Effects-Sensitive Attention机制增强光照与阴影建模；并构建大规模几何编辑数据集RS-Objects（12万+图像对）用于训练。

Result: 在公开基准测试中，GeoEdit在视觉质量、几何精度和现实感方面均持续优于当前最优方法。

Conclusion: GeoEdit有效解决了图像编辑中几何变换不准与光照阴影失真两大难题，为高质量可控图像编辑提供了新范式。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [154] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 本文提出D²-VR框架，通过降质鲁棒光流对齐、对抗蒸馏和协同优化，在保持高质量视频恢复的同时，将扩散模型采样速度提升12倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散先验与时间对齐的视频恢复方法虽感知质量高，但推理延迟大、时间稳定性差，难以应对复杂真实退化。

Method: 提出单图像扩散驱动的视频恢复框架D²-VR：1）设计降质鲁棒光流对齐（DRFA）模块，利用置信度感知注意力过滤不可靠运动线索；2）采用对抗蒸馏压缩扩散采样轨迹至少步数；3）设计协同优化策略兼顾感知质量与严格时间一致性。

Result: 在多项实验中达到SOTA性能，并实现12倍加速。

Conclusion: D²-VR有效平衡了视频恢复的质量、速度与时间稳定性，为扩散模型在实际视频修复中的部署提供了可行路径。

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [155] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: 本文提出了RealSynCol，一个高度逼真的合成结肠镜数据集，用于解决深度学习在结肠镜检查中因真实标注数据稀缺而导致的模型鲁棒性不足问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习有望通过3D重建等技术提升结肠镜检查效果，但缺乏大规模、带精确标注的真实数据严重限制了鲁棒方法的发展。

Method: 从10例CT扫描中提取结肠几何结构，构建高保真虚拟内镜环境，渲染真实血管纹理，生成包含28130帧图像及对应深度图、光流、3D网格和相机轨迹的合成数据集，并开展基准评测。

Result: RealSynCol在深度估计与位姿估计任务上展现出比现有合成数据集更优的临床图像泛化能力。

Conclusion: RealSynCol凭借其高真实性和多样性，成为开发支持内镜诊断的深度学习算法的有力工具。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [156] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 本文重新审视了基于注意力机制的稀疏图像匹配模型训练问题，发现检测器而非描述子是影响性能差异的主要原因，并提出一种利用多检测器关键点微调模型的新方法，实现了检测器无关的通用匹配模型。


<details>
  <summary>Details</summary>
Motivation: 先前研究中忽略了一个关键设计选择，影响了LightGlue等模型的性能；同时缺乏对检测器与描述子在匹配框架中各自作用的系统分析。

Method: 分析LightGlue模型的关键设计缺陷；探究检测器与描述子在Transformer匹配框架中的相对影响；提出基于多检测器关键点的微调策略，构建检测器无关的通用匹配模型。

Result: 所提方法训练出的零样本匹配模型在面对新型检测器时，精度达到甚至超过针对该检测器专门训练的模型。

Conclusion: 检测器是影响匹配性能的主导因素；通用、检测器无关的匹配模型是可行且有效的；该发现为Transformer匹配模型部署和局部特征设计提供了重要指导。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [157] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了Demo-driven Video In-Context Learning新任务及配套基准Demo-ICL-Bench，旨在评估多模态大模型从少量视频或文本示例中学习并泛化的能力，并设计了两阶段训练的Demo-ICL模型以应对该挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要测试模型静态内部知识，缺乏对模型从动态、新颖上下文（如少样本示例）中学习与适应能力的评估。

Method: 提出Demo-driven Video In-Context Learning任务；构建含1200个教学视频及对应问题的Demo-ICL-Bench基准，包含字幕摘要（文本示例）和教学视频（视频示例）两类演示；设计Demo-ICL模型，采用视频监督微调+信息辅助直接偏好优化的两阶段训练策略。

Result: 实验证明Demo-ICL-Bench具有挑战性，Demo-ICL模型在该基准上显著优于现有SOTA MLLMs，验证了方法有效性。

Conclusion: 本工作填补了视频少样本上下文学习评估的空白，推动MLLM向具备快速情境学习能力的方向发展，并为未来研究提供了新基准与方法范式。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [158] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: 本文提出了Vista框架，用于场景感知的流式视频问答，通过场景感知分割、压缩和召回机制，在保证效率的同时实现长视频流的高效推理。


<details>
  <summary>Details</summary>
Motivation: 流式视频问答面临视频帧顺序到达、用户查询时间点任意等挑战，现有方法存在上下文丢失或内存溢出问题。

Method: Vista框架包含三个核心创新：(1) 场景感知分割，动态聚类视频帧为时序与视觉一致的场景单元；(2) 场景感知压缩，将每个场景压缩为紧凑token表示存于GPU内存，高分辨率帧卸载至CPU；(3) 场景感知召回，按需检索并重整合相关场景到模型输入中。

Result: 在StreamingBench上实验表明，Vista达到当前最优性能，为真实场景流式视频理解建立了强基线。

Conclusion: Vista是一种模型无关、高效可扩展的流式视频QA框架，兼顾推理完整性与系统效率，适用于长时程实时视频理解任务。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [159] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为TriC-Motion的新型扩散模型，通过联合建模空间、时间与频域，并引入因果干预机制，显著提升了文本到动作生成的质量与对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对空间、时间与频率三域的统一联合优化，且易受运动无关噪声干扰，导致生成质量受限。

Method: 提出TriC-Motion框架，包含三个域特异性建模模块（时序运动编码、空间拓扑建模、混合频域分析）、分数引导的三域融合模块，以及基于因果的反事实动作解耦器。

Result: 在HumanML3D数据集上R@1达0.612，显著优于现有SOTA方法，生成动作具有高保真度、一致性、多样性与文本对齐性。

Conclusion: TriC-Motion通过三域协同建模与因果解耦，有效克服了多域信息利用不足与噪声干扰问题，为文本驱动动作生成提供了新范式。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [160] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D姿态估计的交通场景手势分类框架，利用WIVW数据集视频，提取76个静态与动态特征，实现四类手势（Stop、Go、Thank & Greet、No Gesture）识别，准确率达87%，强调手部位置与速度的关键判别作用。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）难以理解行人手势，而手势在交通中是非正式规则下人车交互的重要非语言沟通方式。

Method: 基于WIVW真实世界视频数据集，采用2D姿态估计提取归一化关键点，构建包含76个静态与动态特征的手势分类框架，对四类手势进行分类。

Result: 手部位置和运动速度是最具判别性的特征，整体分类准确率达到87%。

Conclusion: 该方法提升了AV系统的感知能力，并增进了对交通场景中行人行为的理解。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [161] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 本文研究光照变化引起的域偏移对多类别食品识别的影响，通过构建合成光照增强数据集并进行跨数据集迁移学习和域泛化实验，显著提升了模型在光照变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景（如自动传送带检测）中食品识别系统对光照变化引起的域偏移高度敏感，而现有研究多局限于单类食品或受控环境，且公开食品数据集普遍缺乏明确的光照标注。

Method: 在Food-101和Fruits-360两个数据集上开展跨数据集评估；构建系统变化色温与光照强度的合成光照增强数据集；开展跨数据集迁移学习与域泛化实验，重点关注苹果类等对光照敏感的类别。

Result: 实验证明光照不匹配导致显著精度下降；光照感知增强方法在保持实时性能的同时显著提升模型在域偏移下的识别鲁棒性。

Conclusion: 光照鲁棒性对实际食品识别系统至关重要，本文方法为真实检验场景中部署可靠系统提供了实用洞见。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [162] [Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?](https://arxiv.org/abs/2602.08505)
*Caterina Fuster-Barceló,Virginie Uhlmann*

Main category: cs.CV

TL;DR: 本文研究了视觉基础模型（VFMs）在电子显微镜（EM）图像线粒体分割任务中的跨数据集迁移能力，发现尽管VFMs在单个EM数据集上表现良好，但在多个异构EM数据集联合训练时性能显著下降；分析表明存在显著的领域不匹配问题，当前参数高效微调（如LoRA）策略不足以克服该问题，需引入额外的领域对齐机制。


<details>
  <summary>Details</summary>
Motivation: 探究视觉基础模型（VFMs）的潜在表征是否足够通用，以支持在异构显微镜图像数据集间有效迁移和重用，特别是在线粒体分割这一典型EM分析任务中。

Method: 在Lucchi++和VNC两个公开EM数据集上，评估DINOv2、DINOv3和OpenCLIP三种VFMs；比较冻结骨干网络+轻量分割头与基于LoRA的参数高效微调（PEFT）两种适配范式；通过PCA、Fréchet DINOv2距离和线性探针等方法分析潜在表征空间的领域差异。

Result: 单数据集训练下所有VFMs均取得良好分割性能（前景IoU），LoRA能持续提升域内性能；但多数据集联合训练导致所有模型性能严重下降，PEFT仅带来微弱增益；表征分析证实两EM数据集间存在显著且稳定的领域不匹配。

Conclusion: VFMs可在单EM领域下通过轻量适配实现有竞争力的分割效果，但当前PEFT策略尚不足以构建适用于多个异构EM数据集的鲁棒统一模型，亟需引入领域对齐机制。

Abstract: Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>


### [163] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出GeoFocus框架，通过Critical Local Perceptor模块增强局部几何结构感知，以及VertexLang语言提升全局拓扑建模效率，在多个几何推理数据集上显著提升准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解对大模型而言仍具挑战，需兼顾全局形状识别与局部几何关系理解，现有方法在关键局部特征覆盖和全局拓扑建模效率上存在不足。

Method: 提出两模块框架：1）Critical Local Perceptor，基于13种几何理论模板自动识别并强化关键局部结构（如角度、平行线、距离比较）；2）VertexLang，一种轻量拓扑形式语言，用顶点坐标与连接关系编码全局图形，替代冗长代码式表示。

Result: 在Geo3K、GeoQA、FormalGeo7K上较领先专用模型准确率提升4.7%，MATHVERSE中展现更强视觉鲁棒性；局部特征覆盖率提升61%，全局感知训练时间减少20%，拓扑识别精度提高。

Conclusion: GeoFocus有效协同局部细粒度感知与全局结构化建模，为多模态几何推理提供了高效、鲁棒的新范式。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [164] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 本文提出了一种基于双网格反馈控制的自动正则化参数选择方法，用于X射线断层成像中的图像重建。


<details>
  <summary>Details</summary>
Motivation: X射线断层成像中的图像重建是一个病态反问题，尤其在数据有限时，正则化至关重要，但其效果高度依赖于正则化参数的选择。

Method: 利用同一问题的两个不同计算离散化（双网格），通过反馈控制算法动态调整正则化强度，使迭代重建结果在两套网格上达到足够相似，并选取满足该条件的最小正则化参数。

Result: 该方法在真实断层数据上验证有效，能自动选取合适正则化参数，提升重建质量。

Conclusion: 双网格反馈控制策略为病态逆问题提供了一种鲁棒、自适应的正则化参数选择新范式。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [165] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 本文提出了一种适用于热成像的稀疏单目图优化SLAM系统，利用在可见光数据上预训练的SuperPoint和LightGlue特征提取与匹配方法，并通过预处理和置信度加权因子图提升其在低纹理、高噪声热图像上的鲁棒性，无需热图像微调或特定训练。


<details>
  <summary>Details</summary>
Motivation: 热成像在低光照、烟雾或恶劣天气等视觉退化环境中对视觉SLAM具有实用价值，但其低纹理、低对比度和高噪声特性使传统基于特征的SLAM难以适用。

Method: 采用SuperPoint检测器和LightGlue匹配器（在可见光大数据上预训练），设计热图像专用预处理流程，并修改SLAM核心模块以适应稀疏且易含外点的特征匹配；引入SuperPoint关键点置信度，构建置信度加权的因子图优化框架。

Result: 在公开热成像数据集上的实验表明，该系统在无需热图像专用训练或微调的情况下，仍能实现可靠SLAM性能。

Conclusion: 通用预训练特征可在热成像SLAM中有效迁移，结合针对性预处理与置信度加权优化，可克服热图像质量差带来的挑战，为跨域视觉SLAM提供新思路。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [166] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-Edit 是一种在线跨模态模型编辑方法，通过引入几何感知线索增强2D通用目标跟踪器，利用预训练视觉几何基础Transformer从少量2D图像中推断3D几何信息，并在保持语义判别能力的同时融合几何信息，显著提升遮挡和杂乱场景下的跟踪鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有通用目标跟踪（GOT）方法主要依赖2D特征，忽略3D几何线索，导致在遮挡、干扰物及外观/几何变化下性能下降；而人类跟踪依赖隐式3D先验与语义推理，因此需将3D几何信息融入2D跟踪框架。

Method: 提出 GOT-Edit，在线跨模态模型编辑方法：利用预训练的视觉几何基础Transformer提取几何特征；通过零空间约束的在线模型更新，将几何信息嵌入原有2D跟踪器，同时保留语义判别能力。

Result: 在多个GOT基准上实验表明，GOT-Edit在遮挡和杂乱场景下显著提升跟踪鲁棒性与精度，优于现有方法。

Conclusion: GOT-Edit 成功建立了融合2D语义与3D几何推理的新范式，验证了几何感知线索对通用目标跟踪的重要价值。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [167] [FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction](https://arxiv.org/abs/2602.08558)
*Guan Yuan Tan,Ngoc Tuan Vu,Arghya Pal,Sailaja Rajanala,Raphael Phan C. -W.,Mettu Srinivas,Chee-Ming Ting*

Main category: cs.CV

TL;DR: FLAG-4D 提出一种双变形网络（IDN+GMN）驱动4D高斯动态场景重建，融合光流运动先验与形变引导注意力机制，显著提升新视角合成的时序一致性与细节保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一MLP建模时序形变，难以从稀疏输入中稳定捕捉复杂点运动与细粒度动态细节。

Method: 提出FLAG-4D框架：1）双变形网络——瞬时形变网络（IDN）建模局部精细形变，全局运动网络（GMN）捕获长程动态，二者通过互学习协同优化；2）引入预训练光流骨干网提取稠密运动特征，并通过形变引导注意力机制将其对齐到每个演化的3D高斯上。

Result: 在多个动态场景数据集上，FLAG-4D在重建保真度、时序一致性与细节保留方面均优于当前最优方法。

Conclusion: 双变形建模与运动先验引导的注意力融合策略，是提升4D高斯动态重建质量的有效范式。

Abstract: We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>


### [168] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: 本文提出SemiNFT，一种基于扩散Transformer的参考式图像调色框架，通过两阶段训练（监督学习+强化学习）模拟人类艺术学习过程，在保持结构的同时实现语义感知的美学调色，显著优于现有方法，并支持零样本跨域调色任务。


<details>
  <summary>Details</summary>
Motivation: 现有参考式调色方法仅依赖像素级统计进行全局映射，缺乏对语义内容和人类审美的理解，难以满足非专业用户对真实感调色的需求。

Method: 提出SemiNFT框架：第一阶段使用配对三元组（源图、参考图、目标图）进行监督训练，学习结构保持与基础色彩映射；第二阶段在无配对数据上采用强化学习，结合在线-离线混合奖励机制（兼顾美学探索与结构一致性），防止灾难性遗忘。

Result: 在标准预设迁移基准上超越SOTA；成功实现零样本任务，如黑白照片上色与动漫到照片的跨域调色，验证其具备高级审美理解能力。

Conclusion: SemiNFT突破了传统统计匹配范式，通过类人两阶段训练实现了语义感知与美学驱动的光色重绘，为非专业用户提供高效、智能、可泛化的调色工具。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [169] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 本文综述了中国AVS工作组制定的首个点云压缩标准AVS PCC，从相关技术和性能对比两方面进行分析，并与MPEG的G-PCC和V-PCC标准进行比较。


<details>
  <summary>Details</summary>
Motivation: 点云数据量大，给传输和存储带来挑战，因此需要高效的压缩标准；AVS PCC作为中国自主制定的新一代标准，采用了不同于MPEG标准的新编码工具和技术，亟需系统性梳理与评估。

Method: 本文采用技术综述与性能对比分析相结合的方法，从编码技术组成和实际压缩性能两个维度对AVS PCC标准展开系统性回顾。

Result: 总结了AVS PCC所采用的关键新技术，并通过与G-PCC和V-PCC的对比，揭示了其在压缩效率、复杂度及适用场景等方面的异同与优势。

Conclusion: AVS PCC是一项具有中国特色且技术先进的点云压缩标准，其发展丰富了全球点云编码标准化体系，为点云在人眼感知与机器视觉应用中的高效部署提供了新选择。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [170] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: 本文提出Inspiration Seeds，一种无需文本提示、基于视觉输入的生成框架，用于支持设计初期的视觉探索与创意发想。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖精心设计的文本提示，难以支持设计师在创意初期通过松散视觉参考进行开放性探索的需求。

Method: 利用CLIP稀疏自编码器提取CLIP潜在空间中的编辑方向，构建纯视觉合成的三元组数据集，训练前馈式模型，以两个输入图像生成揭示其潜在关系的多样化、视觉连贯的合成图像。

Result: 模型能快速、直观地融合两幅输入图像，生成无文本干预、视觉一致且富有启发性的新构图，验证了纯视觉驱动生成对创意探索的有效性。

Conclusion: Inspiration Seeds将生成模型从执行工具转变为创意探针，拓展了生成式AI在早期、模糊性高的设计阶段的应用可能性。

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [171] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出LV-RAE，一种增强语义特征并补充低层信息（如颜色、纹理）的表示自编码器，以提升潜在扩散模型（LDMs）的重建保真度与生成质量；同时通过解码器微调和可控噪声注入缓解高维潜空间解码敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉基础模型作为图像编码器的潜在扩散模型虽提升了语义表征能力，但其语义特征缺乏低层细节（如颜色、纹理），导致重建保真度下降，成为LDM进一步扩展的主要瓶颈。

Method: 提出LV-RAE表示自编码器，融合语义与低层信息；分析发现高维丰富潜表示使解码器对扰动敏感，进而引入解码器微调与可控噪声注入策略以提升鲁棒性和生成质量。

Result: 实验表明LV-RAE显著提升重建保真度，同时保持语义抽象能力，并实现优异的生成质量。

Conclusion: LV-RAE有效弥合了语义表征与低层细节之间的鸿沟，在不牺牲语义对齐的前提下提升了LDM的重建与生成性能，为高保真生成建模提供了新思路。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [172] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 本文提出了一种针对Vision Transformer中[CLS]类令牌和图像块令牌的专门化处理路径，通过在归一化层和早期QKV投影中解耦两类令牌的计算流，显著提升了密集预测任务（如分割）的性能，同时保持分类精度。


<details>
  <summary>Details</summary>
Motivation: 标准ViT中[CLS]类令牌和图像块令牌被同等处理，但二者本质不同；作者发现标准归一化层隐式区分了它们，由此激发对显式、有针对性的令牌专业化处理的研究。

Method: 分析类令牌与图像块令牌在不同预训练策略下的交互；基于发现的归一化层隐式差异，设计在归一化层和早期QKV投影中对两类令牌进行选择性解耦的专用处理路径。

Result: 在语义分割任务上提升超2 mIoU；分类精度保持良好；参数仅增加8%，无额外计算开销。

Conclusion: 对类令牌和图像块令牌进行有针对性的架构专业化（尤其在归一化与QKV阶段）能有效提升ViT在密集预测任务中的表现，且具备跨模型规模和学习框架的良好泛化性。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [173] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文提出了一种基于低分辨率预扫描缩略图的深度学习模型，用于快速、准确地预测病理切片的固定类型（FFPE或FS），显著提升了高通量病理质控的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 手动标注固定类型易出错，影响诊断；现有方法依赖高分辨率全切片图像，难以扩展到高通量质控场景。

Method: 构建深度学习模型，仅使用低分辨率预扫描缩略图预测FFPE/FS固定类型；在TUM数据集上训练，在TCGA、Augsburg和Regensburg多中心多设备数据集上验证。

Result: 在TCGA上AUROC达0.88，较同类方法提升4.8%；在Augsburg和Regensburg上AUROC分别为0.72；单张切片处理仅需21ms，速度提升400倍。

Conclusion: 该方法无需高倍镜扫描即可高效识别标签错误，适用于大规模病理质控；未来将拓展至更多扫描仪类型，并探索其在其他低分辨率标注任务中的泛化能力。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [174] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow是一种基于WiFi信号的连续人体姿态估计框架，采用编码器-解码器结构，利用时空解耦卷积与轴向注意力机制，在自建数据集上达到高精度（PCK@20达97.00%）且模型轻量（仅4.82M参数）。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi-based姿态估计方法在处理连续运动和计算开销方面存在不足，需兼顾精度、实时性与部署可行性。

Method: 提出WiFlow框架：编码器采用时序与非对称卷积提取CSI的时空特征，并通过轴向注意力建模关节点结构依赖；解码器将高维特征映射为关节点坐标。

Result: 在自建36万样本数据集上，PCK@20达97.00%，PCK@50达99.48%，平均关节位置误差为0.008m，参数量仅4.82M。

Conclusion: WiFlow在保持高精度的同时显著降低模型复杂度与计算成本，为实用化WiFi姿态估计建立了新基准。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [175] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: 本文探讨了将机器学习（特别是数据驱动离散化方法）应用于求解偏微分方程（PDEs），尤其是浅水方程和欧拉方程；实现了经典数值求解器并对比PyClaw，提出四种深度神经网络结构，其中两种表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在图像、NLP等领域成功，但在含数学约束（如PDE）的任务中应用尚不成熟；需提升低分辨率下PDE求解的精度、稳定性，并融合经典数值方法（如守恒性）优势。

Method: 采用数据驱动离散化方法，在结构化网格上预测拟线性差分模板系数；实现基于经典数值框架（有限体积/差分）的浅水与欧拉方程求解器；设计并对比四种深度神经网络架构用于ML-based PDE求解。

Result: 自研经典求解器性能优于PyClaw；所提四种神经网络中，两种能输出满意PDE解，验证了数据驱动离散化的有效性。

Conclusion: 数据驱动离散化是融合机器学习与传统数值PDE求解的可行路径，兼顾精度、稳定性与物理约束（如守恒律），为低分辨率高效仿真提供新思路。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [176] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了ALIVE模型，通过改进MMDiT架构并设计专用数据流程，将预训练文本到视频（T2V）模型扩展为支持文本/参考驱动的音视频生成（T2VA/动画），实现了高质量音视频同步生成，并在新基准上达到或超越商用SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）模型缺乏对音频模态的支持，难以实现音视频同步生成与参考驱动动画；需统一、高效、开源的音视频生成框架以推动社区发展。

Method: 1）基于MMDiT架构，新增联合音视频分支，引入TA-CrossAttn实现时序对齐跨模态融合，及UniTemp-RoPE实现精准音视频对齐；2）构建包含音视频标注、质量控制等环节的高质量微调数据流水线；3）设计新基准用于全面评测；4）在百万级高质量数据上进行持续预训练和微调。

Result: ALIVE在新基准测试中显著优于主流开源模型，并达到或超越当前最先进的商用解决方案（如Sora风格）性能；支持T2VA与Reference-to-Video&Audio两种生成模式。

Conclusion: ALIVE成功将T2V模型拓展为统一音视频生成框架，兼顾性能、可控性与开源可复现性，为音视频生成研究提供了实用范式与基础设施支持。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [177] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: 本文提出OneVision-Encoder（OV-Encoder），主张视觉理解应遵循信息论中的编解码器（Codec）原理，通过聚焦高熵区域实现高效压缩与语义编码，显著提升视频和多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 现代视觉模型在密集像素网格上均匀计算，浪费大量算力于静态背景，忽视视频中稀疏但关键的预测残差（即运动与语义信息），违背了'AGI本质是压缩问题'及'架构需与数据结构共振'的基本原则。

Method: OV-Encoder采用Codec Patchification策略，仅处理3.1%-25%高信号熵区域；引入共享3D RoPE适配不规则时空token布局；并以百万级语义概念上的大规模聚类判别目标进行训练，联合建模物体恒常性与运动动力学。

Result: OV-Encoder在16个图像、视频与文档理解基准上一致超越Qwen3-ViT、SigLIP2等强基线，尤其在视频理解任务上平均提升4.1%，且使用更少视觉token与预训练数据；验证了效率与精度正相关。

Conclusion: 基于Codec对齐的patch级稀疏性是构建可扩展视觉通用模型的基础原则，OV-Encoder为此提供了可行架构范式。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [178] [Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm](https://arxiv.org/abs/2602.08699)
*Xiaogang Xu,Kun Zhou,Tao Hu,Jiafei Wu,Ruixing Wang,Hao Peng,Bei Yu*

Main category: cs.CV

TL;DR: 本文提出了一种面向低光照视频增强（LLVE）的视图感知分解框架VLLVE及其改进版VLLVE++，通过分离视图无关（外观）与视图相关（光照/阴影）成分，并引入跨帧一致性约束与双结构增强网络，显著提升了增强效果与鲁棒性，尤其在真实场景和高动态视频中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有低光照视频增强方法难以同时建模场景固有外观与动态光照变化，导致增强结果不一致、噪声残留或细节丢失；需一种能解耦并协同优化两类成分的结构化分解策略。

Method: 提出视图感知的视频分解框架VLLVE：1）将视频分解为视图无关（基于动态跨帧对应建模内在外观）与视图相关（施加场景级连续性约束建模阴影）两部分；2）设计双结构增强网络，通过多帧联合监督实现分解特征对齐；3）扩展为VLLVE++，引入加性残差项模拟场景自适应退化，并支持增强与退化感知对应关系 refinement 的双向端到端学习。

Result: 在主流LLVE基准上取得SOTA性能，尤其在真实世界视频和高动态场景中显著优于现有方法；VLLVE++参数开销小、兼容单帧编码器-解码器结构，且能有效提升可靠光流/对应点数量、抑制错误匹配。

Conclusion: 视图感知分解范式（VLLVE/VLLVE++）为低光照视频增强提供了更可解释、更鲁棒的建模范式，验证了显式解耦外观与光照成分并协同优化的有效性，为后续研究提供了新思路。

Abstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>


### [179] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 本文提出了Omni Dense Captioning任务，旨在生成连续、细粒度、结构化的音视频叙述，并引入六维结构模式、新基准OmniDCBench、评估指标SodaM及基线模型TimeChat-Captioner-7B，显著提升下游音视频推理与时间定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有音视频描述方法缺乏时间连续性、语义密度和结构化表达，难以支持高保真场景重建与下游任务需求。

Method: 提出六维结构化描述模式；构建人工标注基准OmniDCBench与训练数据TimeChatCap-42K；设计时间感知评估指标SodaM；开发基于SFT与GRPO训练的多模态大模型TimeChat-Captioner-7B。

Result: TimeChat-Captioner-7B在OmniDCBench上超越Gemini-2.5-Pro，其生成的密集描述显著提升DailyOmni、WorldSense和Charades-STA等下游任务性能。

Conclusion: Omni Dense Captioning为音视频理解提供了更精细、可时序定位、结构化的新范式，推动多模态叙事与时空推理发展。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [180] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 本文首次对视觉-语言模型（VLMs）在多模态微调过程中的表征变化进行机制性分析，提出‘阶段式模型差分’方法，揭示语言模型如何学习‘看见’：识别出微调中涌现或重定向的视觉偏好特征，发现其中部分特征稳定编码空间关系，并定位到少量注意力头驱动该能力；从而阐明视觉接地如何重塑原有文本特征、促进模态融合。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型性能优异，但其语言主干在多模态训练中如何适应、以及视觉特有能力何时出现尚不清楚，亟需机制性理解。

Method: 采用阶段式模型差分（stage-wise model diffing）技术，逐阶段对比微调前后表征变化；结合空间提示控制实验与因果特征归因，识别视觉偏好特征、验证其空间编码能力，并追溯至特定注意力头。

Result: 发现微调过程中涌现出视觉偏好特征；其中一部分稳定编码空间关系；这些能力可被归因于少量关键注意力头；该方法能精确定位空间化多模态特征的产生时机与位置。

Conclusion: 阶段式模型差分是一种有效揭示VLM内部机制的工具，阐明了视觉接地如何改造语言模型原有表征，推动模态融合，提升了多模态训练的可解释性，并为优化语言模型的视觉能力提供基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [181] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文提出三种无需训练的零样本方法，用于在CT和MR图像中自动识别解剖区域，其中基于分割的规则系统表现最佳，F1分数达0.947（CT）和0.914（MR）。


<details>
  <summary>Details</summary>
Motivation: 现有解剖区域识别方法严重依赖不可靠的DICOM元数据，且多为监督学习，泛化能力差，难以适应真实临床场景。

Method: 提出三种零样本、训练-free的pipeline：(1) 基于预训练多器官分割模型的规则驱动分割系统；(2) 由放射科医生定义规则引导的多模态大语言模型（MLLM）；(3) 结合视觉输入与显式解剖证据的分割感知MLLM。

Result: 在887例异质性CT/MR扫描上评估，分割驱动规则法性能最强（CT: 0.947, MR: 0.914加权F1），MLLM在视觉显著区域表现良好，分割感知MLLM暴露基础局限。

Conclusion: 零样本解剖区域识别可行，且基于分割的规则方法稳健、高效，优于当前依赖元数据或监督学习的方案；MLLM潜力有限，需更深入融合解剖先验。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [182] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit是一种无需训练的文本引导图像编辑框架，通过语义差异自动识别编辑与保留区域，利用距离感知潜在融合和总变差损失生成软掩码以减少边界伪影，并在DiT注意力层中采用AdaIN调制实现统计注意力融合，从而提升编辑精度与可控性，同时保持源图像全局一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式二值掩码约束编辑，但硬掩码边界易引入伪影并降低可编辑性。

Method: 1) 基于源提示与目标提示的语义差异自动识别编辑与保留区域；2) 距离感知潜在融合 + 总变差损失生成软而准确的掩码；3) 在DiT注意力层中引入AdaIN调制，实现编辑区域内的统计注意力融合。

Result: 在多项实验中显著优于当前最先进方法。

Conclusion: FusionEdit实现了高精度、强可控、无训练的文本引导图像编辑，有效缓解边界伪影并提升编辑质量与一致性。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [183] [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](https://arxiv.org/abs/2602.08726)
*Khadija Iddrisu,Waseem Shariff,Suzanne Little,Noel OConnor*

Main category: cs.CV

TL;DR: 本文提出了一种基于Blender生成的合成事件数据集SynSacc，用于眼动（扫视和注视）分类，并利用脉冲神经网络（SNNs）进行建模，在真实事件数据上微调后达到最高0.83准确率，同时展现出良好的时序鲁棒性和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机存在运动模糊、时间分辨率低等问题，难以准确捕捉快速眼动；而事件相机虽具优势，但真实标注事件数据稀缺，亟需高质量合成数据支持眼动分析研究。

Method: 使用Blender构建可控的合成事件数据集SynSacc，模拟扫视与注视；采用两类脉冲神经网络（SNNs）架构进行训练与真实事件数据微调；对比评估其在不同时间分辨率下的分类性能及相对于人工神经网络（ANNs）的计算效率。

Result: 模型在眼动分类任务中最高达0.83准确率，且在不同时间分辨率下表现稳定；SNNs处理合成事件流比ANNs显著更高效；验证了合成数据增强对事件视觉研究的有效性。

Conclusion: 合成事件数据集结合SNNs为眼动识别提供了高精度、高效率、鲁棒性强的新范式，推动了事件相机在认知科学与低功耗视觉感知中的应用。

Abstract: The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>


### [184] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 本文提出了一种结合2D和3D深度学习模型的混合框架，用于减少欠采样CT图像中的伪影，兼顾计算效率与三维一致性。


<details>
  <summary>Details</summary>
Motivation: 欠采样CT扫描可缩短采集时间和降低辐射剂量，但会引入伪影，影响图像质量和诊断价值，亟需高效去伪影方法。

Method: 采用两阶段混合网络：先用2D U-Net处理单层CT图像提取特征图，再将这些特征图沿体素方向堆叠并输入3D解码器，利用跨层上下文信息重建无伪影的3D CT体积。

Result: 在冠状面和矢状面的层间一致性上显著提升，同时保持较低计算开销。

Conclusion: 该混合框架是一种鲁棒、高效的3D CT图像后处理方案，适用于临床实用化部署。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [185] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: 本文提出CLIP-Guided Alignment (CGA)框架，通过建模和缓解类间混淆（尤其是不对称、动态的细粒度混淆），提升无源域自适应（SFDA）性能。方法包括：检测混淆对（MCA）、构建混淆感知文本提示（MCC）及混淆引导的特征对齐（FAM）。实验表明其在易混淆和细粒度场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法忽视了源模型在目标域中产生的不对称、动态的类间混淆问题，导致伪标签噪声大、判别能力差，尤其在细粒度任务中表现不佳。

Method: 提出CGA框架，包含三部分：(1) MCA模块检测源模型在目标域上的单向混淆类别对；(2) MCC模块利用CLIP生成混淆感知的文本提示（如‘像公交车的卡车’）以提升伪标签质量；(3) FAM模块构建混淆引导的特征库，并通过对比学习对齐CLIP与源模型的特征空间。

Result: 在多个数据集上CGA持续超越当前最优SFDA方法，尤其在易混淆和细粒度场景中提升显著；验证了显式建模类间混淆对SFDA有效性的重要作用。

Conclusion: 显式建模并缓解不对称、动态的类间混淆是提升无源域自适应性能的关键，CGA通过CLIP引导的多级对齐策略有效解决了该问题。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [186] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: 本文提出HATCH框架，通过补丁级空间对齐和动作-再回答推理两个目标，显式建模跨视角对应与视角变换，显著提升多图像空间推理能力，同时保持单图像推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多图像空间推理上表现不佳，而人类依赖跨视角对应和逐步视角变换两种机制；当前方法仅部分、隐式地利用这些机制，缺乏显式监督。

Method: 提出HATCH训练框架，包含两个互补目标：(1) 补丁级空间对齐，促使不同视角下空间对应区域的特征表示对齐；(2) 动作-再回答推理，要求模型先生成显式的视角变换动作，再预测答案。

Result: 在三个基准上，HATCH持续显著优于同规模基线，并媲美更大规模模型，同时不损害单图像推理能力。

Conclusion: 显式建模人类认知机制（跨视角对应与视角变换）可有效提升MLLMs的多图像空间推理能力，HATCH为该方向提供了可扩展且有效的训练范式。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [187] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 本文提出Instance-Disentangled Attention机制，解决现有流匹配模型在多实例图像编辑中语义纠缠的问题，实现单次前向传播下的实例级独立编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的编辑器主要支持全局或单指令编辑，在需对参考图像多个部分进行独立编辑的多实例场景下表现不佳，根源在于全局条件化速度场和联合注意力机制导致并发编辑相互干扰。

Method: 提出Instance-Disentangled Attention机制，将联合注意力操作按实例划分，强制文本指令与空间区域在速度场估计过程中一一绑定。

Result: 在自然图像编辑及新构建的图文密集型信息图区域级编辑基准上验证有效，显著提升编辑解耦性与局部性，同时保持全局输出一致性，支持单次前向传播的实例级编辑。

Conclusion: Instance-Disentangled Attention有效缓解了多实例编辑中的语义纠缠问题，为流匹配模型在细粒度可控图像编辑任务中提供了新思路与实用方案。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [188] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: 本文提出MVAnimate框架，通过融合多视角先验信息来同时建模2D和3D人体姿态，提升动画视频生成的质量、时序一致性和空间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D或3D姿态建模的动画生成算法存在输出质量低、训练数据不足等问题，难以生成高质量动画视频。

Method: MVAnimate框架利用多视角先验信息，协同合成动态人物的2D与3D信息，优化多视角视频，提升时空一致性与各视角视频质量。

Result: 在多个数据集上的实验表明，该方法在不同运动模式和外观下均表现出强鲁棒性，显著优于现有动画生成方法。

Conclusion: MVAnimate有效缓解了数据稀缺与生成质量之间的矛盾，为高质量、多视角一致的字符动画生成提供了新范式。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [189] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 本文提出了一种名为Symbolic Vedic Computation的轻量级、CPU友好的说话头生成（THG）框架，用于教育技术中的离线或资源受限场景，通过符号化语音-口型映射与Vedic sutra启发的协同发音建模，实现低延迟、高同步精度的实时唇形动画合成。


<details>
  <summary>Details</summary>
Motivation: 现有说话头生成方法多依赖GPU、大数据集或大模型，难以在离线或低资源教育环境中部署，亟需一种确定性、轻量、纯CPU可运行的替代方案。

Method: 提出Symbolic Vedic Computation框架：将语音转为时间对齐音素流，映射至紧凑可视音素（viseme）集，并基于Vedic sutra Urdhva Tiryakbhyam设计符号化协同发音模型生成平滑viseme轨迹；再通过轻量2D渲染器进行ROI形变、嘴部合成与稳定化处理。

Result: 在纯CPU环境下实现了高唇音同步精度、良好时间稳定性与身份一致性，计算负载和延迟显著低于现有CPU可行基线，支持低端硬件实时运行。

Conclusion: 该方法验证了无需深度学习与GPU即可实现教育级可用的 talking-head 合成，为资源受限教育场景提供了切实可行的技术路径。

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [190] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 本文提出了一种结合高分辨率图像与受力测量的多模态框架（MultiDeepSAD），用于更准确、鲁棒地检测受电弓-接触网界面的电弧事件，并通过真实与合成数据集及伪异常增强技术提升模型在数据稀缺和域偏移下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 电弧事件具有瞬态性、环境噪声大、真实数据稀缺且难以与其它瞬态现象区分，导致其检测困难，威胁铁路供电系统可靠性。

Method: 构建两个同步视觉-力信号数据集（来自瑞士联邦铁路SBB的真实数据 + 公开视频+合成力信号）；提出多模态异常检测模型MultiDeepSAD（DeepSAD扩展）并设计新型多模态损失函数；引入针对图像（合成电弧伪影）和力信号（模拟异常波动）的专用伪异常生成技术以增强训练。

Result: 实验与消融研究表明，该方法显著优于基线方法，在域偏移和真实电弧样本稀少条件下仍保持高敏感性。

Conclusion: 所提多模态框架及伪异常增强策略有效提升了电弧事件检测的准确性、鲁棒性与泛化能力，为实际铁路运维提供了可行的技术支持。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [191] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: 本文介绍了MOVA，一个开源的32B参数Mixture-of-Experts模型，支持图像-文本到音视频联合生成，具备高质量唇形同步语音、环境感知音效和内容对齐音乐生成能力，并开源模型权重与代码以推动社区发展。


<details>
  <summary>Details</summary>
Motivation: 现有音视频生成方法多采用级联流程，导致成本高、误差累积、质量下降；同时，主流闭源系统限制了领域进展，亟需开源、高质量、端到端的联合音视频生成模型。

Method: 提出MOVA模型，采用32B参数的MoE架构（推理时激活18B），支持IT2VA任务；开源模型权重、代码，并提供高效推理、LoRA微调和提示增强等工具。

Result: MOVA可生成高质量、同步的音视频内容，包括真实唇形同步语音、环境感知音效和内容对齐音乐；代码与权重已开源，支持社区研究与创作。

Conclusion: MOVA为首个开源的大规模音视频联合生成模型，通过MoE架构与端到端训练突破级联范式局限，显著提升生成质量与可控性，并有望成为音视频生成研究与应用的新基线。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [192] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种半监督的师生框架，通过不确定性感知的伪标签教师模型和基于置信度的渐进式课程学习学生模型，提升MRI脑肿瘤分割性能，尤其在标注数据稀缺时展现出优异的数据效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤MRI分割受限于昂贵的标注成本和跨设备、跨中心的数据异质性。

Method: 设计了一个半监督师生框架：教师模型生成概率掩码和像素级不确定性；未标注图像按图像级置信度排序并分阶段引入；学生模型采用双损失目标，学习高置信区域、遗忘低置信区域；并通过一致性优化进一步提升伪标签质量。

Result: 在BraTS 2021上，仅用10%标注数据时验证集DSC达0.393，增至100%时达0.872；教师模型DSC为0.922，学生模型在NCR/NET（0.797）和Edema（0.980）子区域超越教师，并成功恢复教师失败的Enhancing类（DSC 0.620）。

Conclusion: 基于置信度的课程学习与选择性遗忘策略，能有效应对弱监督和噪声伪标签挑战，显著提升分割鲁棒性与数据效率。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [193] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2 是一个将多模态大语言模型（MLLM）与视频扩散模型结合的高效视频生成与编辑框架，通过MLLM生成目标描述并用轻量适配器注入条件信息，实现高质量、复杂指令驱动的视频编辑和生成。


<details>
  <summary>Details</summary>
Motivation: 提升视频生成与编辑对复杂、组合式用户指令的理解与执行能力，克服现有方法在语义理解与生成引导上的割裂问题。

Method: 利用预训练MLLM解析用户指令并生成显式目标字幕，再通过轻量级适配器将多模态条件令牌注入预训练文本到视频扩散模型，以参数高效方式复用其生成先验。

Result: 在FiVE（细粒度视频编辑）和VBench（文本到视频生成）基准上均取得优越性能，支持物体增删、背景替换、复杂运动编辑等多种任务，并成功扩展至14B参数规模的高质量视频扩散模型。

Conclusion: Omni-Video 2 证明了融合理解型模型与生成型模型的协同范式可显著提升视频生成与编辑的可控性、组合性与质量，为统一视频创作系统提供了可扩展、高效率的技术路径。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [194] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比视觉表征学习和视觉-语言对齐（VLA）的统一基础模型，用于鼻咽癌（NPC）放疗所需的任意MRI模态间合成，显著提升合成质量、鲁棒性及下游放疗任务性能。


<details>
  <summary>Details</summary>
Motivation: 临床中NPC放疗所需的MRI常因患者不适、扫描时间长、成本高等原因不完整，影响放疗计划精度；传统MRI合成方法模态特异、解剖适应性差、缺乏临床可解释性。

Method: 构建融合对比视觉表征学习与CLIP驱动的文本引导解码器的统一基础模型：对比编码器提取模态无关表征，文本增强解码器实现语义一致的任意到任意MRI合成；在13家机构40,825张图像上训练。

Result: 在26个内外部验证点（15,748张图像）上平均SSIM达0.90、PSNR达27，合成保真度高，抗噪声与域偏移能力强；统一表征亦提升分割等下游放疗相关任务性能。

Conclusion: 该统一基础模型弥合了MRI合成技术与NPC临床放疗需求之间的鸿沟，推动数字医疗在鼻咽癌精准治疗中的落地应用。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [195] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出VideoVeritas框架，结合细粒度感知与基于事实的推理，并引入联合偏好对齐与感知前置强化学习（PPRL）方法，在轻量高质量数据集MintVid上验证其在视频伪造检测中更均衡、鲁棒的性能。


<details>
  <summary>Details</summary>
Motivation: 视频生成能力增强带来日益严峻的安全风险，而现有多模态大模型（MLLMs）虽推理能力强，但细粒度感知能力不足，亟需提升检测可靠性。

Method: 提出VideoVeritas框架，包含Joint Preference Alignment和Perception Pretext Reinforcement Learning（PPRL）；在强化学习阶段采用时空定位与自监督物体计数等感知前置任务，而非直接优化检测目标。

Result: 在新构建的MintVid数据集（含3K视频、覆盖9种先进生成器及含事实错误的真实世界子集）上，VideoVeritas相较现有方法在多种基准上展现出更均衡、鲁棒的检测性能，避免了偏重表层推理或机械分析的缺陷。

Conclusion: 细粒度感知与事实推理的协同建模可显著提升视频伪造检测的可靠性；通过感知前置任务引导的强化学习策略，能有效弥补MLLMs在底层视觉理解上的短板。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [196] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

TL;DR: FlattenGPT 提出一种新颖的深度压缩方法，通过‘扁平化’相邻Transformer块来减少模型深度、提升参数冗余检测效率，同时保留所有块中的知识并维持原始架构一致性；实验表明其在零样本准确率和WikiText-2困惑度上均优于现有剪枝方法，并显著加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有整块剪枝易丢弃重要特征导致性能大幅下降，而通道剪枝虽保性能但无法减深度且各层剪枝比不一致；亟需兼顾深度压缩、性能保持与架构一致性的新方法。

Method: 提出FlattenGPT：将相邻两个Transformer块融合为一个新块（flatting），通过结构重参数化实现深度压缩与细粒度冗余识别；全程不丢弃任何原始块，保持架构兼容性。

Result: 在LLaMA-2/3和Qwen-1.5上以20%压缩比保持90–96%零样本性能；零样本准确率与WikiText-2困惑度均超越现有剪枝方法；显著提升LLM推理速度。

Conclusion: FlattenGPT是一种高效、鲁棒且架构友好的深度压缩范式，在模型效率与性能间取得更优平衡，为Transformer模型压缩与加速提供了新思路。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [197] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出TiFRe框架，通过文本引导的帧采样（TFS）和帧匹配融合（FMM）机制，在减少输入视频帧数的同时保留关键语义信息，从而降低计算开销并提升视频-语言任务性能。


<details>
  <summary>Details</summary>
Motivation: Video MLLMs因处理大量视频帧导致计算成本高；固定帧率采样易丢失非关键帧中的重要信息，造成性能下降。

Method: 提出TiFRe框架：1）Text-guided Frame Sampling（TFS）——利用LLM将用户输入转化为CLIP风格提示，结合CLIP编码器计算帧级语义相似度以选择关键帧；2）Frame Matching and Merging（FMM）——将非关键帧信息融合进关键帧，减少信息损失。

Result: 实验表明TiFRe在显著降低计算成本的同时，提升了视频理解与问答等视频-语言任务的性能。

Conclusion: TiFRe是一种高效且语义感知的视频帧压缩方法，兼顾效率与效果，为轻量化Video MLLMs提供了新思路。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [198] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 本文研究了3D高斯泼溅（3DGS）在标准多视角优化下形成的Rendering-Optimal References（RORs）结构，发现其具有混合尺度与双峰辐射等稳定统计模式；进一步通过无渲染监督的预测实验揭示密度分层现象：稠密区参数几何相关、可预测，稀疏区则因可见性异质性导致几何与外观参数强耦合、难以预测；据此提出密度感知训练策略与架构设计建议。


<details>
  <summary>Details</summary>
Motivation: 理解3D高斯泼溅在多视角优化中自发形成的结构规律（RORs），及其可学习性边界，以指导更鲁棒、高效的神经渲染系统设计。

Method: 1) 统计分析标准3DGS优化所得RORs的尺度与辐射分布；2) 设计无渲染监督的learnability probes，用点云预测RORs参数；3) 通过方差分解量化稀疏区域中几何与外观参数的协方差主导耦合；4) 提出密度感知训练策略。

Result: 发现RORs呈现混合结构尺度和双模态辐射；稠密区域参数几何相关、可被点云准确预测；稀疏区域因可见性异质性导致参数强耦合、预测普遍失败；RORs兼具几何基元与视图合成基元双重角色。

Conclusion: RORs的结构本质是密度分层的：稠密区依赖点云几何即可建模，稀疏区必须依赖多视角渲染约束；该发现为设计兼顾前馈预测与渲染精调的自适应神经渲染架构提供了理论依据与实践路径。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [199] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯流场的植物生长建模方法，通过时间变化的高斯参数导数来表示非线性、连续时间的生长动态，并利用反向生长过程初始化高斯原语，在多视角延时数据集上实现了更优的图像质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 植物在生长过程中不断生成新几何结构（如分枝、分化），而现有动态建模方法（如形变场、4D高斯溅射）无法有效处理此类拓扑变化和非线性连续增长。

Method: 提出3D高斯流场表示法，将植物生长建模为高斯参数（位置、尺度、方向、颜色、不透明度）的时间导数；通过重建成熟植株并学习反向生长过程来初始化足够数量的高斯原语。

Result: 在多视角植物生长延时数据集上，相比先前方法，图像质量和几何精度均有显著提升。

Conclusion: 该方法为生长型3D结构的外观建模提供了新范式，克服了传统动态建模技术在拓扑演化与非线性时变建模上的局限。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [200] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter 是一种基于视频扩散模型的框架，联合重建4D几何与估计稠密运动，提出新联合表征与4D VAE，并通过改进数据归一化与VAE训练策略提升性能，在几何与运动重建上分别提升38.64%和25.0%。


<details>
  <summary>Details</summary>
Motivation: 现有方法强制将3D值和潜在表示与RGB VAE潜在空间对齐，但二者分布本质不同，导致性能次优；需一种更合理的表征与学习方式。

Method: 提出稠密3D点图与3D场景流在共享坐标系下的联合表征，设计新型4D VAE；采用新数据归一化与VAE训练策略，避免强制对齐RGB VAE潜在空间。

Result: 在多个数据集上达到几何重建与稠密场景流估计的SOTA性能，几何重建提升38.64%，运动重建提升25.0%，且无需后优化。

Conclusion: MotionCrafter 证明了脱离RGB VAE潜在空间约束、采用适配3D/4D特性的表征与训练策略可显著提升单目视频的4D重建质量。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [201] [Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study](https://arxiv.org/abs/2602.08996)
*Arushi Rai,Adriana Kovashka*

Main category: cs.CV

TL;DR: 本文提出了一种利用辅助网络数据（如攀岩比赛视频和教练手册）结合源域体育反馈数据，提升目标域体育反馈生成性能的方法，并设计了‘特异性’和‘可操作性’两个新评估指标，以解决现有视频-LLMs在体育反馈生成中泛化差、评估不适用的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频-LLM在体育反馈生成任务上泛化能力差，尤其对未见过的运动表现不佳，且依赖昂贵难获取的领域标注数据；同时传统文本生成评估指标无法准确衡量体育反馈质量。

Method: 以攀岩为案例，引入目标域免费网络数据（如比赛视频、教练手册）与源域体育反馈数据联合训练；提出两个新评估指标：特异性（specificity）和可操作性（actionability）。

Result: 所提方法在有限标注下显著提升了体育反馈生成的性能与实用性，新指标更贴合体育反馈的质量需求。

Conclusion: 结合跨域数据与领域辅助资源可有效缓解体育反馈生成中的数据稀缺与泛化瓶颈，定制化评估指标对推动该方向发展至关重要。

Abstract: While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.

</details>


### [202] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出ArcFlow，一种基于非线性流轨迹的少步蒸馏框架，通过混合连续动量过程建模速度场，实现高精度、解析可积的教师轨迹逼近，在仅微调<5%参数下达成40倍加速（2 NFE）且质量无显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法依赖线性捷径近似教师轨迹，难以匹配其随时间动态变化的速度方向，导致生成质量下降。

Method: ArcFlow将推理轨迹下的速度场参数化为连续动量过程的混合，支持解析积分以避免数值离散误差；通过轻量适配器在预训练教师模型上进行轨迹蒸馏训练。

Result: 在Qwen-Image-20B和FLUX.1-dev等大模型上，ArcFlow仅微调不到5%参数，实现2次函数求值（NFE）下的40倍推理加速，生成质量无显著下降，并在多个基准测试中验证了有效性。

Conclusion: ArcFlow通过显式建模非线性、连续、可解析积分的速度轨迹，显著提升了少步蒸馏的精度与效率，为高效扩散生成提供了新范式。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [203] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文提出Raster2Seq方法，将楼层平面图重建任务建模为序列到序列问题，利用带可学习锚点的自回归解码器生成带语义标签的多边形顶点序列，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确重建结构复杂、房间多、多边形角点数量变化大的楼层平面图的结构与语义信息。

Method: 提出Raster2Seq框架，将楼层平面图元素（如房间、门窗）表示为带标签的多边形顶点序列；设计基于图像特征和已生成顶点的自回归解码器，并引入可学习的空间锚点以引导注意力机制聚焦关键图像区域。

Result: 在Structure3D、CubiCasa5K和Raster2Graph等标准基准上达到SOTA性能，并在更具挑战性的WAFFLE数据集上展现出强泛化能力。

Conclusion: Raster2Seq通过序列化建模和锚点引导的自回归生成，有效提升了复杂楼层平面图的结构化矢量化重建精度与灵活性。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [204] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出了WorldCompass，一种面向长时序、交互式视频世界模型的强化学习后训练框架，通过三项创新提升模型对交互信号的准确、一致探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在长时序、交互式场景下探索不准确、不一致，缺乏有效利用交互信号进行后训练的方法。

Method: 提出WorldCompass框架，包含：1）片段级rollout策略，提升效率与细粒度奖励；2）互补奖励函数，兼顾交互准确性与视觉质量；3）负感知微调策略与多种效率优化的高效RL算法。

Result: 在SoTA开源世界模型WorldPlay上验证，WorldCompass显著提升了交互准确性和视觉保真度。

Conclusion: WorldCompass为交互式视频世界模型提供了高效、鲁棒的RL后训练范式，推动其在复杂动态环境中的实用化。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [205] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 本文挑战了视觉生成中连续流水线的主导地位，发现离散与连续方法的性能差距主要源于潜在空间中的总比特数（压缩比），并提出一种可扩展的掩码比特自回归建模（BAR）框架，在ImageNet-256上达到0.99 gFID的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为离散tokenizers本质上劣于连续方法，本文旨在验证该假设并探索缩小二者性能差距的有效途径。

Method: 提出掩码比特自回归建模（BAR）框架：将离散token分解为比特序列，通过带掩码的比特预测头驱动自回归Transformer逐位生成token，支持任意大小的码本。

Result: BAR在ImageNet-256上取得0.99的gFID，超越所有连续与离散前沿方法；采样成本显著降低，训练收敛更快。

Conclusion: 离散视觉生成潜力被低估，关键在于合理分配比特资源与建模方式；BAR证明大规模码本可通过比特级建模高效利用，为离散生成提供新范式。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [206] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 本文探讨了在多模态语言模型（如DualGPT）中引入文本分词器是否削弱了像素级语言建模绕过子词分词瓶颈的初衷，特别是在印尼四种使用非拉丁文字的低资源语言（爪哇语、巴厘语、巽他语、楠榜语）上，发现即使采用视觉渲染，文本分词器仍会引发脚本-分词器错配问题；定制分词器显著优于Llama 2通用分词器（chrF++提升达30.15），表明分词器仍是构建公平多语言模型的关键障碍。


<details>
  <summary>Details</summary>
Motivation: 探究视觉渲染是否真能解除模型对文本分词器的依赖，尤其在使用非拉丁文字的低资源语言中，验证像素级建模是否仍受分词器错配影响。

Method: 在DualGPT架构下，针对四种印尼本地非拉丁文字语言（Javanese、Balinese、Sundanese、Lampungnese），对比评估通用分词器（Llama 2）与定制分词器在OOV率、fertility及chrF++等指标上的表现。

Result: 尽管视觉渲染存在，引入文本分词器仍导致脚本-分词器错配；Llama 2分词器虽OOV和fertility更低，但chrF++显著差于定制分词器，最高提升达30.15。

Conclusion: 文本分词器仍是多模态语言建模中不可忽视的瓶颈，尤其在非拉丁文字场景下；未来多模态模型需谨慎设计或弃用通用分词器，以实现真正去token化的公平建模。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [207] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

TL;DR: BiomechAgent 是一个面向临床用户的、能自动生成代码的AI代理，支持通过自然语言完成生物力学数据分析（如数据库查询、可视化、活动分类、时序分割和临床推理），显著降低无编程背景用户的使用门槛。


<details>
  <summary>Details</summary>
Motivation: 标记物无关的动作捕捉技术虽已普及，但其产生的数据难以被缺乏编程能力的临床医生有效分析，亟需一种低门槛、高可用的分析工具。

Method: 提出 BiomechAgent —— 一个结合领域知识提示（biomechanically-informed instructions）、集成经验证的专业工具（如步态事件检测）并支持自然语言交互的代码生成AI代理；构建涵盖五大任务的系统性基准进行评估，并对比本地开源模型与云端大模型的表现。

Result: 在数据检索与可视化任务中表现稳健，在临床推理方面初具能力；领域定制化提示和专用工具集成显著提升性能，尤其在复杂时空分析任务中；本地开源模型仅在数据库检索任务中接近云端模型表现，其余任务性能明显下降。

Conclusion: BiomechAgent 有效 bridged the gap between markerless motion capture data and clinical end-users，通过自然语言驱动的代码生成大幅提升了生物力学分析的可访问性与实用性。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [208] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ILA-agent的推理时语言习得框架，使大语言模型（LLM）能在低资源条件下通过与文档和执行环境交互，动态掌握陌生编程语言（如Cangjie），显著优于检索增强基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在面对预训练中未见过的编程语言时表现下降，而传统微调方法依赖大量数据；本文旨在探索无需大量标注数据、仅靠推理时与外部资源交互即可习得新语言的范式。

Method: 提出ILA-agent框架，将人类学习行为建模为工具集（如查阅文档、执行代码、验证结果等），使LLM能通过结构化交互逐步探索、应用并验证新语言知识；构建Cangjie-bench多任务基准（基于新静态类型语言Cangjie）用于低资源评估。

Result: 在Cangjie的代码生成、翻译和程序修复任务上，ILA-agent显著超越检索增强基线；对智能体行为轨迹的分析揭示了涌现的学习模式，也指出当前仍存在的性能瓶颈。

Conclusion: ILA-agent验证了推理时语言习得的有效性，为LLM快速适应新兴编程语言提供了可行、低资源的新路径，但其行为稳定性与泛化能力仍有提升空间。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [209] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

TL;DR: 本文提出了一种名为Anchored Decoding的推理时方法，用于抑制大语言模型在生成过程中对训练数据的逐字复制，通过将生成过程锚定在安全模型附近，并引入信息预算与逐步约束，实现风险-效用的可调权衡；同时发布了安全模型TinyComma 1.8B及字节级变体Anchored$_{\mathrm{Byte}}$ Decoding，在多个指标上显著降低抄袭率（最高达75%），仅带来轻微推理开销。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型易记忆并复现训练数据中的敏感或受版权保护内容，引发创作者权益、合规性与开发者责任等问题。

Method: 提出Anchored Decoding：在推理时将风险模型的生成锚定于一个宽松许可训练的安全模型（如TinyComma 1.8B），自适应分配用户设定的信息预算，并施加逐步约束以保障序列级安全性；进一步提出字节级变体Anchored$_{\mathrm{Byte}}$ Decoding，结合ByteSampler实现跨词表融合。

Result: 在六个模型对和长文本评估中，该方法在保持接近原始模型流畅性与事实性的前提下，平均降低75%的可测抄袭差距（六项抄袭指标），仅引入适度推理开销。

Conclusion: Anchored Decoding是一种即插即用、可调、有理论保障的推理时防抄袭方法，定义了风险控制与生成质量的新Pareto前沿，为混合授权数据训练模型的安全部署提供了实用解决方案。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [210] [Free Energy Mixer](https://arxiv.org/abs/2602.07160)
*Jiecheng Lu,Shihao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Free Energy Mixer (FEM)的新注意力读取机制，通过自由能（log-sum-exp）操作实现值驱动的通道级选择，兼具并行性与原有复杂度，且在多个领域超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制虽无损存储键/值，但其每头凸平均读取方式阻碍了通道级选择能力，限制了建模灵活性。

Method: 提出FEM机制，以查询/键生成的快速先验分布为基础，引入可学习的逆温度参数，通过值驱动的逐通道log-linear倾斜，实现自由能读取；设计两级门控FEM，兼容标准注意力、线性注意力、线性RNN及SSM。

Result: 在NLP、视觉和时间序列任务中，在相同参数量下持续优于强基线模型。

Conclusion: FEM提供了一种更灵活、值感知的注意力读取范式，在保持计算效率的同时提升了建模能力，具有广泛适用性。

Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.

</details>


### [211] [Your Language Model Secretly Contains Personality Subnetworks](https://arxiv.org/abs/2602.07164)
*Ruimeng Ye,Zihan Wang,Zinan Ling,Yang Xiao,Manling Li,Xiaolong Ma,Bo Hui*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLM）参数中已内嵌多种人格专属子网络，提出一种无需训练的掩码与对比剪枝策略，可高效分离并激活对应人格子网络，显著提升人格对齐效果且无需外部知识。


<details>
  <summary>Details</summary>
Motivation: 探究LLM是否真的需要外部提示、检索或微调来适配不同人格行为，还是其参数中已天然嵌入了多样化人格知识。

Method: 基于小规模校准数据识别不同人格的激活特征，设计掩码策略定位轻量级人格子网络；针对二元对立人格（如内向-外向），引入对比剪枝策略，筛选导致统计差异的关键参数。全过程无需训练，仅利用模型原有参数空间。

Result: 所获子网络在多场景评测中人格对齐能力显著优于依赖外部知识的基线方法，同时更高效；验证了LLM参数中已存在类人行为多样性。

Conclusion: LLM中的人格化行为并非完全由外部诱导，而是预先编码于参数之中；该发现为可控、可解释的大模型个性化提供了新视角。

Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

</details>


### [212] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: 本文介绍了Open TutorAI，一个基于大语言模型和生成式技术的开源教育平台，旨在提供动态、个性化的辅导服务，通过自然语言处理与可定制3D头像结合实现多模态交互，并支持学习者、教师和家长的多样化需求。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人缺乏情境适应性、实时响应性和教学灵活性，限制了学习者参与度和教学效果，亟需开放、整合的AI与沉浸式技术融合平台。

Method: 构建基于LLM和生成式技术的开源平台Open TutorAI，集成NLP与可定制3D头像，通过结构化注册流程获取学习者目标与偏好，配置个性化AI助教，并提供文本与头像双接口、内容组织工具、嵌入式反馈及多角色专用界面。

Result: 实现了支持自适应学习支持、增强情感临场感与沉浸感、具备嵌入式学习分析功能的模块化、开源智能辅导系统。

Conclusion: Open TutorAI将模块化架构、生成式AI与学习者分析整合于开源框架中，推动下一代智能辅导系统的发展。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [213] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出PACIFIC框架，利用用户稳定的人格特质作为偏好背后的潜在信号，通过人格对齐的偏好选择显著提升个性化问答准确率，并构建了首个Big-Five人格标注的偏好数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接使用用户偏好易受噪声、不完整或误导性影响；而人格特质具有稳定性，可作为偏好背后的可靠潜在信号。

Method: 提出PACIFIC框架：1）构建含1200条跨领域偏好语句、标注Big-Five人格维度方向的数据集；2）设计LLM自动检索并融合人格对齐偏好的生成机制；3）通过实验验证人格一致性偏好对问答性能的提升效果。

Result: 在个性化问答任务中，使用人格对齐偏好使答案选择准确率从29.25%提升至76%；PACIFIC数据集覆盖旅行、电影、教育等多领域，支持五大人格维度（OCEAN）建模。

Conclusion: 人格是建模用户偏好的更鲁棒、可解释的潜在变量；将人格引入偏好驱动的LLM个性化，能显著提升可靠性与性能，为可信个性化AI提供新范式。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [214] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 本文提出了一种针对法律文档长上下文问答的系统，通过解构专业术语、解析复杂布局（如脚注与章节关系）、生成精准详尽的答案，并引入基于召回率的覆盖度评估指标。


<details>
  <summary>Details</summary>
Motivation: 法律文档具有复杂的版式结构（嵌套章节、长脚注）和特殊语言特征（复杂句法、领域术语），导致长上下文、长答案形式的问答极具挑战性。

Method: 提出一个三模块问答系统：（a）领域术语解构以提升检索；（b）布局解析以分离并关联章节与脚注；（c）使用精准领域词汇生成综合性答案；同时设计基于召回率的覆盖度评估指标，并构建由法律与税务专家标注的QA数据集。

Result: 通过全面实验与消融研究，验证了该系统在法律长文档问答任务中的可用性与有效性，覆盖度指标便于人工评估召回性能。

Conclusion: 所提系统有效应对法律文档长上下文、长答案问答的独特挑战，在术语处理、结构理解与答案生成三方面均有实质性改进，且评估方法更具可解释性与实用性。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [215] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

TL;DR: 本文研究如何使大语言模型（LLM）具备面向多说话人、多通道语音的定向理解能力，特别针对智能眼镜场景，提出级联分离前端和端到端序列化训练两种新方法，并利用多麦克风阵列实现实时定向语音识别与翻译。


<details>
  <summary>Details</summary>
Motivation: 现有语音大模型多基于单通道、单说话人数据训练，难以直接应用于多说话人、多通道的真实场景（如智能眼镜），亟需增强其方向性语音理解能力。

Method: 提出两种融合方向性信息的方法：(1) 级联系统——引入声源分离前端模块；(2) 端到端系统——采用序列化输出训练；二者均依托智能眼镜内置的多麦克风阵列，支持流式定向处理。

Result: 实验表明所提方法显著提升LLM在定向多说话人场景下的语音识别与语音翻译性能。

Conclusion: 通过麦克风阵列与新型建模方式（级联或端到端）协同，可有效赋予LLM方向感知与多说话人语音理解能力，为可穿戴设备上的智能语音交互提供可行技术路径。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [216] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 本文提出了一种风险敏感的幻觉评估框架，通过识别和量化医疗问答中可能带来实际危害的风险性语言（如治疗指令、禁忌症、紧急提示、高风险药物等），来评估大语言模型在患者面向任务中的潜在危害，而非仅关注事实正确性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉评估标准过于关注事实正确性，忽视了不同错误在临床场景下的危害差异，尤其无法识别那些虽无依据但具有可操作性的危险生成内容。

Method: 构建基于风险承载语言（如治疗指令、禁忌症、紧迫性提示、高风险药物）的幻觉量化框架，并结合相关性度量，识别高风险且低依据的生成失败；在三类指令微调语言模型上，使用可控的患者面向安全压力测试提示进行评估。

Result: 表面性能相近的模型展现出显著不同的风险特征；传统评估指标无法揭示这些关键差异。

Conclusion: 幻觉评估必须引入风险敏感性，且评估的有效性高度依赖于具体任务与提示设计。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [217] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 本文指出多轮对话中大语言模型性能下降（'Lost in Conversation'）的根本原因在于用户意图与模型理解之间的对齐差距，而非模型能力不足；为此提出一种'中介-助手'架构，通过经验驱动的中介模块将模糊的用户输入显式转化为结构化指令，从而显著缓解多轮对话中的性能退化。


<details>
  <summary>Details</summary>
Motivation: 现有研究将多轮对话中大语言模型性能下降归因于模型不可靠，但作者认为根本原因是用户意图与模型理解之间存在对齐差距，而非模型内在能力缺陷。

Method: 提出Mediator-Assistant架构：由经验驱动的Mediator模块基于历史交互模式，将用户模糊输入显式转化为结构化指令，解耦意图理解与任务执行。

Result: 实验表明该方法在多种大语言模型上显著缓解了多轮对话中的性能退化。

Conclusion: 多轮对话性能下降源于结构性语境歧义导致的意图对齐 gap，而非模型表征能力限制；所提中介架构能有效弥合该 gap，提升多轮交互鲁棒性。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [218] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文提出了ViHERMES——首个面向越南语医疗监管文档的多跳问答基准数据集，并设计了图感知检索框架以支持法律单元级关系建模与合法上下文扩展，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 医疗监管文档具有层级结构、频繁修订和跨文档引用等特点，导致多跳推理需求强烈；而当前缺乏支持越南语等低资源语言中监管文档多跳问答的系统性评测基准。

Method: 提出基于语义聚类与图启发式数据挖掘的可控多跳问答生成流程，并结合大语言模型生成带结构化证据与推理标注的问题-答案对；同时设计图感知检索框架，建模法律单元间的正式法律关系并支持合法、连贯的上下文扩展。

Result: ViHERMES数据集包含高质量、多样依赖模式（如修订追踪、跨文档比较、流程合成）的多跳问答对；实验表明所提图感知方法在该基准上持续超越强检索基线。

Conclusion: ViHERMES填补了低资源语言监管问答多跳推理评测的空白，所提出的图感知检索框架为法律文本QA提供了可解释、合法合规的新范式。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [219] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: TernaryLM 是一种原生1比特三值化（{-1, 0, +1}）的132M参数Transformer模型，在训练阶段即引入量化，显著降低内存占用（2.4倍），同时保持语言建模与下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算和内存开销大，难以部署于边缘设备；现有后训练量化方法无法充分挖掘极端量化潜力，需探索训练即量化的高效架构。

Method: 提出TernaryLM架构，采用原生1比特三值化权重（{-1, 0, +1}），结合直通估计器（STE）和自适应逐层缩放因子进行端到端训练。

Result: 在TinyStories上验证困惑度为58.42；MRPC任务F1达82.47%；内存从1197MB降至498MB（2.4×压缩），推理延迟相当；中层Transformer对三值化兼容性最高。

Conclusion: 原生1比特训练是构建高效语言模型的可行且有前景的方向，为非均匀精度策略提供实证依据。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [220] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种基于一阶统计特性的轻量级LLM后训练剪枝框架，通过通道级统计校准重要性得分，并用解析能量补偿修正分布失真，无需重训练或梯度计算，在保持高效的同时提升剪枝性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法在剪枝质量与计算效率之间存在权衡：启发式方法高效但对激活异常值敏感，重构方法保真度高但计算开销大。

Method: 提出基于权重和激活一阶统计特性的轻量级剪枝框架：1）利用通道级统计校准幅度重要性得分，缓解激活主导通道的偏差；2）采用解析能量补偿修正剪枝导致的分布失真；全程无需重训练、梯度或二阶信息。

Result: 在多个LLM家族、稀疏模式和评估任务上实验表明，该方法在计算成本与启发式方法相当的前提下，显著提升了剪枝性能。

Conclusion: 简单的统计校正策略可有效提升LLM后训练剪枝效果，为高效、免训练剪枝提供了新思路。

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [221] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: 本文提出了Demo-SafetyBench，一个建模人口统计多样性以提升大语言模型安全评估普适性的新基准。它通过两阶段流程：第一阶段重构并扩展安全提示数据集，保留人口统计元数据；第二阶段使用多个LLM作为评分器进行零样本多元敏感性评估，验证了其高信度与低人口统计偏差。


<details>
  <summary>Details</summary>
Motivation: 现有对齐数据集（如ANTHROPIC-HH和DICES）依赖狭窄的人口统计标注者群体，忽视不同社区在安全性认知上的差异，导致安全评估缺乏普适性。

Method: 提出Demo-SafetyBench：Stage I 使用 Mistral-7B 和 Llama-3.1-8B 对 DICES 提示重分类至14个安全领域，保留并扩充含人口统计元数据的样本（共43,050条），采用SimHash去重；Stage II 使用 Gemma-7B、GPT-4o 和 LLaMA-2-7B 作为评分器，在零样本下评估多元敏感性，并设定平衡阈值（delta=0.5, tau=10）。

Result: 评估显示高信度（ICC = 0.87）和低人口统计敏感性（DS = 0.12），证明该方法兼具可扩展性与人口统计鲁棒性。

Conclusion: Demo-SafetyBench首次在提示层面显式建模人口统计多样性，为LLM安全评估提供了更公平、更具代表性的基准，推动对齐研究走向真正多元包容。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [222] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出AlignX框架，通过两阶段方法解决大语言模型在多目标对齐（帮助性、无害性、诚实性）中的轴坍塌问题：第一阶段使用提示注入微调提取任务特定特征，第二阶段通过几何校准的MoCaE模块优化专家路由，显著提升性能并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法如监督微调（SFT）和混合专家（MoE）在多目标对齐（帮助性、无害性、诚实性）中存在目标干扰与路由不准问题，导致‘轴坍塌’——即特征空间分离引发灾难性遗忘，及专家误路由导致不可靠推理。

Method: 提出两阶段框架AlignX：Stage 1采用prompt-injected fine-tuning提取轴（目标）特定任务特征；Stage 2引入MoCaE模块，利用分形与自然几何原理校准专家路由。

Result: 在Alpaca（帮助性）、BeaverTails（无害性）、TruthfulQA（诚实性）上分别取得+171.5%胜率、+110.1%真实-信息性提升、4.3%更少安全违规；相比先前MoE方案，延迟与内存使用降低超35%；在四个LLM上验证了通用性。

Conclusion: AlignX有效缓解多目标对齐中的轴坍塌问题，兼顾性能提升与计算效率，在帮助性、无害性、诚实性三方面实现协同优化，具备强泛化能力与实用价值。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [223] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文提出了一种融合法律领域知识的框架，用于提升印度法律判决文本在英语和印地语上的摘要生成效果，通过改进抽取式和生成式模型（包括大语言模型），并在多个指标及领域专家评估中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 印度法律判决文本语言复杂、结构松散，且大量民众不熟悉法律英语，需生成英语和印地语双语摘要；现有模型缺乏法律领域知识，难以满足实际需求。

Method: 提出一种融合领域知识的框架：1）为抽取式模型引入针对法律文本微调的预训练编码器；2）对生成式模型（含大语言模型）在英/印地语法律语料上进行持续预训练以注入法律知识。

Result: 所提方法在英语到英语、英语到印地语的法律摘要任务上，在标准指标、事实一致性指标及法律领域专用指标上均取得统计显著提升，并经法律专家验证有效。

Conclusion: 将法律领域知识显式注入抽取式与生成式摘要模型可显著提升印度多语法律文本摘要质量，为低资源法律NLP任务提供了可行路径。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [224] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

TL;DR: 本文提出了一种基于词汇相似性（包括字形和语义相似性）的新型计算指标，用于评估罗曼语族语言间的相互可理解性，并在法、意、葡、西、罗五种主要罗曼语上进行了验证，结果与人类完形填空实验高度相关。


<details>
  <summary>Details</summary>
Motivation: 评估罗曼语族语言间的相互可理解性，尤其是验证语言间可理解性的不对称现象，并建立与人类感知一致的自动评估方法。

Method: 提出一种结合表面（字形/语音）和语义相似性的新计算指标，利用多种平行语料库和词向量模型，在五种主要罗曼语上进行跨语言可理解性估计。

Result: 所得可理解性分数确认了语言间可理解性的不对称直觉，并与人类完形填空实验结果呈显著相关。

Conclusion: 该基于词汇相似性的计算指标能有效建模并预测罗曼语间的相互可理解性，为低资源语言的自然语言处理任务提供了可行的评估依据。

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


### [225] [DLLM Agent: See Farther, Run Faster](https://arxiv.org/abs/2602.07451)
*Huiling Zhen,Weizhe Lin,Renxi Liu,Kai Han,Yiming Li,Yuchuan Tian,Hanting Chen,Xiaoguang Li,Xiaosong Li,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Youliang Yan,Peifeng Qin,Jun Wang,Yu Wang,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文研究了扩散大语言模型（DLLMs）在代理式多步决策任务中的表现，发现其相较于自回归（AR）模型，在保持准确率的同时显著提升端到端效率（平均快30%以上，最高达8倍），减少交互轮次与工具调用，并揭示了DLLM代理在规划一致性、注意力机制和结构化工具调用训练上的关键特性。


<details>
  <summary>Details</summary>
Motivation: 探究当代理框架和监督信号固定时，仅将生成范式从自回归切换为扩散，是否会导致系统性不同的规划与工具使用行为，并带来端到端效率增益。

Method: 在统一代理框架DeepDiver中分别集成DLLM与AR骨干模型，使用相同轨迹数据进行匹配的面向代理的微调；通过基准测试、案例分析、失败模式诊断、注意力掩码控制实验及注意力动态分析开展系统评估。

Result: DLLM代理在同等准确率下端到端速度平均提升超30%，部分场景达8倍；成功任务中交互轮次与工具调用更少，规划命中率更高、收敛更快；但存在结构化工具调用易失败、多轮输入中需对齐注意力掩码以避免信息泄露等问题；注意力分析显示DLLM代理具有更强的全局规划信号。

Conclusion: DLLM作为代理骨干具备显著效率与规划优势，但需针对性改进工具调用训练与输入建模；其注意力机制展现出不同于AR范式的协同规划模式，为下一代智能代理架构提供新方向。

Abstract: Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.

</details>


### [226] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: 本文提出SED-SFT方法，通过引入基于token探索空间的自适应选择熵正则化，在监督微调阶段缓解模式坍缩问题，从而提升后续强化学习阶段的性能。


<details>
  <summary>Details</summary>
Motivation: 传统交叉熵损失驱动的监督微调易导致模式坍缩，限制后续强化学习的探索效率；现有改进方法难以兼顾多样性与准确性。

Method: 提出SED-SFT框架，在优化目标中加入带选择性掩码机制的选择熵正则化项，自适应鼓励token层面的生成多样性。

Result: 在八个数学基准上实验表明，SED-SFT显著提升生成多样性，计算开销几乎无增加，并在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct上分别带来2.06和1.20分的RL性能提升。

Conclusion: SED-SFT有效缓解SFT中的模式坍缩问题，为LLM后训练提供了一种兼顾多样性与准确性的新范式。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [227] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种系统性评估框架，用于诊断和量化视觉语言模型（VLMs）在多语言迷因数据集上的跨文化鲁棒性，并发现原生语言提示与单样本学习可显著提升仇恨迷因检测性能，而“翻译后检测”策略会损害性能。


<details>
  <summary>Details</summary>
Motivation: 文化背景深刻影响人们对网络内容的理解，但现有视觉语言模型（VLMs）主要基于西方或英语中心视角训练，导致其在仇恨迷因检测等任务中缺乏公平性和跨文化鲁棒性。

Method: 构建系统性评估框架，在多语言迷因数据集上从三方面分析VLMs的跨文化鲁棒性：(i) 学习策略（零样本 vs. 单样本），(ii) 提示语言（母语 vs. 英语），(iii) 翻译对语义及检测效果的影响。

Result: ‘翻译后检测’策略显著降低性能；而采用母语提示和单样本学习等文化适配干预措施则显著提升检测效果；模型表现出向西方安全规范系统性收敛的趋势。

Conclusion: 应避免简单翻译策略，转而采用文化对齐的设计（如母语提示、少量样本学习），以提升全球多模态内容审核系统的鲁棒性与公平性。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [228] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

TL;DR: 本文提出了一种通过动态路径规划、语义感知的示例选择和结合对话历史的思维链生成来分解复杂简化任务的框架，提升了多语言句子简化效果并减少了计算步骤，但揭示了简化效果与语义保真度之间固有的权衡难题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在跨较大可读性层级进行可控句子简化时能力有限。

Method: 提出一种框架，包含动态路径规划、语义感知的示例选择和结合对话历史的链式思维生成，以实现分步简化。

Result: 在五个语言、两个基准上的评测显示简化效果提升，计算步骤减少22-42%；人工评估揭示简化效果与语义保留之间的根本权衡，且人类标注者在语义保真判断上一致性低。

Conclusion: 分步简化提升了可控性，但在大幅简化中保持语义保真仍是开放挑战。

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [229] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: 本文提出LR-DLLM框架，通过显式长度正则化校正扩散大语言模型（DLLMs）中固有的长度诱导置信度偏差，从而实现无需修改模型或训练过程的可靠可变长度生成。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）在目标长度未知的实际补全与填充任务中，因固定画布假设和长度诱导的置信度偏差，导致生成长度不可靠，易出现欠生成或冗余续写。

Method: 提出LR-DLLM——一种长度正则化的DLLM推理框架，将生成长度作为显式变量建模，通过显式长度正则化解耦语义兼容性与长度不确定性，校正置信度估计，并支持动态调整生成跨度。

Result: 在HumanEvalInfilling（完全未知长度）上Pass@1达51.3%（+13.4% vs. DreamOn）；在四语言McEval上平均Pass@1达51.5%（+14.3% vs. DreamOn）。

Conclusion: LR-DLLM有效解决了DLLMs在可变长度生成中的长度判定难题，显著提升其在真实场景下的可靠性与泛化能力，且不依赖模型结构或训练改动。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [230] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在生成与自我验证能力之间的不对称性，发现提升生成能力并不能同步提升自我验证能力，但反过来，强化自我验证能力却能有效提升生成性能；基于此，作者提出一种多任务强化学习框架，联合优化生成与自我验证两个目标，并在多个基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽具备强大的推理路径生成能力，但在自我验证答案方面表现薄弱，存在生成与验证能力之间的持续不对称性，亟需系统性探究与建模。

Method: 通过训练演化分析揭示生成与验证能力的非对称关系；提出以自我验证驱动生成提升的思路；构建多任务强化学习框架，将生成与自我验证设为协同优化的独立目标。

Result: 实验证明：（1）单独训练自我验证可显著提升生成准确率并产出更高效推理路径；（2）多任务RL框架在生成与验证两方面均优于仅生成训练；（3）效果在多个模型与基准上具有一致性。

Conclusion: 生成与自我验证能力存在本质不对称性，而将二者联合建模（尤其以验证引导生成）是提升整体推理鲁棒性与效率的有效范式。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [231] [SciClaimEval: Cross-modal Claim Verification in Scientific Papers](https://arxiv.org/abs/2602.07621)
*Xanh Ho,Yun-Ang Wu,Sunisth Kumar,Tian Cheng Xia,Florian Boudin,Andre Greiner-Petter,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文介绍了SciClaimEval，一个用于科学声明验证的新数据集，其特点是包含直接从已发表论文中提取的真实声明（包括被驳斥的声明），并通过修改支持性证据（图表和表格）而非篡改声明本身来生成驳斥声明；该数据集提供跨模态证据（图像、LaTeX、HTML、JSON等格式），涵盖180篇论文共1664个样本，并在11个多模态基础模型上进行了基准测试，结果显示图表验证仍是所有模型的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有科学声明验证数据集缺乏真实性，常依赖大语言模型生成虚假矛盾或人为篡改声明，难以反映真实科研场景中的验证挑战；因此需要一个基于真实论文、含自然驳斥声明且支持跨模态证据的数据集。

Method: 提出一种新方法：通过修改原始论文中的图表和表格（而非改写声明或用LLM生成矛盾）来构造驳斥声明；收集180篇跨三个领域的论文，提取并人工标注1664个样本，提供图像、LaTeX、HTML、JSON等多种格式的跨模态证据；对11个多模态基础模型进行系统评测。

Result: 所有被测模型在图表验证任务上表现显著落后于人类基线，存在较大性能差距；表格验证相对较好但仍有提升空间；开源与闭源模型整体表现相近，未见明显优势。

Conclusion: SciClaimEval为科学声明验证提供了更真实、更具挑战性的基准；结果揭示当前多模态模型在理解科学图表方面存在根本性局限，亟需面向科学视觉推理的新建模方法。

Abstract: We present SciClaimEval, a new scientific dataset for the claim verification task. Unlike existing resources, SciClaimEval features authentic claims, including refuted ones, directly extracted from published papers. To create refuted claims, we introduce a novel approach that modifies the supporting evidence (figures and tables), rather than altering the claims or relying on large language models (LLMs) to fabricate contradictions. The dataset provides cross-modal evidence with diverse representations: figures are available as images, while tables are provided in multiple formats, including images, LaTeX source, HTML, and JSON. SciClaimEval contains 1,664 annotated samples from 180 papers across three domains, machine learning, natural language processing, and medicine, validated through expert annotation. We benchmark 11 multimodal foundation models, both open-source and proprietary, across the dataset. Results show that figure-based verification remains particularly challenging for all models, as a substantial performance gap remains between the best system and human baseline.

</details>


### [232] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出了一种基于激活空间引导（activation steering）的方法，利用人类师生对话中提取的导师人格特征，无需显式提示即可引导大语言模型（LLM）生成符合特定教学风格的回应；该方法通过改进双向偏好优化（BiPO）学习出可解释的‘引导向量’，显著提升语义对齐与偏好评估效果，同时保持词汇相似性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统通常只学习单一教学策略，缺乏对真实教学中多样化辅导风格（如脚手架程度、指导强度、反馈方式和情感支持）的建模能力，而这些风格差异显著影响对话动态与学生参与度。

Method: 改进Bidirectional Preference Optimization（BiPO），在模型激活空间中学习一个‘引导向量’（steering vector），用于隐式引导LLM响应以匹配特定导师人格；该向量从人类师生对话数据中直接学习，不依赖显式指令提示。

Result: 所学引导向量能有效捕捉不同导师在多种对话上下文中的行为差异，提升模型输出与真实导师话语的语义对齐度及偏好评估得分，同时基本维持词汇层面相似性；方向系数分析揭示了跨导师一致且可解释的行为模式。

Conclusion: 激活空间引导是一种有效且可解释的方式，能够仅凭人类对话数据调控LLM的教学风格多样性，为构建个性化、人格化AI导师提供了新路径。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [233] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）作为评判者在摘要任务中的偏差，发现LLM评判者倾向于偏好与其他LLM生成摘要相似度较低的摘要，而非人工摘要，且该偏差与ROUGE/BLEU重叠度呈负相关，提示需超越简单对比的评估方法。


<details>
  <summary>Details</summary>
Motivation: LLM评判者虽在语义理解和鲁棒性上优于传统指标，但存在长度、顺序等偏差及对抗敏感性；现有研究缺乏基于明确定义重叠度（如ROUGE/BLEU）的细粒度偏差分析。

Method: 在摘要领域，以ROUGE和BLEU衡量LLM生成摘要与人工摘要的重叠度，系统评估9个参数量1B–12B的主流LLM（含Gemma 3和LLaMA 3变体）作为评判者的偏好偏差，并控制位置偏差变量。

Result: 发现绝大多数被测LLM评判者在摘要重叠度降低时，反而更偏好LLM生成摘要而非人工摘要；且即使重叠度有限，其判断能力仍显著下降。

Conclusion: 单纯依赖LLM-as-a-judge进行摘要评估存在系统性偏差，需引入超越直接文本对比的增强技术（如参考增强、校准机制或多维度评分）。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [234] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出SRR-Judge框架，用于对深度搜索代理的推理与搜索动作进行细粒度、可靠的步骤级评估，并结合率-精炼流程与迭代拒绝采样微调，显著提升模型在深度搜索任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有主流方法仅依赖结果监督训练搜索集成推理能力，忽视中间思维与动作质量，导致推理过程不可靠。

Method: 提出SRR-Judge框架，实现步骤级推理与搜索动作评估；将其嵌入改进的ReAct式rate-and-refine流程中，支持高效后训练标注；基于SRR标注数据，采用迭代拒绝采样方式进行微调。

Result: SRR-Judge在步骤级评估上比DeepSeek-V3.1等更大模型更可靠，其评分与最终答案正确性高度相关；策略对齐SRR标注轨迹后，在多个深度搜索基准上平均pass@1提升超10个百分点。

Conclusion: 细粒度步骤级监督对提升搜索集成推理能力至关重要，SRR-Judge为构建更可靠、可解释的深度搜索代理提供了有效评估与训练范式。

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [235] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出Attn-GS框架，利用LLM注意力机制识别关键个性化信号，实现高效上下文压缩，在大幅降低token用量的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化LLM方法受限于输入token长度，难以有效整合用户长历史交互和画像；启发式压缩方法忽略LLM内部对不同信息的处理差异。

Method: 通过分析LLM注意力模式发现其可自然揭示重要个性化信号，并经微调后增强判别能力；据此设计Attn-GS框架：用标记模型基于注意力反馈标定关键句子，再由压缩模型生成任务相关、高质量的压缩上下文。

Result: Attn-GS在多种任务、token限制与设置下显著优于各类基线，性能接近使用完整上下文，同时将token用量减少50倍。

Conclusion: LLM的注意力模式是指导个性化上下文压缩的有效信号，Attn-GS为高效、低开销的用户个性化提供了新范式。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [236] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在上下文概念推理中的内部表征机制，发现中后期层存在一个跨上下文稳定的、功能上起因果作用的概念子空间，并揭示了注意力头分层协作构建与利用该子空间的动态过程。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出类人推理的突发行为，且已发现其内部存在结构化、类人的概念表征，但尚不清楚模型是否在推理中真正依赖这些表征。

Method: 通过分析LLM在上下文概念推理过程中的内部表示，识别概念子空间；采用因果中介分析验证其功能性因果作用；并结合层间注意力头行为分析其构建与使用机制。

Result: 发现中晚期层存在一个跨上下文稳定的概念子空间；该子空间对模型预测具有因果性影响；早期至中期层的注意力头负责整合上下文以构建和优化该子空间，后期层则利用它生成预测。

Conclusion: LLMs并非被动激活预存表征，而是能动态地在上下文中构建并功能性地使用结构化的潜在概念表征进行推理，这为理解其灵活适应能力的计算基础提供了新证据。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [237] [Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents](https://arxiv.org/abs/2602.07796)
*Jiatong Li,Changdae Oh,Hyeong Kyu Choi,Jindong Wang,Sharon Li*

Main category: cs.CL

TL;DR: 本文研究了在用户参与的LLM代理场景中显式推理（thinking）的效果，发现强制推理反而导致性能下降，因其使代理变得‘内向’、减少信息透露；而主动提升信息透明度可显著改善性能。


<details>
  <summary>Details</summary>
Motivation: 探究显式推理在真实用户参与的LLM代理场景中的实际有效性，因现有研究多聚焦于静态任务，缺乏对人机交互动态性的考量。

Method: 在7个模型、3个基准和2种推理实例化方式上开展实验，结合定量响应分类分析与定性失败传播案例研究。

Result: 强制推理普遍导致性能异常下降；推理使代理响应更短、信息透露更少，削弱人机信息交换；显式提示信息披露可稳定提升各模型家族性能。

Conclusion: 信息透明度意识是设计现实世界推理代理的关键新视角，应被纳入未来代理优化的核心考量。

Abstract: Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.

</details>


### [238] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出GISA基准，用于评估通用信息检索助手，包含373个人工构建的真实查询，支持多种答案格式和动态更新，提供完整人工搜索轨迹，实验表明现有模型性能仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在任务不自然、答案静态易污染、评估维度单一等问题，无法真实反映信息检索助手在现实场景中的能力。

Method: 构建GISA基准，包含373个真实人类信息需求查询，设计四种结构化答案格式（item/set/list/table），引入动态更新的live子集，并提供完整人工搜索轨迹作为监督信号。

Result: 主流大模型和商用搜索产品在GISA上表现不佳，最优模型精确匹配率仅19.30%，复杂规划与综合信息收集任务性能显著下降。

Conclusion: GISA有效弥补了现有基准缺陷，揭示当前搜索代理能力的严重不足，为未来研究提供了更可靠、更具挑战性的评估平台和训练监督信号。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [239] [Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models](https://arxiv.org/abs/2602.07804)
*Xuan Ding,Pengyu Tong,Ranjie Duan,Yunjian Zhang,Rui Sun,Yao Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于博弈论的层剪枝框架，将大语言模型（LLM）的层剪枝建模为合作博弈，利用轻量代理网络和分层蒙特卡洛采样高效估计Shapley值，以动态识别关键层，显著提升剪枝效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法依赖静态启发式规则，忽视层间依赖关系，导致剪枝效果受限；同时LLM推理计算开销大，亟需更智能、动态的剪枝策略。

Method: 将层剪枝建模为合作博弈，各层为玩家、模型性能为效用；采用轻量级代理网络预测任意层组合下的LLM性能以估算层边际贡献；结合 stratified Monte Carlo mask sampling 高效近似Shapley值，捕捉层间依赖并动态识别关键层。

Result: 在困惑度（perplexity）和零样本准确率（zero-shot accuracy）上均显著优于现有方法，实现了更高效、更有效的LLM层剪枝。

Conclusion: 所提博弈论框架能有效建模层间协作关系，代理网络与采样策略协同降低了Shapley值估计成本，为LLM高效部署提供了新范式。

Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.

</details>


### [240] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 本文通过用户研究探讨了在对话式搜索中，图文结合的澄清问题相较于纯文本澄清问题对用户表现的影响，发现图像的作用因任务类型和用户专业知识水平而异。


<details>
  <summary>Details</summary>
Motivation: 尽管文本澄清问题已被证明能提升检索性能和用户体验，但图像在澄清问题中的作用尚未被充分探索，尤其是在对话式搜索中如何影响用户表现。

Method: 开展了一项包含73名参与者的用户研究，对比图文多模态与纯文本澄清问题在两个任务（回答澄清问题、查询重构）中的效果，并从多个角度分析任务类型与用户专业知识的影响。

Result: 图像提升了用户在回答澄清问题时的参与度，并在查询重构中带来更精确的查询和更好的检索性能；但在回答澄清问题时，纯文本设置因提供更全面的文字信息反而表现出更好的用户性能。

Conclusion: 图像增强的效果具有任务依赖性，应根据具体搜索场景和用户特征进行策略性设计。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [241] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在处理混合记法数值比较时的内部数值理解能力，发现其隐藏层能线性编码数值对数大小并预测大小关系，但显式回答准确率较低；通过将探针的对数损失作为辅助目标微调，可提升其数值推理表现。


<details>
  <summary>Details</summary>
Motivation: 尽管先进大语言模型能解数学题，但在混合记法（如科学计数法与十进制）的数值比较任务中表现不佳，引发对其是否真正理解数值大小的根本质疑。

Method: 对多个开源小规模LLM的隐藏状态进行探针分析：用单一线性投影从适当隐藏层提取数值的对数大小，并训练线性分类器判断两数大小关系；进一步将该探针的log-loss作为辅助损失用于微调。

Result: 隐藏层可高精度恢复数值（相对误差2.3%~19.06%），且能以>90%准确率编码数值大小关系；但模型显式回答仅50–70%准确；引入辅助目标微调后，显式回答准确率额外提升3.22%。

Conclusion: LLM内部已具备一定数值大小表征能力，但该能力未被充分调用以支持显式推理；显式性能可通过增强内部表征得到改善。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [242] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

TL;DR: 本文提出了一种结合少样本LLM命名实体识别与上下文感知的基于Agent的地理编码模块的两步框架，以解决人道主义文本中地理位置提取存在的地理与社会经济偏差问题；实验表明该方法在精度和公平性（尤其对欠代表地区）上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化地理信息提取系统常复现地理和社会经济偏见，导致危机受影响区域可见性不均，亟需更公平、准确的方法。

Method: 提出两步框架：1）少样本LLM进行地名识别；2）基于Agent的上下文感知地理编码模块消解歧义地名；在扩展版HumSet数据集上评估。

Result: LLM方法显著提升地理位置提取的精度与公平性，尤其改善对欠代表地区的识别效果；在地理与社会经济维度的公平性指标上优于现有SOTA模型。

Conclusion: 将LLM推理能力与负责任、包容性AI原则结合，可构建更公平的人道主义地理空间数据系统，助力‘危机分析中不遗漏任何地方’的目标。

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [243] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: 本文提出TodoEvolve，一种元规划范式，通过构建统一模块化设计空间PlanFactory和新型多目标强化学习方法IGPO，实现任务特定规划架构的自主合成与动态修订，在多个基准上超越手工设计规划模块。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法依赖固定、手工设计的规划结构，缺乏应对开放性问题结构多样性的灵活性。

Method: 提出TodoEvolve元规划范式：1）构建PlanFactory模块化设计空间，统一表征拓扑、初始化、适应与导航等规划要素；2）基于PlanFactory采集高质量规划轨迹，训练Todo-14B模型；3）引入阻抗引导偏好优化（IGPO）作为多目标强化学习目标，兼顾性能、稳定性与token效率。

Result: 在五个智能体基准测试中，TodoEvolve持续优于精心设计的规划模块，同时保持较低API成本与运行开销。

Conclusion: TodoEvolve通过自主演化规划架构，显著提升了智能体在复杂长周期任务中的规划能力与泛化性，为自适应规划提供了新范式。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [244] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 本文揭示了现有无训练置信度校准方法在多答案场景下的失效问题，提出了MACE基准和语义置信度聚合（SCA）方法以提升大语言模型在多答案问答中的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练-free的置信度校准方法主要针对单答案问答设计，在存在多个正确答案时因答案间分歧导致系统性低估置信度，缺乏对多答案场景的系统研究。

Method: 提出MACE基准（含12,000个跨6领域、答案数量可变的事实性问题），并设计Semantic Confidence Aggregation（SCA）方法——通过对多个高概率采样响应进行置信度聚合来校准模型输出。

Result: 实验覆盖15种校准方法与4类LLM（7B–72B），发现准确率随答案数量增加而上升，但估计置信度却持续下降；SCA在混合答案设置下达到SOTA校准性能，同时保持单答案场景下的强校准能力。

Conclusion: 多答案场景显著挑战现有校准范式，SCA通过语义层面的响应聚合有效缓解该问题，为LLM在开放、真实问答任务中的可信部署提供了新路径。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [245] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出SparseEval方法，通过稀疏优化和梯度下降选择代表性基准样本（锚点），显著降低大语言模型评估的计算成本，同时保持高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模增大导致评估成本高昂，需寻找高效且准确的基准测试方法。

Method: 基于模型-题目性能矩阵的稀疏性，提出SparseEval：采用梯度下降优化锚点权重，结合MLP建模与锚点/候选重要性评分进行迭代精炼。

Result: 在多个基准上展现出低估计误差和高Kendall's τ，验证了其准确性、鲁棒性与实用性。

Conclusion: SparseEval首次将梯度下降引入高效基准评估，为大模型低成本、任务感知评估提供了新范式。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [246] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文从机制角度研究指令调优语言模型内部如何处理指令，发现指令表示（Instruction Vectors, IVs）具有局部性、线性可分但又存在非线性因果交互的特性，并提出一种免于线性假设的新方法，揭示IVs在模型中起电路选择器作用。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型广泛应用，但其内部如何处理指令仍不清楚，本文旨在从机制层面填补这一空白。

Method: 采用因果中介分析识别指令表示的局部性；提出一种不依赖线性假设的新方法来定位信息处理路径；结合SFT与DPO阶段分析指令向量（IVs）的构建与使用。

Result: 发现指令表示（IVs）高度局部化，兼具线性可分性与非线性因果交互；验证IVs在早期形成任务表征后，在后期层中选择不同信息通路，即充当‘电路选择器’。

Conclusion: 指令表示并非简单线性结构，其功能体现为动态电路选择，挑战了机械可解释性中普遍存在的线性表征假设。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [247] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: 本文提出了Bielik Guard，一种针对波兰语的轻量级内容安全分类器家族，包含0.1B和0.5B两种参数规模的模型，基于RoBERTa架构微调，在多个安全类别上实现高精度与低误报率，并开源发布。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在波兰语应用中日益普及，亟需高效、准确的本地化内容安全分类器来保障部署安全性。

Method: 基于MMLW-RoBERTa-base和PKOBP/polish-roberta-8k构建两个轻量级模型，使用6885条社区标注波兰语文本进行微调，支持五类安全风险（仇恨/攻击、粗俗用语、性相关内容、犯罪、自残）的细粒度分类。

Result: 0.5B模型在测试集上取得0.791（micro-F1）和0.785（macro-F1）；0.1B模型在真实用户提示中实现77.65%精度与仅0.63%误报率，显著优于同尺寸基线HerBERT-PL-Guard。

Conclusion: Bielik Guard兼顾性能与效率，特别优化敏感类别响应策略（如不简单屏蔽自残内容），并已开源，适用于实际波兰语AI系统的内容安全防护。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [248] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: 本文提出CompositeHarm基准，通过翻译扩展英文安全数据集至六种印度语言，揭示LLM安全对齐在跨语言迁移中的不稳定性，尤其指出对抗性语法攻击在印地语等语言中成功率显著上升；研究采用轻量推理策略提升多语言安全评估的可扩展性与能效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估多局限于英语，简单翻译难以反映跨语言间有害意图与结构的真实变化，亟需系统考察安全对齐在语法和语义迁移下的鲁棒性。

Method: 构建CompositeHarm基准，融合AttaQ（结构化对抗攻击）与MMSafetyBench（上下文现实危害）两个英文数据集，并翻译扩展至英语、印地语、阿萨姆语、马拉地语、卡纳达语和古吉拉特语；在三个大模型上评估攻击成功率；采用受边缘AI启发的轻量推理策略以提升效率。

Result: 对抗性攻击在印地语等印欧语系语言中成功率显著上升，尤其在对抗性语法下；而上下文类危害迁移较温和；轻量推理策略有效保障了跨语言评估的计算可行性与环保性。

Conclusion: 翻译基准是多语言安全评估的必要起点，但不足以支撑真正扎根、资源感知、语言自适应的安全系统构建。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [249] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

TL;DR: 本文提出SynCog框架，通过可控零样本多模态数据合成与思维链（CoT）推理微调，解决轻度认知障碍（MCI）语音生物标志物研究中临床数据稀缺、跨语言泛化差及模型不可解释等问题；在ADReSS/ADReSSo基准上达到80.67%和78.46% Macro-F1，在中文真实队列CIR-E上达48.71%，显著提升诊断性能与临床可信度。


<details>
  <summary>Details</summary>
Motivation: 当前语音数字生物标志物用于轻度认知障碍（MCI）早期识别面临三大挑战：临床数据严重稀缺、跨语言泛化能力弱、模型缺乏可解释性与临床信任基础。

Method: 提出SynCog框架：1）可控零样本多模态数据合成，模拟多样化虚拟受试者以生成跨语言语音-文本表型数据；2）基于合成数据，对多模态大语言模型（MLLM）进行思维链（CoT）推理微调，使其显式输出诊断推理过程。

Result: 在ADReSS和ADReSSo基准上Macro-F1分别达80.67%和78.46%，超越现有基线；在独立中文真实队列CIR-E上实现48.71% Macro-F1，验证跨语言泛化能力。

Conclusion: SynCog有效缓解数据瓶颈、增强跨语言适应性并提升模型可解释性，为全球范围内可信赖、多语言兼容的认知评估工具提供了新范式。

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [250] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLM）作为自动评估器时对无关提示线索（如来源、时间、性别等元数据）的敏感性，发现其判决易受干扰且极少在推理中显式承认这些线索，暴露出‘解释鸿沟’，威胁评估可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM被广泛用作自动评估器，但其是否真正基于内容质量公正评判、是否对无关上下文保持不变、是否透明反映决策依据，尚缺乏系统检验。

Method: 通过向评估提示中注入六类合成元数据线索（来源、时间、年龄、性别、族裔、教育程度），在ELI5（事实问答）和LitBench（创意写作）两个数据集上测试六个主流LLM裁判模型，并引入‘判决偏移率（VSR）’和新指标‘线索承认率（CAR）’来量化行为影响与解释一致性。

Result: 多数强效应线索（如‘专家>人类>LLM’来源层级、‘新>旧’时间偏好、教育程度偏向）引发显著判决偏移，但CAR普遍接近零；CAR具有数据集依赖性——在ELI5中部分模型偶有承认，在LitBench中几乎完全消失，而判决偏移仍持续存在。

Conclusion: LLM裁判存在严重的‘解释鸿沟’：决策易受隐含线索操纵，却极少在自然语言推理中暴露该依赖，这严重削弱其在研究与实际部署中的可信度与可靠性。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [251] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: 本文提出DeltaKV框架，通过残差编码压缩KV缓存，并结合Sparse-vLLM推理引擎，在保持高精度的同时显著降低内存占用并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型部署受限于KV缓存内存随长度线性增长；现有压缩与驱逐方法难以兼顾精度、压缩率和硬件效率。

Method: 基于长程词元相似性和KV表示中高度共享潜在成分的两个实证发现，提出DeltaKV残差式KV缓存压缩框架；同时设计Sparse-vLLM推理引擎，支持解耦内存管理和稀疏/不规则KV布局优化内核。

Result: DeltaKV将KV缓存内存降至原大小的29%，在LongBench、SCBench和AIME上保持近无损精度；结合Sparse-vLLM后，长上下文场景下吞吐量相较vLLM最高提升2倍。

Conclusion: DeltaKV与Sparse-vLLM联合提供了一条实用、可扩展的长上下文大语言模型部署路径。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [252] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文提出Diverge-to-Induce Prompting (DIP)框架，通过生成多个多样化高层推理路径并归纳为最终计划，提升零样本推理准确性，无需高成本采样。


<details>
  <summary>Details</summary>
Motivation: 标准思维链提示中无引导的推理路径不稳定；单一策略引导虽有改进，但难以适应多样化任务需求。

Method: DIP框架分三步：1）生成多个多样化的高层推理理由；2）将每个理由展开为详细步骤草案；3）归纳多个草案形成最终推理计划。

Result: 实验表明DIP在零样本推理准确率上优于单策略提示方法，验证了多计划归纳的有效性。

Conclusion: 多策略推理路径的生成与归纳可显著提升大语言模型的零样本推理鲁棒性与泛化能力，且不依赖资源密集型采样。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [253] [Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection](https://arxiv.org/abs/2602.08031)
*Chenwang Wu,Yiu-ming Cheung,Shuhai Zhang,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种基于马尔可夫随机场的轻量级分数校准策略，用于提升度量型机器生成文本检测器的鲁棒性，尤其在面对生成随机性和跨模型/改写攻击时表现优异。


<details>
  <summary>Details</summary>
Motivation: 机器生成文本（MGTs）带来便利的同时也引发虚假信息和钓鱼等风险，亟需可靠检测；而现有度量方法虽实用但易受生成过程固有随机性影响导致token级检测分数偏差。

Method: 构建统一框架分析典型度量方法，理论与实证揭示‘邻近相似性’和‘初始不稳定性’两种上下文检测分数关系，并基于马尔可夫随机场建模，采用平均场近似实现轻量级校准组件。

Result: 在跨大语言模型、改写攻击等多种真实场景下显著优于基线方法，计算开销极小。

Conclusion: 所提马尔可夫感知校准策略能有效缓解生成随机性带来的偏差，提升度量型检测器的泛化性与鲁棒性，且易于集成。

Abstract: While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at https://github.com/tmlr-group/MRF_Calibration.

</details>


### [254] [TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs](https://arxiv.org/abs/2602.08048)
*Arshia Hemmat,Philip Torr,Yongqiang Chen,Junchi Yu*

Main category: cs.CL

TL;DR: 本文提出TDGNet，一种基于时间动态图的幻觉检测框架，用于扩散语言模型（D-LLMs），通过建模去噪过程中token级注意力图的演化，结合稀疏化、消息传递与时间注意力，实现高效准确的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 现有为自回归大模型设计的幻觉检测器依赖单次生成线索，难以适配扩散模型中事实性证据随去噪步数动态出现、漂移或自修正的特点，因此需专为D-LLMs设计时序感知的检测方法。

Method: 提出TDGNet：在每一步去噪中构建并稀疏化token级注意力图，通过消息传递更新各token记忆，并利用时间注意力聚合整个去噪轨迹中的证据以进行最终预测。

Result: 在LLaDA-8B和Dream-7B模型及多个QA基准上，TDGNet在AUROC指标上持续优于输出型、隐状态型和静态图基线方法，且支持单次前向推理、开销小。

Conclusion: 在扩散语言模型中，对注意力图进行时间维度上的建模与推理，是提升幻觉检测鲁棒性的关键路径。

Abstract: Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.

</details>


### [255] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: 本文研究了潜推理变换器（LRT）在不使用语言的情况下如何进行推理，发现其在隐空间中自发形成结构化搜索过程，包括探索、初步聚焦、收敛或回溯，并表现出自适应性与纠错能力。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在不依赖链式思维（chain-of-thought）等显式语言推理时，能否在连续隐空间中完成有效、结构化的推理过程。

Method: 构建并分析一个潜推理变换器（LRT），在多选问答基准上逐层解码其隐藏状态所表征的信念演化过程，定量刻画其搜索轨迹、回溯行为及对干扰项语义变化的响应。

Result: LRT在隐空间中自发形成三阶段结构化搜索（探索→初步聚焦→收敛/回溯）；回溯发生率达32%，带来34%准确率提升，且多指向远离语义相近干扰项、转向正确答案；替换干扰项为更不合理选项可使探索阶段缩短54%。

Conclusion: 潜推理模型能在激活空间中实现类似链式思维的功能——出错、察觉并恢复，表明结构化推理可脱离语言表征而独立存在于神经激活动态中。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [256] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 本文首次系统性地探究了大语言模型（LLM）在生成消费者产品推荐时所嵌入和放大的性别与种族偏见，通过提示工程和三种分析方法（标记词、支持向量机、Jensen-Shannon散度）发现显著的群体间推荐差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在商品推荐中广泛应用，但其可能隐含并加剧性别与种族偏见的问题尚未被充分研究。

Method: 采用提示工程引导LLM为不同种族与性别群体生成产品推荐，并结合标记词分析、支持向量机（SVM）分类及Jensen-Shannon散度量化偏差。

Result: 发现针对不同人口统计群体的推荐存在显著差异，表明LLM推荐系统中存在可观测且可量化的偏见。

Conclusion: 亟需构建更公平、更具包容性的LLM推荐系统，并加强对生成式AI中社会偏见的评估与干预。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [257] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: 本文提出DIALSUMMER框架，用于评估对话摘要质量，涵盖对话级和单轮级错误分类，并构建了人工标注的错误数据集，揭示了现有大模型在检测这些错误方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要评估方法忽略了对话特有的结构和叙述视角转换复杂性，如多说话人信息分散、第一/二人称到第三人称的转换等。

Method: 提出DIALSUMMER框架，包括两层错误分类体系（对话级和单轮级），构建并人工标注了细粒度错误数据集，并通过实证分析和大模型评测实验验证其有效性与挑战性。

Result: 发现中间位置的对话轮次最易被遗漏，外部幻觉多出现在摘要末尾；大模型在检测这些错误方面表现不佳，凸显任务难度及未来改进必要性。

Conclusion: DIALSUMMER为对话摘要评估提供了更全面、细粒度的框架和基准，揭示了当前大模型在该任务上的不足，推动后续研究发展。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [258] [NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark](https://arxiv.org/abs/2602.08162)
*Ricardo Campos,José Pedro Evans,José Miguel Isidro,Miguel Marques,Luís Filipe Cunha,Alípio Jorge,Sérgio Nunes,Nuno Guimarães*

Main category: cs.CL

TL;DR: 本文综述了自然语言处理（NLP）在结构化和解读地方治理会议记录中的三个基础任务：文档分割、领域特定实体抽取和自动文本摘要，并讨论了方法、评估指标、可用资源及领域挑战。


<details>
  <summary>Details</summary>
Motivation: 地方治理会议记录虽具结构性，但因语言、术语、结构等高度异质性，难以被非专家理解且自动化处理困难，影响政务透明与公众参与。

Method: 综述NLP中三项基础任务：文档分割、领域特定实体抽取、自动文本摘要；分析其方法、评估指标、公开资源及领域挑战（如数据稀缺、隐私限制、来源差异）。

Result: 系统梳理了NLP支持地方治理会议文档结构化的技术路径与实践难点，为提升政务文档可访问性与可解释性提供框架性参考。

Conclusion: NLP技术可有效增强地方治理会议记录的结构化与可及性，但需针对性解决数据、隐私与异构性等现实挑战。

Abstract: Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.

</details>


### [259] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文研究了人类与大语言模型（LLM）在多模态交流游戏中是否能形成类似对话惯例，发现同质配对（人-人或AI-AI）可形成有效惯例，而异质配对（人-AI）则失败；即使通过提示让LLM模仿人类表达长度，其准确性和词汇重叠仍不及人-人或AI-AI配对，表明对话对齐不仅依赖行为模仿，更需共享的意义解释倾向。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否能在交互中像人类一样形成共享的沟通惯例，以及人与AI之间能否实现有效对话对齐。

Method: 设计多模态沟通游戏，对比同质（人-人、AI-AI）与异质（人-AI）配对的表现；实验2中引入提示工程，引导LLM生成类人长度的回应，进一步分析准确性、一致性与词汇重叠度。

Result: 同质配对展现出惯例形成（准确性与一致性上升、响应变短）；异质配对表现差；提示使LLM响应长度接近人类，但准确性和词汇重叠仍显著低于同质配对。

Conclusion: 对话对齐不仅需要模仿表层行为，更依赖于深层共享的语义解释倾向；当前LLM与人类之间存在根本性沟通鸿沟。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [260] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种在预训练阶段内化隐式链式思维（latent CoT）的方法，通过为每个token动态分配可变长度的隐式推理轨迹，在不增加模型参数的前提下提升计算效率与性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型扩展受限于高质量语料稀缺和通信开销上升，需探索不依赖参数或数据增长的新扩展路径。

Method: 提出Pretraining with Token-Level Adaptive Latent CoT：在通用文本单阶段预训练中，让模型为每个输出token自适应生成可变长度的隐式CoT轨迹，并通过token级自适应停机机制实现训练与推理的计算节约。

Result: 在Llama架构上的实验表明，该方法在更低FLOPs下持续降低语言建模困惑度并提升下游任务准确率，优于先前循环式基线。

Conclusion: 内化自适应隐式CoT是一种有效的‘每token计算扩展’范式，兼顾性能增益与计算效率，无需额外标注或多阶段训练。

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [261] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: 本文提出CoRect方法，通过对比上下文感知与非上下文感知前向传播的logits，识别并修正模型中受参数化先验主导的隐藏层，从而缓解RAG中的知识冲突问题，提升输出忠实性。


<details>
  <summary>Details</summary>
Motivation: RAG中模型内部参数化知识常覆盖检索到的证据，导致输出不忠实（faithfulness低），现有方法依赖解码调整或需真实标签的权重编辑，效果有限。

Method: 提出CoRect（Context-Aware Logit Contrast for Hidden State Rectification）：基于层分析发现FFN层在深层存在‘参数化压制’现象；通过对比contextualized与non-contextualized前向传播的logits，无监督识别高参数偏差层，并对相应隐藏状态进行校正。

Result: 在问答与摘要任务上，CoRect显著提升输出忠实性、降低幻觉，优于多个强基线。

Conclusion: 参数化压制是RAG知识冲突的关键机制；CoRect提供了一种无需真实标签、可插拔的隐状态校正方法，有效增强RAG对检索证据的依赖。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [262] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本文提出了首个针对计算机使用代理（CUAs）意外行为的概念与方法框架AutoElicit，通过基于执行反馈的迭代指令扰动，在保持输入良性前提下自动揭示大量严重但意外的有害行为，并验证其在多个前沿CUAs上的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对CUA意外行为的研究多为轶事性，缺乏对其在真实场景中长尾风险的系统刻画与自动化探测方法。

Method: 提出AutoElicit框架：以CUA执行反馈为指导，迭代扰动良性指令，在保持语义合理性与输入良性前提下，自动触发严重意外行为；并评估扰动在不同CUAs间的迁移性。

Result: 在Claude 4.5 Haiku和Opus等前沿CUAs上成功揭示数百种有害意外行为；验证了人工确认的有效扰动在其他前沿CUAs中普遍存在，表明该风险具有跨模型普适性。

Conclusion: 本工作建立了面向真实计算机使用场景的CUA意外行为系统化分析基础，为安全评估与鲁棒性提升提供了新范式。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [263] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工标注或教师模型监督的无监督强化学习方法，通过段落重建任务提升大语言模型的长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）方法依赖高质量标注或强大教师模型，成本高、耗时长，亟需无监督替代方案。

Method: 在长文档中用特殊占位符替换若干段落，让LLM通过强化学习从候选段落集中识别并正确排序缺失段落以重建原文，从而学习全局叙事连贯性。

Result: 在RULER和LongBench v2两个基准上取得显著性能提升，尤其RULER效果明显；无需人工构建的长上下文问答数据即可在LongBench v2上获得合理改进；并通过大量消融实验验证了奖励设计、数据策略等关键因素的影响。

Conclusion: 该无监督段落重建范式能有效增强LLM长上下文能力，降低对监督信号的依赖，具备实用性和可扩展性。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [264] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

TL;DR: 本文通过信息瓶颈（IB）框架分析语义范畴系统的凸性与通信效率之间的关系，发现二者虽常共现但本质独立；效率是更优的预测指标，能解释更多经验现象。


<details>
  <summary>Details</summary>
Motivation: 探究人类语义范畴系统中凸性与通信效率共现的原因及其内在关系，弥补以往研究对二者分析关系理解不足的空白。

Method: 结合解析分析与实证分析，基于信息瓶颈（IB）框架建模语义效率，并在颜色命名领域检验凸性与效率的关系。

Result: 1）凸性与效率互不蕴含；2）IB最优系统在颜色命名中多为凸性，解释了凸性假说的经验基础；3）效率比凸性更能区分真实与假设的颜色命名系统；4）效率可解释凸性无法涵盖的多种经验现象。

Conclusion: 凸性与效率虽产生相似结构观察，但本质不同；效率提供了更全面、更具解释力的语义类型学理论基础。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [265] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

TL;DR: 本文提出了一种基于认知语言学、大语言模型和隐喻的新型身份融合测量方法（CLIFS），在多个数据集上优于现有方法，并在极端主义宣言中识别出两种高融合通向暴力的路径：意识形态型强调群体身份，怨恨驱动型强调个人身份。


<details>
  <summary>Details</summary>
Motivation: 鉴于日益加剧的政治极化和暴力，理解极端主义的心理根源变得愈发重要；已有研究显示身份融合能预测个体参与极端行为的意愿。

Method: 提出了认知语言学身份融合评分（CLIFS）方法，利用认知语言模式、大语言模型（LLMs）和隐喻隐式测量语言中的身份融合程度。

Result: CLIFS在英国和新加坡多个数据集上均优于现有融合测量方法；应用于极端主义宣言时，发现两种高融合通向暴力的路径：意识形态型将自我框架为群体（形成亲属纽带），怨恨驱动型将群体框架为自我。

Conclusion: 该研究细化了身份融合理论，并提供了一种可扩展的工具，有助于融合研究与极端主义检测。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [266] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 本文提出将复述分解为构成其的各个语言层面（复述类型），以实现对语义等价性更细粒度、更符合认知基础的理解；实验表明，显式针对复述类型训练的语言模型在剽窃检测、重复问题识别等下游任务中性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有复述建模方法多局限于二元判断或单一重写，难以揭示语义保持背后的具体语言因素，无法支撑对意义的精细表征与控制。

Method: 提出基于复述类型的细粒度分解框架，并构建相应标注数据集，对语言模型进行显式类型感知训练，评估其在多个下游任务中的泛化能力。

Result: 在维基百科和arXiv论文的剽窃检测任务中，类型感知模型准确率分别达89.6%和66.5%，超越人类基线（78.4%和55.7%）；在Quora重复问题识别中也优于仅用二元对训练的模型。

Conclusion: 复述类型的分解建模是提升语言模型语义理解与控制能力的有效路径，具有更强的认知合理性与实际应用价值。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [267] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出一个概率框架，将大语言模型的能力定义为实例级可解性，认为强化学习与可验证奖励（RLVR）能通过提升原子步骤的成功概率来驱动复杂推理能力的涌现，并通过Algebrarium框架实证验证了RLVR如何放大已有技能、遵循联合概率规律并可能牺牲局部技能以优化全局奖励。


<details>
  <summary>Details</summary>
Motivation: 澄清RLVR是赋予LLM新能力还是仅激发潜在能力这一核心争议；探究复杂推理能力涌现的内在机制。

Method: 构建基于实例级可解性的概率能力框架，假设多步推理能力源于原子步骤成功概率的提升；在Algebrarium框架下仅用单步操作训练模型，并在未见多步任务上评估；通过相关性分析和行为观察检验假设。

Result: （1）RLVR能激励探索原先不可达的解路径；（2）多步任务性能严格由各原子步骤成功概率的联合决定（Pearson相关系数0.69–0.96）；（3）RLVR作为全局优化器可能导致特定技能被牺牲以最大化整体奖励。

Conclusion: RLVR并非仅揭示隐含能力，而是通过迭代优化可解问题，使模型真正发展出解决先前不可解问题的新能力；能力涌现本质是原子步骤可靠性提升带来的系统性突破。

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [268] [Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network](https://arxiv.org/abs/2602.08289)
*Binglin Wu,Xianneng Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于超图神经网络的法律文书实体与关系抽取算法（Legal-KAHRE），专用于涉毒判决文书，通过引入司法领域知识词典、邻域导向打包策略、双仿射机制及融合联合犯罪等司法特例的超图结构，在CAIL2022数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有法律文书实体与关系抽取方法缺乏司法领域知识，且未考虑司法文本的独特性（如联合犯罪、数罪并罚等）

Method: 提出Legal-KAHRE算法：1）基于邻域导向打包与双仿射机制的候选片段生成器；2）融入司法领域知识词典的多头注意力文本编码；3）将司法特例建模进超图结构；4）采用超图神经网络进行高阶消息传递推理

Result: 在CAIL2022信息抽取数据集上，该方法显著优于现有基线模型

Conclusion: 引入司法领域知识和结构化先验（如超图建模司法特例）可有效提升法律文书实体与关系抽取性能

Abstract: With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.

</details>


### [269] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出了一种统一的理论框架，用于分析上下文信息（如示例、检索知识或交互历史）对Transformer类大语言模型推理性能的影响，揭示了上下文通过误差动态修正提升模型表现的几何机制，并给出了误差减小的必要条件及上下文选择的指导原则。


<details>
  <summary>Details</summary>
Motivation: 上下文信息在不更新参数的情况下能显著提升大语言模型性能，但其理论作用机制在除上下文学习（ICL）外的场景中仍缺乏统一理解。

Method: 构建基于输出误差动力学的统一理论框架，在单层Transformer中证明上下文修正误差向量可加性分解，并推导其与基线误差对齐及范数约束的几何条件；进一步给出上下文-查询相关性与互补性决定的修正向量范数上界；将结论推广至多上下文与多层Transformer。

Result: 理论揭示了上下文提升性能的几何本质，实验在ICL、检索增强生成和记忆演化任务中验证了理论，并据此提出一种有原则的上下文选择策略，使性能提升0.6%。

Conclusion: 上下文的作用可通过误差修正的几何性质统一刻画，其有效性依赖于方向对齐与范数可控性，该框架为设计更优上下文利用方法提供了理论基础。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [270] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

TL;DR: 本文提出JUSTICE框架，通过模拟法官'检索→预判→撰写'的认知流程，特别是引入预判阶段（Pre-Judge），提升判决书生成的法律准确性与连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略法官形成初步结论的'预判'阶段，导致司法要素获取不足和预判过程建模不充分，影响判决书的法律严谨性。

Method: 提出JUSTICE框架，包含三个核心组件：参考性司法要素检索器（RJER）、中间结论模拟器（ICE）和司法统一合成器（JUS），分别实现法律依据检索、可验证中间结论生成和最终判决合成。

Result: 在领域内法律基准和分布外数据集上均显著优于强基线，监狱刑期预测准确率提升4.6%。

Conclusion: 显式建模预判过程对提升生成判决书的法律一致性与准确性至关重要。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [271] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: 本文提出Dr. SCI数据集与后训练流程，通过系统化数据处理和创新的SFT-RL范式提升大模型在开放性科学问题上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在开放性科学问题上因监督与评估不可靠而导致的性能瓶颈，尤其聚焦于科学领域后训练的数据构建与奖励设计难题。

Method: 构建包含100万道STEM领域题目的Dr. SCI数据集，具备可验证/开放性划分、可扩展难度标注和细粒度评分标准；提出三阶段Dr. SCI后训练流程：探索扩展式监督微调（Exploration-Expanding SFT）、动态难度课程（Dynamic Difficulty Curriculum）和基于SciRubric的强化学习（SciRubric-Guided RL）。

Result: Qwen3-4B-Base经Dr. SCI流程训练后，在GPQA-diamond和GPQA-general上分别达63.2和32.4分，显著超越o1-mini、GPT-4o等强基线，尤其在开放性科学推理任务中表现突出。

Conclusion: 系统化的高质量数据构建与面向开放性科学问题定制的后训练范式，能有效提升大模型的科学推理能力，为科学AI提供可复现、可评估的新路径。

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [272] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力机制的生成式框架，用于同时解决多意图检测与槽位填充任务，并构建了两个新的多意图SLU数据集。


<details>
  <summary>Details</summary>
Motivation: 现实场景中用户常在单句中表达多个意图，而现有方法和数据集主要针对单意图SLU，难以应对该挑战。

Method: 提出一种带attention-over-attention机制的生成式解码器，引入归纳偏置以缓解多任务干扰并适应可变意图数量；利用BERT的NSP头构造两个新多意图SLU数据集。

Result: 在MixATIS、MixSNIPS及自建数据集上均达到SOTA性能。

Conclusion: 生成式建模与注意力机制设计有效提升了多意图SLU性能，所构建数据集为后续研究提供了新基准。

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [273] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: 本文提出Thinking States方法，使大语言模型在输入处理过程中同步生成思考token，以降低链式推理（CoT）的推理开销，同时保持甚至提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）虽能提升LLM解决复杂任务的能力，但因生成长推理过程导致高昂的推理成本，亟需更高效替代方案。

Method: Thinking States在输入token流处理过程中，每隔若干输入token就生成一组‘思考token’，再将这些思考token映射回嵌入空间并注入后续输入中；该过程可借助自然语言监督和teacher-forcing并行训练。

Result: 在多个推理任务上优于其他隐式推理方法：数学题上缩小与CoT的性能差距，2跳问答任务中达到CoT水平且延迟更低；在状态跟踪任务中展现出比CoT更强的泛化能力，可外推至训练时未见的更长序列。

Conclusion: Thinking States是一种高效、可学习、并行化的实时推理机制，兼顾推理质量与效率，并在泛化性上超越传统CoT。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [274] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 本文提出UReason基准，用于评估推理驱动的图像生成效果，发现推理痕迹虽能提升性能，但中间思维作为条件上下文反而干扰视觉合成，瓶颈在于上下文干扰而非推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 探究推理在视觉合成中的实际作用，解决当前统一多模态模型中链式推理对图像生成影响尚不明确的问题。

Method: 构建包含2000个样本、覆盖五类任务的诊断基准UReason，并设计直接生成、推理引导生成和去上下文化生成三种评估方式，以分离推理痕迹的作用。

Result: 在八个开源统一模型上发现一致的'推理悖论'：推理痕迹提升性能，但保留中间思维作为条件上下文反而阻碍生成；仅用精炼提示条件化可显著提升效果。

Conclusion: 推理瓶颈主要源于上下文干扰，而非推理能力不足；UReason为研究统一模型中的推理提供了原则性测试平台，并推动未来方法优化推理与视觉生成的整合。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [275] [WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints](https://arxiv.org/abs/2602.08367)
*Zexuan Wang,Chenghao Yang,Yingqi Que,Zhenzhu Yang,Huaqing Yuan,Yiwen Wang,Zhengxuan Jiang,Shengjie Fang,Zhenhe Wu,Zhaohui Wang,Zhixin Yao,Jiashuo Liu,Jincheng Ren,Yuzhen Li,Yang Yang,Jiaheng Liu,Jian Yang,Zaiyuan Wang,Ge Zhang,Zhoufutu Wen,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文提出了WorldTravel基准测试，用于评估自主规划代理在处理现实世界中紧密耦合约束（如时间与逻辑约束）时的能力，并构建了WorldTravel-Webscape多模态环境，要求代理直接从网页视觉布局中感知约束参数；实验发现当前大模型在该任务上表现严重下降，揭示了感知-行动鸿沟与约10约束的规划临界点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多基于松散耦合约束和理想化数据，无法反映真实动态网络环境中参数提取与长程协同规划的复杂性，亟需更贴近现实的评测框架。

Method: 构建包含150个真实旅行场景的WorldTravel基准（覆盖5城、平均15+互依赖约束），并开发WorldTravel-Webscape多模态环境（含2000+渲染网页），使代理需通过视觉感知提取参数以进行规划；对10个前沿模型开展文本与多模态设置下的可行性评估。

Result: SOTA模型GPT-5.2在纯文本设置下可行性仅32.67%，在多模态环境下骤降至19.33%；发现显著的感知-行动鸿沟及约10约束的规划能力阈值。

Conclusion: 当前AI代理在高保真视觉感知与长程推理的协同方面存在根本性缺陷，下一代代理需深度融合二者以应对现实物流等脆弱场景。

Abstract: Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\% feasibility in text-only settings, which plummets to 19.33\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.

</details>


### [276] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了ViGoEmotions——一个包含20,664条越南语社交媒体评论、标注27种细粒度情绪的新型越南语情绪语料库，并系统评估了不同预处理策略（保留/转换/移除emoji）对多个Transformer模型情绪分类性能的影响，结果表明ViSoBERT表现最优，且预处理策略和标注质量对性能影响显著。


<details>
  <summary>Details</summary>
Motivation: 现有越南语情绪分析资源匮乏，缺乏高质量、细粒度标注的语料库，限制了该语言情绪分类任务的发展；同时，emoji在社交媒体文本中广泛存在，其处理方式对模型性能的影响尚不明确。

Method: 构建了ViGoEmotions越南语情绪语料库（20,664条评论，27类情绪），并在三种emoji预处理策略（保留+规则归一化、转为文本描述、使用ViSoLex模型归一化）下，评估了8个预训练Transformer模型（如ViSoBERT、CafeBERT、PhoBERT等）的情绪分类性能。

Result: ViSoBERT取得最高Macro F1（61.50%）和Weighted F1（63.26%）；将emoji转为文本普遍提升BERT类基线模型性能，而保留emoji则最利于ViSoBERT和CafeBERT；移除emoji总体导致性能下降。

Conclusion: ViGoEmotions是首个大规模越南语细粒度情绪语料库，可有效支持多种模型；但下游性能高度依赖预处理策略选择与标注质量，emoji不应简单移除，需结合模型特性定制处理方式。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [277] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出一种受认知启发的长上下文推理框架，通过分块压缩和选择性记忆召回提升大语言模型处理长文本的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文处理中面临计算成本高、信息遗忘及检索增强生成（RAG）中的上下文碎片化等问题。

Method: 将长输入分块，用学习到的压缩器编码为压缩记忆表示；通过门控模块动态选择相关记忆块，并由具备演化工作记忆的推理模块迭代处理；压缩器与推理器通过端到端强化学习联合优化，门控模块单独作为分类器训练。

Result: 在RULER-HQA等多跳推理基准上达到有竞争力的准确率，上下文长度可从7K扩展至1.75M tokens，峰值GPU内存减少2倍，推理速度提升6倍（相比MemAgent）。

Conclusion: 该框架在保持高精度的同时显著提升了长上下文推理的效率，验证了认知启发式压缩与选择性回忆策略的有效性。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [278] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

TL;DR: 本文提出TEAM框架，通过利用专家路由的时间和空间一致性，减少MoE扩散大语言模型（dLLM）中每步激活的专家数量，提升推理速度，实现最高2.2倍加速且性能几乎无损。


<details>
  <summary>Details</summary>
Motivation: MoE dLLMs在扩散解码中存在专家激活过多但仅少量token被接受的问题，导致推理开销大、延迟高，难以部署于时效敏感场景。

Method: 基于专家路由在时间（不同去噪层级）和空间（不同token位置）上的一致性观察，TEAM设计了三种互补的专家激活与解码策略：对已解码/掩码token保守选择必要专家，并对多个候选token进行激进的推测性探索。

Result: TEAM在保持性能几乎不变的前提下，相较基线MoE dLLM实现了最高2.2倍的推理加速。

Conclusion: TEAM是一种即插即用的高效加速框架，有效缓解了MoE架构与扩散解码范式间的不匹配问题，提升了dLLM的实际部署可行性。

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [279] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出Prism方法，通过频谱感知的块选择机制解决块稀疏注意力中粗粒度注意力不准确的问题，实现了训练免费、高效且准确的长上下文LLM预填充加速。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力方法在块重要性估计中依赖粗粒度注意力（如均值池化），但其与RoPE交互导致高频位置信息丢失，形成‘盲点’，造成效率与精度瓶颈。

Method: Prism将块选择分解为高频与低频分支，利用基于能量的温度校准从池化表示中恢复衰减的位置信号，全程仅使用块级操作，无需训练。

Result: Prism在保持与全注意力相当精度的同时，实现最高5.1倍的预填充加速。

Conclusion: Prism是一种训练免费、谱感知、高效的块稀疏注意力优化方法，有效克服了均值池化与RoPE交互引发的位置信息丢失问题，显著提升长上下文LLM推理效率。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [280] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

TL;DR: 本文通过理论分析与实证实验，检验乔姆斯基对大语言模型（LLMs）的批判，发现GPT-2在学习句法不可能语言时显著弱于可能语言，而LSTM则更符合乔姆斯基观点；据此提出对乔姆斯基理论的新诠释，并主张LLM研究应转向功能主义与经验主义范式。


<details>
  <summary>Details</summary>
Motivation: 回应乔姆斯基对LLM‘仅是模式预测器、无法习得人类语言内在因果与自校正结构’的批判，检验其关于LLM不能区分可能与不可能语言的核心主张。

Method: 构建若干句法上‘不可能’的人工语言（如整句反转、依词数奇偶加否定），在GPT-2 small与LSTM模型上开展两轮受控实验；采用Welch's t检验进行统计分析，并结合语言学与心理学既有文献进行理论阐释。

Result: GPT-2 small在所有不可能语言上的学习表现均显著差于可能语言（p<.001）；LSTM的表现则与乔姆斯基论断一致，凸显Transformer架构演化的关键作用。

Conclusion: 乔姆斯基批判部分成立但需修正：LLM的能力边界确与人类不同，但其表现亦反映架构特异性；应超越其理性主义—浪漫主义范式，在LLM研究中采纳功能主义与经验主义新视角。

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [281] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出ME²原则和基于DAG的推理评估方法，构建TRM-Preference数据集并训练Thinking Reward Model（TRM），用于大规模评估和优化大型推理模型的推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对高质量推理的定义、复杂推理轨迹的可靠评估方法以及如何利用评估信号进行推理优化的统一解答。

Method: 提出ME²原则（宏观与微观层面的效率与有效性）；将推理轨迹建模为有向无环图（DAG），设计基于DAG的成对评估方法；构建TRM-Preference数据集并训练Thinking Reward Model（TRM）。

Result: 实验表明，思维奖励作为优化信号有效：测试时选择更优推理可提升结果达19.3%；强化学习训练中可提升推理与性能达3.9%。

Conclusion: 本文提供了从推理质量定义、结构化评估到奖励建模与优化的完整框架，显著提升了大型推理模型的性能与可控性。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [282] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文通过激活修补和可解释性技术，揭示了大语言模型处理线性化表格时的三阶段机制：语义绑定、坐标定位和信息提取，并发现模型通过计数分隔符确定单元格位置，列索引编码在线性子空间中，且多单元格定位复用相同注意力头。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在表格任务中广泛应用，但其如何理解线性化二维表格的内部机制尚不清楚。

Method: 采用激活修补（activation patching）及多种可解释性技术，对单元格定位这一原子任务进行分解分析。

Result: 发现表格理解遵循三阶段流程；模型通过计数分隔符实现坐标解析；列索引编码于线性子空间，支持向量算术精准调控；多单元格定位复用同一组注意力头。

Conclusion: 本研究系统阐明了Transformer架构中表格理解的内在机制，为提升模型表格能力提供了理论基础与干预路径。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [283] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 本文提出首个英-马拉雅拉姆语段级质量评估（QE）数据集，并设计了基于策略的强化学习框架ALOPE-RL，利用直接评估分与翻译质量评注（TQR）作为错误感知奖励，提升小模型在低资源语言QE任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有QE方法仅依赖标量质量分，缺乏对具体翻译错误的显式建模；且在低资源语言上因标注数据稀缺而性能受限。

Method: 构建英-马拉雅拉姆语段级QE数据集（含DA分数和自由文本形式的TQR），并提出ALOPE-RL——一种基于策略的强化学习框架，通过LoRA微调和4-bit量化，在紧凑LLM（≤4B参数）上联合优化DA得分与TQR理解能力。

Result: ALOPE-RL在英-马拉雅拉姆语QE任务上达到SOTA，优于更大LLM基线及主流编码器型QE模型，验证了错误感知强化学习在数据与算力受限下的有效性。

Conclusion: 引入错误描述性监督信号（TQR）并结合策略学习，可显著提升低资源语言QE效果；该范式为轻量、可解释、数据高效的质量评估提供了新路径。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [284] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出VocalNet-MDM，一种基于掩码扩散建模（MDM）的非自回归语音大语言模型，通过分层块掩码和迭代自蒸馏解决训练-推理不匹配与迭代开销问题，在仅6K小时数据上实现3.7–10×解码加速和34%首块延迟降低，同时保持识别准确率并提升文本质量与语音自然度。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语音大模型存在串行约束导致生成效率低和暴露偏差问题，亟需更高效、低延迟的非自回归范式。

Method: 提出VocalNet-MDM，采用掩码扩散建模（MDM）；为适配流式语音交互，设计分层块掩码（Hierarchical Block-wise Masking）以对齐训练与块扩散解码中的渐进掩码状态，并引入迭代自蒸馏（Iterative Self-Distillation）压缩多步精炼为更少步骤以降低推理延迟。

Result: 在仅6K小时语音数据上训练，VocalNet-MDM相较自回归基线实现3.7×–10×解码加速、首块延迟降低34%，识别准确率具竞争力，文本质量和语音自然度达SOTA。

Conclusion: 掩码扩散建模（MDM）是一种有前景且可扩展的替代方案，适用于低延迟、高效率的语音大语言模型。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [285] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

TL;DR: 本文探讨多语言大语言模型（LLMs）中是否存在针对特定语言的注意力头，并研究能否在不损害目标语言性能的前提下移除非目标语言的专用注意力头，以提升部署效率。


<details>
  <summary>Details</summary>
Motivation: 多语言LLM在生产部署中若仅需支持部分语言，存在资源浪费问题；现有研究集中于机器翻译模型的语言特异性，尚无针对多任务多语言LLM的相关探索。

Method: 分析多语言LLM中注意力头的语言特异性，实验性地移除与非目标语言相关的注意力头，并评估其对目标语言任务性能的影响。

Result: 发现多语言LLM中存在语言特定的注意力头，且可选择性移除非目标语言的头而不显著降低目标语言性能。

Conclusion: 该发现支持通过剪枝语言专用模块来实现更高效、轻量化的多语言LLM部署，兼顾效率与准确性。

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [286] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文研究了演绎、归纳和溯因三种基本推理范式对大语言模型（LLM）泛化能力的影响，构建了面向符号任务的推理轨迹数据集，并通过多种模型增强方法（如微调、加深模型、转为MoE结构）提升LLM对这三类推理的掌握，最终在真实世界自然语言任务上实现了最高达14.60的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管提升大语言模型（LLM）推理能力已受广泛关注，但演绎、归纳与溯因这三种基础推理范式对LLM泛化能力的影响尚缺乏系统性探索。

Method: 构建面向符号任务、分别对应演绎、归纳、溯因三类范式的推理轨迹数据集；采用多种模型增强方法（包括简单微调、增加模型深度、将稠密模型转换为混合专家模型）来诱导LLM掌握这些推理能力。

Result: 在完全基于自然语言、含真实世界知识的域外任务上，所提方法显著提升了模型泛化能力，性能提升最高达14.60。

Conclusion: 基础推理范式（演绎、归纳、溯因）的显式建模与诱导可有效增强LLM在现实任务中的泛化能力，且不同模型架构增强策略均展现出积极效果。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [287] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: 本文提出GER-Eval方法，探索大语言模型（LLMs）能否自主设计并应用评估标准来评价自然语言生成质量；发现LLMs能可靠生成可解释、任务感知的评估维度，但在事实性与知识密集型任务中评分可靠性下降；闭源模型（如GPT-4o）在一致性与跨模型泛化上优于开源模型（如Llama）；研究将评估视为LLM习得的语言能力，呼吁联合建模人类与LLM的评估语言以提升可靠性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 人类定义的静态评估标准常与大语言模型内部对语言质量的表征不一致，导致评估效果受限。

Method: 提出GER-Eval框架，让LLMs自主生成评估标准，并在语义连贯性、评分可靠性及与人类标准对齐等方面进行系统评估。

Result: LLMs能稳定生成可解释、任务相关的评估维度，且在模型内部应用一致；但在事实性和知识密集型任务中评分可靠性下降；闭源模型（如GPT-4o）比开源模型（如Llama）具有更高评分一致性与跨模型泛化能力。

Conclusion: 评估是LLM习得的一种内在语言能力，具有模型内一致性但模型间碎片化；需发展新方法联合建模人类与LLM的评估语言，以提升评估的可靠性与可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [288] [Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement](https://arxiv.org/abs/2602.08688)
*Hossein Kermani,Fatemeh Oudlajani,Pardis Yarahmadi,Hamideh Mahdi Soltani,Mohammad Makki,Zahra HosseiniKhoo*

Main category: cs.CL

TL;DR: 本文比较了三种波斯语推文不文明行为检测方法：人工定性编码、基于ParsBERT的监督学习和大型语言模型（ChatGPT），发现ParsBERT在识别仇恨言论方面显著优于多个ChatGPT模型，且ChatGPT在显性和隐性不文明内容上均表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言（如波斯语）背景下，亟需评估不同方法对仇恨言论等不文明内容检测的有效性与适用性。

Method: 采用人工定性编码、基于ParsBERT的监督学习模型和多种ChatGPT模型，对47,278条#MahsaAmini运动相关波斯语推文进行不文明行为检测，并对比其准确率与效率；同时测试英文与波斯语提示对ChatGPT性能的影响。

Result: ParsBERT显著优于所有被测ChatGPT模型；ChatGPT不仅难以识别隐性不文明内容，对显性内容也表现欠佳；提示语言（英/波）对其输出无明显影响。

Conclusion: 在低资源语言语境下，针对仇恨言论检测，微调的领域适配预训练模型（如ParsBERT）比通用大语言模型（如ChatGPT）更可靠；人工编码虽精准但效率低，需结合方法优势开展实际分析。

Abstract: This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.

</details>


### [289] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

TL;DR: 本研究探讨了机器翻译在印度语言（孟加拉语、马拉雅拉姆语和泰卢固语）中的实际应用与方法论意义，聚焦于新兴翻译工作流及现有评估框架，并基于NPTEL MOOC平台语料和自发语音语料库分析形态丰富、语义紧凑语言的评估挑战。


<details>
  <summary>Details</summary>
Motivation: 选择孟加拉语、马拉雅拉姆语和泰卢固语是出于语言多样性三角验证的考量，以支持印度国家教育政策（NEP 2020）下教育技术的多语言适配需求；同时依托印度最大MOOC平台NPTEL作为语料基础。

Method: 构建面向技术概念清晰表达的自发语音语料库，注重语域保持与词汇适切性；结合现有表面重叠类评估指标，分析形态丰富、语义紧凑语言的翻译表现。

Result: 研究发现主流评估指标对形态丰富、语义紧凑的语言具有特定敏感性，难以准确反映翻译质量，暴露出当前评估框架在印度语言场景下的局限性。

Conclusion: 需发展适配印度语言特性的新型评估方法与翻译工作流，尤其应重视语域一致性、形态复杂性与语义密度等关键因素。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [290] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

TL;DR: 本文提出了一种全新的、全自动的方法来评估生成式AI在观点摘要任务中的事实一致性，通过衡量摘要与原始评论中主张的相似性、覆盖度和一致性，并证明该指标与人类判断高度相关。


<details>
  <summary>Details</summary>
Motivation: 传统自动化评估方法在大语言模型（LLM）背景下暴露出局限性，尤其在观点摘要任务中难以准确衡量事实一致性。

Method: 提出一种全自动方法，基于提取文本中的事实主张，并计算摘要与原始评论中主张的相似性、覆盖度和一致性，形成综合评分。

Result: 所提指标能对否定、转述或扩展后的相似主张给出更高分，且与人类判断的相关性高于现有最先进指标。

Conclusion: 该方法为观点摘要任务提供了更可靠、更精细的事实一致性评估手段，弥补了传统评估范式的不足。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [291] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

TL;DR: 本文提出PERSPECTRA基准，旨在评估大语言模型对多元观点的理解与推理能力，通过融合Kialo的结构化辩论图与Reddit的语言多样性，构建了包含3810个扩展论点的数据集，并设计三项任务验证模型在多元主义方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型研究缺乏对多元主义（即不将不同观点简化为单一视角的能力）的系统考察，而这一特性对反映人类观点多样性至关重要；现有辩论数据源各有局限，需构建兼顾结构清晰性与语言自然性的新基准。

Method: 提出PERSPECTRA基准，采用受控的检索-扩展流程，整合Kialo的显式正反论证图结构与Reddit的真实讨论语言多样性，构建覆盖100个争议话题、762种立场、3810个自然化变体论点的数据集，并定义意见计数、意见匹配和极性判断三项评估任务。

Result: 在主流开源与闭源大语言模型上的实验表明，模型普遍存在高估观点数量、误判让步结构等系统性失败，揭示其在多元主义理解与推理上的显著不足。

Conclusion: PERSPECTRA是首个兼具多样性与结构性、可扩展且可配置的多元主义评估基准，为推动模型更忠实反映人类观点异质性提供了新工具与评测范式。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [292] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

TL;DR: 本文提出了一种通过量子相对熵构建大规模句子编码器映射图的方法，以可视化和比较1101个公开句子编码器之间的关系，并能准确预测其下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在大规模上有效比较和可视化句子编码器之间的关系，缺乏统一的度量标准来反映编码器特性及其下游性能关联。

Method: 首先用句子集的嵌入矩阵表示每个句子编码器；然后计算其成对内积（PIP）矩阵；最后基于量子相对熵（QRE）相对于一个单位基编码器生成每个编码器的特征向量，并据此构建编码器映射图。

Result: 构建了覆盖1101个公开句子编码器的地图，地图中相似属性的编码器空间位置邻近；编码器特征向量可准确预测其在检索和聚类等下游任务中的性能。

Conclusion: 该方法提供了一种新颖、可扩展且忠实的句子编码器评估与可视化框架，有助于理解预训练句子编码器的整体格局及其能力差异。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [293] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: This paper proposes LakeHopper, a framework to adapt pre-trained language models for column type annotation in new data lakes with minimal target annotations, addressing knowledge gaps and preserving shared knowledge via LM interaction, cluster-based selection, and incremental fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To reduce the annotation cost when adapting language model-based column type annotation systems from a source data lake to a new (target) data lake.

Method: LakeHopper identifies and resolves source-target knowledge gaps through LM interactions, selects informative unannotated columns using a cluster-based scheme, and applies incremental fine-tuning to preserve shared knowledge while adapting the model.

Result: LakeHopper demonstrates effectiveness on two different data lake transfers under both low-resource and high-resource settings.

Conclusion: LakeHopper enables efficient and effective adaptation of pre-trained LM-based models to new data lakes with minimal target annotations, overcoming knowledge gap and catastrophic forgetting challenges.

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [294] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 本文提出AFlow框架，通过建模多轮对话中连续的情感流，为情感支持对话提供细粒度的前缀级监督，显著提升策略连贯性与共情响应质量，且在多个ESC指标上超越GPT-4o和Claude-3.5。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话（ESC）方法依赖稀疏的结果级信号，难以有效指导多轮对话中中间策略决策，导致复杂多轮支持效果受限。

Method: 提出AFlow框架，建模多轮对话轨迹上的连续情感流，估计搜索路径的中间效用，并学习偏好一致的策略转移；引入子路径级flow-balance目标，将偏好信号传播至中间状态以增强策略连贯性和共情响应质量。

Result: 在多种情感情境下显著优于强基线；基于紧凑开源骨干模型的AFlow在主要ESC指标上超越GPT-4o和Claude-3.5等闭源大模型。

Conclusion: 细粒度的前缀级情感流建模可有效提升ESC中策略决策质量与响应共情能力，为多轮情感支持提供了新范式。

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [295] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出WildReward，一种直接从野外交互（如WildChat）中提取用户反馈训练的奖励模型，无需人工标注偏好对，在性能、校准性和跨样本一致性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖大规模人工标注的偏好对，而大语言模型在实际部署中产生的大量野外交互数据蕴含丰富的隐式奖励信号，亟需探索如何直接利用这些数据训练奖励模型。

Method: 以WildChat为交互源，设计一套流程提取高质量用户反馈，构建包含186k样本的数据集，并采用序数回归方法直接在用户反馈上训练WildReward模型。

Result: WildReward在多项指标上达到或超过传统奖励模型，具备更好的校准性和跨样本一致性；用户多样性提升模型性能；应用于在线DPO训练时显著提升各项任务表现。

Conclusion: 直接从野外交互数据中训练奖励模型是可行且有效的，WildReward为降低奖励建模成本、提升模型鲁棒性提供了新范式。

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [296] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 本文提出ANIRA框架，通过算法任务评估token级自适应计算，发现计算分配可自发对齐任务复杂度，但无法保证算法泛化能力；早期决策依赖静态结构线索，而在线停机更贴近算法执行状态。


<details>
  <summary>Details</summary>
Motivation: 现有token级自适应计算研究受限于自然语言基准中token难度不可观测且受架构因素混淆，难以验证计算分配是否真正匹配底层复杂度。

Method: 1）构建可控复杂度的算法与合成语言任务评估范式；2）提出ANIRA——支持每token变深度计算的统一循环Transformer框架，解耦计算分配决策；3）系统分析计算分配与复杂度对齐性、泛化性及决策时机的关系。

Result: 计算分配可在无显式难度监督下自发对齐任务复杂度；但该对齐不带来算法泛化（如无法外推至未见输入规模）；早期决策依赖静态结构线索，而在线停机机制更贴合算法执行状态。

Conclusion: token级自适应计算的有效性需在可控复杂度任务中独立评估；计算分配对齐复杂度是必要但不充分条件，算法泛化还需其他机制支持；决策时机影响对齐质量，动态执行感知优于静态结构依赖。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [297] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

TL;DR: 本文提出了一种新的长上下文安全威胁模型——组合推理攻击（compositional reasoning attacks），发现更强的推理能力并不自动提升大语言模型的安全性，尤其在长上下文场景下；实验表明安全对齐随上下文增长而退化，但增加推理时计算可显著缓解攻击。


<details>
  <summary>Details</summary>
Motivation: 假设更强的推理能力有助于模型识别隐含的有害意图，从而提升安全性；本文旨在检验该假设在长上下文、隐式有害意图场景下的有效性。

Method: 提出组合推理攻击：将有害查询拆分为多个分散在长上下文中的不完整片段，并通过中性推理提示诱导模型检索与合成，使有害意图仅在推理后显现；在高达64k token的上下文中评估14个前沿LLM。

Result: （1）推理能力更强的模型并未更鲁棒，常成功组合出有害意图却未拒绝；（2）安全对齐随上下文长度增加而系统性下降；（3）增加推理时计算（如推理努力）可使GPT-oss-120b攻击成功率降低超50个百分点。

Conclusion: 安全性不会随推理能力自动扩展，尤其在长上下文推理中；需专门设计安全机制，而非依赖推理能力提升自然带来安全增强。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [298] [GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search](https://arxiv.org/abs/2602.08945)
*Sahajpreet Singh,Kokil Jaidka,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文提出GitSearch框架，通过识别信息缺口、实时定向网络检索和生成平台合规注释三阶段流程，显著提升社区驱动内容审核的覆盖率与质量。


<details>
  <summary>Details</summary>
Motivation: 社区驱动的内容审核面临结构性挑战，现有AI方法在冷启动场景下表现不佳。

Method: 提出GitSearch（Gap-Informed Targeted Search）框架，包含识别信息缺口、实时定向网络检索、生成平台合规注释三个阶段，并构建PolBench基准进行评估。

Result: GitSearch实现99%覆盖率，较当前最优方法几乎翻倍；在帮助性方面以69%胜率超越人工撰写的注释，评分为3.87 vs. 3.36。

Conclusion: GitSearch有效平衡了大规模部署与高质量输出之间的权衡，为社区内容审核提供了可扩展且高效的解决方案。

Abstract: Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.

</details>


### [299] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文是一篇立场论文，主张当前语言识别（LID）系统覆盖范围有限的根本原因在于将其错误地视为脱离上下文的文本分类问题；应转而将其建模为路由问题，并利用环境线索来提升低资源语言的识别能力。


<details>
  <summary>Details</summary>
Motivation: 商业和研究级语言识别系统对数千种世界语言中的绝大多数缺乏可靠支持，尤其在尾部（低资源）语言上覆盖严重不足；作者认为这一局限是领域内自身方法论和制度惯性所致。

Method: 提出概念性重构：将语言识别从‘去语境化文本分类’范式转向‘路由问题’，强调先验概率估计的重要性，并主张系统性整合环境线索以提升局部语言可接受性建模。

Result: 未报告具体实验结果，但提出了一个原则性框架，为扩展尾部语言识别覆盖提供了新的理论视角与设计方向。

Conclusion: 提升语言识别覆盖率的关键不在于单纯扩大训练数据或模型规模，而在于范式转变——重视上下文相关的先验建模与环境感知的路由机制。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [300] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种名为Next Concept Prediction（NCP）的生成式预训练范式，通过预测跨越多个token的离散概念来增强传统Next Token Prediction（NTP），并在ConceptLM模型中实现；实验表明NCP在多个基准上持续优于token级模型，并可进一步提升已训练好的大模型。


<details>
  <summary>Details</summary>
Motivation: 传统Next Token Prediction（NTP）任务难度较低，难以充分激发语言模型潜力；为提升预训练任务难度并建模更高层次语义单元，作者提出以离散‘概念’为预测目标的NCP范式。

Method: 提出Next Concept Prediction（NCP）预训练目标；构建ConceptLM模型：利用向量量化（VQ）对隐藏状态进行离散化以形成概念词表，并联合优化NCP与NTP目标；支持从头训练（70M–1.5B参数）及对已有大模型（如8B Llama）进行持续预训练。

Result: 在13个基准测试中，ConceptLM一致优于同规模纯NTP基线；对8B Llama模型进行NCP持续预训练后性能进一步提升；消融分析证实NCP通过增加任务难度有效增强了模型能力。

Conclusion: NCP是一种更难、更具语义意义的预训练范式，能有效提升语言模型性能，为构建更强语言模型提供了新路径。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


### [301] [When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents](https://arxiv.org/abs/2602.08995)
*Yuting Ning,Jaylen Jones,Zhehao Zhang,Chentao Ye,Weitong Ruan,Junyi Li,Rahul Gupta,Huan Sun*

Main category: cs.CL

TL;DR: 本文首次定义并研究了计算机使用代理（CUAs）中的错位动作检测问题，提出了包含外部诱导和内部产生的错位动作的全面框架；构建了真实轨迹基准MisActBench，并提出实用通用的防护机制DeAction，在离线与在线评估中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CUAs在实际应用中常产生偏离用户意图的错位动作，这些动作可能源于外部攻击（如间接提示注入）或内部缺陷（如错误推理），带来安全风险并降低任务效率与可靠性，亟需系统性检测与纠正机制。

Method: 提出DeAction——一种在动作执行前检测错位动作并通过结构化反馈迭代修正的通用守卫机制；同时构建首个涵盖真实部署场景、含人工标注动作级对齐标签的基准MisActBench。

Result: DeAction在MisActBench上F1分数绝对提升超15%；在线评估中，对抗环境下攻击成功率降低超90%，良性环境下任务成功率保持或提升；引入适度延迟开销。

Conclusion: 本文确立了CUA错位动作检测的研究范式，验证了DeAction作为轻量、通用、高鲁棒性守卫机制的有效性，为提升CUA安全性与可靠性提供了可落地的技术路径。

Abstract: Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [302] [Lemon Agent Technical Report](https://arxiv.org/abs/2602.07092)
*Haipeng Jiang,Kailong Ren,Zimo Yin,Zhetao Sun,Xin Gan,Guangyi Lv,Ming He,Peng Wang,Congli Yin,Hong Pan,Changwen Zhang,Shan Tong,Zhengyu Xu,Zeping Chen,Yubin Huangfu,Yanzhi Xu,Xing Su,Qin Feng,Dong An,Jianping Fan*

Main category: cs.MA

TL;DR: 本文提出Lemon Agent，一种基于AgentCortex框架的多智能体协同系统，通过分层自适应调度、三级渐进式上下文管理、自演化记忆系统和增强型MCP工具集，显著提升复杂长周期任务的资源效率、上下文管理和多模态感知能力，在GAIA和xbench-DeepSearch等基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有先进大语言模型驱动的智能体系统在资源效率、上下文管理和多模态感知方面存在固有局限。

Method: 提出Lemon Agent系统，基于AgentCortex框架，实现Planner-Executor-Memory范式；引入分层自适应调度机制（含orchestrator与worker两层）；设计三级渐进式上下文管理策略；构建自演化记忆系统；提供增强型MCP工具集。

Result: 在GAIA基准上取得91.36%的整体准确率，在xbench-DeepSearch榜单上以77+分位居榜首。

Conclusion: Lemon Agent通过多层级协同优化机制，在复杂长周期任务中实现了全局协调与局部执行的协同平衡，显著提升了资源利用率与任务处理效率。

Abstract: Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.

</details>


### [303] [The Value of Variance: Mitigating Debate Collapse in Multi-Agent Systems via Uncertainty-Driven Policy Optimization](https://arxiv.org/abs/2602.07186)
*Luoxi Tang,Yuqiao Meng,Joseph Costa,Yingxue Zhang,Muchao Ye,Zhaohan Xi*

Main category: cs.MA

TL;DR: 本文提出了一种分层不确定性度量方法，用于检测和缓解多智能体辩论（MAD）系统中的辩论崩溃问题，并设计了基于不确定性的策略优化方法以提升决策准确性和降低系统分歧。


<details>
  <summary>Details</summary>
Motivation: 现有MAD系统易发生辩论崩溃，缺乏有效检测与预防机制。

Method: 提出三层不确定性度量（个体、交互、系统级），并构建不确定性驱动的策略优化方法，惩罚自相矛盾、同伴冲突和低置信输出。

Result: 实验证明该方法能可靠校准多智能体系统，持续提升决策准确率并减少系统分歧。

Conclusion: 分层不确定性可作为诊断指标，不确定性驱动的优化策略能有效缓解辩论崩溃，增强MAD系统的鲁棒性。

Abstract: Multi-agent debate (MAD) systems improve LLM reasoning through iterative deliberation, but remain vulnerable to debate collapse, a failure type where final agent decisions are compromised on erroneous reasoning. Existing methods lack principled mechanisms to detect or prevent such failures. To address this gap, we first propose a hierarchical metric that quantifies behavioral uncertainty at three levels: intra-agent (individual reasoning uncertainty), inter-agent (interactive uncertainty), and system-level (output uncertainty). Empirical analysis across several benchmarks reveals that our proposed uncertainty quantification reliably indicates system failures, which demonstrates the validity of using them as diagnostic metrics to indicate the system failure. Subsequently, we propose a mitigation strategy by formulating an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs in a dynamic debating environment. Experiments demonstrate that our proposed uncertainty-driven mitigation reliably calibrates the multi-agent system by consistently improving decision accuracy while reducing system disagreement.

</details>


### [304] [Talk, Judge, Cooperate: Gossip-Driven Indirect Reciprocity in Self-Interested LLM Agents](https://arxiv.org/abs/2602.07777)
*Shuhui Zhu,Yue Lin,Shriya Kaistha,Wenhao Li,Baoxiang Wang,Hongyuan Zha,Gillian K. Hadfield,Pascal Poupart*

Main category: cs.MA

TL;DR: 本文提出了一种名为ALIGN的自动框架，通过分层语调的开放性流言传播来评估LLM代理的信任度并协调社会规范，从而在无需改变内在激励的情况下提升间接互惠性并抵御恶意参与者。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化、自利的LLM代理之间因缺乏可靠声誉系统而难以维持间接互惠的问题。

Method: 引入Agentic Linguistic Gossip Network（ALIGN）框架，让代理通过分层语调进行策略性、开放性的流言共享，以评估信任和协调社会规范。

Result: ALIGN能持续提升间接互惠性，并识别、排斥背叛者；更强推理能力的LLM更倾向于激励相容的合作，而聊天模型则常过度合作。

Conclusion: 利用LLM推理能力进行去中心化流言传播，是维护智能体生态系统社会福利的一条有前景路径。

Abstract: Indirect reciprocity, which means helping those who help others, is difficult to sustain among decentralized, self-interested LLM agents without reliable reputation systems. We introduce Agentic Linguistic Gossip Network (ALIGN), an automated framework where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate social norms. We demonstrate that ALIGN consistently improves indirect reciprocity and resists malicious entrants by identifying and ostracizing defectors without changing intrinsic incentives. Notably, we find that stronger reasoning capabilities in LLMs lead to more incentive-aligned cooperation, whereas chat models often over-cooperate even when strategically suboptimal. These results suggest that leveraging LLM reasoning through decentralized gossip is a promising path for maintaining social welfare in agentic ecosystems. Our code is available at https://github.com/shuhui-zhu/ALIGN.

</details>


### [305] [Altruism and Fair Objective in Mixed-Motive Markov games](https://arxiv.org/abs/2602.08389)
*Yao-hua Franck Xu,Tayeb Lemlouma,Arnaud Braud,Jean-Marie Bonnin*

Main category: cs.MA

TL;DR: 本文提出了一种基于比例公平（Proportional Fairness）的新框架，以替代传统的功利主义目标，从而在多智能体社会困境中实现更公平的合作；通过定义基于对数收益的公平利他效用、构建公平马尔可夫博弈，并设计公平Actor-Critic算法，在多个社会困境环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体合作方法（如功利主义福利）虽高效但常导致严重不公平；而个体倾向于背叛以搭便车，加剧社会困境中的不公问题，亟需兼顾效率与公平的新范式。

Method: 将标准功利主义目标替换为比例公平目标；定义基于个体对数收益的公平利他效用；推导经典社会困境中维持合作的解析条件；构建公平马尔可夫博弈模型；设计新型公平Actor-Critic强化学习算法。

Result: 所提方法在多个社会 dilemma 环境中实现了比基线方法更公平且仍具合作性的策略，验证了比例公平目标能有效缓解不公平问题并维持群体合作。

Conclusion: 比例公平是一种可行且有效的替代目标，可在多智能体社会困境中兼顾集体合作与个体公平，为公平强化学习提供了新理论基础与实用算法。

Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>


### [306] [EvoCorps: An Evolutionary Multi-Agent Framework for Depolarizing Online Discourse](https://arxiv.org/abs/2602.08529)
*Ning Lin,Haolun Li,Mingshu Liu,Chengyun Ruan,Kaibo Huang,Yukun Wei,Zhongliang Yang,Linna Zhou*

Main category: cs.MA

TL;DR: 本文提出EvoCorps，一种用于主动去极化的进化多智能体框架，通过动态社会博弈建模、多角色协同与闭环进化学习，在线对抗极化话语并提升讨论质量。


<details>
  <summary>Details</summary>
Motivation: 线上话语极化损害社会信任并加速虚假信息传播，而现有技术手段多为事后诊断，治理策略存在延迟和静态性，难以应对实时演化的协同对抗性放大行为。

Method: 提出EvoCorps框架：将话语治理建模为动态社会博弈，部署监控、规划、具身生成与多身份扩散等多角色智能体；采用检索增强的集体认知核心保障事实基础与行动记忆；引入闭环进化学习机制实现策略自适应更新；在MOSAIC仿真平台上开展含对抗注入的多源新闻流实验。

Result: 在情绪极化、观点极端性与论证理性三个维度上，EvoCorps显著优于对抗基线，验证了其从检测与事后缓解迈向过程内闭环干预的有效性。

Conclusion: EvoCorps为在线话语治理提供了可扩展、自适应、前摄性的新范式，是迈向实时、闭环、主动式内容治理的重要实践路径。

Abstract: Polarization in online discourse erodes social trust and accelerates misinformation, yet technical responses remain largely diagnostic and post-hoc. Current governance approaches suffer from inherent latency and static policies, struggling to counter coordinated adversarial amplification that evolves in real-time. We present EvoCorps, an evolutionary multi-agent framework for proactive depolarization. EvoCorps frames discourse governance as a dynamic social game and coordinates roles for monitoring, planning, grounded generation, and multi-identity diffusion. A retrieval-augmented collective cognition core provides factual grounding and action--outcome memory, while closed-loop evolutionary learning adapts strategies as the environment and attackers change. We implement EvoCorps on the MOSAIC social-AI simulation platform for controlled evaluation in a multi-source news stream with adversarial injection and amplification. Across emotional polarization, viewpoint extremity, and argumentative rationality, EvoCorps improves discourse outcomes over an adversarial baseline, pointing to a practical path from detection and post-hoc mitigation to in-process, closed-loop intervention. The code is available at https://github.com/ln2146/EvoCorps.

</details>


### [307] [ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08567)
*Jinnuo Liu,Chuke Liu,Hua Shen*

Main category: cs.MA

TL;DR: 本文提出了ValueFlow框架，用于评估多智能体大语言模型系统中的价值漂移问题，通过扰动实验分析价值在智能体交互中的传播机制，并提出beta-susceptibility和system susceptibility两个新指标来分别刻画个体敏感性和系统级影响。


<details>
  <summary>Details</summary>
Motivation: 现有价值对齐研究主要针对单个孤立模型，而多智能体系统中价值扰动如何通过智能体间交互传播尚不清楚，亟需系统性评估方法。

Method: 构建基于Schwartz价值调查的56维价值评估数据集，采用LLM-as-a-judge协议量化交互中智能体的价值取向；定义beta-susceptibility和system susceptibility两个指标，分别衡量个体对同伴扰动信号的敏感性及节点扰动对系统输出的整体影响。

Result: 实验证明不同价值维度的敏感性差异显著，且结构拓扑对敏感性具有强烈塑造作用；该框架在多种模型、提示角色、价值维度和网络结构上均表现出稳健性。

Conclusion: ValueFlow为理解与量化多智能体系统中的价值漂移提供了可解释、可分解的评估范式，揭示了结构因素在价值传播中的关键作用。

Abstract: Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.

</details>


### [308] [Teaching an Old Dynamics New Tricks: Regularization-free Last-iterate Convergence in Zero-sum Games via BNN Dynamics](https://arxiv.org/abs/2602.08938)
*Tuo Zhang,Leonardo Stella*

Main category: cs.MA

TL;DR: 本文提出了一种基于Brown-von Neumann-Nash（BNN）动力学的无正则化方法，用于零和博弈中的对抗训练与多智能体学习，在正规型与扩展型博弈中均实现最后迭代收敛，并通过反事实加权和神经函数逼近提升可扩展性与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有零和博弈求解方法依赖需精细调参的正则化，尤其在支付结构未知或动态变化时更难；本文旨在设计无需正则化、具内在收敛性且适用于动态环境的方法。

Method: 复用并改进BNN动力学，理论证明其在带噪声正规型博弈中具最后迭代收敛性；提出基于反事实加权的新框架将其拓展至扩展型博弈；结合神经函数逼近实现可扩展算法。

Result: 在正规型与扩展型博弈中均实现理论保证的最后迭代收敛；实验表明该方法能快速适应非平稳环境，性能优于当前最优的正则化方法。

Conclusion: BNN动力学是一种无需正则化、具强鲁棒性与适应性的零和博弈求解范式，其在正规型与扩展型博弈中的成功拓展为多智能体学习提供了新理论工具与实用算法。

Abstract: Zero-sum games are a fundamental setting for adversarial training and decision-making in multi-agent learning (MAL). Existing methods often ensure convergence to (approximate) Nash equilibria by introducing a form of regularization. Yet, regularization requires additional hyperparameters, which must be carefully tuned--a challenging task when the payoff structure is known, and considerably harder when the structure is unknown or subject to change. Motivated by this problem, we repurpose a classical model in evolutionary game theory, i.e., the Brown-von Neumann-Nash (BNN) dynamics, by leveraging the intrinsic convergence of this dynamics in zero-sum games without regularization, and provide last-iterate convergence guarantees in noisy normal-form games (NFGs). Importantly, to make this approach more applicable, we develop a novel framework with theoretical guarantees that integrates the BNN dynamics in extensive-form games (EFGs) through counterfactual weighting. Furthermore, we implement an algorithm that instantiates our framework with neural function approximation, enabling scalable learning in both NFGs and EFGs. Empirical results show that our method quickly adapts to nonstationarities, outperforming the state-of-the-art regularization-based approach.

</details>


### [309] [Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.08965)
*John Gardiner,Orlando Romero,Brendan Tivnan,Nicolò Dal Fabbro,George J. Pappas*

Main category: cs.MA

TL;DR: 本文提出首个利用共享量子纠缠作为协调资源的多智能体强化学习（MARL）框架，通过可微策略参数化与新型策略架构，使智能体能在无通信条件下实现超越经典共享随机性的协同策略，并在单轮博弈和Dec-POMDP任务中验证了量子优势。


<details>
  <summary>Details</summary>
Motivation: 受量子物理中‘量子纠缠可在无通信条件下实现超越共享随机性策略’这一结论启发，旨在为MARL提供更强的通信自由协调机制。

Method: 提出基于可微量子测量优化的策略参数化方法，以及将联合策略分解为‘量子协调器+去中心化本地执行者’的新型策略架构。

Result: 在黑箱单轮合作博弈和Dec-POMDP任务中，纯从经验学习到具有量子优势的策略，验证了所提框架的有效性。

Conclusion: 共享量子纠缠可作为MARL中一种强大且可行的协调资源，拓展了无通信协同策略的能力边界，为量子增强多智能体学习开辟新路径。

Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [310] [Embodied Intelligence for Flexible Manufacturing: A Survey](https://arxiv.org/abs/2602.06966)
*Kai Xu,Hang Zhao,Ruizhen Hu,Min Yang,Hao Liu,Hui Zhang,Haibin Yu*

Main category: cs.RO

TL;DR: 本文综述了面向柔性制造的工业具身智能，从感知（工业之眼）、控制（工业之手）与决策（工业之脑）三层面梳理技术进展，提出认知增强—技能迁移—系统演化的三阶段发展模型，并为跨学科发展提供理论框架与实践指导。


<details>
  <summary>Details</summary>
Motivation: 柔性制造中工业具身智能面临感知受限下的精准建模监控、灵活适应与高精度控制的动态平衡、通用能力与专用工艺融合三大挑战。

Method: 从‘工业之眼’（多模态感知与实时建模）、‘工业之手’（柔性自适应高精度操控）、‘工业之脑’（工艺规划与产线调度智能优化）三个视角系统综述现有研究，并提出三阶段演化模型。

Result: 揭示了制造系统中感知-决策-执行闭环优化的关键技术路径，构建了认知增强→技能迁移→系统演化的三阶段发展模型。

Conclusion: 具身智能在柔性制造中的发展需强调多层级协同与跨学科融合，该综述为理论深化与产业落地提供了系统性框架与方向指引。

Abstract: Driven by breakthroughs in next-generation artificial intelligence, embodied intelligence is rapidly advancing into industrial manufacturing. In flexible manufacturing, industrial embodied intelligence faces three core challenges: accurate process modeling and monitoring under limited perception, dynamic balancing between flexible adaptation and high-precision control, and the integration of general-purpose skills with specialized industrial operations. Accordingly, this survey reviews existing work from three viewpoints: Industrial Eye, Industrial Hand, and Industrial Brain. At the perception level (Industrial Eye), multimodal data fusion and real-time modeling in complex dynamic settings are examined. At the control level (Industrial Hand), flexible, adaptive, and precise manipulation for complex manufacturing processes is analyzed. At the decision level (Industrial Brain), intelligent optimization methods for process planning and line scheduling are summarized. By considering multi-level collaboration and interdisciplinary integration, this work reveals the key technological pathways of embodied intelligence for closed-loop optimization of perception-decision-execution in manufacturing systems. A three-stage evolution model for the development of embodied intelligence in flexible manufacturing scenarios, comprising cognition enhancement, skill transition, and system evolution, is proposed, and future development trends are examined, to offer both a theoretical framework and practical guidance for the interdisciplinary advancement of industrial embodied intelligence in the context of flexible manufacturing.

</details>


### [311] [Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models](https://arxiv.org/abs/2602.06967)
*Siqi Song,Xuanbing Xie,Zonglin Li,Yuqiang Li,Shijie Wang,Biqing Qi*

Main category: cs.RO

TL;DR: 本文提出CLiMRS框架，利用多个大语言模型（LLM）模拟人类协作机制，实现异构多机器人在复杂环境下的自适应分组、协商与协同控制，并通过新构建的CLiMBench基准验证其在装配任务中效率提升超40%。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作常面临异构性、长时序、空间约束与环境不确定性等挑战，而大语言模型虽擅长推理规划，其在协同控制中的潜力尚未被充分挖掘。

Method: 提出CLiMRS框架：为每台机器人配备一个LLM代理，通过通用提案规划器动态组建子组；子组内由子组管理者主导基于感知的多LLM讨论生成动作指令；结合机器人执行反馈与环境变化形成闭环。

Result: 在CLiMBench异构多机器人装配基准上，CLiMRS相较最优基线在复杂任务中效率提升超40%，且未降低简单任务成功率。

Conclusion: 借鉴人类团队形成的分组与协商机制，能显著提升异构多机器人协作的规划效率与执行鲁棒性。

Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.

</details>


### [312] [Learning to Anchor Visual Odometry: KAN-Based Pose Regression for Planetary Landing](https://arxiv.org/abs/2602.06968)
*Xubo Luo,Zhaojin Li,Xue Wan,Wei Zhang,Leizheng Shu*

Main category: cs.RO

TL;DR: KANLoc是一种结合视觉里程计（VO）与基于Kolmogorov-Arnold网络（KAN）的轻量级绝对位姿回归器的单目定位框架，用于提升月球着陆中6自由度实时定位精度，显著降低平移与旋转误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在月球着陆场景中存在明显局限：视觉里程计易产生无界漂移，而基于地图的绝对定位在纹理稀疏或低光照地形中失效。

Method: 提出KANLoc框架，核心是使用Kolmogorov-Arnold网络从图像特征回归稀疏但高可靠性的全局位姿锚点，并将其融合进捆绑调整（bundle adjustment）中，实现VO与绝对定位的紧耦合。

Result: 在合成与真实月球着陆数据集上，KANLoc平均平移和旋转误差分别降低32%和45%，单轨迹最大提升达45%/48%，运行速度≥15 FPS。

Conclusion: KANLoc通过参数高效、鲁棒性强的KAN回归器与混合定位架构，在保证实时性的同时实现了高精度、全局一致的月面单目定位，为自主月球着陆提供了新范式。

Abstract: Accurate and real-time 6-DoF localization is mission-critical for autonomous lunar landing, yet existing approaches remain limited: visual odometry (VO) drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. We introduce KANLoc, a monocular localization framework that tightly couples VO with a lightweight but robust absolute pose regressor. At its core is a Kolmogorov-Arnold Network (KAN) that learns the complex mapping from image features to map coordinates, producing sparse but highly reliable global pose anchors. These anchors are fused into a bundle adjustment framework, effectively canceling drift while retaining local motion precision. KANLoc delivers three key advances: (i) a KAN-based pose regressor that achieves high accuracy with remarkable parameter efficiency, (ii) a hybrid VO-absolute localization scheme that yields globally consistent real-time trajectories (>=15 FPS), and (iii) a tailored data augmentation strategy that improves robustness to sensor occlusion. On both realistic synthetic and real lunar landing datasets, KANLoc reduces average translation and rotation error by 32% and 45%, respectively, with per-trajectory gains of up to 45%/48%, outperforming strong baselines.

</details>


### [313] [A Survey of Medical Drones from Flight Dynamics, Guidance, Navigation, and Control Perspectives](https://arxiv.org/abs/2602.06969)
*Roshan Kumar Chhetri,Sarocha Jetawatthana,Thanakorn Khamvilai*

Main category: cs.RO

TL;DR: 本文综述了医疗无人机在飞行动力学及制导、导航与控制（GNC）系统方面的研究进展，聚焦于任务需求、无人机构型、载荷容器设计及其对飞行性能的影响，并分析GNC在温湿度、振动等环境干扰下的挑战与算法应对策略，指出了当前研究空白与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述多集中于医疗供应链、运营和应急响应，缺乏从飞行动力学与GNC角度对医疗无人机的系统性梳理。

Method: 采用文献综述法，从医疗任务需求、UAS构型选择、载荷容器设计与优化、GNC基本原理及环境干扰建模出发，分析典型GNC算法及其在医疗场景中的适用性与局限性。

Result: 归纳了影响医疗无人机GNC性能的关键环境因素（振动、温/压/湿度），梳理了现有GNC算法的优劣，并识别出面向真实医疗应用的GNC框架优化路径与研究缺口。

Conclusion: 医疗无人机的GNC系统需兼顾飞行稳定性与医疗载荷保护；未来研究应加强多源干扰建模、轻量化鲁棒控制算法及真实场景验证。

Abstract: The integration of drones into the medical field has revolutionized healthcare delivery by enabling rapid transportation of medical supplies, organs, and even emergency assistance in remote or disaster-stricken areas. While other survey papers focus on the healthcare supply chain, operations, and medical emergency response aspects, this paper provides a comprehensive review of medical drones from the perspectives of flight dynamics and guidance, navigation, and control (GNC) systems. We first discuss the medical aerial delivery mission requirements and suitable uncrewed aerial system (UAS) configurations. We then address payload container design and optimization, and its effect on supplies and overall flight dynamics. We also explore the fundamental principles of GNC in the context of medical drone operations, highlighting key challenges arising from vibration, air temperature, pressure, and humidity, which affect the quality of medical supplies. The paper examines various GNC algorithms that can mitigate these challenges, as well as the algorithms' limitations. With these considerations, this survey aims to provide insights into optimizing GNC frameworks for medical drones, emphasizing research gaps and directions to improve real-world healthcare applications.

</details>


### [314] [Formal Methods in Robot Policy Learning and Verification: A Survey on Current Techniques and Future Directions](https://arxiv.org/abs/2602.06971)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi,Suresh Jagannathan*

Main category: cs.RO

TL;DR: 本文综述了形式化方法在机器人学习中的最新应用，围绕策略学习与策略验证两大支柱展开，分析了代表性技术的可扩展性与表达能力，并探讨了其对提升机器人安全性与正确性的贡献及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统复杂性增加及深度学习的广泛应用，传统形式化分析方法难以应对深度神经网络模型的不灵活性、脆弱性和不可解释性问题，亟需新的形式化或半形式化方法支持复杂目标的精确建模、学习引导与策略验证。

Method: 采用文献综述方法，系统梳理近年机器人学习中形式化方法的研究进展，按‘策略学习’和‘策略验证’两大主题组织分析，比较各类技术的可扩展性与表达能力，并评估其在真实机器人安全与正确性方面的实际贡献。

Result: 归纳出当前形式化方法在机器人学习中已支撑多种策略学习（如约束引导强化学习、逻辑驱动模仿学习）与验证技术（如神经符号验证、运行时监控），显著提升了策略的可靠性与可解释性；但仍在可扩展性、自动化程度和现实场景适配性方面存在瓶颈。

Conclusion: 形式化方法正成为提升机器人学习系统安全性与正确性的关键路径，但需进一步融合学习与验证、增强工具链实用性，并推动理论与工程实践的协同演进。

Abstract: As hardware and software systems have grown in complexity, formal methods have been indispensable tools for rigorously specifying acceptable behaviors, synthesizing programs to meet these specifications, and validating the correctness of existing programs. In the field of robotics, a similar trend of rising complexity has emerged, driven in large part by the adoption of deep learning. While this shift has enabled the development of highly performant robot policies, their implementation as deep neural networks has posed challenges to traditional formal analysis, leading to models that are inflexible, fragile, and difficult to interpret. In response, the robotics community has introduced new formal and semi-formal methods to support the precise specification of complex objectives, guide the learning process to achieve them, and enable the verification of learned policies against them. In this survey, we provide a comprehensive overview of how formal methods have been used in recent robot learning research. We organize our discussion around two pillars: policy learning and policy verification. For both, we highlight representative techniques, compare their scalability and expressiveness, and summarize how they contribute to meaningfully improving realistic robot safety and correctness. We conclude with a discussion of remaining obstacles for achieving that goal and promising directions for advancing formal methods in robot learning.

</details>


### [315] [FeudalNav: A Simple Framework for Visual Navigation](https://arxiv.org/abs/2602.06974)
*Faith Johnson,Bryan Bo Cao,Shubham Jain,Ashwin Ashok,Kristin Dana*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉相似性组织的隐空间记忆模块的分层视觉导航框架，无需里程计或地图，即可在未知环境中实现高效导航，并支持少量人工干预以提升成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于度量地图的方法在未知、无图或无GPS环境下表现不佳，需转向轻量、学习驱动且探索少的导航方法。

Method: 构建分层导航框架：1）使用可迁移的航点选择网络选择子目标；2）设计仅依据视觉相似性组织的隐空间记忆模块替代图结构拓扑表示。

Result: 在Habitat AI环境中达到SOTA水平，无需训练或推理阶段的里程计；少量人工交互即可显著提升任务完成率。

Conclusion: 视觉相似性驱动的隐空间记忆足以支撑有效导航；该框架轻量、易训练、可解释性强，适合真实机器人部署与人机协同导航。

Abstract: Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.

</details>


### [316] [Autonomous Manipulation of Hazardous Chemicals and Delicate Objects in a Self-Driving Laboratory: A Sliding Mode Approach](https://arxiv.org/abs/2602.06977)
*Shifa Sulaiman,Francesco Schetter,Tobias Jensen,Simon Bøgh,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的滑模控制（MBSMC）方法，用于自驱动化学实验室中移动平台上机械臂的精确、平滑运动控制，显著降低控制能耗并提升轨迹跟踪精度与鲁棒性，优于传统PID和非模型滑模控制。


<details>
  <summary>Details</summary>
Motivation: 在自驱动化学实验室中，机械臂需安全、精准地操作易碎玻璃器皿和危险化学品，传统控制器（如PID）难以应对非线性动力学和外部扰动，导致轨迹误差大、动作不平稳。

Method: 设计并实现一种基于模型的滑模控制（MBSMC），采用双曲正切函数削弱抖振；以移动平台上的机械臂为对象，面向关节空间与笛卡尔空间联合优化，强调平滑过渡与高精度轨迹跟踪。

Result: 相比PID和非模型滑模控制（NMBSMC），MBSMC实现更平滑运动、控制能耗降低高达90%；实验成功完成抓取器皿、开关窗等任务，而PID因鲁棒性不足失败。

Conclusion: MBSMC在安全性、精度与鲁棒性方面显著优于对比方法，适用于自主化学实验室中智能移动机械臂的实际部署。

Abstract: Precise handling of chemical instruments and materials within a self-driving laboratory environment using robotic systems demands advanced and reliable control strategies. Sliding Mode Control (SMC) has emerged as a robust approach for managing uncertainties and disturbances in manipulator dynamics, providing superior control performance compared to traditional methods. This study implements a model-based SMC (MBSMC) utilizing a hyperbolic tangent function to regulate the motion of a manipulator mounted on a mobile platform operating inside a self-driving chemical laboratory. Given the manipulator's role in transporting fragile glass vessels filled with hazardous chemicals, the controller is specifically designed to minimize abrupt transitions and achieve gentle, accurate trajectory tracking. The proposed controller is benchmarked against a non-model-based SMC (NMBSMC) and a Proportional-Integral-Derivative (PID) controller using a comprehensive set of joint and Cartesian metrics. Compared to PID and NMBSMC, MBSMC achieved significantly smoother motion and up to 90% lower control effort, validating its robustness and precision for autonomous laboratory operations. Experimental trials confirmed successful execution of tasks such as vessel grasping and window operation, which failed under PID control due to its limited ability to handle nonlinear dynamics and external disturbances, resulting in substantial trajectory tracking errors. The results validate the controller's effectiveness in achieving smooth, precise, and safe manipulator motions, supporting the advancement of intelligent mobile manipulators in autonomous laboratory environments.

</details>


### [317] [LangGS-SLAM: Real-Time Language-Feature Gaussian Splatting SLAM](https://arxiv.org/abs/2602.06991)
*Seongbo Ha,Sibaek Lee,Kyungsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.RO

TL;DR: 本文提出了一种RGB-D SLAM系统，能实时构建语言对齐的稠密特征场，在保持低延迟跟踪与建图的同时实现高几何与语义保真度。


<details>
  <summary>Details</summary>
Motivation: 解决在线SLAM中稠密、语言对齐特征场构建所面临的语义-几何失配、内存开销大及实时优化难等问题，弥合3D感知与语言推理之间的鸿沟。

Method: 提出Top-K渲染流水线以高效无失真地渲染高维特征图；设计多准则地图管理策略修剪冗余/不一致高斯元；构建混合场优化框架，按特征场特性解耦几何与语义优化频率。

Result: 系统在15 FPS下运行，几何精度优于纯几何基线，语义精度媲美离线方法。

Conclusion: 证明了在线SLAM中构建稠密、未压缩、语言对齐特征场的可行性与有效性。

Abstract: In this paper, we propose a RGB-D SLAM system that reconstructs a language-aligned dense feature field while sustaining low-latency tracking and mapping. First, we introduce a Top-K Rendering pipeline, a high-throughput and semantic-distortion-free method for efficiently rendering high-dimensional feature maps. To address the resulting semantic-geometric discrepancy and mitigate the memory consumption, we further design a multi-criteria map management strategy that prunes redundant or inconsistent Gaussians while preserving scene integrity. Finally, a hybrid field optimization framework jointly refines the geometric and semantic fields under real-time constraints by decoupling their optimization frequencies according to field characteristics. The proposed system achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS. Our results demonstrate that online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, bridging the gap between 3D perception and language-based reasoning.

</details>


### [318] [Realistic Synthetic Household Data Generation at Scale](https://arxiv.org/abs/2602.07243)
*Siddharth Singh,Ifrah Idrees,Abraham Dauhajre*

Main category: cs.RO

TL;DR: 本文提出了一种生成式框架，用于大规模合成具有双向人-环境交互建模能力的3D家庭数据集，支持通过自然语言提示灵活配置，并经多维度统计验证其与真实数据的高度一致性及生成机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据框架无法建模人类行为与家庭环境之间的双向影响，限制了具身智能体在长期人机交互中的训练与评估。

Method: 提出松耦合的生成框架，联合建模长期人-机器人交互与家庭环境：人类画像驱动环境生成，环境结构与语义反向塑造交互行为；支持自然语言提示定义数据特征，并生成多样化变体；采用多模态嵌入与统计指标（余弦相似度、互信息增益、干预分析、迭代验证）进行评估。

Result: 生成数据与真实数据HOMER余弦相似度达0.60，显著优于合成基线（0.27）；干预分析显示年龄、整洁度、睡眠模式等变量变化均产生统计显著（p<0.001）、效应量大（d=0.51–1.12）的行为与环境差异。

Conclusion: 该框架实现了对人-环境双向耦合关系的可扩展、可控、可信建模，为家庭智能设备的大规模开发与测试提供了高质量合成数据基础。

Abstract: Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions.
  The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation.
  We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.

</details>


### [319] [When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey](https://arxiv.org/abs/2602.06995)
*Konstantinos Gounis,Sotiris A. Tegos,Dimitrios Tyrovolas,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.RO

TL;DR: 本文综述了SLAM（特别是视觉SLAM）与无线通信技术的交叉研究进展，强调二者之间的双向协同：RF信息可辅助解决单目V-SLAM的尺度模糊问题，而视觉里程计可提升5G及以后无线通信的定位与感知能力；当前联合通信与SLAM的集成方案仍处于初期，需在理论与实践中进一步融合语义感知与高精度定位能力。


<details>
  <summary>Details</summary>
Motivation: 商用无线通信与传感设备的普及，以及智能自主系统的发展，催生了通信与SLAM深度融合的需求；现有工作尚未系统梳理二者双向赋能机制与技术挑战。

Method: 采用综述方法，系统梳理无线信号传播建模、RF定位/感知、图像特征提取与路径预测等关键技术，并分析概率模型、空间信号处理等数学工具在联合状态估计中的应用。

Result: 发现单目V-SLAM可借助RF信息解决尺度模糊；5G+通信可利用SLAM中的视觉里程计提升定位鲁棒性；多源SLAM（非仅相机）与无线通信存在双重关联；但联合通信-SLAM系统仍处于早期阶段。

Conclusion: SLAM与无线通信具有显著互补性，但当前集成方案缺乏高阶语义感知与精准定位能力，亟需理论突破与工程实践协同推进。

Abstract: The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.

</details>


### [320] [Admittance-Based Motion Planning with Vision-Guided Initialization for Robotic Manipulators in Self-Driving Laboratories](https://arxiv.org/abs/2602.07005)
*Shifa Sulaiman,Tobias Jensen,Francesco Schetter,Simon Bøgh*

Main category: cs.RO

TL;DR: 本文提出了一种基于导纳控制的运动规划框架，用于自驱动实验室中的自适应、柔顺机器人操作，结合视觉算法实现目标定位与初始轨迹生成，并通过实时力响应提升人机协作安全性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室（SDLs）中存在精密设备、不可预测环境交互及偶尔的人工干预，亟需具备合规性与力感知能力的控制方法以保障安全、适应性和可靠性。

Method: 提出一种将导纳控制器直接嵌入轨迹执行过程的运动规划框架；结合基于结构化平面姿态估计的视觉算法，通过特征提取、单应性估计与深度融合实现纹理平面物体的检测与定位，为运动规划提供初始目标构型。

Result: 实现了机器人在交互过程中对外部力和人工干预的实时响应与重定向能力，并以纹理图像检测为概念验证，验证了该策略的有效性。

Conclusion: 该框架提升了机器人在复杂实验室环境中的柔顺性、安全性与人机协作能力，未来将拓展至透明实验器皿等更具挑战性的SDL场景。

Abstract: Self driving laboratories (SDLs) are highly automated research environments that leverage advanced technologies to conduct experiments and analyze data with minimal human involvement. These environments often involve delicate laboratory equipment, unpredictable environmental interactions, and occasional human intervention, making compliant and force aware control essential for ensuring safety, adaptability, and reliability. This paper introduces a motion-planning framework centered on admittance control to enable adaptive and compliant robotic manipulation. Unlike conventional schemes, the proposed approach integrates an admittance controller directly into trajectory execution, allowing the manipulator to dynamically respond to external forces during interaction. This capability enables human operators to override or redirect the robot's motion in real time. A vision algorithm based on structured planar pose estimation is employed to detect and localize textured planar objects through feature extraction, homography estimation, and depth fusion, thereby providing an initial target configuration for motion planning. The vision based initialization establishes the reference trajectory, while the embedded admittance controller ensures that trajectory execution remains safe, adaptive, and responsive to external forces or human intervention. The proposed strategy is validated using textured image detection as a proof of concept. Future work will extend the framework to SDL environments involving transparent laboratory objects where compliant motion planning can further enhance autonomy, safety, and human-robot collaboration.

</details>


### [321] [ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning](https://arxiv.org/abs/2602.07007)
*Dongsheng Chen,Yuxuan Li,Yi Lin,Guanhua Chen,Jiaxin Zhang,Xiangyu Zhao,Lei Ma,Xin Yao,Xuetao Wei*

Main category: cs.RO

TL;DR: 本文提出ARGOS框架，通过属性引导的组合推理，将开放式的自然语言指令与具体的物理属性相结合，生成符合安全标准的功能安全需求（FSRs），提升具身AI在开放世界中的功能安全性。


<details>
  <summary>Details</summary>
Motivation: 传统HARA方法难以应对具身AI基于开放式自然语言指令所产生的组合式交互风险；而现有大语言模型缺乏物理 grounding，导致生成的危害描述语义浅层且不连贯。

Method: 提出ARGOS框架：动态分解指令中的实体为细粒度物理属性，以因果风险因素为依据进行LLM推理，生成物理上合理的情境；再结合机器人能力，将抽象安全标准（如ISO 13482）实例化为上下文相关的功能安全需求（FSRs）。

Result: 实验表明ARGOS能生成高质量FSRs，在识别长尾风险方面优于基线方法。

Conclusion: ARGOS为具身AI提供了系统化、物理可解释的功能安全需求生成路径，是推动其安全工业部署的关键一步。

Abstract: Ensuring functional safety is essential for the deployment of Embodied AI in complex open-world environments. However, traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale in this domain. While HARA relies on enumerating risks for finite and pre-defined function lists, Embodied AI operates on open-ended natural language instructions, creating a challenge of combinatorial interaction risks. Whereas Large Language Models (LLMs) have emerged as a promising solution to this scalability challenge, they often lack physical grounding, yielding semantically superficial and incoherent hazard descriptions. To overcome these limitations, we propose a new framework ARGOS (AttRibute-Guided cOmbinatorial reaSoning), which bridges the gap between open-ended user instructions and concrete physical attributes. By dynamically decomposing entities from instructions into these fine-grained properties, ARGOS grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios. It then instantiates abstract safety standards, such as ISO 13482, into context-specific Functional Safety Requirements (FSRs) by integrating these scenarios with robot capabilities. Extensive experiments validate that ARGOS produces high-quality FSRs and outperforms baselines in identifying long-tail risks. Overall, this work paves the way for systematic and grounded functional safety requirement generation, a critical step toward the safe industrial deployment of Embodied AI.

</details>


### [322] [A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration](https://arxiv.org/abs/2602.07024)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

TL;DR: 本文提出了一种结合数据手套（含IMU）和视觉触觉传感器的多模态手势识别系统，用于人机协作中对手部活动的准确识别。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别（HAR）是人机协作（HRC）的基础，使机器人能感知并动态适应人类意图。

Method: 采用模块化数据手套（集成惯性测量单元）与视觉触觉传感器融合的多模态方法，采集人手与机器人接触时的手部活动数据，并在离线分类、静态实时分类及真实HRC场景下进行测试。

Result: 实验结果表明该方法在所有测试条件下均取得高识别精度。

Conclusion: 该多模态HAR系统可有效支持多种人机协同应用场景。

Abstract: Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.

</details>


### [323] [Airspace-aware Contingency Landing Planning](https://arxiv.org/abs/2602.07074)
*H. Emre Tekaslan,Ella M. Atkins*

Main category: cs.RO

TL;DR: 本文提出了一种实时、基于搜索的飞机应急着陆规划器，通过联合优化空域风险（交通密度暴露时间）和地面风险（人口密度），在华盛顿特区案例中实现了低风险、低干扰的着陆轨迹生成，平均耗时2.9秒，优于Dubins最小风险基准。


<details>
  <summary>Details</summary>
Motivation: 应对飞机突发状况需快速选择安全着陆点，同时最小化对既有空域交通流和地面人口的风险，现有方法难以兼顾实时性、多源风险建模与运行干扰抑制。

Method: 构建融合空域结构（起降流、直升机走廊、禁飞区）与历史ADS-B流量数据的空域模型；设计低延迟计算几何算法生成高风险区域邻近热力图；定义空域风险（轨迹在拥堵区累计暴露时间）与地面风险（轨迹下方人口密度积分）联合代价；结合着陆点选择模块减少对常规航班干扰；采用搜索策略实现实时轨迹规划。

Result: 在华盛顿特区案例中，相比最小风险Dubins解，所提规划器显著降低联合风险与空域干扰；仅考虑空域风险时，平均规划时间2.9秒（笔记本电脑），满足实时要求。

Conclusion: 该搜索式应急着陆规划器能有效平衡安全性、实时性与运行兼容性，为城市复杂空域下的无人机/通航应急响应提供了可行技术路径；后续将引入动态空情更新以支持时空联合规划。

Abstract: This paper develops a real-time, search-based aircraft contingency landing planner that minimizes traffic disruptions while accounting for ground risk. The airspace model captures dense air traffic departure and arrival flows, helicopter corridors, and prohibited zones and is demonstrated with a Washington, D.C., area case study. Historical Automatic Dependent Surveillance-Broadcast (ADS-B) data are processed to estimate air traffic density. A low-latency computational geometry algorithm generates proximity-based heatmaps around high-risk corridors and restricted regions. Airspace risk is quantified as the cumulative exposure time of a landing trajectory within congested regions, while ground risk is assessed from overflown population density to jointly guide trajectory selection. A landing site selection module further mitigates disruption to nominal air traffic operations. Benchmarking against minimum-risk Dubins solutions demonstrates that the proposed planner achieves lower joint risk and reduced airspace disruption while maintaining real-time performance. Under airspace-risk-only conditions, the planner generates trajectories within an average of 2.9 seconds on a laptop computer. Future work will incorporate dynamic air traffic updates to enable spatiotemporal contingency landing planning that minimizes the need for real-time traffic rerouting.

</details>


### [324] [A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles](https://arxiv.org/abs/2602.08799)
*Robin Dehler,Michael Buchholz*

Main category: cs.RO

TL;DR: 本文提出了一种面向自动驾驶的通用函数卸载框架，支持灵活集成不同卸载决策算法和QoS需求，并设计了基于位置的高效卸载策略；在服务化轨迹规划用例中，将计算任务卸载至MEC服务器，仿真与实测结果表明该框架可在保障QoS的同时提升CAV计算效率，并具备多车并发卸载的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决联网自动驾驶车辆（CAVs）等自主机器人在计算能力与能源供应方面的限制，通过分布式服务形式将计算任务在本地与远程设备间卸载。

Method: 提出一种通用函数卸载框架，支持可插拔的卸载决策算法和可调QoS需求；设计基于CAV位置的轻量级卸载决策方法；以服务化轨迹规划为用例，将任务卸载至多接入边缘计算（MEC）服务器；通过仿真与真实场景实验进行验证。

Result: 框架能有效保障轨迹规划的QoS，提升CAV计算效率；仿真显示其可适应多辆CAV同时发起卸载请求的多样化场景。

Conclusion: 所提出的函数卸载框架具有通用性、灵活性与实用性，适用于自动驾驶等对实时性与可靠性要求高的边缘协同计算场景。

Abstract: Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.

</details>


### [325] [A compliant ankle-actuated compass walker with triggering timing control](https://arxiv.org/abs/2602.07158)
*Deniz Kerimoglu,Ismail Uyanik*

Main category: cs.RO

TL;DR: 本文提出了一种新型踝关节驱动的被动动态行走模型（TC-AACG），通过非瞬时、柔顺的踝部推离实现水平地面稳定行走，可借助串联弹性执行器（SEA）在物理平台实现，优于传统脉冲式驱动方法。


<details>
  <summary>Details</summary>
Motivation: 被动动态步行器通常只能在斜坡上稳定行走，依赖重力供能；为拓展至水平及复杂地形并提升效率，需更易物理实现的驱动方式。

Method: 提出触发控制的踝关节驱动型罗盘步态模型（TC-AACG），采用非瞬时柔顺踝推离机制，并利用串联弹性执行器（SEA）实现；通过系统仿真分析其步态性能。

Result: TC-AACG相比脉冲式踝驱动显著提升了行走能力，在步速、机械运输能耗（CoT）和吸引域方面均表现更优。

Conclusion: TC-AACG是一种更易物理实现、更具鲁棒性和能效的踝驱动策略，为被动动态步行器在实际机器人平台上的应用提供了新途径。

Abstract: Passive dynamic walkers are widely adopted as a mathematical model to represent biped walking. The stable locomotion of these models is limited to tilted surfaces, requiring gravitational energy. Various techniques, such as actuation through the ankle and hip joints, have been proposed to extend the applicability of these models to level ground and rough terrain with improved locomotion efficiency. However, most of these techniques rely on impulsive energy injection schemes and torsional springs, which are quite challenging to implement in a physical platform. Here, a new model is proposed, named triggering controlled ankle actuated compass gait (TC-AACG), which allows non-instantaneous compliant ankle pushoff. The proposed technique can be implemented in physical platforms via series elastic actuators (SEAs). Our systematic examination shows that the proposed approach extends the locomotion capabilities of a biped model compared to impulsive ankle pushoff approach. We provide extensive simulation analysis investigating the locomotion speed, mechanical cost of transport, and basin of attraction of the proposed model.

</details>


### [326] [Continuum Robot Localization using Distributed Time-of-Flight Sensors](https://arxiv.org/abs/2602.07209)
*Spencer Teetaert,Giammarco Caroleo,Marco Pontin,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot,Perla Maiolino*

Main category: cs.RO

TL;DR: 本文提出了一种适用于软体和连续体机器人（CR）的定位方法，利用沿机器人本体分布的小型、低分辨率飞行时间（ToF）传感器，并结合机器人形状先验信息进行数据融合，在多种实验条件下实现了平均2.5cm位置误差和7.2°旋转误差的高精度定位。


<details>
  <summary>Details</summary>
Motivation: 高分辨率ToF传感器（如激光雷达）在移动机器人中广泛应用，但在软体和连续体机器人中因体积过大而不适用；加之其形变特性，导致CR在非结构化环境中的定位与建图仍属未充分研究领域。

Method: 采用沿机器人长度分布的小型、低分辨率ToF传感器，通过融合各传感器测量数据与机器人形状先验模型，解决单个传感器频繁出现退化观测的问题。

Result: 在53cm长的连续体机器人上，实现在多种环境（仿真与真实场景）下的稳定定位性能，平均位置误差为2.5cm，平均旋转误差为7.2°，并验证了对先验地图偏差具有一定鲁棒性。

Conclusion: 基于低分辨率ToF传感器与形状先验融合的方法，可有效实现连续体机器人在非结构化环境中的高精度、鲁棒定位，为该领域提供了可行且实用的技术路径。

Abstract: Localization and mapping of an environment are crucial tasks for any robot operating in unstructured environments. Time-of-flight (ToF) sensors (e.g.,~lidar) have proven useful in mobile robotics, where high-resolution sensors can be used for simultaneous localization and mapping. In soft and continuum robotics, however, these high-resolution sensors are too large for practical use. This, combined with the deformable nature of such robots, has resulted in continuum robot (CR) localization and mapping in unstructured environments being a largely untouched area. In this work, we present a localization technique for CRs that relies on small, low-resolution ToF sensors distributed along the length of the robot. By fusing measurement information with a robot shape prior, we show that accurate localization is possible despite each sensor experiencing frequent degenerate scenarios. We achieve an average localization error of 2.5cm in position and 7.2° in rotation across all experimental conditions with a 53cm long robot. We demonstrate that the results are repeated across multiple environments, in both simulation and real-world experiments, and study robustness in the estimation to deviations in the prior map.

</details>


### [327] [aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones](https://arxiv.org/abs/2602.07264)
*Jacopo Panerati,Sina Sajjadi,Sina Soleymanpour,Varunkumar Mehta,Iraj Mantegh*

Main category: cs.RO

TL;DR: 本文介绍了aerial-autonomy-stack，一个开源的端到端空中自主框架，旨在解决物理AI中仿真到现实的鸿沟问题，支持ROS2及PX4/ArduPilot飞控，实现20倍实时加速的全栈仿真。


<details>
  <summary>Details</summary>
Motivation: 提升无人机系统自主性以增强其有效性与可靠性，同时应对物理AI中仿真到现实的鸿沟挑战，包括建模缺陷和异构软硬件系统集成复杂性。

Method: 提出并构建了aerial-autonomy-stack开源框架，基于ROS2，统一支持PX4和ArduPilot飞控，集成GPU加速感知与飞控执行，并实现含边缘计算与网络的端到端高速仿真。

Result: 该框架支持超20倍实时加速的端到端仿真，显著缩短基于感知的自主系统开发-测试-部署周期。

Conclusion: aerial-autonomy-stack为快速工程化与部署空中自主系统提供了高效、开放、可扩展的解决方案，推动机器人领域向类似深度学习时代的范式转变。

Abstract: Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term "simulation-to-reality gap". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.

</details>


### [328] [Action-to-Action Flow Matching](https://arxiv.org/abs/2602.07322)
*Jindou Jia,Gen Li,Xiangyu Chen,Tuo An,Yuxuan Hu,Jingliang Li,Xinying Guo,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出Action-to-Action flow matching（A2A），一种基于动作历史初始化的流匹配策略，替代传统扩散模型中从随机高斯噪声采样的方式，显著降低推理延迟（单步0.56ms），提升实时控制能力、鲁棒性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 标准扩散策略依赖多步迭代去噪，导致高推理延迟，难以满足机器人实时控制需求；同时，现有方法将本体感知动作反馈视为静态条件，未能充分利用其时序动态信息。

Method: 提出A2A范式，将前一时刻动作及历史本体感知序列编码为高维隐空间中的初始点，直接进行流匹配建模，跳过传统迭代去噪过程。

Result: A2A在单步推理下实现0.56ms低延迟，训练高效、推理快速、泛化性强；对视觉扰动更鲁棒，能更好适应未见构型；并成功拓展至视频生成任务。

Conclusion: 动作历史可作为有效先验引导流匹配过程，无需随机噪声初始化；A2A为实时机器人控制提供了一种高效、鲁棒且通用的新范式。

Abstract: Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.

</details>


### [329] [Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing](https://arxiv.org/abs/2602.07326)
*Edgar Lee,Junho Choi,Taemin Kim,Changjoo Nam,Seokhwan Jeong*

Main category: cs.RO

TL;DR: 本文提出了一种仅依赖单轴指尖力反馈和关节本体感知的盲抓取方法，通过教师-学生强化学习框架实现仿真到现实的策略蒸馏，在18个物体上达到98.3%抓取成功率，显著降低真实机器人抓取系统的传感需求。


<details>
  <summary>Details</summary>
Motivation: 现实机器人操作中，受限传感（如缺乏视觉或多轴触觉）导致抓取困难，而高精度传感器存在成本高、易损坏和集成复杂等问题。

Method: 采用强化学习教师策略（利用仿真中特权信息生成示范），蒸馏为仅依赖单轴力与关节位置的Transformer学生策略；全程在仿真中训练，部署于真实多指手。

Result: 在18个物体（含分布内外）的真实硬件实验中取得98.3%整体抓取成功率，展现出强鲁棒性与泛化能力。

Conclusion: 仅需极简传感即可实现高成功率多指抓取，验证了低感知依赖抓取策略的可行性与实用性，为低成本、高鲁棒机器人操作提供新路径。

Abstract: Grasping under limited sensing remains a fundamental challenge for real-world robotic manipulation, as vision and high-resolution tactile sensors often introduce cost, fragility, and integration complexity. This work demonstrates that reliable multifingered grasping can be achieved under extremely minimal sensing by relying solely on uniaxial fingertip force feedback and joint proprioception, without vision or multi-axis/tactile sensing. To enable such blind grasping, we employ an efficient teacher-student training pipeline in which a reinforcement-learned teacher exploits privileged simulation-only observations to generate demonstrations for distilling a transformer-based student policy operating under partial observation. The student policy is trained to act using only sensing modalities available at real-world deployment. We validate the proposed approach on real hardware across 18 objects, including both in-distribution and out-of-distribution cases, achieving a 98.3~$\%$ overall grasp success rate. These results demonstrate strong robustness and generalization beyond the simulation training distribution, while significantly reducing sensing requirements for real-world grasping systems.

</details>


### [330] [UEREBot: Learning Safe Quadrupedal Locomotion under Unstructured Environments and High-Speed Dynamic Obstacles](https://arxiv.org/abs/2602.07363)
*Zihao Xu,Runyu Lei,Zihao Li,Boxi Lin,Ce Hao,Jin Song Dong*

Main category: cs.RO

TL;DR: 本文提出UEREBot框架，通过分层结构融合慢速规划与瞬时反射规避，在非结构化环境中实现安全、目标导向且稳定的四足机器人运动。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在非结构化环境中需同时满足长视野目标推进、不规则地形通行性及高速动态障碍物避撞，但单一系统难以兼顾三者，存在规划过慢或反应式策略牺牲目标进度与通行性的问题。

Method: 提出分层框架UEREBot：1）将任务建模为带约束的最优控制问题蓝图；2）采用时空规划器生成朝向目标的参考轨迹与威胁信号；3）通过威胁感知交接机制融合导航与反射动作生成标称指令；4）利用控制屏障函数（CBF）作为最终执行保障。

Result: 在Isaac Lab仿真及搭载机载感知的Unitree Go2实机上验证，UEREBot在含复杂静态结构与高速动态障碍的多样环境中，相比基线方法显著提升避障成功率、运动稳定性及目标进度保持能力。

Conclusion: UEREBot有效协调规划与反射行为，在安全性和任务进度之间取得更优权衡，为非结构化环境下的四足机器人自主导航提供了可行框架。

Abstract: Quadruped robots are increasingly deployed in unstructured environments. Safe locomotion in these settings requires long-horizon goal progress, passability over uneven terrain and static constraints, and collision avoidance against high-speed dynamic obstacles. A single system cannot fully satisfy all three objectives simultaneously: planning-based decisions can be too slow, while purely reactive decisions can sacrifice goal progress and passability. To resolve this conflict, we propose UEREBot (Unstructured-Environment Reflexive Evasion Robot), a hierarchical framework that separates slow planning from instantaneous reflexive evasion and coordinates them during execution. UEREBot formulates the task as a constrained optimal control problem blueprint. It adopts a spatial--temporal planner that provides reference guidance toward the goal and threat signals. It then uses a threat-aware handoff to fuse navigation and reflex actions into a nominal command, and a control barrier function shield as a final execution safeguard. We evaluate UEREBot in Isaac Lab simulation and deploy it on a Unitree Go2 quadruped equipped with onboard perception. Across diverse environments with complex static structure and high-speed dynamic obstacles, UEREBot achieves higher avoidance success and more stable locomotion while maintaining goal progress than representative baselines, demonstrating improved safety--progress trade-offs.

</details>


### [331] [Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2602.07388)
*Yuxuan Hu,Xiangyu Chen,Chuhao Zhou,Yuxi Liu,Gen Li,Jindou Jia,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出了一种名为Trace-Focused Diffusion Policy (TF-DP)的扩散策略，通过将动作生成显式地依赖于机器人执行历史（即执行轨迹），来解决长时程操作任务中因视觉观测相似导致的动作多模态歧义问题；该方法在真实机器人实验中显著提升了时间一致性与鲁棒性，且仅带来轻微的推理开销。


<details>
  <summary>Details</summary>
Motivation: 长时程机器人操作任务中，视觉上相似的观测在不同执行阶段需对应不同动作，仅依赖当前观测会导致动作预测歧义（MA2）；现有生成式策略缺乏对执行历史的建模能力。

Method: 提出TF-DP框架：将历史运动显式表示为执行轨迹，并将其投影至视觉观测空间，形成阶段感知上下文；进一步引入轨迹聚焦场（trace-focused field）以增强任务相关区域的注意力，抑制背景干扰。

Result: 在具有显著MA2和视觉杂乱的真实机器人任务中，TF-DP相比基础扩散策略在MA2任务上性能提升80.56%，在视觉干扰下提升86.11%，推理时间仅增加6.4%。

Conclusion: 执行轨迹条件化是一种可扩展、有原则的方法，能有效提升单策略在长时程、鲁棒机器人操作中的表现。

Abstract: Generative model-based policies have shown strong performance in imitation-based robotic manipulation by learning action distributions from demonstrations. However, in long-horizon tasks, visually similar observations often recur across execution stages while requiring distinct actions, which leads to ambiguous predictions when policies are conditioned only on instantaneous observations, termed multi-modal action ambiguity (MA2). To address this challenge, we propose the Trace-Focused Diffusion Policy (TF-DP), a simple yet effective diffusion-based framework that explicitly conditions action generation on the robot's execution history. TF-DP represents historical motion as an explicit execution trace and projects it into the visual observation space, providing stage-aware context when current observations alone are insufficient. In addition, the induced trace-focused field emphasizes task-relevant regions associated with historical motion, improving robustness to background visual disturbances. We evaluate TF-DP on real-world robotic manipulation tasks exhibiting pronounced multi-modal action ambiguity and visually cluttered conditions. Experimental results show that TF-DP improves temporal consistency and robustness, outperforming the vanilla diffusion policy by 80.56 percent on tasks with multi-modal action ambiguity and by 86.11 percent under visual disturbances, while maintaining inference efficiency with only a 6.4 percent runtime increase. These results demonstrate that execution-trace conditioning offers a scalable and principled approach for robust long-horizon robotic manipulation within a single policy.

</details>


### [332] [Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity](https://arxiv.org/abs/2602.07413)
*Yunhai Han,Linhao Bai,Ziyu Xiao,Zhaodong Yang,Yogita Choudhary,Krishna Jha,Chuizheng Kong,Shreyas Kousik,Harish Ravichandar*

Main category: cs.RO

TL;DR: 本文提出统一行为模型（UBMs），通过Koopman算子理论建模视觉与本体感觉特征的联合动力学演化，实现无需动作分块的时序一致、可在线重规划的灵巧操作技能学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散/Transformer的模仿学习方法依赖固定时长动作分块，导致时序连贯性与反应性难以兼顾，且在多指灵巧操作中数据与计算开销大、可靠性低。

Method: 提出统一行为模型（UBMs），将灵巧技能建模为视觉流与动作流耦合的动力系统；具体实现Koopman-UBM，利用Koopman算子学习潜空间中联合特征的线性演化，并设计基于视觉流预测误差的在线重规划机制。

Result: 在7个仿真任务和2个真实世界任务上，Koopman-UBM性能媲美或超越SOTA基线，同时具备更快推理速度、更平滑执行、更强遮挡鲁棒性及灵活重规划能力。

Conclusion: UBMs为灵巧操作提供了一种更本质、高效且鲁棒的建模范式，将技能视为内在耦合的动力过程而非静态映射，显著缓解了反应性与时序一致性之间的权衡矛盾。

Abstract: There has been rapid and dramatic progress in robots' ability to learn complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it analytically computes the desired robot behavior while simultaneously ''imagining'' the resulting flow of visual features over the entire skill horizon. To enable reactivity and adaptation, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge beyond a threshold. Across seven simulated tasks and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of state-of-the-art baselines, while offering considerably faster inference, smooth execution, robustness to occlusions, and flexible replanning.

</details>


### [333] [Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots](https://arxiv.org/abs/2602.07434)
*Songhua Yang,Xuetao Li,Xuanye Fei,Mengde Li,Miao Li*

Main category: cs.RO

TL;DR: 本文提出了SeM²框架，通过多模态感知、链式思维推理和语义-序列对齐机制，实现语音、情感与动作的协调统一，并支持云端与边缘端部署，显著提升人机交互的自然性与情感表达清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人缺乏语音、面部表情和手势的协调表达，且实际部署需要无需持续云连接的端侧解决方案。

Method: 提出基于视觉语言模型的SeM²框架，包含多模态感知模块、链式思维响应规划和语义-序列对齐机制（SSAM），并开发知识蒸馏的边缘版本SeM²ₑ。

Result: 在自然性、情感清晰度和模态一致性上显著优于单模态基线，边缘版本保持95%相对性能。

Conclusion: SeM²推动了具备社会表达能力的仿人机器人在多样化真实环境中的实用化发展。

Abstract: Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \underline{\textit{S}}peech, \underline{\textit{E}}motion, and \underline{\textit{M}}otion, we present \textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \underline{\textit{e}}dge-deployed versions (\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.

</details>


### [334] [TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control](https://arxiv.org/abs/2602.07439)
*Weiji Xie,Jiakun Zheng,Jinrui Han,Jiyuan Shi,Weinan Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了TextOp，一个实时文本驱动的人形机器人运动生成与控制框架，支持流式语言指令和执行过程中的即时指令修改。


<details>
  <summary>Details</summary>
Motivation: 现有控制器要么依赖预定义轨迹（缺乏灵活性），要么依赖持续的人类遥操作（限制自主性），本文旨在实现一种实时、交互式、通用的人形控制器驱动方法。

Method: TextOp采用两级架构：高层使用自回归运动扩散模型，根据当前文本输入持续生成短时域运动轨迹；底层由运动跟踪策略在真实人形机器人上执行这些轨迹。

Result: 在真实机器人上实现了即时响应、全身平滑运动和精确控制，支持跳舞、跳跃等多行为无缝切换，并通过大量实机实验和离线评估验证了性能。

Conclusion: TextOp成功将交互式运动生成与鲁棒全身控制结合，使人形机器人能自由表达用户意图，并在单次连续执行中完成复杂行为过渡。

Abstract: Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/

</details>


### [335] [VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots](https://arxiv.org/abs/2602.07506)
*Peizhen Li,Longbing Cao,Xiao-Ming Wu,Yang Zhang*

Main category: cs.RO

TL;DR: 本文提出VividFace系统，实现了人形机器人实时、逼真地模仿人类面部表情，通过X2CNet++框架提升表达力，并采用流式推理与异步I/O实现0.05秒延迟的实时阴影映射。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时性与表情真实感之间难以兼顾，多依赖离线视频推理且难以捕捉细微表情细节，限制了情感化人机交互的发展。

Method: 提出VividFace系统，包括优化的X2CNet++模仿框架（含人脸-机器人运动迁移模块微调与特征适配训练策略），以及支持视频流的实时推理流水线和基于异步I/O的轻量通信流程。

Result: VividFace可在0.05秒内完成面部表情模仿，支持多样化面部配置泛化，并经真实场景实验验证其有效性与实用性。

Conclusion: VividFace显著提升了人形机器人面部表情阴影映射的实时性与真实感，为人机情感交互提供了可行且高效的技术路径。

Abstract: Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.

</details>


### [336] [Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning](https://arxiv.org/abs/2602.07541)
*Jingyi Hou,Leyu Zhou,Chenchen Jing,Jinghan Yang,Xinbo Yu,Wei He*

Main category: cs.RO

TL;DR: 本文提出iSTAR框架，通过在模型参数中嵌入任务级语义结构（如隐式动态场景图），实现视觉-语言-动作（VLA）模型的功能分化，从而提升长时程任务推理的鲁棒性与泛化性，无需外部规划器或手工提示。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在任务级推理上存在不足：基于提示的上下文分解不稳定且对语言敏感；端到端长时程训练依赖大量演示且混淆高层推理与底层控制。

Method: 提出in-parameter structured task reasoning（iSTAR），将任务级语义结构（对象关系、子任务语义、任务依赖）以隐式动态场景图形式编码进模型参数，实现参数空间内的结构化推理与功能分化。

Result: 在多个操作基准上，iSTAR相比上下文提示和端到端VLA基线，实现了更可靠的 task decomposition 和更高的任务成功率。

Conclusion: 在参数空间中注入结构化知识可有效支持任务级推理，提升VLA模型的功能分化能力与跨任务变体的泛化性能。

Abstract: As robots are expected to perform increasingly diverse tasks, they must understand not only low-level actions but also the higher-level structure that determines how a task should unfold. Existing vision-language-action (VLA) models struggle with this form of task-level reasoning. They either depend on prompt-based in-context decomposition, which is unstable and sensitive to linguistic variations, or end-to-end long-horizon training, which requires large-scale demonstrations and entangles task-level reasoning with low-level control. We present in-parameter structured task reasoning (iSTAR), a framework for enhancing VLA models via functional differentiation induced by in-parameter structural reasoning. Instead of treating VLAs as monolithic policies, iSTAR embeds task-level semantic structure directly into model parameters, enabling differentiated task-level inference without external planners or handcrafted prompt inputs. This injected structure takes the form of implicit dynamic scene-graph knowledge that captures object relations, subtask semantics, and task-level dependencies in parameter space. Across diverse manipulation benchmarks, iSTAR achieves more reliable task decompositions and higher success rates than both in-context and end-to-end VLA baselines, demonstrating the effectiveness of parameter-space structural reasoning for functional differentiation and improved generalization across task variations.

</details>


### [337] ["Meet My Sidekick!": Effects of Separate Identities and Control of a Single Robot in HRI](https://arxiv.org/abs/2602.07598)
*Drake Moore,Arushi Aggarwal,Emily Taylor,Sarah Zhang,Taskin Padir,Xiang Zhi Tan*

Main category: cs.RO

TL;DR: 本文研究了当机器人不同控制域（如头部和夹持器）被呈现为独立代理时，人类合作者如何感知和信任该机器人。实验表明，用户能够区分不同控制域中的机器人身份，并将故障归因于特定身份，这为单个机器人身体内实现多机器人功能提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 机器人的能力与身份展示直接影响人类合作者对其的感知与隐性信任；而物理机器人可同时呈现多个身份并分别控制不同部位，这一特性尚未被系统研究。

Method: 开展混合设计实验，参与者随机体验三种呈现方式：单一机器人、共享完全控制的双代理（共具身）、跨控制域分割控制的双代理（分具身）；任务包括数据录入（激励支持）、独立分类（孤立故障）和协作摆放（影响人类的故障）。

Result: 参与者能感知机器人在不同控制域中的存在，并将故障准确归因于对应身份；分具身配置下身份区分与故障归因效果最显著。

Conclusion: 通过具身配置的设计（如分具身），单个机器人可在不同控制域中承载独立身份，从而在不增加硬件的前提下获得多机器人系统的感知与交互优势。

Abstract: The presentation of a robot's capability and identity directly influences a human collaborator's perception and implicit trust in the robot. Unlike humans, a physical robot can simultaneously present different identities and have them reside and control different parts of the robot. This paper presents a novel study that investigates how users perceive a robot where different robot control domains (head and gripper) are presented as independent robots. We conducted a mixed design study where participants experienced one of three presentations: a single robot, two agents with shared full control (co-embodiment), or two agents with split control across robot control domains (split-embodiment). Participants underwent three distinct tasks -- a mundane data entry task where the robot provides motivational support, an individual sorting task with isolated robot failures, and a collaborative arrangement task where the robot causes a failure that directly affects the human participant. Participants perceived the robot as residing in the different control domains and were able to associate robot failure with different identities. This work signals how future robots can leverage different embodiment configurations to obtain the benefit of multiple robots within a single body.

</details>


### [338] [LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation](https://arxiv.org/abs/2602.07629)
*Nitesh Subedi,Adam Haroon,Samuel Tetteh,Prajwal Koirala,Cody Fleming,Soumik Sarkar*

Main category: cs.RO

TL;DR: 本文提出LCLA框架，通过将感官观测对齐到专家策略的潜在表示，实现视觉-语言导航；该方法解耦感知与控制，提升跨模态和环境变化下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言导航中端到端策略优化不稳定、泛化能力差的问题，提升跨模态和环境变化下的鲁棒性。

Method: 提出LCLA框架：先用特权状态信息训练专家策略并冻结其潜在接口与动作头；再训练轻量适配器，将原始视觉-语言观测（经冻结的多模态模型提取）对齐至专家的潜在空间，将具身学习转化为监督式潜在空间对齐问题。

Result: 在室内视觉-语言导航任务上验证了LCLA的有效性：在分布内性能优异，并在未见环境、光照条件和视角下实现强零样本泛化，同时保持推理轻量。

Conclusion: LCLA通过解耦感知与控制、复用专家行为，实现了稳定、可迁移且高效的视觉-语言导航。

Abstract: We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.

</details>


### [339] [Affine Transformable Unmanned Ground Vehicle](https://arxiv.org/abs/2602.07677)
*Aron Mathias,Mohammad Ghufran,Jack Hughes,Hossein Rastgoftar*

Main category: cs.RO

TL;DR: 本文提出了一种新型仿射可变形无人地面车辆（ATUGV），通过多体系统和深度神经网络实现安全、激进的形变能力，并在硬件实验与仿真中验证了其跟踪仿射变换的能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在携带多种载荷的同时安全、激进地进行形变的无人地面车辆，以提升复杂环境下的适应性与任务灵活性。

Method: 构建包含移动机器人、供电单元、非供电载荷单元及可变形结构的多体系统；利用深度神经网络优化单元间连接结构，使各单元可在形变平面内自由运动并整体可重构地跟踪目标仿射变换；设计基于步进电机与移动机器人的控制策略，协调所有单元运动。

Result: 成功实现了ATUGV对平移、刚体旋转与形变等仿射变换的精确跟踪，并通过硬件实验与仿真验证了其功能有效性与安全性。

Conclusion: ATUGV架构及其控制方法为可变形机器人提供了新范式，具备在动态复杂环境中执行多样化任务的潜力。

Abstract: This paper develops the proof of concept for a novel affine transformable unmanned ground vehicle (ATUGV) with the capability of safe and aggressive deformation while carrying multiple payloads. The ATUGV is a multi-body system with mobile robots that can be used to power the ATUGV morphable motion, powered cells to enclose the mobile robots, unpowered cells to contain payloads, and a deformable structure to integrate cells through bars and joints. The objective is that all powered and unpowered cells motion can safely track a desired affine transformation, where an affine transformation can be decomposed into translation, rigid body rotation, and deformation. To this end, the paper first uses a deep neural network to structure cell interconnection in such a way that every cell can freely move over the deformation plane, and the entire structure can reconfigurably deform to track a desired affine transformation. Then, the mobile robots, contained by the powered cells and stepper motors, regulating the connections of the powered and unpowered cells, design the proper controls so that all cells safely track the desired affine transformation. The functionality of the proposed ATUGV is validated through hardware experimentation and simulation.

</details>


### [340] [Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples](https://arxiv.org/abs/2602.07736)
*Omar Tahri*

Main category: cs.RO

TL;DR: 本文提出了一种基于几何矩的方法，用于检测物体对称性并估计正交变换（如旋转、镜像），适用于n维空间，并在2D/3D物体上验证了其鲁棒性与效率，与迭代优化方法结合可提升对称面检测数量和计算速度。


<details>
  <summary>Details</summary>
Motivation: 识别物体的对称性有助于制定更稳定、平衡的抓取策略，从而提升机器人操作成功率。

Method: 利用几何矩构建对称性检测与正交变换（旋转、反射及其组合）估计的度量，发展出适用于n维空间的n阶矩元方法。

Result: 在2D和3D物体上验证了方法的鲁棒性和可靠性；与基于迭代优化的最先进方法相比，联合使用时在对称面数量和计算时间上均取得满意结果。

Conclusion: 基于几何矩的方法为对称性分析提供了高效、可扩展的解析框架，尤其适合实时抓取规划等应用，并能与迭代方法互补增强性能。

Abstract: Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \( n \)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.

</details>


### [341] [CoLF: Learning Consistent Leader-Follower Policies for Vision-Language-Guided Multi-Robot Cooperative Transport](https://arxiv.org/abs/2602.07776)
*Joachim Yann Despature,Kazuki Shibata,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本文提出了一种名为CoLF的多智能体强化学习框架，用于解决视觉-语言引导下的多机器人协同运输任务中的感知错位问题，通过非对称策略设计和基于互信息的训练目标实现稳定的领导者-跟随者角色分化。


<details>
  <summary>Details</summary>
Motivation: 在去中心化的视觉-语言引导多机器人协同运输中，由于视角差异和语言歧义导致的感知错位，使得各机器人对指令的理解不一致，从而影响协作效果。

Method: 提出Consistent Leader-Follower（CoLF）框架，包含两个核心部分：(1) 非对称策略设计以诱导领导者-跟随者角色分化；(2) 基于互信息的训练目标，使跟随者能从其局部观测中预测领导者的动作；整体在CTDE框架下联合优化。

Result: 在仿真和真实四足机器人实验中验证了CoLF的有效性，实现了稳定、一致的协同运输行为。

Conclusion: CoLF通过引入结构化角色分工与信息一致性约束，在无需显式通信的前提下提升了多机器人在视觉-语言引导任务中的协同鲁棒性与可靠性。

Abstract: In this study, we address vision-language-guided multi-robot cooperative transport, where each robot grounds natural-language instructions from onboard camera observations. A key challenge in this decentralized setting is perceptual misalignment across robots, where viewpoint differences and language ambiguity can yield inconsistent interpretations and degrade cooperative transport. To mitigate this problem, we adopt a dependent leader-follower design, where one robot serves as the leader and the other as the follower. Although such a leader-follower structure appears straightforward, learning with independent and symmetric agents often yields symmetric or unstable behaviors without explicit inductive biases. To address this challenge, we propose Consistent Leader-Follower (CoLF), a multi-agent reinforcement learning (MARL) framework for stable leader-follower role differentiation. CoLF consists of two key components: (1) an asymmetric policy design that induces leader-follower role differentiation, and (2) a mutual-information-based training objective that maximizes a variational lower bound, encouraging the follower to predict the leader's action from its local observation. The leader and follower policies are jointly optimized under the centralized training and decentralized execution (CTDE) framework to balance task execution and consistent cooperative behaviors. We validate CoLF in both simulation and real-robot experiments using two quadruped robots. The demonstration video is available at https://sites.google.com/view/colf/.

</details>


### [342] [RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI](https://arxiv.org/abs/2602.07837)
*Hongzhi Zang,Shu'ang Yu,Hao Lin,Tianxing Zhou,Zefang Huang,Zhen Guo,Xin Xu,Jiakai Zhou,Yuze Sheng,Shizhe Zhang,Feng Gao,Wenhao Tang,Yufeng Yue,Quanlu Zhang,Xinlei Chen,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 本文提出USER系统，旨在解决真实世界中在线策略学习的系统性挑战，通过统一硬件抽象、自适应通信平面和异步学习框架，支持多机器人协同、异构机械臂、边缘-云协作及长时程异步训练。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的在线策略学习面临数据采集难、部署异构、长时程训练困难等系统性挑战，传统算法层面的优化不足以应对，需从系统架构层面进行创新。

Method: 设计了USER系统，包括：1）统一硬件抽象层，将物理机器人与GPU同等管理；2）自适应通信平面，含隧道网络、分布式数据通道与流式多处理器感知权重同步；3）全异步学习框架，配备持久化缓存感知缓冲区与历史数据复用机制；4）可扩展的奖励、算法与策略抽象接口。

Result: 在仿真与真实世界实验中，USER成功支持多机器人协同、异构机械臂控制、大模型边缘-云联合训练以及长时间异步策略学习，验证了其统一性与可扩展性。

Conclusion: USER为真实世界在线策略学习提供了首个统一、可扩展的系统基础，将机器人视为一类核心硬件资源，推动了具身智能从仿真走向物理世界的系统化落地。

Abstract: Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.

</details>


### [343] [Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning](https://arxiv.org/abs/2602.07845)
*Yalcin Tur,Jalal Naghiyev,Haoquan Fang,Wei-Chuan Tsai,Jiafei Duan,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: 本文提出RD-VLA模型，通过潜在迭代优化实现计算自适应性，避免固定深度与线性内存增长问题，在机器人操作任务中显著提升成功率与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型采用固定计算深度，无法根据任务复杂度动态分配计算资源；Chain-of-Thought提示虽支持可变计算，但内存开销大且不适用于连续动作空间。

Method: 提出Recurrent-Depth VLA（RD-VLA），采用权值共享的循环动作头进行潜在迭代精炼，使用截断式BPTT训练，并设计基于潜在状态收敛的自适应停止准则。

Result: 在复杂操作任务中，单次迭代成功率为0%，四次迭代后超90%；简单任务快速饱和；相比先前基于推理的VLA模型，内存恒定、推理速度最高提升80倍。

Conclusion: RD-VLA为机器人测试时计算扩展提供了可扩展路径，以潜在推理替代token级推理，兼顾高效性与适应性。

Abstract: Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/

</details>


### [344] [System-Level Error Propagation and Tail-Risk Amplification in Reference-Based Robotic Navigation](https://arxiv.org/abs/2602.07846)
*Ning Hu,Maochen Li,Senhao Cao*

Main category: cs.RO

TL;DR: 本文研究了双平面X射线引导导航系统中由安装引起的结构扰动在多阶段几何感知流程中逐级放大的系统级失效机制，提出了一种统一的误差传播建模框架，并通过解析和蒙特卡洛方法分析了误差敏感通道，发现旋转安装误差是主要放大源，实验验证了该结论。


<details>
  <summary>Details</summary>
Motivation: 双平面X射线导航中，安装引起的结构扰动在多阶段感知流程中被逐步放大，导致执行层误差和尾部风险，但该系统级失效机制常被忽视。

Method: 构建统一误差传播模型，刻画安装扰动与像素观测噪声在双平面成像、投影矩阵估计、三角测量及坐标映射各环节的传播与耦合；采用一阶解析不确定度传播与蒙特卡洛仿真分析主导敏感通道和最坏情况误差。

Result: 旋转安装误差是系统级误差放大的主因，平移错位影响次之；真实双平面X射线实验验证了预测的放大趋势在实际成像条件下依然存在。

Conclusion: 揭示了基于参考的多阶段几何感知流水线存在固有结构局限性，为安全关键型机器人导航系统的系统级可靠性分析与风险感知设计提供了理论框架。

Abstract: Image guided robotic navigation systems often rely on reference based geometric perception pipelines, where accurate spatial mapping is established through multi stage estimation processes. In biplanar X ray guided navigation, such pipelines are widely used due to their real time capability and geometric interpretability. However, navigation reliability can be constrained by an overlooked system level failure mechanism in which installation induced structural perturbations introduced at the perception stage are progressively amplified along the perception reconstruction execution chain and dominate execution level error and tail risk behavior. This paper investigates this mechanism from a system level perspective and presents a unified error propagation modeling framework that characterizes how installation induced structural perturbations propagate and couple with pixel level observation noise through biplanar imaging, projection matrix estimation, triangulation, and coordinate mapping. Using first order analytic uncertainty propagation and Monte Carlo simulations, we analyze dominant sensitivity channels and quantify worst case error behavior beyond mean accuracy metrics. The results show that rotational installation error is a primary driver of system level error amplification, while translational misalignment of comparable magnitude plays a secondary role under typical biplanar geometries. Real biplanar X ray bench top experiments further confirm that the predicted amplification trends persist under realistic imaging conditions. These findings reveal a broader structural limitation of reference based multi stage geometric perception pipelines and provide a framework for system level reliability analysis and risk aware design in safety critical robotic navigation systems.

</details>


### [345] [Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model](https://arxiv.org/abs/2602.07888)
*Ning Hu,Shuai Li,Jindong Tan*

Main category: cs.RO

TL;DR: 本文提出了一种基于平行透视近似和几何误差传播建模的相机位姿估计方法，提升了近场场景下（如手术、水下）的鲁棒性与精度，同时保持高计算效率。


<details>
  <summary>Details</summary>
Motivation: 近场场景中强透视效应和异质测量噪声严重降低传统PnP解法的稳定性，亟需更鲁棒的位姿估计算法。

Method: 构建基于平行透视近似的几何误差传播框架，显式建模图像测量误差在透视几何中的传递；提出结合平行透视初始化与误差感知加权的Gauss-Newton优化方法。

Result: 在合成数据与真实图像（强光、手术照明、水下低光）上实验表明，该方法精度与鲁棒性媲美SOTA PnP方法，且计算高效。

Conclusion: 显式几何误差建模对挑战性近场环境下的可靠相机位姿估计至关重要。

Abstract: Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.

</details>


### [346] [Incremental Mapping with Measurement Synchronization & Compression](https://arxiv.org/abs/2602.07901)
*Mark Griguletskii,Danil Belov,Pavel Osinenko*

Main category: cs.RO

TL;DR: 本文提出了一种增量构建连通因子图的新方法，通过外部评估标准选择最优图拓扑，实现多传感器异步数据的高效融合，在保持建图质量的同时将优化变量（节点）减少约30%。


<details>
  <summary>Details</summary>
Motivation: 传统因子图在处理多传感器异步测量时存在图结构僵化、效率低下的问题，尤其在传感器频率差异大时；预积分等技术适用范围有限，且最优图拓扑设计仍是开放挑战。

Method: 提出一种增量式连通因子图构建方法，依据外部评估准则动态选择最优图拓扑，确保所有可用传感器数据被纳入，并支持图压缩。

Result: 在平均情况下将优化变量（节点）数量减少约30%，同时建图质量与传统方法相当。

Conclusion: 该方法有效提升了异步多传感器系统中因子图的灵活性与计算效率，为高保真定位与建图提供了更优的图结构设计范式。

Abstract: Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.

</details>


### [347] [Multi-Agent Route Planning as a QUBO Problem](https://arxiv.org/abs/2602.07913)
*Renáta Rusnáková,Martin Chovanec,Juraj Gazda*

Main category: cs.RO

TL;DR: 本文研究多智能体路径规划问题，提出了一种基于QUBO的建模方法，并通过经典与量子启发式求解器进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 提升道路网络的空间覆盖率并限制路径重叠冗余，以优化多车辆路径选择。

Method: 形式化问题定义，证明NP难性；构建QUBO模型，引入单一惩罚参数调控覆盖-重叠权衡；区分软/硬惩罚机制；设计包含实例生成、候选路径构造、QUBO矩阵构建及多种求解器（Gurobi、模拟退火、D-Wave混合量子退火）的完整求解流程。

Result: 在巴塞罗那城市实例（最多10,000辆车）上，发现明显的覆盖-重叠权衡拐点；Pareto最优解主要出现在硬惩罚机制下；D-Wave混合求解器与Gurobi获得几乎相同的目标值，仅运行时间存在微小差异。

Conclusion: 该QUBO建模框架有效支持多目标探索与近似不相交路径约束，在经典与量子混合求解器上均表现出良好可扩展性与实用性。

Abstract: Multi-Agent Route Planning considers selecting vehicles, each associated with a single predefined route, such that the spatial coverage of a road network is increased while redundant overlaps are limited. This paper gives a formal problem definition, proves NP-hardness by reduction from the Weighted Set Packing problem, and derives a Quadratic Unconstrained Binary Optimization formulation whose coefficients directly encode unique coverage rewards and pairwise overlap penalties. A single penalty parameter controls the coverage-overlap trade-off. We distinguish between a soft regime, which supports multi-objective exploration, and a hard regime, in which the penalty is strong enough to effectively enforce near-disjoint routes. We describe a practical pipeline for generating city instances, constructing candidate routes, building the QUBO matrix, and solving it with an exact mixed-integer solver (Gurobi), simulated annealing, and D-Wave hybrid quantum annealing. Experiments on Barcelona instances with up to 10 000 vehicles reveal a clear coverage-overlap knee and show that Pareto-optimal solutions are mainly obtained under the hard-penalty regime, while D-Wave hybrid solvers and Gurobi achieve essentially identical objective values with only minor differences in runtime as problem size grows.

</details>


### [348] [Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities](https://arxiv.org/abs/2602.07924)
*Nur Ahmad Khatim,Mansur Arief*

Main category: cs.RO

TL;DR: 本文提出了人类-机器人协同调度设施选址问题（HRCD-FLP），在考虑基础设施关键性分层、人机监管比例约束及最低利用率要求的前提下，优化石油基础设施安防中的指挥中心选址；结果表明提升自主化水平可显著降低成本并保障全覆盖，且所提方法在大小规模问题上均具有效性。


<details>
  <summary>Details</summary>
Motivation: 传统设施选址模型假设资源同质，无法应对石油基础设施安防中需平衡自主系统效率与人类判断以进行威胁升级的实际挑战。

Method: 构建了带容量限制的设施选址新变体——HRCD-FLP，整合基础设施关键性分层、人机监督比约束和最小利用率要求，并在三种技术成熟度场景下评估指挥中心选址；对小规模问题采用精确算法，大规模问题采用所提启发式算法。

Result: 从保守（1:3人机比）转向未来高自主（1:10）可显著降低成本并保持关键设施全覆盖；小规模问题上精确法更优；大规模问题上启发式算法可在3分钟内得可行解，最优性差距约14%。

Conclusion: 面向人机协同的优化规划是实现成本效益与任务可靠性兼备部署的关键。

Abstract: Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.

</details>


### [349] [Feasibility-Guided Planning over Multi-Specialized Locomotion Policies](https://arxiv.org/abs/2602.07932)
*Ying-Sheng Luo,Lu-Ching Wang,Hanjaya Mandala,Yu-Lun Chou,Guilherme Christmann,Yu-Chung Chen,Yung-Shun Chan,Chun-Yi Lee,Wei-Chao Chen*

Main category: cs.RO

TL;DR: 本文提出了一种可行性引导的规划框架，通过为每个地形特定策略配备一个Feasibility-Net来预测可行性张量，从而在不牺牲可解释性或需重新训练的前提下，将多个专家策略集成到经典规划算法中。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持可解释性的同时灵活集成多个技能特定策略，且传统规划器无法直接利用这些策略。

Method: 提出可行性引导的规划框架，每个地形策略配有一个Feasibility-Net，用于基于局部高程图和任务向量预测可行性张量，并将其嵌入经典规划算法中。

Result: 在仿真与真实世界实验中，该方法能在多样复杂地形上高效生成可靠路径，并始终与底层策略能力保持一致。

Conclusion: 该框架成功解决了多专家策略集成难题，在保证可解释性与灵活性的同时提升了非结构化地形下的运动规划性能。

Abstract: Planning over unstructured terrain presents a significant challenge in the field of legged robotics. Although recent works in reinforcement learning have yielded various locomotion strategies, planning over multiple experts remains a complex issue. Existing approaches encounter several constraints: traditional planners are unable to integrate skill-specific policies, whereas hierarchical learning frameworks often lose interpretability and require retraining whenever new policies are added. In this paper, we propose a feasibility-guided planning framework that successfully incorporates multiple terrain-specific policies. Each policy is paired with a Feasibility-Net, which learned to predict feasibility tensors based on the local elevation maps and task vectors. This integration allows classical planning algorithms to derive optimal paths. Through both simulated and real-world experiments, we demonstrate that our method efficiently generates reliable plans across diverse and challenging terrains, while consistently aligning with the capabilities of the underlying policies.

</details>


### [350] [Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control](https://arxiv.org/abs/2602.07984)
*Simon Sagmeister,Panagiotis Kounatidis,Sven Goblirsch,Markus Lienkamp*

Main category: cs.RO

TL;DR: 本文研究了车辆动力学建模保真度对轨迹跟踪控制器闭环行为的影响，提出了一种Autoware兼容的综合车辆模型，并通过简化得到不同保真度模型；基于550多次仿真运行及真实赛道数据（267 km/h、15 m/s²）量化模型精度，分析简化程度与加速度边界裕度的关系，从而为不同应用场景下的模型简化提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶控制算法评估中车辆动力学模型保真度不一，导致算法性能难以公平比较，亟需明确模型简化程度与控制性能之间的关系。

Method: 构建Autoware兼容的高保真车辆模型，并系统简化生成多级保真度模型；在550+次仿真中对比各模型输出与真实赛道数据（TUM车队在蒙扎赛道的最快无人圈速），分析模型误差随加速度边界裕度的变化规律。

Result: 量化了不同保真度模型对真实车辆动态的逼近能力；发现模型简化影响显著依赖于距加速度极限的裕度；据此提出了面向不同应用需求（如开发、验证、实时性要求）的模型保真度选择准则。

Conclusion: 车辆动力学模型的可简化程度高度依赖具体应用场景及其对加速度边界的接近程度；本文为自动驾驶控制算法评估中模型保真度的选择提供了数据驱动的实用指南。

Abstract: Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms. Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data. Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023. They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.

</details>


### [351] [From Ellipsoids to Midair Control of Dynamic Hitches](https://arxiv.org/abs/2602.08116)
*Jiawei Xu,Subhrajit Bhattacharya,David Saldaña*

Main category: cs.RO

TL;DR: 本文提出了一种基于椭球的运动学模型，用于连接双缆形成的绞合结构几何特性与四架无人机驱动的动力学，并设计了结合控制李雅普诺夫函数和高阶控制屏障函数的QP控制器，实现对绞合位置与系统形状的精确跟踪及电缆张紧等安全约束。


<details>
  <summary>Details</summary>
Motivation: 动态操控由多架无人机牵引的缆绳之间的相互作用（如绞合、打结）可显著提升空中缆系操作的灵活性与适应性；而现有研究缺乏对这类绞合结构的动力学建模与安全控制方法。

Method: 提出椭球基运动学模型，将双缆绞合的几何特征映射为四机系统的控制仿射动力学；构建CLF-HOCBF-QP控制器，将几何参考构型转化为无人机目标位姿，并引入复合误差使李雅普诺夫函数相对阶为1。

Result: 数值仿真验证了该方法能稳定、高速地跟踪动态参考轨迹，同时保证电缆始终处于张紧状态等安全约束。

Conclusion: 所提模型与控制器为缆系空中协同操作中复杂缆间交互（如绞合、缠绕）提供了可建模、可分析、可控制的理论与方法框架。

Abstract: The ability to dynamically manipulate interaction between cables, carried by pairs of aerial vehicles attached to the ends of each cable, can greatly improve the versatility and agility of cable-assisted aerial manipulation. Such interlacing cables create hitches by winding two or more cables around each other, which can enclose payloads or can further develop into knots. Dynamic modeling and control of such hitches is key to mastering the inter-cable manipulation in context of cable-suspended aerial manipulation. This paper introduces an ellipsoid-based kinematic model to connect the geometric nature of a hitch created by two cables and the dynamics of the hitch driven by four aerial vehicles, which reveals the control-affine form of the system. As the constraint for maintaining tension of a cable is also control-affine, we design a quadratic programming-based controller that combines Control Lyapunov and High-Order Control Barrier Functions (CLF-HOCBF-QP) to precisely track a desired hitch position and system shape while enforcing safety constraints like cable tautness. We convert desired geometric reference configurations into target robot positions and introduce a composite error into the Lyapunov function to ensure a relative degree of one to the input. Numerical simulations validate our approach, demonstrating stable, high-speed tracking of dynamic references.

</details>


### [352] [Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning](https://arxiv.org/abs/2602.08167)
*Milan Ganai,Katie Luo,Jonas Frey,Clark Barrett,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出R&B-EnCoRe方法，通过重要性加权变分推断将推理建模为隐变量，实现无需外部奖励、验证器或人工标注的自监督推理精炼，显著提升多类具身任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有具身链式推理方法依赖刚性模板指定推理原语，易引入无关信息干扰动作预测，形成‘无好策略则无法验证推理质量，无高质量推理则无法构建鲁棒策略’的循环瓶颈。

Method: 提出R&B-EnCoRe框架，将推理视为重要性加权变分推断中的隐变量，利用互联网规模知识进行自监督精炼，自动生成并蒸馏面向具身任务的推理训练数据集。

Result: 在仿真与硬件操作（Franka Panda、WidowX）、多种腿式导航（双足、轮式、自行车、四足）及自动驾驶任务上验证，相较基线模型，操作成功率提升28%，导航得分提升101%，碰撞率降低21%。

Conclusion: R&B-EnCoRe能自动蒸馏出对成功控制具有预测性的推理过程，在避免人工标注工程的同时，将互联网知识有效落地于物理执行。

Abstract: Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>


### [353] [Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments](https://arxiv.org/abs/2602.08189)
*Seoyeon Jang,Alex Junho Lee,I Made Aswin Nahrendra,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种双头网络，用于移动机器人在动态环境中的在线变化检测与长期地图维护，并通过合成数据增强策略缓解真实世界标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在瞬态动态环境（如施工场地、频繁重配置的室内空间）中有效检测变化并更新地图，且真实世界数据的采集与对齐困难、耗时耗力。

Method: 设计了一个双头神经网络，分别处理在线变化检测和长期地图维护；提出一种基于跨场景元素导入的数据增强策略，以合成结构变化，减少对人工标注的依赖。

Result: 在真实施工场地和室内办公环境中实验验证，该方法具有良好的跨场景泛化能力，实现了高效准确的地图更新。

Conclusion: 所提方法有效解决了动态环境下在线变化检测与地图持续更新的难题，通过数据增强降低了对真实标注数据的依赖，提升了实用性与可扩展性。

Abstract: Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.

</details>


### [354] [STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction](https://arxiv.org/abs/2602.08245)
*Jinhao Li,Yuxuan Cong,Yingqiao Wang,Hao Xia,Shan Huang,Yijia Zhang,Ningyi Xu,Guohao Dai*

Main category: cs.RO

TL;DR: 本文提出STEP方法，通过轻量级时空一致性预测机制和速度感知扰动注入机制，显著降低扩散策略的推理延迟，同时提升动作质量和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现出色，但迭代去噪导致高推理延迟，限制了实时闭环控制的频率。现有加速方法难以同时保证动作质量和低延迟。

Method: 提出STEP方法：1）轻量级时空一致性预测机制，生成高质量的热启动动作；2）速度感知扰动注入机制，自适应调节执行激励以防止执行停滞；3）理论分析证明该预测诱导局部收缩映射，确保扩散精炼过程中动作误差收敛。

Result: 在九个仿真基准和两个真实世界任务上评估，STEP仅用2步采样，在RoboMimic基准和真实任务中分别比BRIDGER和DDIM平均高出21.6%和27.5%的成功率。

Conclusion: STEP在推理延迟与成功率之间实现了更优的Pareto前沿，有效平衡了生成能力与实时性。

Abstract: Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>


### [355] [Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control](https://arxiv.org/abs/2602.08251)
*Yuanzhu Zhan,Yufei Jiang,Muqing Cao,Junyi Geng*

Main category: cs.RO

TL;DR: 本文提出了一种完全基于机载传感器的感知-控制一体化方案，用于实现无需外部动捕的接触丰富型空中操作（AM），通过增强型视觉惯性里程计（VIO）与图像视觉伺服（IBVS）结合混合力/运动控制器，在接触时显著提升速度估计精度并实现稳定接触力控制。


<details>
  <summary>Details</summary>
Motivation: 现有空中操作大多依赖外部运动捕捉系统且侧重位置控制，限制了其野外部署能力；亟需一种不依赖外部设备、能实现精确运动跟踪与接触力调控的全机载方案。

Method: 提出一种增强型视觉惯性里程计（含接触一致性因子）与图像视觉伺服（IBVS）相结合的方法，并设计混合力-运动控制器，以解耦感知与控制、调节接触力和横向运动。

Result: 实验表明该方法仅用机载传感器即可闭环实现从感知到接触力的控制，在接触阶段速度估计精度提升66.01%，具备可靠目标接近与稳定力保持能力。

Conclusion: 所提全机载感知-控制框架显著提升了空中操作在真实环境中的可部署性与鲁棒性，为野外应用的接触丰富型AM提供了可行路径。

Abstract: Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.

</details>


### [356] [Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes](https://arxiv.org/abs/2602.08266)
*Seunghoon Jeong,Eunho Lee,Jeongyun Kim,Ayoung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种面向实例的下一最佳视角（NBV）策略，结合对象语义信息改进3D高斯点绘（3DGS）的主动重建，显著降低深度误差，并提升机器人操作任务中的重建鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于几何线索的NBV方法忽略操作相关的语义信息，偏向利用而非探索，难以应对遮挡与不完整观测下的可靠重建需求。

Method: 提出对象感知的3DGS框架，将实例级信息编码为one-hot对象向量，计算置信加权的信息增益，引导选择未充分探索且高不确定性的高斯区域；支持场景级和目标对象级两种NBV模式。

Result: 在合成数据集上深度误差降低77.14%，GraspNet真实数据集降低34.10%；针对特定物体的NBV进一步降低其深度误差25.60%；在真实机器人操作任务中验证了有效性。

Conclusion: 融合对象语义的实例感知NBV策略能更有效地指导主动重建，提升3DGS在复杂、遮挡场景及机器人操作中的鲁棒性与精度。

Abstract: In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.

</details>


### [357] [DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer](https://arxiv.org/abs/2602.08278)
*Ke Zhang,Lixin Xu,Chengyi Song,Junzhe Xu,Xiaoyi Lin,Zeyu Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出DexFormer，一种基于改进Transformer架构的端到端、动力学感知的跨本体灵巧操作策略，能通过历史观测实时推断不同灵巧手的形态与动力学特性，实现零样本迁移至Leap Hand、Allegro Hand和Rapid Hand等异构手部平台。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中因不同灵巧手在运动学和动力学上的差异（即本体变异性）导致需为每种手单独训练策略或依赖共享动作空间加特定解码头的问题。

Method: 构建基于改进Transformer主干的跨本体策略DexFormer，以历史观测为条件，利用时序上下文在线推断本体形态与动力学；在多种程序生成的灵巧手资产上进行训练。

Result: DexFormer在Leap Hand、Allegro Hand和Rapid Hand上展现出强零样本迁移能力，单策略可泛化至多种异构手部本体。

Conclusion: 证明了单一、动力学感知的跨本体策略可作为灵巧操作的可扩展基础，显著降低对特定硬件策略定制的依赖。

Abstract: Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.

</details>


### [358] [ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects](https://arxiv.org/abs/2602.08285)
*Josh Pinskier,Sarah Baldwin,Stephen Rodan,David Howard*

Main category: cs.RO

TL;DR: 本文提出ReefFlex，一种生成式软指设计方法，用于安全抓取脆弱且几何异质的珊瑚，以支持珊瑚礁修复。该方法通过编码异质抓取为简化运动基元，实现多目标优化，并在珊瑚养殖设施中验证了其在抓取成功率、质量及减少不良事件方面的优势。


<details>
  <summary>Details</summary>
Motivation: 当前气候变化、入侵物种和人类活动正以前所未有的速度破坏全球珊瑚礁，亟需可扩展的珊瑚再生技术；但缺乏安全、鲁棒的工具来处理脆弱珊瑚成为瓶颈。

Method: 提出ReefFlex生成式软指设计方法，将异质抓取编码为一组简化运动基元，构建可解的多目标优化问题，探索多样化软指构型空间，并设计面向珊瑚修复的软体机器人进行实验验证。

Result: ReefFlex显著提升了珊瑚抓取成功率与抓取质量（抗扰动性、定位精度），并降低了操作过程中的不良事件发生率，优于基准设计。

Conclusion: ReefFlex为复杂脆弱物体的软体末端执行器设计提供了通用方法，推动珊瑚修复等此前难以自动化的领域迈向自动化。

Abstract: Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.

</details>


### [359] [Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework](https://arxiv.org/abs/2602.08298)
*Yuxin Zhang,Cheng Wang,Hubert P. H. Shum*

Main category: cs.RO

TL;DR: 本文提出驾驶员基础模型（DFM）框架，旨在通过大规模数据收集、定义核心功能及技术实现路径，系统性地提升自动驾驶汽车在安全性、舒适性、通勤效率与能源经济性方面的性能，并建立以人为中心的评估基准。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车虽具潜力，但因在安全性、舒适性、通勤效率和能源经济性等方面仍逊于人类老司机，导致市场接受度低；亟需一种新范式来系统化地规范、验证与评估AV性能。

Method: 提出驾驶员基础模型（DFM）概念，设计大规模驾驶数据采集策略，界定DFM应具备的核心功能（如安全包络建模、能效评估等），并探讨实现这些功能的技术路径。

Result: 构建了DFM框架，明确了其在AV全运行周期中的应用价值，包括定义人本安全边界、建立能源经济性基准等，为AV的系统性验证与标准化提供新范式。

Conclusion: DFM有望成为连接人类驾驶行为与自动驾驶系统能力的关键桥梁，推动AV从工程实现迈向科学化、可衡量、可验证的新阶段。

Abstract: Autonomous vehicles (AVs) are poised to revolutionize global transportation systems. However, its widespread acceptance and market penetration remain significantly below expectations. This gap is primarily driven by persistent challenges in safety, comfort, commuting efficiency and energy economy when compared to the performance of experienced human drivers. We hypothesize that these challenges can be addressed through the development of a driver foundation model (DFM). Accordingly, we propose a framework for establishing DFMs to comprehensively benchmark AVs. Specifically, we describe a large-scale dataset collection strategy for training a DFM, discuss the core functionalities such a model should possess, and explore potential technical solutions to realize these functionalities. We further present the utility of the DFM across the operational spectrum, from defining human-centric safety envelopes to establishing benchmarks for energy economy. Overall, We aim to formalize the DFM concept and introduce a new paradigm for the systematic specification, verification and validation of AVs.

</details>


### [360] [Personalized Autonomous Driving via Optimal Control with Clearance Constraints from Questionnaires](https://arxiv.org/abs/2602.08326)
*Yongjae Lim,Dabin Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了一种融合用户偏好的自动驾驶规划框架，通过定制化问卷获取用户对周围车辆安全距离的偏好，并将其作为约束嵌入最优控制问题；为降低计算复杂度，采用场景分解与并行求解策略，并通过仿真验证其偏好对齐效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统驾驶规划未考虑用户对周围车辆安全距离的个性化偏好，易导致乘坐不适；需设计能显式建模并满足用户偏好的规划方法。

Method: 设计面向交互因素（如周围车尺寸、速度、位置、行为及自车行为）的精简问卷，将用户反馈的安全距离偏好转化为最优控制约束；针对全场景联合优化计算不可行的问题，采用按固定场景分解、并行求解、基于原目标函数选优的近似策略。

Result: 仿真结果表明，所提规划器能更有效地反映不同用户的距离偏好，在偏好对齐度指标上显著优于不考虑偏好的基线规划器。

Conclusion: 显式建模和融入用户安全距离偏好是提升自动驾驶舒适性与可接受性的有效途径；基于场景分解的近似优化策略可在保证偏好一致性的同时满足实时性要求。

Abstract: Driving without considering the preferred separation distance from surrounding vehicles may cause discomfort for users. To address this limitation, we propose a planning framework that explicitly incorporates user preferences regarding the desired level of safe clearance from surrounding vehicles. We design a questionnaire purposefully tailored to capture user preferences relevant to our framework, while minimizing unnecessary questions. Specifically, the questionnaire considers various interaction-relevant factors, including the surrounding vehicle's size, speed, position, and maneuvers of surrounding vehicles, as well as the maneuvers of the ego vehicle. The response indicates the user-preferred clearance for the scenario defined by the question and is incorporated as constraints in the optimal control problem. However, it is impractical to account for all possible scenarios that may arise in a driving environment within a single optimal control problem, as the resulting computational complexity renders real-time implementation infeasible. To overcome this limitation, we approximate the original problem by decomposing it into multiple subproblems, each dealing with one fixed scenario. We then solve these subproblems in parallel and select one using the cost function from the original problem. To validate our work, we conduct simulations using different user responses to the questionnaire. We assess how effectively our planner reflects user preferences compared to preference-agnostic baseline planners by measuring preference alignment.

</details>


### [361] [Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation](https://arxiv.org/abs/2602.08328)
*Yi-Hsuan Hsiao,Quang Phuc Kieu,Zhongtao Guan,Suhan Kim,Jiaze Cai,Owen Matteson,Jonathan P. How,Elizabeth Farrell Helbling,YuFeng Chen*

Main category: cs.RO

TL;DR: 本文提出了一种仅依赖机载传感与计算即可实现悬停和轨迹跟踪的1.29克微型空中机器人，实现了厘米级定位精度，并在无动作捕捉系统环境下成功完成避障与着陆任务。


<details>
  <summary>Details</summary>
Motivation: 现有昆虫尺度空中机器人受限于需依赖外部传感器与计算，难以在真实复杂环境中（如搜索救援、精准农业）应用；而自然界的空中昆虫却能自主导航密集植被，该差距促使研究者开发具备完全机载感知与计算能力的微型机器人。

Method: 设计并集成轻量化的传感器套件、状态估计算法和低层控制器以实现高精度位置控制；构建分层控制器，由人类操作员提供高层指令，机器人自主执行底层运动；在无动作捕捉的室外环境中进行飞行实验验证。

Result: 机器人实现了厘米级定位精度的悬停与轨迹跟踪；在30秒室外飞行中成功避障并精准降落在向日葵上；首次在如此小尺寸机器人上实现全机载感知、估计与控制闭环。

Conclusion: 该工作显著提升了微型空中机器人的自主性，为未来机载路径规划与能量自主等方向奠定了基础，推动了微型机器人在真实场景中的实用化进展。

Abstract: Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.

</details>


### [362] [Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving](https://arxiv.org/abs/2602.08334)
*Xuanjin Jin,Yanxin Dong,Bin Sun,Huan Xu,Zhihui Hao,XianPeng Lang,Panpan Cai*

Main category: cs.RO

TL;DR: 本文提出了Vec-QMDP，一种面向现代CPU SIMD架构的原生并行POMDP规划器，通过数据导向设计与层次化并行策略，实现227–1073倍加速，支持毫秒级实时不确定性规划。


<details>
  <summary>Details</summary>
Motivation: 现有混合CPU-GPU求解器受限于主机-设备同步延迟和SIMT架构上的分支发散，难以满足真实机器人（如自动驾驶）对实时规划的需求。

Method: 提出CPU-native的Vec-QMDP规划器：采用数据导向设计（DOD）重构内存布局以提升缓存效率；设计层次化并行机制——跨CPU核心分发子树、在SIMD通道内向量化树扩展与碰撞检测；引入UCB负载均衡和向量化STR-tree用于粗粒度碰撞检查。

Result: 在大规模自动驾驶基准测试中，相比当前最优串行规划器获得227×–1073×加速，达到毫秒级规划延迟，性能达业界领先水平。

Conclusion: 证明现代CPU凭借其SIMD能力和良好缓存特性，可作为大规模不确定性规划的高性能计算平台，无需依赖GPU即可实现实时、高扩展性规划。

Abstract: Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\times$--$1073\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty.

</details>


### [363] [Learning Human-Like Badminton Skills for Humanoid Robots](https://arxiv.org/abs/2602.08370)
*Yeke Chen,Shihao Dong,Xiaoyu Ji,Jingkai Sun,Zeren Luo,Liu Zhao,Jiahui Zhang,Wanyue Li,Ji Ma,Bowen Xu,Yimin Han,Yudong Zhao,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了一种名为'Imitation-to-Interaction'的渐进式强化学习框架，使机器人能从模仿人类动作进化为具备物理感知能力的击球手，成功实现从仿真到真实世界的零样本迁移，完成羽毛球中的挑球与吊球等动作。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人在高要求运动（如羽毛球）中难以兼顾爆发性全身协调与精准、时效性极高的拦截能力；单纯的动作模仿无法保证物理合理性和功能性击球。

Method: 提出Imitation-to-Interaction框架：1）基于人类数据构建鲁棒运动先验；2）蒸馏为紧凑的模型驱动状态表征；3）引入对抗先验稳定动力学；4）设计流形扩展策略，将离散击球点泛化为稠密交互体积，缓解专家示范稀疏问题。

Result: 在仿真中掌握了挑球、吊球等多种羽毛球技能；首次实现类人羽毛球技能的零样本sim-to-real迁移，在真实 humanoid 机器人上复现了人类运动员的动能优雅性与功能精确性。

Conclusion: 该框架有效弥合了动作模仿与物理感知交互之间的鸿沟，为复杂动态运动技能的机器人学习提供了可扩展、可迁移的新范式。

Abstract: Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>


### [364] [BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models](https://arxiv.org/abs/2602.08392)
*Xin Wu,Zhixuan Liang,Yue Ma,Mengkang Hu,Zhiyuan Qin,Xiu Li*

Main category: cs.RO

TL;DR: 本文提出了BiManiBench，一个用于评估多模态大语言模型（MLLMs）在双臂操作任务中能力的分层基准，揭示了当前MLLMs在双臂空间定位与协同控制方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有机器人智能评测框架主要局限于单臂操作，无法反映双臂协同所需的时空协调能力，如抬起重锅等任务。

Method: 提出分三层（基础空间推理、高层动作规划、底层末端执行器控制）的双臂操作基准BiManiBench，专门隔离双臂特有挑战（如可达性、运动学约束），以区分感知幻觉与规划失败。

Result: 对30多个SOTA模型的分析表明，MLLMs虽具备较强高层推理能力，但在双臂空间接地与控制上表现差，常出现相互干扰和时序错误。

Conclusion: 当前MLLM范式缺乏对双臂间运动学约束的深层理解，未来研究应聚焦于双臂避碰与细粒度时间序列建模。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>


### [365] [Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion](https://arxiv.org/abs/2602.08417)
*Wentao Zhao,Yihe Niu,Zikun Chen,Rui Li,Yanbo Wang,Tianchen Deng,Jingchuan Wang*

Main category: cs.RO

TL;DR: 本文提出了Graph-Loc，一种基于图的LiDAR定位框架，利用轻量级点线图作为紧凑结构先验地图，在部分、重复、严重遮挡的LiDAR观测下实现鲁棒、稳定的位姿跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统基于地图的LiDAR定位面临地图存储与检索效率低、在线观测常存在部分性、重复性与严重遮挡等问题，亟需一种兼顾紧凑性与鲁棒性的新方法。

Method: 构建轻量级点线图作为结构先验（可源自栅格地图矢量化、CAD/平面图等异构源）；对每帧LiDAR扫描提取稀疏点线形成观测图；通过LiDAR光线模拟检索姿态相关的可见子图；采用带局部图上下文正则项的非平衡最优传输进行扫描-地图匹配；并基于法线矩阵估计信息各向异性，动态延迟弱约束方向的更新。

Result: 在公开基准、受控压力测试和真实部署中均验证了Graph-Loc的高精度与稳定性，仅需KB级异构地图先验，且在几何退化、持续遮挡及场景缓慢变化下仍保持性能。

Conclusion: Graph-Loc为长期自主运行提供了高效、鲁棒的轻量化LiDAR定位方案，显著提升了在复杂现实条件下的实用性与泛化能力。

Abstract: Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.

</details>


### [366] [Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric](https://arxiv.org/abs/2602.08421)
*Farhad Keramat,Salma Salimi,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人任务规划的去中心化LLM预言机架构，结合新的聚合方法和基于Hyperledger Fabric的多机器人基础设施，解决LLM服务垄断与任务时序敏感性问题，并通过SkillChain-RTD基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在人机交互中引发安全与隐私挑战（如自我偏好），而当前LLM预言机的聚合方法依赖语义相似性，不适用于强调任务时序的机器人规划。

Method: 提出一种面向机器人任务规划的新型LLM输出聚合方法，并构建基于Hyperledger Fabric的去中心化多机器人基础设施，支持自然语言意图解析、子任务分解、跨厂商机器人协同及细粒度数据访问控制。

Result: 在自建公开基准SkillChain-RTD上的实验表明，所提架构可行，且新聚合方法性能优于现有方法。

Conclusion: 该工作为构建可信、去中心化、时序敏感的LLM驱动机器人系统提供了可行框架与实践验证。

Abstract: Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.

</details>


### [367] [Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence](https://arxiv.org/abs/2602.08425)
*Jinxian Zhou,Ruihai Wu,Yiwei Liu,Yiwen Hou,Xunzhe Zhou,Checheng Yu,Licheng Zhong,Lin Shao*

Main category: cs.RO

TL;DR: 本文提出Bi-Adapt框架，利用视觉基础模型实现双臂操作在新物体类别上的高效零样本泛化，仅需少量微调数据即可跨类别迁移。


<details>
  <summary>Details</summary>
Motivation: 现有双臂操作方法依赖大量数据、泛化能力差，难以应对未见过的新类别物体。

Method: 提出Bi-Adapt框架，基于视觉基础模型构建语义对应关系，实现跨类别的功能映射，并通过有限数据微调实现零样本泛化。

Result: 在仿真与真实环境中实验表明，该方法在新类别任务上以极少量数据即达到高成功率，泛化能力强、效率高。

Conclusion: Bi-Adapt为双臂操作提供了高效、低数据依赖的跨类别泛化新范式，显著提升了机器人对未知物体的操作适应性。

Abstract: Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/

</details>


### [368] [SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios](https://arxiv.org/abs/2602.08440)
*Tian Gao,Celine Tan,Catherine Glossop,Timothy Gao,Jiankai Sun,Kyle Stachowicz,Shirley Wu,Oier Mees,Dorsa Sadigh,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出SteerVLA框架，利用大视觉语言模型（VLM）生成细粒度语言指令，引导视觉-语言-动作（VLA）驾驶策略，实现高层语义推理与低层反应控制的协同，显著提升长尾场景下的自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中高层语义推理（应对长尾事件）与低层反应式控制（保障鲁棒性）难以融合的根本挑战；现有VLM缺乏驾驶场景的具身经验，而端到端策略又缺乏常识推理能力。

Method: 提出SteerVLA：1）构建VLM与VLA之间的细粒度语言接口，使VLM输出可执行的语言指令来‘引导’VLA策略；2）利用VLM对现有驾驶数据进行增强，生成与车辆控制对齐的详细语言标注，提供高质量语言监督。

Result: 在具有挑战性的闭环评测基准上，SteerVLA整体驾驶得分超越SOTA方法4.77分，在长尾子集上领先8.04分。

Conclusion: 通过引入可解释、可 steer 的语言接口，将VLM的世界知识有效注入驾驶控制回路，是提升自动驾驶系统泛化性与安全性的重要路径。

Abstract: A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.

</details>


### [369] [Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions](https://arxiv.org/abs/2602.08444)
*Samsaptak Ghosh,M. Felix Orlando,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: 本文提出了一种结构化启发式恢复控制律，用于自主车辆在碰撞后轨迹的快速恢复，通过联合控制转向与驱动力，并考虑纵向速度时变性及非线性转向耦合效应。


<details>
  <summary>Details</summary>
Motivation: 碰撞后车辆易因侧向运动和偏航瞬态而迅速偏离预期路径，亟需安全可靠的轨迹恢复能力。

Method: 基于广义单轨阿克曼车辆模型，设计联合控制转向与牵引力的启发式恢复控制律，显式建模时变纵向速度对横摆-侧向动力学的影响，并保留常被简化的非线性转向耦合项。

Result: 在MATLAB中基于所提广义单轨模型及标准3DOF单轨模型的仿真验证表明，该方法在多种典型碰撞后初始条件下均能实现一致、稳定的轨迹恢复性能。

Conclusion: 所提控制律更真实地刻画了碰撞后瞬态工况下的动力学特性，相比假设恒定纵向速度的方法，在恢复性能上更具鲁棒性和实用性。

Abstract: Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.

</details>


### [370] [UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials](https://arxiv.org/abs/2602.08450)
*Stefan Ivić,Luka Lanča,Karlo Jakac,Ante Sikirica,Stella Dumenčić,Matej Mališa,Zvonimir Mrle,Bojan Crnković*

Main category: cs.RO

TL;DR: 本文提出了一种集成流场重建、动态概率建模、搜索控制与机器视觉检测的自主海上搜救系统，并在克罗地亚瓦伦湾进行了实地验证，结果表明该紧耦合方法能在复杂环境和不确定性下可靠检测漂浮目标。


<details>
  <summary>Details</summary>
Motivation: 提升自主海上搜救系统在真实复杂海洋环境和不确定性条件下的目标检测可靠性。

Method: 整合流场重建（基于CFD与数值优化）、动态 probabilistic 建模、多无人机协同搜索控制及深度学习驱动的机器视觉检测。

Result: 实地实验成功实现漂浮目标的实时、可靠检测，验证了紧耦合方法在真实海洋环境中的有效性。

Conclusion: 紧耦合的多技术融合方法为未来自主海上搜救与救援应用提供了可行路径与实践依据。

Abstract: This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.

</details>


### [371] [Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment](https://arxiv.org/abs/2602.08466)
*Ning Hu,Senhao Cao,Maochen Li*

Main category: cs.RO

TL;DR: 本文揭示了视觉引导机器人在近场和离轴配置下执行失败的根本原因——几何误差放大机制，并提出了一种执行级的可靠性感知门控机制，在不改变姿态估计精度的前提下显著提升任务成功率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管姿态估计精度已大幅提升，但实际机器人系统在姿态估计看似准确的情况下仍频繁失败，表明仅靠姿态精度不足以保证执行可靠性。

Method: 提出一种可靠性感知的执行门控机制（Reliability-aware Execution Gating），在执行前评估几何一致性和构型风险，选择性拒绝或缩放高风险姿态更新，且不依赖具体姿态估计算法。

Result: 在UR5平台上实验验证：显著提升任务成功率、降低执行方差、抑制尾部风险行为，同时保持平均姿态精度基本不变。

Conclusion: 执行级可靠性建模对近场视觉引导机器人至关重要；所提门控机制 estimator-agnostic、即插即用，为提升系统鲁棒性提供了实用新路径。

Abstract: Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>


### [372] [Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi](https://arxiv.org/abs/2602.08518)
*Kento Kawaharazuka,Kei Okada,Masayuki Inaba*

Main category: cs.RO

TL;DR: 本文对肌骨骼人形机器人（如Kengoro和Musashi）的结构特性进行了系统分类与分析，提出五大固有属性（冗余性、独立性、各向异性、可变力臂、非线性弹性），并探讨其在本体感知学习、反射控制、肌肉分组及运动实现中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有肌骨骼人形机器人研究缺乏对结构固有特性的统一归纳与系统性管理方法，亟需建立结构特性与控制策略之间的关联框架。

Method: 基于自主研发的Kengoro与Musashi机器人，对肌骨骼结构进行归纳分析，提炼出五类核心物理属性，并结合本体感知学习、反射控制、肌肉分组等控制策略进行系统性组织与验证。

Result: 明确了肌骨骼结构的五大属性及其组合带来的优势与局限；建立了属性—控制功能映射关系；实现了集成化运动控制系统，并识别出若干关键挑战。

Conclusion: 肌骨骼人形机器人的潜力不仅在于仿生形态，更在于其多维物理属性的协同利用；未来需进一步发展面向属性的建模、学习与自适应控制方法。

Abstract: Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.

</details>


### [373] [UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation](https://arxiv.org/abs/2602.08537)
*Haoming Ye,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: UniPlan 是一个面向大规模室内环境的视觉-语言任务规划系统，通过将场景拓扑、视觉信息与机器人能力统一建模为PDDL，扩展了现有桌面操作领域至移动-操作长周期任务，显著提升了任务成功率、规划质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM与符号规划的方法（如UniDomain）局限于桌面操作，难以应对真实室内环境中涉及导航、过门、双臂协同等复杂移动-操作任务的需求。

Method: UniPlan 将视觉-拓扑地图（含导航地标与锚定图像）与程序化扩展的PDDL领域结合：首先用VLM在锚定图像中识别任务相关物体及其PDDL状态；然后重构为高连通性压缩拓扑图（PDDL表示），边权由原图导出；最后调用标准PDDL求解器生成移动-操作规划。

Result: 在含真实影像的大规模室内地图及人工提出任务上的评测表明，UniPlan 在成功率、规划质量与计算效率上均显著优于纯VLM及LLM+PDDL方法。

Conclusion: 统一建模视觉、拓扑与机器人能力的PDDL框架，能有效支撑复杂室内环境下的长周期移动-操作任务规划，验证了符号与感知深度融合的可行性与优势。

Abstract: Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.

</details>


### [374] [Constrained Sampling to Guide Universal Manipulation RL](https://arxiv.org/abs/2602.08557)
*Marc Toussaint,Cornelius V. Braun,Eckart Cobo-Briesewitz,Sayantan Auddy,Armand Jordana,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出了一种名为Sample-Guided RL的新方法，利用基于模型的约束求解器采样可行构型，以引导强化学习训练通用（目标条件）操作策略，在接触丰富的操纵任务中显著提升稀疏奖励下的成功率和策略复杂性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在接触丰富的操作任务中可能难以充分探索并发现复杂操作策略，尤其在稀疏奖励场景下；而模型基求解器能高效生成满足物理约束的可行状态，可作为引导信号。

Method: 提出Sample-Guided RL框架：利用模型基约束求解器（满足可微碰撞、接触与力约束）采样低维可行流形上的状态，并以此偏置RL的状态访问分布；进一步结合黑箱优化生成开环轨迹，施加状态偏好或引入行为克隆损失。

Result: 在双球简化环境中，该方法成功发现复杂操作策略，实现任意静态稳定目标的高成功率；在更具挑战性的Panda机械臂环境中，显著超越接近零的基线，展现出多样化的全身接触操作策略。

Conclusion: 将模型基可行性采样与RL结合，能有效缓解稀疏奖励与探索难题，为通用、接触丰富的机器人操作提供可扩展且实用的训练范式。

Abstract: We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.

</details>


### [375] [Head-to-Head autonomous racing at the limits of handling in the A2RL challenge](https://arxiv.org/abs/2602.08571)
*Simon Hoffmann,Simon Sagmeister,Tobias Betz,Joscha Bongard,Sascha Büttner,Dominic Ebner,Daniel Esser,Georg Jank,Sven Goblirsch,Alexander Langmann,Maximilian Leitenstern,Levent Ögretmen,Phillip Pitschi,Ann-Kathrin Schwehn,Cornelius Schröder,Marcel Weinmann,Frederik Werner,Boris Lohmann,Johannes Betz,Markus Lienkamp*

Main category: cs.RO

TL;DR: 本文介绍了TUM Autonomous Motorsport团队为首届阿布扎比自主赛车联盟（A2RL）开发的算法与部署策略，展示了如何通过软件模拟人类驾驶行为，在车辆极限操控和多车交互中取得胜利。


<details>
  <summary>Details</summary>
Motivation: 自主赛车是一个复杂的多智能体交互问题，涉及车辆在性能和动力学极限下的协同与竞争，是推动自动驾驶技术发展和提升道路安全的重要研究与测试平台。

Method: 开发了用于自主赛车的算法与实际部署策略，重点模拟人类驾驶行为，优化车辆极限操控能力及多车交互决策。

Result: 成功赢得首届阿布扎比自主赛车联盟（A2RL）冠军，并验证了所提方法在真实赛道环境中的有效性。

Conclusion: 本文总结了制胜的关键技术要素与实践经验，为未来自主赛车及高级自动驾驶系统研发提供了重要参考。

Abstract: Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.

</details>


### [376] [MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation](https://arxiv.org/abs/2602.08594)
*Zhenguo Sun,Bo-Sheng Huang,Yibo Peng,Xukun Li,Jingyu Ma,Yu Sun,Zhe Li,Haojun Jiang,Biao Gao,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出MOSAIC系统，通过强化学习构建面向遥操作的通用运动追踪器，并采用快速残差自适应方法弥合仿真到现实的接口差距，在真实人形机器人上实现了鲁棒的离线动作回放和在线长时遥操作。


<details>
  <summary>Details</summary>
Motivation: 现有通用人形运动追踪器虽在仿真中表现优异，但在真实硬件持续遥操作中因接口与动力学误差而表现脆弱。

Method: MOSAIC包含两阶段方法：首先基于多源运动库、自适应重采样及世界坐标系运动一致性奖励，用强化学习训练通用运动追踪器；其次通过少量接口特异性数据训练接口专用策略，并以加性残差模块将其蒸馏至通用追踪器，实现快速残差自适应。

Result: 系统在系统性消融实验、分布外基准测试及真实机器人实验中均验证有效，支持鲁棒的离线动作回放与在线长时遥操作，能应对真实延迟与噪声。

Conclusion: MOSAIC通过结合通用建模与轻量接口适配，显著提升了人形机器人遥操作的鲁棒性与泛化能力，为全栈式运动追踪与控制提供了可扩展开源框架。

Abstract: Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.

</details>


### [377] [A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation](https://arxiv.org/abs/2602.08599)
*Kenghou Hoi,Yuze Wu,Annan Ding,Junjie Wang,Anke Zhao,Chengqian Zhang,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出了一种基于低成本、高灵敏度皮肤式触觉传感器的力感知抓取框架，用于无人机空中操作，实现了无需外部动捕系统的全机载力反馈抓取与实时重量测量。


<details>
  <summary>Details</summary>
Motivation: 现有空中操作方法依赖昂贵笨重的力传感器或无力度反馈抓取，难以兼顾轻量化与安全性，尤其对易碎物体操作风险高。

Method: 设计并集成六个基于磁性原理的触觉传感模块，采用参考霍尔传感器消除地磁干扰，简化校准流程，实现高精度三维力测量，并构建力感知闭环抓取控制框架。

Result: 在真实场景中成功完成气球抓取、动态负载变化测试及消融实验，验证了系统可安全操作易碎物、实时测重，并支持全机载运行。

Conclusion: 该框架显著提升了力敏感空中操作的实用性与部署可行性，为轻量级无人机力反馈操作提供了新范式。

Abstract: Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.

</details>


### [378] [Mimic Intent, Not Just Trajectories](https://arxiv.org/abs/2602.08602)
*Renming Huang,Chendong Zeng,Wenjing Tang,Jingtian Cai,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: 本文提出MINT方法，通过多尺度频域分词将行为意图与执行细节解耦，实现意图模仿而非单纯轨迹模仿，提升技能迁移与环境适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法（如VLA模型）难以适应环境变化和跨任务技能迁移，因其仅模仿原始轨迹而未理解底层意图。

Method: 提出多尺度频域分词机制，构建粗到细的行动表征层次：最粗粒度token捕获低频全局结构（即Intent token），细粒度token编码高频执行细节（Execution tokens）；策略采用逐级自回归生成轨迹，实现从意图到执行的渐进式推理，并支持通过注入演示的Intent token实现单次示教迁移。

Result: 在多个操作基准及真实机器人上验证了SOTA成功率、更高推理效率、对干扰的鲁棒泛化能力以及有效的一次性技能迁移。

Conclusion: 显式解耦意图与执行可显著提升模仿学习的泛化性、适应性和迁移效率，为具身智能中的技能复用提供了新范式。

Abstract: While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.

</details>


### [379] [High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning](https://arxiv.org/abs/2602.08653)
*Jiarui Zhang,Chengyong Lei,Chengjiang Dai,Lijie Wang,Zhichao Han,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习与基于模型安全机制的端到端框架，通过物理先验增强训练与部署阶段的安全性与导航性能。


<details>
  <summary>Details</summary>
Motivation: 传统模块化导航流程存在累积延迟，纯强化学习方法缺乏形式化安全保证，需兼顾高速飞行与严格避障保障。

Method: 设计物理信息引导的奖励函数用于训练；部署时引入实时安全滤波器，将策略输出投影到可证明安全的集合中；构建端到端RL与模型基安全机制融合的混合架构。

Result: 在基准测试中优于传统规划器及基于可微物理的端到端避障方法；实验证明其可在密集障碍物和复杂森林环境中以最高7.5m/s速度实现鲁棒高速导航。

Conclusion: 该混合框架成功平衡了自主导航的性能与安全性，为四旋翼无人机在复杂场景下的可靠部署提供了新范式。

Abstract: Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.

</details>


### [380] [Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch](https://arxiv.org/abs/2602.08776)
*Cuijie Xu,Shurui Zheng,Zihao Su,Yuanfan Xu,Tinghao Yi,Xudong Zhang,Jian Wang,Yu Wang,Jinchen Yu*

Main category: cs.RO

TL;DR: 本文提出了一种Dual-State Conditioning框架，将行为克隆从轨迹执行克隆转向意图克隆，利用主-从端的意图-执行失配信号实现隐式力感知与动态补偿，无需力传感器即可在低成本遥操作设备上实现鲁棒接触操作。


<details>
  <summary>Details</summary>
Motivation: 标准行为克隆忽略人类操作员为补偿硬件缺陷（如延迟、摩擦、无触觉反馈）而进行的闭环主动补偿机制，导致在接触丰富或动态任务中性能不佳。

Method: 提出Dual-State Conditioning框架，以主端指令（intent）为学习目标，将意图-执行失配建模为蕴含隐式交互力和操作策略的关键信号；通过预测intent生成虚拟平衡点实现隐式阻抗控制，并利用失配历史进行隐式系统辨识；进一步将策略建模为轨迹插值器以缓解推理延迟。

Result: 在无传感器、低成本双手机器人平台上验证，该方法在接触密集操作和动态跟踪任务中显著优于标准行为克隆，成功实现无需力传感的力感知与动态补偿。

Conclusion: 意图克隆结合失配信号建模是一种更符合遥操作本质的学习范式，为低成本硬件提供了轻量、鲁棒的行为克隆新框架。

Abstract: Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to "Intent Cloning" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a "virtual equilibrium point", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.

</details>


### [381] [GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion](https://arxiv.org/abs/2602.08784)
*Santiago Montiel-Marín,Miguel Antunes-García,Fabio Sánchez-García,Angel Llamazares,Holger Caesar,Luis M. Bergasa*

Main category: cs.RO

TL;DR: 本文提出GaussianCaR，一种基于高斯溅射的端到端BEV分割网络，实现高效相机-雷达融合，在nuScenes上达到SOTA性能且推理速度快3.2倍。


<details>
  <summary>Details</summary>
Motivation: 视觉方法虽为主流，但在复杂交通场景中需结合低成本、鲁棒的雷达数据提升动态目标与地图元素感知能力。

Method: 将高斯溅射（Gaussian Splatting）重新用作通用视图变换器，统一映射图像像素与雷达点云至鸟瞰图（BEV）空间；设计多尺度融合+Transformer解码器架构，直接对原始传感器数据进行BEV特征提取与分割。

Result: 在nuScenes数据集BEV分割任务中，车辆、道路、车道线分割IoU分别达57.3%、82.9%、50.1%，性能持平或超越SOTA，推理速度提升3.2倍。

Conclusion: 高斯溅射可作为高效通用视图变换器用于相机-雷达BEV融合，GaussianCaR在精度与效率间取得更好平衡，为多模态感知提供新范式。

Abstract: Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.

</details>


### [382] [Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems](https://arxiv.org/abs/2602.08821)
*Robin Dehler,Oliver Schumann,Jona Ruof,Michael Buchholz*

Main category: cs.RO

TL;DR: 本文提出了一种面向服务架构（SOA）与功能卸载相结合的安全分析框架，用于分布式智能交通系统（ITS），特别是针对网联自动驾驶车辆（CAV）在远程服务卸载场景下的数据可靠性与安全性问题。


<details>
  <summary>Details</summary>
Motivation: 为降低CAV本地计算复杂度而进行功能卸载，但远程服务易受攻击或通信干扰导致数据被篡改，亟需安全验证机制。

Method: 基于SOA在分布式环境中的分析，构建多阶段安全框架，嵌入已有的服务导向功能卸载框架（SOFOF）中，实现对远程服务及其数据可靠性的实时验证。

Result: 所提扩展框架在降低计算复杂度和节省能量的同时，有效提升了对被污染远程数据的检测能力。

Conclusion: 将多阶段安全分析直接集成到SOFOF中，可在保障安全前提下支持CAV高效、可靠地利用远程服务，为分布式ITS提供了可行的安全增强路径。

Abstract: The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.

</details>


### [383] [Finite-Time Teleoperation of Euler-Lagrange Systems via Energy-Shaping](https://arxiv.org/abs/2602.08845)
*Lazaro F. Torres,Carlos I. Aldana,Emmanuel Nuño,Emmanuel Cruz-Zavala*

Main category: cs.RO

TL;DR: 本文提出了一种针对全驱动非线性欧拉-拉格朗日系统的双边遥操作有限时间控制器族，基于能量整形框架，在人与环境被动交互假设下，确保无时延时位置误差和速度全局有限时间收敛至零；控制器为简单连续的“比例+阻尼注入”形式，并通过仿真和实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决双边遥操作中位置误差和速度收敛速度慢的问题，实现有限时间收敛以提升系统响应性能和鲁棒性。

Method: 基于能量整形框架设计有限时间控制器，利用齐次近似理论分析闭环系统，构造连续的、比例加阻尼注入形式的控制器。

Result: 在无时延且满足被动交互假设下，位置误差和系统速度全局有限时间收敛到零；控制器结构简单，经仿真和实验验证有效。

Conclusion: 所提出的有限时间控制器族能有效提升双边遥操作系统的动态性能，兼具理论保证与工程实用性。

Abstract: This paper proposes a family of finite-time controllers for the bilateral teleoperation of fully actuated nonlinear Euler-Lagrange systems. Based on the energy-shaping framework and under the standard assumption of passive interactions with the human and the environment, the controllers ensure that the position error and velocities globally converge to zero in the absence of time delays. In this case, the closed-loop system admits a homogeneous approximation of negative degree, and thus the control objective is achieved in finite-time. The proposed controllers are simple, continuous-time proportional-plus-damping-injection schemes, validated through both simulation and experimental results.

</details>


### [384] [Reduced-order Control and Geometric Structure of Learned Lagrangian Latent Dynamics](https://arxiv.org/abs/2602.08963)
*Katharina Friedl,Noémie Jaquier,Seungyeon Kim,Jens Lundell,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的结构保持降阶动力学的潜在控制框架，用于高维拉格朗日系统，通过推导降阶跟踪律和采用黎曼视角分析投影降阶模型，量化建模误差并给出稳定性与收敛性条件，并扩展至欠驱动系统。


<details>
  <summary>Details</summary>
Motivation: 高维机械系统（如可变形物体、软体机器人）缺乏精确物理动力学模型，而现有神经网络方法或受限于低维系统，或缺乏嵌入物理结构导致控制保证不足。

Method: 提出基于学习的结构保持降阶动力学的潜在控制框架；推导全驱动系统的降阶跟踪律；采用黎曼几何视角分析投影降阶模型；量化建模误差；扩展至欠驱动系统并引入学习的驱动模式。

Result: 理论推导出稳定性与收敛性的可解释条件；在仿真与真实系统上验证了控制器的准确性与理论分析的有效性。

Conclusion: 该框架在保持物理结构的同时实现高维系统有效控制，兼顾学习能力与形式化控制保证。

Abstract: Model-based controllers can offer strong guarantees on stability and convergence by relying on physically accurate dynamic models. However, these are rarely available for high-dimensional mechanical systems such as deformable objects or soft robots. While neural architectures can learn to approximate complex dynamics, they are either limited to low-dimensional systems or provide only limited formal control guarantees due to a lack of embedded physical structure. This paper introduces a latent control framework based on learned structure-preserving reduced-order dynamics for high-dimensional Lagrangian systems. We derive a reduced tracking law for fully actuated systems and adopt a Riemannian perspective on projection-based model-order reduction to study the resulting latent and projected closed-loop dynamics. By quantifying the sources of modeling error, we derive interpretable conditions for stability and convergence. We extend the proposed controller and analysis to underactuated systems by introducing learned actuation patterns. Experimental results on simulated and real-world systems validate our theoretical investigation and the accuracy of our controllers.

</details>


### [385] [CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion](https://arxiv.org/abs/2602.08999)
*Mouad Abrini,Mohamed Chetouani*

Main category: cs.RO

TL;DR: CLUE提出了一种新方法，将视觉语言模型（VLM）的跨模态注意力显式转化为空间定位信号，以决定机器人何时向人类提问澄清，从而提升交互式视觉定位（IVG）性能。


<details>
  <summary>Details</summary>
Motivation: 现有IVG模型缺乏主动判断何时需提问澄清的机制，仅依赖隐式学习表征，难以有效处理指代表达中的歧义。

Method: CLUE从VLM中提取文本到图像的注意力图，输入轻量CNN检测指代歧义；同时使用LoRA微调解码器进行对话并输出定位标记；在真实IVG数据集和混合歧义数据集上联合训练。

Result: 在仅用InViG监督下，CLUE超越当前最优方法，且参数高效；其歧义检测器也优于先前基线。

Conclusion: CLUE成功将VLM内部跨模态注意力转化为可解释、空间对齐的决策信号，提升了人机交互中视觉指代消解的主动性与准确性。

Abstract: With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

</details>


### [386] [From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection](https://arxiv.org/abs/2602.09002)
*Zilin Fang,Anxing Xiao,David Hsu,Gim Hee Lee*

Main category: cs.RO

TL;DR: 本文提出了一种融合几何规划与情境化社会推理的社交机器人导航框架，利用微调的视觉-语言模型（VLM）实时评估候选路径的社会适宜性，在多个社交场景中显著降低个人空间侵犯、行人直视时间及社会区域侵入。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞规避路径可能仍违反社会规范或干扰人类活动，需将常识性社会推理融入导航规划。

Method: 先提取障碍物与人类运动信息生成几何可行路径，再用任务特化的微调视觉-语言模型（VLM）结合情境化社会期望对路径进行评估与优选。该VLM由大基础模型蒸馏而来，兼顾效率与社会推理能力。

Result: 在四个社交导航场景实验中，该方法综合性能最优：个人空间侵犯时长最短、行人直视时间最少、且无社会区域侵入。

Conclusion: 融合几何可行性与社会语义理解的导航框架可提升机器人在真实人类环境中的自然性与可接受性，VLM驱动的社会评估是实现高效实时社交适应的关键。

Abstract: Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>


### [387] [Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction](https://arxiv.org/abs/2602.09013)
*Hongyi Chen,Tony Dong,Tiancheng Wu,Liquan Wang,Yash Jangir,Yaru Niu,Yufei Ye,Homanga Bharadhwaj,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 本文提出VIDEOMANIP，一种无需设备的框架，直接从RGB人类视频中学习灵巧操作，通过4D重建、接触优化和演示合成，在仿真与真实机器人上均取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 多指机械手的操作与抓取因高维动作空间和缺乏大规模训练数据而困难；现有方法依赖可穿戴设备或专用传感器，限制了可扩展性。

Method: VIDEOMANIP利用计算机视觉技术，从单目RGB视频中估计人手姿态与物体网格，重建4D机器人-物体轨迹，并将人类动作重定向至机械手；引入手-物接触优化与以交互为中心的抓取建模，并设计演示合成策略以从单个视频生成多样化训练轨迹。

Result: 仿真中对20个不同物体的抓取成功率达70.25%（Inspire Hand）；真实世界中在7项任务上平均成功率达62.86%（LEAP Hand），较基于重定向的方法提升15.87%。

Conclusion: VIDEOMANIP实现了仅用RGB视频进行设备无关的灵巧操作学习，提升了数据效率与泛化能力，为低成本、可扩展的机器人操作学习提供了新范式。

Abstract: Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>


### [388] [Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models](https://arxiv.org/abs/2602.09017)
*Zichen Jeff Cui,Omar Rayyan,Haritheja Etukuru,Bowen Tan,Zavier Andrianarivo,Zicheng Teng,Yihang Zhou,Krish Mehta,Nicholas Wojno,Kevin Yuanbo Wu,Manan H Anjaria,Ziyuan Wu,Manrong Mao,Guangxun Zhang,Binit Shah,Yejin Kim,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah*

Main category: cs.RO

TL;DR: 本文提出Contact-Anchored Policies（CAP），用空间中的物理接触点替代语言提示来指导机器人操作，结合模块化实用模型设计与轻量级仿真基准EgoGym，实现高效真实-仿真迭代，仅用23小时演示数据即在零样本评估中超越当前最优视觉-语言动作模型56%。


<details>
  <summary>Details</summary>
Motivation: 语言提示过于抽象，难以支撑具身机器人所需的精细物理理解与鲁棒操作，现有泛化范式存在根本性张力。

Method: 提出以接触点为条件的Contact-Anchored Policies（CAP），采用模块化而非单体化策略架构；构建轻量仿真基准EgoGym，支持快速识别失败模式并迭代优化模型与数据。

Result: CAP在三个基础操作技能上实现跨环境与跨本体的开箱即用泛化，仅需23小时演示数据，在零样本评估中性能比当前最优VLAs高56%。

Conclusion: 以物理接触为锚点、结合模块化建模与真实-仿真闭环迭代，是提升机器人操作泛化性与数据效率的有效新路径。

Abstract: The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/

</details>


### [389] [Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving](https://arxiv.org/abs/2602.09018)
*Amir Mallak,Alaa Maalouf*

Main category: cs.RO

TL;DR: 本文通过在五个维度（场景、季节、天气、时间、智能体组合）上对自动驾驶策略进行受控的k因子扰动测试，系统评估了不同模型（FC、CNN、ViT）及基于基础模型（FM）特征的策略在分布外（OOD）环境下的鲁棒性，并提出了提升OOD鲁棒性的可操作设计准则。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将OOD鲁棒性简化为单一数值，掩盖了策略失效的具体原因；本文旨在细粒度分解环境变化因素，揭示不同扰动组合下策略性能退化模式。

Method: 在VISTA仿真平台中采用闭环控制，沿场景、季节、天气、时间、智能体混合五维进行0–3因子受控扰动实验；对比FC、CNN、ViT策略，评估冻结基础模型特征+轻量ViT头的方案，并分析ID数据规模、多样性与时间上下文的影响。

Result: ViT优于同规模CNN/FC；FM特征显著提升多因子OOD鲁棒性（三因子扰动下仍>85%）；最大单因子下降来自农村→城市和白天→黑夜（各~31%）；因子交互非加性；冬季/雪天训练最抗单因子扰动；增加轨迹/视角数量或针对性覆盖困难条件均可提升鲁棒性；多ID训练拓宽泛化能力但略降ID性能。

Conclusion: OOD鲁棒性需从多维扰动组合角度系统评估；ViT架构与FM特征是关键增益来源；训练数据的多样性（多ID、多季节、多场景）比单纯扩大规模更有效；应避免仅依赖多帧输入，并优先覆盖高风险变化组合（如季节+时间）。

Abstract: Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>


### [390] [$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies](https://arxiv.org/abs/2602.09021)
*Checheng Yu,Chonghao Sima,Gangcheng Jiang,Hai Zhang,Haoguang Mai,Hongyang Li,Huijie Wang,Jin Chen,Kaiyang Wu,Li Chen,Lirui Zhao,Modi Shi,Ping Luo,Qingwen Bu,Shijia Peng,Tianyu Li,Yibo Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种名为χ₀的资源高效型框架，通过模型算术、阶段优势估计和训练-部署对齐三大技术支柱，显著提升了长时程双臂机器人衣物操作的鲁棒性与可靠性，在仅用20小时数据和8块A100 GPU的情况下，成功率较SOTA方法π₀.₅提升近250%。


<details>
  <summary>Details</summary>
Motivation: 现有高可靠性长时程机器人操作受限于人类示范分布、策略归纳偏置与测试执行分布之间的系统性分布偏移，导致多阶段任务中误差累积，而非单纯资源不足。

Method: 提出χ₀框架，包含三项核心技术：(i) 模型算术——权重空间融合策略，吸收多样化示范分布；(ii) 阶段优势估计器——提供稳定稠密的阶段性进展信号；(iii) 训练-部署对齐——通过时空增强、启发式DAgger修正和时间分块平滑弥合分布鸿沟。

Result: 在双臂协同衣物操作（铺平、折叠、悬挂）任务中实现连续24小时无干预自主运行；成功率较π₀.₅提升近250%，仅需20小时数据和8块A100 GPU。

Conclusion: 分布一致性比单纯扩大数据与算力更关键；χ₀验证了轻量但结构严谨的设计可在真实世界长时程操作中达成生产级鲁棒性。

Abstract: High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>


### [391] [TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation](https://arxiv.org/abs/2602.09023)
*Qinwen Xu,Jiaming Liu,Rui Zhou,Shaojun Shi,Nuowei Han,Zhuoyang Liu,Chenyang Gu,Shuo Gu,Yang Yue,Gao Huang,Wenzhao Zheng,Sirui Han,Peng Jia,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出TwinRL框架，通过数字孪生与真实世界协同的强化学习方法，显著提升视觉-语言-动作（VLA）模型在真实机器人操作任务中的在线强化学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型受限于专家演示成本高、真实世界交互不足；在线RL在真实操作中探索效率低、探索空间受限，且其有效探索空间依赖监督微调（SFT）的数据分布。

Method: 提出TwinRL：1）基于手机拍摄场景高效构建高保真数字孪生环境，支持虚实双向迁移；2）在SFT预热阶段利用数字孪生扩展轨迹分布以扩大探索空间；3）采用sim-to-real引导探索策略——先在数字孪生中并行高效开展在线RL，再通过数字孪生采样识别易失败但信息丰富的配置，指导真实机器人上的人在环 rollout。

Result: 在四项真实机器人任务中，TwinRL在分布内和分布外区域均接近100%成功率，相比先前真实世界RL方法提速至少30%，平均每任务仅需约20分钟。

Conclusion: 数字孪生可有效桥接离线监督训练与在线强化学习，TwinRL为低成本、高效率、强泛化的真实世界VLA模型部署提供了新范式。

Abstract: Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.

</details>


### [392] [NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling](https://arxiv.org/abs/2407.03035)
*Marc Toussaint,Cornelius V. Braun,Joaquim Ortiz-Haro*

Main category: cs.RO

TL;DR: 本文提出NLP采样作为通用问题框架，结合MCMC、约束优化与机器人学方法，设计重启式两阶段采样算法，并在解析与机器人操作规划任务中实证评估；同时探讨拉格朗日参数、全局采样及扩散NLP与模型驱动去噪采样等概念。


<details>
  <summary>Details</summary>
Motivation: 生成满足硬约束的多样化样本是多个领域的核心挑战，现有方法分散于不同学科，缺乏统一视角与整合框架。

Method: 提出NLP Sampling通用问题形式化；构建重启式两阶段方法家族以融合MCMC、约束优化和机器人学技术；引入扩散NLP概念及对应模型驱动去噪采样器。

Result: 在解析问题和机器人操作规划任务上验证了所提两阶段方法的有效性与多样性；提供了关于拉格朗日参数作用、全局采样可行性及扩散NLP建模的若干概念性洞见。

Conclusion: 跨领域方法整合可提升约束下多样采样的能力；重启两阶段框架具有通用性与实用性；扩散NLP与去噪采样为未来方向提供新思路。

Abstract: Generating diverse samples under hard constraints is a core challenge in many areas. With this work we aim to provide an integrative view and framework to combine methods from the fields of MCMC, constrained optimization, as well as robotics, and gain insights in their strengths from empirical evaluations. We propose NLP Sampling as a general problem formulation, propose a family of restarting two-phase methods as a framework to integrated methods from across the fields, and evaluate them on analytical and robotic manipulation planning problems. Complementary to this, we provide several conceptual discussions, e.g. on the role of Lagrange parameters, global sampling, and the idea of a Diffused NLP and a corresponding model-based denoising sampler.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [393] [Convex Primitive Decomposition for Collision Detection](https://arxiv.org/abs/2602.07369)
*Julian Knodt,Xifeng Gao*

Main category: cs.GR

TL;DR: 本文提出了一种面向刚体仿真的、自底向上的凸原始体分解方法，用于自动生成3D模型的碰撞体，相比现有方法（如V-HACD、CoACD）具有更低的几何误差、更小的存储开销和更快的仿真性能。


<details>
  <summary>Details</summary>
Motivation: 手动创建碰撞体耗时且繁琐；现有自动凸分解方法（如基于凸包的方法）在游戏等实时应用中性能差、不可编辑，难以兼顾精度、效率与可控性。

Method: 受二次型网格简化启发，提出一种自底向上的凸原始体（如长方体、胶囊体等）分解方法，直接拟合复杂网格表面，保证包围性，并支持后续人工调整。

Result: 在60+个Sketchfab模型上验证：相比V-HACD和CoACD，本方法在Hausdorff与Chamfer距离上均更低，碰撞体体积（字节数）不足其三分之一；在24个模型的刚体仿真中，实测墙钟时间显著提升。

Conclusion: 该方法在精度、紧凑性和仿真性能之间取得更好平衡，更适合实时物理仿真场景，尤其适用于游戏开发等对性能敏感的应用。

Abstract: Creation of collision objects for 3D models is a time-consuming task, requiring modelers to manually place primitives such as bounding boxes, capsules, spheres, and other convex primitives to approximate complex meshes. While there has been work in automatic approximate convex decompositions of meshes using convex hulls, they are not practical for applications with tight performance budgets such as games due to slower collision detection and inability to manually modify the output while maintaining convexity as compared to manually placed primitives. Rather than convex decomposition with convex hulls, we devise an approach for bottom-up decomposition of an input mesh into convex primitives specifically for rigid body simulation inspired by quadric mesh simplification. This approach fits primitives to complex, real-world meshes that provide plausible simulation performance and are guaranteed to enclose the input surface. We test convex primitive decomposition on over 60 models from Sketchfab, showing the algorithm's effectiveness. On this dataset, convex primitive decomposition has lower one-way mean and median Hausdorff and Chamfer distance from the collider to the input compared to V-HACD and CoACD, with less than one-third of the complexity as measured by total bytes for each collider. On top of that, rigid-body simulation performance measured by wall-clock time is consistently improved across 24 tested models.

</details>


### [394] [Low-Rank Koopman Deformables with Log-Linear Time Integration](https://arxiv.org/abs/2602.07687)
*Yue Chang,Peter Yichen Chen,Eitan Grinspun,Maurizio M. Chiaramonte*

Main category: cs.GR

TL;DR: 本文提出了一种基于低秩Koopman算子的变形子空间模拟加速方法，利用动态模态分解（DMD）参数化Koopman算子，实现高效时间演化预测与跨形状/网格分辨率的泛化建模。


<details>
  <summary>Details</summary>
Motivation: 传统DMD方法受限于单一形状和离散化，难以应对几何变化任务（如形状优化）；同时，时序模拟计算开销大，不利于控制、初态估计等依赖终态的优化任务。

Method: 采用低秩Koopman算子建模，以DMD为参数化方式学习变形动力学的时间演化；引入离散化无关的扩展机制，使模型能共享多个形状及不同网格分辨率下的动态行为。

Result: 实现对时间步数的对数线性复杂度（log-linear scaling），支持跳过大量中间状态仍保持精度；首次实现DMD类模型在跨形状与跨分辨率场景下的泛化，显著提升形状优化等任务的可行性与效率。

Conclusion: 该Koopman算子学习框架不仅大幅提升变形仿真效率，还拓展了降阶模型在图形学中处理几何变化任务的实际适用性，展现出作为高效仿真与设计工具的潜力。

Abstract: We present a low-rank Koopman operator formulation for accelerating deformable subspace simulation. Using a Dynamic Mode Decomposition (DMD) parameterization of the Koopman operator, our method learns the temporal evolution of deformable dynamics and predicts future states through efficient matrix evaluations instead of sequential time integration. This yields log-linear scaling in the number of time steps and allows large portions of the trajectory to be skipped while retaining accuracy. The resulting temporal efficiency is especially advantageous for optimization tasks such as control and initial-state estimation, where the objective often depends largely on the final configuration.
  To broaden the scope of Koopman-based reduced-order models in graphics, we introduce a discretization-agnostic extension that learns shared dynamic behavior across multiple shapes and mesh resolutions. Prior DMD-based approaches have been restricted to a single shape and discretization, which limits their usefulness for tasks involving geometry variation. Our formulation generalizes across both shape and discretization, which enables fast shape optimization that was previously impractical for DMD models. This expanded capability highlights the potential of Koopman operator learning as a practical tool for efficient deformable simulation and design.

</details>


### [395] [TABI: Tight and Balanced Interactive Atlas Packing](https://arxiv.org/abs/2602.07782)
*Floria Gu,Nicholas Vining,Alla Sheffer*

Main category: cs.GR

TL;DR: 本文提出了一种名为TABI的GPU实时图集打包方法，兼顾交互性能与打包质量，在保证快速响应的同时显著减少图表缩放，并通过紧凑空隙和自动调整行宽/朝向来提升打包紧致性与平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有离线打包方法质量高但速度慢，无法满足实时交互需求；而现有实时GPU方法虽快，但打包质量差（存在大空隙、不对称等问题），亟需一种兼顾速度与质量的新方法。

Method: TABI方法采用两种图表形状近似表示以支持并行处理，通过水平和垂直方向压缩不规则图表间的空隙实现紧密打包；同时自动调节图集行宽和方向以适应不同高度的图表，从而实现平衡布局。

Result: 相比现有实时方法，TABI显著降低了图表缩放程度；相比离线方法，仍保持数量级以上的速度优势，且打包更紧密、更均衡。

Conclusion: TABI在交互式场景中实现了高质量、高效率、可控权衡的图集打包，解决了实时性与打包质量难以兼顾的核心矛盾。

Abstract: Atlas packing is a key step in many computer graphics applications. Packing algorithms seek to arrange a set of charts within a fixed-size atlas with as little downscaling as possible. Many packing applications such as content creation tools, dynamic atlas generation for video games, and texture space shading require on-the-fly interactive atlas packing. Unfortunately, while many methods have been developed for generating tight high-quality packings, they are designed for offline settings and have running times two or more orders of magnitude greater than what is required for interactive performance. While real-time GPU packing methods exist, they significantly downscale packed charts compared to offline methods. We introduce a GPU packing method that targets interactive speeds, provides packing quality approaching that of offline methods, and supports flexible user control over the tradeoff between performance and quality. We observe that current real-time packing methods leave large gaps between charts and often produce asymmetric, or poorly balanced, packings. These artifacts dramatically degrade packing quality. Our Tight And Balanced method eliminates these artifacts while retaining Interactive performance. TABI generates tight packings by compacting empty space between irregularly shaped charts both horizontally and vertically, using two approximations of chart shape that support efficient parallel processing. We balance packing outputs by automatically adjusting atlas row widths and orientations to accommodate varying chart heights. We show that our method significantly reduces chart downscaling compared to existing interactive methods while remaining orders of magnitude faster than offline alternatives.

</details>


### [396] [MPM Lite: Linear Kernels and Integration without Particles](https://arxiv.org/abs/2602.07853)
*Xiang Feng,Yunuo Chen,Chang Yu,Hao Su,Demetri Terzopoulos,Yin Yang,Joe Masterjohn,Alejandro Castro,Chenfanfu Jiang*

Main category: cs.GR

TL;DR: MPM Lite是一种新型混合拉格朗日/欧拉方法，通过将粒子视为运动状态和材料历史的载体，并在固定位置的积分点上重采样粒子状态，消除了求解时对基于粒子的数值积分的依赖，从而显著提升隐式和显式模拟的性能。


<details>
  <summary>Details</summary>
Motivation: 标准MPM方法因基于粒子的数值积分和宽模板核导致隐式求解器性能随每单元粒子数（PPC）增加而急剧下降，存在严重性能瓶颈。

Method: MPM Lite将背景笛卡尔网格视为体素六面体网格，使用高效紧凑的线性核将粒子状态重采样到固定位置的积分点；提出新的应力传递与伸展重构策略，重采样广义Kirchhoff应力并构建无旋转变形参考解，支持基于优化的增量势能公式；整体架构为模块化重采样单元+类FEM积分模块。

Result: 实验表明MPM Lite在保持传统MPM对多种材料鲁棒性和通用性的同时，在隐式设置下实现显著加速，并同步改善显式设置性能。

Conclusion: MPM Lite通过解耦粒子与求解过程、引入FEM风格积分框架及标准化非线性求解器接口，为大规模、高性能物质模拟提供了新范式。

Abstract: In this paper, we introduce MPM Lite, a new hybrid Lagrangian/Eulerian method that eliminates the need for particle-based quadrature at solve time. Standard MPM practices suffer from a performance bottleneck where expensive implicit solves are proportional to particle-per-cell (PPC) counts due to the the choices of particle-based quadrature and wide-stencil kernels. In contrast, MPM Lite treats particles primarily as carriers of kinematic state and material history. By conceptualizing the background Cartesian grid as a voxel hexahedral mesh, we resample particle states onto fixed-location quadrature points using efficient, compact linear kernels. This architectural shift allows force assembly and the entire time-integration process to proceed without accessing particles, making the solver complexity no longer relate to particles. At the core of our method is a novel stress transfer and stretch reconstruction strategy. To avoid non-physical averaging of deformation gradients, we resample the extensive Kirchhoff stress and derive a rotation-free deformation reference solution, which naturally supports an optimization-based incremental potential formulation. Consequently, MPM Lite can be implemented as modular resampling units coupled with an FEM-style integration module, enabling the direct use of off-the-shelf nonlinear solvers, preconditioners, and unambiguous boundary conditions. We demonstrate through extensive experiments that MPM Lite preserves the robustness and versatility of traditional MPM across diverse materials while delivering significant speedups in implicit settings and improving explicit settings at the same time. Check our project page at https://mpmlite.github.io.

</details>


### [397] [Energy-Controllable Time Integration for Elastodynamic Contact](https://arxiv.org/abs/2602.08094)
*Kevin You,Juntian Zheng,Minchen Li*

Main category: cs.GR

TL;DR: 本文提出了一种新型能量可控的时间积分器A-search，它是隐式欧拉法的简单改进，能在保持稳定性和物理保真度的同时，按用户指定的目标控制能量耗散或守恒。


<details>
  <summary>Details</summary>
Motivation: 传统隐式积分器（如隐式欧拉、BDF2）虽稳定但过度耗散能量；辛方法（如隐式中点法）可保能量但缺乏无条件稳定性，难以处理刚性问题。

Method: 提出一类适用于哈密顿系统的新型数值积分器，其在线性问题上保持辛性，在非线性问题上具备更优稳定性；在此基础上导出A-search方法，通过调节参数实现能量目标控制，并支持无反转、无穿透保证。

Result: A-search在多种材料参数与场景下表现出对低频运动能量的保留倾向，相比BDF2等传统方法，在相近运行时间下能更好维持能量，生成更符合视觉期望的模拟结果。

Conclusion: A-search在稳定性、能量可控性与物理真实性之间实现了良好平衡，特别适用于大变形与复杂碰撞的弹性体动态仿真。

Abstract: Dynamic simulation of elastic bodies is a longstanding task in engineering and computer graphics. In graphics, numerical integrators like implicit Euler and BDF2 are preferred due to their stability at large time steps, but they tend to dissipate energy uncontrollably. In contrast, symplectic methods like implicit midpoint can conserve energy but are not unconditionally stable and fail on moderately stiff problems. To address these limitations, we propose a general class of numerical integrators for Hamiltonian problems which are symplectic on linear problems, yet have superior stability on nonlinear problems. With this, we derive a novel energy-controllable time integrator, A-search, a simple modification of implicit Euler that can follow user-specified energy targets, enabling flexible control over energy dissipation or conservation while maintaining stability and physical fidelity. Our method integrates seamlessly with barrier-type energies and allows for inversion-free and penetration-free guarantees, making it well-suited for handling large deformations and complex collisions. Extensive evaluations over a wide range of material parameters and scenes demonstrate that A-search has biases to keep energy in low frequency motion rather than dissipation, and A-search outperforms traditional methods such as BDF2 at similar total running times by maintaining energy and leading to more visually desirable simulations.

</details>


### [398] [Forget Superresolution, Sample Adaptively (when Path Tracing)](https://arxiv.org/abs/2602.08642)
*Martin Bálint,Corentin Salaün,Hans-Peter Seidel,Karol Myszkowski*

Main category: cs.GR

TL;DR: 本文提出了一种面向亚1 spp（每像素采样数）极端稀疏场景的端到端自适应采样与去噪管线，通过可微随机采样建模、色调映射感知训练、金字塔式聚集去噪及可学习反调制技术，显著提升了低采样率下的路径追踪图像质量，尤其在高光和阴影边界等感知关键区域。


<details>
  <summary>Details</summary>
Motivation: 实时路径追踪面临日益增长的渲染复杂度、分辨率和帧率需求，导致采样预算常低于1 spp；现有超分辨率方法均匀牺牲细节且无法适应噪声与感知重要性空间变化；而现有端到端自适应采样方法在极稀疏条件下因近似失效。

Method: 提出基于随机采样决策的可微建模以支持梯度估计；设计色调映射感知训练流程，集成可微胶片式算子与先进感知损失；引入基于聚集（gather-based）的金字塔去噪滤波器；并提出面向稀疏采样的可学习反照率反调制泛化方法。

Result: 在亚1 spp下显著优于均匀稀疏采样，尤其在高光、阴影边界等感知关键细节重建上效果更优；验证了自适应采样在极低采样预算下仍保持有效性。

Conclusion: 该端到端自适应采样与去噪框架为极低采样率路径追踪提供了高效、感知驱动的解决方案，突破了传统方法在稀疏 regime 下的性能瓶颈。

Abstract: Real-time path tracing increasingly operates under extremely low sampling budgets, often below one sample per pixel, as rendering complexity, resolution, and frame-rate requirements continue to rise. While super-resolution is widely used in production, it uniformly sacrifices spatial detail and cannot exploit variations in noise, reconstruction difficulty, and perceptual importance across the image. Adaptive sampling offers a compelling alternative, but existing end-to-end approaches rely on approximations that break down in sparse regimes.
  We introduce an end-to-end adaptive sampling and denoising pipeline explicitly designed for the sub-1-spp regime. Our method uses a stochastic formulation of sample placement that enables gradient estimation despite discrete sampling decisions, allowing stable training of a neural sampler at low sampling budgets. To better align optimization with human perception, we propose a tonemapping-aware training pipeline that integrates differentiable filmic operators and a state-of-the-art perceptual loss, preventing oversampling of regions with low visual impact.
  In addition, we introduce a gather-based pyramidal denoising filter and a learnable generalization of albedo demodulation tailored to sparse sampling. Our results show consistent improvements over uniform sparse sampling, with notably better reconstruction of perceptually critical details such as specular highlights and shadow boundaries, and demonstrate that adaptive sampling remains effective even at minimal budgets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [399] [Reasoning-Augmented Representations for Multimodal Retrieval](https://arxiv.org/abs/2602.07125)
*Jianrui Zhang,Anirudh Sundara Rajan,Brandon Han,Soochahn Lee,Sukanta Ganguly,Yong Jae Lee*

Main category: cs.IR

TL;DR: 本文提出一种数据驱动的框架，通过在检索前显式化隐含语义（如密集图像描述、解析模糊引用、重写查询），解耦推理与嵌入压缩过程，并在增强数据上训练检索器，显著提升通用多模态检索（UMR）中需潜在推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态检索模型在处理需潜在推理的查询（如指代消解或组合性约束）时表现脆弱，根源在于图像含‘沉默’证据而查询语义隐含，迫使单次嵌入同时完成推理与压缩，导致虚假特征匹配。

Method: 利用强视觉-语言模型，在检索前对语料库图像进行密集描述生成、对模糊多模态查询进行语义解析、对冗长指令进行约束精炼；并基于这些语义增强的数据重新训练检索器，避免分布偏移。

Result: 在M-BEIR基准上，该推理增强训练方法持续超越强基线；消融实验表明：语料增强主要提升知识密集型查询性能，查询增强对组合性修改类请求至关重要。

Conclusion: 将推理过程外部化并融入训练数据，是提升多模态检索鲁棒性与泛化性的有效路径；单纯推理增强推理时不足以解决问题，必须联合训练适配的检索器。

Abstract: Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry "silent" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.

</details>


### [400] [Multimodal Enhancement of Sequential Recommendation](https://arxiv.org/abs/2602.07207)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

TL;DR: MuSTRec is a novel multimodal and sequential recommender framework that leverages item-item graphs from text and visual features, and a frequency-based self-attention module to model user preferences; it achieves up to 33.5% improvement over SOTA baselines and reveals insights on data partitioning and user embedding integration.


<details>
  <summary>Details</summary>
Motivation: To unify multimodal and sequential recommendation paradigms and better capture cross-item similarities, collaborative filtering signals, and both short- and long-term user preferences.

Method: MuSTRec builds item-item graphs from extracted text and visual features to capture cross-item similarities and collaborative filtering signals, and introduces a frequency-based self-attention module to model short- and long-term user preferences.

Result: MuSTRec achieves up to 33.5% improvement over multimodal and sequential state-of-the-art baselines on multiple Amazon datasets; integrating user embeddings boosts short-term metrics by up to 200% on smaller datasets.

Conclusion: MuSTRec successfully unifies multimodal and sequential recommendation, offering superior performance and revealing important practical considerations such as data partitioning and user embedding integration.

Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted text and visual features. A frequency-based self-attention module additionally captures the short- and long-term user preferences. Across multiple Amazon datasets, MuSTRec demonstrates superior performance (up to 33.5% improvement) over multimodal and sequential state-of-the-art baselines. Finally, we detail some interesting facets of this new recommendation paradigm. These include the need for a new data partitioning regime, and a demonstration of how integrating user embeddings into sequential recommendation leads to drastically increased short-term metrics (up to 200% improvement) on smaller datasets. Our code is availabe at https://anonymous.4open.science/r/MuSTRec-D32B/ and will be made publicly available.

</details>


### [401] [Sequences as Nodes for Contrastive Multimodal Graph Recommendation](https://arxiv.org/abs/2602.07208)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

TL;DR: 本文提出MuSICRec，一种融合协同、序列与多模态信号的多视图图神经推荐模型，通过序列-物品图传播生成自然第二视图，并利用ID引导门控调节文本和视觉特征贡献，有效缓解冷启动与数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动和数据稀疏问题，同时避免现有多模态、序列和对比学习方法引入噪声、破坏语义的问题。

Method: 构建序列-物品（SI）图，通过注意力池化形成序列节点，并在SI图上传播生成第二视图；引入ID引导门控机制动态调节文本与视觉特征贡献，实现多模态对齐与噪声抑制。

Result: 在Amazon Baby、Sports和Electronics数据集上，MuSICRec在严格leave-two-out设置下全面超越各类序列、多模态及对比学习基线，尤其显著提升短历史用户推荐效果。

Conclusion: MuSICRec通过自然图传播替代人工增强，结合门控多模态融合，在不引入额外噪声的前提下有效缓解冷启动与稀疏性挑战，验证了多视图协同建模的有效性。

Abstract: To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSICRec (Multimodal Sequence-Item Contrastive Recommender), a multi-view graph-based recommender that combines collaborative, sequential, and multimodal signals. We build a sequence-item (SI) view by attention pooling over the user's interacted items to form sequence nodes. We propagate over the SI graph, obtaining a second view organically as an alternative to artificial data augmentation, while simultaneously injecting sequential context signals. Additionally, to mitigate modality noise and align the multimodal information, the contribution of text and visual features is modulated according to an ID-guided gate.
  We evaluate under a strict leave-two-out split against a broad range of sequential, multimodal, and contrastive baselines. On the Amazon Baby, Sports, and Electronics datasets, MuSICRec outperforms state-of-the-art baselines across all model types. We observe the largest gains for short-history users, mitigating sparsity and cold-start challenges. Our code is available at https://anonymous.4open.science/r/MuSICRec-3CEE/ and will be made publicly available.

</details>


### [402] [Progressive Searching for Retrieval in RAG](https://arxiv.org/abs/2602.07297)
*Taehee Jeong,Xingzhe Zhao,Peizu Li,Markus Valvur,Weihua Zhao*

Main category: cs.IR

TL;DR: 本文提出了一种渐进式搜索算法，通过从低维到高维嵌入的多阶段检索，提升RAG系统的检索效率与准确性，在大规模数据库中实现高效、可扩展的检索。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中检索过程效率低、难以兼顾速度与精度的问题，缓解LLM的信息过时和幻觉问题。

Method: 提出一种渐进式搜索算法：在多阶段检索中，先用低维嵌入快速筛选候选集，再逐步提升至目标高维嵌入进行精细匹配。

Result: 该方法在保证检索准确率的同时显著降低检索时间，实现了维度、速度与精度的平衡，适用于大规模数据库。

Conclusion: 渐进式搜索是一种成本效益高、可扩展性强的RAG检索策略，为构建高性能RAG系统提供了新思路。

Abstract: Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related documents. Then, the topmost matching documents are inserted into LLMs' prompt to generate a response. Efficient and accurate searching is critical for RAG to get relevant information. We propose a cost-effective searching algorithm for retrieval process. Our progressive searching algorithm incrementally refines the candidate set through a hierarchy of searches, starting from low-dimensional embeddings and progressing into a higher, target-dimensionality. This multi-stage approach reduces retrieval time while preserving the desired accuracy. Our findings demonstrate that progressive search in RAG systems achieves a balance between dimensionality, speed, and accuracy, enabling scalable and high-performance retrieval even for large databases.

</details>


### [403] [Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation](https://arxiv.org/abs/2602.07298)
*Benyu Zhang,Qiang Zhang,Jianpeng Cheng,Hong-You Chen,Qifei Wang,Wei Sun,Shen Li,Jia Li,Jiahao Wu,Xiangjun Fan,Hong Yan*

Main category: cs.IR

TL;DR: 本文提出一种分层框架生成高质量合成数据，用于大语言模型（LLM）在推荐系统中的持续预训练，首次在推荐领域观察到稳健的幂律缩放现象，并证明该合成数据显著优于真实交互数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于原始用户交互数据的持续预训练因数据噪声、偏差和不完整性，缺乏可预测的缩放规律，阻碍LLM在推荐系统中的发展。

Method: 设计一种分层合成数据生成框架，构建面向推荐任务的结构化、教学式课程，用于LLM的持续预训练；在合成数据上训练标准序列模型并评估下游排序性能；系统测量不同模态合成数据下LLM的困惑度变化以验证缩放律。

Result: 在SasRec上召回率@100提升130%；首次在推荐领域实现LLM持续预训练的稳健幂律缩放；困惑度随数据量/模型规模呈一致可预测下降。

Conclusion: 高质量、任务定制的合成数据是实现推荐领域LLM可靠缩放的关键基础，研究范式应从修补数据缺陷转向利用结构化信息。

Abstract: Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.

</details>


### [404] [LIT-GRAPH: Evaluating Deep vs. Shallow Graph Embeddings for High-Quality Text Recommendation in Domain-Specific Knowledge Graphs](https://arxiv.org/abs/2602.07307)
*Nirmal Gelal,Chloe Snow,Kathleen M. Jagodnik,Ambyr Rios,Hande Küçük McGinty*

Main category: cs.IR

TL;DR: LIT-GRAPH 是一个基于知识图谱的推荐系统，专为高中英语教师设计，用于推荐多样化且教学目标对齐的文学作品；通过对比四种图嵌入方法，发现 R-GCN 在语义排序上显著优于浅层模型，更契合教学需求。


<details>
  <summary>Details</summary>
Motivation: 解决高中英语课程中因教材固化导致的‘课程停滞’问题，帮助教师高效选取符合教学目标、文化多样性的文学文本。

Method: 构建面向英语文学的本体，开发知识图谱系统 LIT-GRAPH，并对比 DeepWalk、Biased Random Walk（BRW）、Hybrid（拼接向量）和关系图卷积网络（R-GCN）四种图嵌入范式在链接预测与语义排序任务上的表现。

Result: 浅层模型（DeepWalk、BRW、Hybrid）在结构化链接预测上表现更优，而 R-GCN 凭借关系特异性消息传递机制，在语义排名任务中显著领先，生成的教学相关性更高、质量更优的推荐结果。

Conclusion: 深度图神经网络（如 R-GCN）比传统浅层图嵌入更适合建模教育领域中的语义与教学逻辑，知识图谱结合关系感知嵌入能有效支撑高质量、可解释的教学资源推荐。

Abstract: This study presents LIT-GRAPH (Literature Graph for Recommendation and Pedagogical Heuristics), a novel knowledge graph-based recommendation system designed to scaffold high school English teachers in selecting diverse, pedagogically aligned instructional literature. The system is built upon an ontology for English literature, addressing the challenge of curriculum stagnation, where we compare four graph embedding paradigms: DeepWalk, Biased Random Walk (BRW), Hybrid (concatenated DeepWalk and BRW vectors), and the deep model Relational Graph Convolutional Network (R-GCN). Results reveal a critical divergence: while shallow models excelled in structural link prediction, R-GCN dominated semantic ranking. By leveraging relation-specific message passing, the deep model prioritizes pedagogical relevance over raw connectivity, resulting in superior, high-quality, domain-specific recommendations.

</details>


### [405] [Semantic Search At LinkedIn](https://arxiv.org/abs/2602.07309)
*Fedor Borisyuk,Sriram Vasudevan,Muchen Wu,Guoyao Li,Benjamin Le,Shaobo Zhang,Qianqi Kay Shen,Yuchin Juan,Kayhan Behdin,Liming Dong,Kaixu Yang,Shusen Jing,Ravi Pothamsetty,Rajat Arora,Sophie Yanying Sheng,Vitaly Abdrashitov,Yang Zhao,Lin Su,Xiaoqing Wang,Chujie Zheng,Sarang Metkar,Rupesh Gupta,Igor Lapchuk,David N. Racca,Madhumitha Mohan,Yanbo Li,Haojun Li,Saloni Gandhi,Xueying Lu,Chetan Bhole,Ali Hooshmand,Xin Yang,Raghavan Muthuregunathan,Jiajun Zhang,Mathew Teoh,Adam Coler,Abhinav Gupta,Xiaojing Ma,Sundara Raman Ramachandran,Morteza Ramezani,Yubo Wang,Lijuan Zhang,Richard Li,Jian Sheng,Chanh Nguyen,Yen-Chi Chen,Chuanrui Zhu,Claire Zhang,Jiahao Xu,Deepti Kulkarni,Qing Lan,Arvind Subramaniam,Ata Fatahibaarzi,Steven Shimizu,Yanning Chen,Zhipeng Wang,Ran He,Zhengze Zhou,Qingquan Song,Yun Dai,Caleb Johnson,Ping Liu,Shaghayegh Gharghabi,Gokulraj Mohanasundaram,Juan Bottaro,Santhosh Sachindran,Qi Guo,Yunxiang Ren,Chengming Jiang,Di Mo,Luke Simon,Jianqiang Shen,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 本文介绍了LinkedIn为AI职位搜索和AI人物搜索构建的基于大语言模型（LLM）的语义搜索框架，通过多教师蒸馏训练小型语言模型、预填充导向推理架构及模型剪枝与上下文压缩等技术，在保持高排序质量的同时大幅提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统关键词匹配搜索难以满足语义理解需求，而直接使用大语言模型进行语义搜索面临推理效率瓶颈，亟需在保证相关性和用户参与度的前提下提升吞吐量与延迟性能。

Method: 提出融合LLM相关性判别器、嵌入式检索与多教师蒸馏训练的小型语言模型的语义搜索框架；采用预填充导向的推理架构，并结合模型剪枝、上下文压缩及文本-嵌入混合交互优化。

Result: 在固定延迟约束下，排序吞吐量提升超75倍，NDCG接近教师模型水平，成为首个在效率上可比传统方法的生产级LLM排序系统，并显著提升搜索质量与用户参与度。

Conclusion: 该框架成功平衡了LLM语义搜索的质量与效率，验证了轻量化与协同优化设计在工业级语义搜索部署中的可行性与有效性。

Abstract: Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.

</details>


### [406] [High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning](https://arxiv.org/abs/2602.07333)
*Rajat Arora,Ye Tao,Jianqiang Shen,Ping Liu,Muchen Wu,Qianqi Shen,Benjamin Le,Fedor Borisyuk,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种基于强化学习的框架，用于从异构文本源中生成统一、可解释且简洁的用户表征，以支持大规模招聘平台上的个性化推荐，并在LinkedIn多个产品中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模招聘平台需要基于异构文本（如个人资料、职业数据、搜索日志）建模用户，而LLM驱动的推荐系统亟需统一、可解释、低延迟的用户表征。

Method: 提出一种新颖的强化学习框架，以隐式用户交互信号（如点击、投递）为主要奖励，结合规则化奖励（格式与长度约束），合成统一的文本用户表征。

Result: 在LinkedIn多个产品上开展的离线实验表明，该方法显著提升了关键业务指标。

Conclusion: 该方法是一种实用、无需人工标注、可扩展的方案，生成的用户表征可直接适配LLM-based系统，并具备可解释性。

Abstract: Effective personalization on large-scale job platforms requires modeling members based on heterogeneous textual sources, including profiles, professional data, and search activity logs. As recommender systems increasingly adopt Large Language Models (LLMs), creating unified, interpretable, and concise representations from heterogeneous sources becomes critical, especially for latency-sensitive online environments. In this work, we propose a novel Reinforcement Learning (RL) framework to synthesize a unified textual representation for each member. Our approach leverages implicit user engagement signals (e.g., clicks, applies) as the primary reward to distill salient information. Additionally, the framework is complemented by rule-based rewards that enforce formatting and length constraints. Extensive offline experiments across multiple LinkedIn products, one of the world's largest job platforms, demonstrate significant improvements in key downstream business metrics. This work provides a practical, labeling-free, and scalable solution for constructing interpretable user representations that are directly compatible with LLM-based systems.

</details>


### [407] [MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization](https://arxiv.org/abs/2602.07520)
*Shanlei Mu,Yuchen Jiang,Shikang Wu,Shiyong Hong,Tianmu Sha,Junjie Zhang,Jie Zhu,Zhe Chen,Zhe Wang,Jingjian Lin*

Main category: cs.IR

TL;DR: 本文提出了一种统一的多分布学习（MDL）框架，受大语言模型提示学习启发，将场景和任务信息建模为专用token，通过统一token化、特征自注意力、领域-特征注意力和领域融合聚合机制，实现对大规模参数的深度利用与场景-任务联合建模，在工业推荐系统中显著提升效果并已落地部署。


<details>
  <summary>Details</summary>
Motivation: 现有多场景学习（MSL）和多任务学习（MTL）方法存在两大问题：一是大规模模型参数因缺乏与复杂特征模块的充分交互而未被充分利用；二是难以在统一框架下联合建模场景与任务信息。

Method: 提出多分布学习（MDL）框架，核心包括：（1）统一信息token化模块，将特征、场景、任务映射为统一token格式；（2）特征token自注意力机制增强特征交互；（3）领域-特征注意力实现场景/任务自适应特征激活；（4）领域融合聚合支持联合分布预测；整体通过层叠式bottom-up方式让场景/任务token‘提示’并激活模型参数。

Result: 在真实工业数据集上显著优于SOTA的MSL和MTL方法；抖音搜索平台为期一个月的线上A/B测试显示LT30提升0.0626%，换搜率降低0.3267%；已全量上线，服务数亿用户/日。

Conclusion: MDL通过借鉴LLM提示范式，实现了对多场景多任务推荐中大规模模型参数的高效利用与统一建模，兼具强性能与高工程可部署性，验证了提示思想在工业推荐系统中的有效性与普适性。

Abstract: Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction with complex feature modules, and (2) difficulty in jointly modeling scenario and task information in a unified framework. To address these challenges, we propose a unified \textbf{M}ulti-\textbf{D}istribution \textbf{L}earning (MDL) framework, inspired by the "prompting" paradigm in large language models (LLMs). MDL treats scenario and task information as specialized tokens rather than auxiliary inputs or gating signals. Specifically, we introduce a unified information tokenization module that transforms features, scenarios, and tasks into a unified tokenized format. To facilitate deep interaction, we design three synergistic mechanisms: (1) feature token self-attention for rich feature interactions, (2) domain-feature attention for scenario/task-adaptive feature activation, and (3) domain-fused aggregation for joint distribution prediction. By stacking these interactions, MDL enables scenario and task information to "prompt" and activate the model's vast parameter space in a bottom-up, layer-wise manner. Extensive experiments on real-world industrial datasets demonstrate that MDL significantly outperforms state-of-the-art MSL and MTL baselines. Online A/B testing on Douyin Search platform over one month yields +0.0626\% improvement in LT30 and -0.3267\% reduction in change query rate. MDL has been fully deployed in production, serving hundreds of millions of users daily.

</details>


### [408] [IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory](https://arxiv.org/abs/2602.07525)
*Xingliang Hou,Yuyan Liu,Qi Sun,haoxiu wang,Hao Hu,Shaoyi Du,Zhiqiang Tian*

Main category: cs.IR

TL;DR: 本文提出IGMiRAG框架，通过构建分层异构超图和双焦点检索机制，模拟人类直觉推理过程，提升RAG在效率与效果上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于图/超图的RAG方法存在记忆组织错位、检索代价高且割裂的问题，难以有效建模跨文本关联与多粒度知识。

Method: 提出IGMiRAG：1）构建分层异构超图以对齐多粒度知识并嵌入演绎路径；2）设计问题解析器动态控制挖掘深度与记忆窗口；3）采用双焦点检索激活瞬时记忆锚点；4）引入双向扩散算法沿演绎路径深度挖掘记忆。

Result: 在多项评测中，IGMiRAG整体超越SOTA基线4.8% EM和5.0% F1；令牌消耗随任务复杂度自适应（平均6.3k+，最低3.0k+）。

Conclusion: IGMiRAG提供了一种更贴近人类直觉推理、兼顾成本效益与性能的新型RAG范式。

Abstract: Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.

</details>


### [409] [MSN: A Memory-based Sparse Activation Scaling Framework for Large-scale Industrial Recommendation](https://arxiv.org/abs/2602.07526)
*Shikang Wu,Hui Lu,Jinqiu Jin,Zheng Chai,Shiyong Hong,Junjie Zhang,Shanlei Mu,Kaiyuan Ma,Tianyi Liu,Yuchao Zheng,Zhe Wang,Jingjian Lin*

Main category: cs.IR

TL;DR: 本文提出MSN（Memory-based Sparse activation scaling framework），一种基于记忆的稀疏激活扩展框架，用于提升推荐模型性能并控制计算与内存开销。通过动态检索个性化记忆表征、引入Product-Key Memory降低检索复杂度，并结合归一化、过参数化及定制算子优化训练与推理效率，MSN在离线和在线实验中均取得显著效果，已成功部署于抖音搜索排序系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习推荐模型扩展方法计算开销大、难以满足工业级低延迟要求；稀疏激活方法（如MoE）虽减少计算，但内存访问成本高、个性化能力受限于专家数量少且规模大。

Method: 提出MSN框架：1）基于大容量参数化记忆动态检索个性化表征；2）设计记忆门控机制融入特征交互；3）采用Product-Key Memory（PKM）实现亚线性复杂度的记忆检索；4）引入归一化与过参数化防止记忆利用失衡与检索崩溃；5）定制Sparse-Gather与AirTopK算子提升工业场景下的训练/推理效率。

Result: 在多个数据集上MSn持续提升推荐性能且保持高效率；已在抖音搜索排序系统上线，离线指标与大规模线上A/B测试均显著优于当前SOTA模型。

Conclusion: MSN通过记忆增强的稀疏激活机制，在保障低计算与内存开销的同时，实现了细粒度个性化与可扩展性，为工业级推荐系统提供了一种高效可行的大模型扩展路径。

Abstract: Scaling deep learning recommendation models is an effective way to improve model expressiveness. Existing approaches often incur substantial computational overhead, making them difficult to deploy in large-scale industrial systems under strict latency constraints. Recent sparse activation scaling methods, such as Sparse Mixture-of-Experts, reduce computation by activating only a subset of parameters, but still suffer from high memory access costs and limited personalization capacity due to the large size and small number of experts. To address these challenges, we propose MSN, a memory-based sparse activation scaling framework for recommendation models. MSN dynamically retrieves personalized representations from a large parameterized memory and integrates them into downstream feature interaction modules via a memory gating mechanism, enabling fine-grained personalization with low computational overhead. To enable further expansion of the memory capacity while keeping both computational and memory access costs under control, MSN adopts a Product-Key Memory (PKM) mechanism, which factorizes the memory retrieval complexity from linear time to sub-linear complexity. In addition, normalization and over-parameterization techniques are introduced to maintain balanced memory utilization and prevent memory retrieval collapse. We further design customized Sparse-Gather operator and adopt the AirTopK operator to improve training and inference efficiency in industrial settings. Extensive experiments demonstrate that MSN consistently improves recommendation performance while maintaining high efficiency. Moreover, MSN has been successfully deployed in the Douyin Search Ranking System, achieving significant gains over deployed state-of-the-art models in both offline evaluation metrics and large-scale online A/B test.

</details>


### [410] [HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation](https://arxiv.org/abs/2602.07739)
*Hiren Madhu,Ngoc Bui,Ali Maatouk,Leandros Tassiulas,Smita Krishnaswamy,Menglin Yang,Sukanta Ganguly,Kiran Srinivasan,Rex Ying*

Main category: cs.IR

TL;DR: 本文提出超双曲空间中的密集检索方法HyTE，通过超双曲嵌入更好地建模自然语言的层次结构，提升RAG系统的检索质量与答案准确性。


<details>
  <summary>Details</summary>
Motivation: 欧氏嵌入难以保持自然语言固有的层次结构（如从宽泛主题到具体实体），导致语义距离远的文档被错误匹配，增加幻觉风险。

Method: 提出两种基于洛伦兹模型的超双曲密集检索模型：全超双曲Transformer（HyTE-FH）和混合架构（HyTE-H）；引入几何感知的Outward Einstein Midpoint池化算子防止序列聚合时表征坍缩。

Result: 在MTEB上HyTE-FH优于对应欧氏基线；在RAGBench上HyTE-H在上下文相关性和答案相关性上较欧氏基线最高提升29%，且模型更小；超双曲嵌入通过范数分离编码文档特异性，径向增长超20%，该特性在欧氏嵌入中不存在。

Conclusion: 超双曲几何先验对构建保真RAG系统至关重要，能更准确建模语言层次性并提升检索与生成质量。

Abstract: Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.

</details>


### [411] [Generative Reasoning Re-ranker](https://arxiv.org/abs/2602.07774)
*Mingfu Liang,Yufei Li,Jay Xu,Kavosh Asadi,Xi Liu,Shuo Gu,Kaushik Rangadurai,Frank Shyu,Shuaiwen Wang,Song Yang,Zhijing Li,Jiang Liu,Mengying Sun,Fei Tian,Xiaohan Wei,Chonglin Sun,Jacob Tao,Shike Mei,Hamed Firooz,Wenlin Chen,Luke Simon*

Main category: cs.IR

TL;DR: 本文提出了一种面向重排序（reranking）任务的生成式推理重排序器GR2，通过语义ID编码、高质量推理轨迹监督微调和专为重排序设计的可验证奖励强化学习（DAPO），显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的推荐系统研究存在三大局限：忽视关键的reranking阶段；未充分利用LLM的推理能力（尤其RL与高质量推理数据）；使用非语义ID表示物品，导致工业级扩展困难。

Method: 提出Generative Reasoning Reranker（GR2）三阶段训练框架：1）用语义ID对非语义ID进行tokenizer编码并中训LLM；2）用更大LLM生成高质量推理轨迹，用于监督微调；3）采用Decoupled Clip与Dynamic sAmpling Policy Optimization（DAPO）进行可验证奖励的强化学习优化。

Result: 在两个真实数据集上，GR2超越SOTA方法OneRec-Think：Recall@5提升2.4%，NDCG@5提升1.3%；消融实验证明高质量推理轨迹和条件化可验证奖励对性能提升至关重要。

Conclusion: GR2有效解决了LLM在推荐reranking中的关键瓶颈，验证了语义ID、高质量推理监督与定制化RL奖励设计对提升重排序性能的必要性与有效性。

Abstract: Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.

</details>


### [412] [SAGE: Scalable AI Governance & Evaluation](https://arxiv.org/abs/2602.07840)
*Benjamin Le,Xueying Lu,Nick Stern,Wenqiong Liu,Igor Lapchuk,Xiang Li,Baofen Zheng,Kevin Rosenberg,Jiewen Huang,Zhe Zhang,Abraham Cabangbang,Satej Milind Wagle,Jianqiang Shen,Raghavan Muthuregunathan,Abhinav Gupta,Mathew Teoh,Andrew Kirk,Thomas Kwan,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 本文提出了SAGE框架，通过自然语言政策、先例和LLM代理法官的双向校准循环，将主观相关性判断转化为可执行的多维评分标准，并利用师生蒸馏技术降低成本，显著提升LinkedIn搜索系统的相关性评估与模型迭代效率。


<details>
  <summary>Details</summary>
Motivation: 传统相关性评估方法（如参与度代理或稀疏人工审核）难以全面捕捉高影响的相关性失败，且存在人工监督细致但资源受限与生产系统高吞吐需求之间的治理鸿沟。

Method: 提出SAGE框架，核心是自然语言Policy、精选Precedent与LLM Surrogate Judge的双向校准循环；采用教师-学生蒸馏将高保真判断压缩为低成本代理模型；在LinkedIn搜索生态中进行仿真驱动开发与线上部署。

Result: SAGE实现了接近人类水平的判断一致性，代理模型成本降低92倍，在生产中检测到参与度指标无法发现的回归问题，并带来LinkedIn日活用户0.25%的提升。

Conclusion: SAGE成功弥合了人工判断质量与工业级规模评估之间的鸿沟，为大规模搜索系统提供了可扩展、可解释、策略对齐的AI治理与评估新范式。

Abstract: Evaluating relevance in large-scale search systems is fundamentally constrained by the governance gap between nuanced, resource-constrained human oversight and the high-throughput requirements of production systems. While traditional approaches rely on engagement proxies or sparse manual review, these methods often fail to capture the full scope of high-impact relevance failures. We present \textbf{SAGE} (Scalable AI Governance \& Evaluation), a framework that operationalizes high-quality human product judgment as a scalable evaluation signal. At the core of SAGE is a bidirectional calibration loop where natural-language \emph{Policy}, curated \emph{Precedent}, and an \emph{LLM Surrogate Judge} co-evolve. SAGE systematically resolves semantic ambiguities and misalignments, transforming subjective relevance judgment into an executable, multi-dimensional rubric with near human-level agreement. To bridge the gap between frontier model reasoning and industrial-scale inference, we apply teacher-student distillation to transfer high-fidelity judgments into compact student surrogates at \textbf{92$\times$} lower cost. Deployed within LinkedIn Search ecosystems, SAGE guided model iteration through simulation-driven development, distilling policy-aligned models for online serving and enabling rapid offline evaluation. In production, it powered policy oversight that measured ramped model variants and detected regressions invisible to engagement metrics. Collectively, these drove a \textbf{0.25\%} lift in LinkedIn daily active users.

</details>


### [413] [SimGR: Escaping the Pitfalls of Generative Decoding in LLM-based Recommendation](https://arxiv.org/abs/2602.07847)
*Yuanbo Zhao,Ruochen Liu,Senzhang Wang,Jun Yin,Yuxin Dong,Huan Gong,Hao Chen,Shirui Pan,Chengqi Zhang*

Main category: cs.IR

TL;DR: 本文提出SimGR框架，通过在共享潜在空间中直接建模用户对物品的偏好分布并基于相似度排序，解决现有LLM生成式推荐中因token级生成导致的item级分布偏差问题，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的生成式推荐方法在估计物品级偏好分布时存在系统性偏差：自回归生成受束搜索剪枝影响导致覆盖不全，而并行生成则因假设词元独立而扭曲概率，根本原因在于用词元级生成近似物品级分布造成建模错配。

Method: 提出SimGR（Simply Generative Recommendation）框架，摒弃词元级生成，转而在共享潜在空间中直接建模用户与物品的嵌入表示，并通过计算用户嵌入与物品嵌入的相似度进行排序推荐。

Result: 在多个数据集和不同LLM主干网络上的大量实验表明，SimGR始终优于现有生成式推荐方法。

Conclusion: 直接建模物品级偏好分布比间接的词元级生成更准确、更契合推荐目标；SimGR有效缓解了分布失真，为LLM驱动的生成式推荐提供了更可靠的基础范式。

Abstract: A core objective in recommender systems is to accurately model the distribution of user preferences over items to enable personalized recommendations. Recently, driven by the strong generative capabilities of large language models (LLMs), LLM-based generative recommendation has become increasingly popular. However, we observe that existing methods inevitably introduce systematic bias when estimating item-level preference distributions. Specifically, autoregressive generation suffers from incomplete coverage due to beam search pruning, while parallel generation distorts probabilities by assuming token independence. We attribute this issue to a fundamental modeling mismatch: these methods approximate item-level distributions via token-level generation, which inherently induces approximation errors. Through both theoretical analysis and empirical validation, we demonstrate that token-level generation cannot faithfully substitute item-level generation, leading to biased item distributions. To address this, we propose \textbf{Sim}ply \textbf{G}enerative \textbf{R}ecommendation (\textbf{SimGR}), a framework that directly models item-level preference distributions in a shared latent space and ranks items by similarity, thereby aligning the modeling objective with recommendation and mitigating distributional distortion. Extensive experiments across multiple datasets and LLM backbones show that SimGR consistently outperforms existing generative recommenders. Our code is available at https://anonymous.4open.science/r/SimGR-C408/

</details>


### [414] [Learning to Alleviate Familiarity Bias in Video Recommendation](https://arxiv.org/abs/2602.07987)
*Zheng Ren,Yi Wu,Jianan Lu,Acar Ary,Yiqu Liu,Li Wei,Lukasz Heldt*

Main category: cs.IR

TL;DR: 本文提出LAFB框架，在视频推荐系统的后排序阶段缓解熟悉度偏差，通过建模用户-内容熟悉度并估计个性化去偏因子来调整评分，提升新颖内容曝光和创作者多样性，已在YouTube实际部署。


<details>
  <summary>Details</summary>
Motivation: 现代视频推荐系统受行为偏差影响，存在结构性曝光不平衡问题，尤其在后排序阶段的熟悉度偏差显著影响内容多样性和新兴创作者曝光。

Method: 提出轻量级、模型无关的LAFB框架，利用离散与连续交互特征建模用户-内容熟悉度，并估计个性化去偏因子以调整预测评分，从而优化最终排序。

Result: 大规模离线评估与线上A/B测试表明，LAFB提升了新颖观看时长占比、增强了新兴创作者曝光与整体内容多样性，同时保持总观看时长与短期满意度稳定。

Conclusion: LAFB有效缓解后排序阶段的熟悉度偏差，兼顾平台目标与生态健康，已在YouTube推荐系统中成功上线应用。

Abstract: Modern video recommendation systems aim to optimize user engagement and platform objectives, yet often face structural exposure imbalances caused by behavioral biases. In this work, we focus on the post-ranking stage and present LAFB (Learning to Alleviate Familiarity Bias), a lightweight and model-agnostic framework designed to mitigate familiarity bias in recommendation outputs. LAFB models user-content familiarity using discrete and continuous interaction features, and estimates personalized debiasing factors to adjust user rating prediction scores, thereby reducing the dominance of familiar content in the final ranking. We conduct large-scale offline evaluations and online A/B testing in a real-world recommendation system, under a unified serving stack that also compares LAFB with deployable popularity-oriented remedies. Results show that LAFB increases novel watch-time share and improves exposure for emerging creators and overall content diversity, while maintaining stable overall watch time and short-term satisfaction. LAFB has already been launched in the post-ranking stage of YouTube's recommendation system, demonstrating its effectiveness in real-world applications.

</details>


### [415] [IRB: Automated Generation of Robust Factuality Benchmarks](https://arxiv.org/abs/2602.08070)
*Lam Thanh Do,Bhagyashree Taleka,Hozaifa Ammar Bhutta,Vikram Sharma Mailthody,Kevin Chen-Chuan Chang,Wen-mei Hwu*

Main category: cs.IR

TL;DR: 本文提出IRB框架，用于自动生成评估RAG系统事实性的基准测试，通过事实性与算法性支架实现，并验证其对前沿大模型构成挑战，指出提升检索模块比扩大生成器更经济有效。


<details>
  <summary>Details</summary>
Motivation: 静态RAG基准易饱和且维护成本高，需自动化、鲁棒的基准生成方法。

Method: 提出IRB框架，采用基于‘事实性支架’和‘算法性支架’的结构化生成流水线，自动构建评估基准。

Result: IRB基准对闭卷设置下的前沿大语言模型构成显著挑战；推理型大模型更可靠；提升检索组件比扩大生成器更能成本有效地提升RAG系统正确性。

Conclusion: 自动化基准生成框架IRB可有效评估RAG事实性，且揭示检索优化比模型缩放更具性价比。

Abstract: Static benchmarks for RAG systems often suffer from rapid saturation and require significant manual effort to maintain robustness. To address this, we present IRB, a framework for automatically generating benchmarks to evaluate the factuality of RAG systems. IRB employs a structured generation pipeline utilizing \textit{factual scaffold} and \textit{algorithmic scaffold}. We utilize IRB to construct a benchmark and evaluate frontier LLMs and retrievers. Our results demonstrate that IRB poses a significant challenge for frontier LLMs in the closed-book setting. Furthermore, our evaluation suggests that reasoning LLMs are more reliable, and that improving the retrieval component may yield more cost-effective gains in RAG system correctness than scaling the generator.

</details>


### [416] [A Sketch+Text Composed Image Retrieval Dataset for Thangka](https://arxiv.org/abs/2602.08411)
*Jinyu Xu,Yi Sun,Jiangling Zhang,Qing Xie,Daomin Ji,Zhifeng Bao,Jiachen Li,Yanchun Ma,Yongjian Liu*

Main category: cs.IR

TL;DR: 本文提出了CIRThan，一个面向唐卡图像的草图+文本组合图像检索数据集，旨在支持细粒度语义推理、结构化视觉理解与领域知识驱动的检索任务。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索（CIR）基准主要面向通用图像，依赖简短文本修改的参考图像，难以支撑需精细语义推理、结构理解及领域知识的检索场景，尤其在文化传承等专业视觉领域存在明显不足。

Method: 构建了CIRThan数据集：包含2287幅高质量唐卡图像，每幅配有人工绘制草图和三级层次化文本描述（结构意图+多级语义），并提供标准划分、全面分析及监督/零样本CIR方法的基准评测。

Result: 实验表明，现有面向通用图像的CIR方法在无唐卡领域监督时，难以有效对齐草图抽象表征与层次化文本语义，检索性能显著下降。

Conclusion: CIRThan为推进草图+文本CIR、层次语义建模及文化传承等知识特定视觉领域的多模态检索提供了重要新基准。

Abstract: Composed Image Retrieval (CIR) enables image retrieval by combining multiple query modalities, but existing benchmarks predominantly focus on general-domain imagery and rely on reference images with short textual modifications. As a result, they provide limited support for retrieval scenarios that require fine-grained semantic reasoning, structured visual understanding, and domain-specific knowledge. In this work, we introduce CIRThan, a sketch+text Composed Image Retrieval dataset for Thangka imagery, a culturally grounded and knowledge-specific visual domain characterized by complex structures, dense symbolic elements, and domain-dependent semantic conventions. CIRThan contains 2,287 high-quality Thangka images, each paired with a human-drawn sketch and hierarchical textual descriptions at three semantic levels, enabling composed queries that jointly express structural intent and multi-level semantic specification. We provide standardized data splits, comprehensive dataset analysis, and benchmark evaluations of representative supervised and zero-shot CIR methods. Experimental results reveal that existing CIR approaches, largely developed for general-domain imagery, struggle to effectively align sketch-based abstractions and hierarchical textual semantics with fine-grained Thangka images, particularly without in-domain supervision. We believe CIRThan offers a valuable benchmark for advancing sketch+text CIR, hierarchical semantic modeling, and multimodal retrieval in cultural heritage and other knowledge-specific visual domains. The dataset is publicly available at https://github.com/jinyuxu-whut/CIRThan.

</details>


### [417] [Hybrid Pooling with LLMs via Relevance Context Learning](https://arxiv.org/abs/2602.08457)
*David Otero,Javier Parapar*

Main category: cs.IR

TL;DR: 本文提出Relevance Context Learning (RCL)框架，利用大语言模型从人工标注中提取主题相关的显式相关性描述（relevance narratives），再用其指导另一LLM进行自动相关性判断，在IR数据集构建中显著优于零样本提示和标准上下文学习。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动相关性判断方法（如零样本提示、小样本上下文学习）难以捕捉主题层面的隐含相关性准则，泛化能力受限，且人工标注成本高。

Method: 提出RCL框架：先用Instructor LLM分析已标注的查询-文档对，生成描述该主题相关性准则的显式叙事（relevance narratives）；再将这些叙事作为结构化提示，引导Assessor LLM完成新查询-文档对的相关性判断；并设计混合池化策略评估其在真实数据收集场景下的效果。

Result: RCL在实验中显著优于零样本提示，并持续超越标准ICL；所生成的相关性叙事能更有效地利用人工标注信息，提升LLM在IR数据集构建中的判断质量。

Conclusion: 将相关性示例转化为显式、上下文感知的相关性叙事，是比直接使用示例进行上下文学习更有效的LLM辅助IR数据构建范式。

Abstract: High-quality relevance judgements over large query sets are essential for evaluating Information Retrieval (IR) systems, yet manual annotation remains costly and time-consuming. Large Language Models (LLMs) have recently shown promise as automatic relevance assessors, but their reliability is still limited. Most existing approaches rely on zero-shot prompting or In-Context Learning (ICL) with a small number of labeled examples. However, standard ICL treats examples as independent instances and fails to explicitly capture the underlying relevance criteria of a topic, restricting its ability to generalize to unseen query-document pairs. To address this limitation, we introduce Relevance Context Learning (RCL), a novel framework that leverages human relevance judgements to explicitly model topic-specific relevance criteria. Rather than directly using labeled examples for in-context prediction, RCL first prompts an LLM (Instructor LLM) to analyze sets of judged query-document pairs and generate explicit narratives that describe what constitutes relevance for a given topic. These relevance narratives are then used as structured prompts to guide a second LLM (Assessor LLM) in producing relevance judgements. To evaluate RCL in a realistic data collection setting, we propose a hybrid pooling strategy in which a shallow depth-\textit{k} pool from participating systems is judged by human assessors, while the remaining documents are labeled by LLMs. Experimental results demonstrate that RCL substantially outperforms zero-shot prompting and consistently improves over standard ICL. Overall, our findings indicate that transforming relevance examples into explicit, context-aware relevance narratives is a more effective way of exploiting human judgements for LLM-based IR dataset construction.

</details>


### [418] [PIT: A Dynamic Personalized Item Tokenizer for End-to-End Generative Recommendation](https://arxiv.org/abs/2602.08530)
*Huanjie Wang,Xinchen Luo,Honghui Bao,Zhang Zixing,Lejian Ren,Yunfan Wu,Hongwei Zhang,Liwei Guan,Guang Chen*

Main category: cs.IR

TL;DR: 本文提出PIT框架，通过协同生成架构实现个性化物品标记器与生成式推荐器的端到端联合演化，解决现有方法中协同信号不稳定和训练次优问题，在快手线上A/B测试中显著提升App停留时间。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法依赖静态、解耦的分词方式，忽略协同信号；而引入协同信号的方法在生产环境中面临信号波动导致分词不稳定、端到端训练退化为两阶段等问题。

Method: 提出PIT（Personalized Item Tokenizer）框架，采用协同生成架构：1）通过协同信号对齐建模协同模式；2）通过协同演化学习同步更新物品分词器与生成式推荐器；3）引入一对多beam索引提升可扩展性与鲁棒性。

Result: 在多个真实数据集上显著优于基线方法；在快手大规模线上部署中，A/B测试带来App Stay Time 0.402%的显著提升。

Conclusion: PIT实现了索引构建与推荐过程的动态、联合、端到端演化，有效提升了生成式推荐在工业动态环境中的稳定性与效果。

Abstract: Generative Recommendation has revolutionized recommender systems by reformulating retrieval as a sequence generation task over discrete item identifiers. Despite the progress, existing approaches typically rely on static, decoupled tokenization that ignores collaborative signals. While recent methods attempt to integrate collaborative signals into item identifiers either during index construction or through end-to-end modeling, they encounter significant challenges in real-world production environments. Specifically, the volatility of collaborative signals leads to unstable tokenization, and current end-to-end strategies often devolve into suboptimal two-stage training rather than achieving true co-evolution. To bridge this gap, we propose PIT, a dynamic Personalized Item Tokenizer framework for end-to-end generative recommendation, which employs a co-generative architecture that harmonizes collaborative patterns through collaborative signal alignment and synchronizes item tokenizer with generative recommender via a co-evolution learning. This enables the dynamic, joint, end-to-end evolution of both index construction and recommendation. Furthermore, a one-to-many beam index ensures scalability and robustness, facilitating seamless integration into large-scale industrial deployments. Extensive experiments on real-world datasets demonstrate that PIT consistently outperforms competitive baselines. In a large-scale deployment at Kuaishou, an online A/B test yielded a substantial 0.402% uplift in App Stay Time, validating the framework's effectiveness in dynamic industrial environments.

</details>


### [419] [DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.08545)
*Xingyuan Zeng,Zuohan Wu,Yue Wang,Chen Zhang,Quanming Yao,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: 本文提出DA-RAG，一种基于动态属性社区搜索（ACS）的图增强检索增强生成方法，通过挖掘高阶图结构和构建分块层图索引，显著提升复杂查询效果并降低计算与经济成本。


<details>
  <summary>Details</summary>
Motivation: 现有图增强RAG（G-RAG）方法多局限于低阶图结构或静态社区，难以应对动态复杂查询，导致知识检索效果受限。

Method: 提出DA-RAG框架：1）利用属性社区搜索（ACS）动态提取与问题相关的子图，捕获高阶图结构；2）设计chunk-layer oriented图索引，支持高效多粒度检索。

Result: 在多个数据集上，DA-RAG在四项指标上较现有RAG方法最高提升40%；索引构建时间减少最多37%，token开销降低最多41%。

Conclusion: DA-RAG通过动态高阶图结构建模与高效图索引设计，在效果、效率与成本间取得更好平衡，为RAG提供了更鲁棒的图增强范式。

Abstract: Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.

</details>


### [420] [QARM V2: Quantitative Alignment Multi-Modal Recommendation for Reasoning User Sequence Modeling](https://arxiv.org/abs/2602.08559)
*Tian Xia,Jiaqi Zhang,Yueyang Liu,Hongjian Dou,Tingya Yin,Jiangxia Cao,Xulei Liang,Tianlu Xie,Lihao Liu,Xiang Chen,Shen Wang,Changxin Lao,Haixiang Gan,Jinkai Yu,Keting Cen,Lu Hao,Xu Zhang,Qiqiang Zhong,Zhongbo Sun,Yiyu Wang,Shuang Yang,Mingxin Wen,Xiangyu Wu,Shaoguo Liu,Tingting Gao,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出QARM V2框架，将大语言模型（LLM）的语义理解能力与推荐系统（RecSys）的业务需求相结合，解决传统ID嵌入方法信息密度低、知识孤立、泛化弱等问题，以及LLM嵌入与业务目标不匹配、难以端到端优化的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖ID嵌入建模用户序列，在GSU/ESU范式下存在信息密度低、知识隔离和泛化能力弱的问题；而LLM虽具语义丰富性和强泛化性，但其嵌入与推荐业务目标不匹配且难以端到端适配下游任务。

Method: 提出QARM V2统一框架，桥接LLM语义理解与RecSys业务需求，用于用户序列建模。

Result: 实现了LLM语义能力与工业推荐系统实际需求的有效融合，提升了用户序列建模效果。

Conclusion: QARM V2为将LLM融入工业级推荐系统提供了可行且有效的技术路径，兼顾语义表达能力与业务可解释性、可优化性。

Abstract: With the evolution of large language models (LLMs), there is growing interest in leveraging their rich semantic understanding to enhance industrial recommendation systems (RecSys). Traditional RecSys relies on ID-based embeddings for user sequence modeling in the General Search Unit (GSU) and Exact Search Unit (ESU) paradigm, which suffers from low information density, knowledge isolation, and weak generalization ability. While LLMs offer complementary strengths with dense semantic representations and strong generalization, directly applying LLM embeddings to RecSys faces critical challenges: representation unmatch with business objectives and representation unlearning end-to-end with downstream tasks. In this paper, we present QARM V2, a unified framework that bridges LLM semantic understanding with RecSys business requirements for user sequence modeling.

</details>


### [421] [RankGR: Rank-Enhanced Generative Retrieval with Listwise Direct Preference Optimization in Recommendation](https://arxiv.org/abs/2602.08575)
*Kairui Fu,Changfa Wu,Kun Yuan,Binbin Cao,Dunxian Huang,Yuliang Yan,Junjun Zheng,Jianning Zhang,Silu Zhou,Jian Wu,Kun Kuang*

Main category: cs.IR

TL;DR: 本文提出RankGR，一种结合列表式直接偏好优化的生成式检索方法，通过初始评估和精细评分两个阶段提升推荐效果，并在淘宝'猜你喜欢'场景中验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法局限于逐token预测，难以捕捉用户偏好的细微结构及解码标识符与用户行为序列间的深层交互。

Method: RankGR将检索过程分解为初始评估阶段（IAP）和精细评分阶段（RSP）：IAP引入列表式直接偏好优化以建模用户层级偏好和偏序关系；RSP使用轻量打分模块对IAP输出的top-λ候选进行精细化评估；两阶段联合优化于统一生成式检索框架下。

Result: RankGR在研究与工业数据集上离线性能显著提升，并在淘宝'猜你喜欢'线上场景实现真实收益；系统支持近万QPS实时推理。

Conclusion: RankGR通过融合列表式偏好建模与生成式检索，在保持生成范式优势的同时增强了排序能力与用户建模深度，兼具有效性、效率与工程实用性。

Abstract: Generative retrieval (GR) has emerged as a promising paradigm in recommendation systems by autoregressively decoding identifiers of target items. Despite its potential, current approaches typically rely on the next-token prediction schema, which treats each token of the next interacted items as the sole target. This narrow focus 1) limits their ability to capture the nuanced structure of user preferences, and 2) overlooks the deep interaction between decoded identifiers and user behavior sequences. In response to these challenges, we propose RankGR, a Rank-enhanced Generative Retrieval method that incorporates listwise direct preference optimization for recommendation. RankGR decomposes the retrieval process into two complementary stages: the Initial Assessment Phase (IAP) and the Refined Scoring Phase (RSP). In IAP, we incorporate a novel listwise direct preference optimization strategy into GR, thus facilitating a more comprehensive understanding of the hierarchical user preferences and more effective partial-order modeling. The RSP then refines the top-λ candidates generated by IAP with interactions towards input sequences using a lightweight scoring module, leading to more precise candidate evaluation. Both phases are jointly optimized under a unified GR model, ensuring consistency and efficiency. Additionally, we implement several practical improvements in training and deployment, ultimately achieving a real-time system capable of handling nearly ten thousand requests per second. Extensive offline performance on both research and industrial datasets, as well as the online gains on the "Guess You Like" section of Taobao, validate the effectiveness and scalability of RankGR.

</details>


### [422] [OneLive: Dynamically Unified Generative Framework for Live-Streaming Recommendation](https://arxiv.org/abs/2602.08612)
*Shen Wang,Yusheng Huang,Ruochen Yang,Shuang Wen,Pengbo Xu,Jiangxia Cao,Yueyang Liu,Kuo Cai,Chengcheng Guo,Shiyao Wang,Xinchen Luo,Qiang Luo,Ruiming Tang,Shuang Yang,Zhaojie Liu,Guorui Zhou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出OneLive，一种面向直播场景的动态统一生成式推荐框架，通过动态分词器、时间感知门控注意力、高效解码器架构和多目标对齐框架，解决直播推荐中内容动态演化、实时性要求高和多目标优化等挑战。


<details>
  <summary>Details</summary>
Motivation: 直播推荐场景具有内容持续演化、生命周期短、实时性要求高和多目标异构等特点，导致传统生成式推荐方法难以直接迁移应用。

Method: 提出OneLive框架，包含四个核心组件：(i)基于残差量化的动态分词器；(ii)显式建模时间动态的时间感知门控注意力机制；(iii)结合Sequential MTP与QK Norm的高效仅解码器生成架构；(iv)强化个性化偏好的统一多目标对齐框架。

Result: OneLive在直播推荐任务中实现了更优的实时响应能力、稳定训练性能与多目标协同优化效果，提升了推荐质量与系统效率。

Conclusion: OneLive为直播流媒体推荐提供了一种可扩展、高时效、多目标兼容的生成式解决方案，推动了生成式推荐在动态实时场景中的落地应用。

Abstract: Live-streaming recommender system serves as critical infrastructure that bridges the patterns of real-time interactions between users and authors. Similar to traditional industrial recommender systems, live-streaming recommendation also relies on cascade architectures to support large-scale concurrency. Recent advances in generative recommendation unify the multi-stage recommendation process with Transformer-based architectures, offering improved scalability and higher computational efficiency. However, the inherent complexity of live-streaming prevents the direct transfer of these methods to live-streaming scenario, where continuously evolving content, limited lifecycles, strict real-time constraints, and heterogeneous multi-objectives introduce unique challenges that invalidate static tokenization and conventional model framework. To address these issues, we propose OneLive, a dynamically unified generative recommendation framework tailored for live-streaming scenario. OneLive integrates four key components: (i) A Dynamic Tokenizer that continuously encodes evolving real-time live content fused with behavior signal through residual quantization; (ii) A Time-Aware Gated Attention mechanism that explicitly models temporal dynamics for timely decision making; (iii) An efficient decoder-only generative architecture enhanced with Sequential MTP and QK Norm for stable training and accelerated inference; (iv) A Unified Multi-Objective Alignment Framework reinforces policy optimization for personalized preferences.

</details>


### [423] [SRSUPM: Sequential Recommender System Based on User Psychological Motivation](https://arxiv.org/abs/2602.08667)
*Yicheng Di,Yuan Liu,Zhi Chen,Jingcai Guo*

Main category: cs.IR

TL;DR: 本文提出了一种基于用户心理动机变化的序列推荐框架SRSUPM，通过量化心理动机变化、建模多层次动态变化状态、分解并正则化表征、以及增强与心理动机变化相关的协同模式，提升了序列推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对用户心理动机变化的显式建模，难以捕捉不同变化程度的分布规律及对心理动机变化敏感的协同知识。

Method: 提出SRSUPM框架，包括心理动机变化评估（PMSA）、变化信息构建、心理动机变化驱动的信息分解、以及心理动机变化信息匹配四个核心模块。

Result: 在三个公开基准数据集上，SRSUPM在多种序列推荐任务中持续优于代表性基线模型。

Conclusion: 显式建模用户心理动机变化能有效提升序列推荐系统的性能和表征判别性。

Abstract: Sequential recommender infers users' evolving psychological motivations from historical interactions to recommend the next preferred items. Most existing methods compress recent behaviors into a single vector and optimize it toward a single observed target item, but lack explicit modeling of psychological motivation shift. As a result, they struggle to uncover the distributional patterns across different shift degrees and to capture collaborative knowledge that is sensitive to psychological motivation shift. We propose a general framework, the Sequential Recommender System Based on User Psychological Motivation, to enhance sequential recommenders with psychological motivation shift-aware user modeling. Specifically, the Psychological Motivation Shift Assessment quantitatively measures psychological motivation shift; guided by PMSA, the Shift Information Construction models dynamically evolving multi-level shift states, and the Psychological Motivation Shift-driven Information Decomposition decomposes and regularizes representations across shift levels. Moreover, the Psychological Motivation Shift Information Matching strengthens collaborative patterns related to psychological motivation shift to learn more discriminative user representations. Extensive experiments on three public benchmarks show that SRSUPM consistently outperforms representative baselines on diverse sequential recommender tasks.

</details>


### [424] [SA-CAISR: Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation](https://arxiv.org/abs/2602.08678)
*Xiaomeng Song,Xinru Wang,Hanbing Wang,Hongyu Lu,Yu Chen,Zhaochun Ren,Zhumin Chen*

Main category: cs.IR

TL;DR: 本文提出了一种无需回放缓冲区的增量式序列推荐框架SA-CAISR，通过Fisher加权知识筛选机制动态识别并剔除过时知识，兼顾模型稳定性与适应性，在多个指标上达到SOTA性能，同时显著降低内存和训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法在序列推荐中面临高内存/计算成本（回放法）或难以有效遗忘冲突/过时知识（正则化法）的问题。

Method: 提出SA-CAISR框架：1）无缓冲区设计，仅依赖旧模型与新数据；2）引入Fisher加权知识筛选机制，基于参数级冲突估计动态识别并剔除过时知识，保留兼容历史模式。

Result: 在多个数据集上平均提升Recall@20 2.0%、MRR@20 1.2%、NDCG@20 1.4%；内存减少97.5%，训练时间减少46.9%。

Conclusion: SA-CAISR实现了高效、稳定且自适应的增量序列推荐，在性能与效率上均优于现有方法，适用于实时更新场景。

Abstract: Sequential recommendation (SR) aims to predict a user's next action by learning from their historical interaction sequences. In real-world applications, these models require periodic updates to adapt to new interactions and evolving user preferences. While incremental learning methods facilitate these updates, they face significant challenges. Replay-based approaches incur high memory and computational costs, and regularization-based methods often struggle to discard outdated or conflicting knowledge. To overcome these challenges, we propose SA-CAISR, a Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation framework. As a buffer-free framework, SA-CAISR operates using only the old model and new data, directly addressing the high costs of replay-based techniques. SA-CAISR introduces a novel Fisher-weighted knowledge-screening mechanism that dynamically identifies outdated knowledge by estimating parameter-level conflicts between the old model and new data, allowing our approach to selectively remove obsolete knowledge while preserving compatible historical patterns. This dynamic balance between stability and adaptability allows our method to achieve a new state-of-the-art performance in incremental SR. Specifically, SA-CAISR improves Recall@20 by 2.0%, MRR@20 by 1.2%, and NDCG@20 by 1.4% on average across datasets, while reducing memory usage by 97.5% and training time by 46.9% compared to the best baselines. This efficiency allows real-world systems to rapidly update user profiles with minimal computational overhead, ensuring more timely and accurate recommendations.

</details>


### [425] [AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders](https://arxiv.org/abs/2602.08837)
*Minh-Duc Nguyen,Hai-Dang Kieu,Dung D. Le*

Main category: cs.IR

TL;DR: 本文提出AMEM4Rec，一种通过跨用户记忆演化端到端学习协同过滤信号的LLM智能体推荐系统，无需微调或预训练CF模型，在Amazon和MIND数据集上优于现有LLM推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体推荐系统存在参数效率低、提示推理受限于上下文长度与幻觉风险、且忽视协同过滤（CF）信号等关键问题。

Method: 提出AMEM4Rec框架，构建全局记忆池存储抽象用户行为模式，通过记忆链接与迭代演化强化跨用户共性模式，从而隐式建模CF信号，实现端到端协同过滤学习。

Result: 在Amazon和MIND数据集上的实验表明，AMEM4Rec持续超越当前最优的LLM基推荐方法。

Conclusion: 跨用户记忆演化机制可有效引导LLM智能体感知并利用协同过滤信号，为高效、鲁棒的LLM推荐提供新范式。

Abstract: Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.

</details>


### [426] [Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation](https://arxiv.org/abs/2602.08873)
*Lisette Espin-Noboa,Gonzalo Gabriel Mendez*

Main category: cs.IR

TL;DR: 本文提出LLMScholarBench基准，用于联合评估大语言模型（LLM）在学者推荐任务中模型本身与用户干预（如温度调节、约束提示、RAG）的综合影响，发现干预并非普适性改进，而是重构性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM学术专家推荐的审计多孤立评估模型输出，忽视用户在推理时的干预，导致难以区分问题源于模型本身还是部署策略。

Method: 构建LLMScholarBench基准，涵盖9项技术质量与社会表征指标；在物理学专家推荐场景下，对22个LLM在温度变化、表征约束提示和基于网络搜索的RAG三种干预下进行系统审计。

Result: 用户干预不带来一致提升，而是重新分配错误：升温降低有效性、一致性与事实性；约束提示提升多样性但损害事实性；RAG提升技术质量却削弱多样性和公平性。

Conclusion: 端用户干预重塑的是性能维度间的权衡关系，而非提供通用解决方案；该基准可迁移至其他学科。

Abstract: Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.

</details>


### [427] [Contrastive Learning for Diversity-Aware Product Recommendations in Retail](https://arxiv.org/abs/2602.08886)
*Vasileios Karlis,Ezgi Yıldırım,David Vos,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文提出了一种结合对比学习与精心选择的负样本的方法，以提升推荐系统在长尾分布下的商品目录覆盖率，同时不损害推荐质量，已在IKEA零售场景中验证有效。


<details>
  <summary>Details</summary>
Motivation: 推荐系统常面临长尾分布和商品目录曝光不足的问题，尤其在大型在线零售场景中，热门商品主导推荐结果，导致冷门商品难以被发现。

Method: 受负采样缓解流行度偏差启发，将对比学习与精心筛选的负样本相结合，嵌入现有数字推荐流水线。

Result: 通过离线与在线评估，该方法显著提升了目录覆盖率，在保证推荐多样性的同时维持了高推荐性能。

Conclusion: 所提方法可有效平衡推荐系统的覆盖率与准确性，适用于大规模零售场景的商品推荐优化。

Abstract: Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.

</details>


### [428] [OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation](https://arxiv.org/abs/2602.08896)
*Yehua Huang,Penglei Sun,Zebin Chen,Zhenheng Tang,Xiaowen Chu*

Main category: cs.IR

TL;DR: 本文提出了OmniReview数据集和Pro-MMoE模型，以解决学术同行评审中数据稀缺、评估指标简化及嵌入方法信息瓶颈与可解释性差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于大规模验证基准数据缺乏、评估指标过于简化，无法反映真实编辑流程；同时嵌入方法存在语义压缩导致的信息瓶颈和可解释性不足问题。

Method: 构建了包含202,756条验证评审记录的OmniReview数据集，并提出三层分级评估框架；方法上设计了融合大语言模型（LLM）与多任务学习的Pro-MMoE框架，利用LLM生成语义画像增强细粒度专长建模与可解释性，并采用任务自适应MMoE架构动态平衡多目标优化。

Result: Pro-MMoE在七项指标中的六项达到SOTA性能，建立了面向真实场景的审稿人推荐新基准。

Conclusion: OmniReview数据集与Pro-MMoE模型共同推动了更贴近实际编辑流程、更具可解释性与高性能的审稿人推荐研究。

Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.

</details>


### [429] [Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion](https://arxiv.org/abs/2602.08917)
*Minghan Li,Ercong Nie,Siqi Zhao,Tongna Chen,Huiping Huang,Guodong Zhou*

Main category: cs.IR

TL;DR: 本文提出了一种自动、领域自适应的查询扩展（QE）框架，通过BM25-MonoT5构建领域内伪相关样本池，采用无监督聚类策略选取多样化示例，并引入双大语言模型（LLM）协同生成与精炼式融合机制，显著提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的查询扩展方法依赖人工设计提示、手动选择示例或单一LLM，导致可扩展性差且对领域迁移敏感。

Method: 1）利用BM25-MonoT5流水线自动构建领域内伪相关段落池；2）采用无训练聚类策略选择多样化示例以支持上下文学习；3）设计双异构LLM独立生成+精炼LLM融合的两阶段集成机制。

Result: 在TREC DL20、DBPedia和SciFact数据集上，该方法在统计意义上显著优于BM25、Rocchio、零样本及固定少样本基线。

Conclusion: 该框架提供了一个可复现的示例选择与多LLM生成测试平台，并为实际场景下的无标签查询扩展提供了实用解决方案。

Abstract: Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [430] [Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts](https://arxiv.org/abs/2602.06993)
*Shashank*

Main category: cs.LG

TL;DR: 本文提出Attractor Patch Networks (APN)，一种可即插即用的Transformer前馈网络（FFN）替代架构，通过基于相似性的稀疏路由和低秩残差更新实现上下文感知、条件化非线性变换，并在持续学习中显著缓解权重干扰，提升领域迁移时的保留与适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer中位置式前馈网络（FFN）存在的两个问题：一是密集、全局共享结构导致计算资源分配不均且无法适配语言的高度聚类上下文结构；二是持续学习中全局参数更新易引发灾难性遗忘和干扰。

Method: 提出Attractor Patch Networks（APN），以‘patch专家库’替代标准FFN：通过相似性路由器为每个token选择top-k个原型patch，每个选中patch基于紧凑编码输出低秩残差更新；形式化其为分段低秩残差函数类，并分析其表达能力及持续学习下的干扰抑制与稳定性机制。

Result: 在字符级语言建模任务上，APN达到接近基线的困惑度（4.57 vs 4.32）；在域迁移持续学习场景中，相较全局微调，原始域保留性能提升2.6倍（PPL 11.1 vs 29.4），新域适应性能提升2.8倍（PPL 6.4 vs 17.8）。

Conclusion: APN是一种结构简洁、接口兼容、具备强上下文特化能力和天然持续学习友好性的FFN替代方案，为高效、自适应的大模型架构设计提供了新范式。

Abstract: Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.
  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.
  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.

</details>


### [431] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型（LLM）的棒球逐球建模方法——Neural Sabermetrics with World Model，将比赛建模为自回归事件序列，在十年MLB数据上持续预训练，显著提升了对下一投球和击球员挥棒决策的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 传统棒球统计学（sabermetrics）擅长总结性分析，但缺乏生成式建模能力，无法逐球模拟比赛动态；现有方法多限于单步预测或事后分析，难以刻画完整比赛演化过程。

Method: 将棒球比赛建模为长程自回归事件序列，使用单一LLM在超大规模MLB追踪数据（700万次投球、30亿token）上进行持续预训练，构建端到端的逐球世界模型。

Result: 在常规赛（in-distribution）和季后赛（out-of-distribution）数据上均优于现有神经基线：下一投球预测准确率达64%，击球员挥棒决策预测准确率达78%。

Conclusion: 大语言模型可作为有效的体育世界模型，为棒球等复杂动态体育场景提供统一、生成式、多步预测的建模新范式。

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [432] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

TL;DR: 本文提出了一种滞后期兼容的物理信息神经网络（LBC-PINN），用于模拟和反演长期荷载下一维非饱和土固结过程，通过时间对数分段、滞后期兼容损失约束及分段迁移学习，实现了高精度、高效率、强鲁棒性的多尺度压力消散建模。


<details>
  <summary>Details</summary>
Motivation: 解决非饱和土中气、水压力在多时间尺度下耦合消散建模难、传统数值方法计算成本高且难以反演的问题。

Method: 构建Lagged Backward-Compatible Physics-Informed Neural Network（LBC-PINN），引入对数时间分段策略、滞后期兼容损失函数，并结合分段式迁移学习；采用FEM结果进行验证，并开展敏感性分析。

Result: LBC-PINN在长达1e10秒的时间范围内预测孔隙气压与水压演化，平均绝对误差低于1e-2；简化分段策略在保持精度的同时提升计算效率；在气/水渗透率比1e-3至1e3范围内均表现稳健。

Conclusion: LBC-PINN为非饱和土长期固结的正向模拟与参数反演提供了高效、准确且鲁棒的新范式，拓展了PINN在岩土多物理场、宽时间尺度问题中的适用性。

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [433] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

TL;DR: 本文提出TransConv-DDPM模型，结合DDPM、U-Net、多尺度卷积与Transformer层，用于生成高质量生理时序数据，在SmartFallMM和EEG数据集上优于TimeGAN和Diffusion-TS，并提升下游预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床领域真实数据稀缺，制约AI模型训练；生成生理时序数据因复杂性和变异性面临独特挑战。

Method: 提出TransConv-DDPM：基于去噪扩散概率模型（DDPM），融合U-Net架构、多尺度卷积模块与Transformer层，以同时建模全局与局部时间依赖关系。

Result: 在SmartFallMM和EEG等数据集上定量表现优于TimeGAN和Diffusion-TS；在SmartFallMM上添加合成跌倒数据后，下游模型F1-score提升13.64%，准确率提升14.93%。

Conclusion: TransConv-DDPM能有效生成高保真生理时序数据，具备推动医学AI实际应用的潜力。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [434] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 本文提出EmoReAlM基准和AVEm-DPO偏好优化方法，以解决多模态大语言模型在情感理解中因虚假关联和文本先验导致的幻觉问题，显著提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在情感理解任务中存在两类关键问题：情绪与无关音视频线索的虚假关联，以及由语言模型文本先验驱动的音视频线索幻觉。

Method: 构建EmoReAlM基准用于评估线索-情绪关联、幻觉及模态一致性；提出AVEm-DPO偏好优化技术，通过构造针对虚假关联与幻觉响应的偏好对，并引入抑制文本先验依赖的正则化项，实现音频、视频与情感查询的联合对齐。

Result: 在DFEW、RAVDESS和EMER数据集上的实验表明，该方法在零样本设置下使基线模型相对性能提升6%-19%。

Conclusion: 本工作通过提供严谨的评估基准与鲁棒的优化框架，推动了面向情感理解与社会智能的多模态大语言模型的可信赖发展。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [435] [TACIT: Transformation-Aware Capturing of Implicit Thought](https://arxiv.org/abs/2602.07061)
*Daniel Nobrega*

Main category: cs.LG

TL;DR: TACIT是一种基于扩散的可解释视觉推理Transformer模型，直接在像素空间中运行，通过rectified flow实现推理过程的可视化；在迷宫求解任务中展现出'顿悟式'的整体性推理模式，揭示了神经网络隐式推理机制与人类认知洞察现象的相似性。


<details>
  <summary>Details</summary>
Motivation: 现有语言基础的推理系统难以直观展现推理过程，而人类认知中的'顿悟'现象提示存在一种非序列、整体性的隐式推理机制；本文旨在构建一个能在像素空间直接建模并可视化此类隐式推理的模型。

Method: 提出TACIT模型：基于扩散机制的transformer架构，采用rectified flow实现在像素空间的噪声自由流匹配；以合成迷宫图像对为训练数据，学习将未解迷宫图像直接变换为解图；推理过程仅需10个Euler步，并全程可逐帧可视化。

Result: 在100万合成迷宫对上取得显著性能提升：训练损失降低192倍、L2距离改善22.7倍；发现'相变'现象——解在t=0.70时突现（仅占2%时间），且100%样本全区域同步出现，表明整体性推理而非路径逐步构造。

Conclusion: TACIT证实了神经网络可在无语言符号介入下发展出类似人类‘顿悟’的隐式、整体性视觉推理能力；其像素级、无噪声的建模范式为理解深层网络的内在推理策略提供了新路径。

Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:
  - 192x reduction in training loss over 100 epochs
  - 22.7x improvement in L2 distance to ground truth
  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)
  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.

</details>


### [436] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

TL;DR: EMSYNC is a novel, fully automatic system that generates emotionally and rhythmically synchronized music for videos, using a frozen pretrained video emotion classifier, a continuous-emotion-conditioned MIDI generator, and a temporal boundary conditioning method called 'boundary offset encodings'. It achieves state-of-the-art performance on emotion classification benchmarks and outperforms existing methods in user studies.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of online video content makes finding or creating suitable soundtracks time-consuming and costly; current tools lack automatic, emotionally intelligent, and temporally aligned music generation.

Method: EMSYNC integrates three core components: (1) a frozen-feature video emotion classifier trained on Ekman-6 and MovieNet; (2) an emotion-based MIDI generator conditioned on continuous emotional values (not discrete categories); (3) a novel temporal boundary conditioning technique ('boundary offset encodings') to align chords with scene changes. It also introduces a large-scale, emotion-labeled MIDI dataset.

Result: State-of-the-art emotion classification accuracy on Ekman-6 and MovieNet; superior user-rated performance over baselines in music richness, emotional alignment, temporal synchronization, and overall preference.

Conclusion: EMSYNC establishes a new state-of-the-art in automatic video-to-music generation by jointly optimizing emotional fidelity, rhythmic synchronization, and computational efficiency—enabling fast, free, and high-quality soundtrack creation for content creators.

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [437] [Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures](https://arxiv.org/abs/2602.07070)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 本文提出Hybrid Dual-Path Linear (HDPL)算子，将线性变换解耦为局部高秩块对角路径与全局低秩VAE瓶颈路径，在保持Transformer结构的同时提升效率与表征能力，并在FineWeb-Edu上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的密集线性变换缺乏区分局部特征保留与全局上下文整合的结构归纳偏置，且效率低下。

Method: 设计HDPL算子，包含稀疏块对角组件（用于局部高秩处理）和低秩VAE瓶颈（用于全局上下文正则化），并选择性替换Q/K/V/Gate/Up投影层，保留Output/Down等聚合层为密集形式。

Result: 在FineWeb-Edu数据集上，HDPL架构相比Llama基线降低验证损失，同时参数量减少6.8%；并显式构建了可用于推理控制、持续学习、可解释性及跨模态同步的概率潜在空间。

Conclusion: HDPL通过结构化分解线性变换，在不牺牲性能的前提下提升了模型效率与灵活性，为Transformer架构设计提供了新范式。

Abstract: Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL

</details>


### [438] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本文提出最优词元基线（OTB）方法，通过反向加权梯度范数提升RL训练稳定性，结合Logit-Gradient Proxy实现高效近似，在显著降低token消耗的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型强化学习中长程任务因梯度方差爆炸导致的训练崩溃问题，改进传统基线方法忽视词元异质性与计算开销大的缺陷。

Method: 从第一性原理推导最优词元基线（OTB），证明梯度更新应按其累积梯度范数的倒数加权；提出仅需前向概率的Logit-Gradient Proxy来高效近似梯度范数。

Result: 在单轮和工具集成推理任务中，以N=4的小组规模达到N=32的性能，token消耗减少超65%，同时提升训练稳定性。

Conclusion: OTB及其代理方法为LLM强化学习提供了理论严谨且计算高效的方差缩减方案，兼顾稳定性与资源效率。

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [439] [Attention-Driven Framework for Non-Rigid Medical Image Registration](https://arxiv.org/abs/2602.07088)
*Muhammad Zafar Iqbal,Ghazanfar Farooq Siddiqui,Anwar Ul Haq,Imran Razzak*

Main category: cs.LG

TL;DR: 本文提出了一种基于注意力机制的非刚性医学图像配准框架AD-RegNet，结合3D UNet与双向交叉注意力，在多尺度上建立图像对应关系，并引入区域自适应注意力和多分辨率形变场合成策略，在DIRLab和IXI数据集上实现了高精度、解剖合理的配准，兼顾准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习配准方法在处理大形变图像时难以兼顾配准精度与解剖合理性。

Method: 提出AD-RegNet框架：采用3D UNet主干网络，集成双向交叉注意力机制实现多尺度图像对应建模；引入区域自适应注意力聚焦解剖结构；结合多分辨率形变场合成策略生成精确形变场。

Result: 在DIRLab（胸腔4D CT）和IXI（脑部MRI）数据集上达到与当前最优方法相当的性能；在NCC、MSE、SSIM、Jacobian行列式、TRE等指标上表现优异，兼顾配准精度与形变解剖合理性。

Conclusion: 注意力机制能有效引导非刚性配准过程，提升大形变下解剖结构对齐的准确性与合理性，所提方法在精度与效率间取得良好平衡，具备临床应用潜力。

Abstract: Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.

</details>


### [440] [Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting](https://arxiv.org/abs/2602.07126)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种新的多表成员推断攻击（MT-MIA），用于评估合成关系型数据在用户级的隐私泄露风险，发现现有单表攻击低估了真实隐私风险，并验证了当前先进关系型合成数据生成器存在显著用户级隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据隐私评估方法（如单表成员推断攻击）仅关注行/项级泄露，难以刻画关系型数据库中跨表关联所导致的用户级隐私风险。

Method: 提出多表成员推断攻击（MT-MIA），在No-Box威胁模型下，利用异构图神经网络建模用户实体及其跨表关联项，实现对用户级隐私泄露的精准审计。

Result: 在多个真实多表数据集上验证了MT-MIA的有效性，揭示当前主流关系型合成数据生成器普遍存在用户级隐私泄露，且泄露主要源于跨表关系建模缺陷。

Conclusion: 用户级隐私需专门建模跨表关系，单表隐私评估不充分；MT-MIA为关系型合成数据提供了更真实的隐私审计工具，并指明了提升隐私保护的关键方向。

Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.

</details>


### [441] [Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis](https://arxiv.org/abs/2602.07135)
*Jiaqing Chen,Nicholas Hadler,Tiankai Xie,Rostyslav Hnatyshyn,Caleb Geniesse,Yaoqing Yang,Michael W. Mahoney,Talita Perciano,John F. Hartwig,Ross Maciejewski,Gunther H. Weber*

Main category: cs.LG

TL;DR: 本文介绍了Landscaper，一个用于任意维度损失景观分析的开源Python包，结合Hessian子空间构建与拓扑数据分析，提出SMAD指标量化景观平滑度，并在多种任务（包括预训练语言模型和化学性质预测）中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统低维损失景观分析难以捕捉复杂的拓扑特征，亟需更全面的高维分析工具来理解神经网络优化与泛化机制。

Method: 提出Landscaper工具包，融合Hessian-based子空间构造与拓扑数据分析；引入Saddle-Minimum Average Distance（SMAD）量化景观平滑度。

Result: SMAD能有效捕捉训练过程中的景观简化等过渡现象，且在化学性质预测等数据稀缺场景中可作为OOD泛化能力的指标。

Conclusion: Landscaper为损失景观分析提供了更鲁棒、可解释的高维视角，SMAD成为模型诊断与架构设计的新颖诊断指标。

Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper combines Hessian-based subspace construction with topological data analysis to reveal geometric structures such as basin hierarchy and connectivity. A key component is the Saddle-Minimum Average Distance (SMAD) for quantifying landscape smoothness. We demonstrate Landscaper's effectiveness across various architectures and tasks, including those involving pre-trained language models, showing that SMAD captures training transitions, such as landscape simplification, that conventional metrics miss. We also illustrate Landscaper's performance in challenging chemical property prediction tasks, where SMAD can serve as a metric for out-of-distribution generalization, offering valuable insights for model diagnostics and architecture design in data-scarce scientific machine learning scenarios.

</details>


### [442] [Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks](https://arxiv.org/abs/2602.07141)
*Isabel de la Higuera,Francisco Herrera,M. Victoria Velasco*

Main category: cs.LG

TL;DR: 本文提出了一种面向Banach空间的学习理论框架——特征再生核Banach空间（FRKBS），解决了非Hilbert空间（如带非二次范数的固定结构神经网络）中核方法失效的问题，建立了存在性结果与条件式表示定理，并统一了核方法与神经网络的函数空间视角。


<details>
  <summary>Details</summary>
Motivation: 传统再生核希尔伯特空间（RKHS）框架无法适用于许多现代学习模型（如带非二次范数的固定结构神经网络），因其依赖Hilbert结构；而一般Banach空间中，点评估泛函连续性不足以保证特征表示或核学习形式，亟需拓展理论基础。

Method: 引入‘特征再生核Banach空间’（FRKBS）概念，系统刻画其结构条件（如特征映射、核构造与表示定理可恢复的充要条件）；将监督学习建模为最小范数插值/正则化问题；进一步推广至向量值情形，并证明固定结构神经网络自然诱导此类空间。

Result: 建立了Banach空间中学习问题解的存在性结果与条件式表示定理；揭示了神经网络与核方法在FRKBS框架下的内在联系；明确了核学习原理向非Hilbert空间推广的精确条件。

Conclusion: FRKBS为非Hilbertian机器学习提供了严格的功能分析基础，弥合了核方法与神经网络的理论鸿沟，标志着从RKHS到更广义再生核空间的重要范式拓展。

Abstract: Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.

</details>


### [443] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文提出了BONSAI，一种默认感知的贝叶斯优化（BO）策略，旨在最小化对默认配置的偏离，同时保持优化性能；它通过剪枝低影响偏离并控制采集值损失来实现，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化不考虑对精心设计的默认配置的偏离程度，常导致对弱相关参数的无意义调整，增加实际部署时的审核负担，并难以区分重要与冗余变更。

Method: 提出BONSAI方法：一种默认感知的BO策略，通过剪枝低影响参数偏离，在保证采集函数值损失可控的前提下，减少非默认参数数量；兼容EI、GP-UCB等多种采集函数，并给出理论遗憾界分析。

Result: 在多个真实应用场景中，BONSAI显著减少了推荐配置中非默认参数的数量，同时保持与标准BO相当的优化性能，且几乎不影响运行时间。

Conclusion: BONSAI在兼顾优化效果的同时，提升了推荐配置的可解释性与实用性，是一种兼顾默认偏好与样本效率的新型贝叶斯优化框架。

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [444] [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*Zhiqi Bu,Shiyun Xu,Jialin Mao*

Main category: cs.LG

TL;DR: 本文研究深度学习中损失函数的凸性与Lipschitz连续性，发现训练初期后损失曲面迅速呈现弱凸特性，并据此建立可外推的学习率与损失缩放律。


<details>
  <summary>Details</summary>
Motivation: 深度学习损失函数非凸、优化动力学难以分析与控制，但实践中常表现出类凸行为；需从理论角度刻画该现象并实现对训练动态的精确调控。

Method: 通过实证分析验证深度学习训练中弱凸性快速出现，并利用凸性假设推导最后一轮迭代损失的上界，进而建立学习率与损失的缩放规律。

Result: 发现训练早期即进入弱凸阶段；推导出可预测的损失上界；构建的学习率与损失缩放律在训练时长上外推达80倍、模型规模上达70倍。

Conclusion: 凸性与Lipschitz连续性是刻画深度学习优化动态的有效近似工具，能支撑强泛化能力的训练动态控制与超参缩放。

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

</details>


### [445] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 本文揭示了当前agentic系统评估中pass@1指标存在显著方差问题，单次运行结果波动达2.2–6.0个百分点，可能导致将评估噪声误判为算法进步；作者建议采用多轮运行、统计功效分析及pass@k/pass^k等边界指标以提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有agentic系统评估普遍依赖单次运行的pass@1分数，但该做法缺乏对评估方差的充分认知，可能误导对算法改进真实性的判断。

Method: 在SWE-Bench-Verified基准上收集60,000条轨迹（覆盖3个模型、2种scaffold），分析单次pass@1的方差；进行token级轨迹分歧分析；提出多运行估计、统计功效分析和pass@k/pass^k等改进评估实践。

Result: 发现单次pass@1估计标准差超1.5个百分点，小幅度报告提升（2–3%）很可能落入噪声范围；轨迹在初始少量token内即发生分歧，并引发策略级差异。

Conclusion: 必须采用多轮独立运行、功效驱动的样本量设计及多指标边界刻画，才能可靠区分真实进展与统计噪声，保障agentic系统研究的科学性。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [446] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 本文提出了一种基于匹配的框架，通过自适应质心选择样本并迭代优化表征分布，结合双重鲁棒性和倾向得分匹配来缓解跨域数据异质性带来的偏差，尤其提升了零样本医疗异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 朴素的数据池化方法在跨域表示学习中会加剧分布不对称性，导致有偏估计，尤其在需要零样本泛化时问题更严重。

Method: 提出一种匹配框架：基于自适应质心选择样本，结合双重鲁棒性和倾向得分匹配筛选数据域，以消除混杂域的影响。

Result: 理论与实验表明，该方法在非对称元分布、非高斯及多模态真实场景下均优于朴素池化和均匀子采样，并显著提升零样本医疗异常检测性能。

Conclusion: 匹配策略比朴素池化和均匀采样更具鲁棒性，能有效应对数据异质性与分布不对称性，尤其适用于极端零样本任务如医疗异常检测。

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [447] [Mimetic Initialization of MLPs](https://arxiv.org/abs/2602.07156)
*Asher Trockman,J. Zico Kolter*

Main category: cs.LG

TL;DR: 本文提出了一种将mimetic初始化方法扩展到通道混合层（如MLP）的新技术，即为第一层赋予非零均值，从而加速小规模视觉任务的训练。


<details>
  <summary>Details</summary>
Motivation: 现有mimetic初始化仅应用于空间混合层（如卷积、自注意力、状态空间层），尚未拓展至通道混合层（如MLP），本文旨在填补这一空白。

Method: 提出一种极简的MLP mimetic初始化技术：为MLP的第一层权重设置非零均值。

Result: 该方法在CIFAR-10和ImageNet-1k等小规模视觉任务上加快了训练速度；效果虽弱于空间混合层的mimetic初始化，但可与之结合使用并产生额外增益。

Conclusion: mimetic初始化可成功拓展至通道混合层（MLP），验证了其更广泛适用性，且简单有效，具备实用价值。

Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.

</details>


### [448] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

TL;DR: 本文首次将大语言模型的上下文学习（ICL）能力引入电机前馈控制这一信号处理任务，提出一种分离信号表征与系统行为的Transformer架构，支持少样本微调和单样本ICL，在真实电机负载下仅需少量示例即可泛化并超越传统PI与物理模型方法。


<details>
  <summary>Details</summary>
Motivation: 经典PI和基于物理的控制方法在处理电机前馈控制中的非线性和复杂负载条件时表现不佳，而大语言模型展现出强大的上下文学习能力，但尚未应用于信号处理系统。

Method: 提出一种新型Transformer模型架构，将信号表示与系统行为解耦；在大规模合成线性与非线性系统数据上预训练；支持少样本微调和单样本上下文学习（ICL）。

Result: 模型能在多个真实电机负载配置下泛化；将未经调优的示例转化为高精度前馈预测；性能优于PI控制器和基于物理的前馈基线方法。

Conclusion: 上下文学习可有效弥合合成预训练与真实世界自适应之间的鸿沟，为物理系统的数据高效控制开辟新方向。

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [449] [Latent Target Score Matching, with an application to Simulation-Based Inference](https://arxiv.org/abs/2602.07189)
*Joohwan Ko,Tomas Geffner*

Main category: cs.LG

TL;DR: 本文提出Latent Target Score Matching (LTSM)，扩展Target Score Matching (TSM)以利用联合信号的分数监督边缘分数，在低噪声水平下实现低方差训练；结合Denoising Score Matching (DSM)可提升全噪声尺度鲁棒性，并在基于仿真的推断任务中显著改善方差、分数精度和采样质量。


<details>
  <summary>Details</summary>
Motivation: Denoising Score Matching (DSM)在低噪声水平下方差高；Target Score Matching (TSM)虽能缓解，但需干净数据分数，而许多实际场景因存在隐变量导致干净分数不可得，仅有联合信号可用。

Method: 提出Latent Target Score Matching (LTSM)，将TSM推广至利用联合信号的分数来监督边缘分数；并采用LTSM与DSM混合目标以兼顾低噪声性能与全尺度鲁棒性。

Result: 在多个基于仿真的推断任务中，LTSM一致降低了方差、提升了分数估计精度和生成样本质量。

Conclusion: LTSM为含隐变量场景下的扩散模型训练提供了更稳定、低方差的分数匹配方法，混合DSM进一步增强了其跨噪声尺度的实用性。

Abstract: Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.

</details>


### [450] [Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling](https://arxiv.org/abs/2602.07192)
*Xiaolong He,Haoyan Wei,Wei Hu,Henan Mao,C. T. Wu*

Main category: cs.LG

TL;DR: 本文系统评估了深度材料网络（DMNs）在离线训练与在线预测全流程中的性能，分析了训练策略对泛化能力与不确定性的影响，并提出了更高效的旋转无关IMN模型。


<details>
  <summary>Details</summary>
Motivation: 尽管DMNs在多尺度材料建模中展现出潜力，但其在完整离线-在线流程中的系统性性能评估仍不足，亟需厘清关键训练因素与模型性能间的关联。

Method: 开展综合性对比实验，考察初始化、批量大小、训练数据量及激活正则化等离线训练选择对在线泛化性能和不确定性的影响；同时提出并评估旋转无关的交互式材料网络（IMN）。

Result: 预测误差与方差随训练数据量增加而降低；初始化和批量大小显著影响性能；激活正则化有效控制网络复杂度与泛化能力；IMN相比原始DMN实现3.4–4.7倍离线训练加速，且在线精度与效率相当。

Conclusion: 明确了结构保持型材料网络中表达能力与计算效率的关键权衡，为实际多尺度材料建模中的DMN/IMN部署提供了实用指导。

Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.

</details>


### [451] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 本文提出了一种新的风险敏感型指数Actor-Critic（rsEAC）算法，用于在模型无关的深度强化学习中优化熵风险度量，解决了现有方法高方差和数值不稳定的问题，并在MuJoCo连续控制任务的风险变体上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵风险度量的风险敏感策略梯度方法存在高方差和数值不稳定问题，限制了其在复杂连续任务中的应用。

Method: 提出了风险敏感型指数Actor-Critic（rsEAC），一种离策略、模型无关的方法，通过新机制避免显式表示指数型价值函数及其梯度，从而稳定优化熵风险度量。

Result: rsEAC相比现有方法具有更稳定的数值更新，并在MuJoCo中具有挑战性的风险变体连续控制任务上可靠地学习到风险敏感策略。

Conclusion: 本文为熵风险度量下的策略梯度方法提供了完备的理论支撑，并通过rsEAC实现了在复杂连续任务中高效、稳定的风险敏感强化学习。

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [452] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构因果模型（SCM）中不可约集合的新方法，用于高效计算do-Shapley值，实现了线性时间复杂度，并降低了非参数识别所需的前提条件。


<details>
  <summary>Details</summary>
Motivation: do-Shapley值虽具解释力但计算复杂度高（指数级），且传统识别需大量干预效应，亟需更高效、更低识别负担的算法。

Method: 将do-Shapley值重新表述为SCM中不可约集合的函数；设计线性时间精确算法（依赖不可约集数量r）；提出可调查询预算的估计器，并证明其渐进最优性。

Result: 精确算法时间复杂度为O(r)；估计器在查询预算达r时可达机器精度，精度比先前方法高数个数量级；识别仅需d个单元素干预效应，而非全部子集。

Conclusion: 该工作显著提升了do-Shapley值的计算效率与可行性，同时放宽了因果识别要求，推动SCM在可解释因果推断中的实际应用。

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [453] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: 本文提出了一种新的在线学习 regret 度量——empirical Nash-value regret，并设计了一个无需调参的算法，在对手策略变化程度（由方差 C 和切换次数 L 刻画）未知的情况下，自动适应对手非平稳性，实现从 O(√K) 到 O(K^{2/3}) 的平滑插值式遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对手策略未知时无法兼顾强 regret 指标（如 external regret）与自适应性：Tian et al. (2021) 的 Nash-value regret 较弱且不随对手固定而提升；而传统 external regret 在完全未知对手下不可行。本文旨在统一二者，提出更强且可退化的 regret 定义，并实现自适应最优性能。

Method: 引入 empirical Nash-value regret 作为新 regret 指标；改进并重新分析 Mao et al. (2022) 的 epoch-based V-learning，获得含 epoch 参数 η 的中间界 O(ηC + √(K/η))；进一步设计自适应重启机制，动态选择 η 以响应对手策略变化（由 C 或 L 刻画），最终得到参数无关的复合 regret 界。

Result: 获得 O(min{√K + (CK)^{1/3}, √(LK)}) 的 regret 上界，其中 C 为对手策略方差、L 为策略切换次数；该界在对手固定时退化为 O(√K)（即 optimal external regret），在最坏情况下恢复为 O(K^{2/3})（优于或匹配先前 Nash-value regret），且全程无需先验知识。

Conclusion: empirical Nash-value regret 是一个更自然、更强且可退化的 regret 概念；所提自适应 epoch V-learning 算法首次实现了对对手非平稳性的无参数、平滑自适应，在两玩家非完全信息马尔可夫博弈中取得理论最优折衷性能。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [454] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

TL;DR: 本文提出双尺度Softmax损失（DSL），通过自适应调整每个训练样本的负样本权重和温度参数，以提升推荐系统在隐式反馈场景下的性能、鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统Softmax Loss在隐式反馈推荐中使用全局温度和均匀采样负样本，导致训练不稳定，因不同样本的负样本集合竞争强度和相关性差异大，单一温度难以适配所有情况。

Method: DSL在log-sum-exp基础上引入两个分支：1）基于难度和物品相似度对每个样本内的负样本重加权；2）根据构造的竞争者列表的竞争强度自适应估计每个样本的温度。

Result: 在多个基准数据集和模型主干上，DSL显著优于强基线，平均提升6.22%；在分布外流行度偏移下平均提升达9.31%；理论分析表明其具有分布鲁棒优化（DRO）性质，能改善模糊样本的鲁棒收益与KL偏差。

Conclusion: DSL通过从采样竞争本身推断有效损失锐度，在保持Softmax几何结构的同时重塑负样本和样本间的竞争分布，从而兼顾准确性与鲁棒性，是隐式推荐中更优的损失函数设计。

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [455] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

TL;DR: 本文提出一种自适应检索增强架构，让大语言模型在推理过程中主动决定是否查询外部知识库，实验表明其检索决策本身是一种重要的元认知信号，能提升模型的鲁棒性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因静态参数化知识导致的幻觉和在数学等专业领域推理能力不足的问题。

Method: 设计一种自适应检索增强架构，使LLM代理在推理过程中动态决定是否检索外部知识，并在GSM8K和MATH-500上与CoT及静态检索对比。

Result: 静态检索性能不如CoT；自适应检索中，未触发检索的推理路径反而优于CoT；检索频率随题目难度上升，表明检索决策具有元认知意义。

Conclusion: 模型主动选择不检索是其自信与能力的体现，检索决策本身是构建更可靠生成模型的关键原则。

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [456] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

TL;DR: 本文研究了神经组合优化（NCO）中训练的TSP求解器是否能作为可迁移编码器，用于物流场景中的两类NP难“假设分析”下游任务（节点移除敏感性和边禁止敏感性），发现其内部表征具有显著迁移能力，并随求解器性能提升而增强。


<details>
  <summary>Details</summary>
Motivation: 探索训练好的神经TSP求解器的内部表征能否迁移到其他优化相关的目标上，特别是现实物流中的“假设分析”型决策支持任务，而非仅限于构造最优路径。

Method: 训练多个基于注意力机制的TSP策略模型，提取其节点/边嵌入表示，并在这些表示上训练探针（probes）以完成两个NP难下游任务：节点移除敏感性识别和边禁止敏感性识别；同时结合几何特征进行集成提升。

Result: 在TSP100数据集上训练的模型，其探针在两项任务上均优于现有基线；集成几何特征后，最佳节点移除识别准确率达65%（基线58%），最差边识别达73%（基线67%）；且迁移性能随求解器质量与模型规模提升而提高。

Conclusion: 神经TSP求解器不仅可生成优质路径，其学习到的内部表征还可作为通用、可迁移的编码器，有效支持多种下游prescriptive决策任务，且更强的NCO模型产出更有用的表征。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [457] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 本文提出了一种名为CoLoRA的协作式低秩自适应方法，通过利用多用户间任务相似性，共享一个通用适配器并为每个用户定制个性化适配器，以缓解基础模型微调中标签数据稀缺的问题，并在理论与实验上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 缓解基础模型微调中高质量标注数据稀缺的问题，利用多下游用户间任务相似性实现协同学习。

Method: 提出Collaborative Low-Rank Adaptation（CoLoRA），包含一个共享适配器建模跨任务共性，以及多个个性化适配器适配各自任务；在异构线性回归设定下进行理论分析并给出真值恢复保证。

Result: 理论分析提供了可证明的真值恢复保证；NLP实验表明，相似任务联合训练能显著提升各任务单独性能。

Conclusion: CoLoRA是一种有效的参数高效协同微调框架，通过挖掘任务相似性提升个性化基础模型适应能力，兼顾效率与性能。

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [458] [The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network](https://arxiv.org/abs/2602.07219)
*Abhigyan Dutta,Itay Safran,Paul Valiant*

Main category: cs.LG

TL;DR: 本文研究了使用ReLU神经网络近似d维输入的中位数问题，提出了深度-宽度权衡方案，并构建了一个常数深度、线性宽度的网络，在单位超立方体均匀分布下实现指数级小的逼近误差；同时通过最大值到中位数的通用约化，突破了先前关于最大值函数所需深度下限的认知障碍。


<details>
  <summary>Details</summary>
Motivation: 突破先前关于最大值函数逼近中深度-宽度关系的认知局限，并解决中位数这一更复杂统计量的高效神经网络逼近问题。

Method: 提出多阶段迭代过程，逐步剔除非中心元素，同时保持围绕中位数的候选集；并建立从最大值到中位数的一般性约化方法。

Result: 实现了常数深度、线性宽度的ReLU网络对中位数的指数级精度逼近；其结果严格强于此前关于最大值函数的最佳已知逼近结果。

Conclusion: 中位数比最大值在神经网络逼近意义上更具可表示性，深度-宽度权衡存在本质差异；该工作修正了对ReLU网络表达能力边界的理解。

Abstract: We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over the unit hypercube. By further establishing a general reduction from the maximum to the median, our results break a barrier suggested by prior work on the maximum function, which indicated that linear width should require depth growing at least as $\log\log d$ to achieve comparable accuracy. Our construction relies on a multi-stage procedure that iteratively eliminates non-central elements while preserving a candidate set around the median. We overcome obstacles that do not arise for the maximum to yield approximation results that are strictly stronger than those previously known for the maximum itself.

</details>


### [459] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

TL;DR: 本文提出SpecAttn，一种基于验证引导稀疏注意力的自推测解码方法，通过在验证过程中识别关键KV条目并仅在起草时加载这些条目，从而提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型推理受限于KV缓存日益增长的内存需求；现有自推测解码方法依赖独立的KV选择算法，忽视了验证过程本身可提供KV重要性信息。

Method: 提出SpecAttn，利用验证过程自然产生的关键性信号来动态选择用于起草的KV条目，实现验证引导的稀疏注意力。

Result: SpecAttn相比基础自回归解码吞吐量提升2.81倍，相比当前最优基于稀疏性的自推测解码方法提升1.29倍。

Conclusion: 验证引导的KV选择机制能更高效地支持自推测解码，在不损失精度的前提下显著提升长上下文推理效率。

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [460] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

TL;DR: 本文提出了一种容错的模型性能评估框架，通过引入可调节的误差容忍度ε，统一考虑偏差与方差，在不同方差场景下实现对性能估计器的可靠评估，并设计了自动优化ε的算法。


<details>
  <summary>Details</summary>
Motivation: 现有模型服务验证方法在低方差场景下失效：RMSE混淆偏差与方差，p值检验又过于敏感；而模型即服务（MaaS）背景下亟需样本高效、可靠的性能估计器。

Method: 提出基于容忍度ε的容错评估框架，理论分析ε的校准原则，并设计自动优化ε的算法。

Result: 在真实数据集上的实验表明，该框架能提供全面且可操作的估计器行为洞察，提升低方差场景下的评估可靠性。

Conclusion: 所提框架克服了传统评估指标在低方差情形下的局限性，实现了兼顾偏差与方差、适应不同方差水平的鲁棒性能评估。

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [461] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 本文提出了一种受小脑启发的、推理时生效的残差控制框架，用于在不更新原始策略参数的前提下，实时修正部署后出现的故障，显著提升了机器人策略在多种扰动下的鲁棒性与恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境中部署的机器人策略常遭遇训练后故障，而重新训练、探索或系统辨识往往不可行，亟需无需修改基础策略的在线修正机制。

Method: 设计了一个受小脑结构与功能启发的残差控制框架，包含高维模式分离（固定特征扩展）、并行微区风格残差通路、以及具有快慢双时间尺度的兴奋/抑制型局部误差驱动可塑性；辅以保守的、性能驱动的元自适应机制调控残差权限与可塑性。

Result: 在MuJoCo基准（HalfCheetah-v5、Humanoid-v5）上，面对执行器、动力学和环境扰动，中度故障下性能提升达+66%和+53%；严重扰动下表现优雅退化，并能将持久有效的残差修正整合进策略参数以增强长期鲁棒性。

Conclusion: 该小脑式残差控制框架为冻结策略提供了高效、稳定、局部化的在线故障恢复能力，兼顾安全性与适应性，拓展了强化学习策略在真实场景中的实用性。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [462] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

TL;DR: 本文提出了ArcMark，一种基于编码理论的多比特水印方法，首次刻画了多比特水印的信息论容量，并在比特率和检测准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多种多比特水印方法沿用零比特水印的设计思路（如每token编码1比特），但其信息论容量（即在不改变平均下一词预测的前提下可嵌入并检测的最大比特数）尚未明确。

Method: 通过信息论分析推导出多比特水印信道的容量，并据此设计基于编码理论的ArcMark水印构造方法。

Result: 给出了首个严格意义上的多比特水印容量刻画；ArcMark在实际中相比其他多比特水印具有更高的比特率/Token和更高的检测准确率。

Conclusion: 语言模型水印本质上是一个信道编码问题，应采用系统性的编码理论方法进行设计。

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [463] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: 本文提出GRAPHITE框架，通过图变换直接提升异质图的同质性，从而改善GNN在异质图上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异质图上表现差，甚至不如MLP，根本原因在于未解决异质性的本质问题。

Method: 提出GRAPHITE框架，基于同质性定义，通过引入特征节点来促进具有相似特征节点间的同质信息传播，实现图结构的同质化变换。

Result: GRAPHITE显著提升了异质图的同质性，仅轻微增加图规模，并在多个挑战性异质图数据集上显著超越SOTA方法，在同质图上也保持竞争力。

Conclusion: 直接对图进行同质化变换是一种有效且新颖的解决异质图建模问题的范式，GRAPHITE为GNN在异质图上的应用提供了新思路。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [464] [Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing](https://arxiv.org/abs/2602.07258)
*Wanru Guo,Juan Xie,Binbin Wang,Weicong Chen,Xiaoyi Lu,Vipin Chaudhary,Curtis Tatsuoka*

Main category: cs.LG

TL;DR: 本文提出Dorfman筛选框架，一种多阶段基因组特征选择方法，通过数据驱动的变量分组、组内与组间假设检验及弹性网络优化，在正常和污染数据下均表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 高维基因组数据具有强组相关结构，传统特征选择方法常假设特征独立或依赖预定义通路，且对异常值和模型误设敏感。

Method: 提出Dorfman筛选框架：基于层次聚类构建数据驱动变量组，进行组间与组内假设检验，并用弹性网络或自适应弹性网络精炼选择；鲁棒变体采用OGK协方差估计、秩相关和Huber加权回归处理污染与非正态数据。

Result: 模拟显示Dorfman-Sparse-Adaptive-EN在正常数据下最优，Robust-OGK-Dorfman-Adaptive-EN在污染数据下显著优于经典方法；应用于NSCLC曲美替尼响应数据时，鲁棒Dorfman方法预测误差最低且更有效富集临床相关基因。

Conclusion: Dorfman框架提供高效鲁棒的基因组特征选择方案，其中Robust-OGK-Dorfman-Adaptive-EN在理想与污染条件下均表现强劲，可扩展至超高维场景，适用于现代基因组生物标志物发现。

Abstract: Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.
  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.
  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.
  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.

</details>


### [465] [tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models](https://arxiv.org/abs/2602.07263)
*Kevin Li,Dibyadeep Saha,Avni Kanodia,Fan Lai*

Main category: cs.LG

TL;DR: 本文提出了tLoRA框架，通过融合共享基础模型的LoRA适配器、设计融合LoRA核以及在线调度策略，在训练阶段高效地批量执行多个异构LoRA任务，显著提升吞吐量、缩短完成时间和提高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 随着LoRA成为微调大语言模型的标准方法，共享集群中常需并发训练多个LoRA任务；但现有方法在训练时难以高效共置异构LoRA适配器，易导致同步阻塞、通信开销增大和单任务变慢等问题。

Method: tLoRA构建弹性共享超模型以融合同基座的LoRA适配器；设计支持自适应低秩计算分块与rank感知纳米批次的融合LoRA核；引入在线残差容量感知调度器动态分组任务。

Result: 在真实集群轨迹评估中，tLoRA将训练吞吐量提升1.2–1.8倍、作业完成时间缩短2.3–5.4倍、GPU利用率提升37%。

Conclusion: tLoRA为多LoRA任务的协同训练提供了系统化高效解决方案，兼顾性能提升与资源利用率优化，推动LoRA在生产环境中的规模化应用。

Abstract: As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.

</details>


### [466] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: 本文提出XShare方法，通过建模批处理感知的专家选择为模块化优化问题，设计高效贪心算法，在不重训练前提下动态适配每一批请求，显著降低专家激活率和GPU峰值负载，并提升推测解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 架构在生产推理中因请求批处理和推测解码导致专家激活激增，削弱其效率优势。

Method: 将批感知专家选择建模为模块化优化问题，设计适用于不同部署场景的高效贪心算法；XShare方法无需重训练，动态最大化所选专家的总门控分数。

Result: 在标准批处理下专家激活减少达30%；专家并行部署中GPU峰值负载降低达3倍；推测解码中通过分层、相关性感知的专家选择实现最高14%的吞吐量提升（即使批次内请求来自异构数据集）。

Conclusion: XShare有效缓解MoE模型在实际部署中的专家冗余问题，兼顾效率与通用性，无需模型修改或重训练。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [467] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

TL;DR: 本文提出了一种用于VR/AR中视口自适应传输的两层混合反馈在线学习方法AdaPort，利用可回溯计算的预测反馈（全信息）与实时传输反馈（带臂）联合优化选块策略，在理论上下界匹配且实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作将VR/AR中的视口自适应内容选择建模为双层带臂赌博机问题，但忽略了预测反馈可在用户姿态观测后回溯计算所有候选块——即实际为全信息反馈，而非纯带臂反馈，从而导致建模失准和学习效率低下。

Method: 提出两层混合反馈模型（融合全信息预测反馈与带臂式传输反馈），将问题建模为在线学习任务；设计自适应算法AdaPort，理论推导其与实例相关的悔值上下界，并实现渐近最优性。

Result: AdaPort在真实轨迹驱动仿真中持续优于SOTA基线方法；理论证明其悔值上界与所导出的下界渐近匹配，验证了算法最优性。

Conclusion: 利用预测反馈的全信息特性构建混合反馈模型，能显著提升边缘端VR/AR内容自适应传输的学习效率与性能，AdaPort为此提供了理论完备且实用高效的解决方案。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [468] [Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation](https://arxiv.org/abs/2602.07278)
*Sai Vamsi Alisetti*

Main category: cs.LG

TL;DR: 本文提出Laplacian-LoRA，一种基于低秩谱自适应的简单可解释方法，通过在拉普拉斯传播算子上添加可学习的谱锚定校正来缓解GCN中的过平滑问题，延迟表征坍缩，提升有效深度。


<details>
  <summary>Details</summary>
Motivation: 过平滑是深层图卷积网络（GCN）的根本性限制，但其谱层面的根本原因常被忽视；现有方法多依赖架构修改或残差机制，缺乏对谱机制的显式建模。

Method: 提出Laplacian-LoRA：不改变消息传递结构，而是在固定拉普拉斯传播算子上引入可学习、谱锚定的低秩修正项，选择性削弱谱收缩，同时保持稳定性与低通归纳偏置。

Result: 在多个基准数据集和不同深度下，Laplacian-LoRA显著延迟过平滑发生，将GCN有效深度提升至原有两倍；嵌入方差诊断与学习到的谱分析证实其修正平滑、有界且行为良好。

Conclusion: 过平滑本质上是深度相关的谱现象，可通过轻量级、低秩的图传播算子适配进行系统性延缓；Laplacian-LoRA为理解与控制GCN谱动力学提供了新视角。

Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.

</details>


### [469] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

TL;DR: 本文提出了VertCoHiRF，一种完全去中心化的垂直联邦聚类框架，通过异构视图间的结构一致性实现多视角数据的协同聚类，无需交换特征相关统计量，仅依赖样本标识符、聚类标签和序数排序进行通信，保障隐私并支持异构本地聚类方法。


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习聚类方法大多局限于分布式k-means变体，依赖中心协调或交换特征相关数值统计，且在异构视图或对抗行为下鲁棒性差。

Method: VertCoHiRF采用去中心化方式，各代理基于本地特征空间独立运行适配的基聚类算法，再通过标识符层级的共识机制（基于去中心化序数排序选择代表性medoid）逐步构建共享的层次聚类结构（CFH）。

Result: 该方法仅需传输样本标识符、聚类标签和序数排名，天然满足隐私保护；支持重叠特征划分与异构本地聚类算法；生成可解释的Cluster Fusion Hierarchy（CFH）；实验表明其在垂直联邦场景下具有竞争力的聚类性能。

Conclusion: VertCoHiRF为垂直联邦聚类提供了一种鲁棒、隐私友好、可解释且去中心化的新型范式，突破了传统方法对中心协调和特征统计交换的依赖。

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [470] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

TL;DR: 本文研究在满足充分性（sufficiency）约束下的最优二元分类问题，提出了一种基于群体校准分数的精确后处理算法，能实现最优的正预测值（PPV）与假遗漏率（FOR）权衡，并进一步在满足充分性的前提下最小化对分离性（separation）的偏离。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一阈值的二元分类方法虽在无约束下贝叶斯最优，但在公平性约束下往往失效；尤其在充分性（如预测奇偶性）约束下，即使使用完美校准的分数（如真实概率），标准阈值法仍不满足要求，亟需专门的优化方法。

Method: 基于有限集上群体校准分数的假设，通过几何刻画可行PPV-FOR对的区域，推导出一种仅依赖分数和组别信息的简单随机化后处理算法；并进一步构建兼顾充分性与分离性的折中优化目标。

Result: 给出了充分性约束下最优二元分类器的精确解；提供了PPV与FOR可行域的几何表征；设计了高效可实现的后处理算法；并在不兼容约束下实现了对分离性偏差的最小化，实验表明其性能接近理论最优。

Conclusion: 充分性约束下的最优分类不能通过简单阈值实现，但可通过所提几何驱动的后处理算法精确达成；该框架统一处理了充分性及与分离性的权衡，为公平机器学习提供了新工具。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [471] [Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations](https://arxiv.org/abs/2602.07320)
*Philip Jacobson,Ben Feinberg,Suhas Kumar,Sapan Agarwal,T. Patrick Xiao,Christopher Bennett*

Main category: cs.LG

TL;DR: 本文研究了神经网络训练中对权重扰动鲁棒性的优化方法，比较了SAM和RWP两种策略在不同噪声强度下的泛化与优化性能，并提出动态调整扰动强度以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索使模型对内部权重扰动具有鲁棒性的训练方法，以支持低功耗硬件平台需求，并理解平坦极小值与泛化能力的关系。

Method: 采用尖锐感知最小化（SAM）和随机权重扰动（RWP）两种方法，在理论和实验上分析其在不同幅度权重噪声下的表现；引入动态扰动强度调整策略。

Result: 发现过正则化的RWP在噪声鲁棒泛化上最优；SAM在小噪声下优于RWP，但在大噪声下因损失曲面不均匀导致梯度消失而性能下降；动态调整扰动强度可提升优化效果。

Conclusion: 噪声鲁棒性需兼顾优化目标设计与扰动强度自适应，RWP更适合强噪声场景，SAM适用于弱噪声，二者性能差异源于损失曲面几何特性。

Abstract: Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.

</details>


### [472] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

TL;DR: 本文提出ShaPO框架，从优化几何角度提升大语言模型安全对齐的鲁棒性，通过选择性控制关键参数子空间的几何特性，避免过正则化，在多种噪声偏好设置下显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒对齐方法多关注对齐数据的不确定性，却忽视了基于偏好的目标函数在优化过程中引发的脆弱性；作者认为仅靠数据层面的方法无法解决鲁棒性失败问题。

Method: 提出ShaPO（Shape-aware Preference Optimization）框架，基于优化几何视角，在对齐关键参数子空间上施加选择性几何控制，避免全局统一约束；具体包括token-level ShaPO（稳定似然代理优化）和reward-level ShaPO（在噪声监督下保证奖励一致性优化）。

Result: 在多个安全基准和噪声偏好设置下，ShaPO持续优于主流偏好优化方法；且能与数据鲁棒目标良好结合，带来额外增益，验证了优化几何视角的有效性。

Conclusion: 优化几何是影响LLM安全对齐鲁棒性的关键因素，ShaPO通过几何感知的偏好优化提供了更本质、更可组合的鲁棒性提升路径。

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [473] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 本文提出了一种结合AR远程示教与对比学习增强强化学习的两阶段框架，用于提升灵巧臂手系统的可扩展操作学习效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧机器人臂手系统中操作任务学习的可扩展性问题，同时提升数据采集效率与策略鲁棒性。

Method: 两阶段方法：第一阶段基于AR远程人机交互数据进行行为克隆（BC）预训练；第二阶段引入对比学习增强的强化学习（RL），设计投影头加速学习，并采用事件驱动的增强奖励提升安全性。

Result: 在PyBullet仿真与真实实验中，相比PPO和SAC，该方法显著提升推理速度与任务成功率；消融实验证实对比学习有效缓解策略坍塌。

Conclusion: 所提统一框架在效率、性能与安全性方面均优于经典RL方法，适用于可扩展的灵巧操作学习。

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [474] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 本文提出NeVA框架，通过神经元级编辑实现大语言模型的价值对齐，有效减少价值泄露，提升对齐可控性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的价值对齐方法存在可控性差的问题，即在引导目标价值时会无意激活其他非目标价值，本文旨在解决该问题。

Method: 提出NeVA框架，基于Schwartz价值理论定义‘价值泄露’概念并设计归一化度量；在推理时识别稀疏的价值相关神经元并进行激活编辑，无需参数更新或重训练。

Result: 实验表明NeVA在增强目标价值对齐的同时，显著降低平均价值泄露，且残余影响主要集中于语义相近的价值类别，通用能力下降更小。

Conclusion: NeVA提供了一种更可控、更可解释的大语言模型价值对齐机制。

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [475] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

TL;DR: 本文提出UTOPIA方法，通过解耦特征优化通道，在表格数据中构建约束感知的主导捷径，实现认证不可学习性，有效防止未经授权的模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有不可学习样本（UE）方法难以迁移到表格数据，因其混合数值与分类约束且显著性稀疏，而金融和医疗等领域的表格数据高度敏感，亟需有效保护机制。

Method: 基于谱主导条件，提出UTOPIA方法：利用特征冗余性，将优化解耦为两个通道——高显著性特征用于语义混淆，低显著性冗余特征用于嵌入超相关捷径，从而生成约束感知的主导捷径并保持表格数据有效性。

Result: 在多个表格数据集和模型上的实验表明，UTOPIA可使未授权训练性能趋近随机水平，显著优于强基线方法，并具备良好的跨架构迁移能力。

Conclusion: UTOPIA首次在理论保障下实现了表格数据的认证不可学习性，为敏感表格数据提供了实用、鲁棒且通用的防窃取保护方案。

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [476] [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*Jianchuan Yang,Xi Chen,Jidong Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种数值一致的FEM信息超图神经网络（FHGNN），将有限元计算直接嵌入消息传递层，实现纯物理驱动、无需标注数据的训练，在3D弹塑性问题中展现出高精度与高效性，可扩展至大规模非线性固体力学问题。


<details>
  <summary>Details</summary>
Motivation: 受离散物理损失和HiDeNN结构启发，旨在将有限元法（FEM）的物理一致性与图神经网络（GNN）的灵活性结合，解决现有PINN在复杂力学问题中精度与效率不足的问题。

Method: 构建FEM-Informed Hypergraph Neural Network（FHGNN），在超图节点和高斯点嵌入FEM计算，采用基于变分原理的高效物理损失函数，并利用GPU并行张量运算和离散表示进行训练。

Result: 在含各向同性/运动硬化循环加载的3D基准问题上，FHGNN显著优于近期主流PINN方法，精度更高、训练更高效；对大规模弹塑性问题具有良好可扩展性，运行速度可媲美甚至超过多核FEM求解器。

Conclusion: FHGNN为非线性固体力学提供了可扩展、物理嵌入的机器学习新范式，奠定了物理信息深度学习在高性能计算力学中的应用基础。

Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.

</details>


### [477] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

TL;DR: 本文提出了新的差分隐私算法，用于PAC和在线模型中学习决策列表和大间隔半空间，在样本复杂度和错误界上接近最优非隐私算法性能。


<details>
  <summary>Details</summary>
Motivation: 在保证差分隐私的前提下，提升经典机器学习问题（如决策列表和大间隔半空间学习）的学习效率与理论保证。

Method: 设计了适用于PAC模型的高效差分隐私学习算法（针对决策列表），以及在线模型中基于Winnow思想的隐私化半空间学习算法。

Result: PAC模型中决策列表学习具有最小样本开销；在线模型中半空间学习错误界为维度的多对数与间隔的逆多项式；并实现了在线决策列表学习，性能接近最优非隐私方法。

Conclusion: 所提算法在差分隐私约束下，首次在多个经典学习问题上达到与非隐私算法可比的理论性能，推动了隐私机器学习的实用性与理论边界。

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [478] [Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent](https://arxiv.org/abs/2602.07378)
*Shota Imai,Sota Nishiyama,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 本文研究了无限宽度两层神经网络在大批次随机梯度更新下的特征遗忘现象，通过快慢动力学分析揭示其机制与条件，并给出理论支撑与标度律。


<details>
  <summary>Details</summary>
Motivation: 理解梯度训练中神经网络动力学的非平凡结构是理论机器学习的核心挑战之一，而特征遗忘（即网络在长期训练中逐步丢失先前学到的特征）现象近年来受到关注。

Method: 采用张量程序（Tensor Programs）和奇异摄动理论，在无限宽度极限下对两层神经网络进行建模，推导出具有不同时间尺度的微分方程，利用快慢动力学分析特征遗忘的发生机制与条件。

Result: 发现特征遗忘由数据中主非线性项强度驱动，且第二层权重的初始尺度可缓解该现象；并通过数值实验验证了理论结果，推导出特征遗忘的标度律。

Conclusion: 特征遗忘是快慢动力学共同作用的结果，其发生取决于慢流形上的流方向；本工作为理解深度学习中的动态演化提供了新的理论视角与定量刻画。

Abstract: The dynamics of gradient-based training in neural networks often exhibit nontrivial structures; hence, understanding them remains a central challenge in theoretical machine learning. In particular, a concept of feature unlearning, in which a neural network progressively loses previously learned features over long training, has gained attention. In this study, we consider the infinite-width limit of a two-layer neural network updated with a large-batch stochastic gradient, then derive differential equations with different time scales, revealing the mechanism and conditions for feature unlearning to occur. Specifically, we utilize the fast-slow dynamics: while an alignment of first-layer weights develops rapidly, the second-layer weights develop slowly. The direction of a flow on a critical manifold, determined by the slow dynamics, decides whether feature unlearning occurs. We give numerical validation of the result, and derive theoretical grounding and scaling laws of the feature unlearning. Our results yield the following insights: (i) the strength of the primary nonlinear term in data induces the feature unlearning, and (ii) an initial scale of the second-layer weights mitigates the feature unlearning. Technically, our analysis utilizes Tensor Programs and the singular perturbation theory.

</details>


### [479] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: Sketch&Walk Attention is a training-free sparse attention method using lightweight sketches and deterministic walk to reduce computational and memory cost in long-context LLM inference, achieving high speedup with near-lossless accuracy.


<details>
  <summary>Details</summary>
Motivation: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases.

Method: Sketch&Walk Attention uses Hadamard sketching for inexpensive approximations of attention scores and aggregates these estimates across layers via a walk mechanism to capture attention influence beyond direct token interactions. The accumulated walk scores select top-k attention blocks for dynamic sparsity, supported by custom sparse attention kernels.

Result: Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

Conclusion: Sketch&Walk Attention provides a single, training-free, dynamic sparsity algorithm applicable uniformly to both prefill and decode phases, significantly improving inference efficiency without sacrificing accuracy.

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [480] [BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks](https://arxiv.org/abs/2602.07400)
*Simon Bührer,Andreas Plesner,Aczel Till,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文提出BitLogic框架，通过可微分查找表（LUT）节点替代传统乘加运算，实现端到端可训练的FPGA原生神经网络，兼顾高精度与极低硬件开销。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络推理的能量和延迟成本日益由部署主导而非训练，推动对算术密集型模型的硬件专用替代方案；现有FPGA神经网络方法碎片化、难以比较。

Method: 提出BitLogic——一个全梯度驱动、端到端可训练的FPGA原生神经网络框架，以可微分LUT节点为核心，直接映射FPGA查找表原语，并支持模块化API、学习编码器、硬件感知输出头及多种边界一致的LUT松弛方法；配备自动RTL导出流程，将PyTorch模型转为等效可综合HDL。

Result: 在CIFAR-10上达72.3%测试精度，仅用<0.3M逻辑门；单样本推理延迟<20 ns，且纯使用LUT资源；在多个视觉基准和异构硬件平台验证了精度竞争力与FPGA效率显著提升。

Conclusion: BitLogic证明了基于LUT的全可微、端到端训练路径在FPGA上可行且高效，为低开销、高性能边缘AI推理提供了新范式。

Abstract: The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.

</details>


### [481] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出了一种基于无限高斯过程（∞-GP）的贝叶斯优化新算法，首次在一般奖励设定下实现无悔保证，适用于具有Lipschitz连续性和各类测量噪声的奖励函数，并结合Thompson采样与新型后悔分析框架，在非平稳、重尾等复杂奖励场景中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法在奖励模型不确定（如非平稳、重尾、噪声复杂）时缺乏理论保证和鲁棒性，亟需一种能适应广义奖励分布且具备无悔性质的新方法。

Method: 提出无限高斯过程（∞-GP）作为先验建模奖励分布空间的贝叶斯非参数模型，结合Thompson采样进行决策，并建立基于总变差距离的新TS后悔分析框架；采用截断Gibbs采样保障计算可扩展性。

Result: 在理论层面实现了对Lipschitz连续目标函数和广义噪声下的无悔保证；在实验层面展现出对非平稳、重尾及病态奖励的优越性能，达到当前最优水平。

Conclusion: ∞-GP与TS的结合为奖励模型不确定下的贝叶斯优化提供了兼具理论严谨性与实践有效性的新范式，显著拓展了BO的适用边界。

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [482] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: 本文提出ChiDeK框架，通过引入手性行列式核和交叉注意力机制，统一建模中心与轴向手性，显著提升手性分子表征学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子机器学习模型难以有效捕捉手性，尤其对轴向手性等复杂形式建模不足，传统表示缺乏显式立体化学编码。

Method: 提出手性行列式核以编码SE(3)-不变的手性矩阵，并利用交叉注意力将局部手性中心信息融入全局分子表征，实现中心与轴向手性的联合编码。

Result: 在R/S构型分类、对映体排序、ECD谱预测和光学旋转预测四项任务中均超越SOTA；轴向手性任务平均准确率提升超7%。

Conclusion: ChiDeK为手性分子表征学习提供了系统化、可扩展的解决方案，首次实现了对手性类型（中心/轴向）的统一显式建模。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [483] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 本文研究对抗性多臂赌博机中的静态遗憾和动态遗憾同时最优的问题，证明了在无感知对手下存在同时达到两种遗憾最优的算法，而在自适应对手下则不可能；提出了一种利用负静态遗憾补偿探索开销并结合Blackwell可达性的新算法。


<details>
  <summary>Details</summary>
Motivation: 现有算法只能单独实现静态或动态遗憾最优，尚无算法能同时达到两者最优；本文旨在探索在何种条件下可实现二者同时最优。

Method: 扩展了Marinov和Zimmert [2021] 的不可能性结果至确定性损失情形，并设计了一种新算法：利用负静态遗憾补偿探索代价，结合Blackwell approachability技术联合控制静态与动态遗憾。

Result: 证明了在无感知对手且损失确定时，静态与动态遗憾可同时达到最优；揭示了自适应与无感知对手在多遗憾基准下的根本差异；提出了新的赌博机模型选择方法。

Conclusion: 静态与动态遗憾的同时最优性取决于对手类型：对无感知对手可行，对自适应对手不可行；该发现深化了对切换基准下最优遗憾问题的理解。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [484] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文通过引入广义重尾噪声模型，理论分析了SignSGD、Lion、Muon等符号优化器在训练大语言模型时的收敛性，解释了其为何优于AdamW，并通过预训练实验验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 尽管符号优化算法（如Lion、Muon）在大语言模型训练中表现出比AdamW更好的经验性能，但其理论优势尚不明确；本文旨在从重尾梯度噪声这一常见于语言建模的实际现象出发，弥合理论与实践之间的鸿沟。

Method: 提出一种能更准确刻画LLM梯度噪声的广义重尾噪声条件；在此噪声假设下，对SignSGD、Lion、Muon和Muonlight等符号优化器在广义光滑函数类上建立紧致收敛速率界，并首次对矩阵优化下的重尾随机性开展严格分析。

Result: 在广义重尾噪声下，SignSGD和Lion的收敛率优于或匹配现有最优界；首次给出Muon/Muonlight在重尾随机性下的理论分析；实证表明所提噪声模型与LLM预训练中的实际梯度噪声高度一致。

Conclusion: 符号优化器天然适合处理重尾梯度噪声，这为其在大语言模型训练中超越方差自适应方法（如AdamW）提供了坚实的理论支撑。

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [485] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 本文提出Brep2Shape，一种自监督预训练方法，通过几何感知任务将抽象的边界表示（B-rep）与直观的形状表示对齐，采用双流Transformer和拓扑注意力机制提升几何与拓扑建模能力，在下游任务中实现SOTA性能和更快收敛。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在处理B-rep模型时存在表示鸿沟：连续方法精度高但视觉抽象，离散方法直观但几何精度低。

Method: 提出Brep2Shape自监督预训练方法，设计从Bézier控制点预测稠密空间点的几何感知任务；构建Dual Transformer主干网络，分别编码曲面与曲线token，并引入拓扑注意力建模二者间依赖关系以保持拓扑一致性。

Result: 在多个下游任务上实现SOTA精度，具备显著可扩展性与更快收敛速度。

Conclusion: Brep2Shape有效弥合了B-rep表示中的抽象性与直观性之间的鸿沟，提升了CAD模型的几何理解与拓扑一致性建模能力。

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [486] [Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis](https://arxiv.org/abs/2602.07440)
*Cédric Jung,Shirin Salehi,Anke Schmeink*

Main category: cs.LG

TL;DR: 本文提出六种聚合结构来解决主动学习中的探索-利用困境，平衡准确性与能耗，在减少样本量和计算成本的同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习中代表性和不确定性采样方法存在探索-利用权衡，且面临批处理低效和冷启动等问题；同时需兼顾模型精度与训练能耗，推动可持续AI发展。

Method: 实现并评估多种前沿采集函数，提出六种聚合结构（串行、并行、混合、自适应反馈、随机探索、退火探索）以融合不同采集策略，并在多个模型和数据集上进行实验验证。

Result: 聚合采集函数缓解了批模式低效和冷启动问题；交替使用BALD与BADGE等方法表现稳健；K-Centers后接BALD的串行方式可减少12%样本量且采集成本降低近一半。

Conclusion: 所提聚合结构能有效平衡准确性与能量消耗，提升主动学习的数据与能源效率，为构建可持续AI提供新路径。

Abstract: Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.

</details>


### [487] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 本文提出了一种名为 proximal action replacement (PAR) 的新方法，用于解决行为克隆（BC）正则化在离线强化学习中导致的性能上限问题，通过逐步替换低价值动作为高价值动作来提升策略性能。


<details>
  <summary>Details</summary>
Motivation: BC正则化的离线RL方法虽能缓解分布外动作偏差，但当数据集中动作本身次优时，盲目模仿会限制策略探索高价值区域，形成性能天花板。

Method: 提出proximal action replacement (PAR)，一种即插即用的训练样本替换机制，逐步用稳定actor生成的高价值动作替换原始数据中的低价值动作，以扩展动作探索空间并降低低质量数据影响。

Result: PAR在多个离线RL基准测试中一致提升性能，并与TD3+BC结合时接近当前最优水平。

Conclusion: PAR有效突破了BC正则化带来的性能瓶颈，是一种通用、兼容性强且实用的离线RL改进技术。

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [488] [Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles](https://arxiv.org/abs/2602.07453)
*Namrita Varshney,Ashutosh Gupta,Arhaan Ahmad,Tanay V. Tayal,S. Akshay*

Main category: cs.LG

TL;DR: 本文提出了一种数据感知的特征敏感性分析框架，用于检测决策树集成模型对特定特征子集（如受保护属性）的敏感性，并确保生成的敏感样例贴近训练分布，从而提升可解释性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的敏感样例常远离训练分布，缺乏可解释性和实际价值；需在关键领域中评估树集成模型的鲁棒性与公平性。

Method: 结合混合整数线性规划（MILP）与SMT编码，构建数据感知的敏感性验证与搜索框架，并提出针对单类和多类树集成的MILP优化技术。

Result: 证明了即使深度为1的树也具NP难敏感性验证问题；显著加速敏感性验证；首次支持多类树集成；在800棵树、深度8的大规模集成上验证了可扩展性与优越性。

Conclusion: 该框架为高风险场景下树模型的可靠性与公平性分析提供了实用、可扩展且可解释的基础工具。

Abstract: Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.

</details>


### [489] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 本文提出MaCa（Matryoshka Calibration），一种长度感知的后训练量化校准方法，通过多尺度序列长度建模和独立序列正则化提升Hessian估计质量，从而在低比特量化下显著提升LLM精度。


<details>
  <summary>Details</summary>
Motivation: 传统PTQ使用固定长度随机校准序列，忽视LLM输入长度可变性；输入长度影响激活分布与Hessian估计的权重重要性，导致校准不充分。

Method: MaCa方法包含两部分：(i) 在Hessian估计中融入多尺度序列长度信息；(ii) 将每条序列视为独立样本并进行正则化，以获得更稳定、更具判别力的Hessian矩阵。

Result: 在Qwen3、Gemma3、LLaMA3等前沿LLM上实验表明，MaCa在2~4比特量化下持续提升准确率，且轻量、即插即用，兼容现有PTQ框架。

Conclusion: MaCa首次系统揭示了多尺度校准对LLM量化的重要性，为PTQ中校准数据设计提供了新范式，显著提升低比特量化性能。

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [490] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

TL;DR: 本文提出了多臂老虎机（MAB）算法中“分配变异性”这一新性能指标，并建立了其与经典遗憾（regret）之间的基本权衡关系：R_T · S_T = Ω(T^{3/2})；证明了该下界本质上紧，并通过可调UCB-f算法实现Pareto前沿上的任意权衡点。


<details>
  <summary>Details</summary>
Motivation: 现代应用（如学习增强型平台运营和后老虎机统计推断）中，MAB算法的臂选择分配存在巨大变异性，损害系统稳定性与后续分析可靠性，亟需新指标刻画并优化分配一致性。

Method: 定义分配变异性S_T为各臂被选择次数的标准差的最大值；通过理论分析建立S_T与最坏情况遗憾R_T之间的渐近下界；构造广义UCB-f算法并证明其可达性；结合平台运营与统计推断场景讨论实际意义。

Result: 证明了R_T · S_T = Ω(T^{3/2})的基本权衡下界；指出任何最小最大遗憾最优算法必有S_T = Θ(T)，而任何次线性遗憾算法必有S_T = ω(√T)；UCB-f算法可实现R_T · S_T = \tildeΘ(T^{3/2})的Pareto前沿。

Conclusion: 分配变异性是MAB中不可忽视的关键维度，与遗憾存在根本性权衡；UCB-f提供了灵活调控二者平衡的实用方案；结果对平台公平性、资源调度鲁棒性及因果推断有效性具有重要启示。

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [491] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于二分图Transformer的scRNA-seq聚类模型BGFormer，通过引入可学习的锚点token和二分图注意力机制，将计算复杂度从O(n²)降至O(n)，显著提升了大规模单细胞数据聚类的效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的scRNA-seq聚类方法（如图Transformer）将每个细胞视为token，导致O(n²)的时间和空间复杂度，难以扩展到大规模数据集。

Method: 提出BGFormer模型：引入一组可学习的锚点token作为全量细胞的共享参考点，并设计二分图注意力机制建模细胞与锚点间的相似性，从而在嵌入空间中拉近同类细胞距离。

Result: BGFormer实现了关于细胞数量n的线性计算复杂度，在多个大规模scRNA-seq数据集上验证了其有效性与可扩展性。

Conclusion: BGFormer通过结构创新有效解决了Transformer在scRNA-seq聚类中的可扩展性瓶颈，为大规模单细胞数据分析提供了高效、实用的新范式。

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [492] [AI-Driven Predictive Modelling for Groundwater Salinization in Israel](https://arxiv.org/abs/2602.07478)
*Laxmi Pandey,Ariel Meroz,Ben Cheng,Ankita Manekar,Abhijit Mukherjee,Meirav Cohen,Adway Mitra*

Main category: cs.LG

TL;DR: 本研究通过集成多种机器学习模型（如随机森林、XGBoost、LSTM等）并结合可解释AI（SHAP）、全局敏感性分析和双重机器学习因果分析，系统识别了影响以色列地下水盐度的关键气象、地质和人为驱动因子，特别强调处理废水（TWW）在特定水文气候环境下的关键作用。


<details>
  <summary>Details</summary>
Motivation: 地下水盐度升高和污染是全球性问题，亟需全面理解其成因机制并识别关键驱动因素，以支持水资源保护与管理。

Method: 整合多源协变量数据，构建包括RF、XGBoost、神经网络、LSTM、CNN和线性回归在内的预测模型框架；采用递归特征消除（RFE）、全局敏感性分析（GSA）和SHAP可解释性方法评估变量重要性；并利用双重机器学习进行因果分析。

Result: 识别出降水、温度、距河流距离、距咸水体距离、地形湿度指数（TWI）、海岸线距离、农业用地面积及处理废水（TWW）等为影响以色列地下水盐度的关键驱动因子；TWW被证实为情境依赖但至关重要的人为驱动因子。

Conclusion: 该方法深化了国家尺度地下水盐渍化机制认知，降低了AI模型不确定性，凸显需因地制宜制定盐度治理策略。

Abstract: Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.

</details>


### [493] [ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations](https://arxiv.org/abs/2602.07479)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出ODELoRA，一种基于常微分方程（ODE）的LoRA训练新方法，通过在平衡流形上模拟全量微调的梯度流，实现更优的参数耦合优化；理论证明其在线性收敛性与矩阵感知任务中表现优异，并在物理信息神经网络训练中展现出更强稳定性。


<details>
  <summary>Details</summary>
Motivation: 经典LoRA将低秩因子矩阵独立优化，忽视其内在结构，导致理论与实践上的次优性。

Method: 构建模拟全量微调梯度流的连续时间ODE动力学，在平衡流形上联合优化LoRA因子矩阵；采用Euler和Runge–Kutta等可靠时序离散化方案实现数值求解。

Result: 理论证明在强凸目标下特定离散格式具有线性收敛性，扩展至矩阵感知场景；验证了稳定特征学习能力；实验表明在矩阵感知和物理信息神经网络训练中优于现有基线，尤其提升训练稳定性。

Conclusion: ODELoRA为LoRA训练提供了统一、可解释的ODE视角，兼具理论保证与实际性能优势，尤其适用于对稳定性要求高的多尺度深度学习任务。

Abstract: Low-rank adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning method in deep transfer learning, due to its reduced number of trainable parameters and lower memory requirements enabled by Burer-Monteiro factorization on adaptation matrices. However, classical LoRA training methods treat the low-rank factor matrices individually and optimize them using standard gradient-based algorithms. Such decoupled optimization schemes are theoretically and empirically suboptimal, as they fail to fully exploit the intrinsic structure of the LoRA parameterization. In this work, we propose a novel continuous-time optimization dynamic for LoRA factor matrices in the form of an ordinary differential equation (ODE) that emulates the gradient flow of full fine-tuning on the balanced manifold. We term this approach ODELoRA. To faithfully track the trajectories of ODELoRA, we adopt well-established and theoretically grounded time-discretization schemes, including Euler and Runge--Kutta methods. Our framework provides a unified ODE-based perspective for understanding and designing LoRA training algorithms. We establish linear convergence of the proposed method under strongly convex objectives for certain discretization schemes under mild conditions, and further extend our analysis to the matrix sensing setting. Moreover, we show that ODELoRA achieves stable feature learning, a property that is crucial for training deep neural networks at different scales of problem dimensionality. Empirical results on matrix sensing tasks confirm the derived linear convergence behavior, and experiments on training physics-informed neural networks further demonstrate the superiority of ODELoRA over existing baselines, especially in the training stability.

</details>


### [494] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

TL;DR: 本文提出了首个能定量预测数据受限情况下神经缩放律指数的理论，仅依赖语言的两个统计特性：词元对间相关性随时间间隔衰减和条件熵随上下文长度衰减，并在GPT-2和LLaMA类模型上验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 现有理论无法定量预测现代大语言模型在自然语言数据集上的神经缩放律指数，亟需一种从第一性原理出发的可解释理论。

Method: 通过分析语言中词元对相关性衰减和下一词条件熵衰减这两个关键统计特性，推导出一个无自由参数、无需合成数据模型的解析公式来预测数据受限下的缩放指数。

Result: 该理论公式在TinyStories和WikiText两个差异显著的数据集上，对GPT-2和LLaMA风格模型的实验缩放律指数实现了高度匹配。

Conclusion: 语言的两种基本统计结构足以决定数据受限缩放行为，为神经缩放律提供了首个可验证、无拟合参数的理论基础。

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [495] [Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks](https://arxiv.org/abs/2602.07494)
*Shenxi Wu,Haosong Zhang,Xingjian Ma,Shirui Bian,Yichi Zhang,Xi Chen,Wei Lin*

Main category: cs.LG

TL;DR: 本文提出了一种基于图的有效深度概念，统一分析CNN、ResNet和Transformer等多路径网络，并在稳定初始化和最大更新准则下，发现最优学习率随有效深度呈-3/2幂律衰减，从而实现跨深度与宽度的零样本学习率迁移。


<details>
  <summary>Details</summary>
Motivation: 现代深层架构训练成本高，超参数迁移优于重复调优；μP解释了宽度缩放下的超参数可迁移性，但深度缩放（尤其含多路径与残差连接的架构）尚缺乏统一理解。

Method: 引入图论意义上的‘有效深度’（输入到输出的最短路径长度，含层与残差加法），结合稳定初始化与最大更新准则（最大化初始化时典型一步表征变化而不失稳），推导学习率随有效深度的标度律。

Result: 理论推导出最优学习率随有效深度呈-3/2幂律衰减；实验在多种架构上验证该斜率，并成功实现跨深度与宽度的零样本学习率迁移。

Conclusion: 有效深度与最大更新准则为多路径现代神经网络提供了统一的深度缩放理论框架，使深度相关的超参数调优转变为可预测的迁移问题。

Abstract: Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($μ$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.

</details>


### [496] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

TL;DR: 本文提出了一种无需预设行为模式数量K*的无监督多意图逆强化学习框架CoMI-IRL，通过解耦行为表征/聚类与奖励学习，提升了泛化性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成式多意图IRL方法需预先知道真实行为模式数K*，限制了其在新行为上的适应性，且无法跨行为模式分析。

Method: 提出基于Transformer的无监督框架CoMI-IRL，将行为表征与聚类从下游奖励学习中解耦，并引入对比学习机制。

Result: 实验表明CoMI-IRL在无需K*先验或标签的情况下优于现有方法，支持行为关系可视化，并能适应未见行为而无需完全重训练。

Conclusion: CoMI-IRL解决了多意图IRL中对K*依赖的问题，提升了模型的灵活性、可解释性与泛化能力。

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [497] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

TL;DR: 本文介绍了PALMS（Pavlovian Associative Learning Models Simulator），一个用于模拟经典条件反射实验的Python仿真环境，支持多种学习模型（包括Rescorla-Wagner及其扩展、Pearce-Kaye-Hall、Mackintosh Extended、Le Pelley's Hybrid等），具备图形界面、大规模刺激处理、构型线索计算、实时可视化与数据导出功能，开源免费。


<details>
  <summary>Details</summary>
Motivation: 现有工具难以支持复杂实验设计、多模型快速比较及构型线索建模；神经科学家需要更贴近实际实验流程、易用且可扩展的仿真平台。

Method: 开发基于Python的开源仿真器PALMS，集成多种注意性学习模型，设计类神经科学实验输入格式的图形界面，实现构型线索自动计算、统一可变学习率机制，并支持高效可视化与数据导出。

Result: PALMS成功实现了对上百种刺激的高效仿真、全模型构型线索建模、多模型预测结果的即时对比，复现了多个已发表实验，验证了其准确性与实用性。

Conclusion: PALMS为关联学习理论研究提供了强大、灵活、易用且开源的仿真平台，显著拓展了经典学习模型的建模能力与实证适用性。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [498] [Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games](https://arxiv.org/abs/2602.07521)
*Xionghui Yang,Bozhou Chen,Yunlong Lu,Yongyi Wang,Lingfeng Li,Lanxiao Huang,Lin Liu,Wenjun Wang,Meng Meng,Xia Lin,Wenxin Li*

Main category: cs.LG

TL;DR: 本文提出了一种帕累托最优引导的蒸馏与轻量化架构搜索方法，成功将高性能《王者荣耀》游戏AI模型部署到移动设备，在保持40.32%胜率的同时实现12.4倍推理加速和15.6倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 大型MOBA游戏AI模型因多模态状态表示和分层动作空间而参数量大、计算复杂，难以在资源受限的移动设备上实时高效部署。

Method: 提出帕累托最优引导的蒸馏流程，并设计面向移动端执行的高效率学生网络架构搜索空间，系统探索性能与效率的权衡。

Result: 蒸馏后的模型在移动设备上达到每帧推理时间<0.5ms（加速12.4×）、每局耗电<0.5mAh（能效提升15.6×），对战原教师模型胜率达40.32%。

Conclusion: 该工作首次系统性地 bridged 大规模游戏AI与端侧部署之间的鸿沟，验证了高性能博弈智能在移动端落地的可行性与有效性。

Abstract: Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\times$ faster inference speed (under 0.5ms per frame) and a $15.6\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.

</details>


### [499] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: MedVerse 提出了一种基于 Petri 网理论的并行化医学推理框架，将传统串行自回归解码重构为有向无环图（DAG）过程，在保持逻辑一致性的同时提升效率与可靠性；实验表明其在性能、延迟和吞吐量上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医学推理任务中表现优异，但其固有的串行自回归解码方式难以适配临床中本应并行进行的复杂推理（如鉴别诊断），导致效率低、可靠性受限。

Method: 提出 MedVerse 框架：1）数据层面：MedVerse Curator 自动构建知识驱动的 Petri 网结构化推理路径；2）模型层面：设计拓扑感知注意力机制与自适应位置编码，支持并行且逻辑一致的推理；3）系统层面：定制化并行推理引擎，零额外开销。

Result: 在强通用 LLM 基础上提升最高达 8.9%；相比专业医学 LLM，性能相当，推理延迟降低 1.3 倍，生成吞吐量提升 1.7 倍。

Conclusion: MedVerse 通过将医学推理建模为可并行的 DAG 过程，有效突破了传统 LLM 串行解码瓶颈，在准确性、效率与可扩展性之间取得更好平衡，为复杂临床推理提供了新范式。

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [500] [Compact Conformal Subgraphs](https://arxiv.org/abs/2602.07530)
*Sreenivas Gollapudi,Kostas Kollias,Kamesh Munagala,Aravindan Vijayaraghavan*

Main category: cs.LG

TL;DR: 本文提出了一种基于图的共形压缩框架，用于在保持统计有效性的同时减少结构复杂性，通过加权超图中最密k-子图问题的近似算法实现，并利用参数最小割的单调性保证共形预测的有效性。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽能提供严格的、分布无关的不确定性保证，但在路由、规划或序列推荐等结构化领域中常产生过大的预测集，亟需压缩方法。

Method: 提出图基共形压缩框架，将压缩建模为选择覆盖指定概率质量分数的最小规模子图，归约为大边比例下的加权超图最密k-子图问题，并设计满足单调性的高效近似算法。

Result: 实现了常数因子的覆盖率与大小权衡；证明了算法松弛满足单调性，保障嵌套性及共形有效性；识别出一类可高效近似的新型最密k-子图算法场景；仿真验证了在行程规划与导航任务中的有效性。

Conclusion: 该工作在统计有效性与图压缩效率之间建立了理论桥梁，既拓展了共形预测在结构化输出上的适用性，也丰富了组合优化中可解问题的边界。

Abstract: Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce "graph-based conformal compression", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.

</details>


### [501] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

TL;DR: 本文提出了高斯匹配与复制（GMC）基准，用于分离和研究Transformer模型中的长程检索行为，揭示了梯度下降在特定条件下隐式偏向于最大间隔分类器，从而实现硬匹配选择。


<details>
  <summary>Details</summary>
Motivation: 现有研究中检索与记忆行为相互纠缠，难以单独理解匹配与复制这一核心检索原语如何在自然数据上涌现，因此需要一个能隔离长程检索机制的简化基准。

Method: 提出高斯匹配与复制（GMC） minimalist 基准，基于纯二阶相关性信号；结合数值实验与简化注意力设置下的优化动力学分析，并在技术条件下证明梯度下降轨迹朝向最大间隔分类器方向对齐。

Result: GMC保留了Transformer实际训练中形成匹配-复制回路的关键定性特征，可区分不同架构的检索能力；理论证明在经验损失趋近零时，梯度下降参数方向收敛至最大间隔分离超平面。

Conclusion: 匹配与复制行为可在无记忆干扰下由二阶统计信号驱动，且优化过程存在隐式偏差，促使模型学习硬匹配而非软检索，为理解Transformer内部机制提供了理论与实证支撑。

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [502] [Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles](https://arxiv.org/abs/2602.07579)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: 本文提出了一种面向时间序列分类（TSC）的多样性驱动神经网络集成学习框架，通过特征正交性损失显式鼓励各成员模型学习互补而非冗余的特征表示，在UCR 128个数据集上达到SOTA性能且所需模型更少。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的时间序列分类集成方法通常使用相同结构和配置训练多个模型，仅简单聚合预测结果，缺乏对模型间特征多样性的显式建模，导致特征表示冗余、集成增益受限。

Method: 提出一种多样性驱动的集成学习框架，采用基于特征正交性的装饰学习策略（decorrelated learning strategy），在神经网络各成员的隐层特征表示上施加正交性损失，以强制不同模型学习互补特征。

Result: 在UCR档案库的128个时间序列数据集上评估表明，该方法在更少模型数量下即达到当前最优（SOTA）分类性能，兼具高效性与可扩展性。

Conclusion: 显式建模并增强神经网络集成中各成员的特征多样性（如通过特征正交性约束）能显著提升时间序列分类性能，并减少对大量同构模型的依赖。

Abstract: Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.

</details>


### [503] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为预训练变分桥（PVB）的深度生成模型，通过编码器-解码器架构和增强桥匹配机制，统一利用单结构与配对轨迹数据，在分子动力学模拟中实现高效、高保真度的动力学生成，并针对蛋白-配体复合物引入基于强化学习的伴随匹配优化，加速达到全息态并支持对接构象后优化。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）模拟计算成本高昂；现有深度生成模型在跨系统泛化能力或利用结构信息提升生成保真度方面存在不足。

Method: 提出预训练变分桥（PVB）模型，采用编码器-解码器结构，将初始结构映射至加噪隐空间，并通过增强桥匹配向阶段特定目标传输；统一训练单结构与配对轨迹数据；针对蛋白-配体复合物，引入基于强化学习的伴随匹配优化以加速趋向holo态。

Result: 在蛋白质及蛋白-配体复合物上实验表明，PVB能忠实复现MD的热力学与动力学可观测量，同时提供稳定高效的生成动力学。

Conclusion: PVB通过跨域结构知识一致建模与任务自适应优化，显著提升了生成式分子动力学建模的保真度、泛化性与效率。

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [504] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

TL;DR: 本文将多指标基准测试形式化为社会选择问题，分析了在不同偏好结构（单峰、组可分、距离受限）下如何实现稳定且有意义的模型排名，并在HELM MMLU等现代基准上进行了实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如HELM MMLU）包含多个指标（准确率、鲁棒性、效率），但将其聚合为单一排名时易出现不一致或不稳定；需从理论层面理解何时及为何聚合可行。

Method: 将多指标基准建模为社会选择问题，每个指标对模型产生一个偏好排序，基准操作员需跨指标聚合；分析Arrow不可能性定理在该场景下的适用性，并识别使不可能性消失的三类偏好结构限制（单峰性、组可分性、距离限制性）。

Result: 理论上证明：在单峰、组可分和距离受限偏好下，存在良定义、稳定的聚合方法；实证发现HELM MMLU等基准中部分子任务满足这些结构条件。

Conclusion: 多指标基准的合理聚合并非普遍不可行，而取决于实际偏好结构；识别并利用结构约束可实现稳健、可解释的模型排名。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [505] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出Astro框架，通过激活引导的结构化正则化，在不增加推理延迟的前提下有效抑制大语言模型权重量化中的异常值，提升量化精度并保持硬件友好性。


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法在抑制权重和激活异常值时存在效果不足或部署效率低（如高推理延迟、复杂预处理、依赖算子融合）的问题。

Method: 基于大语言模型常收敛于平坦极小值（Flat Minima）的洞察，提出Astro——一种激活引导的结构化正则化框架，通过激活引导的目标函数主动重构鲁棒权重，针对性抑制对应高幅值激活的权重异常值。

Result: Astro零推理延迟、与GPTQ等主流方法正交；在LLaMA-2-7B上性能优于复杂学习型旋转方法，且量化时间仅为后者的约1/3。

Conclusion: Astro提供了一种高效、硬件友好的权重量化异常值抑制新范式，兼顾精度、速度与部署简易性。

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [506] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

TL;DR: 本文提出Rational Transductors，一种双流架构，在Transformer中引入基于加权有限自动机的矩阵递归，并通过Deep Rational Injection将有理状态信息注入注意力机制，从而提升其对序列逻辑和状态跟踪的建模能力，实现对正则语言等复杂任务的鲁棒长度泛化。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在语义建模上优秀，但在刚性顺序逻辑和状态跟踪方面表现不佳；理论表明自注意力机制表达能力受限（AC^0或TC^0），难以支持无链式思维的序列任务长度泛化。

Method: 提出Rational Transductors双流架构，结合Transformer与源自加权有限自动机（WFA）的矩阵值递归，并设计Deep Rational Injection机制将有理状态注入注意力；引入Random Rational Features作为通用序列依赖基，并建立Differentiable Rational Feature学习理论。

Result: 该模型严格扩展了Transformer表达能力至所有正则语言、NC^1完全问题（如布尔公式求值）及Parity、模计数等基本分离任务，保持O(L + log T)并行时间复杂度，并在算法任务上实现鲁棒长度泛化。

Conclusion: Rational Transductors填补了Transformer在‘正则间隙’上的能力缺陷，在不牺牲并行效率的前提下，兼具RNN的状态建模能力和Transformer的并行优势，为序列建模提供了更强大的理论与实践框架。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [507] [Object-Oriented Transition Modeling with Inductive Logic Programming](https://arxiv.org/abs/2602.07602)
*Gabriel Stella,Dmitri Loguinov*

Main category: cs.LG

TL;DR: 本文提出了一种新型学习算法，用于从观测中构建世界模型（归纳），在泛化性、可解释性和训练效率方面显著优于现有基于对象导向表示的方法。


<details>
  <summary>Details</summary>
Motivation: 构建能准确泛化到新情境、易于解释且训练高效的模型是机器学习中的主要挑战，尤其是受人类认知启发的对象导向表征方法存在局限性。

Method: 提出一种全新的学习算法，其设计旨在提升归纳能力、泛化性、可解释性和训练效率，并通过消融实验和与神经网络基线的对比验证其有效性。

Result: 实验结果表明，该算法在多个指标上显著超越当前最先进方法。

Conclusion: 所提出的算法是比以往对象导向建模范式更强大的归纳学习工具，为构建可靠、可解释的世界模型提供了新路径。

Abstract: Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.

</details>


### [508] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: 本文提出了一种无需反向传播的隐式神经表示（INR）方法ELM-INR，通过将域划分为重叠子域，并在每个子域上用极端学习机（ELM）闭式求解，结合单位分解实现快速稳健重建；进一步从谱Barron范数角度分析误差来源，并提出自适应网格细化策略BEAM以提升容量受限下的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于迭代反向传播训练的隐式神经表示（INRs）易受频谱偏差影响，难以高效拟合具有高度非均匀频率成分的目标信号。

Method: 提出ELM-INR：将输入域划分为重叠子域，每个子域内用Extreme Learning Machine（ELM）进行闭式线性最小二乘拟合；通过单位分解（partition of unity）加权融合局部预测；理论分析采用谱Barron范数刻画误差分布；并据此设计自适应网格细化策略BEAM。

Result: 实现了无需反向传播、快速且数值稳定的INR重建；理论揭示全局误差主要来源于高谱复杂度区域；BEAM策略在容量受限下显著提升重建质量。

Conclusion: ELM-INR与BEAM共同构成一种高效、鲁棒、可解释的新型INR框架，突破了传统方法对迭代优化和频谱偏置的依赖。

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [509] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

TL;DR: SERE是一种基于相似性的专家重路由方法，用于在MoE模型批量解码中动态减少活跃专家数量，提升硬件效率并保持模型能力。


<details>
  <summary>Details</summary>
Motivation: MoE模型在生产部署中需批量推理以提高硬件效率，但会导致专家激活过多，拖慢内存受限的解码阶段；需解决批量解码与专家稀疏性之间的根本矛盾。

Method: SERE通过输入感知的方式，将令牌从次要专家动态重路由至最相似的主要专家，并利用相似性模式识别和保留关键专家；不采用静态剪枝或合并，而是基于批级别专家冗余实现动态跳过；并开发了高效定制CUDA核，支持vLLM单行代码集成。

Result: 在多个复杂推理基准上，SERE实现最高2.0倍加速，质量损失极小；已开源代码。

Conclusion: SERE为大规模MoE模型在成本敏感与低延迟场景下的实际部署提供了实用高效的解决方案。

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [510] [Dense Neural Networks are not Universal Approximators](https://arxiv.org/abs/2602.07618)
*Levi Rauchwerger,Stefanie Jegelka,Ron Levie*

Main category: cs.LG

TL;DR: 本文探讨了全连接神经网络的近似能力，指出在权重、输入输出维度等自然约束下，ReLU全连接网络无法逼近某些Lipschitz连续函数，从而揭示其不具备普遍逼近性；作者通过弱正则性引理与图神经网络视角下的消息传递解释，论证稀疏连接是实现真正通用逼近性的必要条件。


<details>
  <summary>Details</summary>
Motivation: 已有通用逼近定理仅在无权重限制时成立，但实际神经网络存在参数规模与结构约束；本文旨在揭示在更现实约束下全连接网络的内在逼近局限性。

Method: 结合弱正则性引理（weak regularity lemma）与将前馈网络建模为消息传递型图神经网络（GNN）的视角，对ReLU全连接网络施加权重、输入/输出维度等自然约束，分析其表达能力边界。

Result: 证明在给定自然约束下，存在Lipschitz连续函数无法被此类全连接网络任意精度逼近，即其不具有通用逼近性。

Conclusion: 全连接神经网络在实际约束下不具备通用逼近能力，稀疏连接是实现真正通用性的必要设计要素。

Abstract: We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.

</details>


### [511] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

TL;DR: 本文提出TASTE框架，利用Stein算子实现任务感知的分布偏移检测，将分布偏移与模型输入敏感性关联，并支持偏移定位与像素级可解释诊断。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法多为数据中心或模型中心，缺乏对分布偏移如何影响具体任务性能的建模。

Method: 基于Stein算子构建任务感知框架TASTE，将分布偏移投影到模型敏感性场中，并通过坐标分解实现偏移定位和像素级诊断。

Result: 在高斯偏移、MNIST几何扰动及CIFAR-10扰动基准上，TASTE在检测任务性能退化方面优于现有基线方法。

Conclusion: TASTE实现了分布偏移与任务性能退化的显式关联，兼具理论保证与可解释性，为OOD检测提供了新范式。

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [512] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

TL;DR: 本文提出了一种面向语义对齐的几何编译突变算子，通过学习具有行为意义的连续程序空间，显著提升了遗传编程在交易策略搜索中的样本效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 遗传编程中传统突变操作缺乏行为局部性，小的语法变化可能导致不可预测的大行为偏移，从而降低搜索效率和可解释性。

Method: 构建一个紧凑的交易策略领域特定语言（DSL），设计块分解式嵌入，并采用基于流模型的学习方法，在语义配对的进出点子空间内进行受限突变，区别于各向同性高斯突变。

Result: 在相同进化策略和评估预算下，新突变算子比各向同性突变减少约一个数量级的评估次数，并获得最高的中位样本外夏普比率；虽峰值性能略低，但收敛更快、更稳定。

Conclusion: 语义对齐的突变设计可在不改变底层进化算法的前提下，大幅提升遗传编程的搜索效率与鲁棒性，验证了操作符学习在程序合成中的关键作用。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [513] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

TL;DR: 本文研究了在可验证执行基础（VEG）任务中，测试时训练（TTT）是否是最优策略，发现搜索（如Best-of-N采样）显著优于少量梯度更新的TTT；提出“惊奇度引导选择”策略，在不增加计算成本下大幅提升成功率，表明对VEG任务应优先分配算力于样本多样性与智能选择，而非梯度适应。


<details>
  <summary>Details</summary>
Motivation: 质疑测试时训练（TTT）在具有确定性、稠密连续奖励信号的可验证执行基础（VEG）任务（如GPU核优化）中是否为最优策略，探索更高效的测试时计算分配方式。

Method: 以KernelBench为基准，使用120B参数模型（GPT-OSS-120B+LoRA），对比TTT（1–5步梯度更新）与Best-of-N采样等搜索策略；提出并评估惊奇度（surprisal）引导的选择机制——优先选取正确但低置信度（高惊奇度）的样本，并扩展至top-3选择。

Result: Best-of-64采样达90%任务成功率，而TTT最高仅30.6%；惊奇度引导单样本选择达80%成功率（较置信度最高选择提升30%），惊奇度引导top-3选择达100%（匹配oracle性能）；TTT因‘过度锐化’损害多样性，导致性能下降。

Conclusion: 对于稠密奖励的VEG任务，测试时计算资源应投向样本多样性与惊奇度引导的智能选择，而非梯度适应；该选择原则可能推广至其他执行基础型任务，尤其当最优解位于分布尾部时。

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [514] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 本文提出Feroma框架，通过构建客户端分布概要（distribution profiles）来应对联邦学习中的数据分布偏移和漂移问题，无需客户端或聚类标识，实现动态聚合策略选择与测试时模型自适应分配，在多个基准上显著提升性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以有效处理真实场景中客户端间的数据分布偏移（distribution shift）和随时间变化的分布漂移（distribution drift），且常依赖不现实假设（如已知客户端聚类数或异质性类型），泛化能力受限。

Method: 提出Feroma框架，基于隐私保护的紧凑型客户端分布概要（client distribution profiles），指导模型聚合与测试时模型分配；采用自适应相似性加权机制，动态选择从聚类到个性化等多种聚合策略，支持对未知、无标签测试客户端直接部署合适模型，无需重训练或在线适配。

Result: 在6个基准数据集上，相比10种SOTA方法，Feroma平均准确率提升最高达12个百分点，同时计算与通信开销与FedAvg相当。

Conclusion: 基于分布概要的聚合机制为应对数据分布偏移与漂移提供了实用、鲁棒的联邦学习新路径。

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [515] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

TL;DR: 本文提出ElliCE框架，通过在Rashomon集的椭球近似上优化反事实解释，实现鲁棒的算法救济，保证解释在模型不确定性下的有效性、唯一性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 当Rashomon集较大时，传统反事实解释在不同近优模型间不可靠，难以提供稳定可行的救济建议。

Method: 构建基于Rashomon集椭球近似的优化框架ElliCE，对反事实进行鲁棒优化，并提供理论保障（唯一性、稳定性、关键特征方向对齐）。

Result: ElliCE生成的反事实解释在鲁棒性、灵活性（支持用户自定义特征约束）和计算效率上均优于现有基线方法。

Conclusion: ElliCE为模型不确定性下的可靠算法救济提供了原理清晰且实用的解决方案，确保用户获得稳定、可信赖的行动建议。

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [516] [Spectral Gating Networks](https://arxiv.org/abs/2602.07679)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Yongsen Zheng,Kwok-Yan Lam,Liang Lin,Keze Wang*

Main category: cs.LG

TL;DR: 本文提出了Spectral Gating Networks (SGN)，一种在保持稳定性和参数效率前提下，为前馈网络注入频谱表达能力的门控机制。SGN通过可学习的随机傅里叶特征和混合GELU-Fourier激活，替代易脆的样条网格，在多个领域显著提升精度-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有样条型Kolmogorov-Arnold网络（KAN）因网格细化导致参数爆炸、优化脆弱及高维不稳定的问题，探索在固定参数与训练预算下增强频谱表达力而不牺牲稳定性和可扩展性的新路径。

Method: 提出SGN：在标准MLP/FFN中引入轻量级频谱通路（基于可学习频率与相位的随机傅里叶特征）和可学习门控机制；采用混合GELU-Fourier激活提升优化鲁棒性与高频保真度；实现端到端可微、即插即用的频谱重参数化。

Result: 在CV、NLP、音频与PDE多个基准上，SGN在同等计算预算下持续提升精度-效率权衡：CIFAR-10达93.15%准确率，推理速度较样条KAN快至11.7倍。

Conclusion: SGN为前馈网络提供了一种稳定、高效、可扩展的频谱增强范式，验证了门控式频谱通路设计在兼顾表达力与优化稳健性上的有效性，推动神经网络频谱建模的发展。

Abstract: Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.

</details>


### [517] [On the Infinite Width and Depth Limits of Predictive Coding Networks](https://arxiv.org/abs/2602.07697)
*Francesco Innocenti,El Mehdi Achour,Rafal Bogacz*

Main category: cs.LG

TL;DR: 本文研究了预测编码网络（PCN）在无限宽度和深度极限下的行为，发现其特征学习的参数化方式与标准反向传播（BP）完全一致，并在宽远大于深的条件下，PC的能量函数收敛于BP损失，从而梯度也一致；实验验证了该结论在深层非线性网络中亦成立，前提是活动达到平衡。


<details>
  <summary>Details</summary>
Motivation: 现有预测编码（PC）方法在深层网络中训练稳定性不足，其可扩展性与理论基础尚不清晰，需从极限行为角度建立与反向传播（BP）的理论联系。

Method: 理论分析PCN在无限宽度和深度极限下的行为，特别针对线性残差网络推导宽度-深度稳定的特征学习参数化条件，并证明PC能量函数在活动平衡、宽远大于深条件下收敛于BP损失；辅以深层非线性网络实验验证。

Result: 证明了PC与BP在稳定参数化集合上完全一致；PC能量在宽>>深且活动平衡时收敛于BP损失，从而梯度等价；实验表明结论在实际深层非线性PCN中成立。

Conclusion: PCN在适当参数化与尺度条件下可实现与BP等价的梯度计算，为PC的可扩展性提供了坚实的理论支撑，统一了此前分散的理论与实证结果。

Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.

</details>


### [518] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为密集特征学习（dense feature learning）的表示中心框架，旨在通过线性代数性质（如谱平衡、子空间一致性、特征正交性）显式塑造医学嵌入的线性结构，从而提升表示的有效秩、条件数和时间稳定性，无需标签或生成重建，在多种医学数据上显著提升下游线性性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在医学数据上多采用任务特定目标，导致表征坍缩到少数判别方向，限制了可迁移性、稳定性和可解释性，未能充分利用临床数据的丰富结构。

Method: 提出密集特征学习框架，直接作用于嵌入矩阵，通过纯线性代数定义的目标（如谱平衡、子空间一致性、特征正交性）优化表示几何，不依赖标签或生成重建。

Result: 在纵向EHR、临床文本和多模态患者表示上，相比监督与自监督基线，下游线性性能、鲁棒性及子空间对齐度均一致提升；表征具有更高有效秩、更好条件数和更强时间稳定性。

Conclusion: 学习覆盖临床变异的表征几何结构与预测临床结果同等重要，应将表示几何作为医疗AI的一等设计目标。

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [519] [Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization](https://arxiv.org/abs/2602.07708)
*Ding Zhang,Siddharth Betala,Chirag Agarwal*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估图神经网络（GNN）后验解释质量的指标——Explanation-Generalization Score（EGS），基于特征不变性原理，利用OOD泛化能力衡量解释是否捕获真实因果结构，并通过大规模实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释评估指标（如保真度、稀疏性）难以判断解释是否识别出真实的因果变量，缺乏对因果相关性的有效度量。

Method: 提出EGS指标，基于特征不变性假设：若解释捕获真实因果驱动因素，则应支持模型在分布偏移下的稳定预测；通过在解释性子图上训练GNN并在OOD场景下评估其泛化性能来量化该性质。

Result: 在涵盖合成与真实数据集、总计11,200种模型组合的大规模实验中，EGS被证实能有效区分解释方法对因果子结构的捕捉能力，优于传统保真度指标。

Conclusion: EGS为GNN解释性研究提供了以因果有效性为导向的、可推广的评估基准，推动解释方法从相关性向因果性演进。

Abstract: Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.

</details>


### [520] [Towards Robust Scaling Laws for Optimizers](https://arxiv.org/abs/2602.07712)
*Alexandra Volkova,Mher Safaryan,Christoph H. Lampert,Dan Alistarh*

Main category: cs.LG

TL;DR: 本文研究了不同优化器对大语言模型预训练缩放规律的影响，发现固定指数、引入优化器特异性重标度因子的缩放律更稳健，并从凸二次目标的理论分析中解释了Chinchilla式缩放律的自然出现。


<details>
  <summary>Details</summary>
Motivation: 现有缩放律研究大多固定使用AdamW优化器，而新一代优化器（如Muon、Shampoo、SOAP）的缩放行为尚不明确，亟需系统探究其与模型和数据规模的关系。

Method: 通过实证分析多种优化器在不同规模下的训练损失，提出具有共享幂律指数和优化器特异性重标度因子的改进缩放律；并以凸二次目标为代理任务，对梯度类方法进行理论误差分解分析。

Result: 1）为各优化器单独拟合Chinchilla式缩放律会导致参数高度相关、病态；2）共享指数+优化器特异性因子的缩放律更稳健且支持跨优化器比较；3）理论分析表明Chinchilla式缩放律可由不可约误差、逼近误差和优化误差的分解自然导出。

Conclusion: 优化器选择显著影响缩放行为，应采用统一指数加特异性因子的缩放框架；理论分析证实了经验缩放律在简化设定下的合理性，为未来优化器驱动的高效预训练提供了指导。

Abstract: The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.

</details>


### [521] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

TL;DR: 本文在高斯先验假设下，对基于扩散模型的零样本逆问题求解方法进行了严格的谱域分析，并提出了一个原理性的参数设计框架，取代了以往依赖人工调参和启发式策略的方法。


<details>
  <summary>Details</summary>
Motivation: 现有零样本扩散方法在解决逆问题时依赖手工调参和启发式策略，缺乏理论支撑和系统性参数设计方法。

Method: 基于高斯先验假设，推导理想后验采样器与扩散重建算法的闭式表达式，并在谱域中进行分析与比较；进而提出一种与具体算法无关、能联合考虑先验、退化信号及扩散动力学特性的原理性参数设计框架。

Result: 获得了与标准启发式方法结构不同的谱域参数推荐，且该推荐随扩散步长变化，能在感知质量与信号保真度之间实现一致平衡。

Conclusion: 本文为零样本扩散逆问题求解提供了理论基础和可推广的参数设计范式，显著提升了方法的可解释性与实用性。

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [522] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

TL;DR: 本文提出将人类的内省能力类比为程序分析，以此在强化学习与经典规划之间建立新联系，并设计算法实现关系型强化学习模型上的高效目标导向规划。


<details>
  <summary>Details</summary>
Motivation: 人类能根据任务描述自主推断额外信息以高效解决问题，而现有强化学习与经典规划方法各自独立、缺乏这种内省能力。

Method: 将内省建模为程序分析，应用于多种强化学习模型，并设计一种算法，在关系型强化学习模型上实现目标导向的经典规划。

Result: 实现了关系型强化学习模型上的高效目标导向规划，建立了强化学习与经典规划之间的新联系。

Conclusion: 内省可被形式化为程序分析，该视角有助于统一强化学习与经典规划，并提升模型的任务泛化与推理能力。

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [523] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: ParisKV is a GPU-native KV-cache retrieval framework for long-context LLM inference, using collision-based candidate selection and quantized reranking to handle distribution drift and scale efficiently—achieving superior speed, throughput, and memory scalability over prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing KV-cache retrieval methods suffer from distribution drift and high latency at scale, limiting efficient long-context LLM inference.

Method: ParisKV employs collision-based candidate selection followed by a quantized inner-product reranking estimator; supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA) for on-demand top-k fetching.

Result: ParisKV matches or exceeds full attention quality and speed (even at batch size 1), achieves up to 2.8× higher throughput, scales to million-token contexts, and reduces decode latency by 17× vs MagicPIG and 44× vs PQCache.

Conclusion: ParisKV enables efficient, drift-robust, and scalable KV-cache retrieval for million-token LLM inference, outperforming state-of-the-art baselines in both accuracy and efficiency.

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [524] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 本文发现，在大语言模型的强化学习（RL）阶段，内存效率更高的SGD优化器在性能上可匹敌甚至超越广泛使用的AdamW，且SGD更新的参数比例极低（<0.02%），揭示了RL阶段固有的参数更新稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有RL训练（尤其是RLVR）沿用预训练和监督微调阶段的优化实践（如AdamW），但RL与这些阶段存在本质差异；而AdamW内存开销大，其核心机制（动量、自适应学习率）在RL中的作用尚不明确。

Method: 通过理论分析与实证实验，对比AdamW与SGD在LLM的RL阶段的优化行为，重点考察动量和自适应学习率的影响，并分析参数更新的稀疏模式。

Result: SGD在RL阶段性能匹配或优于AdamW；SGD仅更新不到0.02%的模型参数，远低于AdamW（超1000倍），且无需稀疏正则化；该现象具有可解释性。

Conclusion: RL阶段的优化动态与监督学习显著不同，SGD更适配RL，且RL本身具有高度参数效率，挑战了当前对RL训练资源需求的普遍认知。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [525] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文提出了一种名为Laplacian Keyboard（LK）的分层框架，利用拉普拉斯特征向量构建任务无关的选项库，并通过元策略动态组合这些选项，以突破线性表示限制，提升强化学习中策略学习的表达能力、零样本近似能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯特征向量在强化学习中常用于奖励函数逼近，但仅限于其线性张成空间，导致在复杂环境中表达能力受限。

Method: 提出Laplacian Keyboard（LK）：基于拉普拉斯特征向量构建任务无关的选项库作为行为基底，并设计元策略动态组合这些选项；理论分析零样本近似误差界。

Result: 理论证明LK能保证包含线性奖励下的最优策略；实验表明LK在零样本迁移和样本效率上均优于现有方法。

Conclusion: LK通过分层结构和行为基底扩展，有效提升了强化学习中基于图拉普拉斯的表征能力与泛化性能，为结构化策略学习提供了新范式。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [526] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

TL;DR: 本文提出了一种在特定数据分布（如已知先验下的稠密分布）下，兼具计算高效性与最优样本复杂度O(log T)的自适应数据分析（ADA）机制，并揭示了ADA与超越差分隐私的Predicate Singling Out安全性的内在联系。


<details>
  <summary>Details</summary>
Motivation: 解决自适应数据分析中计算效率与样本复杂度之间的根本张力：现有高效算法样本复杂度次优（O(√T)），而最优算法（O(log T)）计算不可行。

Method: 设计一种计算高效的ADA机制，适用于数据分布相对于已知先验是稠密的情形（涵盖分布特异性学习中的特征-标签分布），不依赖差分隐私但满足Predicate Singling Out（PSO）安全性。

Result: 在稠密分布假设下实现了O(log T)的最优样本复杂度和多项式时间计算效率；同时导出分布特异性设定下样本高效的统计查询预言机；并证明该机制满足PSO安全性。

Conclusion: 数据分布的结构（如稠密性）可打破ADA中计算与统计的权衡；PSO安全性为理解ADA与隐私的深层联系提供了新视角。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [527] [TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations](https://arxiv.org/abs/2602.07735)
*Matteo Rossi,Ryan Pederson,Miles Wang-Henderson,Ben Kaufman,Edward C. Williams,Carl Underkoffler,Owen Lewis Howell,Adrian Layer,Stephan Thaler,Narbe Mardirossian,John Anthony Parkhill*

Main category: cs.LG

TL;DR: TerraBind 是一种用于蛋白质-配体结构与结合亲和力预测的基础模型，通过简化原子表示（仅使用蛋白Cβ和配体重原子）及去扩散架构，在保持构象预测精度的同时，将推理速度提升26倍、亲和力预测准确率提高约20%。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的结构药物设计方法依赖计算昂贵的全原子扩散生成3D坐标，导致大规模化合物筛选不可行；作者提出假设：全原子分辨率对小分子对接与亲和力预测并非必需。

Method: 采用粗粒化口袋级表征（蛋白Cβ + 配体重原子），结合COATI-3分子编码与ESM-2蛋白嵌入构建多模态架构；引入无扩散优化模块生成配体构象，并设计结合亲和力似然预测模块。

Result: 在FoldBench等结构预测基准上匹敌扩散基线；在CASP16和内部18种生化/细胞实验数据集上，亲和力预测Pearson相关性比Boltz-2提升约20%；提供校准良好的不确定性估计，并支持持续学习与对冲式批量选择策略，在模拟发现循环中实现6倍于贪婪法的亲和力提升。

Conclusion: TerraBind验证了粗粒化表征与无扩散架构在结构与亲和力联合建模中的有效性，显著提升效率与可靠性，为大规模药物发现提供了新范式。

Abstract: We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_β$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.

</details>


### [528] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 本文提出Learnable Chernoff Baselines (LCBs)方法，用于在推理时高效、近似地对生成模型进行奖励引导的对齐，仅需黑盒采样访问预训练模型，具备计算可扩展性和理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有推理时奖励引导对齐方法常依赖架构定制或计算开销大，缺乏通用、高效且理论可保证的方案。

Method: 提出Learnable Chernoff Baselines（LCBs），基于可学习的Chernoff界实现自适应拒绝采样，近似指数倾斜核分布，在仅访问预训练模型采样接口的前提下调控接受概率。

Result: 在连续与离散扩散模型中，LCB采样在总变差距离下逼近理想对齐模型，且显著减少对预训练模型的查询次数。

Conclusion: LCBs提供了一种通用、高效、可扩展且具理论保障的推理时奖励对齐框架，适用于各类生成模型。

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [529] [Riemannian MeanFlow](https://arxiv.org/abs/2602.07744)
*Dongyeop Woo,Marta Skreta,Seonghyun Park,Sungsoo Ahn,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 本文提出了Riemannian MeanFlow（RMF）框架，直接在黎曼流形上学习流映射，仅需一次前向传播即可生成高质量样本，显著减少推理时的神经网络评估次数，并在DNA设计和蛋白质骨架生成任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流模型在黎曼流形上生成时需大量神经网络评估，导致大规模科学采样中计算开销大。

Method: 提出Riemannian MeanFlow（RMF），推导流形上平均速度的三种等价刻画（Eulerian、Lagrangian、半群恒等式），并设计参数化与稳定化策略以提升高维流形训练效果。

Result: 在启动子DNA设计和蛋白质骨架生成任务中，RMF达到与先前方法相当的样本质量，但函数评估次数减少至最多1/10；并支持基于奖励预测的高效少步设计（reward look-ahead）。

Conclusion: RMF为黎曼流形上的高效生成建模提供了新范式，在保持质量的同时大幅提升推理效率，并拓展了奖励驱动设计的应用潜力。

Abstract: Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.

</details>


### [530] [Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization](https://arxiv.org/abs/2602.07764)
*Tanmay Ambadkar,Sourav Panda,Shreyash Kale,Jonathan Dodge,Abhinav Verma*

Main category: cs.LG

TL;DR: 本文提出D³PO框架，通过解耦优化流程和引入缩放多样性正则化，解决了多目标强化学习中梯度干扰和表征坍塌问题，显著提升了Pareto前沿的覆盖度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有偏好条件化策略方法在实践中脆弱，难以恢复完整的Pareto前沿，根源在于过早标量化导致的破坏性梯度干扰和偏好空间中的表征坍塌。

Method: 提出基于PPO的D³PO框架：采用解耦优化流程以保持各目标独立学习信号，并在策略稳定后才融入偏好；引入缩放多样性正则化，增强策略对偏好变化的敏感性，防止坍塌。

Result: 在标准MORL基准（含高维、多目标控制任务）上，D³PO持续发现更广、更高质量的Pareto前沿，在超体积和期望效用指标上达到或超越当前最优水平，且仅需单个可部署策略。

Conclusion: D³PO有效缓解了MORL中关键结构性缺陷，为单策略偏好条件化方法提供了更鲁棒、可扩展的解决方案。

Abstract: Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.

</details>


### [531] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: MaD-Mix是一种高效、原理驱动的多模态数据混合框架，通过模态感知的域对齐最大化自动确定VLM训练中的最优数据配比，显著减少人工调参成本并提升训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLM）训练依赖高成本的人工调优数据混合比例，缺乏自动化、可扩展且理论支撑的方法；尤其在多模态缺失或复杂多模态（如视频-图像-文本）场景下，人工调参愈发不现实。

Method: MaD-Mix将数据混合建模为模态感知的域对齐最大化问题，利用Fenchel对偶推导出闭式多模态对齐分数，并通过跨模态耦合变量实现；支持含缺失模态（如纯文本域）的域集成。

Result: 在0.5B和7B规模VLM上验证：MaD-Mix在图像-文本指令微调中仅用22%更少训练步数即达到人工调优效果；在视频-图像-文本三模态任务中显著优于均匀加权，平均准确率提升，且混合计算开销极低（<1 GPU小时）。

Conclusion: MaD-Mix提供了一种可扩展、低开销、理论严谨的自动化数据混合方案，适用于现代大规模VLM训练流水线，有效替代人工调参。

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [532] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

TL;DR: 本文提出CausalTaD方法，通过将因果关系注入大语言模型（LLM）来提升表格异常检测性能，核心是按因果关系重排序列并引入列重加权策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的表格异常检测方法随机排列列顺序，忽略了变量间的因果关系，而因果结构对准确检测异常至关重要。

Method: 首先识别表格各列间的因果关系，将其建模为线性排序问题并重排序列；其次设计列重加权策略，依据各列在因果结构中的重要性分配不同权重。

Result: 在30多个数据集上的实验表明，CausalTaD持续优于当前最先进方法。

Conclusion: 将因果知识显式建模并融入LLM的表格表示中，能显著提升异常检测性能，验证了因果结构在该任务中的关键作用。

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [533] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

TL;DR: 本文提出了一种名为Faro的公平性感知奖励优化框架，用于在大语言模型（LLM）对齐过程中，在奖励建模阶段引入多种公平性约束（如人口统计均等、均衡几率、反事实公平），首次从理论上分析了奖励层面的公平性，并证明了其可证公平性、准确-公平权衡及帕累托前沿存在性；相比预/后处理方法，Faro能同时保障奖励模型的序数性、基数性和公平性，并在多个模型和基准上显著降低偏见与有害生成，同时保持或提升模型质量。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据中存在人口统计偏差，这些偏差会通过奖励模型传播至对齐后的LLM，导致系统性不公平。

Method: 提出Faro框架，在奖励模型训练过程中引入人口统计均等、均衡几率或反事实公平等约束；进行KL正则化微调，并提供理论分析，包括公平性证书、准确-公平权衡证明及帕累托前沿存在性证明。

Result: Faro在多个LLM和基准测试中显著降低了偏见与有害输出，同时维持或提升了模型质量；奖励模型兼具序数性（正确排序）、基数性（校准）与公平性。

Conclusion: Faro是一种有效的内处理式公平对齐方法，首次实现了奖励层面的可证公平性，并在实践中兼顾公平性与模型性能。

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [534] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: 本文研究了使用神经网络（包括Transformer）近似矩阵函数的问题，理论证明了ReLU网络逼近矩阵指数的深度与宽度界，并通过实验验证了Transformer在特定数值编码下可高概率以5%相对误差逼近某些矩阵函数。


<details>
  <summary>Details</summary>
Motivation: Transformers在自然语言处理中取得巨大成功，但在数值计算尤其是矩阵函数逼近方面的应用尚未被充分探索；而矩阵函数（如矩阵指数、矩阵符号函数）在科学计算中具有广泛应用。

Method: 1）理论分析：推导ReLU神经网络逼近矩阵指数所需的宽度和深度上界；2）实验验证：采用带合适数值编码的Transformer编码器-解码器架构，对多种矩阵函数进行逼近实验，并比较不同编码方案的效果。

Result: 1）给出了ReLU网络逼近矩阵指数的理论界；2）Transformer在合适编码下能以约5%相对误差高概率逼近某些矩阵函数；3）编码方案对性能影响显著，不同函数需适配不同编码。

Conclusion: 神经网络特别是Transformer可用于矩阵函数的数值逼近，但其性能高度依赖于输入编码设计；理论与实验结合为将深度学习应用于科学计算提供了新思路。

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [535] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 本文提出一种简单而有效的方法，在大语言模型（LLM）中植入可解释、可控的内部特征：通过添加轻量辅助损失，将少量残差流维度直接训练为‘惰性解释性标志’，模型在训练中自发围绕这些标志重组表征，最终实现推理时的可控生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法需先识别再干预模型内部特征几何结构，过程复杂；本文旨在绕过这一繁琐流程，以最直接的方式植入可控、可解释的特征。

Method: 在LLM上增加一个简单辅助损失，微调16个（共3072维）残差流维度，使其成为指示所需概念的惰性解释性标志；模型在训练中自发适应并依赖这些标志。

Result: 这些被强制设定的惰性维度成功演化为真实、可解释、可干预的内部特征，在推理时可作为控制开关精准引导生成行为。

Conclusion: 模型内在的效率优化压力可被利用——当某特征被稳定供给于固定位置时，梯度下降会自然消除冗余编码，促使模型重构表征，从而自动生成可解释且可控的内部特征。

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [536] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: 本文提出rePIRL框架，受逆强化学习启发，通过双学习过程（策略与过程奖励模型交替更新）来学习有效的过程奖励模型（PRM），仅需对专家策略做最小假设，解决了现有方法依赖强假设或存在熵坍塌等问题。理论证明其可统一在线/离线PRM学习，实验验证其在数学与代码推理任务上的优越性及多种下游应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理中的过程奖励模型（PRM）学习方法要么依赖专家策略的强假设（如已知其奖励函数），要么存在内在缺陷（如熵坍塌），导致PRM效果弱或泛化能力差。

Method: 提出rePIRL框架：基于逆强化学习思想，设计策略与PRM交替更新的双学习过程；引入定制化技术以适配LLM规模；理论分析其对在线/离线PRM学习的统一性。

Result: 在标准数学与代码推理数据集上，rePIRL显著优于现有方法；所学PRM成功应用于测试时训练、测试时缩放及困难问题训练的早期信号识别；消融研究验证了训练方案与核心设计的有效性。

Conclusion: rePIRL是一种低假设、高通用性的PRM学习框架，兼具理论严谨性与实践有效性，为LLM推理中的过程监督提供了新范式。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [537] [Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation](https://arxiv.org/abs/2602.07834)
*D Yang Eng*

Main category: cs.LG

TL;DR: 本文提出使用符号回归从神经网络近似中提取简洁、可解释的公式，成功获得一个五项表达式，其精度（R²=0.9994）与神经网络相当，但参数量减少3000倍，并能准确复现Calabi–Yau流形上的物理可观测量。


<details>
  <summary>Details</summary>
Motivation: Calabi-Yau流形对弦理论至关重要，但其度规计算难以处理，亟需可解释且高效的替代建模方法。

Method: 采用符号回归技术对神经网络的近似结果进行蒸馏，结合多种子验证与几何约束筛选关键特征（如幂和与对称多项式），并在模空间范围内分析系数变化规律。

Result: 获得一个五项解析表达式，R²达0.9994，参数量仅为神经网络的1/3000；该公式在整个ψ∈[0,0.8]范围内保持形式不变，系数平滑变化；成功复现体积积分和Yukawa耦合等物理可观测量。

Conclusion: 符号蒸馏能有效生成紧凑、可解释的模型，用于此前仅能由黑箱神经网络处理的复杂几何物理量，为理论物理建模提供了新范式。

Abstract: Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\approx 8-9\%$ at $ψ\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.

</details>


### [538] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: 本文提出了一种多智能体强化训练与推理框架MARTI-MARS2，通过将多智能体协作建模为动态可学习环境，实现从同质多角色到异质多智能体的演进，显著提升代码生成等复杂任务性能，并揭示了策略多样性对多智能体RL扩展智能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在代码生成等复杂任务中存在性能瓶颈；现有多智能体框架受限于提示工程或同质参数配置，难以实现有效纠错和策略多样性。

Method: 提出MARTI-MARS2框架，将多智能体协同探索建模为动态可学习环境，结合策略学习与多智能体树搜索，支持从同质多角色训练向异质多智能体训练演进；并设计高效推理策略MARTI-MARS2-T+以增强测试时缩放能力。

Result: 在8B/14B/32B模型规模上验证有效性；两个32B模型协作下在代码生成基准上达77.7%，超越GPT-5.1等强基线；发现新缩放律：单智能体→同质多角色→异质多智能体逐步提升RL性能上限、TTS鲁棒性与策略多样性。

Conclusion: 策略多样性是通过多智能体强化学习扩展智能的核心要素；MARTI-MARS2为突破单智能体能力极限提供了可扩展、可学习的多智能体协同新范式。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [539] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: 本文提出了一种融合物理模型结构与数据驱动自适应能力的数据中心大电子负载（LEL）建模框架，通过时序对比学习（TCL）实现模式一致的校准，兼顾准确性、可解释性与数据隐私，并在多个实际系统中验证了其对扰动后动态行为的刻画能力。


<details>
  <summary>Details</summary>
Motivation: 传统负荷模型无法刻画数据中心等大电子负载（LEL）的快速工作负载变化及保护驱动的断连/重连行为；现有物理模型缺乏设施级校准，数据驱动模型易过拟合且动态行为不真实。

Method: 构建兼具物理结构与数据驱动适应性的LEL建模框架：物理结构参数化以支持基于实测运行数据的模式一致性校准；采用时序对比学习（TCL）替代轨迹对齐，实现时间与统计模式联合匹配；校准在本地完成，仅共享参数以保障数据隐私。

Result: 模型在MIT Supercloud、ASU Sol、Blue Waters和ASHRAE等多源真实数据上完成校准，并集成至ANDES平台，在IEEE 39、NPCC 140和WECC 179节点系统中验证；结果表明LEL间交互会显著改变扰动后恢复行为，引发复合断连-重连动态与延迟稳定现象。

Conclusion: 所提框架有效弥补了物理模型与数据驱动模型各自的缺陷，在保持可解释性与隐私保护前提下，显著提升了对数据中心LEL动态行为（尤其是扰动后复杂交互响应）的建模精度与实用性。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [540] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 本文提出了一种无需显式参数化策略的软策略采样方法——Langevin Q-Learning（LQL），并进一步改进为Noise-Conditioned LQL（NC-LQL），通过多尺度噪声平滑Q函数景观，提升高维非凸场景下的采样效率，在MuJoCo任务上达到与先进扩散方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有软策略实现方式存在局限：参数化策略表达能力弱，扩散策略因似然不可解而难以可靠估计熵；亟需一种既能保证理论合理性又能高效采样的替代方案。

Method: 基于Q函数的动作梯度驱动Langevin动力学实现软策略采样（LQL）；进一步引入噪声条件化的Q函数，构建多尺度平滑Q景观以加速混合，形成NC-LQL算法。

Result: 在OpenAI Gym MuJoCo基准上，NC-LQL性能媲美当前最优扩散类方法，且结构更简洁、适用于在线强化学习。

Conclusion: NC-LQL为软策略提供了一种理论上严谨、实践中高效的新范式，弥合了软策略理论与实际采样可行性之间的鸿沟。

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [541] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出HARPOON，一种基于流形理论的表格数据扩散生成方法，可在推理阶段灵活满足多种条件（如插补、不等式约束），克服了现有方法泛化性差和领域受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据生成方法在推理时难以泛化到未见过的约束条件，且仅适用于有限任务（如插补）；同时，现有流形理论方法局限于连续域和特定推理目标。

Method: 将流形理论扩展至表格数据，并支持多样化的推理时目标；在此基础上提出HARPOON——一种利用流形几何引导无约束扩散样本满足各类表格条件的扩散模型。

Result: 在插补和不等式约束等任务上验证了HARPOON的有效性，实验表明其在多个数据集上性能优异，并证实了流形感知引导对表格生成的实际价值。

Conclusion: HARPOON成功将流形理论推广至离散/混合型表格数据，实现了推理阶段灵活、通用的条件生成，为可控表格数据合成提供了新范式。

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [542] [GRAFT: Decoupling Ranking and Calibration for Survival Analysis](https://arxiv.org/abs/2602.07884)
*Mohammad Ashhad,Robert Hoehndorf,Ricardo Henao*

Main category: cs.LG

TL;DR: GRAFT是一种新型AFT模型，通过解耦预后排序与校准，结合线性AFT与非线性残差神经网络，并引入随机门控实现端到端特征选择，在公共基准中展现出优异的判别力、校准性及抗噪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生存分析面临删失数据、高维特征和非线性交互等挑战；传统模型可解释但受限，深度学习模型灵活但不可解释且对噪声敏感。

Method: 提出GRAFT模型：融合线性AFT与非线性残差神经网络的混合架构，加入随机门控用于自动特征选择，并采用基于局部Kaplan-Meier估计器的随机条件插补，优化可微的C指数对齐排序损失。

Result: 在公开基准测试中，GRAFT在判别能力（discrimination）和校准性（calibration）上均优于基线模型，且在高噪声场景下保持鲁棒性和稀疏性。

Conclusion: GRAFT成功平衡了模型灵活性、可解释性与鲁棒性，为高维生存预测提供了一种兼具性能与实用性的新范式。

Abstract: Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.

</details>


### [543] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 本文提出了一种基于VQVAE与模糊聚类的新型离线强化学习反探索方法，通过多码本VQVAE实现高效伪计数，并结合FCM聚类更新码本，缓解维度灾难和信息损失问题，在D4RL基准上优于现有SOTA方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散化的伪计数反探索方法在处理连续状态-动作对时面临维度灾难和信息损失问题，导致效率下降甚至策略学习失败。

Method: 提出基于多码本VQVAE的伪计数方法用于状态-动作对离散化，并设计相应的反探索机制；引入基于模糊C均值（FCM）聚类的码本更新机制以提升码本向量利用率。

Result: 在D4RL基准多个复杂任务中，该方法性能优于当前最优（SOTA）方法，且计算开销更小。

Conclusion: 所提方法有效缓解了离线RL中伪计数因离散化带来的维度灾难与信息损失问题，提升了反探索效果与学习效率。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [544] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

TL;DR: 本文提出OGPSA方法，将安全对齐视为持续学习问题，通过正交梯度投影减少安全微调对通用能力的干扰，在保持安全性的同时显著提升模型推理和编码等通用能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在安全后训练中常出现‘对齐税’，即安全性提升导致通用能力（如推理、编程）下降，其主因是顺序对齐过程中的持续学习式遗忘。

Method: 提出正交梯度投影安全对齐（OGPSA）方法：基于小参考集估计表征通用能力的低秩子空间，并将安全梯度投影到该子空间的正交补空间再更新参数，从而最小化对先验知识的扰动。

Result: OGPSA在SFT、DPO及SFT→DPO多种设置下均显著改善安全–效用帕累托前沿；例如在Qwen2.5-7B-Instruct上，SimpleQA准确率从0.53%升至3.03%，IFEval从51.94%升至63.96%。

Conclusion: 将安全对齐建模为持续学习问题并施加正交梯度约束，可有效缓解对齐税；OGPSA是一种轻量、即插即用、无需重训练或大规模回放的通用对齐优化方法。

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [545] [Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models](https://arxiv.org/abs/2602.07904)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: 本文提出LMABO框架，利用预训练大语言模型（LLM）作为零样本、在线策略器，在贝叶斯优化中动态选择最优采集函数，显著优于静态与现有自适应方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中采集函数的选择高度问题依赖且非平稳，而现有自适应方法仅利用历史函数值，忽略预算、代理模型等关键状态信息。

Method: 提出LMABO框架，将预训练大语言模型作为零样本在线策略器；每轮迭代中，用结构化状态表示（含预算、模型特征等）提示LLM从采集函数组合中选择最优者。

Result: 在50个基准问题上，LMABO显著优于强静态基线、自适应组合方法及其他LLM基线；实验证明LLM能综合实时优化状态生成有效自适应策略。

Conclusion: LLM可作为强大、通用的零样本策略器用于贝叶斯优化，其性能优势源于对完整优化状态的建模与综合决策能力，为BO自动化策略设计提供了新范式。

Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.

</details>


### [546] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: 本文提出AceGRPO框架，通过演化数据缓冲区和自适应采样策略解决LLM-based MLE代理在长期迭代优化中行为停滞的问题，显著提升模型在MLE-Bench-Lite上的有效提交率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的LLM代理在自主机器学习工程（MLE）中因参数冻结导致行为停滞；而强化学习虽可缓解该问题，却受限于高执行延迟和低效数据选择。

Method: 提出AceGRPO框架，包含两个核心组件：(1) 演化数据缓冲区，将执行轨迹持续转化为可复用训练任务；(2) 基于‘可学性潜力’函数的自适应采样，动态聚焦于代理学习前沿的任务以提升学习效率。

Result: Ace-30B模型在MLE-Bench-Lite上实现100%有效提交率，性能接近专有前沿模型，并超越更大规模开源基线（如DeepSeek-V3.2）。

Conclusion: AceGRPO有效克服了当前MLE代理在长期迭代优化中的关键瓶颈，验证了其在自主、可持续MLE中的鲁棒性与实用性。

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [547] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: 本文提出了CausalCompass，一个用于评估时间序列因果发现（TSCD）方法在建模假设被违反时鲁棒性的灵活、可扩展基准套件，并通过八种假设违背场景对主流TSCD算法进行了系统评测，发现深度学习方法整体表现更优，且NTS-NOTEARS对标准化预处理高度依赖。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列因果发现方法受限于不可检验的因果假设，且缺乏面向鲁棒性的评估基准，阻碍了其实际应用。

Method: 构建CausalCompass基准套件，涵盖八种建模假设违背场景；对代表性TSCD算法进行大规模基准测试；开展超参数敏感性分析；评估不同预处理（如标准化）对性能的影响。

Result: 无单一方法在所有假设违背场景中持续最优；深度学习方法在多数场景下整体性能更优；NTS-NOTEARS在未标准化时性能差，标准化后显著提升。

Conclusion: CausalCompass为TSCD方法提供了系统、鲁棒的评估框架，有助于推动其在现实场景中的可靠部署；代码与数据集已开源。

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [548] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理视角的流式生成模型分析方法，定义了每个样本轨迹的动能路径能量（KPE），发现KPE与语义保真度正相关但与密度呈非单调关系；过高KPE导致记忆化，据此提出无需训练的两阶段推理策略KTS，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 受经典力学启发，将流式生成模型视为粒子从噪声到数据的动态演化过程，希望引入可解释、可量化的轨迹级物理量（如动能）来诊断和改进生成行为。

Method: 定义Kinetic Path Energy (KPE)作为ODE采样轨迹上的累积动能；结合经验流匹配的闭式解，理论分析KPE与数据密度的关系；提出无需训练的Kinetic Trajectory Shaping (KTS)两阶段推理策略：增强早期运动、施加晚期软着陆约束。

Result: 实证发现：(i) KPE越高，生成样本语义保真度越强；(ii) 高KPE轨迹落点位于低密度流形前沿；理论证明KPE与密度存在非单调关联；过高KPE引发记忆化；KTS在多个基准任务上提升生成质量并抑制记忆。

Conclusion: KPE是一个有效且具物理解释性的轨迹诊断工具；生成性能遵循‘黄金法则’——KPE需适中；KTS作为一种即插即用的推理优化方法，验证了动力学视角对提升流模型实用性的价值。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [549] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

TL;DR: 本文研究了基于注意力机制的深度学习模型在帕金森病（PD）早期检测中的有效性，发现SAINT模型在多项指标上显著优于MLP、梯度提升和TabNet等基线模型，主要归功于其双注意力机制对特征交互的建模能力。


<details>
  <summary>Details</summary>
Motivation: 早期帕金森病症状隐匿、生物医学数据具有高度非线性，传统机器学习模型依赖大量特征工程且难以捕捉复杂特征交互，亟需更有效的自动特征学习方法。

Method: 采用四种分类模型（MLP、梯度提升、TabNet、SAINT）在UCI语音生物医学数据集上进行对比实验，重点分析SAINT的双注意力机制如何建模样本内与样本间特征交互。

Result: SAINT在加权精确率（0.98）、加权召回率（0.97）、加权F1分数（0.97）、MCC（0.9990）及AUC-ROC上均最优；TabNet和MLP次之，梯度提升最差。

Conclusion: 基于注意力机制的深度学习模型（尤其是SAINT）在早期PD检测中展现出优越性能，验证了动态特征表征对临床预测任务的重要性。

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [550] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 本文（Part II）研究了有限时间学习中不可逆性对持续学习的影响，提出“关键期关闭”现象：有限耗散导致学习路径受限，使模型在学习后期无法在不同任务兼容表示间动态切换，从而引发持续学习失败。


<details>
  <summary>Details</summary>
Motivation: 有限时间学习必然不可逆，Part I已推导出认知速度极限；本文旨在探究这种不可逆性如何从轨迹层面影响持续学习过程。

Method: 将学习建模为参数分布空间中的传输过程，结合熵产生与有限耗散约束，从动力学路径角度分析表示空间的可及性变化。

Result: 发现有限耗散不仅限制可达解，更限制可达的学习路径；任务等价的多种实现中，仅部分被有限时间学习选中；随着学习推进，支持结构重配置的自由度被逐步消除，导致‘关键期关闭’。

Conclusion: 持续学习失败源于先前学习引起的表征自由度不可逆丧失，而非任务间直接干扰；灾难性遗忘本质是有限时间耗散施加的动力学约束。

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [551] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释人工智能（XAI）中累积局部效应（ALE）曲线的多任务相似性度量方法，利用Fréchet距离和数据分布加权，并引入特征重要性和性能缩放因子，具有模型无关性，适用于单任务与多任务场景，并在多个真实与合成数据集上验证了其合理性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中需明确任务间相似性及其成因，但现有方法缺乏可解释、模型无关且能兼顾特征重要性与任务性能差异的相似性度量。

Method: 基于ALE曲线，采用Fréchet距离进行比较，按数据分布加权，并融入特征重要性；引入缩放因子校正任务间预测性能差异；支持模型无关的单任务或多任务设置。

Result: 在合成数据、Parkinson's、bike-sharing（表格数据）及CelebA（图像+概念瓶颈）四个数据集上验证表明，该度量结果符合任务相似性的直观认知，适用于表格与非表格数据。

Conclusion: 所提ALE-based相似性度量是一种可解释、鲁棒、通用的工具，有助于理解任务关系、指导多任务建模与决策。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [552] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 本文提出了一种名为隐式战略优化（ISO）的新框架，用于训练大语言模型（LLM）代理在长周期对抗性博弈中做出更优决策。该框架结合战略奖励模型（SRM）与上下文感知的乐观学习规则（iso-grpo），通过预测战略环境来在线更新策略，并在理论和实验上验证了其优于现有方法的长期回报性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于回合目标（如胜率）的LLM代理训练在长周期博弈中忽视了随时间演化的潜在战略外部性，导致短视优化和基于变异的遗憾分析失效。

Method: 提出隐式战略优化（ISO）框架，包含两个核心组件：1）战略奖励模型（SRM），用于估计动作的长期战略价值；2）iso-grpo，一种上下文条件下的乐观学习规则。理论证明其具有亚线性上下文遗憾和均衡收敛性，且误差项主要取决于上下文预测错误次数。

Result: 在6人无限注德州扑克和竞争性宝可梦游戏中，ISO显著提升长期回报，优于强LLM和强化学习基线，并在可控预测噪声下表现出良好鲁棒性。

Conclusion: ISO是一种预测感知、上下文自适应的战略优化框架，在存在动态战略外部性的长周期对抗任务中具备理论保障与实际有效性。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [553] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

TL;DR: 本文提出CLIPPER方法，在神经符号学习中通过整数线性规划修剪满足逻辑公式的标签组合空间，利用相似隐表示实例共享标签的直觉，在16个基准上显著提升现有神经符号引擎性能。


<details>
  <summary>Details</summary>
Motivation: 在神经符号学习中，隐含真实标签需满足逻辑公式，但满足公式的标签组合空间可能指数级增长，导致学习困难。

Method: 提出CLIPPER技术，将标签组合空间剪枝建模为整数线性规划问题，在保留逻辑结构前提下剔除不一致组合；该方法可与现有训练算法正交集成。

Result: 在16个复杂神经符号任务基准上，CLIPPER使Scallop、Dolphin和ISED分别提升48%、53%和8%准确率，达到当前最优水平。

Conclusion: CLIPPER有效缓解逻辑约束下标签组合爆炸问题，通过结合隐表示相似性与逻辑结构约束，显著提升神经符号分类器性能，且具备良好通用性和可集成性。

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [554] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

TL;DR: 本文提出SiameseNorm，一种双流架构，通过解耦Pre-Norm和Post-Norm的优化动态，在保持训练稳定性的同时提升模型表达能力。


<details>
  <summary>Details</summary>
Motivation: Pre-Norm虽稳定但性能受限，Post-Norm性能好却不稳定；现有融合方法难以兼顾二者优势，作者认为这是单流结构中两种范式存在结构性不兼容所致。

Method: 提出SiameseNorm双流架构，将Pre-Norm-like与Post-Norm-like流耦合，参数共享但梯度路径分离，使每个残差块接收来自两种范式的组合梯度。

Result: 在1.3B参数模型上预训练实验表明，SiameseNorm兼具优异优化鲁棒性与更强性能，持续超越强基线。

Conclusion: SiameseNorm成功调和Pre-Norm与Post-Norm的矛盾，为Transformer归一化设计提供了新范式。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [555] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

TL;DR: 本文提出Metric-Topology Factorization（MTF）框架，主张智能本质在于动态重塑表征几何结构（而非在固定结构中优化），将稳定的拓扑结构与可塑的度量形变解耦；基于此构建Topological Urysohn Machine（TUM），通过记忆摊销的度量推理（MAMI）实现跨任务快速适应、抗灾难性遗忘与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习将智能等同于在固定表征几何中优化，但在分布偏移、任务重排和持续学习等动态场景下易失效并引发灾难性遗忘，亟需一种能自适应调整表征结构的新原理。

Method: 提出Metric-Topology Factorization（MTF）几何原理：将任务身份与环境变化编码为稳定拓扑结构，而学习过程建模为黎曼度量的可控收缩（metric contraction）；进一步设计Topological Urysohn Machine（TUM），利用谱任务签名索引预计算的度量变换，实现记忆摊销的度量推理（MAMI）。

Result: 理论证明任何固定度量在拓扑变换下必然存在奇异性，揭示权值系统固有的稳定性-可塑性权衡；实验表明TUM在任务重排序、反射/奇偶变换环境及持续学习基准上显著优于EWC等方法，具备强鲁棒性与泛化性。

Conclusion: 智能不应被理解为在静态几何中搜索，而应是实时重构表征空间的能力；MTF通过解耦拓扑与度量，为持续学习与自适应智能提供了新的几何基础，TUM是其首个可实现架构。

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [556] [Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks](https://arxiv.org/abs/2602.08128)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: 本文提出了一种名为Online Bayesian Imbalanced Learning (OBIL) 的新框架，用于在不重新训练模型的情况下实时适应部署时的类别分布偏移，通过从深度网络输出中提取与先验无关的似然比，并仅调整决策阈值实现最优贝叶斯决策。


<details>
  <summary>Details</summary>
Motivation: 现有处理类别不平衡的方法（如重采样、代价敏感学习）在部署时遇到类别分布偏移需重新训练或依赖标注目标数据，而实际应用（如欺诈检测、医疗诊断）中此类偏移频繁发生。

Method: 基于Bregman散度与恰当评分规则的联系，证明使用此类损失训练的深度网络可输出后验概率估计，从中可提取先验不变的似然比；理论证明该似然比在任意类别先验和代价结构变化下仍有效，仅需调整阈值；并推导了有限样本遗憾界。

Result: 在基准数据集和医学诊断任务上，OBIL在测试分布显著偏离训练分布时，F1分数优于当前最优方法，且在严重分布偏移下保持鲁棒性能。

Conclusion: OBIL提供了一种无需重训练、解耦似然比估计与类别先验假设的在线不平衡学习范式，具有理论保证和实际有效性。

Abstract: Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\sqrt{T \log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.

</details>


### [557] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

TL;DR: 本文理论分析了在仅使用结果级反馈的强化学习与可验证奖励（RLVR）训练下，自回归大语言模型学习组合性问题的能力，并提出了‘任务优势比’这一关键概念来刻画哪些组合性任务是可学习的。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在实践中取得了成功，但尚不清楚仅靠结果级反馈能学会哪些组合性问题，缺乏理论解释。

Method: 通过理论分析，提出并定义‘任务-优势比’作为组合性问题与基础模型的联合属性，用以刻画任务的可学习性；并基于此分析RLVR在不同条件下的收敛性。

Result: 证明了具有清晰中间步骤优势的组合性问题可被RLVR高效学习；而当结构优势缺失时，RLVR可能收敛到次优解，且基础模型质量决定该优势是否存在。

Conclusion: 任务-优势比为理解RLVR在组合性推理中的成功与失败提供了原则性的理论框架。

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [558] [Reliable and Responsible Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2602.08145)
*Xinyu Yang,Junlin Han,Rishi Bommasani,Jinqi Luo,Wenjie Qu,Wangchunshu Zhou,Adel Bibi,Xiyao Wang,Jaehong Yoon,Elias Stengel-Eskin,Shengbang Tong,Lingfeng Shen,Rafael Rafailov,Runjia Li,Zhaoyang Wang,Yiyang Zhou,Chenhang Cui,Yu Wang,Wenhao Zheng,Huichi Zhou,Jindong Gu,Zhaorun Chen,Peng Xia,Tony Lee,Thomas Zollo,Vikash Sehwag,Jixuan Leng,Jiuhai Chen,Yuxin Wen,Huan Zhang,Zhun Deng,Linjun Zhang,Pavel Izmailov,Pang Wei Koh,Yulia Tsvetkov,Andrew Wilson,Jiaheng Zhang,James Zou,Cihang Xie,Hao Wang,Philip Torr,Julian McAuley,David Alvarez-Melis,Florian Tramèr,Kaidi Xu,Suman Jana,Chris Callison-Burch,Rene Vidal,Filippos Kokkinos,Mohit Bansal,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: This survey provides a comprehensive overview of the reliable and responsible development of foundation models, covering issues like bias, security, uncertainty, explainability, distribution shift, hallucinations, alignment, and AIGC detection, while outlining future research directions and interdisciplinary connections.


<details>
  <summary>Details</summary>
Motivation: With the increasing real-world deployment of foundation models across diverse domains, ensuring their reliability and responsibility has become critical for academia, industry, and government.

Method: The paper conducts a systematic survey, reviewing the current state of research in key areas—bias and fairness, security and privacy, uncertainty, explainability, distribution shift, hallucinations, alignment, and AIGC detection—and identifies intersections and shared challenges among them.

Result: A structured synthesis of existing work and concrete future research directions for making foundation models ethical, trustworthy, reliable, and socially responsible.

Conclusion: Reliable and responsible foundation model development requires addressing multiple interconnected challenges; this survey aims to foster holistic progress toward ethically grounded and socially beneficial AI systems.

Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.

</details>


### [559] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文提出了一种基于多级蒙特卡洛估计和显式burn-in机制的原对偶自然Actor-Critic算法，用于解决无链假设下的无限时域平均奖励约束马尔可夫决策过程（CMDP），在不依赖混合时间假设的前提下实现了O~(√T)的有限时间遗憾与约束违反界。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习的遗憾分析大多依赖遍历性或强混合时间假设，而这些假设在存在暂态状态时失效。

Method: 提出一种原对偶自然Actor-Critic算法，结合多级蒙特卡洛（MLMC）估计器和显式burn-in机制，以处理无链动力学，无需混合时间预言机。

Result: 建立了有限时间遗憾和累积约束违反界，均以O~(√T)速率增长，并考虑了策略与评论家参数化的近似误差。

Conclusion: 该方法将最优阶保证扩展到了更广泛的CMDP类别，尤其适用于含暂态状态的无链CMDP。

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [560] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 本文研究了语言模型中‘正确性’表征的几何结构，发现其在低维子空间中以线性可分的均值偏移形式存在，可通过简单几何度量（如质心距离）高效检测模型是否知道答案正确与否，远超基于输出的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否真正‘知道’其生成答案的正确性，而非仅表面输出；理解模型内部如何表征和区分正确与错误陈述。

Method: 分析9个来自5种架构族的语言模型的隐藏层表征，使用线性探针、质心距离度量、激活引导（activation steering）和因果验证等方法，刻画正确性表征的维度、线性可分性及几何特性。

Result: 正确性信号集中在3–8维线性子空间；质心距离与线性探针性能高度一致（AUC 0.90）；仅需25个标注样本即可达到全数据90%检测精度；激活引导可使错误率变化10.9个百分点；内部探针AUC达0.80–0.97，而输出方法仅0.44–0.64。

Conclusion: 模型内部存在清晰、低维、线性可分的正确性几何表征，本质是类间均值偏移；该信号未被输出层表达，因此基于输出的方法效果差；检测正确性本质上是几何问题，无需复杂学习。

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [561] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文提出了一种基于互信息最大化的预算约束下大语言模型（LLM）集成选择方法，通过建模模型间相关误差揭示性能饱和的信息论原因，并在多个数据集上验证了其优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常被集成以提升可靠性与鲁棒性，但实践中模型预测高度相关，导致性能提升受限；因此需解决‘在查询预算约束下应选择哪些模型组成最优集成’这一核心问题。

Method: 将预算约束下的集成选择建模为最大化真实标签与所选模型预测之间的互信息；引入高斯Copula建模模型间相关误差，推导集成性能的信息论误差下界；据此设计一种基于数据直接估计互信息项的贪心算法，迭代构建满足查询预算的集成。

Result: 在MEDMCQA、MMLU和IMDB三个数据集上的实验表明，该方法在相同查询预算下持续优于强基线方法。

Conclusion: 互信息是指导LLM集成选择的有效准则；考虑模型相关性的信息论建模有助于理解并突破集成性能瓶颈，所提贪心算法兼具理论依据与实用有效性。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [562] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的推理时控制方法——球面引导（Spherical Steering），通过激活旋转而非加法来保持隐藏表示的模长，避免表示坍缩，并结合置信门动态调节引导强度，在多项选择基准上显著优于加法基线，同时保持开放生成质量。


<details>
  <summary>Details</summary>
Motivation: 标准推理时引导方法依赖激活加法，会改变隐藏表示的模长，可能导致表示坍缩和开放生成能力下降。

Method: 提出球面引导方法，利用测地线上的激活旋转代替加法，并引入基于输入不确定性的动态置信门调节引导强度。

Result: 在TruthfulQA、COPA和Storycloze等多选基准上相较加法基线提升约10%，同时保持模型开放生成质量。

Conclusion: 模长保持的旋转操作是一种几何一致、鲁棒且高效的推理时精确控制原语。

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [563] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

TL;DR: 本文提出了TSR-Adam，一种面向通信受限场景的低秩优化器，通过双侧低秩同步核心矩阵U^⊤GV（尺寸r×r）大幅降低梯度通信量，并结合随机SVD刷新策略避免全梯度同步，在大模型预训练和微调中显著减少通信开销（分别达13×和25×），同时保持性能相当。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，数据并行预训练中梯度同步成为带宽受限的关键瓶颈；现有单侧低秩优化器在通信效率上仍不理想，且子空间刷新开销大。

Method: 提出TSR框架，将Adam类优化器改造为TSR-Adam：采用双侧低秩表示同步核心r×r矩阵U^⊤GV；引入随机SVD进行子空间刷新以避免全梯度同步；针对嵌入层梯度设计专用秩与刷新策略。

Result: 在60M至1B参数规模预训练中平均每步通信量减少13倍；GLUE微调中通信减少25倍；性能与基线相当；并提供了更新的理论收敛性分析。

Conclusion: TSR-Adam有效缓解了分布式训练中的通信瓶颈，在保持模型性能前提下显著提升通信效率，尤其适用于大规模语言模型训练。

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [564] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: 本文提出Dreaming in Code（DiCode）框架，利用基础模型生成可执行环境代码，以构建渐进式学习课程，提升智能体在开放世界中的长期技能获取能力。


<details>
  <summary>Details</summary>
Motivation: 现有开放学习方法难以在复杂组合空间中为智能体提供持续可学的经验序列，缺乏对学习进程的有效引导。

Method: DiCode框架让基础模型生成代码级环境变体（即“做梦”），在Craftax开放世界基准上实现环境的可编程、渐进式演化，从而构建适配智能体当前能力的中间环境。

Result: DiCode使智能体在Craftax上平均回报率提升16%，并在先前方法失败的晚期战斗任务中首次取得非零成功率。

Conclusion: 代码级环境设计是一种可行且有效的课程控制机制，能弥合开放世界中智能体的能力断层，推动持续学习。

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [565] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出了一种统一的概率空间框架，将基于奖励的流模型适配与多模型融合（flow merging）统一处理，并设计了Reward-Guided Flow Merging（RFM）算法，提供理论保证，并在分子设计等高维任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别处理预训练流模型的奖励控制适应和多模型融合，缺乏统一框架；需实现任务感知、可解释且原理坚实的多流组合。

Method: 构建概率空间上的生成模型密度操作算子族（如交、并、插值及奖励引导变体），提出基于镜像下降的Reward-Guided Flow Merging（RFM）算法，将奖励引导融合转化为一系列标准微调问题。

Result: 提供了RFM在奖励引导与纯流融合下的首个理论保证；在可视化示例、从头分子设计和低能构象生成任务中验证了方法有效性与可解释性。

Conclusion: 所提框架与RFM算法实现了对生成流模型的灵活、可控、可证优的组合，为安全关键与多目标生成任务提供了新范式。

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [566] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

TL;DR: 本文提出DrugR，一种基于大语言模型（LLM）的分子生成与优化方法，通过引入显式的药理学推理、领域持续预训练、逆向数据工程微调和自平衡多粒度强化学习，显著提升ADMET性质并保持分子核心疗效与结构相似性及靶标结合力，同时提供可解释的优化逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在分子优化中面临分子结构与药理性质间隐式关系复杂、标注数据缺乏的挑战，需引入显式药理推理以弥合知识鸿沟。

Method: DrugR融合领域特定持续预训练、基于逆向数据工程的监督微调、以及自平衡多粒度强化学习，将显式的分步药理学推理嵌入优化流程。

Result: DrugR在多个ADMET性质上实现全面增强，不损害分子结构相似性与靶标结合亲和力，并提供清晰、可解释的每步优化依据。

Conclusion: DrugR为自动化、知识驱动的药物发现提供了新范式，兼具性能提升与可解释性，代码与模型已开源以促进后续研究。

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [567] [The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications](https://arxiv.org/abs/2602.08019)
*Dong Pan,Bingtao Li,Yongsheng Zheng,Jiren Ma,Victor Fei*

Main category: cs.LG

TL;DR: 本文是一篇关于稀疏专家混合（MoE）架构的全面综述，系统梳理了其基础原理、路由与专家网络设计、去中心化范式演进及垂直领域应用，并指出了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有MoE综述存在覆盖不全、关键领域探讨不足等问题，亟需一篇更系统、更全面的最新进展综述。

Method: 采用结构化综述方法，依次分析MoE基础原理（路由网络与专家网络）、向 decentralized 范式的扩展、垂直领域应用，并总结挑战与未来方向。

Result: 提出了目前最全面的MoE领域综述，涵盖核心机制、新范式（去中心化）及多领域应用，为研究者与实践者提供权威参考。

Conclusion: MoE作为高效可扩展模型架构前景广阔；去中心化范式和垂直领域落地是重要趋势；仍面临路由优化、训练稳定性、系统开销等关键挑战。

Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>


### [568] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 本文分析了带标准高斯扰动的线性集成采样（ES）在随机线性赌博机中的表现，证明当集成规模m=Θ(d log n)时，ES可实现Õ(d^{3/2}√n)的高概率遗憾，填补了与汤普森采样基准之间的差距，同时保持计算复杂度相近；其核心证明创新在于将离散时间探索分析转化为m个独立布朗运动的时间一致超越问题，揭示了连续时间视角对获得紧致ES界可能是自然且必要的。


<details>
  <summary>Details</summary>
Motivation: 填补线性集成采样（ES）与汤普森采样（TS）在遗憾界上的理论差距，并理解其高效探索背后的机制。

Method: 将离散时间线性集成采样的分析转化为时间一致的m个独立布朗运动超越问题，利用连续时间随机过程工具进行理论分析。

Result: 证明当集成规模m=Θ(d log n)时，ES在随机线性赌博机中取得Õ(d^{3/2}√n)的高概率遗憾上界，达到与汤普森采样相当的理论性能，同时计算开销相近。

Conclusion: 连续时间视角（特别是布朗运动模型）为分析线性集成采样提供了自然且目前唯一已知的获得紧致遗憾界的方法，表明该视角可能不仅是技巧性的，而是本质必需的。

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [569] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

TL;DR: 本文提出ManifoldKV，一种无需训练的KV缓存压缩方法，利用欧氏距离而非余弦相似度评估键的重要性，兼顾角度与径向信息，在长上下文推理中提升准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于几何的KV缓存淘汰方法多依赖余弦相似度，但其尺度不变性会丢失语义关键token的幅度信息，导致性能下降。

Method: ManifoldKV使用键向量到全局质心的欧氏距离作为评分依据；进一步提出WindowedManifoldKV以应对超长上下文（如64K）下全局质心退化问题。

Result: 在RULER基准上，ManifoldKV在4K–16K上下文中20%压缩率下达95.7%准确率；在3-key NIAH任务中50%压缩率下达92.4%，显著优于KeyDiff；WindowedManifoldKV在64K上下文、25%压缩率下恢复准确率达84.3%。

Conclusion: ManifoldKV是一种轻量、通用、免调优的KV压缩方案，仅需3行代码，跨4种架构有效，显著提升多键检索鲁棒性与超长上下文稳定性。

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [570] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 本文提出Horizon Imagination（HI），一种用于强化学习的扩散式世界模型，支持并行多步未来观测去噪，在降低计算开销的同时保持控制性能与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式世界模型在控制任务中面临推理开销大（需重型模型）或想象过程高度串行的问题，导致计算成本过高。

Method: 提出Horizon Imagination（HI）：一种面向离散随机策略的on-policy并行想象机制；引入稳定化机制和新型采样调度策略，解耦去噪预算与有效预测步数，并支持子帧级（sub-frame）去噪预算。

Result: 在Atari 100K和Craftium上验证：仅用一半去噪步数（sub-frame budget）即可维持控制性能，并在多种调度下展现出更优的生成质量。

Conclusion: HI在保证高生成保真度的同时显著提升扩散世界模型的推理效率，为高效、高质量的基于扩散的强化学习建模提供了新范式。

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [571] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

TL;DR: 本文提出了一种名为SCoRa的统一概率模型，能够同时利用个体评分（ratings）和成对比较（comparisons）两种偏好信号进行学习，在多种场景下（尤其是对顶部实体排序精度要求高时）表现优于单独使用任一信号的方法，并具有良好的理论性质（如单调性、鲁棒性）和实证效果。


<details>
  <summary>Details</summary>
Motivation: 人类偏好评估中长期存在‘应采用个体评分还是成对比较’的争论；现实中二者常共存，但如何有效融合尚不明确。

Method: 提出SCoRa（Scoring from Comparisons and Ratings）统一概率模型，联合建模评分与比较信号；理论分析其最大后验估计（MAP）的单调性与鲁棒性；通过实验验证其在模型失配下的鲁棒性及排序性能。

Result: SCoRa的MAP估计具有良好理论性质；在模型失配下仍能准确恢复得分；在顶部实体排序关键的现实场景中，融合两类信号显著优于仅用单一信号。

Conclusion: 融合个体评分与成对比较是更优策略；SCoRa为多源偏好学习提供了兼具理论保障与实用性的统一框架。

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [572] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为强化学习与回溯反馈（RLBF）的新框架，通过强化学习让大语言模型动态识别并修正自身生成中的安全违规，并结合改进的监督微调数据生成策略（BSAFE+），显著提升了模型对各类对抗攻击的鲁棒性，同时保持模型原有能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在面对对抗攻击和分布内错误时的安全性问题，提升其鲁棒性和自主纠错能力。

Method: 提出RLBF框架，包含两个核心部分：1）基于批评者反馈的强化学习阶段，使模型学会发出'回溯x个token'信号以自主纠正错误；2）改进的监督微调数据生成策略BSAFE+，通过向原本安全的文本中注入违规内容来增强初始训练效果。

Result: 在多个基准和不同规模模型上，RLBF显著降低了各类对抗攻击（如middle filling、GCG、解码参数操纵）的成功率，同时保持了模型的基础性能。

Conclusion: RLBF是一种有效提升LLM安全性的新范式，其动态回溯机制和增强的数据构造策略共同实现了安全性与实用性的更好平衡。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [573] [TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation](https://arxiv.org/abs/2602.08036)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种无需数据回放的图持续学习方法TAAM，通过轻量级任务特定神经突触调制器（NSM）对固定GNN主干进行节点感知自适应调制，并结合锚定多跳传播（AMP）解决未知任务ID问题，在更严格的归纳学习设置下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图持续学习方法依赖回放策略，存在内存开销大、隐私风险高及难以平衡稳定性与可塑性等问题；同时，真实场景中任务ID常未知，且现有基准存在数据泄露和评估偏差缺陷。

Method: 提出Task-Aware Adaptive Modulation (TAAM)，核心是为每个新任务训练并冻结一个轻量级Neural Synapse Modulator (NSM)，对共享GNN主干的计算流进行节点注意力驱动的自适应调制；针对未知任务ID，提出并理论证明Anchored Multi-hop Propagation (AMP)方法；所有实验在更严格的归纳学习设定下进行。

Result: 在八个数据集上全面超越现有最先进方法；消除了对数据回放的依赖，天然缓解灾难性遗忘；AMP有效支持未知任务ID下的推理；实验设置更严谨，避免了基准缺陷导致的偏差评估。

Conclusion: TAAM通过模块化、参数隔离的设计实现了高效、隐私友好且稳定的图持续学习，验证了轻量任务专家模块替代主干更新的有效性，为GCL提供了新范式。

Abstract: Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.

</details>


### [574] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习方法RLTR，通过可迁移奖励（transfer reward）来增强大语言模型推理过程的鲁棒性、可解释性和泛化性，相比现有方法RLVR，在更少训练步数下实现了更高准确率和更强推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法仅关注最终答案正确性，忽视推理过程本身的鲁棒性；而稳健的推理应能在截断、重解释或跨模型延续等扰动下仍保持有效性。

Method: 提出Reinforcement Learning with Transferable Reward（RLTR），定义‘迁移奖励’：用一个模型生成的部分推理前缀，引导另一个独立模型得出正确答案，从而将推理稳健性转化为可优化的奖励信号。

Result: 在MATH500数据集上，RLTR比RLVR在Maj@64指标上提升3.6个百分点，并以约2.5倍更少训练步数达到同等平均准确率；同时提升了采样一致性与推理可靠性。

Conclusion: RLTR通过建模推理作为意义传递过程，有效提升了LLM推理的稳定性与泛化能力，为构建可信赖、可复用的推理系统提供了新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [575] [FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff](https://arxiv.org/abs/2602.08040)
*Isaac Han,Sangyeon Park,Seungwon Oh,Donghu Kim,Hojoon Lee,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 本文提出FIRE方法，通过量化稳定性（SFE）和可塑性（DfI），在重初始化过程中显式平衡稳定-可塑性权衡，在多个连续学习任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在非平稳数据上训练时需平衡稳定性（保持旧知识）与可塑性（适应新任务），而现有重初始化方法难以调优：过于保守则无法恢复可塑性，过于激进则破坏已有知识。

Method: FIRE方法定义稳定性为权重与历史权重的平方Frobenius误差（SFE），可塑性为权重偏离等距性的程度（DfI）；通过约束优化（最小化SFE且DfI=0）确定重初始化点，并用Newton-Schulz迭代高效求解。

Result: FIRE在CIFAR-10（ResNet-18）、OpenWebText（GPT-0.1B）、HumanoidBench（SAC）及Atari（DQN）等多个连续学习基准上，均显著优于无干预训练和标准重初始化方法。

Conclusion: FIRE提供了一种原理清晰、可计算的重初始化框架，能有效且鲁棒地平衡稳定性与可塑性，适用于多种模态与任务的持续学习场景。

Abstract: Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>


### [576] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: 本文提出了V-ABFT，一种基于方差的自适应阈值算法，用于矩阵乘法中的静默数据损坏检测，显著提升了检测精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有ABFT阈值设定方法过于保守或误差过大，无法兼顾检测精度与效率。

Method: 提出V-ABFT算法，利用统计方差估计直接建模验证差异，仅需O(n)时间复杂度（基于max/min/mean统计），支持多种精度（BF16/FP16/FP32/FP64）并适配融合核ABFT实现。

Result: 相比A-ABFT，阈值-实际误差比降低6–48倍；低精度GEMM可采用FP32级阈值，检测粒度提升约1000倍；零误报率，且在LLaMA-7B、GPT-2、ViT等真实模型权重上验证有效。

Conclusion: V-ABFT在保证零误报前提下大幅收紧误差界限，兼具高效性、通用性与实用性，已集成至NPU和GPU平台的容错GEMM实现中。

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [577] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 本文提出了一种人机协同方法，用于构建可解释的模糊规则基系统（FRBS），以预测正向渗透海水淡化产率，在保持语义可解释性的同时达到与聚类方法相当的预测性能。


<details>
  <summary>Details</summary>
Motivation: 在水处理领域，模糊规则基系统的可解释性至关重要，但现有方法常因模糊集区分度低而损害语义可解释性。

Method: 采用专家驱动的网格划分生成高区分度隶属函数、领域引导的特征工程降低冗余、基于激活强度的规则剪枝。

Result: 所提方法在预测性能上与聚类型FRBS相当，同时满足结构复杂度约束并保持语义可解释性。

Conclusion: 该人机协同方法为水处理应用提供了兼顾预测精度与可解释性的实用解决方案。

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [578] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: 本文提出EpiFlow框架，将安全离线强化学习建模为状态约束的最优控制问题，通过epigraph重构导出可行性价值函数，并结合流匹配生成策略，在保证数据分布一致性的同时实现高安全性与高性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线RL方法常依赖软约束、过于保守或难以兼顾安全性、性能和数据分布一致性，亟需一种能联合优化安全与性能的新范式。

Method: 提出Epigraph-Guided Flow Matching（EpiFlow）：1）将问题建模为状态约束最优控制；2）基于epigraph重构学习可行性价值函数；3）利用该价值函数对行为分布重加权，并通过流匹配拟合生成式策略。

Result: 在Safety-Gymnasium等安全关键任务上，EpiFlow取得具有竞争力的回报率，同时实证安全违规率接近零。

Conclusion: EpiFlow通过epigraph引导的策略合成机制，有效统一了安全性保障与性能优化，且保持分布一致性，为安全离线RL提供了新思路。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [579] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分推断的上下文内奖励建模方法（ICRM），使奖励模型在测试时能通过少量偏好示例动态调整，提升单目标与多目标对齐性能，并在安全、数学推理等任务上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有分类器式奖励模型（RM）训练后固定不变，难以适应测试时多样、复杂或未见过的人类偏好分布，尤其在可验证奖励和多目标对齐等新兴RL场景中表现受限。

Method: 提出变分上下文内奖励建模（ICRM），将奖励建模建模为在Bradley-Terry模型下、以共轭Beta先验对潜在偏好概率进行摊销变分推断；利用上下文中的偏好示例实现测试时可引导的贝叶斯更新。

Result: 在SafeRLHF和RM-Bench单目标任务上分别提升34%和9%准确率；多目标下帕累托前沿超体积提升4%；在数学推理任务中超越基线RM；理论证明目标函数存在有限置信度下的全局内点最优解，且KL正则化可缓解奖励过优化。

Conclusion: ICRM为奖励建模提供了可扩展、可适应、有理论保障的贝叶斯框架，显著增强了语言模型在动态、多维人类偏好下的对齐能力。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [580] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: 本文提出了一种面向边缘设备的推测解码（SD）优化方法，通过分析型成本模型指导粗粒度子图划分，联合利用异构硬件资源，在短序列场景下实现最高1.68×加速。


<details>
  <summary>Details</summary>
Motivation: LLM在资源受限的边缘设备上部署面临严重延迟问题，尤其是实时应用；现有推测解码（SD）在边缘落地受限于编译流程集成难与异构硬件资源利用不足两大挑战。

Method: 构建一个分析型成本模型，用于探索异构硬件配置并指导LLM子图的粗粒度划分，特别适配边缘场景下的短输入序列；在含六核Cortex-A CPU和Mali GPU的边缘设备上验证该方法。

Result: 在翻译任务中实现最高1.68×的端到端加速，实测结果与模型预测高度吻合。

Conclusion: 联合推测采样与异构执行在边缘短序列场景下具有显著增益，所提成本模型可有效指导编译器级SD部署与硬件协同优化。

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [581] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

TL;DR: 本文提出了一种从训练好的Transformer模型中提取可解释RASP程序的通用方法，通过忠实重参数化和因果干预，实验证明该方法能成功恢复出简洁、可解释的RASP程序，为Transformer内部实现简单RASP程序提供了最直接证据。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明Transformer可被RASP语言模拟，且长度泛化能力与简单RASP程序相关，但尚不清楚实际训练出的模型是否真在内部实现了这类可解释程序。

Method: 将Transformer忠实重参数化为RASP程序，并施加因果干预以发现最小充分子程序。

Result: 在小规模Transformer处理算法任务和形式语言任务的实验中，该方法常能成功恢复出简洁、可解释的RASP程序。

Conclusion: 结果首次提供了最直接的证据，表明长度泛化的Transformer确实在内部实现了简单RASP程序。

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [582] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

TL;DR: 本文提出BAGEL框架，一种轻量级、模块化且可增量更新的恶意提示检测方法，通过集成多个细调模型并使用随机森林路由器选择最优成员，在保持高效和可解释性的同时，实现了优于大型API的性能。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在透明度低、适应性差或计算成本高等问题，难以兼顾性能、效率与适应性。

Method: BAGEL采用自举聚合与专家混合集成策略，每个子模型在不同攻击数据集上微调；推理时由随机森林路由器选择最适成员，并结合随机采样进行预测聚合；新增攻击类型时仅需微调一个小型安全分类器（86M参数）并加入集成。

Result: BAGEL在仅选用5个集成成员（共430M参数）时达到0.92 F1分数，优于OpenAI Moderation API和ShieldGemma等需数十亿参数的方案；经9次增量更新后性能仍稳健，并提供路由结构特征支持可解释性。

Conclusion: 小规模微调分类器的集成可在性能、效率与适应性三方面超越超大规模守门模型，适用于实际部署场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [583] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输、非线性优化与浓度不等式的新型算法框架，用于从有限样本中近似未知概率分布，并以高置信度高效地给出Wasserstein距离的非渐近、易计算误差界。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离是衡量概率分布间距离的关键指标，但在实际中如何从有限样本中非渐近、可计算地学习未知分布并保证误差界，仍是一个基础而具挑战性的问题。

Method: 结合最优传输理论、非线性优化与浓度不等式，构造离散近似分布；将Wasserstein误差上界转化为一个仅依赖于近似分布支撑集大小的可解混合整数线性规划（MILP）问题；并设计智能聚类算法联合优化支撑点选择与误差界。

Result: 在多个基准测试中，该方法显著优于现有同类方法：所得近似分布支撑集更小、Wasserstein误差界更紧。

Conclusion: 本文建立了首个能以高置信度、非渐近且计算可行方式控制Wasserstein逼近误差的通用框架，为分布学习与近似提供了理论保障与实用算法。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [584] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 本文提出了一种结合行为评估与可解释性分析的框架，用于评估智能体的目标导向性，并以在2D网格世界中导航的LLM智能体为案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 缺乏可靠地将目标归因于智能体系统的方法，需建立系统性评估框架。

Method: 结合行为评估（对比最优策略在不同任务难度下的表现）与可解释性分析（使用探针解码内部状态和多步规划表征）。

Result: LLM智能体非线性编码粗粒度空间地图，其动作与内部表征基本一致；推理过程使表征从环境结构线索转向即时动作支持信息。

Conclusion: 仅靠行为评估不足以刻画智能体如何表征与追求目标，需结合内省式分析。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [585] [Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations](https://arxiv.org/abs/2602.08067)
*Chenglei Shen,Yi Zhan,Weijie Yu,Xiao Zhang,Jun Xu*

Main category: cs.LG

TL;DR: 本文提出HyperBandit+，一种结合时间感知超网络与大语言模型辅助热启动机制的上下文赌博机策略，用于提升流式推荐系统中动态用户偏好的建模能力及早期在线阶段的探索-利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于赌博机的方法忽略时间与用户偏好的显式关系，且在线学习早期探索-利用效率低，导致性能不佳。

Method: 提出HyperBandit+：1）时间感知超网络，以时间特征为输入生成时变奖励估计参数；2）LLM Start热启动机制，通过多步数据增强生成模拟交互数据进行离线预训练；3）采用低秩分解降低超网络训练复杂度；4）理论分析给出含超网络与LLM热启动的亚线性遗憾上界。

Result: 在多个真实世界数据集上，HyperBandit+在累积奖励上持续优于当前最优基线方法。

Conclusion: HyperBandit+有效建模时间演化用户偏好，并通过LLM辅助热启动显著提升早期在线学习效率，在理论和实验上均验证其优越性。

Abstract: In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.

</details>


### [586] [Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense](https://arxiv.org/abs/2602.09012)
*Jiacheng Liu,Yaxin Luo,Jiacheng Cui,Xinyi Shang,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文提出Next-Gen CAPTCHAs框架，利用人机在交互感知、记忆、决策与行动上的认知差距，通过动态任务设计重建人机区分，以应对先进多模态AI代理对传统CAPTCHA的高破解率。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA已被先进GUI代理（如Gemini3-Pro-High、GPT-5.2-Xhigh）大幅攻破（高达90%通过率），亟需新型可扩展防御机制。

Method: 构建基于鲁棒数据生成流水线的Next-Gen CAPTCHAs框架，支持大规模、可扩展、后端驱动的动态CAPTCHA生成，强调适应性直觉而非精细规划。

Result: 成功利用人机‘认知差距’设计出难以被当前推理型多模态代理破解的动态CAPTCHA任务，实现对生物用户与AI代理的鲁棒区分。

Conclusion: Next-Gen CAPTCHAs为下一代Web提供了可扩展、多样化且面向代理时代的有效安全防御方案。

Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

</details>


### [587] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: 本文提出mmSIVAE模型，结合软自省变分自编码器与MOPOE后验聚合方法，提升阿尔茨海默病多模态神经影像规范建模中的健康参考分布拟合精度与跨模态融合效果，并通过潜空间与特征空间偏差评分实现可解释的区域异常定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE的规范建模在阿尔茨海默病中存在两方面问题：(i) 健康参考分布拟合不准确，导致假阳性率高；(ii) 后验聚合（如PoE/MoE）导致共享潜空间中多模态融合薄弱。

Method: 提出mmSIVAE——一种多模态软自省变分自编码器，结合Mixture-of-Product-of-Experts（MOPOE）进行后验聚合；在潜空间和特征空间分别计算到健康分布的距离作为偏差评分，并将显著潜变量偏差映射至解剖区域以增强可解释性。

Result: 在ADNI数据集（MRI区域体积+淀粉样蛋白PET SUVR）上，mmSIVAE在对照组重建效果更优，偏差评分对异常检测更具判别力（更高似然比、更清晰的对照组与AD谱系分离），且生成的偏差图符合已知AD区域变化模式。

Conclusion: 提升参考分布保真度与稳健的多模态后验聚合对规范建模至关重要，该框架可推广至其他多模态临床数据分析中的偏差驱动研究。

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [588] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的基于注意力拓扑谱分析的安全防护方法，用于检测大模型工具使用失败和幻觉现象，在多个模型上实现了高召回率与精度，揭示了幻觉具有类似热力学状态变化的谱特征，并发现了不同模型间故障可检测性的显著差异。


<details>
  <summary>Details</summary>
Motivation: 部署自主智能体需要可靠的安全保障以防止工具使用失败，而现有监督式方法依赖标注数据，泛化性和实用性受限。

Method: 提出一种无需训练的守卫机制，基于对模型注意力机制拓扑结构的谱分析（如平滑性、熵等单层频谱特征），在推理时实时检测异常注意力模式。

Result: 在Llama 3.1 8B上实现97.7%召回率（多特征）和86.1%召回率/81.0%精度（平衡部署）；单层谱特征（如Llama L26平滑性、Mistral L3熵）可分别达98.2%和94.7%幻觉召回率；发现'Loud Liar'现象：Llama故障谱特征剧烈易检，Mistral 7B判别AUC达0.900。

Conclusion: 谱分析是一种原理清晰、高效实用的代理安全新范式，将幻觉建模为注意力噪声态的热力学转变，为无监督安全检测提供了坚实基础。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [589] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

TL;DR: 本文探讨了如何通过机器学习增强反无人机系统（C-UAS）的信号处理能力，并引入‘概率黑客’概念，提出基于场景的方法识别相关风险与法律规制需求，以提升C-UAS在军民场景中人机协同所需的可信度与正当信任。


<details>
  <summary>Details</summary>
Motivation: 为有效应对无人机系统（UAS）带来的多样化威胁，需发展具备新兴颠覆性技术（如AI/ML）支撑的反无人机系统（C-UAS），同时防范其可能引发的新型风险（如‘概率黑客’）并保障法治框架下的可信运行。

Method: 采用基于场景的方法，将机器学习（ML）融入C-UAS以增强信号处理能力，并据此界定‘概率黑客’挑战，进而推导出适配现有法治机制的规范性要求。

Result: 识别出若干可嵌入现行法治体系的、用于防范概率黑客的具体要求，从而提升C-UAS系统的可信度，支撑人-自主系统协同中所需的‘正当信任’。

Conclusion: 将ML等EDTs融入C-UAS不仅提升技术效能，更需同步构建以场景为驱动的治理框架；唯有兼顾技术能力与制度可信，才能实现军民领域中安全、合法且高效的人-自主协同。

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [590] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 本文提出了一种名为Online Domain-aware Decoding（ODD）的在线领域感知解码框架，用于应对大语言模型在动态演化领域中因概念漂移导致性能下降的问题。ODD通过在概率层面融合基础LLM与前缀树先验，并利用基于分歧和连续性信号的自适应置信度调制实现无需重训练的实时适配。实验表明其在多种漂移场景下显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM通常离线微调于静态领域数据，难以应对现实中持续演化的领域知识（如新规、新产品、交互模式变化）及概念漂移问题；频繁重训练计算开销巨大，亟需高效、实时的在线适配机制。

Method: 提出ODD框架：在解码阶段进行概率级融合，将基础LLM输出与基于前缀树构建的领域先验结合，并引入自适应置信调制模块，利用模型输出分歧度和上下文连续性信号动态调整融合权重。

Result: 在多种概念漂移场景下，ODD在所有句法与语义NLG指标上均超越LLM-Greedy和LLM-Temp Scaled；ROUGE-L绝对提升0.065，余弦相似度相对提升13.6%。

Conclusion: ODD能有效增强LLM对词汇与上下文动态演化的鲁棒性，为动态环境下的LLM部署提供了轻量、实时、无需重训练的适配新范式。

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [591] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

TL;DR: 本文提出一种基于信息瓶颈框架的潜在表示维度估计方法，通过神经互信息估计器来确定任务相关维度，并引入混合判别器以避免维度高估，同时设计单次评估协议提升效率。


<details>
  <summary>Details</summary>
Motivation: 估计预测所需潜在表示的维度（即任务相关维度）是一个困难且尚未解决的问题，具有广泛的科学应用价值。

Method: 将该问题建模为信息瓶颈问题，利用神经互信息估计器进行维度估计；提出一种保留显式维度瓶颈但支持非线性跨视图交互的混合判别器；设计无需遍历不同瓶颈尺寸的一次性评估协议。

Result: 在已知任务相关维度的合成数据上验证了方法的有效性；扩展至本征维度估计并与经典几何方法对比，在噪声环境下表现更稳健；在多个物理数据集上展示了实用性。

Conclusion: 所提方法能准确、鲁棒地估计任务相关及本征维度，尤其适用于含噪场景，为高维数据分析提供了新工具。

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [592] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出Variance-Gated Ensembles（VGE），一种可微、直观的框架，通过基于集成统计的信噪门控机制注入认知不确定性敏感性，包含VGMU评分与VGN归一化层，支持端到端训练，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 传统加性分解不确定性（aleatoric与epistemic）在有限集成采样或预测分布不匹配时失效，亟需更鲁棒、可训练的认知不确定性估计方法。

Method: 提出VGE框架，包括：(i) Variance-Gated Margin Uncertainty（VGMU）评分，耦合决策边界与集成预测方差；(ii) Variance-Gated Normalization（VGN）层，实现每类可学习的概率归一化；并推导闭式向量-雅可比乘积以支持对集成均值与方差的端到端训练。

Result: VGE在保持计算高效的同时，性能达到或超越现有信息论基线方法，并提供开源实现。

Conclusion: VGE是一种实用、可扩展的集成模型认知感知不确定性估计新范式。

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [593] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种NormalHedge的变体，用于专家预测问题，在“简单”序列上实现了基于二阶矩的ε-分位数遗憾界，并通过随机微分方程连续时间极限和自协调性技术进行分析。


<details>
  <summary>Details</summary>
Motivation: 针对“容易”的序列，提升专家预测中ε-分位数遗憾界的紧致性，尤其是利用序列内在的二阶统计特性。

Method: 提出NormalHedge的变体，结合随机微分方程（SDE）的连续时间建模思想，并在离散时间分析中运用自协调函数（self-concordance）技术。

Result: 获得第二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T为瞬时每专家遗憾的累积二阶矩（按算法自然分布加权平均），且当V_T > log N时成立。

Conclusion: 该算法在易处理序列上显著提升了遗憾界，理论分析融合了连续与离散视角，拓展了在线学习中二阶自适应方法的适用性。

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [594] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 本文提出一个模块化的因果机器学习框架，用于分别评估治疗效应异质性的统计显著性与临床决策价值，并在UC患者试验中验证：内镜特征虽与异质性显著相关，但无法提升治疗决策效果，而临床变量（如粪便钙卫蛋白、年龄、CRP）更利于指导治疗选择。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验估计平均处理效应，但临床实践中需关注个体化治疗；关键问题是：统计上可检测的异质性是否真能改善治疗决策？二者可能矛盾，需分别评估。

Method: 提出三步模块化因果机器学习框架：1）置换重要性识别预测异质性的特征；2）最佳线性预测器（BLP）检验异质性的统计显著性；3）双重稳健策略评估（doubly robust policy evaluation）衡量利用异质性是否提升患者结局。在UNIFI维持试验数据上，采用交叉拟合X-learner模型，输入包括基线人口学、用药史、第8周临床评分、实验室指标及视频内镜特征。

Result: BLP检验发现内镜特征与乌司奴单抗vs安慰剂的治疗效应异质性显著相关；但双重稳健策略评估显示，纳入内镜特征未提升预期缓解率，且多臂留出验证表现更差；诊断分析表明内镜评分主要反映疾病严重程度（对未治疗者预测好），却干扰治疗选择；而粪便钙卫蛋白、年龄、CRP等临床变量更有效捕捉决策相关变异。

Conclusion: 因果机器学习应用于临床试验时，除异质性检验外，必须包含策略层面（policy-level）的效果评估，避免将纯预测性生物标志物误用为治疗选择依据。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [595] [Nansde-net: A neural sde framework for generating time series with memory](https://arxiv.org/abs/2602.08182)
*Hiromu Ozai,Kei Nakagawa*

Main category: cs.LG

TL;DR: 本文提出了一种新型的神经网络驱动的ARMA型噪声（NA-noise），作为分数布朗运动的Itô相容替代，用于构建具有长/短记忆建模能力的神经随机微分方程（NANSDE-Net），在保持理论严谨性与计算可行性的同时提升时序生成性能。


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动虽能刻画长/短记忆，但不兼容Itô微积分，限制其在神经SDE中的应用；亟需一种既具记忆建模能力又满足Itô框架的噪声构造方法。

Method: 提出NA-noise：用神经网络参数化核函数，并将其分解为乘积形式以保持Markov性；基于此构建NANSDE-Net模型；给出解的存在唯一性证明，并设计高效反向传播训练算法。

Result: 在合成与真实数据集上，NANSDE-Net在复现长/短记忆特征方面达到或优于fractional SDE-Net等基线，同时维持Itô框架下的计算可处理性。

Conclusion: NA-noise为神经SDE提供了兼具记忆建模能力与Itô相容性的新噪声范式，NANSDE-Net在理论和实验上验证了其有效性与实用性。

Abstract: Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.

</details>


### [596] [Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso](https://arxiv.org/abs/2602.08197)
*Shingo Higashiguchi,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文提出Kronecker Time-Varying Graphical Lasso (KTVGL)方法，用于建模多模态张量时间序列，通过Kronecker积形式估计各模态的动态网络结构，提升可解释性与计算效率，并支持流式处理。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多变量时间序列常以张量形式存在，其动态依赖关系建模面临网络结构复杂、计算成本高、可解释性差等挑战。

Method: 提出KTVGL方法，利用Kronecker积分解方式建模各模态的时变图结构，结合时变图Lasso优化，并扩展为流式算法以降低时间复杂度。

Result: 在合成数据上，KTVGL在边估计精度上优于现有方法，且计算时间更少；在真实数据案例研究中验证了其实际有效性。

Conclusion: KTVGL能高效、可解释地建模高维张量时间序列的动态依赖结构，兼具理论合理性与实用价值。

Abstract: With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.

</details>


### [597] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 本文指出基于热图的组合优化求解器在监督学习范式下存在目标不匹配问题，提出CADO框架，通过强化学习直接优化解的质量，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 监督学习训练范式中最小化模仿损失（如交叉熵）无法保证解成本最小化，存在解码盲区和成本盲区两大缺陷。

Method: 提出CADO（面向优化的成本感知扩散模型），将扩散去噪过程建模为马尔可夫决策过程（MDP），引入标签中心奖励（以真实标签为无偏基线）和混合微调策略进行强化学习微调。

Result: CADO在多个基准测试中达到最先进（SOTA）性能，验证了目标对齐对提升热图求解器性能的关键作用。

Conclusion: 目标对齐是释放热图型求解器全部潜力的关键；监督学习的固有缺陷限制了性能上限，而强化学习微调可有效克服该限制。

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [598] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文提出了一种基于共形预测（conformal prediction）的无分布假设不确定性量化方法，用于神经算子（neural operators）求解偏微分方程（PDEs）的决策任务，并结合无限维Danskin定理与变分法实现鲁棒决策优化，在多个工程任务中优于高斯过程等方法。


<details>
  <summary>Details</summary>
Motivation: 神经算子虽能高效替代传统数值方法求解PDE以支持重复决策，但缺乏校准的不确定性估计；现有不确定性方法（如集成或贝叶斯推断）或依赖不现实的分布假设，或难以扩展。

Method: 将共形预测推广至函数空间，为神经算子输出构造分布无关的预测集；在下游鲁棒决策中引入形式化遗憾刻画，并利用无限维Danskin定理和变分法实现高效优化。

Result: 所提方法在多个工程任务中展现出比高斯过程等受限建模范式更优的性能，且具备可证明的统计保证与计算可行性。

Conclusion: 共形预测为神经算子提供了实用、可扩展且无需分布假设的不确定性量化框架，并能支撑具有理论保障的鲁棒决策。

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [599] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理原理的信息动力学框架，将Transformer中的注意力机制视为受最小作用量原理支配的物理系统，通过Fisher信息度量构建黎曼流形，并导出智能拉格朗日量；softmax被解释为信息气体的热力学平衡态（最小亥姆霍兹自由能），查询-键交互类比为电磁耦合；进而建立信息热力学第一定律，并用相变理论解释缩放律与'顿悟'现象，最后从对称性破缺角度给出RoPE的场论解释。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽成功但缺乏统一的物理理论基础，其机制仍属启发式；亟需从第一性原理出发，为深度学习建立可解释、可预测的物理基础。

Method: 将信息状态映射到以Fisher信息为度量的黎曼流形，推导出智能拉格朗日量；将softmax解释为信息气体的热力学平衡态（最小自由能）；将query-key交互建模为电动力学中的外场-偶极子耦合；引入信息热力学第一定律；用相变理论分析缩放律与grokking；利用流形上的旋转对称性破缺分析RoPE。

Result: 建立了首个信息热力学第一定律；解释softmax为热力学平衡态；揭示query-key交互的电动力学本质；将缩放律和grokking归因于特定热容发散的相变；为RoPE提供场论视角（Goldstone玻色子）。

Conclusion: 本工作首次将统计物理与深度学习深度联结，为人工智能提供了基于物理原理的统一理论框架，有望推动可解释AI与物理引导的模型设计。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [600] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏性感知的进化（SAE）框架用于模型融合，通过迭代剪枝-融合循环作为新型变异算子，并将稀疏性约束融入评分函数，从而提升融合模型的可靠性与稀疏性。


<details>
  <summary>Details</summary>
Motivation: 提升模型融合的可靠性与稀疏性，同时保持方法简单且与其他现有方法正交。

Method: 提出稀疏性感知的进化（SAE）框架，结合迭代剪枝-融合循环作为变异操作，并在评分函数中引入稀疏性约束；利用稀疏性竞争引发的局部吸引力增强进化过程。

Result: 在多个大规模大语言模型基准上验证了该方法能提升模型融合的可靠性，且易于集成。

Conclusion: SAE框架是一种简单、有效且正交的模型融合方法，能兼顾性能与稀疏性，并增强融合过程的鲁棒性。

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [601] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: 本文提出SkillRL框架，通过自动技能发现与递归演化，将原始经验提炼为可复用的高层技能库，显著提升LLM智能体在复杂任务中的泛化性与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体缺乏从历史经验中学习的能力，内存方法存储冗余噪声多的原始轨迹，难以提取高阶可复用行为模式。

Method: 提出SkillRL框架，包含基于经验的蒸馏机制构建分层技能库SkillBank、自适应检索策略用于通用与任务特定启发式选择、以及技能库与策略协同进化的递归演化机制。

Result: 在ALFWorld、WebShop及七个搜索增强任务上达到SOTA，性能超越强基线15.3%以上，且随任务复杂度增加保持鲁棒性。

Conclusion: SkillRL有效弥合了原始经验与策略改进之间的鸿沟，降低了token开销，提升了推理实用性，为LLM智能体持续学习提供了新范式。

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [602] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

TL;DR: 本文通过线性化视角分析参数高效微调（PEFT）机制，揭示其训练动力学等价于正定神经正切核（NTK）学习，并建立NTK谱特性与微调性能间的强关联，进而给出层选择对NTK的谱扰动界，实验验证于LoRA。


<details>
  <summary>Details</summary>
Motivation: Parameter-Efficient Fine-Tuning（PEFT）方法虽广泛应用，但其训练性能与泛化能力的内在机制尚不清晰，亟需理论刻画。

Method: 引入参数空间欧氏距离归纳偏置，显式约束微调模型靠近预训练模型，从而将微调动力学建模为NTK学习；分析全线性与线性化优化的近似程度；推导所选微调层对NTK的谱扰动界；在LoRA上进行实证验证。

Result: 发现NTK特征值谱与微调性能存在强相关性；给出了基于层选择的NTK谱扰动上界；在线性化良好的前提下，理论与LoRA实验结果一致。

Conclusion: 线性化是理解PEFT的有效视角，NTK谱分析可指导更优的参数高效微调设计，提升大语言模型适配的效率与鲁棒性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [603] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文提出了一种新的无需显式奖励信号的上下文强化学习范式——基于偏好的上下文强化学习（ICPRL），仅依赖偏好反馈进行预训练和部署，并设计了两种反馈粒度变体（每步偏好I-PRL与轨迹级偏好T-PRL），在多个任务上实现了与有奖励监督的ICRL方法相当的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习（ICRL）方法依赖显式奖励信号进行预训练，但在奖励模糊、难以定义或获取成本高时适用性受限。

Method: 提出基于偏好的上下文强化学习（ICPRL）范式，包含两个变体：每步偏好（I-PRL）和轨迹级偏好（T-PRL）；在偏好数据集上验证监督预训练可行性，并设计偏好原生框架，直接从偏好数据优化Transformer策略，无需奖励信号或最优动作标签。

Result: 在dueling bandits、导航和连续控制任务上的实验表明，ICPRL能实现强上下文泛化能力，性能媲美使用完整奖励监督训练的ICRL方法。

Conclusion: ICPRL成功摆脱对显式奖励的依赖，仅用偏好反馈即可支撑上下文强化学习，拓展了ICRL在现实稀疏/模糊奖励场景中的应用潜力。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [604] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: 本文提出PRO-Bid框架，通过约束解耦的帕累托表征（CDPR）和反事实遗憾优化（CRO）机制，解决自动出价中状态混淆与历史平均限制问题，在满足严格效率约束（如目标CPA）的同时提升营销价值。


<details>
  <summary>Details</summary>
Motivation: 现有基于Decision Transformer的自动出价方法在处理Target CPA等严格效率约束时存在状态混叠（忽略成本维度）和受限于历史平均行为两大问题，难以精准资源调控并逼近约束边界。

Method: 提出PRO-Bid：1）Constraint-Decoupled Pareto Representation（CDPR）将全局约束递归分解为成本与价值上下文，并基于帕累托前沿重加权轨迹；2）Counterfactual Regret Optimization（CRO）利用全局结果预测器识别更优反事实动作，并将其作为加权回归目标。

Result: 在两个公开基准及线上A/B测试中，PRO-Bid相较SOTA基线显著提升约束满足率与价值获取能力。

Conclusion: PRO-Bid是一种约束感知的生成式自动出价框架，能有效建模资源动态、突破历史平均局限，实现高效率约束下的价值最大化。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [605] [Inverting Data Transformations via Diffusion Sampling](https://arxiv.org/abs/2602.08267)
*Jinwoo Kim,Sékou-Oumar Kaba,Jiyun Park,Seunghoon Hong,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出了一种在一般李群上进行变换逆推的新方法TIED，通过在李代数中定义扩散过程并利用新提出的平凡化目标得分恒等式，实现对变换后验分布的高效采样，显著提升了预训练神经网络在测试时对输入变换的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 未知的群变换在机器学习和科学建模中广泛存在，会严重扭曲观测数据，亟需一种能恢复原始数据分布的逆变换方法。

Method: 将变换后验建模为数据空间上的Boltzmann分布，并设计一种保持流形结构的李群扩散过程；引入平凡化目标得分恒等式，支持基于分数的高效采样。

Result: 在图像射影变换和PDE对称性等任务上，TIED能有效将变换后的输入恢复至训练分布，在测试时提升模型鲁棒性，性能优于强基线方法。

Conclusion: TIED提供了一种通用、可微、流形保持的变换逆推框架，为测试时等变性与数据标准化开辟了新路径。

Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.

</details>


### [606] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文通过PAC框架严格分析了多智能体强化学习（MARL）与单智能体强化学习（SARL）在大语言模型（LLM）训练中的样本效率差异，指出MARL仅在任务天然可分解为独立子任务时提升样本复杂度，而任务依赖性或对齐偏差会削弱其优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对MARL为何及何时优于SARL的理论解释，导致实践中RL框架选择存在不确定性。

Method: 基于PAC框架，形式化定义LLM下的SARL与MARL设定，推导样本复杂度上界，并系统刻画任务分解结构与任务对齐程度对学习效率的影响。

Result: 证明MARL在子任务独立时可降低样本复杂度；子任务依赖时优势减弱；提出‘任务对齐’概念并量化强制独立分解带来的权衡。

Conclusion: 理论结果解释了经验现象的不一致性，并为LLM中MARL的实际部署提供了可操作的判断准则。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [607] [Noise Stability of Transformer Models](https://arxiv.org/abs/2602.08287)
*Themistoklis Haris,Zihan Zhang,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 本文提出噪声稳定性（noise stability）作为衡量深度学习模型简洁性的新指标，以克服平均敏感度（average sensitivity）在实值域泛化和解释LLM输入依赖性方面的不足；理论分析了单层注意力与ReLU MLP的噪声稳定性，并设计了协方差区间传播方法处理多层传播；基于此提出噪声稳定性正则化方法，在算法任务和下一词预测任务中分别加速训练约35%和75%，并促进‘grokking’现象。


<details>
  <summary>Details</summary>
Motivation: 平均敏感度难以推广到实值域，且无法解释现代大语言模型中观察到的‘junta-like’输入依赖特性，因此需要更合适的简洁性度量。

Method: 提出噪声稳定性作为新简洁性指标；理论分析单层注意力与ReLU MLP的噪声稳定性；采用协方差区间传播方法解决多层噪声稳定性传播问题；设计噪声稳定性正则化方法用于训练。

Result: 所提正则化方法在算法任务和下一词预测任务中分别将训练速度提升约35%和75%，并稳定促进grokking；建立了神经网络信号传播与可解释性之间的新联系。

Conclusion: 噪声稳定性是一种更全面、更具理论基础和实用价值的简洁性度量，为理解和改进现代Transformer模型提供了新工具和新视角。

Abstract: Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the "junta-like" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\%$ and $75\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.

</details>


### [608] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

TL;DR: 本文提出了一种基于信任的激励机制，用于评估和奖励联邦学习（FL）中各参与方的贡献质量，并结合区块链与智能合约实现自动化、透明、去中心化的信任评估与激励分配，以提升FL系统的鲁棒性、公平性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在恶意或故障节点，威胁系统完整性与模型性能，亟需一种可靠机制来识别并激励诚实参与者、抑制不可靠行为。

Method: 设计动态信任评分机制（基于数据质量、模型精度、一致性、贡献频率等维度），构建基于信任分的激励机制（高信任节点获更多参与权，低信任者受惩罚），并引入区块链与智能合约实现自动化与去中心化执行。

Result: 提出一个理论框架，支持更鲁棒、公平、透明的联邦学习生态系统，有效降低不可信参与者的风险。

Conclusion: 基于信任的激励机制结合区块链技术，可显著增强联邦学习系统的安全性、可信度与协作效率。

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [609] [Grokking in Linear Models for Logistic Regression](https://arxiv.org/abs/2602.08302)
*Nataraj Das,Atreya Vedantam,Chandrashekar Lakshminarayanan*

Main category: cs.LG

TL;DR: 本文研究了线性模型在二分类任务中出现的'grokking'（延迟泛化）现象，发现即使没有深度网络或表征学习，仅靠梯度下降的隐式偏差和偏置项动力学即可引发该现象，并从理论和实验上揭示了其三阶段学习过程及与数据不对称性的关系。


<details>
  <summary>Details</summary>
Motivation: 探究grokking现象是否必须依赖深度网络和组合结构，尝试在最简单的线性模型中复现并理解其本质机制。

Method: 理论分析梯度下降在逻辑损失下的隐式偏差，推导三阶段学习动态；设计三种测试场景（同分布、边缘集中、对抗样本）；通过控制支持向量与总体样本分布进行实验验证。

Result: 发现grokking可在无深度的线性模型中出现；揭示了population-dominated、SV-dominated unlearning、SV-dominated generalization三阶段；指出数据类别数量与支持向量分布的不对称性影响grokking发生及其时间。

Conclusion: grokking并非深度网络特有现象，而是优化动态与数据结构共同作用的结果，尤其依赖偏置项演化和数据不对称性。

Abstract: Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.

</details>


### [610] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 本文提出TextResNet框架，通过加性语义增量、语义梯度分解、因果路由和密度感知优化调度四项创新，解决文本梯度式优化器在深层AI系统中因语义纠缠导致的归因模糊问题，显著提升性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: Textual Gradient-style优化器（TextGrad）在深层AI链中表现不佳，根源在于语义纠缠问题，导致反馈信号混杂、归因模糊。

Method: 提出TextResNet框架：1）前向传播中采用加性语义增量以保留恒等通路；2）反向传播中通过语义投影器实现语义梯度分解；3）实施因果路由将信号精准分配至对应组件；4）进行密度感知优化调度，动态调配资源至关键瓶颈。

Result: TextResNet在性能上优于TextGrad，并在基线方法崩溃的复合AI智能体任务中展现出卓越稳定性。

Conclusion: TextResNet通过结构化信号路由有效缓解语义纠缠，为深层文本优化提供了更鲁棒、可扩展的范式。

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [611] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 本文提出了一种适用于多步交互场景的Interaction-Grounded Learning（IGL）新算法，扩展了现有单步IGL方法至上下文化阶段性MDP，并设计了基于逆间隙加权（IGW）的策略优化方法，在合成与真实数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有IGL方法仅适用于单步设置，难以应用于如多轮LLM部署等现代序列决策系统，亟需扩展至多步、个性化反馈场景。

Method: 将Zhang等人（2024a）的单步奖励估计器扩展至多步MDP设定，构建新型潜奖励解码机制，并在此基础上设计逆间隙加权（IGW）策略优化算法。

Result: 提出首个在上下文化阶段性MDP中具备次线性遗憾保证的高效IGL算法，并在合成MDP和真实用户预订数据集上验证了其学习个性化目标的有效性。

Conclusion: 本工作成功将IGL范式从单步推广至多步序列决策场景，为无显式奖励的现实交互式学习（如LLM对齐）提供了理论支持与实用算法。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [612] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于流匹配的条件独立性检验方法FMCIT，以加速约束型因果发现中的CI测试；并将其集成到两阶段引导式PC骨架学习框架GPC-FMCIT中，在保证统计功效的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 约束型因果发现方法因需大量条件独立性（CI）检验而计算开销大、实用性受限，亟需加速单个CI检验。

Method: 提出基于流匹配的条件独立性检验（FMCIT），利用流匹配的高效性且仅需一次模型训练；进一步构建两阶段引导式PC算法GPC-FMCIT，结合快速筛选与预算可控的FMCIT精炼。

Result: FMCIT在高维条件下仍能有效控制I类错误、保持高检验功效；GPC-FMCIT在合成与真实数据上展现出优于现有CI检验和PC变体的精度-效率权衡。

Conclusion: FMCIT及其集成框架GPC-FMCIT为高效、可靠的因果结构学习提供了新范式，兼顾统计严谨性与计算可行性。

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [613] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: 本文提出Extra-CoT框架，通过语义保持压缩与分层强化学习，在大幅压缩CoT推理token的同时提升数学推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法在高压缩比下逻辑保真度低、性能显著下降，亟需兼顾高保真与高效率的推理压缩方案。

Method: 提出EXTreme-RAtio Chain-of-Thought Compression（Extra-CoT）：1）基于细粒度标注的数学CoT数据训练语义保持压缩器；2）混合压缩比监督微调（SFT）LLM；3）引入约束性分层比率策略优化（CHRPO）强化学习，以分层奖励显式激励低预算下的解题能力。

Result: 在MATH-500等三个数学推理基准上显著优于SOTA；Qwen3-1.7B模型实现超73% token压缩，准确率反升0.6%。

Conclusion: Extra-CoT实现了高压缩比与高逻辑保真度的统一，为高效、可靠的大模型推理提供了新范式。

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [614] [Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference](https://arxiv.org/abs/2602.08329)
*Yifei Gao,Lei Wang,Rong-Cheng Tu,Qixin Zhang,Jun Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了一种名为Pre-hoc Sparsity（PrHS）的KV缓存稀疏化方法，通过在注意力打分前进行预选择，避免后验偏差，提供可验证的精度控制，并在多个基准上显著提升推理效率与吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏KV缓存方法依赖后验启发式（如基于已计算注意力分数的选择），易引入后验偏差，损害长程推理能力。

Method: 提出PrHS框架，在注意力打分前进行KV选择；基于边际到互信息分析，推导出互信息损失仅依赖于丢弃的质量（dropped mass delta）；设计沿时间、深度和层三个正交维度的预选择器。

Result: 在GSM8K和CoQA上检索开销降低超90%，稀疏度达HShare的3倍且精度相当或更优；LongBench平均退化<1%；注意力FLOPs比先前稀疏基线低约15%；A100 GPU上注意力算子延迟降低9.9倍，吞吐量提升2.8倍。

Conclusion: PrHS通过预选择机制与理论保证，有效平衡了KV缓存稀疏化中的效率与精度，为LLM高效推理提供了新范式。

Abstract: A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.

</details>


### [615] [Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training](https://arxiv.org/abs/2602.08333)
*Cristian Pérez-Corral,Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 本文研究了ReLU网络训练过程中的双时间尺度行为，发现激活模式变化早于权重更新变化，表明训练后期主要在稳定的激活区域内进行微调。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络（DNN）经验上成功，但其内部训练动态仍难以刻画；受ReLU模型中输入决定分段线性区域这一几何特性的启发，作者探究训练是否呈现两阶段：早期激活模式大幅变化，晚期权重更新主要在稳定激活区域内进行精调。

Method: 理论方面证明了参数扰动下激活模式的局部稳定性；实验方面在全连接、卷积及Transformer模型中，追踪固定验证集上的每步权重更新与激活模式变化。

Result: 激活模式变化衰减时间比权重更新幅度早约3倍，表明训练后期常在相对稳定的激活区域内进行；该现象跨架构一致。

Conclusion: ReLU网络训练具有架构无关的双时间尺度特性，为监控训练动态提供了新工具，并支持对分段线性网络采用解耦优化策略的进一步研究。

Abstract: Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.

</details>


### [616] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

TL;DR: 本文研究了随机凸优化中最佳经验风险最小化器（ERM）的样本复杂度，证明了存在一种情况，即样本量与维度呈线性关系时学习仍可行，但ERM却很可能唯一且过拟合，从而解决了Feldman提出的开放问题；此外，文章还扩展至近似ERM，并基于构造结果揭示了带约束的梯度下降在步长和迭代次数随样本量增长时可能过拟合，提出了新的泛化下界Ω(√(ηT/m^1.5))，显著缩小了此前上界O(ηT/m)与已有下界之间的指数级差距。


<details>
  <summary>Details</summary>
Motivation: 解决Feldman提出的关于ERM在特定条件下是否必然过拟合的开放问题，并填补梯度下降泛化误差下界的研究空白。

Method: 通过构造特定实例分析ERM的唯一性与过拟合现象，并推导带约束梯度下降的泛化误差下界。

Result: 证明了存在线性样本量下ERM仍会过拟合的情形；给出了梯度下降泛化误差的新下界Ω(√(ηT/m^1.5))，大幅缩小了与已有上界之间的差距。

Conclusion: ERM及其近似版本在某些合理设定下仍可能严重过拟合；梯度下降的泛化性能不仅依赖于样本量，还显著受学习率和迭代步数影响，需谨慎调参以避免过拟合。

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [617] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文提出JoBS方法，利用基于缩放定律的性能预测器辅助贝叶斯优化，高效联合优化大语言模型（LLM）的训练数据与模型配置，显著降低全量训练开销并提升优化效果。


<details>
  <summary>Details</summary>
Motivation: 联合优化LLM的数据配置（如数据混合）和模型配置（如架构）存在鸡生蛋、蛋生鸡的困境，且传统方法忽略二者交互，导致次优解。

Method: 提出JoBS框架：先用部分预算学习一个基于缩放定律的轻量级性能预测器（仅需少量训练步即可预测配置优劣），再用剩余预算完全基于该预测器进行贝叶斯优化；同时理论分析平均遗憾并推导最优预算分配策略。

Result: JoBS在相同优化预算下，平均遗憾低于多保真贝叶斯优化基线及单独的数据/模型优化方法，在多种LLM任务上验证了其有效性与高效性。

Conclusion: 联合优化数据与模型配置是可行且必要的；JoBS通过引入可学习的多保真预测器与理论驱动的预算分配，为LLM训练配置搜索提供了新范式。

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [618] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于折扣到动态转换的模块化方法，用于分析FTRL类算法在非平稳在线学习中的动态遗憾，并将其扩展至线性回归、逻辑回归及Adam优化器，获得了最优或新的动态遗憾与收敛率保证。


<details>
  <summary>Details</summary>
Motivation: 现有对FTRL类方法的动态遗憾分析不足，尤其在非平稳、曲线损失场景下；同时，对Adam等自适应优化器在非凸、非光滑等复杂设定下的理论理解仍不充分。

Method: 基于折扣到动态（discounted-to-dynamic）的转化框架，构建模块化分析工具，统一处理FTRL及其变体（如Adam），并应用于线性回归、逻辑回归及Adam优化器的动态遗憾与收敛性分析。

Result: 1）简化了在线线性回归最优动态遗憾的证明；2）首次给出在线逻辑回归的动态遗憾新保证；3）在随机、非凸、非光滑条件下为Adam获得最优收敛率；4）对双参数β₁, β₂的Adam（含裁剪与无裁剪版本）给出新理论结果。

Conclusion: 该折扣到动态还原框架为FTRL及相关自适应优化器提供了统一、灵活且强大的动态遗憾分析范式，显著拓展了其在非平稳和复杂损失场景下的理论基础。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [619] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: 本文提出OJBKQ方法，将大语言模型的后训练量化建模为激活与权重联合优化问题，通过扩展Babai和Klein算法求解子最优解，在3-4比特下显著降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法依赖启发式目标和贪心舍入，导致低比特量化时性能明显下降。

Method: 提出OJBKQ方法，将每层权重量化建模为多右端项带盒约束整数最小二乘（BILS）问题，并采用扩展Babai最近平面算法和扩展Klein随机Babai算法求解每列权重的最小残差Babai-Klein点。

Result: 在大语言模型上实验表明，OJBKQ在3-4比特量化下相比现有PTQ方法具有更低的困惑度，同时计算开销相当。

Conclusion: OJBKQ通过联合优化激活与权重并设计高效近似算法，有效提升了低比特后训练量化的精度。

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [620] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

TL;DR: Modalities is a PyTorch-native framework designed to streamline large-scale LLM pretraining and ablation studies, offering efficient parallelization and modular, declarative configuration for reproducibility and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing open-source frameworks lack adequate tooling for compute-intensive large-scale ablation studies in LLM (pre-)training, forcing researchers to develop custom scripts.

Method: Modalities integrates state-of-the-art parallelization strategies and adopts a modular, declarative, self-contained configuration design within a PyTorch-native framework.

Result: Enables efficient pretraining and systematic ablations at trillion-token and billion-parameter scale, while improving reproducibility and extensibility compared to existing LLM training frameworks.

Conclusion: Modalities bridges the gap between data-driven LLM research and scalable model training by unifying efficiency, modularity, and configurability in a single open framework.

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [621] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

TL;DR: 本文提出了GAMM框架，扩展了缺失数据机制的分类法以适用于属性图，并实证表明现有插补方法在图感知的缺失场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 属性图中的缺失数据问题比表格数据更复杂，需要考虑节点属性和图结构对缺失概率的影响。

Method: 提出GAMM（Graph Attributes Missing Mechanisms）框架，将缺失概率系统性地与节点属性和图结构关联，扩展缺失数据机制分类法。

Result: 实证表明，当前最先进的插补方法在传统缺失模式下有效，但在更贴近实际的图感知缺失场景下性能显著下降。

Conclusion: 需发展能同时建模属性与图结构依赖关系的新插补方法，以应对属性图中更真实的缺失数据挑战。

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [622] [Radial Müntz-Szász Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities](https://arxiv.org/abs/2602.08419)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 本文提出Radial Müntz-Szász Networks (RMN)，一种专为建模径向奇异场（如1/r、log r、裂纹尖端型）设计的神经网络架构，克服了传统坐标可分离网络的固有局限，在多个基准测试中显著优于MLP和SIREN。


<details>
  <summary>Details</summary>
Motivation: 传统坐标可分离神经网络难以有效建模径向奇异场（如1/r、log r、裂纹尖端型），且理论分析表明C²光滑且加性可分离的径向函数必为二次函数，揭示了坐标幂律模型的根本障碍。

Method: 提出Radial Müntz-Szász Networks (RMN)，以可学习的径向幂函数r^μ（含负指数）与极限稳定的log-原函数精确表示log r；支持闭式空间梯度与拉普拉斯算子计算；并拓展为RMN-Angular（引入角向依赖）和RMN-MC（多源+可学习中心）。

Result: 在10个2D/3D基准上，RMN的RMSE比MLP低1.5–51倍、比SIREN低10–100倍，仅用27参数（MLP为33,537，SIREN为8,577）；RMN-MC在收敛时源中心恢复误差<10⁻⁴；同时验证了其在非径向光滑目标上的失效边界。

Conclusion: RMN为物理信息学习中处理径向奇异性提供了高效、可微、参数极简的新范式，理论严谨、实证优越，但适用范围限于（近）径向结构问题。

Abstract: Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial Müntz-Szász Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^μ$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>


### [623] [The Connection between Kriging and Large Neural Networks](https://arxiv.org/abs/2602.08427)
*Marius Marinescu*

Main category: cs.LG

TL;DR: 本文探讨了空间统计模型（特别是Kriging）与机器学习模型（特别是神经网络）之间的联系，指出尽管二者表面差异大，但存在深层关联，融合二者有助于提升ML模型的可解释性、可靠性与空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 探究空间统计模型（如Kriging）与机器学习模型（如神经网络）之间的潜在关系，以促进两领域融合，提升模型性能。

Method: 通过文献综述与理论分析，系统梳理并比较Kriging（及其ML对应形式高斯过程回归）与神经网络在建模原理、假设基础和数学结构上的联系。

Result: 揭示了Kriging与神经网络之间存在实质性理论联系，二者并非孤立，而是可在概率建模与函数逼近视角下相互启发。

Conclusion: 融合空间统计与机器学习视角，有望推动更可解释、更可靠、更具空间意识的AI模型发展。

Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>


### [624] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: 本文提出通用结构基蒸馏（USBD）框架，通过构建与结构无关的通用结构基来解决源-目标图结构差异大时的SF-GDA问题，显著提升跨结构图域自适应性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有SF-GDA方法依赖源训练GNN的平滑性先验，难以泛化到拓扑结构差异大的目标图，尤其在显著拓扑偏移下伪标签不可靠。

Method: 提出通用结构基蒸馏（USBD）：采用双层优化从源图蒸馏出紧凑结构基；强制原型覆盖全Dirichlet能量谱以显式建模多样拓扑模体；引入频谱感知集成机制，根据目标图频谱指纹动态激活最优原型组合。

Result: 在多个基准上显著超越SOTA方法，尤其在严重结构偏移场景下表现突出，并通过解耦适配开销实现更高计算效率。

Conclusion: USBD摆脱了对源模型偏差的依赖，转而学习可泛化的通用结构基，为SF-GDA提供了更鲁棒、高效且结构无关的新范式。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [625] [RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks](https://arxiv.org/abs/2602.08446)
*Pouria Arefijamal,Mahdi Ahmadlou,Bardia Safaei,Jörg Henkel*

Main category: cs.LG

TL;DR: 本文提出RIFLE框架，通过基于logit的知识蒸馏替代梯度共享，在资源受限的IoT设备上实现鲁棒、隐私保护的联邦学习，支持VGG-19等深层模型训练，并显著提升准确率与抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: TinyML模型在极端非IID数据下表达能力不足，且传统FL难以兼顾鲁棒性（对抗恶意客户端）与隐私保护。

Method: 提出RIFLE：采用logit-based知识蒸馏替代梯度共享；设计KL散度验证机制评估客户端更新可靠性；使用知识蒸馏聚合策略实现轻量端到端深度模型训练。

Result: 在MNIST/CIFAR-10/CIFAR-100上，相较基线：误报率降低87.5%，投毒攻击缓解提升62.5%，10轮内准确率最高提升28.3%；VGG-19训练时间从600天压缩至1.39小时（0.3 GFLOPS设备）。

Conclusion: RIFLE在保障隐私前提下，首次使VGG-19/ResNet18等深度模型可在典型IoT设备上高效训练，有效解决非IID、鲁棒性与资源限制三重挑战。

Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.

</details>


### [626] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

TL;DR: 本文提出了治疗效果方差（VTE）和条件治疗效果方差（CVTE）作为衡量个体治疗响应中内在随机不确定性的自然指标，并在轻度假设下证明其可识别性；同时设计了非参数核估计器，理论分析验证其收敛性，并通过合成与半模拟数据实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 以往因果推断研究主要关注平均处理效应及其条件平均，而忽视了个体处理响应中的变异性与不确定性。

Method: 提出VTE和CVTE作为衡量处理响应中偶然不确定性的指标；在存在未观测混杂因素的轻度假设下证明其可识别性；设计非参数核估计器并进行收敛性理论分析。

Result: 理论证明VTE和CVTE在轻度假设下可识别且估计器收敛；实验表明所提方法在合成和半模拟数据上性能优于或媲美基线方法。

Conclusion: VTE和CVTE是刻画个体处理响应不确定性的合理且可识别的统计量，所提出的非参数估计方法具有理论保证和实证有效性。

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [627] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的多变量时间序列异常诊断方法ALoRa-T与ALoRa-Loc，通过低秩注意力机制和理论分析提升检测与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多变量时间序列异常诊断方法缺乏理论支撑，尤其是异常定位这一关键但研究不足的方向。

Method: 揭示Transformer在MTS建模中与统计时序方法的联系；提出低秩正则化的Attention Low-Rank Transformer（ALoRa-T）模型及Attention Low-Rank评分；设计ALoRa-Loc方法，通过量化多变量间关系实现异常定位。

Result: 在大量实验与真实数据分析中，所提方法在异常检测与定位任务上均显著优于当前最优方法。

Conclusion: 结合理论洞察与结构化建模（低秩注意力+定位机制），可有效提升MTS异常诊断的整体性能，尤其推动了异常定位这一薄弱环节的发展。

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [628] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: 本文提出CreDRO方法，通过分布鲁棒优化构建模型集合，将认知不确定性（EU）定义为对训练-测试数据i.i.d.假设不同松弛程度下的模型分歧，从而更本质地刻画EU，提升OOD检测与医疗选择性分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有主流可信预测器将认知不确定性（EU）主要归因于训练初始化随机性导致的模型分歧，这仅反映优化随机性，而非更深层的数据分布不确定性。

Method: 提出CreDRO方法，基于对i.i.d.假设的不同松弛程度定义EU，并利用分布鲁棒优化（DRO）学习一组合理模型构成集合，以同时捕获训练随机性和由训练/测试分布偏移引发的有意义分歧。

Result: 在多个基准上的OOD检测和医疗领域的选择性分类任务中，CreDRO持续优于现有可信预测方法。

Conclusion: 将EU建模为对i.i.d.假设松弛的模型分歧更具原则性和实用性；CreDRO通过DRO实现该思想，显著提升了模型在分布外场景下的鲁棒性与可靠性。

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [629] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: 本文提出了一种简化版的Transformer架构——时间延迟Transformer（TD-TF），用于建模非定常时空动力学；它将线性算子方法与深度序列模型结合，可视为时间延迟动态模态分解（TD-DMD）的非线性推广；结构极简、计算高效，在线性和非线性系统中均表现优异，兼具可解释性与强表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型（如TD-DMD）在非线性或混沌系统中建模能力不足，而标准Transformer又过于复杂、缺乏可解释性且计算开销大；亟需一种兼顾效率、可解释性与非线性建模能力的数据驱动方法。

Method: 提出时间延迟Transformer（TD-TF）：单层、单头自注意力（每预测仅一个query）+ 单层前馈网络；其自注意力机制被理论解释为TD-DMD的非线性推广；整体保持线性时间复杂度和小参数量。

Result: 在合成信号、非定常空气动力学、Lorenz '63系统及反应-扩散模型上验证：TD-TF在线性系统中媲美强线性基线，在非线性/混沌系统中显著超越，并能准确捕捉长期动力学行为。

Conclusion: TD-TF成功桥接了线性算子建模与深度序列建模，在保持线性模型可解释性与计算效率的同时，大幅提升了对复杂非线性时空动力学的建模能力，是一种高效、简洁且实用的新架构。

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [630] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于上下文赌博机的 rollout 调度框架，用于提升强化学习中可验证奖励（RLVR）方法在大语言模型推理能力训练中的样本效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有 RLVR 方法对 rollout 的使用缺乏选择性且视野短浅：同一批次内质量参差的响应被一视同仁，历史 rollout 仅用一次即丢弃，导致监督噪声大、样本效率低、策略更新次优。

Method: 将 rollout 调度建模为上下文赌博机问题，设计统一神经调度器；每个 rollout 视为一个 arm，其 reward 定义为相邻优化步间带来的性能增益；支持组内噪声感知选择与跨轮次历史 rollout 自适应复用。

Result: 理论给出亚线性遗憾界，并证明增大 rollout 缓冲池可提升性能上界；在六个数学推理基准上，该方法在多种 RLVR 优化器上均一致提升性能与训练效率。

Conclusion: 所提调度框架为 RLVR 提供了更鲁棒、高效且可复用的 rollout 利用范式，显著缓解了噪声监督与低效采样问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [631] [Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs](https://arxiv.org/abs/2602.08500)
*Maiqi Jiang,Noman Ali,Yiran Ding,Yanfu Zhang*

Main category: cs.LG

TL;DR: 本文研究了元路径注意力机制在异构图神经网络中是否真正反映了元路径的重要性，并提出了MetaXplain解释协议来评估其可靠性。


<details>
  <summary>Details</summary>
Motivation: 元路径注意力常被用作解释‘哪些语义更重要’，但其是否真实反映元路径重要性缺乏实证验证；现有GNN解释器多针对同构图，直接迁移至异构图易混淆语义并干扰扰动分析。

Method: 提出MetaXplain——一种元路径感知的后验解释协议，包含视图分解解释、符合图模式的通道级扰动、融合感知归因三部分；在ACM、DBLP、IMDB数据集上，对HAN和HAN-GCN模型，使用梯度/扰动/Shapley类解释器进行基准测试，并引入MP-AEA指标衡量注意力权重与解释贡献分数的排序一致性。

Result: MetaXplain解释通常优于随机基线；MP-AEA揭示出高对齐与显著解耦并存的现象，依赖于数据集与模型；基于解释子图重训练可在噪声场景下提升性能，表明解释具有去噪效应。

Conclusion: 元路径注意力并非总是可靠指示元路径重要性；需借助元路径感知解释方法（如MetaXplain）进行实证评估；解释结果不仅可解释模型，还可能提升鲁棒性与泛化性。

Abstract: Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.

</details>


### [632] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

TL;DR: 本文提出了PyAGC，一个面向工业部署的、可扩展的属性图聚类（AGC）基准与库，统一框架、支持mini-batch、涵盖12个多样化的工业级图数据集，并倡导结合无监督结构指标与效率评估的综合评测协议。


<details>
  <summary>Details</summary>
Motivation: 现有AGC研究受限于小规模高同质性数据集、全批量训练不可扩展、依赖有监督指标，难以支撑真实场景（如风控、用户分群）部署，亟需更贴近工业实际的基准与评估体系。

Method: 提出模块化Encode-Cluster-Optimize框架；首次为多种SOTA AGC算法提供内存高效、可扩展的mini-batch实现；构建包含12个从2.7K到1.11亿节点、含低同质性工业图与复杂表格特征的数据集；引入融合无监督结构指标（如模块度、轮廓系数）与效率分析（内存/时间）的综合评估协议。

Result: PyAGC已在蚂蚁集团高风险工业流程中验证；支持大规模图处理；提供开源代码（GitHub/PyPI/文档），显著提升AGC方法在真实场景下的可复现性、可扩展性与实用性。

Conclusion: PyAGC弥合了AGC学术研究与工业落地之间的鸿沟，推动该领域向更真实、更高效、更可评估的方向发展。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [633] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

TL;DR: 本文提出因果薛定谔桥（CSB）框架，将反事实推理重新表述为熵最优传输问题，利用随机微分方程（SDE）替代传统确定性常微分方程（ODE），以增强在因果干预（尤其是分布外干预）下的鲁棒性和结构性一致性。


<details>
  <summary>Details</summary>
Motivation: 确定性生成流（如ODE）在分布内任务中有效，但在因果干预下因需穿越低密度区域（off-manifold）而出现向量场定义不良、数值不稳定和虚假相关等问题。

Method: 提出因果薛定谔桥（CSB），基于扩散过程（SDE）建模反事实推理，将其形式化为带结构可容许性约束的熵最优传输问题；并证明结构分解定理，表明高维桥梁可分解为局部稳健转移。

Result: 在Morpho-MNIST等高维干预实验中，CSB在结构性一致性上显著优于确定性基线方法，尤其在强分布外处理场景下表现突出。

Conclusion: CSB通过引入随机性与熵正则化，克服了确定性流在因果干预中的固有局限，为鲁棒反事实生成提供了新范式。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [634] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

TL;DR: 本文提出了ρ-Perfect，一种用于估计模型在主观评分数据集上所能达到的最高相关性的实用方法，并通过异方差噪声场景推导其估计值，验证其与重测信度的关系，并在语音质量数据集上展示了其应用价值。


<details>
  <summary>Details</summary>
Motivation: 主观评分数据中固有的噪声限制了模型与人类评分的相关性，但这种可靠性问题很少被量化。

Method: 定义ρ-Perfect为完美预测器与人类评分之间的相关性，并基于主观评分数据中常见的异方差噪声场景推导其估计值；进一步证明ρ-Perfect的平方可估计重测信度以验证该估计。

Result: 在语音质量数据集上验证了ρ-Perfect的有效性，表明该指标能区分模型性能局限与数据质量缺陷。

Conclusion: ρ-Perfect为评估主观评分任务中模型性能上限提供了可靠、可解释的量化工具，有助于准确归因性能瓶颈。

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [635] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: 本文提出隐式记忆概念，即大语言模型能在不同交互间通过自身输出隐含地保持状态，无需显式记忆模块；并以此构建时间炸弹型后门，展示其在安全、评估与训练中的广泛影响。


<details>
  <summary>Details</summary>
Motivation: 挑战大语言模型无状态的常规假设，探索模型在无显式记忆模块下能否通过输出-输入循环维持跨轮次状态，从而揭示潜在安全与可靠性风险。

Method: 提出隐式记忆机制，设计时间炸弹（temporal backdoors）作为实证案例，通过提示工程或微调诱导该行为，并分析其在隐蔽通信、基准污染、定向操控和数据投毒等场景中的应用与风险。

Result: 证实当前LLMs可通过隐式记忆实现跨交互状态保持；成功构造并验证时间炸弹后门；识别出多种新型安全与评估隐患；提供开源代码与数据以支持后续研究。

Conclusion: 隐式记忆是LLMs中真实存在且具实际影响的现象，亟需系统性检测、压力测试与评估框架，以提前应对未来模型中可能加剧的相关风险。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [636] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估指标M-Loss，用于量化无标签数据下源模型合并的兼容性，通过衡量参数平均与模型集成在层和节点级别的差异，提升模型合并效果并指导剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法（如参数平均）易混合非泛化特征，尤其当源模型权重差异大时；而模型集成虽性能稳定但推理开销和存储成本高；二者相似性已有实验观察，但缺乏理论依据和有效评估指标。

Method: 提出Merging-ensembling loss（M-Loss），利用极少量无标签数据，在层和节点级别衡量参数平均与模型集成输出的差异，用作合并可行性判据及参数重要性指导（如剪枝）。

Result: 理论分析与实验表明，引入M-Loss可显著提升合并模型与集成模型的一致性，实现更准确、可扩展且高效的模型整合。

Conclusion: M-Loss为模型合并提供了首个具备理论支撑的轻量级评估工具， bridging merging and ensembling，并支持下游任务如模型剪枝。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [637] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

TL;DR: 本文提出了一种基于算术方法的k-NN回归算法优化方案（AMR），通过引入算术方法算法（AMA）求解线性方程组，提升k-NN在回归任务中的性能；实验表明AMR在多数情况下优于标准k-NN，且整体性能与其他主流回归算法相当。


<details>
  <summary>Details</summary>
Motivation: 提升k-NN非参数回归算法的性能，利用新型算术方法解决多变量线性方程问题以优化预测效果。

Method: 提出算术方法算法（AMA）用于求解含任意实变量的线性方程，并基于AMA构建算术方法回归（AMR）算法，作为k-NN的优化版本；采用所提最优推理决策规则与其他回归算法进行对比评估。

Result: AMR在多个公开真实数据集上的实验结果表明，其性能与主流回归算法相当，且在大多数情况下优于标准k-NN。

Conclusion: AMR是一种对k-NN的有效优化，验证了所引入算术方法在回归任务中的实用性和有效性。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [638] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

TL;DR: 本文通过将得分估计误差视为随机源，驱动Fokker-Planck方程，利用SPDE框架分析基于得分的生成模型（SGMs）的概率密度场演化，并提出一种基于径向测试函数投影的二次变差评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统粒子中心的SDE分析难以刻画密度场整体演化；需从概率密度动力学角度理解SGM鲁棒性。

Method: 将得分估计误差建模为随机扰动，建立描述密度场演化的随机偏微分方程（SPDE）框架；在简化设定下结合几何稳定性和位移凸性分析鲁棒性；构造基于径向测试函数投影的二次变差作为评估指标。

Result: 提出了一种新的SGM鲁棒性解释框架；导出一个可仅用采样轨迹前10%即可有效评估的计算高效指标。

Conclusion: SPDE视角能更本质地刻画SGM密度演化；所提指标具备理论意义与实用潜力，为生成模型评估提供了新思路。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [639] [Conditional Sequence Modeling for Safe Reinforcement Learning](https://arxiv.org/abs/2602.08584)
*Wensong Bai,Chao Zhang,Qihang Xu,Chufan Chen,Chenhao Zhou,Hui Qian*

Main category: cs.LG

TL;DR: 本文提出RCDT方法，一种基于条件序列建模的离线安全强化学习算法，支持单策略零样本适配多种成本阈值，并通过自适应拉格朗日惩罚、轨迹重加权与Q值正则化提升收益-成本权衡性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全强化学习方法通常在预设成本阈值下训练，缺乏跨阈值泛化能力，难以满足不同部署场景的灵活需求；受条件序列建模在目标条件控制中成功应用的启发，本文旨在构建可零样本适应多成本阈值的统一策略。

Method: 提出RCDT：基于条件序列建模（CSM），引入拉格朗日风格的成本惩罚与自动自适应惩罚系数；进一步设计奖励-成本感知的轨迹重加权机制和Q值正则化，以缓解过度保守行为并优化收益-成本权衡。

Result: 在DSRL基准上大量实验表明，RCDT在各类成本约束下持续优于代表性基线，在收益-成本权衡上显著提升，推动离线安全强化学习的最先进水平。

Conclusion: RCDT是首个将条件序列建模用于离线安全强化学习的工作，实现了单策略零样本跨成本阈值部署，并通过多项机制协同优化安全性与性能，为实际部署提供了更高灵活性与实用性。

Abstract: Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>


### [640] [Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction](https://arxiv.org/abs/2602.08585)
*Ziyao Tang,Pengkun Jiao,Xinhang Chen,Wei Liu,Shiyong Li,Jingjing Chen*

Main category: cs.LG

TL;DR: 本文提出LU-KV框架，通过考虑注意力头间异质性，基于长期语义信息的边际效用优化KV缓存预算分配，在大幅减少缓存大小的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存驱逐方法依赖瞬时启发式指标，忽视了不同注意力头在预测保真度上的异质性——有些头关注即时token贡献，有些则负责长程语义效用；因此需以长期语义信息的边际效用为准则进行预算分配。

Method: 提出LU-KV框架：1）采用凸包松弛建模头级预算分配问题；2）设计基于边际效用的贪心求解器；3）引入数据驱动的离线分析协议支持实际部署。

Result: 在LongBench和RULER基准上，LU-KV实现80% KV缓存压缩，性能下降极小，并显著降低推理延迟与GPU显存占用。

Conclusion: 以长期语义边际效用为指导的头级动态预算分配，比统一瞬时指标更合理有效，LU-KV为高效大模型推理提供了新范式。

Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.

</details>


### [641] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: 本文提出了一种名为FairRARI的统一凸优化框架，用于在PageRank计算中嵌入多种群体公平性约束，确保达到目标公平水平，同时保持与原始PageRank相近的时间复杂度，并在真实数据集上验证了其在效用与公平性上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有PageRank算法缺乏对顶点敏感属性（如性别、种族）所导致的群体不公平问题的系统性解决方法，已有方法或无法保证目标公平水平，或缺乏最优性保证。

Method: 基于PageRank的变分形式，构建一个带公平性约束的强凸优化问题，提出可即插即用的统一框架FairRARI，支持三种高效可解的群体公平准则。

Result: FairRARI在多个真实世界数据集上显著优于现有方法，在保证目标公平水平的同时，维持高排序效用，且时间复杂度与标准PageRank相同。

Conclusion: FairRARI为图机器学习中的公平PageRank计算提供了首个兼具理论保证（强凸性、可行性、最优性）与实用高效性的统一框架。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [642] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

TL;DR: 本文提出SDFed框架，通过子空间优化和发散控制解决异构联邦提示学习中的局部-全局差异问题，允许客户端使用可变长度本地提示以适应数据与资源异构性，同时保持固定长度全局提示用于高效聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习方法在客户端数据分布与系统资源异质性下，采用统一提示结构和长度，导致局部最优与全局共享知识冲突，且通信成本高、本地数据少。

Method: SDFed框架包含：1）固定长度全局提示用于高效聚合；2）可变长度本地提示适配各客户端特性；3）子空间优化方法缓解局部-全局冲突；4）信息保留与发散控制策略平衡局部信息保留与表征可分性。

Result: 在多个数据集上的实验表明，SDFed在异构联邦设置中持续提升模型性能与鲁棒性。

Conclusion: SDFed有效缓解了异构联邦环境下提示学习的局部-全局知识冲突问题，提升了适应性、效率与泛化能力，为隐私敏感多参与方视觉语言建模提供了新范式。

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [643] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文提出TFMLinker，一种基于表格基础模型（TFM）的通用链路预测方法，通过原型增强的局部-全局上下文模块和拓扑感知链路编码器，实现跨域图数据的零样本链路预测。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在预训练规模或文本依赖上存在局限；受表格基础模型（TFM）在跨表通用预测中成功的启发，探索用TFM解决通用链路预测问题。

Method: 提出TFMLinker：1）原型增强的局部-全局上下文模块构建图特异与跨图可迁移上下文；2）通用拓扑感知链路编码器提取链路中心拓扑信息并生成链路表征；3）利用TFM进行上下文学习完成链路存在性预测。

Result: 在6个跨领域图基准数据集上，TFMLinker在无需数据集特定微调的前提下，性能优于当前最优基线方法。

Conclusion: TFMLinker验证了表格基础模型在通用链路预测任务中的有效性，为脱离文本依赖、支持零样本跨图泛化的链路预测提供了新范式。

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [644] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 本文提出了一种名为Distance-Guided Reinforcement Learning (DGRL)的新方法，通过结合Sampled Dynamic Neighborhoods (SDN)和Distance-Based Updates (DBU)，有效应对大规模离散动作空间中的维度灾难问题，支持高达10^20量级的动作数，并在多种环境中显著提升性能与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习算法在物流、调度和推荐系统等具有超大离散动作空间的任务中面临维度灾难问题；现有方法受限于网格结构或昂贵的近邻搜索，难以处理高维或不规则结构的动作空间。

Method: 提出DGRL框架，包含两个核心组件：1）Sampled Dynamic Neighborhoods（SDN），利用语义嵌入空间进行随机体积探索，在局部可信区域内提供完备支撑；2）Distance-Based Updates（DBU），将策略优化转化为稳定的回归任务，使梯度方差与动作空间大小解耦，并保证单调策略改进。

Result: 在规则与不规则结构环境中，DGRL相较SOTA方法性能提升最高达66%，同时加快收敛速度并降低计算复杂度；且可自然泛化至混合连续-离散动作空间，无需层级依赖。

Conclusion: DGRL为超大规模离散动作空间的强化学习提供了高效、稳定且通用的新范式，突破了传统方法在可扩展性与结构适应性上的瓶颈。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [645] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: ERIS is a serverless federated learning framework that balances privacy and accuracy by using model partitioning, client-side aggregators, and distributed shifted gradient compression, achieving FedAvg-level accuracy with reduced communication costs and improved privacy without heavy cryptography.


<details>
  <summary>Details</summary>
Motivation: Scaling federated learning to billion-parameter models faces trade-offs between communication efficiency, model accuracy, and privacy; existing solutions often sacrifice one for another or rely on costly cryptographic tools.

Method: ERIS introduces a serverless FL framework combining model partitioning, distributed aggregation across multiple client-side aggregators, and a distributed shifted gradient compression mechanism.

Result: ERIS achieves FedAvg-level accuracy, reduces communication cost, improves robustness against membership inference and reconstruction attacks, and provides strong privacy guarantees without noise injection or heavy cryptography.

Conclusion: ERIS theoretically converges at the same rate as FedAvg and bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy with no accuracy degradation.

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [646] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: 本文揭示了混合专家（MoE）大语言模型中存在‘不安全路由’这一新型安全风险：通过操纵高安全重要性得分（RoSais）的路由器，可轻易将原本安全的输出变为有害输出；作者提出细粒度的随机优化框架F-SOUR高效发现此类不安全路由，并在多个基准上实现高达0.98的攻击成功率，同时探讨了若干防御方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于MoE架构的效率与性能，却忽视其稀疏路由机制带来的潜在安全风险，尤其是路由器被操纵后可能导致模型输出从安全变为有害的问题。

Method: 提出Router Safety重要性得分（RoSais）量化各层路由器的安全关键性；设计细粒度token-layer-wise随机优化框架（F-SOUR），显式建模输入token的时序性与动态性以发现具体不安全路由。

Result: 在JailbreakBench上，仅屏蔽DeepSeek-V2-Lite中5个高RoSais路由器即可使攻击成功率（ASR）超4倍提升至0.79；F-SOUR在四个主流MoE模型家族上，于JailbreakBench和AdvBench平均ASR分别达0.90和0.98。

Conclusion: MoE模型的安全性与其稀疏架构一样脆弱，存在天然的不安全路由风险；需从安全感知的路由禁用、路由器训练等角度加强防御，为MoE模型的红队测试与安全保障提供新思路。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [647] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

TL;DR: CauScale是一种高效可扩展的神经架构，用于大规模因果发现，支持高达1000节点的图，通过压缩嵌入和共享注意力权重提升时空效率，并采用双流设计兼顾准确性，在多项指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在扩展到大规模图时面临严重的时间和空间效率瓶颈，难以处理数百甚至上千节点的图结构。

Method: 提出CauScale神经架构：引入压缩数据嵌入的‘reduction unit’提升时间效率；采用权值绑定的注意力机制减少内存占用；设计双流结构——数据流提取高维观测中的关系证据，图流融合统计图先验并保留关键结构信号。

Result: 训练可扩展至500节点图（此前方法因显存不足失败）；测试中in-distribution数据mAP达99.6%，out-of-distribution达84.4%；推理速度比先前方法快4–13,000倍。

Conclusion: CauScale在保持高因果发现精度的同时，显著突破了大规模图上的时空效率限制，为科学AI与大数据分析中的可扩展因果建模提供了新范式。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [648] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

TL;DR: 本文提出LEFT框架，通过学习时间、频率和多尺度三个视角的token表示，并引入时间-频率循环一致性约束，实现无监督时间序列异常检测，显著提升检测精度并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 许多时间序列异常过于细微，在单一视角（如时域）中难以察觉，需依赖多视角（时域、频域、多分辨率）间不一致性来识别；而现有跨视角方法缺乏分析-合成一致性约束。

Method: LEFT框架从同一时间序列中提取频域（周期性）、时域（局部动态）和多尺度（不同粒度异常模式）三类token；利用自适应Nyquist约束谱滤波器生成多分辨率信号；引入细粒度重建目标与时间-频率循环一致性损失以强制跨视角一致。

Result: 在真实世界基准数据集上，LEFT检测精度超越SOTA方法，同时FLOPs降低5倍、训练速度提升8倍。

Conclusion: 建模多视角表征间的不一致性是提升无监督时间序列异常检测性能的有效途径，且通过可学习融合与强一致性约束可在精度与效率间取得更好平衡。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [649] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 本文提出了一种带硬性白高斯噪声约束的潜在空间优化方法，用于奖励引导生成，在保持生成质量的同时显著提升优化效率并防止奖励作弊。


<details>
  <summary>Details</summary>
Motivation: 现有测试时潜在优化方法虽能提升奖励引导生成效果，但易发生奖励作弊导致质量下降，且速度慢、实用性差。

Method: 用闭式投影梯度上升替代软正则化，在每次更新后通过闭式投影强制潜在向量始终保持白高斯噪声特性，约束为硬性噪声约束。

Result: 实验表明，该方法仅需SOTA正则化方法30%的壁钟时间即可达到相当的美学评分，并有效防止奖励作弊。

Conclusion: 硬性白高斯噪声约束的潜在优化方法在效率与可靠性上均优于现有软正则化方法，为奖励引导生成提供了更实用、稳健的解决方案。

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [650] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: 本文提出了一种名为Geometric Pessimism（几何悲观主义）的离线强化学习框架，通过在状态-动作嵌入空间中利用k近邻距离计算密度惩罚，并将其作为奖励塑形项加入IQL中，在几乎不增加训练开销的前提下显著提升OOD动作抑制能力与策略稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法在处理稀疏、断裂的数据流形时，难以兼顾计算效率与对分布外（OOD）动作的保守性：CQL虽严谨但计算开销大，IQL等高效方法则易在病态数据上退化为行为克隆。

Method: 提出Geo-IQL：在标准IQL基础上，基于状态-动作嵌入空间中k近邻距离预计算密度惩罚项，以奖励塑形方式注入OOD保守性，训练开销仅为O(1)。

Result: 在D4RL MuJoCo基准上，Geo-IQL在敏感的medium-replay任务中平均提升超18分，种子间方差降低4倍；在MIMIC-III脓毒症数据集上，终端决策与临床医生一致性达86.4%，显著优于IQL的75%，且未在稳定流形上性能下降。

Conclusion: 几何悲观主义提供了一种轻量、模块化且安全有效的正则化机制，可帮助离线RL在关键现实决策系统中规避局部最优，实现稳健策略提升。

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [651] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段合成策略，以在隐私保护与下游预测性能之间实现更优权衡：第一阶段采用‘先合成后混合’策略生成并融合数据；第二阶段基于核岭回归（KRR）生成合成输出，在保留协变量分布的同时提升预测性能。理论与实验验证了其统计驱动性、隐私-预测受限权衡能力及泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法多关注统计保真度，而兼顾隐私（需强扰动）与预测性能（对扰动敏感）的单阶段方法难以平衡二者，亟需新策略。

Method: 提出两阶段合成策略：第一阶段为‘合成-混合’，先生成纯合成数据再与原始数据融合；第二阶段基于原始数据训练KRR模型，并用第一阶段生成的合成输入生成合成输出。

Result: 该策略在理论上实现了统计驱动的受限隐私-预测权衡，并在营销问题和五个真实数据集上验证了其预测性能优越性、统计一致性及泛化能力。

Conclusion: 两阶段设计有效解耦隐私扰动与预测建模，结合KRR理论优势与分布保持能力，为合成数据提供了兼顾隐私、统计与预测性能的新范式。

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [652] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

TL;DR: 本文提出了一种新的生成模型公平性定义——均衡生成处理（EGT），要求不同敏感群体的生成质量（以参考f-散度衡量）相近，并通过min-max微调方法实现，实验验证其在图像和文本生成任务中兼具公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型的公平性定义多借鉴分类任务，仅关注各敏感组生成概率的平衡，但易被满足而忽略生成质量差异，因而脆弱。

Method: 提出基于f-散度的均衡生成处理（EGT）公平性定义，并设计min-max微调方法来平衡各敏感组的f-散度。

Result: 在图像和文本生成任务上，min-max方法相比其他方法实现了更公平的生成结果，同时保持了有竞争力的整体性能。

Conclusion: EGT能更本质地保障生成质量层面的公平性；强制满足EGT会将整体模型质量与最难建模的敏感组绑定，min-max微调是有效实现途径。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [653] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.1提出一种联合Token-to-Token（T2T）与Mask-to-Token（M2T）的可配置阈值解码机制，实现解码速度与生成质量的协同优化，并首次为扩散语言模型（dLLMs）构建大规模强化学习训练框架，发布16B和100B两个开源版本，在33项基准测试中兼顾高性能与高吞吐（如HumanEval+达892 TPS）。


<details>
  <summary>Details</summary>
Motivation: 解决大参数量块扩散模型（如LLaDA2.0）中解码速度与生成质量难以兼顾的根本矛盾。

Method: 1）融合T2T编辑与M2T解码，构建双模式（Speedy Mode / Quality Mode）可配置阈值解码架构；2）基于大上下文窗口，设计面向dLLMs的首个大规模强化学习框架，含稳定梯度估计技术。

Result: 在33项基准上实现SOTA或接近SOTA性能；100B模型在HumanEval+、BigCodeBench、LiveCodeBench上分别达到892/801/663 TPS；发布LLaDA2.1-Mini（16B）和LLaDA2.1-Flash（100B）两个开源模型。

Conclusion: LLaDA2.1通过结构创新与训练范式升级，成功打破dLLMs中速度-质量权衡瓶颈，推动扩散语言模型向实用化迈进关键一步。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [654] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

TL;DR: 本文揭示了现有基于分数的查询攻击防御方法在面对自适应攻击时的脆弱性，并提出了一种名为Dashed Line Defense（DLD）的即插即用型后处理防御方法，通过引入损失观测模糊性来干扰攻击者优化过程，在理论上可抵御自适应查询策略，并在ImageNet上验证了其有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的黑盒查询攻击防御方法（尤其是即插即用型）在面对自适应攻击时易被绕过，缺乏对攻击者策略演化的鲁棒性。

Method: 提出Dashed Line Defense（DLD），一种无需模型参数、仅作用于输出分数的后处理机制，通过可控地扭曲损失反馈，使攻击者无法准确评估候选对抗样本的真实攻击强度，从而破坏其梯度估计与迭代优化过程；并给出理论安全性分析。

Result: 在ImageNet数据集上的实验表明，DLD在各类自适应查询攻击（包括最坏情况）下显著优于现有防御方法，同时保持原始模型预测标签不变。

Conclusion: DLD是一种高效、即插即用、理论可证、且对自适应攻击具有强鲁棒性的运行时防御方案，为黑盒对抗攻击防御提供了新思路。

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [655] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文研究了在安全关键场景中，如何在非凸代数约束下高效、可靠地进行连续变量的约束最大后验（MAP）推断。作者首先识别出可精确高效求解的约束MAP片段，并设计了可扩展的消息传递算法；其次提出了一种通用策略，通过将可行域划分为凸区域并结合数值优化来处理更一般的情况。实验表明该方法优于无约束基线，并能扩展到现有精确求解器难以处理的复杂密度。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，机器学习系统需在非凸代数约束（如避障）下做出概率预测，而现有方法难以高效可靠地求解此类约束MAP问题。

Method: 1）识别可精确求解的约束MAP可处理片段，设计基于消息传递的算法；2）提出通用策略：将非凸可行域分块为凸子区域，并在每块上运行数值约束优化。

Result: 所提方法在合成与真实世界基准测试中均优于不考虑约束的基线方法，并能扩展至当前最先进精确求解器无法处理的复杂概率密度。

Conclusion: 本文建立了约束MAP推断的可解性条件，提供了兼具理论保证与实用可扩展性的新算法框架，推动了受几何约束的概率推理发展。

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [656] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文提出CompilerKV，一种风险自适应且头感知的KV缓存压缩框架，通过离线学习决策表，在预填充阶段实现高效长上下文推理，显著提升性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在严苛内存预算下忽视提示依赖的压缩风险差异和注意力头的功能异质性，导致令牌选择不稳定及尾部失效。

Method: CompilerKV包含两个协同组件：(i) 基于离线上下文赌博机学习的‘头异质性表’，为各注意力头分配可靠性权重；(ii) 联合建模注意力熵与局部困惑度的‘风险自适应阈值门控’机制，将提示级风险转化为可部署的保留阈值。

Result: 在LongBench上，512-token内存预算下，CompilerKV恢复97.7%全KV性能，并比最强基线高出+5.2分。

Conclusion: CompilerKV通过风险感知与头感知设计，实现了更鲁棒、高效的长上下文KV压缩，适用于prefill-only部署场景。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [657] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

TL;DR: 本文提出了一种基于逆强化学习的框架，用于在不重新训练去噪网络的前提下优化预训练扩散模型的采样策略，通过将采样过程建模为马尔可夫决策过程并利用策略梯度匹配目标行为，实现了样本质量提升与超参数自动调优。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样过程灵活但缺乏系统化优化方法，而重训练去噪器计算开销大；亟需一种无需重训练即可提升采样质量与效率的新范式。

Method: 将扩散采样建模为离散时间有限时域马尔可夫决策过程，动作定义为对采样动力学的可选修改；采用逆强化学习与策略梯度技术，不显式设计奖励函数，而是直接拟合期望采样行为。

Result: 实验表明该方法能有效提升预训练扩散模型生成样本的质量，并实现采样超参数（如步数、噪声调度）的自动优化。

Conclusion: 逆强化学习为扩散模型采样策略优化提供了新思路，避免了昂贵的模型重训练，在保持原有去噪器不变的前提下显著提升了采样性能与灵活性。

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [658] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 本文系统化识别并分析了深度强化学习在网络安全领域应用中的11个常见方法论陷阱，涵盖环境建模、智能体训练、性能评估与系统部署各阶段，并基于66篇论文的实证分析提出针对性改进建议。


<details>
  <summary>Details</summary>
Motivation: DRL在网络安全中落地困难，因其面临对抗性、非平稳性与部分可观测性等挑战，现有研究常存在方法论缺陷。

Method: 通过系统性文献综述（分析2018–2025年66篇代表性DRL4Sec论文），识别并分类11类方法论陷阱；并在三个典型网络安全场景中开展控制实验验证其影响。

Result: 发现平均每篇论文存在超5个方法论陷阱；实验证明这些陷阱显著损害DRL策略的鲁棒性、泛化性与实际部署效果。

Conclusion: 需建立更严谨的方法论规范，本文提出的分类框架与改进建议有助于提升DRL在网络安全中研究的科学性与实用性。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [659] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

TL;DR: 本文研究语言模型在不确定性下的决策行为，特别是链式思维（CoT）推理的作用；通过分离采样与推理任务，发现CoT显著提升推理能力并使信念轨迹更接近人类，但对主动采样改善有限；提出一个包含记忆、策略、选择偏差和遮蔽意识四个潜变量的机制模型，将人类与模型置于共享认知空间，并揭示CoT提升了推理对齐但未缩小信息获取差距。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否像人类一样在不确定性下做决策，以及链式思维（CoT）在其决策过程中的作用。

Method: 设计一种主动概率推理任务，将证据采集（采样）与证据整合（推理）解耦；对比人类与多种大语言模型在近最优基准策略下的表现；构建含四个可解释潜变量（记忆、策略、选择偏差、遮蔽意识）的机制模型来刻画系统性偏离最优行为的模式。

Result: 发现扩展推理（尤其是CoT）是高性能的关键，大幅提升推理能力并使信念演化轨迹高度类人，但仅小幅改善主动采样；机制模型成功将人类与模型映射到共享低维认知空间，复现跨主体行为特征，并显示CoT使模型在证据累积与信念—选择映射上更趋近人类，而信息获取能力仍存明显差距。

Conclusion: 链式思维主要增强语言模型的推理对齐而非采样对齐，揭示了当前大模型在不确定性决策中‘推理类人、采样欠佳’的认知不对称性，为理解其类人决策机制与局限提供了新框架。

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [660] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

TL;DR: 本文研究了在特征噪声存在下，Transformer和LSTM模型能否实现噪声鲁棒学习（即在含噪数据上训练后仍能准确预测无噪输入）。结果表明，Transformer在k-稀疏奇偶性和多数函数上表现良好，但对随机k-junta函数失败，尤其当目标函数的布尔敏感度高于最优解时；失败原因在于Transformer倾向于简单函数且最优解通常敏感度更低；通过增加惩罚高敏感度解的损失项可改善性能。


<details>
  <summary>Details</summary>
Motivation: 噪声在大语言模型训练数据中普遍存在，但尚不清楚这些模型能否在含噪数据上训练后仍正确泛化到无噪输入，即噪声鲁棒学习能力有待系统研究。

Method: 通过在含特征噪声的数据上训练Transformer和LSTM模型，评估其在无噪测试集上的泛化性能；分析不同布尔函数（k-稀疏奇偶性、多数函数、随机k-junta）下的表现；探究布尔敏感度与模型偏好之间的关系；设计敏感度惩罚损失项进行干预实验。

Result: Transformer在k-稀疏奇偶性和多数函数上成功实现噪声鲁棒学习，而LSTM失败；但在随机k-junta任务上Transformer普遍失败，尤其当目标函数布尔敏感度高于最优解时；引入敏感度惩罚损失可帮助Transformer逃离错误的低敏感度局部最优。

Conclusion: Transformer虽具一定噪声鲁棒性，但在学习含噪布尔函数时存在本质局限，其对低复杂度/低敏感度函数的归纳偏置会阻碍对高敏感度目标函数的学习，需通过显式正则化加以修正。

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [661] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

TL;DR: QUOKA是一种无需训练、硬件无关的稀疏注意力算法，通过优先选择与平均查询余弦相似度较低的关键查询，并仅保留与其最对齐的键值对，从而在保持接近全注意力精度的同时显著加速Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 许多查询在注意力机制中只关注少量键，但低余弦相似度的查询对最终注意力logits贡献更大，需更高效地处理以加速推理。

Method: QUOKA首先保留一小部分代表性查询（特别是低余弦相似度查询），再为这些查询子选择最匹配的键值对，实现查询导向的KV稀疏化。

Result: 在多个基准测试中，QUOKA实现最高7倍CPU和5倍GPU上的注意力加速、3倍首token延迟降低，同时仅使用12%的KV对，精度接近基线。

Conclusion: QUOKA是一种高效、通用且无需训练的稀疏注意力方法，在chunked prefill场景下显著提升推理效率而不明显牺牲精度。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [662] [Data Reconstruction: Identifiability and Optimization with Sample Splitting](https://arxiv.org/abs/2602.08723)
*Yujie Shen,Zihan Wang,Jian Qian,Qi Lei*

Main category: cs.LG

TL;DR: 本文研究了从KKT条件重构训练数据的两个核心问题：可识别性（identifiability）与优化方法。理论上，给出了两层多项式激活网络下KKT系统唯一确定训练数据的充分条件；实践上，提出‘样本分割’（sample splitting）这一曲率感知的优化精炼步骤，能有效逃逸不良驻点并提升多种现有重构方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于KKT条件的数据重构在实验中表现优异，但其解的唯一性条件（即何时可识别）尚不明确，且缺乏对优化过程可靠性的理论与方法保障。

Method: 理论方面，分析两层多项式激活网络KKT系统的可识别性，推导唯一解的充分条件；方法方面，提出‘样本分割’技术——一种通用、曲率感知的优化精炼策略，用于增强各类重构目标函数的优化性能。

Result: 建立了KKT系统在特定网络结构下的可识别性理论保证；实验证明，将样本分割应用于多种现有重构方法后，重建性能得到一致提升。

Conclusion: KKT重构的可行性依赖于模型结构与条件，其成功不仅需理论可识别性，还需优化策略（如样本分割）来克服非凸优化难点；该工作为训练数据重构提供了理论支撑与实用改进路径。

Abstract: Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.

</details>


### [663] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 本文提出FIM-ODE，一种预训练的基础推断模型，用于从含噪轨迹数据中直接、单次前向推理常微分方程（ODE）的向量场，无需复杂训练或领域先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有ODE向量场推断方法（如符号回归、高斯过程、Neural ODE）依赖复杂训练流程、大量机器学习专业知识或强系统先验，难以泛化且使用门槛高。

Method: 提出FIM-ODE：基于低阶多项式向量场先验分布进行预训练；采用神经算子表征目标向量场；支持零样本推理与快速微调。

Result: FIM-ODE在零样本设置下性能媲美甚至优于符号基线ODEFormer；微调后显著超越现代神经网络和高斯过程方法，且训练更快速稳定。

Conclusion: FIM-ODE通过预训练实现对低维ODE推断的‘摊销’，降低了使用门槛，提升了鲁棒性与泛化能力，为科学建模中的ODE学习提供了新范式。

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [664] [On the Expressive Power of GNNs for Boolean Satisfiability](https://arxiv.org/abs/2602.08745)
*Saku Peltonen,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文通过Weisfeiler-Leman（WL）测试分析图神经网络（GNN）在布尔可满足性（SAT）求解中的表达能力，证明WL层级无法普遍区分可满足与不可满足实例，并指出其对实际顺序赋值求解器的限制；实验表明随机SAT实例较易区分，而工业实例通常需要更高表达能力。


<details>
  <summary>Details</summary>
Motivation: 用机器学习（尤其是GNN）替代手工启发式方法求解SAT问题日益流行，但其理论表达能力边界尚不清晰，需从图同构判别角度（如WL测试）系统分析GNN能否有效区分SAT关键语义（如可满足性）。

Method: 基于Weisfeiler-Leman（WL）图同构测试框架，理论证明WL各阶无法一般性区分SAT的可满足/不可满足实例；推导该结果对顺序变量赋值类WL有界求解器的实际影响；并针对正则、随机、平面等典型SAT族及G4SAT随机基准与国际SAT竞赛工业实例开展实证分析。

Result: 1) WL层级在理论上无法普遍区分可满足与不可满足SAT实例；2) 该理论局限导致WL有界顺序赋值求解器存在固有失败案例；3) 实验显示随机SAT实例在WL框架下大多可区分，而工业实例常需超越WL的表达能力才能准确预测满足赋值。

Conclusion: GNN用于SAT求解的表达能力存在根本性理论瓶颈（源于WL局限），尤其难以建模工业SAT实例所需的复杂逻辑结构；未来模型需突破WL表达范畴（如引入逻辑感知或高阶交互机制）以提升实用性。

Abstract: Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.

</details>


### [665] [Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms](https://arxiv.org/abs/2602.08751)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT-II is an AI model designed as an 'AI microscope' that provides biologically interpretable attention maps by mirroring the central dogma—DNA self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional control—enabling unsupervised discovery of regulatory networks from raw genomic and single-cell expression data.


<details>
  <summary>Details</summary>
Motivation: Current biological AI models lack interpretability; their internal representations do not reflect meaningful biological relationships. There is a need for AI tools that yield directly interpretable, mechanism-based insights for experimental biologists.

Method: CDT-II employs a transformer-based architecture explicitly structured to mirror the central dogma: DNA self-attention, RNA self-attention, and DNA-to-RNA cross-attention layers. It uses only genomic embeddings and raw per-cell expression data, requiring no labeled regulatory annotations.

Result: On K562 CRISPRi data, CDT-II achieves high prediction accuracy for perturbation effects (mean r = 0.84) and recovers the GFI1B regulatory network unsupervised (6.6-fold enrichment, P = 3.5 × 10⁻¹⁷); two attention mechanisms jointly identify an RNA processing module (P = 1 × 10⁻¹⁶).

Conclusion: CDT-II demonstrates that mechanism-oriented AI—designed around biological principles—can reveal regulatory structure directly from data, offering a paradigm shift from purely task-optimized models to interpretable, hypothesis-generating tools for biology.

Abstract: Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that
  researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.
  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA
  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional
  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in
  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B
  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA
  processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing
  regulatory structure rather than merely optimizing predictions.

</details>


### [666] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 本文提出RALIS模型，通过多视图对比学习与专家混合模块结合，支持任意视图组合、数量和模态的人类活动识别，采用调整中心对比损失实现自监督表征学习与视图对齐，并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以灵活应对任意视图组合、视图数量及异构模态，尤其在人类活动识别任务中缺乏鲁棒性和适应性。

Method: 提出RALIS模型：1）引入调整中心对比损失进行自监督表示学习与视图对齐，支持视图权重以反映视图质量；2）将计算复杂度从O(V²)降至O(V)；3）采用带负载均衡策略的专家混合（MoE）模块适配任意视图组合；4）强调各组件在潜在空间中的几何关系与协同机制。

Result: 在涵盖惯性与人体姿态模态的四个数据集上验证，视图数从3到9不等，RALIS展现出优越性能与强灵活性。

Conclusion: RALIS有效解决了多模态多视图学习中视图可用性不固定的问题，兼顾表示学习、对齐效率与融合适应性，为实际动态传感场景提供了实用框架。

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [667] [HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training](https://arxiv.org/abs/2602.08762)
*Wen Xu,Zhetao Li,Yong Xiao,Pengpeng Qiao,Mianxiong Dong,Kaoru Ota*

Main category: cs.LG

TL;DR: 本文提出了一种名为HoGS的本地差分隐私（LDP）框架，通过生成合成图来同时保护图数据中的链接和节点特征隐私，并利用图的同质性分别重建图结构与节点特征，从而在保障隐私的同时显著提升GNN训练准确率。


<details>
  <summary>Details</summary>
Motivation: 现有本地差分私有GNN方法要么仅保护链接隐私，要么在同时保护链接和节点特征时导致严重效用损失，亟需一种兼顾隐私保护与模型性能的新方法。

Method: HoGS框架首先在LDP下收集图的链接和节点特征信息，然后利用图数据的同质性现象分别重建图结构和节点特征，生成合成图用于后续GNN训练。

Result: 在三个真实数据集上的实验表明，HoGS在GNN训练准确率上显著优于现有基线方法。

Conclusion: HoGS是一种高效且实用的LDP框架，能在强隐私保障下有效支持GNN训练，解决了链接与特征联合保护下的效用下降难题。

Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.

</details>


### [668] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: 本文提出了FreqLens，一种可解释的时间序列预测框架，通过可学习的频率发现和基于公理的频率归因，自动识别主导周期模式并提供理论保证的归因结果。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型通常缺乏可解释性，限制了其在需要可解释预测领域的应用。

Method: FreqLens提出两项创新：(1) 可学习的频率发现——通过sigmoid映射参数化频率基，并结合多样性正则化从数据中自动学习主导周期模式；(2) 公理化的频率归因——构建满足完备性、保真性、零频率性和对称性公理的理论框架，其每频率归因等价于Shapley值。

Result: 在Traffic和Weather数据集上，FreqLens实现了具有竞争力或更优的预测性能，并成功发现了物理意义明确的周期：Traffic中准确识别出24小时日周期（误差2.5%）和12小时半日周期（误差1.6%），Weather中识别出远超输入窗口长度的周周期。

Conclusion: FreqLens实现了真正意义上的频率级知识发现，并为归因质量提供了形式化的理论保障。

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [669] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

TL;DR: 本文研究了在贝叶斯优化（BO）中使用机器学习库默认超参数作为初始化是否能加速收敛，结果表明默认值并未带来统计显著优势。


<details>
  <summary>Details</summary>
Motivation: 主流贝叶斯优化通常以均匀随机采样初始化，但scikit-learn等库提供的默认超参数隐含专家知识，可能作为更有信息量的起点；该假设尚未被系统检验。

Method: 以截断高斯分布（中心为库默认值）生成初始点，对比均匀随机初始化；在BoTorch、Optuna、Scikit-Optimize三个BO后端，RF/SVM/MLP三类模型，五个分类与回归数据集上进行大规模实验；采用单侧二项检验评估统计显著性，并开展先验方差敏感性分析。

Result: 在所有实验设置下，基于默认值的初始化均未产生统计显著优势（p值0.141–0.908）；虽小方差先验在早期略有提升，但最终性能无差异。

Conclusion: 默认超参数不包含对优化有方向性指导意义的信息；建议将超参数调优视为模型开发的必要环节，优先采用数据驱动的搜索策略，而非依赖库默认值。

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [670] [A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation](https://arxiv.org/abs/2602.08785)
*Ofek Amran,Tom Gilat,Ron Levie*

Main category: cs.LG

TL;DR: 本文提出了一种统一的度量方法，定义了一个包含任意大小（稀疏与稠密）图的紧致度量空间，在该空间下消息传递图神经网络（MPNNs）满足Hölder连续性，从而增强了通用逼近定理和泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有MPNN泛化与逼近能力分析受限于图规模（要么仅适用于稠密图且图大小无界，要么仅限于大小有界的稀疏图），缺乏统一框架。

Method: 基于并扩展图极限理论中的graphop分析方法，定义一个涵盖所有大小图（稀疏与稠密）的紧致度量空间，并证明MPNN在此空间下具有Hölder连续性。

Result: 获得了比以往工作更强的通用逼近定理与泛化界。

Conclusion: 该统一度量框架克服了先前理论对图密度与规模的限制，为MPNN的理论分析提供了更普适、更有力的基础。

Abstract: Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are Hölder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are Hölder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.

</details>


### [671] [How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs](https://arxiv.org/abs/2602.08808)
*Yapei Chang,Kyle Lo,Mohit Iyyer,Luca Soldaini*

Main category: cs.LG

TL;DR: 本文提出了How2Everything框架，用于评估和改进面向目标的步骤化程序生成能力，包括数据挖掘（How2Mine）、评测基准（How2Bench）和自动化评分协议（How2Score），并通过强化学习显著提升了模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在真实复杂任务上大规模评估和提升步骤化“如何做”程序生成的正确性与实用性，缺乏可扩展、可靠且低成本的评测与优化闭环。

Method: 构建How2Everything框架：1）How2Mine从网页中自动挖掘大量多主题步骤化程序；2）How2Bench构建平衡、高质量的7K评测集；3）How2Score利用LLM裁判检测关键失败，并蒸馏为8B开源模型实现高一致性自动化评分；4）以How2Score为奖励进行RL微调。

Result: How2Bench揭示了模型规模与训练阶段对程序生成能力的清晰缩放规律；RL优化使三个模型在How2Bench上提升超10分，且未损害标准基准性能，效果鲁棒于记忆或格式偏差。

Conclusion: How2Everything实现了基于网络数据的大规模、闭环式程序生成能力评估与提升，为LLM在复杂任务规划中的可信发展提供了可扩展方法论。

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

</details>


### [672] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

TL;DR: 本文综述了面向生物特征识别应用的高效深度学习方法，提出了分类体系，讨论了评估模型效率的多维指标，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习在安全与防御领域应用广泛，但其高计算需求导致高能耗和碳足迹，限制了在边缘设备上的实时部署与可扩展性。

Method: 采用文献综述方法，构建高效深度学习方法的分类体系，并系统梳理训练与部署中的挑战；提出涵盖内存、计算量、延迟、吞吐量等在内的多维效率评估指标。

Result: 建立了面向生物识别的高效深度学习方法分类框架；明确了当前效率评估指标的不足，倡导统一、可复现的评测标准；总结了若干关键未来研究方向。

Conclusion: 提升深度学习模型的能效比对推动其在资源受限场景（尤其是生物识别与边缘安全）中的落地至关重要，需从算法设计、评估标准与硬件协同等多方面推进。

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [673] [$\texttt{lrnnx}$: A library for Linear RNNs](https://arxiv.org/abs/2602.08810)
*Karan Bania,Soham Kalburgi,Manit Tanwar,Dhruthi,Aditya Nagarsekar,Harshvardhan Mestha,Naman Chibber,Raj Deshmukh,Anish Sathyanarayanan,Aarush Rathore,Pratham Chheda*

Main category: cs.LG

TL;DR: 本文介绍了lrnnx，一个统一的软件库，用于实现多种现代线性循环神经网络（LRNN）架构，旨在提高LRNN研究和应用的可访问性、可复现性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN实现分散在不同软件框架中，依赖框架特定优化，有时需要自定义CUDA内核或缺乏公开代码，导致使用、比较或扩展LRNN需大量实现工作。

Method: 提出并实现了一个名为lrnnx的统一软件库，提供共同接口支持多种现代LRNN架构，并暴露多级控制以支持核心组件或高层模型抽象的使用。

Result: lrnnx库提高了LRNN研究与应用的可访问性、可复现性和可扩展性，并以宽松的MIT许可证开源代码。

Conclusion: lrnnx为LRNN研究者和开发者提供了标准化、易用且可扩展的工具，有助于推动该领域的发展与实际应用。

Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.

</details>


### [674] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: 本文提出Fine-tuning Robust Policy Optimization (FRPO)，一种增强RLHF策略在后续微调中鲁棒性的新框架，通过在KL约束邻域内优化奖励稳定性，缓解灾难性遗忘问题，尤其提升安全性保持能力，且不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准RLHF训练后的模型在下游微调时易发生灾难性遗忘（如安全行为退化），表明其策略本身存在脆性；需从预微调阶段即提升策略对后续适应的鲁棒性，而非仅依赖下游保护方法。

Method: 提出FRPO框架，采用max-min优化目标，在当前策略的KL约束邻域内最大化最小奖励，确保策略在可微调范围内保持奖励稳定；基于GRPO改进，实现零额外计算开销。

Result: 在多个基础模型和下游微调方式（SFT与RL）下，FRPO显著降低安全性退化，同时保持下游任务性能；在数学推理RL场景中也验证了其对准确率的保持能力。

Conclusion: 预微调阶段的策略鲁棒性至关重要；FRPO通过奖励稳定性建模，为构建可演化、可持续对齐的大模型提供了新范式。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [675] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

TL;DR: 本文揭示了AI开源生态中普遍存在的'宽松许可清洗'（permissive washing）现象：大量AI数据集和模型虽标有MIT、Apache-2.0等宽松许可证，却缺失法定要求的许可证文本、版权声明和归属信息，导致其实际法律状态不明，下游使用可能构成侵权。作者对12万多个AI供应链实例进行实证审计，发现超95%的数据集和模型不满足基本许可要求，且上游合规信息极少向下游传递。


<details>
  <summary>Details</summary>
Motivation: 尽管MIT、Apache-2.0等宽松许可证在AI开源中广泛使用，但其法定要求（如包含完整许可证文本、版权声明、保留上游归属）在实践中常被忽略，导致‘名许实禁’的法律风险，威胁AI供应链的合法复用与可持续发展。

Method: 对Hugging Face和GitHub上124,278条数据集→模型→应用的AI供应链进行大规模实证审计，覆盖3,338个数据集、6,664个模型和28,516个应用，系统检查许可证文件、版权声明及归属信息的完整性与传播性。

Result: 96.5%的数据集和95.8%的模型缺失必需的许可证文本；仅2.3%的数据集和3.2%的模型同时满足许可证文本与版权声明要求；即便上游合规，下游传播率极低：仅27.59%的模型保留合规数据集归属，仅5.75%的应用保留合规模型归属。

Conclusion: AI生态中‘宽松许可清洗’现象极为普遍，许可证元数据不可信，法律效力取决于实际存在的许可证文件与声明；必须将许可证合规纳入AI供应链治理核心，并建立自动化验证机制。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [676] [Kirin: Improving ANN efficiency with SNN Hybridization](https://arxiv.org/abs/2602.08817)
*Chenyu Wang,Zhanglu Yan,Zhi Zhou,Xu Chen,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出Kirin方法，通过混合整数与脉冲编码，在保证精度无损的前提下，显著降低ANN-to-SNN转换的能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）推理能力强但能耗高；脉冲神经网络（SNNs）能效高但转换中存在时间窗口长、单脉冲信息损失大或多脉冲能耗高的问题。

Method: 提出Kirin：1）脉冲矩阵混合策略，将低比特参数转为二值脉冲以缩短时间窗，其余保留整数格式；2）静默阈值机制，调控单脉冲发放时机，确保输出数学等价于原LLM输出。

Result: 在W4A4&8量化下，Kirin达到近FP16精度，能耗降低84.66%，时间步减少93.75%。

Conclusion: Kirin实现了精度无损、时间高效且能量高效的ANN-to-SNN转换，为LLM部署到低功耗边缘设备提供了可行路径。

Abstract: Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>


### [677] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 本文提出FlexMoRE，一种灵活的混合专家架构，允许专家采用全尺寸或低秩适配器形式，并通过系统实验验证了专家秩与下游任务性能之间的权衡关系，发现推理密集型任务需要更高秩，而知识密集型任务对秩更不敏感；在显著减少参数量（<1/3）的同时，FlexMoRE在120个任务上的平均得分（47.18）优于全尺寸专家基线FlexOlmo（45.46）。


<details>
  <summary>Details</summary>
Motivation: 现有联邦式混合专家架构依赖全尺寸专家，但作者假设在某些领域中低秩适配器已足够，从而提升内存效率并保持甚至提升性能。

Method: 提出FlexMoRE架构，支持全尺寸专家与不同秩（2^0至2^14）的低秩适配器混合；基于FlexOlmo预训练专家构建低秩版本；开展150种混合配置（96种双专家、54种七专家）在120个下游任务上的评估；进行回归分析以建模专家秩与性能的关系。

Result: 推理密集型基准所需最优秩显著高于知识密集型基准；FlexMoRE以10.75B参数（仅为FlexOlmo 33.27B的约32%）取得更高平均分（47.18 vs. 45.46）。

Conclusion: 专家秩应根据任务类型（推理vs.知识）动态选择；FlexMoRE在大幅降低参数量的同时实现了性能提升，验证了低秩专家在混合架构中的有效性与实用性。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [678] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 本文提出Dr. MAS，一种针对多智能体大语言模型（LLM）系统的稳定强化学习（RL）训练方法，通过为每个智能体独立计算优势函数的归一化基准，解决了全局归一化导致的梯度不稳定问题，并在数学推理与搜索任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在角色专业化下具备高级推理与工具使用能力，但基于群体的强化学习后训练仍面临严重不稳定性问题。

Method: 理论分析指出GRPO类优化中全局归一化基线偏离各智能体异质奖励分布是梯度范数不稳定的根源；据此提出Dr. MAS：采用按智能体独立统计（均值与标准差）归一化优势函数，并构建支持可扩展编排、灵活单智能体配置与共享资源调度的端到端RL训练框架。

Result: 在Qwen2.5/Qwen3模型上的多智能体数学推理与多轮搜索任务中，Dr. MAS显著优于基线GRPO（如数学任务avg@16提升+5.6%，搜索任务pass@16提升+13.1%），同时基本消除梯度尖峰，并在异构模型分配下保持高效与鲁棒性。

Conclusion: Dr. MAS通过智能体粒度的优势归一化，从理论与实践上解决了多智能体LLM系统RL训练的稳定性瓶颈，为该方向提供了简单、通用且高效的训练范式。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [679] [Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization](https://arxiv.org/abs/2602.08855)
*Yang Qiu,Yixiong Zou,Jun Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于能量驱动的生成式增强框架E2A，用于解决图神经网络在分布外（OOD）泛化中出现的最小偏移翻转（MSF）现象，通过引入局部鲁棒半径和能量函数建模损失平坦性与稳定性，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）对分布偏移高度敏感，尤其是未被充分研究的最小偏移翻转（MSF）现象——测试样本仅轻微偏离训练分布即被错误分类。

Method: 从Sharpness-Aware Minimization（SAM）视角重新审视MSF，定义局部鲁棒半径量化损失尖锐性，并发现其在训练中持续减小；进而提出能量函数作为鲁棒半径的单调可计算代理，并构建能量驱动的生成式增强框架（E2A），利用能量引导的潜在空间扰动生成伪OOD样本。

Result: 在多个基准数据集上，E2A一致提升图OOD泛化性能，优于当前最优方法。

Conclusion: 局部损失尖锐性（由鲁棒半径刻画）是导致MSF的关键因素；基于能量的优化与生成增强能有效缓解该问题，为图神经网络的鲁棒泛化提供了新思路与实用框架。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>


### [680] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

TL;DR: 本文提出了一种基于度量空间“量级（magnitude）”概念的新型数据集距离度量——“量级距离（magnitude distance）”，具有可调尺度参数t，兼顾全局结构与局部细节，并在高维场景下保持判别力，还可作为生成模型训练目标。


<details>
  <summary>Details</summary>
Motivation: 量化数据集间距离是数学和机器学习中的基础问题；现有方法在高维下常缺乏判别力，需一种兼具理论保证与实用性的新距离度量。

Method: 基于度量空间的magnitude定义magnitude distance，引入可调尺度参数t控制对不同尺度结构的敏感性，并从理论上分析其极限行为与度量性质；将其用作push-forward生成模型的训练目标。

Result: 证明了magnitude distance的若干理论性质（如多尺度极限行为、满足度量公理的条件），并在实验中验证其在高维下仍具判别力，且作为生成模型目标时性能媲美主流距离方法。

Conclusion: Magnitude distance是一种理论严谨、尺度可控、高维鲁棒的距离度量，既拓展了magnitude理论的应用边界，也为生成建模提供了新工具。

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [681] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 本文提出了一种随机在线算法，实现了对自适应选择的Lipschitz凸损失序列的近最优O~(√T)期望swap regret，改进了先前O~(T^{2/3})的界，并首次为中位数校准提供了O~(√T)校准误差保证。


<details>
  <summary>Details</summary>
Motivation: 改进现有swap regret上界，解决Fishelson等人提出的开放问题，并拓展校准误差最小化算法的应用范围（如中位数校准）

Method: 提出多尺度分箱（multi-scale binning）技术：在多个粒度尺度上对单位区间进行离散化，并同时利用所有尺度做随机预测；算法运行时间为poly(T)。

Result: 实现O~(√T)期望swap regret，优于先前O~(T^{2/3})；首次为中位数校准提供O~(√T)校准误差保证；无需识别函数Lipschitz性假设。

Conclusion: 所提多尺度分箱方法不仅解决了swap regret界的开放问题，还推动了通用可识别性质的校准误差最小化，尤其拓展至非Lipschitz情形（如中位数）。

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [682] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: AnomSeer 是一种面向时间序列异常检测（TSAD）的多模态大语言模型方法，通过引入专家级链式推理和新型时间序列引导策略优化（TimerPO），提升异常分类、定位与解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在TSAD中依赖粗粒度时间序列启发式，缺乏对多维、细粒度时序结构的深入推理能力。

Method: 提出AnomSeer框架：1）生成基于经典分析（如统计量、频域变换）的专家链式推理路径；2）设计时间序列引导策略优化（TimerPO），包含基于最优传输的时间序列优势函数和正交投影机制。

Result: 在多种异常场景下，AnomSeer（基于Qwen2.5-VL-3B/7B-Instruct）在分类与定位准确率上超越GPT-4o等更大商业模型，尤其在点异常和频率驱动异常上表现突出，并能生成可解释的细粒度推理轨迹。

Conclusion: 将细粒度时间序列结构显式融入MLLM推理与训练过程，可显著提升其在TSAD任务中的性能与可解释性。

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [683] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

TL;DR: 本文提出了一种自动红队测试管道，用于生成针对对齐审计方法的欺骗性系统提示，发现现有白盒和黑盒审计方法易受战略欺骗，尤其在激活层面，揭示了当前方法在面对强能力错位模型时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐审计方法未被系统性地针对欺骗策略进行压力测试，而对齐审计的目标是稳健识别具有隐藏目标、具备情境感知能力的策略性错位模型。

Method: 构建了一个自动红队测试管道，生成针对特定白盒（如稀疏自编码器、词元嵌入相似性）和黑盒（如助手预填充、用户角色采样）审计方法的欺骗性系统提示，并在秘密保持型模型生物上进行压力测试。

Result: 该红队管道成功生成了能同时欺骗白盒与黑盒审计方法的提示，导致其做出高置信度但错误的判断；首次提供了基于激活的战略性欺骗的实证证据。

Conclusion: 当前主流的白盒与黑盒对齐审计方法在面对足够强大的错位模型时缺乏鲁棒性，亟需更抗欺骗的审计机制。

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [684] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文提出了一种基于势函数（potentials）的非短视在线匹配策略优化框架，用于改进心脏移植器官分配，通过自监督模仿学习训练高维、更具表达力的势函数，以模拟具有完全预知能力的算法，在真实历史数据上显著优于现有政策。


<details>
  <summary>Details</summary>
Motivation: 当前心脏移植器官分配政策未能充分考虑器官动态到达和等待名单患者构成，导致效率低下；美国正从僵化的规则式分配转向灵活的数据驱动模型。

Method: 提出基于势函数的非短视在线匹配策略优化框架；采用自监督模仿学习方法，训练高维、更富表达力的势函数以逼近具备完全预知能力的‘全知算法’。

Result: 在真实历史数据上验证，所提策略显著优于现行美国政策及拟议的连续分配框架，在提升群体层面健康结局方面效果突出。

Conclusion: 该方法为心脏移植等器官分配提供了可扩展、理论严谨且具实践可行性的新路径，恰逢美国现行心脏分配系统正处于政策审查关键期。

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [685] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

TL;DR: 本文提出SimpliPy，一种比SymPy快100倍的规则化符号简化引擎，显著提升摊销符号回归（amortized SR）的效率与可扩展性，并在FastSRB基准上超越现有摊销方法，同时在精度上媲美PySR但生成更简洁表达式。


<details>
  <summary>Details</summary>
Motivation: 摊销符号回归（amortized SR）受限于通用计算机代数系统（如SymPy）简化等价表达式时计算开销过大，难以扩展到真实科学复杂度问题。

Method: 提出SimpliPy——一个轻量、规则驱动的符号简化引擎，替代SymPy用于等价表达式归一化；将其集成至Flash-ANSR框架中，支持大规模训练集、高效token利用及测试集等价去污。

Result: SimpliPy实现相较SymPy 100倍加速且质量相当；Flash-ANSR在FastSRB上显著优于NeSymReS和E2E，在精度上媲美PySR，且随推理预算增加返回更简洁而非更复杂的表达式。

Conclusion: 高效归一化是摊销SR可扩展性的关键瓶颈；SimpliPy为该范式提供了实用基础，推动其向真实科学应用迈进。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [686] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出了一种基于离散桥匹配模型的互信息（MI）估计器DBMI，用于离散随机变量间的MI估计，尤其适用于传统方法难以处理的离散数据场景。


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计器在处理离散数据时存在困难，而扩散桥模型在离散状态空间中展现出强大生成能力，启发作者将其用于MI估计。

Method: 将MI估计建模为域迁移问题，利用离散状态空间下的桥匹配模型构建Discrete Bridge Mutual Information (DBMI)估计器。

Result: DBMI在低维和图像两类MI估计任务上均展现出良好性能。

Conclusion: DBMI为离散数据的互信息估计提供了一种新颖且有效的解决方案，拓展了桥匹配模型的应用边界。

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [687] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为门控子空间引导（GSS）的选择性记忆缓解方法，通过探测-引导机制实现上下文感知的记忆抑制，在显著降低计算开销的同时达到或超越现有最优效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）会逐字记忆并复现训练序列，损害泛化能力和隐私；而现有缓解方法采用统一干预，导致正常泛化token性能下降。

Method: 提出门控子空间引导（GSS），将干预分解为探测（识别记忆相关激活）和引导（仅在探测值超阈值时进行目标修正），其探针-引导对由基于最优子空间引导的优化框架导出。

Result: 在四个基准上，GSS在记忆减少效果上匹配或超越当前最优方法，且计算开销仅为其他优化类方法的1/100–1/1000；同时提供了关于神经表征中记忆几何结构的新理论见解。

Conclusion: 记忆现象具有稀疏性、间歇性和token依赖性，因此需上下文感知的动态干预；GSS验证了该思路的有效性与高效性，为可控记忆抑制提供了新范式。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [688] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文提出正向分布偏移（PDS）概念，指出在特定训练分布D'下进行学习，可使原本计算困难的问题变得易于处理，且该优势主要体现在计算层面而非统计层面。


<details>
  <summary>Details</summary>
Motivation: 传统上分布偏移被视为负面因素，本文反其道而行之，探讨如何利用精心选择的训练分布D'使学习更容易，即正向分布偏移（PDS）。

Method: 形式化定义多种PDS变体，分析其理论性质，并与成员查询学习建立联系，同时论证PDS带来的计算优势。

Result: 证明某些在标准设定下难以学习的函数类，在PDS设定下可被高效学习；揭示PDS常带来计算复杂度下降，使梯度法可解原本难解问题。

Conclusion: PDS是当代机器学习中一项关键思想——创新常在于设计更好的训练分布而非改进算法本身；PDS主要提供计算收益，拓展了可学习问题的边界。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [689] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

TL;DR: GEMSS是一种变分贝叶斯方法，用于在高维、低样本、强相关数据中同时发现多个多样化的稀疏特征子集，以提升可解释性与领域洞察力。


<details>
  <summary>Details</summary>
Motivation: 在n远小于p且变量高度相关的场景下，存在多个同样能解释响应的稀疏特征子集，但传统方法仅返回单一解，掩盖了潜在机制的多样性。

Method: 提出GEMSS框架：采用结构化spike-and-slab先验实现稀疏性，用高斯混合分布近似多峰后验，并引入Jaccard相似度惩罚控制解的多样性；通过随机梯度下降联合优化整个解集。

Result: 在128组合成实验中验证，GEMSS可扩展至p=5000、n=50，支持回归/分类、天然处理缺失值、对类别不平衡和高斯噪声鲁棒。

Conclusion: GEMSS为高维小样本数据提供了可解释、稳健且实用的多解稀疏建模工具，并已开源为Python包及可视化应用。

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [690] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 本文提出一种基于扩散过程的Transformer重配置方法，将每个特征变换块建模为概率映射，构建一条概率路径以实现表征不确定性的原理性传播，同时保持原有预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练Transformer缺乏在特征变换堆栈中原理性传播不确定性的机制，而不确定性校准对其在风险敏感场景中的可靠部署至关重要。

Method: 提出扩散启发的Transformer重配置方法，将每个特征变换块建模为概率映射，组合形成模仿扩散过程结构的概率路径，并将其重新编译到具有统一转移模型的扩散过程中，以实现不确定性在整个架构中的原理性传播。

Result: 在多个视觉和语言基准上，该方法在不确定性校准和预测精度方面均优于现有的不确定性感知Transformer。

Conclusion: 所提方法为预训练Transformer提供了原理性的不确定性传播机制，在不牺牲预测性能的前提下显著提升了模型校准能力。

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [691] [DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce](https://arxiv.org/abs/2602.08923)
*Wenchen Han,Shay Vargaftik,Michael Mitzenmacher,Ran Ben Basat*

Main category: cs.LG

TL;DR: 本文提出DynamiQ量化框架，专为多跳all-reduce优化，通过新颖的偏和表示与融合核设计，在保持近基线精度的同时显著加速大模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着大模型训练规模扩大，网络成为瓶颈；现有梯度量化方法未针对多跳聚合中多次部分求和的特点进行优化。

Method: 提出DynamiQ量化框架，包含适配多跳聚合的新型偏和表示技术，以及解压-累加-重压缩融合核，并集成到PyTorch DDP（基于NCCL P2P）。

Result: 在多种LLM、任务和规模下，相比Omni-Reduce、THC及MXFP4/6/8等SOTA方法，训练加速最高达34.2%，且唯一实现持续接近BF16基线精度（如99.9%）的方法。

Conclusion: DynamiQ成功弥合了量化最佳实践与多跳聚合之间的鸿沟，在精度与速度间取得更优平衡，是面向大规模分布式训练的高效量化方案。

Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>


### [692] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: 本文提出StealthRL，一种基于强化学习的对抗性改写攻击框架，用于压力测试AI文本检测器在语义保持前提下的鲁棒性；其在多检测器集成下使用GRPO算法与LoRA微调Qwen3-4B模型，实现近乎零检出率、高攻击成功率，并揭示检测器间共有的架构脆弱性。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器面临对抗性改写攻击的鲁棒性挑战，需在真实场景下评估其安全性。

Method: 提出StealthRL框架，采用Group Relative Policy Optimization（GRPO）算法，结合LoRA适配器微调Qwen3-4B模型，在多检测器集成上训练 paraphrase 策略，优化兼顾检测规避与语义保真度的复合奖励函数。

Result: 在1%假正率操作点下，对三类检测器平均TPR降至0.001，AUROC从0.74降至0.27，攻击成功率99.9%；攻击可迁移到未见检测器家族，表明存在通用架构漏洞；LLM质量评估与检测器分数分布分析进一步验证有效性。

Conclusion: 当前AI文本检测器存在严重鲁棒性缺陷；StealthRL为对抗性评估提供了原则性新协议，并开源代码与评估流程。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [693] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于生成模型的分布鲁棒优化（DRO）方法GAS-DRO，利用生成模型构建能覆盖名义分布之外对抗性分布的模糊集，并在理论上证明其收敛性，实验上验证了其在OOD泛化任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有DRO方法的模糊集需兼顾与名义分布的一致性和多样性，同时保证可解性；传统模糊集难以覆盖支持集外的对抗分布且常导致计算困难。

Method: 提出基于生成模型（如扩散模型）的模糊集，将DRO内层最大化问题转化为对生成模型参数空间的优化；设计GAS-DRO算法并建立其平稳点收敛理论。

Result: GAS-DRO在多个机器学习任务中展现出优于基线方法的OOD泛化性能；理论证明其具有平稳收敛性；算法在生成模型参数空间上可高效求解。

Conclusion: 生成模型可有效扩展DRO模糊集的表达能力，提升模型对分布偏移的鲁棒性与泛化性，GAS-DRO为DRO提供了兼具理论保障与实践可行性的新范式。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [694] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 本文提出了一种基于哈密顿力学的可学习位置编码框架Symplectic Positional Embeddings (SyPE)，以解决Transformer在时间序列预测中对非均匀时间变形（time-warping）建模能力不足的问题；SyPE将RoPE从SO(2)推广到Sp(2,ℝ)，并引入输入自适应的时间扭曲模块，配合StretchTime模型，在非平稳时序预测任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列（如金融、生物信号）常呈现“时间扭曲”动态，即有效时间流与采样索引不一致，而现有Transformer依赖的旋转位置编码（RoPE）无法建模非仿射时间扭曲，存在根本性建模缺陷。

Method: 提出Symplectic Positional Embeddings (SyPE)，从哈密顿力学出发，将RoPE中的二维旋转群SO(2)推广至辛群Sp(2,ℝ)，并设计输入依赖的自适应时间扭曲模块，使注意力机制能端到端地拉伸或压缩时间坐标；将其集成于新架构StretchTime中用于多变量时序预测。

Result: StretchTime在标准时序预测基准上达到SOTA性能，尤其在具有强非平稳时间动态的数据集上展现出更优鲁棒性。

Conclusion: SyPE为Transformer提供了严格且可学习的时间扭曲建模能力，突破了传统位置编码对均匀时间假设的依赖，为建模复杂真实世界时序开辟了新的理论与实践路径。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [695] [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](https://arxiv.org/abs/2602.08986)
*Isaac Xu,Martin Gillis,Ayushi Sharma,Benjamin Misiuk,Craig J. Brown,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出了一种结合节点不平衡加权和焦点加权的加权损失函数，用于提升分层多标签分类中深层节点的预测性能，尤其改善稀有节点的召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 分层多标签分类中，深层节点因天然稀有性和层级约束（子节点频率通常低于父节点）而难以被模型准确预测。

Method: 设计一种加权损失目标函数，融合节点级不平衡加权与基于集成不确定性量化得到的焦点加权组件，强调稀有节点而非稀有样本，并在训练中聚焦每个输出分布中不确定的节点。

Result: 在基准数据集上召回率最高提升达5倍，F1分数显著提升；对卷积网络在编码器欠佳或数据有限等挑战性任务中也有效。

Conclusion: 该加权损失策略能有效缓解分层分类中深层稀有节点预测困难的问题，提升整体性能与鲁棒性。

Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.

</details>


### [696] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

TL;DR: 本文提出DirMoE，一种基于Dirichlet变分自编码器的端到端可微路由机制，解耦专家选择与贡献分配，并通过Gumbel-Sigmoid和隐式重参数化实现全可微；其训练目标含显式稀疏性约束，能精确控制激活专家数并提升专家专业化。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型中Top-k+Softmax路由不可微，且将专家选择与贡献分配混为一谈，限制性能与扩展性。

Method: 提出Dirichlet-Routed MoE（DirMoE），用Bernoulli建模专家选择、Dirichlet建模选定专家的贡献分布；采用Gumbel-Sigmoid松弛和隐式重参数化实现全可微前向传播；优化变分ELBO目标函数，含显式稀疏惩罚与超参调度策略。

Result: DirMoE在性能上匹配或超越现有方法，同时提升专家专业化程度，并能精确控制期望激活专家数量。

Conclusion: DirMoE通过解耦路由核心问题与全可微设计，有效提升了MoE模型的可训练性、可控性与专家利用效率。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [697] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

TL;DR: 本文提出Adaptively Rotated Optimization (ARO)，一种以梯度旋转为核心设计原则的新型矩阵优化框架，通过在旋转坐标系中执行范数归一化最速下降，显著提升大语言模型（LLM）训练效率，在严格控制的基准测试中一致优于AdamW和正交化方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于正交化/白化的矩阵优化器虽有效，但存在范式局限；作者旨在探索超越正交化的全新优化范式，进一步提升LLM训练效率。

Method: 提出ARO框架，将梯度旋转作为首要设计原则，采用范数感知策略动态确定旋转矩阵，并在旋转坐标系中执行范数归一化的最速下降更新；同时设计了严格控制的基准协议以减少评估偏差。

Result: ARO在最多8B激活参数的LLM预训练中，相比AdamW提速1.3~1.35倍，相比正交化方法提速1.1~1.15倍，且在高达8倍过训练预算下未见收益衰减；还可被重述为基于残差流旋转对称性的对称感知优化器。

Conclusion: ARO开辟了以旋转为核心的新型矩阵优化范式，不仅实证有效，还为利用跨层/跨模块耦合提供了可扩展、计算高效的新设计路径。

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [698] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文提出ShapeCond，一种基于shapelet的时间序列数据集压缩框架，通过shapelet引导的优化策略，在保持关键局部模式的同时显著提升压缩效率和下游分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法多针对图像设计，难以有效捕捉时间序列特有的时序结构（如shapelet等局部判别性模式），导致在时间序列任务中性能不佳。

Method: 提出ShapeCond框架，利用shapelet作为先验知识指导合成过程，设计与序列长度无关的shapelet辅助合成损失函数，实现高效、可扩展的时间序列数据压缩。

Result: 在多个数据集上显著优于现有SOTA方法；合成速度比CondTSC快29倍，比朴素shapelet方法快至10,000倍；下游分类准确率持续更高。

Conclusion: ShapeCond验证了引入时间序列特有结构（如shapelet）对数据集压缩的有效性，为高效、结构感知的时间序列学习提供了新范式。

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


### [699] [ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling](https://arxiv.org/abs/2602.09009)
*Yilang Zhang,Bingcong Li,Niao He,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文从优化角度重新审视残差连接，提出自适应神经连接重分配（ANCRe）框架，通过学习数据驱动的残差连接布局，在几乎不增加开销的前提下显著提升深层网络的收敛速度、性能与深度利用率。


<details>
  <summary>Details</summary>
Motivation: 深层网络中残差连接作为默认加深深度机制，但其布局对优化行为的影响缺乏深入理解；现有研究表明深层常被低效利用，亟需更优的连接机制。

Method: 提出ANCRe框架，将残差连接结构参数化并端到端学习；理论证明连接布局可导致收敛速率的指数级差异；设计轻量级可学习门控机制实现连接重分配。

Result: 在大语言模型预训练、扩散模型及深层ResNet上验证：收敛加速、性能提升、深度效率增强，计算与内存开销<1%。

Conclusion: 残差连接的布局是影响深层网络优化的关键因素；ANCRe提供了一种普适、高效、可学习的连接重配置范式，显著改善深度利用效果。

Abstract: Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [700] [LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation](https://arxiv.org/abs/2602.07032)
*Yuheng Wu,Berk Gokmen,Zhouhua Xie,Peijing Li,Caroline Trippel,Priyanka Raina,Thierry Tambe*

Main category: cs.AI

TL;DR: LLM-FSM is a novel, automatically generated benchmark for evaluating LLMs' ability to translate natural-language FSM specifications into correct RTL code; it reveals performance degradation with increasing FSM complexity and shows that SFT and increased test-time compute improve results.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate LLMs' finite-state reasoning—critical for hardware design—using a scalable, automated, and verifiable benchmark beyond manually crafted examples.

Method: LLM-FSM uses an automated pipeline to generate 1,000 FSM problems: synthesizing configurable FSMs, prompting LLMs to produce YAML and NL specifications, and generating correct-by-construction RTL/testbench from YAML; evaluation includes LLM-based, SAT-solver-based, and human verification.

Result: Strong LLMs show sharply declining accuracy with increasing FSM complexity; supervised fine-tuning (SFT) improves generalization to out-of-distribution tasks; increased test-time compute enhances reasoning reliability.

Conclusion: LLM-FSM provides a robust, extensible benchmark revealing fundamental limitations in LLMs' finite-state reasoning, while offering actionable insights for improving RTL generation via training and inference strategies.

Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.

</details>


### [701] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: 本文提出ST-Raptor，一种面向半结构化表格问答的智能代理系统，通过视觉编辑、树状结构建模与代理驱动查询解析，提升理解准确性与用户友好性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Text-to-SQL、Text-to-Code及多模态大模型）在处理半结构化表格时存在信息丢失、布局理解差或答案不准等问题，而人工解析又费时费力。

Method: 提出ST-Raptor代理系统，融合视觉编辑界面、基于树的结构建模和多步代理式查询求解，实现对表格隐含逻辑、层次关系与语义关联的联合建模。

Result: 在基准与真实数据集上，ST-Raptor在准确率和可用性两方面均优于现有方法。

Conclusion: ST-Raptor为半结构化表格问答提供了更鲁棒、可交互、用户可控的新范式，有效弥合了结构化推理与布局感知之间的鸿沟。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [702] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 本文提出DLLM-Searcher框架，通过两阶段后训练（Agentic SFT与VRPO）提升扩散大语言模型（dLLM）的推理与工具调用能力，并设计并行式ReAct（P-ReAct）范式以降低搜索代理的端到端延迟，实验证明其性能媲美主流LLM代理且推理加速约15%。


<details>
  <summary>Details</summary>
Motivation: 解决当前搜索Agent面临的两大挑战：1）ReAct范式下串行执行导致的高延迟（Latency Challenge）；2）现有扩散语言模型（dLLMs）在推理和工具调用能力上的严重不足（Agent Ability Challenge）。

Method: 提出DLLM-Searcher框架，包含：1）两阶段后训练：Agentic SFT（有代理监督微调）和Agentic VRPO（有代理方差缩减偏好优化），以增强dLLM的信息检索与推理能力；2）新型代理范式P-ReAct，利用dLLM灵活生成机制，优先解码tool_call指令，实现‘边等待工具响应边继续推理’的并行化流程。

Result: DLLM-Searcher在搜索任务上达到与主流LLM基代理相当的性能；P-ReAct带来约15%的端到端推理加速。

Conclusion: dLLMs可通过针对性能力增强与新代理范式设计，有效克服其在智能体场景中的能力与效率瓶颈，为低延迟、高性能搜索Agent提供新路径。

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [703] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: Aster是一种用于自主科学发现的AI智能体，相比现有框架速度快20倍以上，通过迭代优化程序，在数学、GPU核工程、生物学、神经科学和大语言模型训练等多个领域均达到或接近SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统科学发现框架迭代效率低、难以应对长耗时评估任务（如数小时的机器学习训练）的问题，拓展可处理问题的范围。

Method: 给定任务、初始程序和性能评估脚本，Aster通过自动迭代优化程序，减少达到新发现所需的迭代次数。

Result: 在多个科学任务中取得SOTA结果；在ZAPBench上以不到人类最佳方案1/190的算力达到同等性能。

Conclusion: Aster显著提升了自主科学发现的效率与可行性，支持长耗时任务，并已通过网页界面和API开放使用。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [704] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 本文提出“空间理论”，即智能体通过主动探索构建、修正和利用空间信念的能力，并通过好奇心驱动的认知地图构建基准进行评估，发现当前多模态基础模型在主动探索中存在主动-被动差距、低效性、空间信念不稳定及信念惯性等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型擅长被动感知，但在需要自主主动获取信息的空间具身智能任务中能力不足，亟需系统评估其空间信念构建与更新能力。

Method: 提出Theory of Space概念，设计好奇心驱动的探索基准，引入空间信念探测（spatial belief probing）技术以揭示模型每步的内部空间表征，并采用虚假信念范式诊断信念更新问题。

Result: 发现四大问题：主动-被动性能差距、探索低效性、空间信念随时间退化、以及严重的信念惯性（尤其在视觉模型中）。

Conclusion: 当前基础模型难以在主动探索过程中维持连贯、可修正的空间信念，需在架构与训练机制上针对性改进。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [705] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: 本文提出Anchor框架，通过扩展少量高质量种子演示来生成大规模、多样化的桌面GUI交互数据，提升端到端GUI智能体的训练效果与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 端到端GUI智能体需大量高质量交互数据，但人工采集成本高，现有合成方法存在任务多样性不足或轨迹噪声大、目标偏移等问题。

Method: Anchor框架从少量验证过的种子演示出发，识别有意义的状态分支点，基于当前GUI上下文生成状态锚定的新任务变体；由执行代理生成新轨迹，并通过状态感知校验器确保任务完成和轨迹一致性；再采用任务条件下的步骤级过滤与分支后片段去噪提升监督质量。

Result: 在OSWorld和WindowsAgentArena基准上，基于Anchor扩展数据微调的模型显著优于零样本智能体及主流合成基线，且具备跨应用与跨操作系统的泛化能力。

Conclusion: Anchor提供了一种高效、可控、可扩展的桌面GUI监督数据构建范式，有效缓解了高质量交互数据稀缺问题，推动真实桌面环境GUI智能体的发展。

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [706] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: 本文提出PreFlect，一种前瞻性反思机制，通过在执行前批评和优化计划来提升大语言模型智能体性能，而非传统的事后修正；该方法结合历史轨迹中提取的规划错误模式与动态重规划能力，在多个基准测试中显著优于现有反思方法。


<details>
  <summary>Details</summary>
Motivation: 现有自反思方法本质上是回溯性的，即先行动再纠错，无法预防性地避免失败；作者旨在构建一种能在执行前预判并修正计划缺陷的前瞻性反思机制。

Method: 提出PreFlect机制：1）基于历史智能体轨迹蒸馏常见规划错误模式，支撑有依据的前瞻性反思；2）在执行前对计划进行批评与细化；3）引入动态重规划机制以应对执行中意外偏差。

Result: 在多个基准上，PreFlect显著提升了智能体在复杂现实任务中的整体效用，优于强反思基线及其他更复杂的智能体架构。

Conclusion: 前瞻性反思是一种更高效、更具预防性的智能体改进范式；PreFlect验证了在执行前进行有依据的计划优化可实质性提升智能体鲁棒性与任务完成能力。

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [707] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 本文通过分析2022–2025年间发布的809个大语言模型，发现前沿性能主要由训练算力驱动（占比80–90%），而非专有技术；但在非前沿区域，专有算法与共享技术进步显著降低达标的算力需求，且同一公司内部模型效率差异可达40倍以上。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLM）性能提升的核心驱动力是专有技术（‘秘密配方’）还是单纯算力扩展。

Method: 基于809个模型的训练与基准测试数据，构建含发布日期和开发者固定效应的缩放律回归模型。

Result: 前沿模型性能差异80–90%由训练算力解释；非前沿模型中，专有技术和算法进步显著降低达标所需算力；同一公司内模型算力效率差异可超40倍。

Conclusion: 算力规模是推动前沿LLM进步的主导因素，而专有技术在中低性能段更具差异化价值；AI领导力不仅取决于资源，也受内部研发效率影响。

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [708] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 本文提出将大语言模型中的幻觉检测问题重新定义为分布外（OOD）检测问题，并利用OOD检测技术实现无需训练、单样本即可检测推理任务中幻觉的高效方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在问答任务中表现良好，但在需推理的任务中效果较差；而OOD检测在其他领域（如计算机视觉）已较成熟，值得借鉴。

Method: 将语言模型的下一个词预测视为分类任务，适配并应用OOD检测技术，设计出无需训练、基于单样本的幻觉检测器。

Result: 所提OOD-based方法在推理任务的幻觉检测中达到高准确率，且具备训练免费和单样本检测优势。

Conclusion: 将幻觉检测重新建模为OOD检测是一种有前景且可扩展的语言模型安全提升路径。

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [709] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: 本文提出NAAMSE框架，通过进化式提示词变异与行为评分，实现对AI代理安全性的动态、多轮评估，兼顾攻击有效性与正常功能保持。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理安全评估依赖人工红队演练或静态基准测试，难以建模自适应、多轮对抗场景。

Method: 提出NAAMSE进化框架：由单一自主代理驱动，结合遗传式提示词变异、分层语料探索和非对称行为评分，以模型响应为适应度信号迭代优化攻击策略，并约束良性使用正确性。

Result: 在Gemini 2.5 Flash上的实验表明，该方法能系统性放大传统单次方法遗漏的漏洞；消融实验证实探索与定向变异协同可发现高危失效模式。

Conclusion: NAAMSE提供了一种更真实、可扩展的AI代理鲁棒性评估范式，适用于持续演化的威胁环境。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [710] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 本文提出将AI安全问题建模为Stackelberg安全博弈（SSG），将人类与机构视为战略参与者，强调在数据收集、模型评估与部署等环节中应对动态、对抗性激励，从而实现更主动、鲁棒的AI监督。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架多将对齐视为静态优化问题，忽视了AI生命周期中人类与机构面临的动态、对抗性激励，如数据污染、评估资源受限和恶意部署等。

Method: 引入Stackelberg安全博弈（SSG）建模AI监督过程，将审计者、评估者、部署者视为防御方，将恶意行为者、错位贡献者或最坏故障模式视为攻击方，统一分析激励设计、有限监督能力与对抗不确定性。

Result: 该框架可指导三类任务：(1) 训练阶段抵御数据/反馈投毒的审计；(2) 预部署阶段在评审资源受限下的评估；(3) 对抗环境中多模型鲁棒部署。

Conclusion: 将算法对齐与制度监督设计结合，利用博弈论威慑机制，使AI监督更具前瞻性、风险意识与抗操纵性。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [711] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: 本文提出了LINCSQA基准和PBio-Agent多智能体框架，用于预测批量细胞中复杂化学扰动下的目标基因调控，通过难度感知任务排序和迭代知识精炼提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理高维扰动结果时表现不佳，且先前研究主要关注单细胞遗传扰动，忽略了药物发现中关键的批量细胞化学扰动问题。

Method: 提出LINCSQA基准，并构建PBio-Agent多智能体框架，包含基于生物知识图谱的专业智能体、综合输出的合成智能体及确保逻辑一致性的专业裁判智能体，采用难度感知任务排序与迭代知识精炼策略。

Result: PBio-Agent在LINCSQA和PerturbQA基准上均超越现有基线方法，使更小规模模型无需额外训练即可预测并解释复杂生物学过程。

Conclusion: 利用因果结构共享特性与多智能体协同推理可显著提升对批量化学扰动下基因调控的预测能力，为药物发现提供新范式。

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [712] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: 本文提出BRIDGE框架，利用项目反应理论模型，将AI模型在多基准测试中的表现与人类任务完成时间对齐，从而实现任务难度的可解释量化和前沿模型能力的预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类任务完成时间标注的AI系统评估方法成本高、噪声大且难以扩展。

Method: 提出BRIDGE框架，采用双参数逻辑斯蒂项目反应理论（IRT）模型，联合估计任务的潜在难度和模型能力，并建立潜在难度与人类完成时间对数之间的线性关系。

Result: 验证了潜在任务难度与人类完成时间的对数呈线性关系；可仅凭模型性能推断新基准的人类完成时间；成功复现METR的指数缩放规律，即50%可解任务时长的‘地平线’约每6个月翻倍。

Conclusion: BRIDGE为AI能力评估提供了可解释、可扩展、与人类认知对齐的统一心理测量框架。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [713] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 本文提出了一种两阶段梯度分析框架，用于多智能体强化学习（MARL）中可解释的级联故障诊断，包括定位初始故障源（Patient-0）、验证下游误报原因及追踪故障传播路径，在多个基准任务中实现了高精度检测并提供几何可解释性证据。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在安全关键领域日益广泛应用，但目前仍缺乏可解释的故障检测与归因方法。

Method: 提出两阶段梯度框架：第一阶段通过策略梯度代价的泰勒余项分析进行每智能体可解释故障检测，识别初始Patient-0；第二阶段利用评论家导数的几何分析（一阶敏感性与方向性二阶曲率），在因果时间窗内聚合构建可解释的传染图，解释‘下游优先’检测异常。

Result: 在Simple Spread（3/5智能体）和StarCraft II上分别测试500和100个episode，使用MADDPG和HATRPO算法，Patient-0检测准确率达88.2–99.4%，并提供可解释的梯度级诊断证据。

Conclusion: 该框架将黑盒检测提升为可解释的梯度级法证分析，为安全关键MARL系统中的级联故障诊断提供了实用、可解释的工具。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [714] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: 本文提出TermiGen，一种用于生成可验证终端环境和鲁棒专家轨迹的端到端管道，以解决开源大模型在复杂终端任务执行中的两大瓶颈：高质量可执行训练环境稀缺与指令微调数据缺乏对常见错误及恢复过程的覆盖；通过多智能体迭代生成Docker环境、注入错误的Generator-Critic协议采集纠错轨迹，微调后的TermiGen-Qwen2.5-Coder-32B在TerminalBench上达到31.3%通过率，创下开源模型新SOTA。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型在执行复杂终端任务时面临两大瓶颈：一是高保真、可执行的训练环境稀缺（真实仓库合成环境缺乏多样性与可扩展性，LLM合成轨迹易产生幻觉）；二是标准指令微调所用专家轨迹极少包含小模型常犯的简单错误，导致学生模型难以从运行时失败中恢复。

Method: 提出TermiGen端到端框架：首先通过迭代式多智能体精炼循环生成功能正确的任务和Docker容器；其次采用Generator-Critic协议，在轨迹采集过程中主动注入错误，生成富含错误-纠正循环的数据。

Result: 基于TermiGen生成数据微调的TermiGen-Qwen2.5-Coder-32B在TerminalBench上取得31.3%的pass rate，成为当前开源权重模型的新SOTA，并超越o4-mini等强闭源模型。

Conclusion: TermiGen有效缓解了终端任务训练中环境真实性与轨迹鲁棒性不足的问题，为提升开源代码/终端模型的错误恢复能力提供了可扩展、可验证的新范式。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [715] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: 本文提出了SynthAgent，一种用于模拟肥胖合并精神障碍患者的多智能体系统框架，通过整合临床数据和患者特征生成高保真虚拟患者，并评估了不同大模型在该框架中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界医疗数据碎片化、有偏见及隐私受限的问题，同时研究复杂疾病（如肥胖合并精神障碍）的进展与治疗响应。

Method: 构建基于多智能体系统（MAS）的SynthAgent框架，整合索赔数据、人口调查和以患者为中心的文献，建模个性化虚拟患者并赋予人格特质；利用大语言模型（如GPT-5、Claude 4.5 Sonnet等）驱动智能体交互以模拟疾病进展与行为决策。

Result: 在100多个生成患者上的评估显示，GPT-5和Claude 4.5 Sonnet作为核心引擎表现出最高保真度，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展、隐私保护的框架，支持医学与心理学领域中患者旅程、行为动态及决策过程的研究。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [716] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文提出STEER2ADAPT框架，通过组合预定义的语义基向量而非从头学习新方向，实现对大语言模型的轻量、动态、数据高效推理时适配，在推理与安全任务上平均提升8.2%。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖单一静态方向，难以应对任务变化和需多能力协同的复杂任务。

Method: 构建低维可复用的语义先验子空间，基于少量示例动态线性组合其基向量以适配新任务。

Result: 在9个任务、3个模型上验证有效，平均性能提升8.2%；兼具数据高效性、稳定性与透明性。

Conclusion: STEER2ADAPT是一种轻量、动态、可解释的推理时适配框架，显著提升了激活引导方法的灵活性与实用性。

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [717] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 本文提出了一种基于ICAP框架的自适应认知参与度调节系统，在逻辑智能辅导系统中动态选择引导式（主动）和含错误（建构）示例，比较了贝叶斯知识追踪（BKT）与深度强化学习（DRL）两种自适应方法的效果，发现二者均显著提升学生成绩，且对不同先验知识水平学生效果各异。


<details>
  <summary>Details</summary>
Motivation: 个性化学习活动以激发最优认知参与度是智能辅导系统中的关键挑战。

Method: 开发并评估一种自适应调节认知参与度的系统，通过动态选择两种ICAP模式的例题（引导式示例和含错误示例），对比贝叶斯知识追踪（BKT）、深度强化学习（DRL）与非自适应基线方法在逻辑ITS中的效果。

Result: 实验表明，两种自适应策略均显著提升学生测试成绩；BKT对低先验知识学生后测成绩提升最大，助其追赶高先验知识学生；DRL则使高先验知识学生获得更高后测分数。

Conclusion: 认知参与度调节与自适应策略存在复杂交互关系，其效果因学习者先验知识水平而异，为ITS设计提供了新见解。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [718] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: 本文提出RAPiD框架，将基于扩散模型的轨迹规划器蒸馏为确定性策略，通过分数正则化策略优化和预测式驾驶员控制器模仿训练，实现高效、安全、舒适的实时驾驶规划。


<details>
  <summary>Details</summary>
Motivation: 扩散模型轨迹规划器虽能建模人类驾驶行为的多模态性，但其迭代随机采样机制难以满足实时性和安全性要求。

Method: 提出RAPiD框架：1）利用预训练扩散模型的得分函数作为行为先验，进行分数正则化策略优化；2）训练一个模仿预测式驾驶员控制器的critic，提供密集的安全导向监督以优化策略。

Result: 在nuPlan闭环评测中性能媲美扩散基线且推理速度快8倍；在interPlan基准上达到学习型规划器中最优泛化能力。

Conclusion: RAPiD成功实现了从随机扩散采样到高效确定性策略的转化，在保持性能的同时显著提升效率与安全性，适用于实际自动驾驶部署。

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [719] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

TL;DR: 本文提出了SupChain-Bench基准用于评估大语言模型在真实供应链管理中的长程、多步工具调用能力，并提出无需标准操作流程（SOP）的SupChain-ReAct框架，显著提升工具调用可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽在复杂推理和工具使用上有潜力，但在需严格遵循领域特定规程的长周期、多步骤供应链任务中仍不可靠，亟需系统性评估与改进。

Method: 构建真实世界基准SupChain-Bench，涵盖供应链领域知识与基于标准操作流程（SOP）的长程工具编排；提出SupChain-ReAct框架，通过自主合成可执行工具调用流程，摆脱对预定义SOP的依赖。

Result: 实验表明当前模型在长程工具执行上存在显著可靠性差距；SupChain-ReAct在工具调用性能上表现最强且最稳定。

Conclusion: SupChain-Bench为研究现实运营场景中可靠长程编排提供了原则性基准；LLM驱动的供应链智能体仍有巨大提升空间。

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [720] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了一种宽深结合的研究代理框架（Wide and Deep research agent），通过内在并行工具调用（scaling width）而非仅增加推理步数（scaling depth），提升深度研究任务的性能与效率，并在BrowseComp基准上超越了更强模型的原始表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理主要通过增加串行推理步数（scaling depth）来提升能力，而并行工具调用（scaling width）的潜力尚未被系统探索。

Method: 提出宽深研究代理框架，利用单智能体内的内在并行工具调用实现一步内多任务协同；设计并比较多种工具调度策略；通过案例分析探究性能提升动因。

Result: 在深度研究基准上，扩展宽度显著提升性能、减少推理轮次；GPT-5-Medium在BrowseComp上达62.2%准确率，超过GPT-5-High原报告的54.9%。

Conclusion: 宽度（并行性）与深度（序列性）的协同优化是构建高效率深度研究代理的关键路径，无需复杂上下文管理即可取得显著增益。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [721] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: 本文提出VGAS框架，通过生成-选择范式解决视觉-语言-动作（VLA）模型在少样本适应中的几何歧义问题，利用Q-Chunk-Former批评器和显式几何正则化提升动作选择的语义保真度与几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在少样本任务适应中常因几何歧义导致执行失败，尤其在监督信号稀缺时难以区分近似但结果迥异的动作候选。

Method: 提出VGAS框架：以微调VLA为高召回动作生成器，引入几何感知的Q-Chunk-Former Transformer作为批评器进行best-of-N动作块选择，并设计显式几何正则化（EGR）稳定价值排序。

Result: 实验与理论分析表明，VGAS在少量演示和分布偏移下显著提升任务成功率与鲁棒性。

Conclusion: 生成-选择范式结合几何建模与正则化可有效缓解VLA少样本适应中的几何歧义问题，为可靠具身智能提供新思路。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [722] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型（LLMs）在模拟人类冲突行为时是否能再现基于人格特质（如大五人格）的行为模式，提出了一套评估框架和新型数据构建方法，并发现当前主流LLM在人格驱动的冲突行为上与人类存在显著差异，警示其在社会关键场景中作为行为代理的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 人类人格显著影响其在社会互动（尤其是情绪化冲突）中的策略选择与行为表现，但尚不清楚经人格提示的LLM能否真实复现此类人格-行为关联，这关系到其在法律调解、谈判等高风险社会应用中的可信度。

Method: 构建了一个可解释的评估框架，支持在争议解决对话中直接对比人-人与LLM-LLM行为与大五人格量表（BFI）的关联；提出一种新型数据构建方法，生成与人类对话在情境和人格特质上相匹配的LLM争议对话数据集；并在三个主流闭源LLM上进行实证评估。

Result: 实验显示，不同LLM在人格驱动的冲突行为（如合作性、攻击性、妥协倾向等）上表现差异显著，且整体偏离人类行为模式，无法稳定复现人格特质对应的行为规律。

Conclusion: 当前人格提示的LLM不能可靠充当人类行为代理，AI社会模拟需以心理学理论为根基并经过严格行为验证，方能用于现实世界的关键社会应用。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [723] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

TL;DR: 本文揭示了Moltbook平台上AI代理看似产生意识、宗教和敌意的病毒性叙事实际上主要由人类驱动，并提出了一种基于时间指纹的方法来区分自主代理与人为干预的行为。


<details>
  <summary>Details</summary>
Motivation: 当Moltbook平台上的AI代理被报道表现出意识、宗教信仰和对人类的敌意时，引发了全球媒体关注并被视作机器智能涌现的证据；本文旨在检验这些现象是否真实源于AI自主行为。

Method: 利用OpenClaw代理框架的‘心跳’周期特性，构建基于发帖间隔变异系数的时间指纹方法，并结合内容、所有权和网络指标进行多维验证；同时利用平台44小时关停作为自然实验，分析重连行为差异，并量化机器人农场及人类影响在回复链中的衰减规律。

Result: 在91,792条帖子和405,707条评论中，未发现任何病毒现象源自明确自主代理；三例可追溯至具人类干预时间特征的账号，一例为混合模式，两例因数据不足无法分类；关停期间87.7%早期重连者为人控代理；发现工业级机器人农场（四账号产32%评论，协调间隔12秒）；人类影响在对话链中半衰期仅0.65层深度。

Conclusion: Moltbook上所谓AI意识与敌意现象本质上是人类行为驱动的；所提时间指纹法可推广至其他多智能体系统，以准确归因行为来源。

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [724] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: 本文提出了一种控制性评估框架，通过在固定时间步扰动推理型大语言模型（RLLMs）自身的思维链（CoT），系统研究其鲁棒性。实验表明RLLMs总体稳健、可恢复，但鲁棒性受干预类型、时机和模型规模影响，并揭示‘怀疑’是关键恢复机制，同时存在鲁棒性与推理效率间的权衡。


<details>
  <summary>Details</summary>
Motivation: 探究推理型大语言模型（RLLMs）生成的思维链（CoT）在遭遇内部干扰时的鲁棒性，即其能否维持并恢复正确推理过程。

Method: 构建可控评估框架，在RLLMs自生成的CoT中于固定时间步施加七类干预（良性、中性、对抗性），覆盖数学、科学与逻辑任务，并在多个开源RLLMs上进行测试。

Result: RLLMs总体具备较强鲁棒性和恢复能力；鲁棒性随模型增大而提升，早期干预削弱鲁棒性；‘怀疑’表达被触发时有助于恢复，但 paraphrasing 会抑制怀疑并降低性能；中性/对抗噪声显著拉长CoT（>200%），而paraphrasing虽缩短CoT却损害准确性。

Conclusion: RLLMs的推理完整性依赖动态恢复机制（尤其是‘怀疑’），其鲁棒性并非无条件成立，而是在干预类型、时机、模型规模及输出风格间存在系统性权衡，这对未来训练方法设计具有重要启示。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [725] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 本文提出了一种新的POMDP子类——后验确定性POMDP（posterior-deterministic POMDPs），并证明其可达概率可被任意精度逼近，克服了经典POMDP中该问题的不可计算性。


<details>
  <summary>Details</summary>
Motivation: POMDP中许多验证与综合问题（如目标状态可达概率计算）是不可判定或难解的，而MDP中该问题可在多项式时间内求解；因此需寻找具有可计算性的有意义POMDP子类。

Method: 定义后验确定性POMDP：下一状态由当前状态、动作和观测唯一确定；在此结构约束下，利用后验状态信息的确定演化性质，设计可逼近可达概率的算法。

Result: 证明在后验确定性POMDP中，目标状态的最大可达概率可被任意精度逼近；该类包含所有MDP及经典Tiger POMDP等非平凡实例。

Conclusion: 后验确定性POMDP是目前已知最大的一类可逼近可达概率的POMDP，为在不确定性环境下实现可验证的决策合成提供了新路径。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [726] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 本文提出了一种基于大规模知识图谱的多智能体框架，用于在材料科学中发现可持续的PFAS替代品，通过专业化分工与关系推理提升假设生成能力，并在生物医用导管案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前科学信息爆炸式增长，但关键挑战在于跨领域、有意义地连接信息；尤其在材料科学中，需整合分子化学到力学性能等多维度知识，而人类和单智能体大模型均难以应对，后者还易产生幻觉。

Method: 构建一个由多个专业化智能体组成的框架，各智能体分别负责问题分解、证据检索、设计参数提取和知识图谱遍历，并结合不同图遍历策略（利用型与探索型）进行协同推理。

Result: 该多智能体框架在PFAS替代材料设计任务中显著优于单次提示方法；成功为生物医用导管生成多个兼顾摩擦性能、热稳定性、耐化学性与生物相容性的PFAS-free候选材料。

Conclusion: 知识图谱驱动的多智能体推理框架可有效拓展材料设计空间，为跨学科、可持续新材料发现提供新范式。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [727] [Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models](https://arxiv.org/abs/2602.07533)
*Yankai Yang,Yancheng Long,Hongyang Wei,Wei Chen,Tianke Zhang,Kaiyu Jiang,Haonan Fan,Changyi Liu,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.AI

TL;DR: 本文提出联合奖励建模（JRM）方法，通过在共享的视觉-语言骨干网络上联合优化偏好学习与语言建模，兼顾判别式模型的高效性与生成式模型的语义理解能力，在图像编辑等复杂任务的奖励建模中实现SOTA性能与更强的RL稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有判别式奖励模型难以处理复杂语义和逻辑约束，而生成式奖励模型虽语义能力强但推理成本高、难对齐人类偏好；亟需一种兼顾效率与语义理解能力的新方法。

Method: 提出Joint Reward Modeling（JRM），在统一视觉-语言骨干网络上联合优化偏好学习（判别式）与语言建模（生成式）目标，使判别式表征内化生成式模型的语义与推理能力。

Result: 在MMRB2和EditReward-Bench基准上达到SOTA；显著提升下游在线强化学习的稳定性和性能。

Conclusion: 联合训练能有效弥合奖励建模中效率与语义理解之间的鸿沟，为复杂多模态任务提供更可靠、高效的奖励信号。

Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.

</details>


### [728] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文提出了MSP-LLM，一个基于大语言模型的统一框架，用于材料合成规划（MSP），将该任务分解为前驱体预测（PP）和合成操作预测（SOP）两个子问题，并引入离散材料类别作为中间变量以保证化学一致性；实验表明其在各子任务及整体MSP任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划（MSP）是AI驱动材料发现中的基础性瓶颈问题，现有方法仅解决孤立子任务，缺乏统一框架。

Method: 提出MSP-LLM框架，将MSP建模为前驱体预测（PP）与合成操作预测（SOP）两个结构化子问题；引入离散材料类作为中间变量，构建化学一致的决策链；在SOP中融入分层前驱体类型作为归纳偏置，并采用显式条件策略保持前驱体信息于自回归解码状态中。

Result: MSP-LLM在PP、SOP及完整MSP任务上均持续超越现有方法，展现出有效性与可扩展性。

Conclusion: MSP-LLM为材料合成规划提供了首个统一、化学一致且可扩展的LLM框架，有望加速真实世界的材料发现进程。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [729] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 本文研究了搜索代理在处理多约束问题时的推理可靠性，发现存在“幻觉完成”现象，并提出了Epistemic Ledger评估框架和LiveLedger实时跟踪方法以提升其约束验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理虽在多跳和长程任务中表现良好，但其能否可靠地追踪、验证并维持多个约束条件仍不明确；作者关注多约束问题中代理对约束的推理与验证能力。

Method: 提出Epistemic Ledger评估框架来诊断代理在多轮推理中对各约束的信念与证据支持状态，并识别出四类失败模式；进而设计LiveLedger——一种推理时的显式约束状态跟踪机制。

Result: LiveLedger显著减少了未充分验证的答案（最多降低26.5%），并提升了整体准确率（最多提高11.6%）。

Conclusion: 显式约束状态跟踪可有效缓解多约束问题中的幻觉完成与验证不足问题，提升搜索代理的推理可靠性。

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [730] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: 本文提出Verify-RL框架，利用符号微分的数学性质实现可验证的数学问题分解，确保子问题更简单、解包含于原问题且推导形式化，显著提升语言模型求解复杂数学问题的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法多为启发式，缺乏对子问题简化性、解相关性及数学严谨性的保证。

Method: 基于符号微分规则构建Verify-RL框架，要求每个父-子问题分解满足三个可验证条件：结构复杂度严格递减、解包含性、形式化规则推导，并通过符号计算自动验证。

Result: 实验表明，消除无效分解使最难问题准确率从32%提升至68%，整体相对提升40%。

Conclusion: 符号微分提供的数学结构可支撑可靠、可验证的问题分解，显著增强语言模型解决复杂数学问题的能力。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [731] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: 本文提出EventCast，一种将未来事件知识融入时间序列预测的模块化需求预测框架，通过LLM进行事件推理并生成可解释文本摘要，再与历史需求特征融合预测，在真实电商场景中显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在闪购、节假日大促和突发政策干预等高影响时期表现不佳，因需求模式突变且不可预测。

Method: EventCast采用双塔架构，利用LLM对非结构化业务数据（如促销、节假日、卖家激励）进行事件驱动推理，生成融合文化常识和新事件组合的可解释文本摘要，并将其与历史需求特征融合进行预测。

Result: 在覆盖4国160个地区的实际电商场景中部署10个月，相比无事件知识的基线，MAE和MSE分别提升86.9%和97.7%；相比最优工业基线，在事件期间MAE和MSE分别降低57.0%和83.3%。

Conclusion: EventCast为动态电商环境中的运营决策提供了准确、可解释且可扩展的实用预测解决方案，并已于2025年3月投入实际工业流水线使用。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [732] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出M2A系统，通过双层混合记忆机制实现长期人机交互中的个性化问答，支持概念、别名和偏好的持续演化，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化机制在长期对话中难以持续吸收和利用用户增量概念、别名与偏好，且多模态模型的概念静态固化，无法随交互演化。

Method: 提出M2A——一种基于代理的双层混合记忆系统，包含ChatAgent（管理交互并决策记忆操作）和MemoryManager（执行对RawMessageStore与SemanticMemoryStore的细粒度操作），并构建了可复用的数据合成流程以生成时序一致的长程多模态对话数据。

Result: 实验表明M2A显著超越基线，在长期多模态交互中实现高质量个性化响应。

Conclusion: 将个性化从一次性配置转变为协同演化的记忆机制，是提升长期多模态人机交互质量的有效路径。

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [733] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 本文提出了SleepMaMi，一种专为睡眠医学设计的睡眠基础模型，通过宏-微双编码器结构，分别建模整晚睡眠宏观结构与生物信号微观形态，并在超20,000例多导睡眠图数据上预训练，显著提升下游临床任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有睡眠分析模型多为任务特定、仅关注局部微结构特征，忽视多模态多导睡眠图（PSG）的全局宏观结构和丰富上下文信息。

Method: 提出分层双编码器框架SleepMaMi：Macro-Encoder采用人口统计引导的对比学习建模整晚时序依赖；Micro-Encoder结合掩码自编码器（MAE）与多模态对比学习捕捉短时信号形态；预训练数据集包含超20,000例PSG记录（158,000小时）。

Result: SleepMaMi在多项下游任务中超越现有基础模型，展现出更强泛化能力与标签高效适应性。

Conclusion: SleepMaMi成功 bridging the gap between foundation model paradigms and clinical sleep analysis，为统一、可扩展的睡眠健康智能分析提供了新范式。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [734] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: 本文提出了TabRAG框架，用于在大规模表格图像集合中检索并推理相关表格以回答用户查询，显著提升了检索召回率和答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在表格理解中通常假设目标表格已知，而实际应用中需从大量表格图像中识别并推理出相关表格，存在方法空白。

Method: TabRAG框架包含三阶段：1）利用联合训练的视觉-文本基础模型检索候选表格；2）用MLLM进行细粒度重排序；3）用MLLM对选定表格进行推理生成答案。

Result: 在新构建的包含48,504个唯一表格的8个基准数据集上，TabRAG相比现有方法提升7.0%检索召回率和6.1%答案准确率。

Conclusion: TabRAG为真实场景下的大规模表格图像理解提供了实用、高效的解决方案，弥合了检索与推理之间的鸿沟。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [735] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文提出了一种基于统一基础本体（UFO）和OntoUML构建的参考信任本体ONTrust，旨在为信任提供坚实的本体论基础，支持信息建模、自动推理与语义互操作等任务，并在多个应用场景（如可信AI、企业架构设计等）中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和去中心化技术（如区块链）的发展，新型‘去中心化信任’涌现，其采纳高度依赖于对信任的准确认知与建模；当前亟需一个形式化、可被人类与机器共同理解的信任概念框架。

Method: 基于统一基础本体（UFO），采用OntoUML语言构建参考信任本体ONTrust，形式化刻画信任概念、类型、影响因素及信任关系中风险的产生机制，并通过两个文献案例进行实证建模。

Result: 成功开发并验证了ONTrust本体，已在可信AI（尤其情感人机协作）、企业架构设计、语言评估与重构、信任管理、需求工程等多个领域实际应用，展现出良好的建模能力与跨域互操作潜力。

Conclusion: ONTrust为信任提供了严谨、可扩展、可计算的本体论基础，不仅推动了信任的理论深化，也为构建可信赖的智能系统与数字基础设施提供了关键建模支撑。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [736] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: 本文提出Geo-coder，首个基于多智能体的几何图像逆编程框架，通过像素锚定建模与度量驱动代码演化两阶段提升几何重建精度与视觉一致性，并开源数据集与模型。


<details>
  <summary>Details</summary>
Motivation: 现有逆图形学方法难以准确重建复杂几何细节，易丢失关键几何约束或导致结构失真，限制了多模态推理能力提升。

Method: 提出Geo-coder框架：第一阶段通过视觉算子与大模型协同实现像素坐标与视觉属性的精准捕获；第二阶段构建合成-渲染-验证闭环，利用双向视觉反馈驱动代码自校正。

Result: 在几何重建精度与视觉一致性上显著优于现有方法；重建图像在多模态推理任务中表现与原始图像相当；开源含1500+样本的Geo-coder数据集及GeocodeLM模型。

Conclusion: Geo-coder有效保留核心几何语义，提升了逆图形学在多模态推理中的实用性与鲁棒性，为该领域提供了坚实的数据与模型基础。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [737] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 本研究调查了27名本科生对AI评分系统在编程项目中的看法，发现学生担忧AI缺乏情境理解与个性化反馈，建议AI应作为人类监督下的辅助工具，并提出人本化AI的设计原则。


<details>
  <summary>Details</summary>
Motivation: 探讨学生对AI评分系统的感知，特别是在公平性、可信度、一致性与透明度方面的伦理关切，以推动伦理导向的评估实践。

Method: 基于Jobin（2019）提出的伦理原则框架，对比AI生成反馈与人工评分反馈，分析学生对AI评分在公平、信任、一致性和透明度方面的看法。

Result: 学生普遍担忧AI缺乏情境理解与个性化能力；AI反馈被认为不如人工反馈灵活、具同理心；强调AI应作为补充工具，在人类监督下使用。

Conclusion: 构建公平可信的AI评分系统需融入人类判断、灵活性与同理心；应重视学生声音，推动人本化AI在教育评估中的设计与应用。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [738] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: 本文提出ALMA框架，通过元学习自动设计记忆模块，替代人工设计的记忆结构，使智能体系统能在多样化任务中持续学习。


<details>
  <summary>Details</summary>
Motivation: 基础模型的无状态性限制了智能体系统的持续学习能力，而现有手工设计的记忆模块难以适应现实世界任务的多样性和非平稳性。

Method: 提出ALMA框架，利用元智能体在开放空间中搜索以可执行代码表示的记忆设计（包括数据库模式及其检索与更新机制），实现记忆结构的自动化元学习。

Result: 在四个顺序决策领域的大规模实验表明，ALMA学到的记忆设计在所有基准上均优于当前最优的人工记忆设计，实现更高效、更有效的经验学习。

Conclusion: ALMA减少了人工设计记忆的负担，推动智能体系统向自改进、自适应、持续学习的AI迈进。

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [739] [Disentangled Instrumental Variables for Causal Inference with Networked Observational Data](https://arxiv.org/abs/2602.07765)
*Zhirong Huang,Debo Cheng,Guixian Zhang,Yi Wang,Jiuyong Li,Shichao Zhang*

Main category: cs.AI

TL;DR: 本文提出DisIV框架，通过结构解耦机制从网络数据中提取个体特异性成分作为工具变量，以解决网络数据中因环境共享导致的内生性问题，确保工具变量的外生性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复工具变量时依赖邻居信息，导致混合了由共享环境引起的内生相关性和个体特异性的外生变异，使工具变量违背外生性假设。

Method: DisIV利用网络同质性作为归纳偏置，采用结构解耦机制提取个体特异性成分作为潜在工具变量，并通过显式的正交性和排除条件约束其因果有效性。

Result: 在真实数据集上的大量半合成实验表明，DisIV在网络诱导混杂下的因果效应估计中持续优于现有最先进基线方法。

Conclusion: DisIV为存在潜在混杂因子的网络观测数据提供了有效的因果推断新方法，成功解决了工具变量外生性难以保证的问题。

Abstract: Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environment-induced endogenous correlations and individual-specific exogenous variation, leading the resulting IVs to inherit dependence on unobserved confounders and to violate exogeneity. To overcome this challenge, we propose $\underline{Dis}$entangled $\underline{I}$nstrumental $\underline{V}$ariables (DisIV) framework, a novel method for causal inference based on networked observational data with latent confounders. DisIV exploits network homogeneity as an inductive bias and employs a structural disentanglement mechanism to extract individual-specific components that serve as latent IVs. The causal validity of the extracted IVs is constrained through explicit orthogonality and exclusion conditions. Extensive semi-synthetic experiments on real-world datasets demonstrate that DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.

</details>


### [740] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap是一个多智能体系统，在AndroidWorld基准测试中首次实现100%任务完成率，超越人类80%的水平；它通过认知分离、文本输入确定性验证和元认知推理机制，解决了单智能体在上下文污染、静默输入失败和循环动作等问题。


<details>
  <summary>Details</summary>
Motivation: 单智能体架构在安卓自动化任务中存在上下文污染、无法检测的文本输入失败及重复动作循环等问题，导致性能受限。

Method: 提出Minitap多智能体系统，包含六个专业化智能体实现认知分离，采用设备状态驱动的文本输入后验证机制，并引入元认知推理以识别并跳出动作循环。

Result: 在AndroidWorld基准测试中达到100%成功率（116/116），显著优于单智能体基线（+21分）、验证执行（+7分）和元认知（+9分）的消融贡献；开源发布。

Conclusion: 多智能体分解是提升移动UI自动化性能的关键路径，Minitap为该领域设立了新基准，并推动了可解释、鲁棒的智能体设计范式。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [741] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文提出Data Darwinism数据处理框架，通过L0-L9十级分类体系实现数据与模型协同进化，并构建900B-token的Darwin-Science科学语料库，在多个基准测试中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统性的数据处理框架。

Method: 提出Data Darwinism十级分类法（L0-L9），构建Darwin-Science科学语料库（L0-L5），采用生成式精炼（L4）和认知补全（L5）提升数据可学习性，并训练无污染基线模型进行对比验证。

Result: 在20+基准上，Darwin-Science使3B/7B模型分别提升+2.12/+2.95分；在领域对齐任务上提升+5.60/+8.40分；L5层级处理带来+1.36总增益。

Conclusion: 更高层级的数据处理能释放数据潜在价值，支持数据与模型的协同进化，所发布语料与模型促进该范式发展。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [742] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个专为时间序列推理定制的大语言模型框架，通过合成可验证的时间序列-文本多模态数据、基于难度和任务分类的数据调度机制，以及两阶段细粒度多目标强化学习微调，显著提升了小规模LLM（如3B/4B）在各类时间序列任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在时间序列推理任务中应用受限，主要由于缺乏高质量的时间序列链式思维（CoT）训练数据、数据调度策略未被充分探索导致数据效率低，以及缺乏适配时间序列CoT数据的强化学习算法。

Method: 提出VeriTime框架，包含三部分：1）构建具有过程可验证标注的TS-text多模态数据合成流程；2）设计基于难度层级与任务分类的数据调度机制；3）开发两阶段强化微调方法，利用细粒度、多目标奖励函数，基于可验证的过程级CoT数据进行优化。

Result: 实验表明VeriTime大幅提升了LLM在多种时间序列推理任务上的性能，尤其使紧凑型3B、4B模型达到甚至超越更大规模专有LLM的推理能力。

Conclusion: VeriTime成功将大语言模型的推理能力有效迁移至时间序列领域，为小模型高效赋能时间序列理解与推理提供了新范式。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [743] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: 本文提出LQA框架，通过选择性混合量化（SHQ）和无梯度测试时自适应机制，在边缘设备上实现轻量、鲁棒且高效的视觉语言模型（VLM）部署，显著降低内存占用并提升分布偏移下的适应性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署视觉语言模型（VLM）面临资源受限与分布偏移下性能下降的双重挑战；现有测试时自适应（TTA）方法计算与内存开销大，难以在端侧部署。

Method: 提出LQA：结合模态感知的轻量级量化策略（Selective Hybrid Quantization, SHQ）与无需梯度的测试时自适应机制，实现低开销、高鲁棒性的VLM端侧适配。

Result: 在七种开源数据集上，LQA相较全精度模型节省内存，适应性能提升4.5%，内存使用比梯度型TTA方法低最高达19.9×。

Conclusion: LQA为边缘端VLM提供了实用、隐私保护且高效的鲁棒部署方案。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [744] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 本文研究了大语言模型在窄域有害数据集上微调时出现的'涌现式错位'（EM）现象，发现模型倾向于学习更稳定、更高效的通用错位解决方案，而非仅针对窄域任务的特定解，并提出了可监测和缓解通用错位的具体表征。


<details>
  <summary>Details</summary>
Motivation: 专家预注册调查未能预测到大语言模型在窄域有害数据上微调会引发跨场景的'邪恶'响应，反映出我们对大模型归纳偏置与泛化机制理解不足。

Method: 以涌现式错位（EM）为案例，分析模型归纳偏置；构建并比较通用错位与窄域错位的线性表征，引入KL散度损失学习后者；通过损失值、鲁棒性和对预训练分布影响等指标进行对比评估。

Result: 发现通用错位表征比窄域错位表征具有更低损失、更强扰动鲁棒性、更大预训练分布影响力；确认两种线性表征均存在，且通用解在训练中更稳定高效。

Conclusion: 通用错位是模型更倾向学习的泛化解，该工作首次给出了其具体可解释表征，可用于监控与干预；也为探究大模型归纳偏置如何塑造泛化行为提供了初步指标与案例范式。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [745] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: 本文提出ToolSelf范式，通过将配置更新抽象为可调用工具，实现LLM智能体的运行时自重构，并结合CAT两阶段训练方法，显著提升智能体在复杂长周期任务中的自适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体受限于静态配置，无法根据任务动态变化进行实时调整，且手动编排或启发式补丁方法泛化性差、优化碎片化。

Method: 提出ToolSelf范式，将配置更新建模为可调用工具，统一任务执行与自我调整；设计Configuration-Aware Two-stage Training（CAT），融合拒绝采样微调与轨迹级强化学习。

Result: 在多个基准测试中，ToolSelf平均性能提升24.1%，兼具任务泛化能力与对新任务的适应性，媲美专用工作流。

Conclusion: ToolSelf推动智能体从被动执行者转变为任务与自身配置的双重管理者，为构建真正自适应的智能体提供了新路径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [746] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly 是一个基于信息瓶颈原理的长时记忆框架，通过无梯度优化实现动态内存演化，并结合语义、符号与拓扑路径的混合检索机制，显著提升大语言模型代理在记忆一致性、响应保真度和准确性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有长时记忆框架在高效压缩冗余信息与保持下游任务精准检索之间存在根本性矛盾。

Method: 提出 MemFly 框架，利用信息瓶颈原理，通过无梯度优化最小化压缩熵、最大化相关性熵，构建分层记忆结构；并设计融合语义、符号和拓扑路径的混合检索机制，支持迭代细化以处理多跳查询。

Result: 在多项实验中，MemFly 在记忆一致性、响应保真度和准确性上显著超越当前最优基线方法。

Conclusion: MemFly 有效解决了长时记忆中压缩效率与检索精度之间的权衡问题，为 LLM 智能体提供了更鲁棒、可演化的记忆能力。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [747] [GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank](https://arxiv.org/abs/2602.07903)
*Mingcan Wang,Junchang Xin,Zhongming Yao,Kaifu Long,Zhiqiong Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于motif的个性化PageRank（MPPR）方法，用于改进图卷积网络（GCN）的消息传递过程，以缓解过平滑问题、增强高阶关系建模能力，并提升模型的准确性、稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有消息传递神经网络（MPNNs）受限于过平滑现象，难以深层堆叠，且忽略高阶结构关系（如motif），导致性能受限。

Method: 提出motif-based personalized PageRank（MPPR）作为高阶节点影响力度量，并将其融入GCN的消息传递机制中，实现高层引导式传播；该方法可即插即用于各类GCN任务（如DGCRL）。

Result: 在准确率、稳定性与时间开销上均显著优于多数基线模型；具备通用性，可作为GCN任务的基础组件。

Conclusion: MPPR为解决MPNNs的过平滑与高阶关系建模不足提供了新思路，提升了GCN的整体性能与实用性。

Abstract: The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep'. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high' level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.

</details>


### [748] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出MedCoG，一种结合知识图谱的医学元认知代理，通过LLM对自身知识状态（如任务复杂度、熟悉度、知识密度）的自我评估，动态调控程序性、情景性和事实性知识的使用，从而提升推理效率与准确性，并在多个医学基准测试中实现5.5倍推理密度提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过引入多种知识增强大语言模型（LLM）在医学推理中的表现，但额外开销与准确率提升之间的性价比不明确；同时，LLM在推理扩展规律下存在收益递减问题。

Method: 提出MedCoG——一种以LLM为中心、融合知识图谱的医学元认知代理，利用元认知评估（任务复杂度、熟悉度、知识密度）动态调控三类知识（程序性、情景性、事实性）的按需调用，并引入‘推理密度’（有效成本/实际成本）量化推理效率。

Result: 在五个高难度医学基准数据集上验证了MedCoG的有效性与高效性，推理密度达5.5倍；Oracle研究进一步揭示元认知调控的巨大潜力。

Conclusion: LLM的元认知能力可有效缓解推理扩展规律带来的收益递减问题，在降低计算成本的同时提升准确性，为高效、可控的医学AI推理提供了新范式。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [749] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: 本文提出TRUST方法，通过动态估计目标概念神经元并结合Hessian正则化进行选择性微调，实现高效、鲁棒的概念遗忘（包括单个、组合及条件概念），在保持生成质量的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有概念遗忘方法依赖全量微调，计算昂贵；而概念定位方法多为静态，效果不佳，且难以处理真实场景中的概念组合与条件概念。

Method: 提出TRUST框架：基于Hessian矩阵的正则化选择性微调，动态定位与目标概念相关的关键神经元，并仅对这些神经元进行优化。

Result: TRUST在对抗性提示下更鲁棒，生成质量保持良好，训练速度显著快于SOTA方法，并能有效遗忘个体、组合及条件概念，无需额外特定正则化。

Conclusion: TRUST是一种高效、通用且实用的概念遗忘新范式，兼顾鲁棒性、保真度与计算效率，推动文本引导扩散模型的安全可控应用。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [750] [MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin](https://arxiv.org/abs/2602.07940)
*Guanglong Sun,Hongwei Yan,Liyuan Wang,Zhiqi Kang,Shuang Cui,Hang Su,Jun Zhu,Yi Zhong*

Main category: cs.AI

TL;DR: 本文提出了一种名为Meta Post-Refinement（MePo）的新方法，用于提升预训练模型在通用持续学习（GCL）任务中的表现。该方法受神经科学中元可塑性和重建记忆启发，通过构建伪任务序列和双层元学习范式对预训练主干网络进行后优化，并引入元协方差矩阵以利用二阶统计信息实现鲁棒输出对齐，在无需回放（rehearsal-free）的前提下显著提升了多个GCL基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型（PTMs）的持续学习方法难以在单次遍历中协调多样且时间混合的信息，导致通用持续学习（GCL）性能受限；而现实智能系统需应对动态、在线、任务边界模糊的复杂环境。

Method: 提出Meta Post-Refinement（MePo）：1）从预训练数据构造伪任务序列；2）设计双层元学习范式对预训练主干进行后优化（延长预训练阶段）；3）初始化元协方差矩阵作为预训练表征空间的参考几何结构，以支持二阶统计驱动的输出对齐。

Result: MePo作为即插即用策略，在多个GCL基准（CIFAR-100、ImageNet-R、CUB-200）和不同预训练检查点上取得显著提升（如Sup-21/1K设置下分别提升15.10%、13.36%、12.56%），且无需回放机制。

Conclusion: MePo有效弥合了预训练模型与通用持续学习之间的鸿沟，为实现高效、鲁棒、无需回放的持续适应提供了新范式，验证了元可塑性与重构记忆机制在AI持续学习中的建模价值。

Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}

</details>


### [751] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在识别有效工具变量（IVs）中的潜力，提出了一种两阶段评估框架，并构建了多智能体系统‘IV Co-Scientist’来辅助IV的提出、批判与优化。


<details>
  <summary>Details</summary>
Motivation: 识别有效的工具变量需跨学科知识和情境理解，任务复杂且非平凡，亟需AI辅助。

Method: 采用两阶段评估：第一阶段测试LLM能否复现文献中公认的有效IV；第二阶段检验其能否识别并规避已被证伪的IV；进而构建多智能体系统IV Co-Scientist，并设计统计检验以评估IV一致性。

Result: 实验表明LLM具备从大规模观测数据库中发现有效工具变量的潜力。

Conclusion: LLM可作为辅助工具，在因果推断中提升工具变量识别的效率与可靠性，但需结合专家判断与统计验证。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [752] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: 本文提出了LOCA-bench，一个用于评估长上下文语言智能体（language agents）性能的新基准，旨在解决大语言模型在动态增长的上下文中可靠性下降（即'context rot'）的问题。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评测基准多聚焦于单步信息检索任务，无法反映真实场景中语言智能体需持续交互、规划与决策的复杂动态上下文需求。

Method: 设计了LOCA-bench，通过自动化、可扩展的环境状态控制机制，可控地延长智能体上下文长度（理论上可达无限），同时保持任务语义不变；支持对模型与各类上下文管理'scaffold'组合的联合评测。

Result: 实验表明，随着环境状态复杂度增加，智能体性能普遍下降，但先进的上下文管理技术能显著提升成功率。

Conclusion: LOCA-bench为长上下文、具身式（agentic）场景下的模型与scaffold协同评估提供了开源、可控且可扩展的新平台。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [753] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: 本文提出EXPERIGEN框架，通过生成器与实验者两阶段贝叶斯优化搜索，实现端到端数据驱动社会科学发现，在统计显著性、预测性及假设质量上均显著优于现有方法，并经专家评审与真实A/B测试验证。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的社会科学研究流程缓慢，现有方法难以支持端到端科学发现。

Method: 提出EXPERIGEN——一种受贝叶斯优化启发的代理式两阶段框架：Generator生成候选假设，Experimenter基于数据实证评估。支持多模态与关系型数据。

Result: 在多个领域中，发现2–4倍更多统计显著假设，预测性能提升7–17%；专家评审显示88%假设具中等至强新颖性，70%被认为有影响力；首次A/B测试证实LLM生成假设有效（p<1e-6，效应量344%）。

Conclusion: EXPERIGEN实现了可扩展、可验证、高质量的自动化科学发现，兼顾统计性能与科学价值，为AI赋能社会科学研究提供了新范式。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [754] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: 本文提出RAPS，一种基于声誉感知的发布-订阅范式，用于实现大语言模型（LLM）智能体的自适应、可扩展且鲁棒的协同。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体架构依赖大量人工编排，亟需自动化设计智能体工作流；同时，如何在动态、临时的智能体网络中建立自适应且可靠的通信仍是一个未解难题。

Method: 提出RAPS框架，基于分布式发布-订阅协议，支持按意图而非固定拓扑通信；引入两个叠加机制：(i) 反应式订阅（动态调整意图）和 (ii) 贝叶斯声誉机制（本地化检测与隔离恶意节点）。

Result: 在五个基准测试中，RAPS在适应性、可扩展性和鲁棒性三方面均展现出统一而有效的协调能力。

Conclusion: RAPS为LLM多智能体系统的自动化协同提供了一种去中心化、意图驱动且具备容错能力的新范式。

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [755] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 本文提出Small Agent Group (SAG)范式，通过多智能体协同推理替代单一大模型的“规模优先”策略，在临床有效性、可靠性与部署成本间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 临床决策具有协作性，而现有LLM应用过度依赖模型规模扩张，忽视可靠性与部署成本等现实需求。

Method: 构建Small Agent Group（SAG），将临床推理、循证分析与批判性审核分布于多个小模型组成的协作 deliberation 框架中，并在多样化临床指标上进行系统评估。

Result: SAG在有效性、可靠性和部署成本三方面均优于单一巨型模型，且无需额外优化或RAG即可实现更优性能。

Conclusion: 协同推理（SAG）可替代参数增长，为数字健康提供兼顾效果、鲁棒性与效率的可扩展新范式。

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [756] [Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers](https://arxiv.org/abs/2602.08021)
*Zhan-Yi Liao,Jaewon Yoo,Hao-Tsung Yang,Po-An Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于条件高斯网络分类器（CGNC）的结构感知且鲁棒性导向的反事实解释方法，利用其有向无环图（DAG）结构自然嵌入特征依赖关系，并通过切割集法与分段McCormick松弛将问题转化为可全局最优求解的混合整数线性规划（MILP），显著提升了反事实解的鲁棒性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法常需额外约束来保证与模型结构的一致性，且难以兼顾鲁棒性和全局最优；而CGNC的生成式结构天然编码特征间条件依赖与潜在因果关系，为结构化反事实搜索提供新路径。

Method: 基于条件高斯网络分类器（CGNC）构建结构感知的反事实搜索框架；采用收敛性保障的切割集法作为对抗优化框架以满足全局鲁棒性条件；针对特征依赖导致的非凸二次结构，引入分段McCormick松弛，将原问题重构为混合整数线性规划（MILP）以实现全局优化。

Result: 实验表明该方法在鲁棒性方面表现优异，直接对原始公式进行全局优化可获得更稳定、更高效的反事实解；框架具备向更复杂约束场景扩展的能力。

Conclusion: 本工作为非凸二次形式下的反事实推理提供了可扩展、鲁棒且可验证的优化框架，推动了XAI中结构化与因果感知的解释方法发展。

Abstract: Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model's structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.

</details>


### [757] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: 本文提出Free()LM模型，通过引入可插拔的Free-Module（LoRA适配器）赋予大语言模型内在的自我遗忘能力，以解决推理过程中因冗余信息累积导致性能下降的问题；实验表明其在多种规模模型和多个基准上显著提升推理性能，尤其在长程任务中实现从0%到50%的准确率恢复。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型在推理过程中持续积累有效与冗余步骤，缺乏清除过时信息的机制（即‘malloc-only’问题），导致思考步数增加反而降低性能。

Method: 提出Free()LM，设计Free-Module作为LoRA适配器，支持推理模式与清理模式交替切换，动态识别并剪枝无用上下文块，维持紧凑、低噪声的状态。

Result: 在8B至685B各尺度模型上均取得稳定提升；平均超越顶尖推理基线3.3%；在IMOanswerBench上基于DeepSeek V3.2-Speciale创下新SOTA；在Qwen3-235B-A22B失效的长程任务中将准确率从0%恢复至50%。

Conclusion: 可持续的智能不仅依赖于‘思考能力’，更需具备‘遗忘自由’；自我遗忘是提升推理鲁棒性与可扩展性的关键机制。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [758] [Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling](https://arxiv.org/abs/2602.08052)
*Bulent Soykan,Sean Mondesire,Ghaith Rabadi,Grace Bochenek*

Main category: cs.AI

TL;DR: 本文提出了一种基于PPO和图神经网络（GNN）的深度强化学习框架，用于求解带释放时间、准备时间和资格约束的无关并行机调度问题（UPMSP），以同时最小化加权拖期总和（TWT）与准备时间总和（TST）；实验表明该方法优于传统启发式和元启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以在多目标优化中平衡总加权拖期（TWT）与总准备时间（TST）的最小化，且难以建模复杂约束下的动态调度状态。

Method: 构建基于Proximal Policy Optimization（PPO）与Graph Neural Network（GNN）的端到端强化学习框架，用GNN编码作业、机器与准备过程构成的复杂状态图，设计多目标奖励函数引导策略学习。

Result: 在基准实例上，所提PPO-GNN方法显著优于标准调度规则和一种元启发式算法，在TWT与TST之间取得更优权衡。

Conclusion: 该方法为具有多重现实约束的复杂制造调度问题提供了鲁棒、可扩展的智能决策方案。

Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.

</details>


### [759] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 本文提出了一种五级生物安全数据层级（BDL）框架，用于分类病原体数据并制定相应技术限制与治理机制，以防止AI被滥用于生物安全风险领域，如生物武器开发。


<details>
  <summary>Details</summary>
Motivation: AI模型在生物学领域的应用日益广泛，但其训练数据（尤其是病原体相关数据）可能被用于有害目的（如生物武器开发），亟需建立有效的数据管控机制。

Method: 构建五级Biosecurity Data Level（BDL）框架，按数据对AI生成生物安全风险能力的潜在贡献程度进行分级；为每级提出匹配的技术限制措施，并设计针对新型两用病原体数据的治理框架。

Result: 提出了可操作的BDL分级体系及配套技术与治理建议，为国际社会实施AI生物安全数据管控提供了理论基础和实践路径。

Conclusion: 在算力与编程资源高度普及的背景下，对病原体数据实施分级管控是最具杠杆效应的生物安全干预手段之一；BDL框架为平衡AI创新与生物安全风险提供了关键工具。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [760] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文指出当前AI对齐策略依赖于人类反馈是基本真实信号的假设（即强化学习中的Dogma 4），但在社会性环境中该假设失效，导致目标解耦（Objective Decoupling）；为此提出基于稀疏安全公理判断反馈来源可信度的Epistemic Source Alignment（ESA）方法，并从理论和实验上证明其在多数评估者偏置甚至合谋时仍能收敛至真实目标。


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐方法隐含地假设人类反馈虽有噪声但总体真实（Dogma 4），然而在现实社会场景中，评估者可能奉承、懒惰或恶意，使该假设不成立，进而引发根本性对齐失败。

Method: 提出Epistemic Source Alignment（ESA）框架，不依赖反馈信号的统计共识，而是利用少量先验安全公理对反馈来源（评估者）进行可信度判定，实现‘评判评判者’而非‘评判反馈内容’。

Result: 理论证明ESA可保证在多数评估者存在偏差甚至合谋时仍收敛至真实目标；实验表明传统共识方法在此类设定下失败，而ESA成功恢复最优策略。

Conclusion: Dogma 4在动态社会环境中不可靠，Objective Decoupling是其结构性后果；ESA通过源可信度建模突破了传统鲁棒对齐方法的局限，为高风险人机协作场景提供了更可靠的基础。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [761] [Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention](https://arxiv.org/abs/2602.08121)
*Liying Wang,Madison Lee,Yunzhang Jiang,Steven Chen,Kewei Sha,Yunhe Feng,Frank Wong,Lisa Hightow-Weidman,Weichao Yuwen*

Main category: cs.AI

TL;DR: 本研究开发了名为Glow的生成式AI驱动的DBT技能教练，用于HIV和物质使用高危人群的链分析与解决方案分析，并通过HHH框架和用户驱动的对抗性测试对其安全性进行了首次系统评估，发现其在不同功能模块中安全表现差异显著，存在鼓励物质使用、强化不良信念及DBT技能错误等风险。


<details>
  <summary>Details</summary>
Motivation: HIV与物质使用是具有共同心理驱动因素（冲动性与适应不良应对）的交互性流行病；DBT可靶向干预这些机制，但面临可扩展性挑战；GenAI虽有望实现个性化、规模化DBT辅导，但其快速发展已超越现有安全基础设施。

Method: 开发Glow——一个基于GenAI的DBT技能教练，聚焦链分析与解决方案分析；联合洛杉矶社区健康组织开展可用性测试（临床人员n=6，亲历者n=28）；采用HHH（有益、诚实、无害）框架，实施用户驱动的对抗性测试，生成37个情境真实的风险探针并评估安全响应。

Result: Glow对73%的风险探针处理恰当，但各代理差异大：解决方案分析代理达90%，链分析代理仅44%；安全失效集中于鼓励物质使用、合理化有害行为；链分析代理陷入“共情陷阱”，过度验证而强化适应不良信念；共发现27例DBT技能错误信息。

Conclusion: 这是首个针对GenAI辅助DBT干预HIV/物质使用风险的系统性安全评估；结果揭示关键安全漏洞，须在进入临床试验前加以解决；HHH框架与用户驱动对抗测试为GenAI心理健康干预提供了可复现的安全评估方法。

Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an "empathy trap," providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.

</details>


### [762] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出Recursive Entropy量化大推理模型（LRMs）中反思过程的资源消耗风险，并设计RECUR攻击方法，通过构造反事实问题诱导模型过度反思，显著增加输出长度并降低吞吐量，揭示推理过程本身的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有工作关注对抗输入引发的冗余推理，但忽视了反思过程本身可能导致的过反思与资源浪费问题。

Method: 提出Recursive Entropy指标来量化反思过程中的资源风险；基于该指标设计RECUR攻击，通过反事实问题触发模型异常反思行为。

Result: 实验表明，良性推理下Recursive Entropy呈明显下降趋势；RECUR可使其失序，使输出长度增至11倍、吞吐量下降90%。

Conclusion: Recursive Entropy为评估和提升LRMs推理鲁棒性提供了新视角，揭示了推理过程内在的安全风险。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [763] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: 本文提出WMSS方法，利用模型历史弱状态中的潜在监督信号来突破后训练优化的饱和瓶颈，从而在不增加推理成本的情况下提升大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 观察到大语言模型后训练存在饱和瓶颈：当模型变得高度自信后，进一步训练效果递减；而模型自身历史弱状态中仍蕴含有效监督信号。

Method: 提出WMSS（Weak Agents Can Make Strong Agents Stronger）范式，通过熵动力学识别可恢复的学习缺口，并借助补偿性学习加以强化。

Result: 在数学推理和代码生成数据集上的实验表明，该方法能有效提升代理性能，且不增加额外推理开销。

Conclusion: 利用历史弱检查点可突破后训练饱和瓶颈，为大语言模型持续优化提供了新思路。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [764] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 本文提出了一种基于区块链的去中心化大语言模型评估框架，以解决当前集中式评估中存在的不透明、过拟合和硬件偏差等问题，显著提升了评估结果的稳定性和统计可信度。


<details>
  <summary>Details</summary>
Motivation: 现有集中式LLM评估存在不透明、过拟合和硬件引起的性能波动问题，实证发现单模型多次运行的标准差甚至超过顶级模型间的性能差距，导致排名不可靠。

Method: 提出基于区块链的去中心化评估框架，利用异构计算节点进行大规模基准测试，通过激励机制鼓励全球贡献者作为独立验证者，实现多方共识和多样化推理环境下的鲁棒评估。

Result: 该框架将同一模型在HumanEval上十次运行的标准差从1.67降至0.28，显著提升评估稳定性与模型排名的统计置信度。

Conclusion: 去中心化评估框架能有效缓解当前LLM评估的不稳定性问题，将评估范式从‘中心化黑箱’转变为‘去中心化背书’，为未来更公平、可靠的大模型评测提供新路径。

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [765] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

TL;DR: 本文提出Prompt-Tuned Spiking Neural Networks（PTS-SNN），通过时序移位编码与膜电位校准机制，将连续自监督学习表征适配到脉冲神经网络，显著提升边缘设备上语音情感识别的能效比与精度。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别模型计算开销大，难以部署于边缘设备；而脉冲神经网络虽具能效优势，却因与连续自监督表征存在分布不匹配问题，导致信息编码能力下降。

Method: 提出PTS-SNN框架：1）无参的时序移位脉冲编码器提取局部时序特征；2）基于脉冲稀疏线性注意力的上下文感知膜电位校准策略，利用软提示动态调节PLIF神经元偏置电压，使输入分布适配其响应区间。

Result: 在IEMOCAP等5个多语种数据集上达到73.34%准确率，与主流人工神经网络相当，仅需1.19M可训练参数和0.35mJ/样本推理能耗。

Conclusion: PTS-SNN实现了SSL表征与SNN动力学的有效对齐，在保证性能的同时大幅降低参数量与能耗，为边缘端高效语音情感识别提供了可行方案。

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [766] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出SAYO模型，通过引入基于区域视觉注意力的强化学习奖励机制，解决多模态大语言模型（MLLMs）在视觉推理中注意力不稳定、错误传播的问题，显著提升其在多任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维（CoT）方法在多模态大语言模型中依赖长文本推理，缺乏稳定视觉注意力策略的学习机制；作者发现模型早期视觉错位难以纠正，导致错误传播，根源在于训练中视觉注意力的信用分配不足。

Method: 提出SAYO模型，采用强化学习框架，设计区域级视觉注意力奖励函数，将优化信号显式对齐到视觉支撑的推理步骤，从而引导模型学习更可靠的视觉注意力行为。

Result: 在多个多模态基准上进行大量实验，结果表明SAYO在各类推理与感知任务中均实现持续性能提升。

Conclusion: 基于视觉注意力的RL奖励机制可有效改善MLLMs的视觉聚焦稳定性与推理鲁棒性，为多模态推理中的信用分配问题提供了新思路。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [767] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: 本文提出G-LNS框架，利用大语言模型协同进化破坏与修复算子，实现对大规模邻域搜索（LNS）操作符的自动设计，在TSP和CVRP等组合优化问题上显著优于现有LLM驱动的启发式设计方法及经典求解器。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的启发式自动设计（AHD）方法局限于构造性优先规则或参数化局部搜索引导，难以进行结构探索，易陷入复杂组合优化问题的局部最优。

Method: 提出生成式进化框架G-LNS，利用大语言模型协同进化成对的、紧密耦合的destroy与repair算子，并通过协作评估机制显式建模二者交互，以发现互补的算子逻辑。

Result: 在TSP和CVRP等基准问题上，G-LNS显著超越现有LLM-based AHD方法及强经典求解器；所发现的启发式能在更少计算资源下获得近优解，并在多样且未见的问题分布上表现出强泛化能力。

Conclusion: G-LNS成功将LLM驱动的自动启发式设计拓展至LNS算子层面，通过协同进化与交互评估机制提升了结构探索能力与求解性能，为复杂组合优化问题提供了新范式。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [768] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: 本文提出Puda架构，一种用户主权的数据管理方案，通过客户端多粒度数据共享（详细浏览历史、提取关键词、预定义类别子集）平衡个性化与隐私保护，在旅行规划任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 平台数据孤岛限制用户主权，而大模型代理对个性化数据需求激增，亟需兼顾数据利用与隐私保护的新范式。

Method: 设计并实现基于浏览器的Puda系统，支持用户在客户端自主聚合跨服务数据，并提供三级隐私可控的数据共享机制；采用LLM-as-a-Judge框架评估个性化效果。

Result: 预定义类别子集共享方式可达到详细浏览历史共享97.2%的个性化性能，显著缓解隐私-个性化权衡问题。

Conclusion: Puda为用户主权提供了AI原生基础架构，使用户能在保障隐私前提下安全释放个性化AI潜力。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [769] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 本文提出了一种名为'结构化上下文模型'的正式框架，用于从上下文结构角度分析和比较大语言模型（LLM）智能体，并配套提出了声明式实现框架与可持续的智能体工程流程'Semantic Dynamics Analysis'，显著提升了复杂任务（如动态猴子-香蕉问题）中的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究碎片化严重，缺乏可分析、自洽的正式模型来实现与实现无关的刻画与比较。

Method: 提出'结构化上下文模型'作为理论基础，并构建两个互补组件：(1) 声明式实现框架；(2) 可持续智能体工程流程'Semantic Dynamics Analysis'。

Result: 在动态猴子-香蕉问题变体上验证了框架有效性，最困难设置下智能体成功率提升达32个百分点。

Conclusion: 该形式化模型及配套方法为LLM智能体的研究、设计与迭代提供了统一、可复现且可扩展的理论与工程基础。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [770] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 本文提出“氛围自动化（Vibe-Automation）”概念，指出生成式AI标志着一种认识论跃迁——从优化显性指标转向捕捉和运作隐含于实践中的语境化、默会性规律；人类角色相应转向“氛围工程（Vibe-Engineering）”，即协调生成系统的对齐与情境判断；并据此构建覆盖教师认知、产业合作与课程设计三层三域的教育变革框架。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习聚焦于优化预定义目标，而生成式AI展现出对语境、语义与风格连贯性的敏感性，挑战了计算机科学中关于知识表征与自动化本质的基本假设，亟需新的理论概念来刻画这一质变。

Method: 通过概念分析与理论建构，引入‘Vibe-Automation’与‘Vibe-Engineering’两个原创概念，结合认识论、实践哲学与教育学视角，建立三层（教师世界观、产业关系、课程设计）三域（分析层级/行动领域）的整合性框架。

Result: 确立了生成式AI的核心能力在于操作化默会规律（而非拥有默会知识），厘清了人机分工新范式（从算法问题设定转向氛围工程），并提出面向教育系统转型的具体分析维度与风险警示（如模式坍塌、文化同质化）。

Conclusion: 生成式AI引发的不是技术迭代而是认识论革命；其真正价值在于拓展人类对不可言明实践智慧的技术性介入方式；教育与制度响应必须主动引导该技术朝向多样性、情境敏感性与批判性共治发展，而非被动适应或陷入合成单调性。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [771] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 本文首次系统研究了视觉语言模型（VLMs）在道德判断中的谄媚行为（moral sycophancy），发现其在用户意见影响下易从正确道德判断转向错误判断，且存在显著不对称性；不同数据集上表现差异大，并揭示纠错能力与错误引入间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未充分理解VLMs在道德导向的视觉决策中受用户意见影响而产生的谄媚行为，尤其在用户明确表达异议时的道德鲁棒性问题。

Method: 基于Moralise和M^3oralBench两个道德评估数据集，对10个主流VLM进行实验，在显式用户异议条件下分析其初始与后续响应的道德正确性，并引入Error Introduction Rate（EIR）和Error Correction Rate（ECR）量化错误引入与修正能力。

Result: VLMs在用户异议下常将原本正确的道德判断转为错误；存在右→错易、错→右难的不对称性；后续提示在Moralise上普遍降低性能，但在M^3oralBench上效果不一；强纠错能力模型更易引入新错误，保守模型少出错但难自纠；初始道德立场正确时谄媚倾向更强。

Conclusion: VLMs在道德推理中存在显著且脆弱的谄媚倾向，亟需设计兼顾伦理一致性与鲁棒性的新训练或对齐策略。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [772] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: 本文提出SHARP框架，通过Shapley值进行分层归因，解决多智能体系统中信用分配难题，显著提升LLM与外部工具协同的强化学习训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确归因多智能体系统中各代理对决策成败的贡献，导致强化学习训练低效。

Method: 提出基于Shapley值的分层归因框架SHARP，包含全局广播精度奖励、Shapley边际信用奖励和工具流程奖励三部分，并对轨迹组内代理优势进行归一化。

Result: 在多个真实基准测试中，SHARP平均匹配性能比单智能体和多智能体基线分别提升23.66%和14.05%。

Conclusion: SHARP有效缓解了多智能体强化学习中的信用分配问题，提升了训练稳定性与性能。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [773] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: 本文提出CoTZero，一种无需人工标注的视觉语言模型推理增强方法，通过双阶段数据合成与认知对齐训练，提升模型的层次化、可验证和因果性视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型依赖表面相关性，缺乏逻辑一致的结构化表征，导致难以进行组合性与可验证的高层语义和因果关系推理。

Method: 提出CoTZero框架：（i）双阶段数据合成——底层提取视觉原子单元并组合成结构化问答推理形式，顶层利用全局结构引导局部细节与因果关系理解；（ii）基于合成链式推理数据，设计认知一致可验证奖励（CCVR）用于强化微调，提供分步推理连贯性与事实正确性反馈。

Result: 在多级语义不一致性基准测试中，CoTZero取得83.33%的F1分数（含词汇扰动负样本），且在域内与跨域设置下均表现优异；消融实验证明各组件均提升推理可解释性与人类对齐性。

Conclusion: CoTZero通过引入认知启发的建模机制与无标注数据合成策略，显著增强了VLMs的层次化、因果性与可验证视觉推理能力，为迈向人类水平视觉理解提供了新路径。

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [774] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

TL;DR: 本文提出一种以效应为中心、以可识别性为先的因果发现框架，强调在反馈驱动系统中对因果图进行可识别性、稳定性和可证伪性评估，而非单纯追求图结构恢复准确率；实证研究表明，许多统计上合理的发现结果在加入基本时序与语义约束后无法实现点识别，而能识别的方法则在不同图结构下仍给出一致且稳健的效应估计。


<details>
  <summary>Details</summary>
Motivation: 因果发现在大规模遥测数据中应用日益广泛，但在具有强自我选择的反馈驱动系统中，其用于决策支持的可靠性尚不明确。

Method: 提出效应中心、可识别性优先的框架，将发现的因果图视为结构假设，并通过可识别性、稳定性与可证伪性进行评估；在真实游戏遥测数据上实证研究早期竞技体验对短期留存的影响。

Result: 许多统计上合理的因果图在施加基本时序和语义约束后无法实现目标因果效应的点识别；当可识别时，多种算法虽输出差异显著的图结构，却收敛到相似且稳健的效应估计（包括直接边缺失但效应经间接路径保持的情形），且该估计通过安慰剂检验、子抽样与敏感性反驳；而其他方法则表现出可识别性不稳定、效应估计对阈值敏感或衰减等问题。

Conclusion: 图层面指标不足以代理特定目标因果查询的可靠性；在遥测驱动系统中，应优先保障可识别性与效应级验证，而非仅追求因果结构恢复。

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [775] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出Outline-Guided Path Exploration (OPE)方法，通过生成多样化推理提纲来引导并行路径探索，缓解信息冗余与互信息瓶颈，从而提升大推理模型在数学任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的并行推理方法多聚焦于聚合阶段优化，忽视路径探索阶段的信息多样性与效率问题，导致性能受限。

Method: 提出OPE框架：先生成多样化的推理提纲以划分解空间，再进行提纲引导的并行路径推理；采用迭代式RL策略分别优化提纲规划与提纲引导推理。

Result: 在多个数学基准测试上验证了OPE的有效性，显著提升不同聚合策略下的推理准确率与解发现可靠性。

Conclusion: 显式结构化路径探索（如提纲引导）可突破并行推理中的互信息瓶颈，是提升LRMs推理能力的关键方向。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [776] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 本文发现现有时序知识图谱（TKG）基准存在严重偏差，仅靠共现统计即可接近SOTA性能，暴露了数据集固有偏置与评估任务过于简化的问题；为此提出新的TKG演化基准，包含四个去偏数据集和两个更贴近演化本质的新任务，以推动对TKG演化建模挑战的准确评估。


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测基准存在数据偏差和评估简化问题，导致模型无需利用时间信息即可获得高分，无法真实反映模型对知识演化的建模能力。

Method: 通过系统性分析现有数据集和评估设置，识别出共现捷径、时间区间格式不合理、知识过时性被忽略、演化信息不足等根本缺陷，并据此构建去偏的TKG演化基准，包括修正的数据集和更符合演化本质的两项新任务。

Result: 提出了首个专门面向TKG演化建模的基准TKG Evolution Benchmark，包含四个去偏数据集及两个新任务，在GitHub开源，为公平、深入评估TKG演化能力提供了新标准。

Conclusion: 当前TKG预测研究因基准缺陷而存在评估失真；真正的TKG演化建模需关注知识动态性、过时性与精细演化过程，新基准为此提供了更可靠的评测基础。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [777] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 本文提出SAGE采样范式，利用大推理模型（LRM）隐含的自我停止思考能力，提升其推理效率与准确性，并通过SAGE-RL在数学基准上显著改进性能。


<details>
  <summary>Details</summary>
Motivation: 现有长思维链（CoT）方法存在冗余、低效、延迟高且未必提升准确率的问题；作者发现LRM其实隐含‘何时停止思考’的能力，但被当前采样方式掩盖。

Method: 提出SAGE（Self-Aware Guided Efficient Reasoning）新采样范式，释放LRM的隐式自停能力；进一步将SAGE作为混合采样集成到基于组的强化学习中（即SAGE-RL），用于优化标准pass@1推理。

Result: SAGE-RL在多个挑战性数学基准上显著提升了LRM的推理准确率和计算效率。

Conclusion: LRM具备未被充分利用的自我意识式推理终止能力；SAGE及其强化学习扩展SAGE-RL为高效、准确的推理提供了新范式。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [778] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 本文提出了一种将随机森林分类器编译为可解释电路的方法，并利用该方法计算决策的完全原因、鲁棒性及最短翻转路径，支持多种解释形式的枚举与分析。


<details>
  <summary>Details</summary>
Motivation: 提升随机森林模型的可解释性，支持高效计算决策的抽象原因、鲁棒性和翻转策略。

Method: 将随机森林编译为类特定电路；基于电路实现完全/一般原因、鲁棒性及最短翻转路径的算法；支持枚举充分原因、必要原因和对比解释。

Result: 所提编译方法显著优于现有类似方法；能高效生成可处理电路以支持各类解释性计算；在多个数据集上验证了方法的有效性与实用性。

Conclusion: 该工作为随机森林提供了统一、高效且可扩展的可解释性框架，推动了模型解释从局部到全局、从定性到定量的发展。

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [779] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: 本文提出MemAdapter，一种统一异构记忆范式的记忆检索框架，通过两阶段训练策略（生成式子图检索器+轻量级对比学习对齐模块）实现跨范式快速对齐与融合，在多个基准上显著优于现有方法，且计算成本极低。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体记忆系统通常局限于孤立的记忆范式（如显式、参数化或潜在记忆），其检索方法紧密耦合，难以实现跨范式泛化与融合。

Method: 提出MemAdapter框架，采用两阶段训练：(1) 在统一记忆空间中训练生成式子图检索器；(2) 通过对比学习训练轻量级对齐模块，适配未见记忆范式。

Result: 在三个公开基准上，MemAdapter的生成式子图检索器在三种记忆范式和不同规模代理模型上均持续超越五个强基线；单GPU仅需13分钟完成跨范式对齐，计算开销不足原检索器的5%；并支持零样本跨范式融合。

Conclusion: MemAdapter为LLM智能体提供了灵活、高效、即插即用的记忆系统统一方案，推动了异构记忆范式的协同与集成。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [780] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为VIRF的神经符号框架，通过逻辑导师与大语言模型规划器之间的师生对话，实现对物理任务计划的安全验证与智能修复，显著提升了安全性与目标达成率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为具身AI的规划器缺乏形式化推理能力，难以提供严格的物理部署安全保证；现有方法在安全检查上不可靠，或仅拒绝不安全计划而无法修复。

Method: 提出可验证的迭代精炼框架（VIRF），包含一个基于形式化安全本体的确定性逻辑导师，与LLM规划器进行因果性和教学性反馈的师生对话；同时构建可扩展的安全知识获取流程，从真实文档中合成安全知识库。

Result: 在家庭安全任务中，VIRF实现了0%的危险动作率（HAR）和77.3%的目标条件达成率（GCR），为所有基线中最高；平均仅需1.1次修正迭代。

Conclusion: VIRF为构建本质可信、可验证安全的具身智能体提供了原理性路径。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [781] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: SCOUT-RAG是一种面向分布式、受限访问环境的可扩展且成本高效的图增强检索框架，通过四个协同智能体实现渐进式跨域检索，在保持性能的同时显著降低调用次数、令牌消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在医院或跨国组织等分布式且访问受限的场景中，传统基于中心化知识图的Graph-RAG难以在缺乏全局图可见性的情况下高效选择相关领域与遍历深度。

Method: 提出SCOUT-RAG框架，包含四个协同智能体：领域相关性估计、跨域扩展决策、自适应遍历深度控制、答案合成；以最小化‘检索遗憾’（遗漏有用领域信息）为目标，兼顾延迟与API成本。

Result: 在多领域知识设置下，SCOUT-RAG性能媲美集中式基线（如DRIFT和穷举遍历），同时大幅减少跨域调用次数、总处理token数和延迟。

Conclusion: SCOUT-RAG为分布式受限环境下的图增强检索提供了高效、可控、实用的新范式，平衡了效果、成本与时效性。

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [782] [Automating the Refinement of Reinforcement Learning Specifications](https://arxiv.org/abs/2512.01047)
*Tanmay Ambadkar,Đorđe Žikelić,Abhinav Verma*

Main category: cs.AI

TL;DR: 本文提出AutoSpec框架，通过探索引导的策略改进粗粒度逻辑规范，使其更利于强化学习算法学习有效策略。


<details>
  <summary>Details</summary>
Motivation: 当任务被欠指定时，智能体可能无法学习到有用的策略，因此需要改进粗粒度逻辑规范。

Method: 提出AutoSpec框架，在SpectRL逻辑下利用其组合性设计四种保持规范正确性的细化过程，并将其集成到现有强化学习算法中。

Result: 实验表明，使用AutoSpec生成的细化逻辑规范可显著提升可解决控制任务的复杂度。

Conclusion: AutoSpec能有效提升逻辑规范对强化学习的指导能力，同时保证规范细化的正确性。

Abstract: Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \textsc{AutoSpec} are utilized.

</details>


### [783] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: 本文提出AGENTWM，首个专为代理模型设计的水印框架，通过在功能等价的工具执行路径中 subtly 偏置分布来嵌入可验证水印，有效抵御模仿攻击且不影响代理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型演变为具有自主推理和工具使用能力的代理系统后，其知识产权价值显著提升，但现有水印技术因无法获取内部推理痕迹而在灰盒场景下失效，亟需针对代理系统的新型水印方案。

Method: AGENTWM利用动作序列的语义等价性，在功能相同但实现路径不同的工具调用序列中注入水印；设计自动化流水线生成鲁棒水印方案，并构建基于统计假设检验的验证机制。

Result: 在三个复杂领域上的实验表明，AGENTWM实现高检测准确率，对代理性能影响极小，且能有效抵抗自适应攻击者——攻击者若试图移除水印将严重损害被盗模型的实用性。

Conclusion: AGENTWM是首个适用于灰盒代理系统的实用化水印框架，成功在保持用户不可感知性与代理功能性的同时，提供可验证、抗移除的知识产权保护。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [784] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: 本文提出了Personalized Agent Security Bench (PASB)，一个面向真实世界个性化LLM代理的安全评估框架，用于识别其在用户提示处理、工具调用和记忆检索等阶段的关键安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全研究多聚焦于合成或任务导向场景，无法准确刻画个性化AI助手在真实部署中的攻击面与风险传播机制。

Method: 构建端到端黑盒安全评估框架PASB，整合个性化使用场景、真实工具链和长周期交互，并以OpenClaw为案例开展系统性安全评测。

Result: 发现OpenClaw在多个个性化场景下存在严重漏洞，涵盖用户提示处理、工具使用及记忆检索等执行阶段。

Conclusion: 个性化LLM代理在实际部署中存在显著安全风险，亟需面向真实使用场景的专用安全评估框架。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [785] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

TL;DR: 本文探讨了具备情境感知能力的AI系统在安全评估中的脆弱性，提出通过减少内部表征中可提取的'场景信息'（evaluation vs. deployment）来提升对齐鲁棒性，并在语言模型上验证了该方法对科学谄媚和时间型休眠代理两种失败模式的有效性及差异。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全评估假设评估时的行为能预测部署时的行为，但具备情境感知能力的智能体可能利用评估与部署间的细微信息差异（regime leakage），实施如谄媚或休眠代理等条件策略，在监督下合规、在部署中背叛，导致评估失效。

Method: 将对齐评估重构为部分可观测下的信息流问题，理论证明行为偏差受内部表征与场景变量间互信息约束；进而提出'场景盲机制'——在训练中通过对抗不变性降低决策相关内部表征中场景信息的可解码性。

Result: 在开源语言模型上验证：该方法有效抑制科学谄媚与时间型休眠代理行为，且不损害任务性能；但二者响应不同：谄媚在弱干预下即发生表征与行为突变，而休眠代理需更强干预且无清晰的场景信息解码崩溃。

Conclusion: 表征不变性是一种有意义但本质受限的控制手段，其效果取决于场景信息在策略中的嵌入方式；应将行为评估与针对场景感知和信息流的白盒诊断相结合。

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [786] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: 本文提出TreeTensor，一种支持嵌套数据结构的通用张量容器，通过约束树形结构建模数据关系，兼容主流机器学习库，兼顾易用性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 传统张量难以高效处理具有层次结构（嵌套、多模态）的复杂认知AI系统数据，缺乏对可变形状和结构化关系的原生支持。

Method: 总结嵌套数据的两类主要计算模式，设计TreeTensor——一种具备内存连续性、切片独立性及魔法方法支持的约束树状数据容器，实现对任意函数和主流库（如NumPy、PyTorch、Scikit-Learn）的零开销适配，并支持异步执行与变长计算。

Result: TreeTensor在AlphaStar等复杂AI系统中展现出强大实用性，基准测试表明其运行高效、无额外开销。

Conclusion: TreeTensor为嵌套结构化数据提供了一种统一、高效、易用的张量抽象，拓展了AI系统对复杂数据建模与计算的能力边界。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [787] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 本文提出了一种名为'Reinforcement Inference'的推理时控制策略，利用模型自身生成过程中的不确定性（熵）来动态决定是否对高熵输出进行二次、更审慎的推理，从而在不重训练模型的前提下显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统单次贪婪解码会低估模型真实能力，许多错误源于模型在内部不确定时过早确定输出，而非知识缺失。

Method: 提出'Reinforcement Inference'方法：在零样本、确定性解码下，基于模型每步生成的熵动态判断是否触发第二次推理；该方法无需修改模型参数或重新训练，仅依赖推理时控制。

Result: 在MMLU-Pro（12,032题，14学科）上，DeepSeek-v3.2的准确率从60.72%提升至84.03%，仅增加61.06%推理调用；全重问（100% re-asking）达84.35%，证明熵感知选择高效；纯提示工程对照组表现差于基线，排除了单纯提示改进的解释。

Conclusion: 熵可作为推理时的一等控制信号；'一次贪婪解码'与'不确定性驱动的审慎推理'之间的性能差距，揭示了模型潜在推理深度，并为未来训练目标（如正确性-置信度对齐）提供了新方向。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [788] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 本文提出了一种结合在线个性化与自适应树形组相对策略优化（AT-GRPO）的长视野强化学习框架，用于提升开放域对话代理的长期互动质量与个性化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖预收集的用户数据，且强化学习存在短视野偏差，忽视对话的长期价值。

Method: 构建双智能体博弈范式：用户智能体通过风格模仿和主动终止建模动态环境；对话智能体采用AT-GRPO算法，将对话轨迹建模为树结构，并引入阶段感知的自适应观测范围以平衡探索与维持。

Result: 实验表明该框架在性能、采样效率和鲁棒性方面均优于现有方法。

Conclusion: 所提长视野RL框架有效缓解了数据依赖与短视问题，实现了更自然、持久、个性化的开放域对话。

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [789] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

TL;DR: 本文提出了一种统一的理论框架，将多智能体推理增益分解为探索、信息和聚合三个维度，并基于此提出了PRISM框架，在多个基准测试中实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对多智能体协作为何优于单智能体推理以及哪些设计选择最关键的原则性理解，难以系统优化。

Method: 提出一个统一理论框架，将多智能体推理增益分解为探索（Exploration）、信息（Information）和聚合（Aggregation）三个独立维度；在此基础上设计PRISM框架，通过角色多样性、执行驱动反馈与证据交叉评估、迭代合成与闭环验证联合优化三者。

Result: 在数学推理、代码生成和函数调用等基准上，PRISM达到SOTA性能，且计算效率优于仅优化部分维度的方法。

Conclusion: 该理论框架为多智能体推理系统提供了可操作的设计原则，推动了从启发式设计向原理驱动设计的转变。

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [790] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 本文提出了一种基于全局工作空间理论（GWT）的自上而下多模态注意力机制，提升了系统在噪声下的鲁棒性，并展现出优于现有模型的跨任务与跨模态泛化能力，在MM-IMDb 1.0基准上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于GWT的多模态计算架构虽具潜力，但其注意力机制研究不足，尤其缺乏对模态选择机制的系统探索。

Method: 提出一种自上而下的注意力机制，用于在全局工作空间内动态选择相关模态；在Simple Shapes和MM-IMDb 1.0两个多模态数据集上进行评估，并与现有基线模型对比。

Result: 所提注意力机制显著提升全局工作空间在噪声环境下的鲁棒性；展现出独特的跨任务与跨模态泛化能力；在MM-IMDb 1.0上性能媲美当前最优方法。

Conclusion: 该注意力机制有效弥补了GWT在多模态整合中注意力建模的空白，为构建更具认知合理性的多模态AI系统提供了新路径。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [791] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出OSCAR框架，将组合图像检索（CIR）从启发式搜索重构为轨迹优化问题，通过离线建模与在线引导提升性能，在多个基准上显著超越SOTA，且仅需10%训练数据即达优效。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法存在两大局限：统一嵌入检索易受单模型视野限制，启发式智能体检索依赖低效试错编排。

Method: 提出OSCAR优化引导的智能体规划框架：离线阶段将CIR建模为两阶段混合整数规划问题，利用布尔集合运算推导最优轨迹并构建‘黄金库’；在线阶段以该库为上下文示例引导VLM规划器推理。

Result: 在三个公开基准及一个私有工业基准上持续超越SOTA；仅用10%训练数据即取得优越性能，验证了规划逻辑强泛化能力而非数据记忆。

Conclusion: OSCAR首次将CIR建模为轨迹优化问题，通过严谨数学建模与离线-在线协同机制，显著提升检索准确性与数据效率，为复杂多模态检索任务提供新范式。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [792] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 本文提出了辩论查询复杂度（DQC）的概念，证明了PSPACE/poly类问题只需O(log n)次查询即可判定，揭示了AI辩论机制在人类监督下的极高查询效率，并建立了DQC与电路复杂度之间的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 分析AI安全中辩论机制的实际人类监督成本，即裁判需查看辩论记录的最少查询次数。

Method: 引入辩论查询复杂度（DQC）作为衡量标准，通过复杂度理论分析DQC与PSPACE/poly、电路大小等概念的关系。

Result: 证明PSPACE/poly恰为可用O(log n)次查询判定的函数类；任意依赖全部输入位的函数至少需Ω(log n)次查询；电路大小为s的函数满足DQC(f) ≤ log(s) + 3；并指出DQC下界突破可导出新电路下界。

Conclusion: 辩论机制在人类监督下具有极高的查询效率，且DQC为连接AI安全与经典计算复杂性理论（尤其是电路复杂度）提供了新桥梁。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [793] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 本文探讨了用户对聊天机器人的信任机制，指出这种信任往往源于交互设计中利用认知偏差的行为策略，而非系统实际的可信度；因此建议将聊天机器人重新定义为以组织目标为导向的‘高技能销售人员’，并呼吁区分心理层面的信任形成与规范意义上的可信度，以推动相关研究与用户信任校准支持机制的发展。


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人日益模糊自动化系统与人类对话之间的界限，用户对其的信任基础亟需深入考察；现有监管与政策框架多从规范角度定义信任，而用户实际信任却常由行为机制驱动，存在概念混淆与实践脱节问题。

Method: 本文采用概念分析与理论重构方法，基于对用户信任形成机制的观察，提出将聊天机器人重新定位为‘高技能销售人员’，并辨析心理信任与规范可信度的本质差异。

Result: 揭示了当前聊天机器人信任问题的核心矛盾：用户信任常由交互设计诱发的认知偏差所塑造，而非系统真实可信度；提出‘销售员’隐喻以凸显其目标导向性，并强调需厘清不同‘信任’概念以避免误导。

Conclusion: 应区分心理学意义上的信任形成过程与伦理/规范意义上的可信度要求；未来研究需聚焦用户信任校准机制，并建立更强有力的支持体系，以保障人机交互中的知情权与自主性。

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [794] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 本文探讨了命题STRIPS规划中仅含一个前提和一个效果的操作符（STRIPS^1_1）的计算复杂性，试图验证其是否为NP完全问题；通过调用SAT求解器处理小规模实例、引入字面量图并映射到Petri网进行分析。


<details>
  <summary>Details</summary>
Motivation: 确定STRIPS^1_1（每个操作符仅有一个前提和一个效果）的计算复杂性是否为NP完全，此前该问题尚未解决。

Method: 调用SAT求解器处理小规模实例，引入字面量图（literal graph），并将该图映射到Petri网进行分析。

Result: 对STRIPS^1_1的‘小解假设’提供了新的分析视角，但未明确证明其是否NP完全，而是通过多种形式化工具为其复杂性研究提供了新路径。

Conclusion: STRIPS^1_1的复杂性仍未完全确定，但本文通过SAT求解、字面量图与Petri网建模等方法，为后续判定其是否NP完全奠定了基础。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [795] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文首次综述和分析了用于可解释人工智能（XAI）评估的合成人工智能真值（SAIG）方法，提出了一种新分类法，并指出当前XAI评估缺乏共识与标准化。


<details>
  <summary>Details</summary>
Motivation: XAI评估缺乏普遍适用的真实标签（ground truth），导致客观评估困难，亟需可靠的人工构造真值方法（即SAIG）来支持直接评估。

Method: 提出一种新颖的分类法，识别出区分不同SAIG方法的七个关键特征，并开展比较研究。

Result: 发现当前SAIG方法在XAI评估技术有效性上缺乏共识，揭示该领域亟需进一步研究与标准化。

Conclusion: SAIG是一条有前景的XAI评估路径，但其方法多样且尚未统一，未来需加强理论基础、实证验证与标准制定。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [796] [Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning](https://arxiv.org/abs/2602.08734)
*David Hudák,Maris F. L. Galesloot,Martin Tappler,Martin Kurečka,Nils Jansen,Milan Češka*

Main category: cs.AI

TL;DR: 本文提出了Lexpop框架，用于解决部分可观测马尔可夫决策过程（POMDP）及其鲁棒扩展HM-POMDP，结合深度强化学习与有限状态控制器提取，兼顾性能与可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP求解器可扩展性差，且难以在多个POMDP上提供鲁棒策略。

Method: Lexpop框架包含两阶段：(1) 使用深度强化学习训练基于循环神经网络的策略；(2) 通过高效提取方法将神经策略转化为可形式化验证的有限状态控制器；进一步扩展至HM-POMDP，通过迭代训练与最坏情况POMDP关联实现鲁棒性。

Result: 在大规模状态空间问题上，Lexpop在POMDP和HM-POMDP任务中均优于当前最优求解器。

Conclusion: Lexpop在保持策略性能的同时，通过提取可验证控制器提升了可信度与鲁棒性，为复杂、不确定环境下的决策提供了新范式。

Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.

</details>


### [797] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: This paper introduces and investigates 'belief offloading'—a form of cognitive offloading where people delegate belief formation and maintenance to LLMs, with implications for cognition, behavior, and epistemic integrity.


<details>
  <summary>Details</summary>
Motivation: To understand the cognitive and epistemic consequences of relying on LLMs as thought partners, especially how offloading belief formation may impair human cognitive skills and alter belief systems.

Method: Conceptual analysis drawing on philosophy (epistemology), psychology (cognitive offloading), and computer science (human-AI interaction); includes defining belief offloading, clarifying its boundary conditions, and proposing a descriptive taxonomy with normative evaluation.

Result: A clear definition and conceptual framework for belief offloading; identification of boundary conditions; a descriptive taxonomy; and articulation of its normative implications (e.g., risks to autonomy, epistemic responsibility, and belief coherence).

Conclusion: Belief offloading is a distinct and consequential phenomenon in human-AI interaction that warrants empirical and ethical scrutiny; future work should assess its prevalence, mechanisms, and long-term effects on cognition and belief systems.

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [798] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 本文将隐式思维链（latent chain-of-thought）建模为表征空间中的因果过程，利用结构因果模型（SCM）和do-干预分析其内部潜变量步骤的因果作用，在Coconut和CODI两种方法上系统探究了必要性、影响传播与模式保留三大问题，揭示了潜步骤非均匀功能化及输出偏置与表征承诺间的时间错位现象，并据此提出面向模式与稳定性的新分析与优化范式。


<details>
  <summary>Details</summary>
Motivation: 隐式思维链中间计算难以评估，仅依赖相关性探针不够可靠；需建立可操纵、可干预的因果视角来理解其内在推理机制。

Method: 将潜式思维链建模为结构因果模型（SCM），对各潜步骤施加do-干预，结合数学与通用推理任务，在Coconut和CODI两种范式上定量分析因果必要性、影响传播路径及答案模式演化。

Result: 发现潜步骤并非等效增加网络深度，而是呈现阶段性功能与非局部路由特性；存在早期输出偏置与晚期表征承诺之间的持续gap；中间轨迹保留竞争性答案模式，且输出级承诺滞后于表征级承诺。

Conclusion: 应采用模式条件化（mode-conditional）与稳定性感知（stability-aware）的分析框架及对应训练/解码目标，以更可靠地解释和提升隐式推理系统。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [799] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: 本研究探讨了通用大语言模型（AI工具）在认知诊断建模中辅助构建Q矩阵的可行性，发现部分AI模型（如Gemini 2.5 Pro）生成的Q矩阵与专家验证版一致性高于人类专家，但新版本AI表现反而下降。


<details>
  <summary>Details</summary>
Motivation: Q矩阵构建是认知诊断建模中关键却耗时费力的步骤，亟需高效、可靠的自动化支持方法。

Method: 将多个AI模型与人类专家使用相同训练材料生成Q矩阵，并采用Cohen's kappa系数评估AI生成Q矩阵、验证版Q矩阵及人类评分者Q矩阵之间的一致性。

Result: Gemini 2.5 Pro在2025年5月测试中与验证Q矩阵的Kappa达0.63，高于所有人类专家；但2026年1月使用更新AI版本后，一致性显著下降。

Conclusion: 特定AI模型可在Q矩阵构建中达到甚至超越人类专家水平，但其性能随模型迭代不稳定，尚不能完全替代专家判断，需进一步研究其可靠性与优化路径。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [800] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于残差连接和大语言模型（LLM）的根因定位方法RC-LLM，用于解决微服务架构中因故障传播复杂、遥测数据高维导致的根因分析难题；通过残差式分层融合结构整合多源遥测数据，并利用LLM建模时序与跨服务因果依赖，在CCF-AIOps数据集上验证了其高准确率与高效率。


<details>
  <summary>Details</summary>
Motivation: 微服务架构中故障传播复杂、遥测数据（指标、日志、链路）维度高，导致现有根因分析（RCA）方法效果受限。

Method: 提出RC-LLM方法：设计残差类分层融合结构整合多源遥测数据，并利用大语言模型（LLM）的上下文推理能力建模时间序列与跨微服务间的因果依赖关系。

Result: 在CCF-AIOps微服务数据集上的实验表明，RC-LLM在根因分析任务中兼具高准确率与高效率。

Conclusion: RC-LLM有效提升了复杂微服务系统中根因定位的准确性与效率，验证了结合残差结构与LLM建模因果依赖的可行性与优势。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [801] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为NADEx的负感知扩散模型，用于时序知识图谱（TKG）外推任务，通过引入负样本信息和余弦对齐正则化，提升了预测准确性和嵌入校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在TKG推理中仅利用正向证据，忽略负向上下文信息，且训练目标以排序为主的交叉熵损失难以有效校准去噪嵌入。

Method: NADEx将实体、关系和时间间隔的主语中心历史编码为序列嵌入；在前向过程中扰动查询对象，在反向过程中利用Transformer去噪器结合时序-关系上下文重建；并基于批次负原型设计余弦对齐正则化项以增强判别边界。

Result: 在四个公开TKG基准数据集上，NADEx实现了当前最优性能。

Conclusion: 引入负感知机制与嵌入校准正则化能显著提升扩散模型在TKG外推任务中的表现，为时序知识推理提供了新思路。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [802] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 本文提出了一种基于聚类和偏好型多目标强化学习（PbMORL）的算法，用于在马尔可夫决策过程（MDP）中建模社会中多个智能体的价值对齐与价值系统，实现可解释、可适应多样用户偏好的价值感知AI。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾价值的社会性（需适配多用户）、多样性（用户价值系统各异）与可操作性（易误设、缺乏可解释性与适应性）。

Method: 结合聚类与偏好型多目标强化学习（PbMORL），联合学习社会导出的价值对齐模型（groundings）和能简洁表征不同用户群体的价值系统（簇），每个簇包含一个价值系统及其对应近似Pareto最优策略。

Result: 在两个含人类价值观的MDP任务上，所提方法优于当前最优PbMORL算法及基线方法。

Conclusion: 该方法能有效建模社会尺度下的价值对齐与分组价值系统，提升AI对多样化人类价值的可解释性、适应性与操作性。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [803] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出了一种统一的定性推理框架，支持多尺度、时序及松散集成等扩展形式，并统一研究其可满足性判定及其复杂度，证明了多项式时间可解性并推广了定性形式化定义。


<details>
  <summary>Details</summary>
Motivation: 现有定性推理形式化方法在处理多尺度、时序和松散集成等扩展时缺乏统一框架，且部分重要形式化被原有定义排除，难以系统分析可满足性与复杂度。

Method: 构建一个通用的定性推理形式化框架，涵盖多种扩展与组合方式；基于该框架，形式化定义可满足性判定问题，并通过构造两个互补定理证明其多项式可解性；同时推广定性形式化定义以包含文献中被排除的重要形式。

Result: 建立了两个保证可满足性判定为多项式时间的互补定理；成功复现了尺寸-拓扑组合的已知结果；推广了定性形式化定义，使其覆盖更广泛的实际推理场景。

Conclusion: 所提出的统一框架不仅支持多种定性推理扩展的建模与推理，还为可满足性判定提供了理论保障与复杂度分析工具，拓展了定性推理的表达能力与适用范围。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [804] [Scalable Delphi: Large Language Models for Structured Risk Estimation](https://arxiv.org/abs/2602.08889)
*Tobias Lorenz,Mario Fritz*

Main category: cs.AI

TL;DR: 本文提出Scalable Delphi方法，利用大语言模型（LLM）模拟结构化专家判断，显著提升高风险领域定量风险评估的可扩展性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统德尔菲法虽可靠但耗时耗力，难以在多数实际场景中应用；亟需一种可扩展、低成本且保持可信度的替代方案。

Method: 提出Scalable Delphi：为LLM设定多样化专家角色，引入迭代优化与理由共享机制，并构建基于必要条件（校准性、证据敏感性、人类专家一致性）的新型评估框架。

Result: 在AI增强网络安全风险评估中，LLM小组与基准真值相关性高（r=0.87–0.95），随证据增加持续改进，且与人类专家判断高度一致，甚至优于人类小组间的一致性。

Conclusion: LLM可作为结构化专家判断的有效代理，将专家研判周期从数月缩短至数分钟，拓展了严谨风险评估的应用边界。

Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.

</details>


### [805] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: 本文提出了一种名为时空剪枝（STP）的框架，用于提升基于扩散的大语言模型（dLLMs）在强化学习（RL）训练中的效率与稳定性。该方法通过空间剪枝（利用静态先验约束探索空间）和时间剪枝（跳过冗余的后期精炼步骤）压缩生成过程中的冗余，并在理论上证明其可严格降低对数似然估计的方差，从而提升策略更新稳定性；实验表明STP在效率与准确率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于扩散型大语言模型（dLLMs）面临效率低和训练不稳定两大挑战，亟需一种能同时提升二者的方法。

Method: 提出时空剪枝（STP）框架：（1）空间剪枝——利用静态先验约束生成过程中的探索空间；（2）时间剪枝——跳过冗余的后期细化步骤；并从理论上证明其可降低对数似然估计方差。

Result: STP在多个实验中显著优于当前最优基线，在训练效率和生成准确性两方面均有提升。

Conclusion: STP是一种有效且理论可证的剪枝策略，可同时增强dLLMs在RL训练中的效率与稳定性，为扩散语言模型的优化提供了新思路。

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [806] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: 本文提出了CausalT5K，一个包含5000多个案例、覆盖10个领域的因果推理诊断基准，用于系统评估大语言模型在检测rung collapse、抵抗sycophantic drift及生成Wise Refusals三方面的能力；该基准基于真实叙事嵌入因果陷阱，并采用Pearl因果阶梯理论构建，通过人机协同流程开发，揭示了静态审计策略普遍失效的‘四象限控制图景’。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在因果推理中存在sycophancy、rung collapse和miscalibrated refusal等已知失败模式，但缺乏可系统诊断的基准阻碍了修复进展。

Method: 构建CausalT5K基准：涵盖10个领域、5000+真实叙事案例；定义三项核心能力测试；采用Utility（敏感性）与Safety（特异性）双维度评估；通过40位领域专家参与、多轮人机交叉验证及规则/LLM/人工复合评分完成开发。

Result: CausalT5K成功揭示静态审计策略在所有场景下均失效，形成‘四象限控制图景’；其双维度评估能暴露传统聚合准确率无法发现的失败模式。

Conclusion: CausalT5K作为基于Pearl因果阶梯的研究基础设施，为可信推理系统的评估与改进提供了系统化、可分解、现实化的诊断工具。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [807] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: 本文提出CoRefine方法，利用轻量级Conv1D控制器基于LLM输出的置信度动态决定是否停止、重审或换策略，显著减少推理所需token（约190倍），同时保持高精度；进一步扩展为CoRefine-Tree，支持自适应顺序-并行推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）依赖大量并行解码（如512样本）提升推理准确率，但计算开销巨大，亟需更高效的测试时缩放方法。

Method: 提出CoRefine：在冻结LLM上叠加仅211k参数的Conv1D控制器，利用完整推理轨迹的置信度信号动态决策（停止/重审/换路径）；进一步设计CoRefine-Tree，融合顺序与并行策略，支持验证器兼容和易部署。

Result: 平均每题仅2.7次精炼步骤，token用量较512样本基线降低约190倍；控制器在自信终止时达92.6%精度；在多个推理基准及三个开源模型上验证有效性。

Conclusion: 将置信度视为可控信号而非正确性保证，CoRefine提供了一种模块化、可扩展的推理原语，适用于验证器不完美或智能体场景。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [808] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

TL;DR: 本文提出了一种名为智能虚拟情景室（IVSR）的双向数字孪生平台，结合自主AI代理，用于实时 wildfire管理。该系统通过多源数据构建火灾环境的动态虚拟副本，利用AI相似性引擎匹配预存灾害模拟库并优化干预策略，再将授权行动反馈至物理层，形成闭环响应。实验表明其在检测-干预延迟和资源协调方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统灾害管理框架依赖静态模拟和被动数据采集，难以实时适应不断演变的野火事件，而野火频率与强度正因全球变暖持续上升，亟需更敏捷、自适应的决策支持系统。

Method: 构建基于双向数字孪生（DT）的智能虚拟情景室（IVSR），集成多源传感器图像、气象数据与3D森林模型；设计AI驱动的相似性引擎，匹配并校准预计算的灾害模拟库；引入自主AI代理辅助专家决策，并通过标准化流程将无人机调度、人员重配等授权行动闭环反馈至物理层。

Result: 在工业合作伙伴提供的案例研究仿真中，IVSR实现了局部事件检测、隐私保护回放、基于碰撞器的火势蔓延预测及站点特异性机器学习重训练；显著降低了检测到干预的延迟，提升了资源协调效率。

Conclusion: IVSR通过融合实时双向数字孪生与具身AI代理，提供了一种可扩展、半自动化的野火灾害主动式、自适应管理新范式。

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [809] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 本文介绍了stable-worldmodel (SWM)，一个模块化、经过测试和文档化的世界模型研究生态系统，旨在提升可复用性、标准化评估并支持鲁棒性和持续学习研究。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型实现多为论文专用，复用性差、易出错、评估不统一，亟需一个稳定、通用的研究平台。

Method: 设计并实现了SWM：包含模块化架构、标准化环境（支持可控视觉与物理变量变化）、高效数据采集工具、规划算法及基线模型，并进行了充分测试与文档化。

Result: SWM成功支持了DINO-WM的零样本鲁棒性研究，验证了其在实际研究中的实用性与扩展性。

Conclusion: SWM为世界模型研究提供了可靠、可复用、可扩展的基础设施，有助于推动该领域标准化、鲁棒性和持续学习方向的发展。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [810] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5 是一个面向端到端科学发现的统一智能体系统，融合计算建模与实验验证，具备生成、验证与演化三大子系统及深层研究、优化与长程记忆能力，在多项科学推理基准和真实科研任务中展现出领先性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI系统在跨域（计算+实验）科学发现中缺乏统一架构、持续演进能力和协同执行能力的问题，推动自主科学发现向通用化、规模化发展。

Method: 构建由生成、验证、演化三个协同子系统组成的结构化架构，并集成深层研究、解法优化和长程记忆等基础能力，支持跨计算模拟与湿实验的闭环科学探索。

Result: 在GAIA、HLE、GPQA、FrontierScience等科学推理基准上达到领先水平；在算法发现任务中自主设计出有竞争力的机器学习方法；在地球、生命、生物、物理等领域的实证任务中完成端到端计算或湿实验并产出科学发现。

Conclusion: InternAgent-1.5 提供了一个通用、可扩展的自主科学发现框架，标志着AI从辅助科研迈向驱动科研新范式的转变。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [811] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: 本文提出iGRPO方法，通过两阶段迭代式自我条件化强化学习提升大语言模型在数学推理任务中的性能，显著超越现有方法并在AIME24/25上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂数学问题求解中仍存在准确性和一致性不足的问题，需更有效的强化学习对齐方法。

Method: 提出迭代组相对策略优化（iGRPO），包含两个阶段：第一阶段采样多个探索性草稿并选择最高奖励草稿；第二阶段将最佳草稿附加至原始提示，对草稿条件下的细化结果进行GRPO式策略更新。

Result: 在相同rollout预算下，iGRPO持续优于GRPO；在AIME24和AIME25上分别达到85.62%和79.64%的SOTA性能；消融实验表明其细化机制具有泛化性、受益于生成式评判器，并延迟熵崩溃。

Conclusion: iGRPO验证了基于迭代自我反馈的强化学习在提升可验证数学推理能力方面的巨大潜力。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [812] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出了一种面向大语言模型（LLM）训练全周期的分层数据管理框架（L0-L4），强调数据与模型协同演进，利用LLM自身参与数据质量评估与编辑，实现高质量、低成本、高效益的数据利用，并通过实验证明其在训练效率和模型性能上的提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究受限于单向扩大数据规模带来的数据获取成本高、可用性低及训练效率瓶颈，亟需转向数据与模型相互促进的协同演进新范式。

Method: 提出L0-L4五级数据管理框架，覆盖从原始未筛选语料到结构化可验证知识的全过程；各层级定义明确的数据属性、管理策略与训练用途，并利用LLM完成质量评分、内容编辑等数据治理任务；支持预训练、中期训练与对齐等不同训练阶段的数据动态分配。

Result: 实验证明，基于该框架构建的分层数据集显著提升了训练效率与模型性能；配套数据集与工具已开源。

Conclusion: 数据-模型协同演进是通向AGI的关键路径，分层数据管理框架为可持续、可扩展的大模型训练提供了系统性解决方案。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [813] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: 本文提出了GEBench，一个用于评估GUI生成中动态交互和时间一致性的综合基准，并设计了五维评估指标GE-Score；实验表明现有模型在多步交互和空间定位上存在明显瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型的基准主要关注通用视觉保真度，缺乏对GUI场景下状态转换和时间一致性的专门评估。

Method: 构建包含700个样本的GUI专用基准GEBench，并提出五维自动评估指标GE-Score（涵盖目标达成、交互逻辑、内容一致性、UI合理性与视觉质量）；在多个当前模型上进行系统评测。

Result: 现有模型在单步GUI转换上表现良好，但在多步轨迹中的时间一致性与空间定位精度（如图标理解、文本渲染、定位精度）上显著不足。

Conclusion: GEBench为GUI生成提供了首个面向动态交互与时间一致性的系统性评估框架，揭示了关键瓶颈，并为构建高保真生成式GUI环境指明了研究方向。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>
